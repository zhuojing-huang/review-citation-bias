[
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes the Induction-Augmented Generation (IAG) framework, designed to enhance implicit reasoning in Open-Domain Question-Answering (QA) tasks. The framework includes two models: IAG-GPT and IAG-Student. IAG-GPT generates inductive knowledge statements using GPT-3 and combines these statements with retrieved documents for QA tasks. Removing the dependency on GPT-3 at inference time, the IAG-Student model is trained through knowledge distillation with GPT-3 pseudo labels and optimized using a differentiable beam search algorithm.\n\nThis paper presents a well-explained methodology and framework for solving the challenging problem of implicit reasoning in open-domain question answering. This work proposes a novel optimization scheme to train the inductor model through distillation and back-propagation of the generator feedback via differentiable beam scores. The authors conduct a convincing evaluation on two open-domain QA benchmark datasets (CSQA2 and Strategy QA) and achieve state-of-the-art performance on both (at least as of January 2023 for Strategy QA).\n\nAs highlighted by the reviewers, the authors could further strengthen their contribution by conducting, for example, a more thorough analysis of the failure modes of the approach, of the generalization of the method, and of the effectiveness of the inductive knowledge statements."
            }
        },
        "id": "JEprHc2swO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zwqDROxClj",
        "replyto": "zwqDROxClj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1595/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518886,
        "cdate": 1696707518886,
        "tmdate": 1701465435630,
        "mdate": 1701465435630,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a newswire dataset with an annotation framework for labeling partisan and counter-partisan events. \nOverall strengths include the novel dataset and annotation framework that are clearly presented. On the other hand, weaknesses include a lack of clear definition of partisan news, lack of error analysis or any insights on results, and the limited size of the dataset."
            }
        },
        "id": "OQviRvRPjS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zrBrl2iQUr",
        "replyto": "zrBrl2iQUr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5006/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707604591,
        "cdate": 1696707604591,
        "tmdate": 1701465547162,
        "mdate": 1701465547162,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper tries to understand how Transformer models with learned positional encodings manage to understand relative position (as seen, e.g., by relative position attention heads). The authors use DFT and PCA to show that positional embeddings encode sinusoid-like waves, and CCA along with spectral analysis for query and key in attention to show that attention to previous/next token is possible because the phases of these waves are shifted by the same number tokens. The paper’s experiments are only focused on RoBERTa models and the results are applicable only to MLM training objectives.\n\nOverall, reviewers agreed that the discovered mechanisms are interesting and the used analysis tools look solid. The main concerns are that the experiments are limited to a single model, a very small subset of examples used in the analysis (100-200 samples from wikitext-2), very weak discussion of connections to other work. I agree with these concerns and they do not seem to be addressed in the rebuttal."
            }
        },
        "id": "npI07mjO8C",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zpayaLaUhL",
        "replyto": "zpayaLaUhL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1988/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531550,
        "cdate": 1696707531550,
        "tmdate": 1701465449430,
        "mdate": 1701465449430,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This aper presents a framework in which sparsification is employed to efficiently correlate and align textual and visual data for improved video-text retrieval. The most novel contribution of the paper is to cluster the textual tokens into concepts, and align the visual and textual representations in that space. Reviewers agree this presents an interesting idea, and is being applied to an important problem, while being applicable to further types of visual-textual applications, for example social media, recommendation systems, and etc. Authors achieve good results and discuss relationship with related work in-depth during the rebuttal period. Overall the paper is well written, and would be further strengthened by the incorporation of the feedback and the discussion. \n\nOne reviewer mentions that a non-anonymized version of the paper has been published *before* the anonymization period, although I was not able to find it - but this would be fine in any case.\nWhile reviewers are split in their evaluation, the AC agrees with the reviewers that explicitly state that they find this paper \"worthy of acceptance\" and that evaluation on additional datasets/ tasks does not seem warranted. Given the strength of the results, the AC's recommendation is for acceptance to Findings, while acceptance to the main conference would require stronger results (either better on-task performance, or other clear advantages of the approach such as efficiency)."
            }
        },
        "id": "bDcecG4gxx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zeGXjQYhXz",
        "replyto": "zeGXjQYhXz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4649/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595217,
        "cdate": 1696707595217,
        "tmdate": 1701465537640,
        "mdate": 1701465537640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a method for zero-shot intent discovery using a BERT adapter, which is evaluated on two datasets and achieves improved performance over compared baselines. However, one reviewer complains that the contribution of the paper is not clearly stated and its effectiveness compared to other language model adapters is unclear. Another reviewer thought more baselines and comparison should be made, while the differences in results between the proposed method and other approaches are marginal. The authors are asked to provide more information on how their method compares to other existing language model adapters, the tunable and frozen parameters in the model, the size of the trainable parameters, and whether there are any baselines that could be compared to for zero-shot intent classification. The work was also considered more suitable for a demo track paper instead of a regular long paper by one reviewer."
            }
        },
        "id": "pP2UmD1E2d",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zdMislOLTv",
        "replyto": "zdMislOLTv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission117/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479688,
        "cdate": 1696707479688,
        "tmdate": 1701465387922,
        "mdate": 1701465387922,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a new large-scale Chinese lexical substitution (LS) dataset for 3 genres News, Novels, and Wikipedia, from different sources. This resource fills a gap in Chinese NLP. This dataset is meant to serve as the first benchmark for Chinese lexical substitution (LS), and thus has a great potential for the community. Together with the dataset, the intriguing contribution of this paper is a novel ensemble method for producing it, which combines four distinct LS techniques with the goal to enhance the diversity of substitutes. Evaluation has been conducted that positively demonstrates both the quality of the dataset and the effectiveness of their ensemble approach. The insights from this study could be of great value esp. for future similar works. Although the paper has many strengths (see pros. for other positive aspects not mentioned above), it also shows a few flaws which would increase the impact of presentation/publication (see cons) if amended. The following summarizes the most salient. Please refer to original reviews for improvement suggestions. \n\n**Other Pros**  \n\n- Dataset and method are clearly described; descriptive statistics of the dataset are included; \n\n- The paper is very well framed and organized, the experiments clearly motivated and argumentation effective and exhaustive notwithstanding the page number limitation; \n\n- Related literature is well covered; \n\n- shortcomings of existing LS dataset are highlighted; \n\n- It introduces an intriguing novel hybrid approach for the creation of annotated Lexical Substitution datasets, which allows for larger-scale corpus creation confronted with human-only labeled data;  \n\n- Includes a meaningful comparison to existing datasets and methods;  \n\n- Development of the first paraphrase corpus for Chinese for training a paraphrase-based method to be used in the ensemble. \n\n \n\n**Cons** \n\n- lack of discussion (or acknowledgement as a limitation) of the potential noise introduced by automatic translation;  \n\n- missing IAA calculation; authors should integrate the data reported in their response; \n\n- lack of cross-genre and cross-language evaluation or a discussion on the generalizability of the methods;"
            }
        },
        "id": "fFdRbLCv44",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zaBPb6Pu21",
        "replyto": "zaBPb6Pu21",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3500/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564003,
        "cdate": 1696707564003,
        "tmdate": 1701465499411,
        "mdate": 1701465499411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper describes an approach for text-to-SQL generation that leverages a structure-enhanced retriever to get similar examples which can guide the generation process. Mahalanobis contrastive learning is also employed to enhance the relevance of the retrieved examples. \n\nExtensive experiments demonstrate the effectiveness of the proposed solution. The paper can improve in terms of writing and clarity: the proposed architecture is overly complex and hard to follow; furthermore, each the authors should better explain the intuition behind each component and better justify its usage. Also an ablation study (which the authors included only in the rebuttal) should be added to the paper."
            }
        },
        "id": "5exxZgwNOc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zWGDn1AmRH",
        "replyto": "zWGDn1AmRH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1338/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510667,
        "cdate": 1696707510667,
        "tmdate": 1701465427780,
        "mdate": 1701465427780,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Reject"
            },
            "comment": {
                "value": "This paper investigates the relations among different attention heads of the Transformer from the multi-view learning perspective. It brings an interesting consensus issue among different heads, and propose an EMHA attention, which encourages the consensus from Inner-Subspace Interaction and Cross-Subspace Interaction. Experiments on different tasks demonstrate the effectiveness of the proposed methods.\n\nSome reviewers have concerns about the overhead of computation. Also the reproductiblity seems to be an issue, too."
            }
        },
        "id": "DwNFIJbyNW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zVi11zjaPe",
        "replyto": "zVi11zjaPe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2581/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545223,
        "cdate": 1696707545223,
        "tmdate": 1701465469764,
        "mdate": 1701465469764,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper advances the state of the art for response forecasting using novel methodology leveraging augmentation of social graph with LLMs. The claims are backed with strong evidence. The reviewers appreciated the novelty of the work along with the experimentation. The authors are recommended to address the reviewers' concerns in the camera ready, if the paper is accepted."
            }
        },
        "id": "OcCl7GWP56",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zSUOfRVl28",
        "replyto": "zSUOfRVl28",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1221/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507640,
        "cdate": 1696707507640,
        "tmdate": 1701465424025,
        "mdate": 1701465424025,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The manuscript underwent a comprehensive evaluation by four reviewers, revealing both strengths and weaknesses in its content.\n\nPros\n\n1. Novelty and Contribution: The reviewers largely concur that the paper provides a novel investigation into the approximation of two-layer feedforward networks.\n2. Empirical Robustness: The empirical outcomes are broadly considered to be compelling, particularly highlighting the MoEs' competitive edge over dense Transformer architectures.\n3. Community Relevance: By focusing on diminishing computational and memory overhead without compromising model efficacy, the paper holds significant appeal for both researchers and practitioners in the field.\n\n\nCons\n\n1. Lack of Rigorous Justification: The design choices within the MoE framework warrant further elucidation. The reviewers note that these choices are not sufficiently justified, especially regarding their impact on model performance, and the authors were unable to clarify their motivations convincingly in their rebuttal.\n2. Need for Novelty Elaboration: Though the paper incorporates novel components, some reviewers question the extent of this novelty and advocate for a more detailed exposition to distinguish the proposed method from existing MoEs.\n3. Comparative Evaluation: Reviewers express the need for a more systematic comparison with established methodologies, focusing on aspects such as computational complexity, performance metrics, and overall efficiency. The rebuttal partially resolved this issue.\n4. Theoretical Depth: A deeper theoretical or analytical discourse is recommended by the reviewers to both validate the design decisions and to situate the work more solidly within the existing body of literature."
            }
        },
        "id": "nqBOYq2y1e",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zM3mlyflTt",
        "replyto": "zM3mlyflTt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3542/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564916,
        "cdate": 1696707564916,
        "tmdate": 1701465500920,
        "mdate": 1701465500920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new LLM decoding method called isotropic and proximal search based on locality and isotropy with a parameter to interpolate them. The reviewers appreciate the novelty and performance of the new method as well as the clarity of presentation. They raise a few concerns regarding evaluation, such as statistical significance and ablations, . The authors address all major concerns, conducting new experiments where needed. For these reasons I believe this paper should be accepted."
            }
        },
        "id": "XLpdEeBGAo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zLAHDHhgLa",
        "replyto": "zLAHDHhgLa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1736/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707525791,
        "cdate": 1696707525791,
        "tmdate": 1701465439954,
        "mdate": 1701465439954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper discusses a new idea for assessing IAA and annotation quality in large multilingual annotation campaigns characterized by high complexity and minimal overlap in the annotated items. Specifically, the authors introduce a new annotator agreement metric based on word embeddings to better account for the difficulties inherent in human annotation of persuasion techniques and to assess the overall coherence of a dataset. Given the subjective nature of the task, investigations into annotators’ disagreement on the specific techniques can significantly help improve future research. The rebuttal phase was constructive and helped clarify several points. I am confident that the authors will enhance the revised version by incorporating many of the details mentioned in their responses. \n\n**Pros**\n- it introduces a novel, robust method for assessing agreement in large multilingual annotation campaigns, where overlap between annotators is limited; \n- it presents a new metric as a proof of concept and adequately highlights the limitations of the approach;\n- a large-scale annotation campaign was conducted (6 languages and about 40 annotators); \n- the paper reveals high expertise ;\n- the work can be of considerable value for other researchers in the field;\n- the article is well written and clear;\n- results analysis is sufficiently thorough.\n\n**Cons**\n- greater clarity seems to be required in stating and explaining the main objectives of the paper, to avoid misunderstanding. The responses provided by the authors during the clarification phase should be integrated into the revised version; \n- a discussion on the role of subjectivity and on the main sources of disagreement should be integrated;\n- the explicit formula for the new metric needs to be given in the paper;\n- essential details about the experimental setting are missing (see reviewers comments). \nIt is worth noting that these deficiencies have been convincingly addressed in the author responses, suggesting that they would necessitate only minor revisions in the final paper."
            }
        },
        "id": "xWnPFqIoJR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zIgc1Qeceh",
        "replyto": "zIgc1Qeceh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4608/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594513,
        "cdate": 1696707594513,
        "tmdate": 1701465536688,
        "mdate": 1701465536688,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers all agree that the paper proposed a novel and useful approach to analyzing scanned historical documents, building on previous work in the area. The problem of analyzing historical text is of great interest to social scientists and researchers in the humanities, and the proposed method could greatly improve their ability to work with historical text. The use of synthetic training data is clever, and the evaluations are well done. One concern is that the training data is taken from modern, as opposed to historical text. The authors, however, provide a reasonable data availability justification for this point. The potential reasons to reject (modern text, 16 pixel size, confusing cluster analysis, missing comparisons to corpora and BERT) are well responded to by the authors in the rebuttal period."
            }
        },
        "id": "AG8QEdxGEI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zIb2DlqBxm",
        "replyto": "zIb2DlqBxm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1679/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707523189,
        "cdate": 1696707523189,
        "tmdate": 1701465438247,
        "mdate": 1701465438247,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper studies the ‘primacy effect’ in ChatGPT (The primacy effect refers to a tendency to better remember the first piece of information compared to information received later on).\nAuthors observe that ChatGPT's decision is sensitive to the order of candidate outputs/labels in the prompt.\n\nAll reviewers pointed out the interestingness of the key research question -- does ChatGPT exhibit a primacy effect or not? The study has the potential to advance our understanding of the working of LLMs. Reviewers had raised a few concerns and authors seem to have addressed most of them through the rebuttals. For example, authors provided results on additional datasets, with additional temperature settings, and also in a setup where CoT (chain-of-thought) strategy is used with ChatGPT. \n\nOne of the reviewers raised a concern if the observed effect can be explained by the low confidence of ChatGPT for a subset of data. Authors should consider addressing the reviewer's point in future revisions through additional experiments."
            }
        },
        "id": "HRnagqJtHP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zEJFYWWmbG",
        "replyto": "zEJFYWWmbG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission649/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492927,
        "cdate": 1696707492927,
        "tmdate": 1701465406433,
        "mdate": 1701465406433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers agree that the new RULE dataset on understanding rationales behind critical reasoning is a valuable resource, and that the paper is well motivated and well-structured. \nThe reviewers questions and concerns regarding licensing of the resource could be resolved during the discussion."
            }
        },
        "id": "PdlWFB8Crr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "zByqDt16qZ",
        "replyto": "zByqDt16qZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission266/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483369,
        "cdate": 1696707483369,
        "tmdate": 1701465393242,
        "mdate": 1701465393242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces \"Adapter-TST,\" an adapter-based framework for multiple-attribute text style transfer, aiming to address the challenge of fine-tuning large language models with limited data. Adapter-TST utilizes neural adapters and offers parallel and stacked connection configurations, allowing simultaneous control of various stylistic attributes. The proposed method is evaluated on two datasets and compared to baseline models.\n\n**Main Criticisms by Reviewers:**\n\n- **Limited Backbone Models:** Reviewers expressed concerns about the evaluation bias resulting from using only one backbone model (BART-Large). They recommended assessing Adapter-TST with multiple backbone models to ensure generalizability.\n\n- **Lack of Specifics in Human Evaluation:** Reviewers noted the need for more transparency in the human evaluation process, including the number of examples used and the number of workers involved.\n\n\nThe authors have made commendable efforts to address the reviewers' concerns. They provided results for the T5 model to assess generalizability, enhancing the paper's rigor. They also clarified the human evaluation process, offering specific details. Moreover, the authors incorporated newer, relevant baselines for comparison, strengthening their argument for the effectiveness of Adapter-TST.\n\nIn summary, the authors have made significant strides in addressing the reviewers' concerns. While some improvements can still be made, their responses demonstrate a commitment to enhancing the paper's quality and addressing potential limitations."
            }
        },
        "id": "ElXRxVk1Ss",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "z9l6nHpTyT",
        "replyto": "z9l6nHpTyT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1404/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512341,
        "cdate": 1696707512341,
        "tmdate": 1701465429885,
        "mdate": 1701465429885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper is well-written, presents the subject clearly, provides strong experimental results, combines academic and real-world metrics, and offers valuable insights, though some aspects of the evaluation and findings could be improved."
            }
        },
        "id": "JVmLi65Rkh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "z9CqYTwOiO",
        "replyto": "z9CqYTwOiO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3586/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565690,
        "cdate": 1696707565690,
        "tmdate": 1701465502773,
        "mdate": 1701465502773,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose an interesting and promising approach, but the reviewers have identified several points that need to be improved before this work can be published: quantification of similarity between languages, analyses that are not sufficiently detailed, etc."
            }
        },
        "id": "tQDD131qAs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "z8gM4ZfK8l",
        "replyto": "z8gM4ZfK8l",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4095/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583077,
        "cdate": 1696707583077,
        "tmdate": 1701465520242,
        "mdate": 1701465520242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper focuses on a newly proposed task, incremental novel slot detection, and introduces a method combining multiple techniques to tackle this task and mitigate the catastrophic forgetting problem. All reviewers agree on the relevance and applicability of the new task to real-world dialogue system development, and the potential of the framework proposed.\n\nDespite recognizing the merits of the paper in targeting an area of interest, the reviewers raise concerns about the paper due to the shortcomings in experiments, supporting methodology and baselining against already established models/methods."
            }
        },
        "id": "ANDBMBVBy8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "z69tlSxAwf",
        "replyto": "z69tlSxAwf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3931/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579658,
        "cdate": 1696707579658,
        "tmdate": 1701465514579,
        "mdate": 1701465514579,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a word on semantic specialisation of word vectors, based on the extrofitting method, without requiring any external lexicon or manually curated resources. The proposed method improves its baseline on various benchmarks and languages, showing the quality of the approach. \nAfter author rebuttal, the reviewers, in general, agree that the work is simple, language independent, and it improves the performance of vanilla word vectors without the need for an external lexicon. They appreciated the author response, however, the paper would need a better explanation of the intuition behind the proposed method, so the clarity of the paper should be improved, as it reads more like an incremental work."
            }
        },
        "id": "1XogtTHlfk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "z2JVmJ6Tlq",
        "replyto": "z2JVmJ6Tlq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission915/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499327,
        "cdate": 1696707499327,
        "tmdate": 1701465414713,
        "mdate": 1701465414713,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The work is the first to study attribution in cross-lingual QA with two scenarios: in-language and in-English attribution. The authors collect multilingual attribution datasets for languages with diverse typologies and analyze the potential confounding factors in the data collection process such as translation quality, interannotators’ agreement, and the annotators’ background. Then, they present the existing attribution problem in the current ORQA systems and perform error analysis. Finally, the authors presented solutions that help cross-lingual attribution for LLMs and improve QA performance substantially. The authors sufficiently addressed reviewer PwG7’s confusion between the retriever and their proposed attribution detection models and justified the study on the Wikipedia domain in their work. The authors also draw upon the results from the paper to answer reviewers’ questions, which suggests that the experimental setup is careful and sound. Note that in the next iteration of the paper, the training costs should be included as the paper works with extremely large LMs such as PaLM 2. \n\nI think the work is timely as the community is working on the trustworthiness and explainability of large language models that are currently widely used for user-facing QA tasks. Agreeing with reviewer PwG7, I think the paper is well-written and easy to read."
            }
        },
        "id": "9Rw2ZUvZiK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "z1RYLqEpuP",
        "replyto": "z1RYLqEpuP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4417/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589233,
        "cdate": 1696707589233,
        "tmdate": 1701465530944,
        "mdate": 1701465530944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores how to combine two different reasoning methods, Chain-of-Thought (CoT) and Program-Aided-Language Models (PAL), for various inference tasks. The paper uses a large language model (LLM) to dynamically select between the outputs of CoT and PAL, based on two factors: the complexity of the problem and the confidence of the methods. The paper evaluates the proposed method on eight inference datasets, using different LLMs such as Codex, ChatGPT, and GPT-4. The paper achieves state-of-the-art results on two datasets, GSM8K and SVAMP, and provides theoretical and empirical analysis of the method’s effectiveness. Additionally the proposed method is complementary to the self-consistency (a prompting technique) and can be combined with it to further improve the performance. The authors have also been able to justify the concerns raised by the reviewers about the high inference cost of the proposed approach."
            }
        },
        "id": "QCI0AYbclV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ytQFU2XsBR",
        "replyto": "ytQFU2XsBR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission553/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490654,
        "cdate": 1696707490654,
        "tmdate": 1701465403265,
        "mdate": 1701465403265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a 3D recognition dataset for small objects. It’s based on the 3D scene dataset ARKitScenes, but adds 15,553 referring expressions of mostly small objects that have been overlooked in previous 3D recognition datasets that focused more on large objects like sofas and chairs.\n\nStrengths:\n- Reviewers unilaterally agree on the merit of the proposed 3D recognition dataset for small objects.\n- Some reviewers highlighted that this dataset is also larger than prior 3D referring expression datasets, regardless of object size.\n- Most reviewers commended the paper as well written and clear \n\nMain unresolved concern:\n- A small fraction of the data is incorrect. In a random test of 50 samples, 16% of objects in the annotations were either difficult or impossible to localize.\n\nAfter rebuttal, reviewers reached a consensus soundness and excitement scores of 3.\n\nDespite the weakness, the dataset adds value to the community and addresses a gap in current datasets. The authors are urged to visibly highlight the 16% unrecognizable objects when publishing the dataset to allow users to make informed assumptions about the dataset."
            }
        },
        "id": "llaUSoFa6U",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yjqgHcTLnP",
        "replyto": "yjqgHcTLnP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2250/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537875,
        "cdate": 1696707537875,
        "tmdate": 1701465459025,
        "mdate": 1701465459025,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper creates a high-quality pretraining corpus in African languages from mC4, pretrains a Transformer-based language model, and further evaluated on multiple downstream tasks. The paper has a clear beneficial contribution to the community, not only in the trained model but also in cleaned corpus and preprocessing strategies. However, some straightforward improvement points of the current submission are identified by the reviewers such as fair comparison (e.g., \"results should include mC4 averages without unseen languages\"), and presentation clarity (e.g., the language code \"pcm\" should be explicitly mentioned as Nigerian Pidgin either in the main text or the appendix). During the rebuttal, further constructive comments on the analysis of the \"quality\" of the pretraining corpus have been brought up, especially since the paper is focused on auditing and cleaning mC4, but less ablation study is conducted. Nevertheless, the paper contributes to low-resource and underrepresented languages, especially on the creation of a new and better quality corpus along with empirical comparison against commonly-used multilingual models. We believe that this contribution is beneficial for the NLP community. Based on the reviews and rebuttal, the mmends accepting this paper, but the authors are highly encouraged to address the comments by Reviewer ckvB and Reviewer A7Gh."
            }
        },
        "id": "6oxCfGr5Fa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ybc9V6Cbq2",
        "replyto": "ybc9V6Cbq2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4823/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599082,
        "cdate": 1696707599082,
        "tmdate": 1701465542682,
        "mdate": 1701465542682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces the Sparse Universal Transformer (SUT), an innovative variant of the Universal Transformer that integrates the Sparse Mixture of Experts (SMoE) into the feedforward and multi-head attention mechanisms. This alteration offers computational efficiency and the presented dynamic halting mechanism provides a simpler approach than the original Universal Transformer. Through experiments, SUT demonstrates enhanced translation quality, improved compositional generalization, and better computational trade-offs on several benchmarks like WMT 14 English-German. However, concerns have been raised about the thoroughness of the experimental evaluation, the clarity of method descriptions, and the originality of some contributions.\n\nPros:\n\nIntroduction of a well-motivated and technically sound SUT architecture that amalgamates the efficiency of Transformers with the recurrence of Universal Transformers.\n\nA more efficient dynamic halting mechanism, which not only simplifies but also improves performance.\n\nThe paper's comprehensive approach, detailing a breadth of related work and providing results against numerous baselines.\n\nCons:\n\nParts of the paper's method, especially concerning the SMoE and MIM loss, seem to rely heavily on prior work, questioning the originality of the contribution.\n\nInsufficient evaluation on some datasets (partially addressed in rebuttal)."
            }
        },
        "id": "n02bXRIHj9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yXYJPAlLqn",
        "replyto": "yXYJPAlLqn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4594/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593960,
        "cdate": 1696707593960,
        "tmdate": 1701465536100,
        "mdate": 1701465536100,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a multi-level content planning approach for generating complex questions, specifically at both the phrase and sentence levels. Experiments, including ablation studies, conducted on standard datasets, i.e., HotpotQA and SQuAD, show the effectiveness of the proposed solution. The paper is well-structured and easy to follow, and its findings may be of interest to the QA community."
            }
        },
        "id": "wAs5TWDuXR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yXVLsdvyg9",
        "replyto": "yXVLsdvyg9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3397/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561854,
        "cdate": 1696707561854,
        "tmdate": 1701465495984,
        "mdate": 1701465495984,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents FinLMEval, an evaluation framework for assessing the performance of language models on financial natural language processing tasks. The authors conduct experiments to compare the performance of ChatGPT with fine-tuned auto-encoding models like BERT, RoBERTa, and FinBERT. The paper addresses a gap in the literature by focusing on the financial understanding abilities of large language models like ChatGPT. Reviewers mention that the paper could have been more comprehensive by including other large language models for comparison. There are also questions about the soundness of the methodology, especially regarding the specific NLP tasks chosen for evaluation. In light of the identified merits and shortcomings, the collective judgment leans toward accepting the paper for the \"Findings\" section. The manuscript makes a sound yet incremental contribution to the field, and its limitations could be addressed in future work."
            }
        },
        "id": "WV4hgrQKDZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yVoLLzLwdp",
        "replyto": "yVoLLzLwdp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2825/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550211,
        "cdate": 1696707550211,
        "tmdate": 1701465477795,
        "mdate": 1701465477795,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper focuses on hallucination detection in the domain of car manual-based QA. The paper introduces a new dataset, DelucionQA, for this task based on the manual of Jeep's 2023 Gladiator model. They use ChatGPT to generate answers and manually verified hallucination in answers based on the question, answer, and context. The authors then use the proposed dataset to develop hallucination detection systems.\n\nPros:\n1. They introduce a new dataset for hallucination detection.\n2. The paper benchmarks the proposed dataset with several methods. \n\nCons:\n1. Limited generalizability. The data is only based on the manual of Jeep's 2023 Gladiator model and answers from the ChatGPT model. Reviewers point out that it is unclear how well the proposed dataset can generalize to different domains and LLMs (Language Model Models).\n2. Lack of reliability in hallucination detection models. Reviewers point out that it is a reasonable approach to detect hallucination by measuring the similarity between the model's answer and context. More sophisticated methods and analyses are expected."
            }
        },
        "id": "3pXH53TLl9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yThuxNysaJ",
        "replyto": "yThuxNysaJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4576/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593402,
        "cdate": 1696707593402,
        "tmdate": 1701465535411,
        "mdate": 1701465535411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviews for this paper were quite consistently positive, with scores 4,4,3 (soundness) and 4,4,4 (excitement).\n\nCondensing the reviews, the following were among the strengths and weaknesses mentioned:\n\nStrengths:\n\n- The reviewers consistently found the paper well-written, and the results interesting.\n- Highlights systematic failures of LLM based agents (R1, R3)\n- interesting psychological settings (R2)\n\nWeaknesses:\n\n- some lack of clarity on implemention/evaluation (R1, R2, R3)\n- small action space (R1) and limited settings (R2, R3)"
            }
        },
        "id": "sZxAXOUYcX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yO4cAfFjlp",
        "replyto": "yO4cAfFjlp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4178/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584826,
        "cdate": 1696707584826,
        "tmdate": 1701465523184,
        "mdate": 1701465523184,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This position paper investigated an important question, i.e. how should we evaluate LLMs. Using David Hays' definition of trustworthiness, the authors outline 4 desiderata for trustworthy models. The authors discuss some avenues for improving the trustworthiness of NLP systems. It is basically an excellent position paper and well motivated by the introduction section. The analysis of the desiderata is also well executed. It also comprehensively surveys and organizes existing work within a limited space."
            }
        },
        "id": "GBENDge5vI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yKLUvxMCQ3",
        "replyto": "yKLUvxMCQ3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3304/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560088,
        "cdate": 1696707560088,
        "tmdate": 1701465493351,
        "mdate": 1701465493351,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This research proposes a novel post-processing methodology to mitigate data degeneration in cross-modal retrieval.  \n\nPros:\n*The research methodology is robust and clear. \n*The results and experiments are well-supported by ablation studies.\n\nCons:\n* The feasibility of the setting requires additional elaboration.\n* The mathematical notation is difficult to follow"
            }
        },
        "id": "gOPh33w04c",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yF3lSXb82y",
        "replyto": "yF3lSXb82y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4066/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582354,
        "cdate": 1696707582354,
        "tmdate": 1701465519000,
        "mdate": 1701465519000,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the impact of individual source and target context in GPT-3 based translation. The authors find that the target context matters more than the source context, and accordingly propose a zero-shot prompting approach. While the finding is interesting, this paper can be further improved from several aspects:\n\n1. Clarify the importance of the findings (e.g. response to Reviewer gFZk);\n2. Clarity the neural-based metric is the main evaluation assessment;\n3. More results on open-sourced Llama2 are appreciated (as promised in the response to Reviewer uUav);\n4. Add the necessary content of the response to Reviewer uUav (e.g. answering in 1., 2., B., C. and D) to make the paper more clear and fluent."
            }
        },
        "id": "9UwxkNpccR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yE44WcphJY",
        "replyto": "yE44WcphJY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5587/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615372,
        "cdate": 1696707615372,
        "tmdate": 1701465562844,
        "mdate": 1701465562844,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper focuses on an unexplored conversational query generation problem setting that generates queries with ambiguous user intent. To tackle this problem, this paper leverages social commonsense reasoning to guide query generation. The main contribution of this paper is the query generation framework that consists of a finetuned topic identifier, a fixed commonsense-based response generation (i.e., Cosmo), and a finetuned instruction-based query generation.\n\nThe problem is practical and the results achieved are encouraging. Nevertheless, reviewers have the following concerns:\n1. Minor improvements in the final response. While the authors provide their response, I do not think this concern has been fully addressed. From the response, it seems that high-quality queries are not that important. \n2. The paper fails to state the benefits of using the proposed framework to generate queries instead of utilizing ChatGPT directly. Besides, there are no efficiency analyses.\n3. Finetuned Flan-T5 without commonsense reasoning not compared."
            }
        },
        "id": "hQKpzEpsfl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yDeIWA7ICp",
        "replyto": "yDeIWA7ICp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4159/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584387,
        "cdate": 1696707584387,
        "tmdate": 1701465522489,
        "mdate": 1701465522489,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose a method, MixTEA, for semi-supervised entity alignment based on a Teacher-Student architecture that works by training the student model using alignment signals from both manually labeled mappings and probabilistic pseudo mappings generated by the teacher model. Compared to existing works, the claimed methodological innovation mitigates issues associated with noisy pseudo mappings and/or lack of pseudo mapping uncertainty consideration in utilizing the pseudo labels.\n\n== Quality ==\nThe reviewers believed the methodological innovations were clearly described and the paper was clearly written overall. This is a widely studied problem and the authors propose a conceptual improvement to account for uncertainly and operationalize it in a sensible way with a teacher-student model. Finally, the authors provide an extensive comparison relative to competing EA methods and a good ablation study -- where the results are good overall and establish a state-of-the-art solution under realistic constraints.\n\n== Clarity ==\nAs stated in the 'quality' section, the paper is well-written overall. Additionally, the authors make good use of figures, etc. to better clarify methodological explanations. However, there were also some concerns regarding attributing empirical improvements to the specific methods proposed. Specifically, even after rebuttal and discussion, it still wasn't clear to the reviewers after discussion how much the empirical improvements were due to the proposed methods and how much was due to general empirical improvements in working to outperform a known target (i.e., hyperparameter optimization). This may be able to be remedied with more details of the experimental setup and further discussion of results. Clearly, additional theoretical understanding of BDV and MDR based improvements would be useful (or references to support this from other settings), but minimally a more detailed empirical analysis is necessary to establish their relative contributions to the the improvement of the models. This was partially addressed in the rebuttal, but the reviewers remained unconvinced that the authors have made a strong case regarding the specific empirical contributions of the proposed improvements.\n\n== Originality ==\nThe paper proposes multiple novel, interesting, and sensible approaches, such as BDV (bi-directional voting for more comprehensive pseudo mappings) and MDR (matching diversity-based rectification for dynamic mitigating the influence of noisy mappings). For the most part, these innovations are specific to EA and build on existing baselines and datasets. However, there is more conceptual innovation than many EA papers.\n\n== Significance ==\nThis is a widely studied problem where performance improvements have practical and potentially impactful implications. Within the EA-specific community, this work will very likely be used as a baseline for future comparisons. That being said, this work builds on existing experimental setups and datasets, so it is primarily a methods contribution. Additionally, the proposed methods to provide support for teacher-student models as an uncertainty modeling technique, but in general isn't clearly going to influence work in other tasks. Thus, it is a solid work within this application."
            }
        },
        "id": "O0fx3TgP32",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yBd2UREDNL",
        "replyto": "yBd2UREDNL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission187/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481379,
        "cdate": 1696707481379,
        "tmdate": 1701465390491,
        "mdate": 1701465390491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a new dataset for zero-shot stance detection called EZ-STANCE which includes both noun-phrase and claim targets, and it is more comprehensive than previous datasets. \nA good set of experiments and analyses is performed on this dataset.  \nThe reviewers all found it a helpful dataset for different research opportunities.\nThe authors reported new results in the rebuttal which should be included in the next version if accepted."
            }
        },
        "id": "hi8dZnHxMz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yB8cQIICqe",
        "replyto": "yB8cQIICqe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3745/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707571289,
        "cdate": 1696707571289,
        "tmdate": 1701465508478,
        "mdate": 1701465508478,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes ABEL, a training strategy to iteratively train retriever and reranker for unsupervised dense retrieval in the zero-shot setting. The reviewers' opinions on this work are not unanimously positive. While the authors have addressed certain concerns raised by the reviewers, there are still some lingering apprehensions. The authors have provided appropriate responses to jdQL's concerns regarding method details and comparison with baselines, which resulted in an increased score from R1. Both 3ATb and ACNC expressed that the novelty and contributions of the proposed method were limited. The authors have provided detailed responses highlighting the differences between ABEL and previous methods, while also clarifying the contributions of ABEL. However, these concerns still persist."
            }
        },
        "id": "5uZ6nNYoPv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "yAZSZob2dN",
        "replyto": "yAZSZob2dN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1266/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508879,
        "cdate": 1696707508879,
        "tmdate": 1701465425638,
        "mdate": 1701465425638,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The core idea of this work is to aggregate similar frames each video and similar patches within each frame, with bipartite matching, to reduce the computation cost of processing long-form videos. Experiments on paragraph-to-video captioning and long-form videoQA, demonstrates the efficiency and performance gain of the approach. While the idea of token merging has been explored previously in ToMe, as mentioned by the reviewers, the adaption of it to long-form video processing requires specific designs such as decoupling frame-level and patch-level aggregation. Thus there is still merit of the approach to the community. One missing point of the work is to test the approach against ToMe on pure video tasks, since the approach works for videos alone as well. AC encourages the authors to provide experiments to make the work in a more complete form."
            }
        },
        "id": "pNQGwMPiEH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "y8ebFPsyET",
        "replyto": "y8ebFPsyET",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1936/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530350,
        "cdate": 1696707530350,
        "tmdate": 1701465447711,
        "mdate": 1701465447711,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This papers presents VIP, an inference-time dataset designed to explore models' reasoning capabilities through video chain-of-thought. To ease the video processing, two forms of textual descriptions are used to describe extracted keyframes, unstructured dense captions and structured scene descriptions that identify the focus, action, mood, objects, and setting. Two tasks are created for the datasets, Video Infilling and Video Prediction. The evaluation is conducted using GPT-4 and Vicuna shows current LLMs still struggle with complex video reasoning. Overall, as pointed out by the reviewers, the paper is well organized with clear motivation, the idea is interesting, the dataset and the pipeline that constructed the dataset could be useful in assisting research in this direction."
            }
        },
        "id": "gtN0gMJ6bH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "y6Ej5BZkrR",
        "replyto": "y6Ej5BZkrR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5079/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606451,
        "cdate": 1696707606451,
        "tmdate": 1701465549355,
        "mdate": 1701465549355,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a large scale evaluation of the performance of ChatGPT on Modern standard Arabic and other Arabic dialects. The evaluation cover 44 distint taks and 60 datasets. \n\nThe reviewers found this to be a valuable contribution in terms of benchmarking ChatGPT performance on Arabic tasks. There are a few concerns about the paper is too condensed giving the number of evaluation tasks and datasets. It would be great if the authors add more insights on the evaluation in the paper or appendix."
            }
        },
        "id": "ZurLYTU469",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "y5ctUSk99X",
        "replyto": "y5ctUSk99X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4304/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587070,
        "cdate": 1696707587070,
        "tmdate": 1701465527363,
        "mdate": 1701465527363,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a dual-channel span generation approach to address the Aspect Sentiment Triplet Extraction (ASTE) task, where two relational graph attention networks are used, namely, syntactic dependency graph and part-of-speech graph. The paper is well written. The solution is reasonable and can help reduce the span candidates. Experiments show the effectiveness of the proposed approach, and can achieve state-of-the-art performance especially on aspect term extraction (ATE) task, while opinion term extraction (OTE) can be improved. The paper uses a lot of acronyms without introduction, e.g., Dual-GAT as pointed out by reviewers. Same for SGAT and PGAT. Please fix all of them by explaining what they mean in the paper. Please add error analysis and improve the related work section. Also please include the responses to reviewers to the paper to make the unclear part clear."
            }
        },
        "id": "hQDQzuOyNS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "y5UTUcTQU5",
        "replyto": "y5UTUcTQU5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3124/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556600,
        "cdate": 1696707556600,
        "tmdate": 1701465487826,
        "mdate": 1701465487826,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work presents a method for addressing time-sensitive questions. The methodology comprises two steps: 1) a temporal extraction system of time and temporal relationships to construct a graph, and 2) the fusion of these temporal graphs with transformer models. Two fusion techniques are explored for this integration: directly as text, and using graph convolutions. The method is evaluated in SituatedQA and TimeQA datasets. The reviewers highlighted the strong empirical results, that the manuscript is well-organised and easy to follow. They also raised issues the work being incremental in relation to similar ideas and that some baseline evaluations were missing. During the rebuttal period those and other concerns were partially addressed by the authors who re-implemented an FiD model."
            }
        },
        "id": "BUGPlSAF6U",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "y34lg6q50A",
        "replyto": "y34lg6q50A",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1178/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506711,
        "cdate": 1696707506711,
        "tmdate": 1701465422895,
        "mdate": 1701465422895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper received slightly disparate scores: 3,2,3,4 (soundness) and 4,3,4,4 (excitement). Three of the reviewers have rated the soundness & excitement as Good or Strong, whereas one reviewer (R2 / DLxR) raises objections and provides lower scores.\n\nCondensing the reviews, the following major strengths and weaknesses were noted:\n\nStrengths:\n- everyone agreed that the paper targets a key issue in LLMs\n- the reviewers specifically praised various aspects of the paper, such that nontriviality of the result (R2), thorough experiments (R3), and good presentation (R4)\n\nWeaknesses:\n\n- testing on a single dataset (R1,R2) and a single model (R2,R3), with some concerns about lacking details on the newly constructed dataset (R1)\n- a limited set of baselines (R1)\n- scope is limited to simple factoid sentences (R1)\n- a range of unclear design decisions were mentioned (R2,R3)\n- concerns about the link between extractability with a probe and causality on the model behavior (R2)\n\nR2 provided a low soundness score. However, the concerns they mentioned were overall similar to those mentioned by the other reviewers, and the authors addressed them to some extent. I thus believe that these soundness concerns do not warrant rejecting the paper."
            }
        },
        "id": "lT8i5MlWsV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "y2V6YgLaW7",
        "replyto": "y2V6YgLaW7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission12/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476757,
        "cdate": 1696707476757,
        "tmdate": 1701465383977,
        "mdate": 1701465383977,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a factuality-oriented abstractive summarization model DASum. The model is based on the factual relation discrimination task, guiding DASum to generate true factual relations. Also, the authors construct a new factual summarization dataset using data augmentation to generate negative samples. Experimental results on two datasets demonstrate the DASum's superiority over several SOTA benchmarks on factual metrics. \n\nThe reviewers mentioned several pros of this work, such as:\n1. The proposed method is novel,\n2. The method is clearly explained and is easy to follow,\n3. Comprehensive experiments show significant improvements compared with other counterparts.\n4. The evaluation is thorough and contains both automatic and human parts.\n\nI also want to mention the contribution to the data resources for the task of factual summarization. \n\nThe authors addressed all comments (including missing evaluation metrics, motivating the choice of augmentation methods, lacking a more thorough limitations section, etc.) in their rebuttal."
            }
        },
        "id": "LhoXOuNVcF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "y0P5KXN5X1",
        "replyto": "y0P5KXN5X1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1302/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509953,
        "cdate": 1696707509953,
        "tmdate": 1701465426707,
        "mdate": 1701465426707,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper focuses on multi-modal entity alignment (MMEA). To make better use of different information of MMKGs (neighbors, attributes, and types in this paper), the authors design a prefix-injected MMKG transformer with a hierarchical modifiable self-attention. The evaluation uses two cross-KG EA datasets FB15K-DB15K and FB15K-YAGO15K. The proposed framework achieves remarkable performance on these datasets, outperforming state-of-the-art methods."
            }
        },
        "id": "5qYWRUCvbQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xzveggFhiQ",
        "replyto": "xzveggFhiQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission314/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484519,
        "cdate": 1696707484519,
        "tmdate": 1701465394589,
        "mdate": 1701465394589,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "(I've used bqm3's updated soundness score of 4, which was mentioned in the comments but not reflected in the numeric score).\n\nThe paper focuses on the problem of cultural adaptation in multimodal reasoning, in which some concepts are difficult to translate directly from one language to another. They suggest an annotation-free process for augmenting data using a \"code switching\" approach. The reviewers are unanimous that the experiments provided are well done and provide support for the authors' claims about the benefits of their approach. The detailed author responses clarify outstanding questions and suggest careful thinking and thorough work.\n\nThe reviewers raised several significant questions about the details of the work, including the source of the cultural concepts they use. The authors provide more details about the crowdsourced data collection effort, which is also described in Appendix A.\n\nSeveral other questions that reviewers raised are thoroughly answered by the authors. Due to space constraints, not all of the answers may appear in the final text, but seem sufficient to address the reviewer questions."
            }
        },
        "id": "l4WfxY9BGw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xyvTFX7hDs",
        "replyto": "xyvTFX7hDs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5403/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612396,
        "cdate": 1696707612396,
        "tmdate": 1701465558522,
        "mdate": 1701465558522,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper analyses a data augmentation technique, STEMCORRUPT. The method is used to produce synthetic examples for the morphological inflection task by substituting stem characters. The authors demonstrate the utility of the technique, both theoretically and empirically. The paper also explores data-efficient strategies to sample from the synthetically generated dataset. As most reviewers outline, these research directions are particularly important for low-resource language technology. The reviewers also noted that experiments are conducted on 7 typologically diverse languages, giving this work a stronger basis. Some reviewers pointed out the flaws such as small training data and the focus on concatenative morphology only but the authors addressed and clarified them very neatly.\nOverall, I think the work will be a nice contribution to NLP, and low-resource technologies in particular."
            }
        },
        "id": "U8navpDZkY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xxTtwEuOpS",
        "replyto": "xxTtwEuOpS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2194/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536634,
        "cdate": 1696707536634,
        "tmdate": 1701465457410,
        "mdate": 1701465457410,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary (adapted from reviewer os6q): This is an exploratory paper that studies and exposes how pre-trained multilingual language models inherently contain bias or make distinctions among European countries. The authors show a reflection of geopolitics (Eastern vs Western European countries) and economy based (in terms of GDP) distinctions. Prestige related to jobs is also explored and is found to generally not correlate between country-related prestige.\n\nThe reviewers found this study to be generally sound (3/3/3) and acknowledged that it covers an interesting and novel topic.\n\nTwo reviewers raised concerns that the stereotypes explored in this work may prove harmful. This was well-addressed in the limitations and ethical considerations section of the paper.\n\nThere was some disagreement about the ease with which the paper could be understood, with one reviewer citing difficulty, one reviewer praising how well-written it is, and the other not mentioning it at all. The reviews suggest some areas that would benefit from expansion of the paper into 5 pages if accepted.\n\nOverall, this paper provides an interesting and unique contribution, even if it addresses just two small questions about the biases of these models."
            }
        },
        "id": "dlWm3cydYA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xp4wEivhM9",
        "replyto": "xp4wEivhM9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3688/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567754,
        "cdate": 1696707567754,
        "tmdate": 1701465506791,
        "mdate": 1701465506791,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Three reviewers provided feedback for this work and were in consensus. They found the paper to be well written, the empirical study to be well done and well presented and the new metric to be sound. There were very few complaints for this paper. One complaint was questioning if object hallucinations were an important problem in todays VLMs. A second complaint was about using yes/no responses to measure this problem. And a third complaint was about expanding to all COCO categories. The rebuttals provided by the authors were very thorough and in my opinion have addressed all these concerns."
            }
        },
        "id": "WXHGMJk7Sb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xozJw0kZXF",
        "replyto": "xozJw0kZXF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1416/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512676,
        "cdate": 1696707512676,
        "tmdate": 1701465430210,
        "mdate": 1701465430210,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper targets the problem of event ontology completion with the help of clustering, type naming, and hierarchy extension, that can be applied to downstream applications.\n\nThe proposed EOC task appears closer to the needs of practical applications than the ETI task it derives from. Solid experimental evaluation providing insights both on HALTON design aspects and the considered EOC sub-tasks themselves.\n\nHowever, some concerns might regard the incremental nature of clustering technique, which is yet considered in ETI."
            }
        },
        "id": "Xe85niFGyo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xn8NKZosDV",
        "replyto": "xn8NKZosDV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1035/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503467,
        "cdate": 1696707503467,
        "tmdate": 1701465418529,
        "mdate": 1701465418529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Overall, this paper addresses the growing discussion of ToM in the context of LMs, working towards a more robust evaluation protocol. \nThe reviewers overall slantly positive on the paper, but don't necessarily arrive at the strongest consensus on its strengths and weaknesses.\n\nCollectively, as AC I see the following points as the clearest strengths and weaknesses:\nS: Rigor/structured approach to ToM, concrete and situated evaluations\nW: Unclear whether the approach to eval (derived from children + 10 tasks initiated based on taxonomy) make sense, concerns about spurious correlations are not fully addressed\n\nIn a brief reading of the paper, I agree that the paper makes important progress on a timely topic with substantive soundness. I will say I do share the concern that matter of spurious correlations, which seems to be at the heart of why evaluations for ToM in particular will be challenging to design + trust, has not been adequately resolved. If accepted, I encourage the authors to spend the majority of the additional page strengthening this topic. Separately, I think the community would be collectively more excited/inspired by the work if the results offered more striking analysis: I am left a bit unwhelemd by the results/discussion of results. I understand a well-designed evaluation is itself a great service to the community, but I think making the results (and/or their analysis) richer can really help draw interest to the evaluation as well, which is likely of interest for driving future work in this space."
            }
        },
        "id": "iM7nOc9pgQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xlg5jVmPSg",
        "replyto": "xlg5jVmPSg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4289/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586739,
        "cdate": 1696707586739,
        "tmdate": 1701465526666,
        "mdate": 1701465526666,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a novel zero-shot referring expression segmentation approach utilizing the SAM model, pretrained CLIP, and BLIP2. Pros include: (1) Strong performance on widely-used benchmarks, as highlighted by its zero-shot capabilities and innovative combination of visual-text matching and alignment quality scores. (2) Comprehensive ablation studies and well-conducted experiments that contribute to a deeper understanding of the method's efficacy. (3) A unique Text Augmented Spatial-aware (TAS) framework that offers new avenues for leveraging text information in image segmentation tasks. \nCons comprise: (1) A sense of incrementalism, with some components of the proposed method bearing similarity to existing methods, thus questioning its true novelty. (2) Lack of clarity on the real-world applicability of the Negative Text Miner in handling diverse image descriptions and semantically varied phrases. After rebuttal and discussion, the AC feels that most concerns are addressed and this work is solid enough to appear in the conference but reviewers are generally less excited about it for the novelty concern."
            }
        },
        "id": "yvDm4w4leU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xhqICRykZk",
        "replyto": "xhqICRykZk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission262/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483296,
        "cdate": 1696707483296,
        "tmdate": 1701465393203,
        "mdate": 1701465393203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new parameter- and memory-efficient fine-tuning method that shows significant memory saving compared to other PEFT methods while achieving competitive performance. Concretely, the paper proposes first fine-tuning PEFT modules in a small language model learning task characteristics, then fine-tuning the resulting PEFT models with up/down projections (plug-in module) and a bridge model that is pruned from the large language model. For the inference, the final PEFT modules are inserted into large LM with plug-in up projection without further fine-tuning. The authors presented extensive experiments that include 2 model families (GPT, T5) at different sizes that are evaluated on SuperGLUE benchmark. \n\nAs the reviewer mentioned, the proposed method consistently achieves comparable performance with full fine-tuning large model saving a significant amount of memory footprint. Compared to standard PEFTs, the proposed method achieves up to 5.7x memory reduction. The experimental setup is sound and the results are convincing. \n\nHowever, also the reviewers noted, that phrasing and the number of updated parameters in tables (as 0%) may mislead the conclusion. PEFT modules are updated but using small LMs (and the bridge model) which leads to significant memory reduction. I believe rephrasing or clarification highlighting this point will improve the paper. Connected to this point, the authors acknowledged the issue with the updated parameter number in the tables.\n\nFurthermore, the authors provided an additional comparison with another memory-efficient baseline based on the reviewer's suggestion during the discussion period which fill the corresponding gap."
            }
        },
        "id": "GAUCFZRmlP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xfTQmGPPtQ",
        "replyto": "xfTQmGPPtQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2776/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549146,
        "cdate": 1696707549146,
        "tmdate": 1701465475982,
        "mdate": 1701465475982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an innovative new benchmark for assessing the comprehension of figurative language / images, and provides baseline performances of several models. The performance of models tested in the paper (sota at submission time) is far from human performance. It also shows that generative models like Dall-E struggle to generate figurative images.\nThe paper provides a useful problem description, distinguishing metaphors, similes, and idioms."
            }
        },
        "id": "kAcxmh2sf7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xeecFHJ4d4",
        "replyto": "xeecFHJ4d4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission375/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485939,
        "cdate": 1696707485939,
        "tmdate": 1701465396622,
        "mdate": 1701465396622,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This is a well-presented paper which connects local (sentence, token-level) propaganda signals with global discourse structure, via both statistical analyses and empirical evidence via discourse aware propaganda prediction models.\n\nThe contribution on the conceptual (local vs global aspects of propaganda) and empirical (strong evaluation) levels are strong and are likely of broader interest to researchers in the field of propaganda prediction.\n\nOne remaining concern with the paper is its contextualization in theory and related work, by (1) incorporating a clearer definition of propaganda and distinguishing it from the phenomenon of \"media bias\" and (2) a more structured and exhaustive review of relevant related work as per the detailed suggestions by reviewer xm9p."
            }
        },
        "id": "PaG0b6apLo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xbnNgqGefc",
        "replyto": "xbnNgqGefc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2079/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533914,
        "cdate": 1696707533914,
        "tmdate": 1701465453207,
        "mdate": 1701465453207,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces SUPMER which leverages meta-learning to enhance the prompting of PLMs. SUPMER utilizes a self-supervised framework to initialize soft-prompts and incorporates meta-gradient regularization, enabling domain generalization. Compared to existing methods, SUPMER demonstrates superior performance by learning meta-prompts through anchor tasks and applying them to a series of meta-tasks. SUPMER addresses common challenges in prompting methods, such as the need for effective soft prompt initialization and mitigating overfitting in few-shot settings due to limited training examples.  \n\n\nThe reviewers initially raised many concerns and the authors provided extensive rebuttal responses. All reviewers engaged in post-rebuttal discussions and converged to find the work sound with moderate excitement scores and acceptable reproducibility."
            }
        },
        "id": "apRi7ntJ4Z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xarWXEhhdy",
        "replyto": "xarWXEhhdy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2629/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546293,
        "cdate": 1696707546293,
        "tmdate": 1701465471341,
        "mdate": 1701465471341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary (adapted from Reviewer 8bjC): This paper creates a multi-lingual data collection that contains information on compounds and their boundaries. Using the data collection, a number of systems are developed that aim to automatically identify the compound boundaries. The dataset and thus the evaluation are performed on a much wider set of languages than previous work, which tended to focus on compound-heavy languages. Wiktionary and the vocabulary of a large-scale web corpus are used as data sources.\n\nThe soundness (3/3/4) and excitement (3/4/4) scores suggest the paper is overall sound and there is excitement for its content. The discussion period identified minor clarifications that the authors should make in revisions, and that there are some desirable follow-up experiments to be run that were not feasible due to computational resource limitations. Reviewer X8iv stated that there are efficient ways to run these resources and that resource limitations should not be a barrier; the authors may want to address this in a discussion or limitations section.\n\nAs reviewer X8iv points out, there is no extrinsic (i.e. a downstream, non-decompounding task) evaluated. As reviewer WAo7 points out, it’s not clear that this approach will have any impact outside of the decompounding task.\n\nHowever, this paper provides both a much larger dataset than was available through previous work, and substantial experiments, including showing that their approach even outperforms hand-tuned per-language models (Table 2). The reviews suggest that this is a substantial contribution, even if perhaps in a narrow area that does not yet have demonstrated downstream impact.\n\nA historical note for the authors: you have correctly identified work such as Koehn and Knight (2003) in the early 2000s as the first widely known statistical unsupervised decompounding approaches. What is less widely known is that there was a series of unsupervised (and later adding semi-supervised) morphological decomposition shared tasks known as Morpho Challenge, running from 2005-2010 [1]. While they did not separately evaluate decompounding, it was key to performance, and [2] explicitly integrates Koehn and Knight’s objective into a morphological analyzer lead to then state-of-the-art results in Finnish. Whether you choose to discuss or review this work is of course, entirely up to you.\n\n[1] http://morpho.aalto.fi/events/morphochallenge/\n[2] C. Lignos, Learning from Unseen Data. Proceedings of Morpho Challenge 2010. https://aaltodoc.aalto.fi/bitstream/handle/123456789/827/isbn9789526033303.pdf?sequence=1&isAllowed=y#page=37"
            }
        },
        "id": "uYzgBHcMTR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xapBkUt0yf",
        "replyto": "xapBkUt0yf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission810/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496543,
        "cdate": 1696707496543,
        "tmdate": 1701465411392,
        "mdate": 1701465411392,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary**: The reviewers reached a consensus that the proposed method of using concept predictor was well-motivated and well-supported in the paper. Also, reviewers acknowledged that the paper was well-written and state-of-the-art performance is achieved. Therefore, all the reviewers are positive about the results in the paper. In the meantime, several reviewers pointed out a few missing prior works and references which the authors acknowledged during the rebuttal and mentioned they would add them in the final paper. Also, a few reviewers questioned about the inference speed comparison with other methods which was presented by the authors during the rebuttal. Overall, the reviewers seemed satisfied with the authors' responses.\n\n**Reviewers' recommendations**: Four reviewers share that the paper is sound or strong in terms of soundness. (3 'Good's and 1 'Strong'). In terms of the excitement, the reviewers also share the same score distribution which make the paper is sound and somewhat exciting."
            }
        },
        "id": "F4y30Ve6Xw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xX2KjzdFPH",
        "replyto": "xX2KjzdFPH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2518/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543779,
        "cdate": 1696707543779,
        "tmdate": 1701465467534,
        "mdate": 1701465467534,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces an adaptive prompt generation framework designed to enhance the performance of off-the-shelf Large Language Models (LLMs) in task-oriented dialogue systems. The proposed framework incorporates a trainable slot generator that produces domain-specific slots, which are then utilized for subsequent prompt generation. This adaptive prompt generator is capable of generating belief states and system responses. The framework's effectiveness was evaluated using the MultiWOZ 2.0 benchmark and tested in conjunction with SOLOIST and ChatGPT.\n\nThe soundness scores were recorded as (2, 3, 2). The reviewers expressed multiple concerns regarding the paper's experimental fairness, highlighting:\n\n1. The use of an outdated off-the-shelf LLM.\n2. The absence of generation metrics.\n3. The lack of complex-slot tasks.\n4. The lack of an ablation study for the proposed sub-techniques.\n\nIn response, the authors presented new experimental results during the rebuttal phase, showcasing updated LLMs, new generation metrics, an additional TOD benchmark, and ablation studies. As a result, the reviewers reached a consensus that, if these rebuttal points are incorporated into the final paper, it would substantially strengthen the work.\n\nAs for the excitement scores, they were (3, 3, 2). While all reviewers recognized the novelty of the approach, they also noted that some of the sub-techniques were inadequately supported in the initial submission. Consequently, the overall excitement level for the paper is moderate.|meta review"
            }
        },
        "id": "WRkQLAUfO2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xVMV2IYbWH",
        "replyto": "xVMV2IYbWH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1243/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508368,
        "cdate": 1696707508368,
        "tmdate": 1701465424957,
        "mdate": 1701465424957,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a novel strategy for modelling temporal knowledge graphs, based on n-tuples associated with timestamps. Further, the authors introduce two novel datasets for N-tuple modelling. Experimental results on these new datasets show that the proposed strategy is superior to current alternatives. \n\nWhile the initial version lacked clarity around certain areas, including the construction process for the two datasets, the authors have addressed this in their rebuttals. The authors have also provided additional evaluation results with more recent baselines, showing continued competitiveness. \n\nOne concern mentioned by several reviewers is the necessity of moving to an N-tuple format rather than the previously used quadruple format; although the introduced datasets are useful, and the introduced models outperform contemporaries on these, this argument may not be sufficiently well-supported."
            }
        },
        "id": "SQdAxAEfuQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xQbFsx8usC",
        "replyto": "xQbFsx8usC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5472/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613619,
        "cdate": 1696707613619,
        "tmdate": 1701465560471,
        "mdate": 1701465560471,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this paper the authors present a curated bilingual lexicon covering 168 language pairs, and\nmake an experimental comparison of different ways of using this lexicon in data augmentation\nfor low-resource MT.\n\nFor soundness, reviewers zAXT, GrpG and TqkJ all agree on a score of 3. The main criticism being\nthat the experiments are impossible to reproduce because of the use of in-house data, and the lack\nof detail on the experimental settings. To me, this is a valid criticism, the authors could have\nused the Opus data set for example, and aspects such as the quality and domain of the dataset used\nin these experiments is not available to the community. Given this, I think a score of 3 is reasonable.\n\nThere is a disagreement between the reviewers about excitement, ranging from 2 (GrpG) to 4 (TqkJ).\nThe latter reviewer emphasises the wide coverage of the experimental work, the interesting implications\nof the results on unsupervised MT, and the usefulness of the data set (which is released as open-source).\nTo me, these all seem like worthy contributions.\n\nReviewer GrpG raised some concerns about the clarity of the paper, although did not back up their\nconcerns with any valid specific points. I note that reviewer zAXT also had several questions and\nsuggestions, and reviewer TqkJ raised issues about the structure of the paper. These comments\nfrom reviewers suggest that there are clarity issues with the paper, and that the\nthe authors should take into account the                                                    \nreviewers that provided constructive comments.\n\nLastly, reviewer GrpG raised a question about the completeness of the data set. In the discussion, it\nwas revealed that the data set was incomplete at submission time, but has now been completed with\n184 languages. The small version (with 24 languages) was used in the experiments. The authors need to\nclarify this is their update. I would also ask them to clarify the licence of the data set.\n\nOverall, I expect that the data set is a worthwhile contribution, and the experiments shed light\non data augmentation for low-resource languages."
            }
        },
        "id": "aViOOoidg1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xOyBEJq0O8",
        "replyto": "xOyBEJq0O8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1054/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503942,
        "cdate": 1696707503942,
        "tmdate": 1701465419096,
        "mdate": 1701465419096,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper describes a pipeline to improve an extractive QA system using human feedback loop. The proposed approach is not only feasible for an extractive QA system, but has potential to be extended to other NLP applications. Reviewers appreciated the well-strutted content, innovative methods, and comprehensive experiments. \n\nThe questions raised by reviewers including the motivation of using human feedback vs. human labels, the quality of human feedback, and uncertainties of human feedback, are appropriately addressed by the authors. After the rebuttal period, the authors mentioned releasing the annotated dataset and annotation guidelines, which would enhance reproducibility."
            }
        },
        "id": "sVrdjGIJ9O",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xNzu8DivUj",
        "replyto": "xNzu8DivUj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission28/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477257,
        "cdate": 1696707477257,
        "tmdate": 1701465384656,
        "mdate": 1701465384656,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a general framework, UIMER, which leverages model interpretation techniques and gold rationales to enhance model performance. The study explores various interpretation methods (e.g., gradient-based, erasure/replace-based, extractor-based) to identify key input features, aligning them with gold rationales during training. The framework combines task-specific objectives with interpretation alignment objectives. Experimental evaluation spans multiple tasks and datasets, showcasing its superiority over baseline models and existing techniques, particularly in low-resource scenarios. \n\nThe paper tackles the promising area of using model interpretation to boost model performance, showing potential in this domain. It excels in conducting extensive experiments across diverse tasks and datasets, making it well-structured and informative.\nNonetheless, it predominantly focuses on low-resource settings, leaving questions about its scalability to larger training data and adaptability to out-of-distribution scenarios. Addressing these aspects could enhance the paper's relevance and impact."
            }
        },
        "id": "RP0X00kiuD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xMudYCcBum",
        "replyto": "xMudYCcBum",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5369/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611759,
        "cdate": 1696707611759,
        "tmdate": 1701465557631,
        "mdate": 1701465557631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary: The paper focuses on generating the expression tree rather than the expression tokens sequentially for math equation generation. The proposed method combines the method from Seq2DAG and DETR, where Seq2DAG constructs the tree in a bottom-up layer-wise fashion and DETR uses a method to extract multiple objects in each layer in parallel. The system minimizes the number of steps involved in generating the expression structure by building the expression tree iteratively in bottom-up order. The system issues multiple queries to the problem representation at each step, generating valid expressions that are added to the problem embedding for the next step. The process stops when no valid expression is generated. The experimental results on the MWP task demonstrate the effectiveness of their approach, particularly for long equations and equations with complex tree structures.\n\nStrengths: All the reviewers unanimously agree that this is a solid contribution with interesting implications. The proposed method adapts query-based detection combined with layer-wise decoding to math word problems seems novel and appropriate. The comprehensive experimentation conducted in this study greatly contributed to understanding the mechanism of the proposed method.\n\nWeaknesses: I don't think there are any major weaknesses -- some of the weaknesses have been addressed during the discussion phase. The authors should take the comments during the rebuttal into account when preparing for the final version of the paper."
            }
        },
        "id": "pQUoMMFc7b",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xL8SLt02mt",
        "replyto": "xL8SLt02mt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2080/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533931,
        "cdate": 1696707533931,
        "tmdate": 1701465453207,
        "mdate": 1701465453207,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper is focused on evaluating BabyBERTa, a pre-existing language model trained on small-scale child-directed speech, on a series of SRL-based downstream tasks. While the goal of training smaller models on fewer data is potentially useful to the research community, this paper partly suffers from a lack of novelty, as many of its ablations (such as unmasking removal) were already present in Huebner et al.'s original BabyBERTa paper, albeit not evaluated on tasks requiring fine-tuning. Moreover, this work would benefit from running statistical significance tests for all different model configurations to obtain more compelling evidence from the results. Overall, this work is incremental and partly limited in scope and applicability (studying just a single LLM on SRL tasks); on the other hand, it provides some interesting insights into optimal pre-training strategies in low-resource scenarios (such as smaller vocabulary sizes and unmasking removal). Hence, this paper might be considered for Findings."
            }
        },
        "id": "jku9Prejcz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xKRg5dfWyv",
        "replyto": "xKRg5dfWyv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4813/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598952,
        "cdate": 1696707598952,
        "tmdate": 1701465542487,
        "mdate": 1701465542487,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a method for self-rationalization. The proposed method is an instance of I->R, IR->O, (given input, generate rationale. Given input + rationale, generate output), which is a good design that 1) improves the accuracy of O because it is conditioned on R, 2) increases the consistency between R and O. \n\nWhile this setup is not new, e.g.[1], the details of how the I->R model and the IR->O model are implemented led to improved results in the 5 datasets of the ERASER benchmark.\n\nDuring the rebuttal, the authors added new results to quantify the consistency between R and O using their method vs. previous work, and showed greater consistency. This result is an afterthought, so the authors should make sure to add it to the paper.\n\n\n[1]Wiegreffe et al., Measuring Association Between Labels and Free-Text Rationales."
            }
        },
        "id": "LyQNj6XpjJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xJ3O94DnMZ",
        "replyto": "xJ3O94DnMZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5011/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707604849,
        "cdate": 1696707604849,
        "tmdate": 1701465547373,
        "mdate": 1701465547373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces an approach to enhance aspect-based sentiment analysis (ABSA) by incorporating graph structure learning (GSL) techniques. The proposed method utilizes a Graph Neural Network (GNN) within a GSL module to induce latent structures from filtered language representations, generated using Frequency Filters. These latent structures aim to improve the accuracy of ABSA. The paper evaluates the approach on three benchmark ABSA datasets and reports favorable results. \nKey strengths of the paper include its solid theoretical foundation and promising experimental results. However, reviewers raised concerns about the clarity of the paper's explanations (lack of clarity regarding hyper-parameter tuning), the novelty of the techniques used, and issues with presentation (need for writing revisions to improve clarity), which could be improved for the camera-ready version if accepted."
            }
        },
        "id": "dDGk66DwfN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xF6ORNff2k",
        "replyto": "xF6ORNff2k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2555/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544732,
        "cdate": 1696707544732,
        "tmdate": 1701465468863,
        "mdate": 1701465468863,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers almost unanimously agree that the purpose of the paper is quite clear. The work is quite well-motivated. Some analyses were missing in the manuscript, such as comparison with GPT-4, ChatGPT, etc, but were later added to the rebuttal. However, everyone also unanimously agree that, while the produce of the paper is important and useful, the core methodology is nothing new--except open-format prompting--, which hurts the overall scientific contribution.\n\n# Pros\n1. NovaComet significantly outperforms the baseline models (which include many recent proprietary LLMs) on benchmark commonsense QA tasks by significant margin.\n2. The open-format masking strategy is interesting and has lead to rich synthetic data, as verified by some reviewers.\n\n# Cons (and suggestions)\n1. Reviewer eW5y points out that the paper lacks analysis on the open-format prompting, despite it being its main scientific contribution. The authors should report on the coverage of ATOMIC relations, as promised.\n2. The authors should report the combined results with the critic model.\n3. The core methodology of generating synthetic data from LLM and finetuning another model is not new at this point."
            }
        },
        "id": "SDFVCWTeTi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xDfyOL1unK",
        "replyto": "xDfyOL1unK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2675/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547291,
        "cdate": 1696707547291,
        "tmdate": 1701465472982,
        "mdate": 1701465472982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Main contributions:\n* This paper introduces a multi-question generation model aimed at producing a wide array of contextually grounded, answerable questions with enhanced diversity. Notably, the LMQS (Maximum Question Similarity Loss) proposed by the authors underscores the generation of a variety of questions. The authors conduct a comprehensive research study, incorporating both human and automated evaluations, to demonstrate the model's effectiveness. In the context of automated diversity assessment, the authors also employ the Self-BLEU metric.\n\nReasons to accept:\n* The approach demonstrates innovation, particularly through the introduction of LMQS to amplify the diversity in question generation. The authors have conducted extensive research, encompassing both human and automated evaluations, to validate its effectiveness. The ablation studies and further analysis of the evaluation model are helpful in order to better understand the proposed model's limitations and applications.\n\nReasons to reject:\n* The use of BART was criticized as outdated and T5 was suggested as a more modern choice. In the rebuttal, the authors noted that T5 did not perform as well and will add details to the paper.\n* Combination of cross entropy and the new LMQS. The authors addressed this concern by explaining that the mix with cross entropy was essential for better performance and will explain this in the final version.\n* The use of ROUGE-L was criticized. In the rebuttal, the authors have addressed this concern with the use of BertScore.\n* Krippendorff's alpha was criticized for being below two-thirds, but this has been shown to be typical for NLP tasks."
            }
        },
        "id": "4tRbUie9Lv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "xCXlOmGimw",
        "replyto": "xCXlOmGimw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2978/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553284,
        "cdate": 1696707553284,
        "tmdate": 1701465483043,
        "mdate": 1701465483043,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work focuses on the problem of weakly labeled data for text classification and proposes a simple yet effective method. Reviewers unanimously applauded the motivation, presentation, and novelty of the work. Only minor issues were regarding breadth of experiments and missing references, none of which detract from the quality of the work."
            }
        },
        "id": "I5YSJW0Lq6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "x9BmfezTvD",
        "replyto": "x9BmfezTvD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4574/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593330,
        "cdate": 1696707593330,
        "tmdate": 1701465535313,
        "mdate": 1701465535313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a narrow and focused contribution to affective reasoning in conversations through the lens of causal discrimination, predicting the presence of causal relationship and direction in pairs of utterances. The paper then presents structural causal model with the  i.i.d. noise inclusion into conversation. To incorporate such i.i.d. noise, the paper presents an Autoencoder framework.  The study of causal relationship (in particular for LLMs or not-so-large LMs) is very important but much less explored compared to the rest of the field. As such, this work provides contributions, although very narrow in its task scope, which are interesting and insightful. The presentation of the paper could be polished further (following the comments of 5DgJ, 2DCP).  The abstract and presentation of results could be decompressed further."
            }
        },
        "id": "jfDOEjHkKE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "x7zquRQfoB",
        "replyto": "x7zquRQfoB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission116/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479675,
        "cdate": 1696707479675,
        "tmdate": 1701465387907,
        "mdate": 1701465387907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "I appreciate authors' full engagement and attempt to answer concerns and questions about the manuscript. The manuscript motivates the problem and provide insight into tradeoff between sparsity and performance and achieve performance parity with significant parameter reduction. While relation between smoothing, compression and bias reduction have been studied in traditional ML techniques, Authors have identified an opportunity to debias and compress at the same time for VLM, which in my view is novel and exciting and get facilitate further research in this area."
            }
        },
        "id": "0AhnTYITGw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "x6aiktiAl8",
        "replyto": "x6aiktiAl8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission940/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499889,
        "cdate": 1696707499889,
        "tmdate": 1701465415564,
        "mdate": 1701465415564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a novel method, named cross-entropy difference (CED), for in-context demonstration selection in Large Language Models (LLMs). The method utilizes parameter efficient fine-tuning and focuses on improving performance on zero-shot tasks by selecting effective in-context demonstrations. The evaluation is done on a mixed-domain dataset that combines various benchmarks and tasks. The results show the proposed method outperforms baseline selection methods for a range of LLMs.\n\n**Summary of Reviews and Author Rebuttal:**\n\nReviewer YS4p found merit in the paper's novel demonstration selection strategy. The paper's strengths include the novelty of the proposed strategy and the experimental validation that suggests performance improvements across various benchmarks. The criticisms were primarily around the computational and storage costs of the method. Additionally, the reviewer expressed confusion regarding the calculation of CED and had some concerns about the paper's reproducibility. In their rebuttal, the authors addressed the computational and storage concerns, citing the PEFT approach's efficiency and pointing out that costs scale favorably as tasks increase. The authors also clarified the method they used to calculate cross entropy. Furthermore, they responded to the reproducibility concerns by committing to release both their code and data splits, ensuring that their experiments would be replicable.\n\nReviewer uof7 acknowledges the relevance of the study given the rising popularity of in-context learning. The method showcased an improvement over the baseline strategies for choosing demonstrations and had theoretical justifications. However, the reviewer points out mixed results, especially in Table 1, and several experimental design choices that were not sufficiently justified. They also raised questions about the novelty of the method, as it seems to be an adaptation of an existing method. The author's rebuttal addressed most of the concerns, highlighting that their work introduces cross-entropy difference in the context of LLMs and in-context demonstration selection for the first time. \n\nReviewer EXPc provided a favorable assessment of the paper, praising its theoretical grounding, experimental coverage, and the robust evaluation. However, the reviewer also raised concerns about potential scalability issues and sought clarifications on the cost of training models, the effect of scaling training examples, and the method's robustness. In their rebuttal, the authors addressed each concern, explaining their methodology further and providing insights into how the training works with different dataset sizes. The author's responses appeared to satisfy the concerns, and the reviewer acknowledged the rebuttal positively.\n\n**Soundness and Excitement:**\n\nThere seems to be a consensus that the method presented is solid, both in its theoretical grounding and empirical evaluation. The reviewers assigned a high score for soundness, noting the robustness and the extensive coverage of experimental benchmarks.\n\nThe paper contributes to the research domain of in-context learning in LLMs and offers a new perspective on demonstration selection. While the idea and the results are noteworthy, the trade-off between performance improvement and the computational/storage costs leads to average excitement around the paper."
            }
        },
        "id": "ob361yWnlm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "x3e1zQ1ub1",
        "replyto": "x3e1zQ1ub1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2054/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533208,
        "cdate": 1696707533208,
        "tmdate": 1701465451992,
        "mdate": 1701465451992,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers of this paper found the topic to be relevant and clear, and especially found that the problems pointed out in past approaches were valid. Generally, the reviewers didn’t seem to point to many specific methodological issues that should result in lower soundness scores but found the lack of discussion in many areas to leave the paper feeling a bit shallow, noting that it could benefit from another round of revision to expand these areas. It was also noted that the scope was narrow in terms of languages and features studied, but given this is a short paper submission, I don’t think it is a major problem given that this did not seem to invalidate any of the findings in the eyes of the reviewers. Reviewers would have liked to have seen more proposed solutions to the issues raised, and in general, were not overly excited about the paper."
            }
        },
        "id": "Ejn0RyzoH9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "x32rlkzM69",
        "replyto": "x32rlkzM69",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3228/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558586,
        "cdate": 1696707558586,
        "tmdate": 1701465490960,
        "mdate": 1701465490960,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies calibration of question answering systems when presented ambiguous questions. It first decomposes uncertainty in this setting into uncertainty about interpreting the question vs. uncertainty in the right answer (due to lack of knowledge). The paper also proposes different ways to measure model confidence; the best method samples 10 outputs and measures their similarity. The reviewers were generally supportive of this paper, finding that it proposed an important setting and a method that performs well. The paper also includes valuable analysis of how things like model scale, prompt format, and fine-tuning affect results.\n\nOne reviewer was concerned about the evaluation setting used, as the paper currently considers a model to be correct if it outputs any correct answer to any valid interpretation of an ambiguous question, even if that interpretation differs from the model's predicted interpretation. During the rebuttal, the authors presented additional results where the model is only considered correct if its answer matches its chosen interpretation. These results show the same patterns as before. I agree with the reviewer who raised this concern that this issue merits further discussion in the camera-ready version, and that both sets of results should be presented (either in the main text or appendix, depending on space considerations)."
            }
        },
        "id": "Sn8y2ktYdy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "x2W2dKdNI8",
        "replyto": "x2W2dKdNI8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4365/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588312,
        "cdate": 1696707588312,
        "tmdate": 1701465529314,
        "mdate": 1701465529314,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposed a dataset on multi-turn empathy conversations, by converting single-turn dialogues to multi-turn dialogues, and performed experiments with different baselines on this dataset.\n\nAll the reviews agree that this paper is sound. The dataset and task will be a good resource for the community. The method to construct multi-turn dialogues from single-turn dialogues is easy to use and valuable. \n\nBut there are questions that need to be addressed, for instance, more detailed dataset analyses and description about the evaluation for reproducibility."
            }
        },
        "id": "XXNQ1B5zzG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wwm55qcNdK",
        "replyto": "wwm55qcNdK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission585/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491322,
        "cdate": 1696707491322,
        "tmdate": 1701465404230,
        "mdate": 1701465404230,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers gave positive reviews for this paper, but I am concerned about computational social science papers that lack rigor in the \"social science\" problem domain. \n\nIn the research field of personality, MBTI is not considered by psychology researchers, to be scientifically sound. This means that the research done in this paper is looking at correlations with a metric that is shown to be inconsistent. This paper should at least mention that and discuss why it is still okay to base this study on MBTI for assessing human personalities through ChatGPT."
            }
        },
        "id": "SbavzWqKh7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wtqb7pNL4e",
        "replyto": "wtqb7pNL4e",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission562/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490864,
        "cdate": 1696707490864,
        "tmdate": 1701465403536,
        "mdate": 1701465403536,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new method for zero-shot multimodal QA, MoqaGPT, which extracts answers in each modality and then later fuses answers using LLMs. The proposed method shows strong performance. While the novelty might be limited, overall the proposed method is well-motivated, sound, and strong. I recommend this paper for acceptance. \nOn the other hand, in the current draft, the main ablations of different models are mostly limited to GPT-4 and ChatGPT, which may introduce issues of data contamination or inability to reproduce the results. I strongly recommend authors include the ablation results with more open-sourced models (for reasoners) as shared in https://openreview.net/forum?id=wrBIS6FOfV&noteId=bPutO2Ja6P in the final version as well. While I understand the scaling of models strongly affects the final performance, due to the nature of such closed and proprietary LLM APIs, it is important to include the results of those open-sourced, reproducible LLM results in the paper."
            }
        },
        "id": "Y8mJHIkUnF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wrBIS6FOfV",
        "replyto": "wrBIS6FOfV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3590/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565747,
        "cdate": 1696707565747,
        "tmdate": 1701465502742,
        "mdate": 1701465502742,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers have mixed feelings about this paper.  The third review feels that the simple method is a novel contribution and the first review has a low excitement about this, both with high reviewer confidence.  Ultimately, LLM4CS is a new, if not unexpected, use of large language models and the authors argue that their empirical results are a significant contribution in the rebuttal."
            }
        },
        "id": "k6y4GkUyCZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wpsbUYi9nN",
        "replyto": "wpsbUYi9nN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission105/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479350,
        "cdate": 1696707479350,
        "tmdate": 1701465387454,
        "mdate": 1701465387454,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel approach for temporal knowledge graph forecasting, based on in-context learning with a large language model. The paper makes several contributions towards this problem, including proposing an evaluation methodology, introducing a new dataset, and using those to compare the performance of LLMs to other models.\n\nThe authors further provide extensive experiments with their proposed in context learning approach, including on older datasets, showing that their approach is competitive with more complex, supervised systems – even when entity names are replaced with random numbers. This will provide a fruitful starting point for further research."
            }
        },
        "id": "qmhfDwkeNK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wpjRa3d9OJ",
        "replyto": "wpjRa3d9OJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4049/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581956,
        "cdate": 1696707581956,
        "tmdate": 1701465518270,
        "mdate": 1701465518270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work aims to facilitate a better generation of commonsense explanations using compressed KGs. In the proposed framework, self-attention scores to select highly relevant entities in the subgraph of the KG. The experimental results on two datasets show that the proposed method can outperform conventional baselines in diversity while keeping the comparable or better relevance to the references. Reviewer wZ5V and 1hNR gave positive scores, whereas Reviewer BbGf pointed out the unnecessity of selecting concepts from the KG for LM due to its knowledge obtained in pretraining. Considering that the authors addressed the question by referring to Table 3 and Figure 4, we can judge that there is no major problem in accepting this paper."
            }
        },
        "id": "mYEe9S3ahu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wnE8wDd61Z",
        "replyto": "wnE8wDd61Z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1146/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505961,
        "cdate": 1696707505961,
        "tmdate": 1701465421673,
        "mdate": 1701465421673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper conceptualizes generalized quantifier semantics as percentage ranges, and introduces 1) an annotated dataset of generalized quantifiers in Wikipedia sentences, and 2) a model framework combining NLI and RSA to predict percentage ranges. The paper shows that the proposed pragmatic model outperforms a “literal” RoBERTa-large model fine-tuned on NLI.\n\nThe reviewers express that the paper is clear and well-motivated, addressing a worthwhile and understudied topic, and contributing a valuable corpus and an interesting pragmatically-driven model. Some concerns are raised about clarity and soundness, but these have largely been addressed in the discussion period. Overall, this paper will likely be a solid, interesting, and novel contribution to the conference."
            }
        },
        "id": "Y4r0BmOh0T",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wirDXDQwYZ",
        "replyto": "wirDXDQwYZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4413/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589153,
        "cdate": 1696707589153,
        "tmdate": 1701465530854,
        "mdate": 1701465530854,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a floating-point-based post-training quantization regime for the transformer models. Reviewers initially had some problems with the clarity of presentation and the novelty of the methods, the authors have addressed the concerns. Reviewers unanimously agreed to accept this paper, thus, the AC deemed this paper should be accepted to the main conference."
            }
        },
        "id": "HPYo9qPDqC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wiI8ycNfgJ",
        "replyto": "wiI8ycNfgJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission908/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499117,
        "cdate": 1696707499117,
        "tmdate": 1701465414487,
        "mdate": 1701465414487,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this paper, the authors propose a method to generate summaries using abstracts of citations of an article and the source article by the proposed aggregation strategy based on the graph-styled citation network. They also create a large-scale dataset for abstractive summarization in the biomedical domain. The experimental results on their created dataset show that their proposed method outperformed baselines. The authors answered reviewers' questions regarding input to the model and cited articles in detail. After the rebuttal, the reviewers agreed with their score towards accepting the paper; thus, there is no reason to unfollow their decision."
            }
        },
        "id": "hDTRkQYLYz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wcqBfk4jv6",
        "replyto": "wcqBfk4jv6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1091/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504820,
        "cdate": 1696707504820,
        "tmdate": 1701465420345,
        "mdate": 1701465420345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper discusses an approach to get candidate explanations of data for use in the chain of thought prompting, starting with crowd-sourced seed explanations set. The proposed approach efficiently searches over the set of explanations using proxy/surrogate metrics to find a suitable combination to be used as few-shot examples.\n\nReviewers appreciated the clarity of the paper and experiments on multiple datasets. \nAll the reviewers found the paper to be sound and interesting. \nAll appreciated that the proposed approach to get explanations from unlabelled data as well as efficiently selecting a useful combination of explanations can be useful in multiple tasks and scenarios.\n\nReviewers also highlighted that the proposed techniques do not guarantee that the optimal combination would be selected. Authors should consider clearly specifying this limitation in any future version of their paper."
            }
        },
        "id": "FqevFjItwx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wcgfB88Slx",
        "replyto": "wcgfB88Slx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission808/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496451,
        "cdate": 1696707496451,
        "tmdate": 1701465411159,
        "mdate": 1701465411159,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper focuses on summarization evaluation and investigates whether reference-based metrics can be repurposed as reference-free metrics by substituting the reference with the input document. Overall, reviewers agree that this paper has merits (simple/sound approach, promising results) but also point out that 1/ it has a number of issues regarding clarity and that 2/ the experimental results are quite preliminary.  For 1/ I feel that the authors's responses provide some feedback to be included in a revised version of the paper. For 2/ I believe that this is not such an issue for a short paper as this format is adequate for describing a work in progress."
            }
        },
        "id": "kN4155Wsxx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wZmgpJMdb3",
        "replyto": "wZmgpJMdb3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3166/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557428,
        "cdate": 1696707557428,
        "tmdate": 1701465489109,
        "mdate": 1701465489109,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This is a comprehensive study, dealing with the toxicity risk in a language model, which is an important issue. Although the results are detailed and thoroughly analysed, there are several matters that raise concerns. \n\nFirstly, it is unclear if the described situations are applicable to regular user interactions and the relevance to real-life scenarios is questionable. The paper also heavily relies on Google's Perspective API for toxicity evaluation, which is problematic as it could be misleading when comparing toxicity scores. Additionally, the manuscript covers only the GPT-3.5 version of ChatGPT, while newer iterations of the model already exist and claim to have significantly improved performance and reduced toxicity. While a comparison would be expensive, it unfortunately would be necessary to make this paper completely sound. Otherwise, it risks being a temproary snapshot. The authors rebuttal that \"while the analysis was performed on ChatGPT, the arguments we make should naturally hold for all LLMs\" is not self-evident.\nOne reviewer noticed that the report doesn't provide enough details on mitigating the highlighted risks."
            }
        },
        "id": "FQjQSK5Hkd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wZKRStVJJe",
        "replyto": "wZKRStVJJe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4313/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587238,
        "cdate": 1696707587238,
        "tmdate": 1701465527637,
        "mdate": 1701465527637,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a multilingual dataset and benchmark for the detection of hallucinations and omissions in machine translation.\nThe dataset is quite general; it contains 18 low and high resourced language pair directions. The dataset is quite useful, as it provides a way for MT systems to be trained to detect such pathological translation mistakes. The authors evaluate multiple hallucination and omission detection methods.  The paper shows how existing system which were developed for detecting such pathologies, perform on the new dataset, with interesting results."
            }
        },
        "id": "i5ZpsPWgUO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wYdA8CF94e",
        "replyto": "wYdA8CF94e",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission156/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480543,
        "cdate": 1696707480543,
        "tmdate": 1701465389038,
        "mdate": 1701465389038,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The overall excitement of the reviewers was ambivalent, but the active author response discussion period showed a lot more positive sentiment from the reviewers on the quality of the work especially with the addition of the extra experimental results provided by the authors and the extra context and thoughtful reasoning provided to answer the questions raised by the reviewers.\n\nWhile the overall assessment is higher than the numerical scores assigned by the reviewers, this is mainly due to the extra information that was provided during the author response, so it is very important that all of the extra experimental results and clarifications are included as part of the paper by the authors. The additional context provided by the information in the author response is a big reason for the final evaluation of this submission in this meta-review."
            }
        },
        "id": "tZKg88BZVq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wWT51dSyBj",
        "replyto": "wWT51dSyBj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3445/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562838,
        "cdate": 1696707562838,
        "tmdate": 1701465497857,
        "mdate": 1701465497857,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores the used of several LLMs (like StableVicuna and GPT-4) like  to generate synthetic data to train a smaller multilingual language models like mBERT and XLM-R. They performed evaluation on several languages and on three tasks: XCOPA, XWinograd and XStoryCloze. \n\nAll the reviewers found this to be interesting and a strong evaluation especially for low-resource settings and the efficiency of smaller models. It's also great the authors compared with machine translated texts which had been done in past works. There a few comments by the reviewers about the diversity of the generated sentences, the use of bigger models, use of more recent LLMs for generation, and some unclear part of the write-up, which the authors have adequately addressed. I believe this paper is a good contribution of LLMs for non-English languages especially in terms of building smaller models with similar or better accuracy than LLMs."
            }
        },
        "id": "osCLf3KWII",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wWFWwyXElN",
        "replyto": "wWFWwyXElN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1156/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506184,
        "cdate": 1696707506184,
        "tmdate": 1701465422102,
        "mdate": 1701465422102,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers view the paper as a pioneering approach to Implicit Discourse Relation Recognition (IDRR) using a method of logical semantics enhancement.  While the concept of using explicit connective prediction has been explored several times before, this might be the first case of introducing the task to pre-training via the loss function. The results indicate that it achieves state-of-the-art performance on two benchmark datasets.\n\nPros:\nIntroduces a novel method for IDRR, leveraging logical semantics enhancement.\nAchieves state-of-the-art performance on benchmark datasets.\nComprehensive experimental design, including diverse analyses.\nPotential applicability to a wide range of real-world scenarios.\nThe paper is well-structured, making it accessible for readers (although one reviewer disagrees).\nCons:\nLacks comprehensive comparison with a broader range of state-of-the-art methods.\nIssues with reproducibility due to lack of sufficient details.\nDoes not address potential ethical implications adequately.\nLanguage and presentation concerns as pointed out by one reviewer.\nSome aspects of the method might seem more incremental than novel. (which I agree with)"
            }
        },
        "id": "SWpRHxDVlv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wV44qtTJ61",
        "replyto": "wV44qtTJ61",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5725/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617380,
        "cdate": 1696707617380,
        "tmdate": 1701465565713,
        "mdate": 1701465565713,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers agreed that the proposed inference-time method of combining a unimodal LM with a VLM has promising/impressive performance improvements, and generally agreed that the experiments are thorough, using a wide range of datasets. Two reviewers described the method as \"innovative\". Reviewer TDrJ was less excited about the paper, given that previous conditioned generation methods also modify token likelihoods. However, I agree with the author response that this doesn't limit the novelty of this paper as that framework is so general, and this paper is an effective and novel application to grounding within this general framework. The same reviewer also had some concerns about fluency and robustness -- but in my opinion, the wide range of datasets and the coherence evaluations alleviate this concern."
            }
        },
        "id": "pGODkqnch3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wRwbv3aWzN",
        "replyto": "wRwbv3aWzN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1006/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502240,
        "cdate": 1696707502240,
        "tmdate": 1701465417562,
        "mdate": 1701465417562,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces ODEX, a pioneering Open-Domain EXecution-based dataset designed for natural language to Python code generation. ODEX features 945 NL-Code pairs that span 79 diverse libraries and are supported in four languages: English, Spanish, Japanese, and Russian. These pairs are sourced from StackOverflow, ensuring relevance to real-world coding queries. Additionally, the dataset includes 1,707 human-written test cases to facilitate execution-based evaluation. A notable observation from the study is the differing behaviors of leading code language models. While CODEX outperforms in general, CODEGEN shows improvement with scaling. The paper highlights the performance gaps of these models in open and closed domains. Reviewers commend ODEX for its \"open-domain\" nature, execution-based evaluation support, and multilingual capability. They recognize its potential to address challenges in open-domain code execution and appreciate its broader coverage of libraries compared to existing solutions. The dataset is expected to significantly benefit the code generation research community.\n\nThe paper presents a significant contribution to the community by introducing a dataset with human-written test cases, emphasizing execution-based evaluation, which aligns more closely with human preferences than n-gram based evaluations. The meticulous process of dataset creation, including a thorough quality check during annotation, is commendable. The dataset's strength lies in its \"open-domain\" nature, derived from genuine and practical queries, necessitating diverse library imports and usage. This diversity is an important feature, setting ODEX apart from other datasets. The paper provides a detailed account of the dataset collection and annotation procedure, offering innovative solutions to challenges like irreproducible runs, randomized outputs, and specialized equivalence checks. Benchmark experiments offer valuable insights, especially the intriguing result comparing the performance gap between Codex and Codegen in open versus closed domains as model size grows. Overall, the dataset's potential for superior data quality compared to existing datasets makes it a worthy addition to the field.\n\nThe authors must consider the following weaknesses: a) The process of showing both the query and canonical code to annotators when writing test cases could introduce bias, potentially limiting the test cases to the canonical code solution. b) The paper's experiments do not conclusively demonstrate the sufficiency of the test cases. c) The dataset's small size, especially when divided by language, raises concerns about the robustness of the results."
            }
        },
        "id": "qtt3e1YeDN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wKqdk1sOMY",
        "replyto": "wKqdk1sOMY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1917/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530043,
        "cdate": 1696707530043,
        "tmdate": 1701465447048,
        "mdate": 1701465447048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work compares conceptual structure in humans and language models. The research question is how consistent is the conceptual strcuture across different tasks for humans vs for models. The findings are that conceptual structure is more consistent across people than it is across models.\n\nOverall, the reviewers unanimously agree that the work is sound and the contributions are valuable and would be exciting to the EMNLP audience. Specifically, the reviewers praise the comprehensive human experimental conditions spanning different datasets, languages, tasks, the clarity of the arguments, and the timely nature of the comparison between human behavior and large language models. Reviewers also point out a small number of opportunities for improvement, including motivating experimental choices made in the human experiments and an addition of an agreement analysis among the human annotators, which the authors responded to and reported to be high. This information should be included in a revision."
            }
        },
        "id": "AmKcLq2DPz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wFaBjgGqaL",
        "replyto": "wFaBjgGqaL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3402/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561899,
        "cdate": 1696707561899,
        "tmdate": 1701465496207,
        "mdate": 1701465496207,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates methods for improving the performance of token-level retrieval augmented models for code generation.\n\nThe proposed framework seems to work well and achieves good results, compared to the chosen baselines.\nAll reviewers comment that the paper is very well written and easy to read.\n\nReviewers highlighted concerns about lack of comparison to state-of-the-art models and LLMs. Authors have provided some additional results in the comments which seem to demonstrate the proposed method outperforms several LLMs.\nReviewers also voiced concerns about the novelty and motivation of the method.\nThe evaluation could be improved, with better ablation tests to clearly show whether the proposed components are responsible for performance improvements."
            }
        },
        "id": "A2gbj3mhpK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wFILOtxmxU",
        "replyto": "wFILOtxmxU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2332/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539643,
        "cdate": 1696707539643,
        "tmdate": 1701465461569,
        "mdate": 1701465461569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents an instruction-tuning method for applying LLMs to the DST task. To build the instruction-tuning datasets, it proposes the Assembled Domain-Slot Instruction Generation method. This method generates diverse instruction samples by randomly combining different instruction and input templates, which helps to reduce the sensitivity to prompts during the fine-tuning process. This paper performs evaluations in the zero-shot, few-shot, and full data settings and the results are very strong. \n\nInitially, the paper fell short in not comparing against other DST works using ChatGPT. This issue has been addressed in the rebuttal, together with further results on different backbone models. The authors are encouraged to add these new results and all implementation details in their final version."
            }
        },
        "id": "TWS6N3u4SD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "wDfXP6uAkR",
        "replyto": "wDfXP6uAkR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission890/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498524,
        "cdate": 1696707498524,
        "tmdate": 1701465413813,
        "mdate": 1701465413813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All the reviewers rated the work as 4, across all categories. \n\nReviewers largely agree that:\n-The paper is well written\n-Experiments were interesting and novel\n-The work achieves state-of-the-art performance \n\nSome reviewers also noted that the gap between the prior state-of-the-art and the SotA that the authors achieve \"seem[s] marginal\"."
            }
        },
        "id": "7Dd5kHB1as",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "w8LoOWsbU7",
        "replyto": "w8LoOWsbU7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission100/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479235,
        "cdate": 1696707479235,
        "tmdate": 1701465387361,
        "mdate": 1701465387361,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper under review presents a methodology to include entity aspect information for zero-shot entity linking. The proposed approach breaks down long entity descriptions into multiple key views. \n\nIn terms of soundness, overall, the paper is regarded as having a good level of support for its major claims. All three reviewers recognize that the paper contributes novel work to the topic of entity linking. The study has been praised as being well-structured, and having a relevant and important challenge at its core.\n\nDespite the acknowledged strengths of the paper, the reviewers collectively pointed out certain weaknesses. Notably, one reviewer noted that the manuscript could benefit from reviewing relevant previous work in the area of selecting relevant sentences and clustering them semantically. Lastly, a concern was raised about how this approach would extend to autoregressive entity linking."
            }
        },
        "id": "3ya3VpDTh4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "w4YwLzuD29",
        "replyto": "w4YwLzuD29",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission446/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488017,
        "cdate": 1696707488017,
        "tmdate": 1701465399283,
        "mdate": 1701465399283,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers viewed this topic as important, and appreciated the strong results in this paper on the DTC benchmark in the Alexa Arena. While reviewers identified that the model and pretraining procedures largely follow past work, none viewed this as a crucial weakness and I agree with reviewer eEZM, and the authors' response to reviewer GKE9 -- that predicting actions and object references as text is an interesting and likely scalable approach. Some clarification concerns identified by one reviewer were also resolved sufficiently in the author response."
            }
        },
        "id": "KlSfMFj0HK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "w4FwmICSHZ",
        "replyto": "w4FwmICSHZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3735/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707570495,
        "cdate": 1696707570495,
        "tmdate": 1701465508063,
        "mdate": 1701465508063,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new NLI benchmark and experiments using LLMs. The authors study several ambiguities and demonstrate that crowdworkers disagree less after they consider their disambiguations.\n\nAll the reviewers are positive.\n\nThe dataset is small (~1,500), and that is fine. Yet the authors only calculated agreement with 50 samples (Section 2.4). I am  surprised they did not do it with all the samples. It is also unclear if the agreement were calculated before or after discarding. The resource is the main contribution of the paper (and the idea / problem). I don't think the prompting experiments add a lot of value other than to say \"off-the-shelf LLMs cannot solve this problem\" (which is expected)."
            }
        },
        "id": "ycvhtThS7N",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "w3hL7wFgb3",
        "replyto": "w3hL7wFgb3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4801/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598737,
        "cdate": 1696707598737,
        "tmdate": 1701465542196,
        "mdate": 1701465542196,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Following discussions with authors and among themselves, the reviewers overall found the paper sound.\n\nThe most prominent critiques with a consensus reached have to do with the lack of evaluation of the generated explanations, limited analyses of the main results, limited robustness tests, further necessary clarifications around the experimental design, and a narrow outline of contributions to the field."
            }
        },
        "id": "CoTj7pe6o3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vvnUi75U9i",
        "replyto": "vvnUi75U9i",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3466/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563299,
        "cdate": 1696707563299,
        "tmdate": 1701465498451,
        "mdate": 1701465498451,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper studies the impact of noise in input for summarization models, and proposes methods to automatically detect and remove such noise. This is quite a relevant and exciting area for research. The main limitation pointed out by multiple reviewers is the synthetic nature of the noise injected into documents, which raises concerns that the proposed method to detect noise may not generalize to real noise distributions. Already, the noise detection experiments are conducted on a pretty high noise level which the authors agree is not the same as noise levels. I would encourage the authors to collect a small test set of examples with 'real noise' to show generalizability of their approach."
            }
        },
        "id": "dzLJo4ZpuP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vuabr8zbCq",
        "replyto": "vuabr8zbCq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission608/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491963,
        "cdate": 1696707491963,
        "tmdate": 1701465405079,
        "mdate": 1701465405079,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an original, efficient, and state-of-the-art (SOTA) paradigm for computing structured representations of natural language inputs. The work uses the concept of a partial order of a string to represent the relations in a bipartite graph. In particular, the authors propose that by generating several total orders of the string and combining them, they can recover the desired relations in the output graph. The study initially introduces a robust and engaging theoretical demonstration of the foundational aspects of the approach. Subsequently, it implements a functional neural model that achieves highly impressive results in the domains of dependency parsing and coreference resolution. \n\nThere is unanimous consensus among the reviewers regarding the acceptance of this paper. They clearly acknowledge its novelty and significance within the field. It's an elegant approach with a strong theoretical foundation and wide applicability across various NLP tasks and structured prediction problems, making it a standout contribution.\n\nThe reviewers suggest minor changes, including providing a more comprehensive overview of the related previous work, along with other minor technical suggestions, which can be easily addressed in the camera-ready version of the paper."
            }
        },
        "id": "jeGpJTBhSe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vtqfPW6OSm",
        "replyto": "vtqfPW6OSm",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission305/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484312,
        "cdate": 1696707484312,
        "tmdate": 1701465394330,
        "mdate": 1701465394330,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a reinforcement learning-based technique to direct language models (LLMs) in generating text that can deceive AI-generated text detectors. The aim is to evaluate the reliability and trustworthiness of these detection systems. The proposed approach is transferable to other LLMs with similar performance levels.\n\nStrengths:\n* The paper addresses an important and timely issue, assessing the reliability of AI-generated text detectors.\n* The proposed method effectively reveals the unreliability of current AI-generated text detectors.\n* The transferability of the evasive soft prompts makes the proposed method applicable to current and future LLMs.\n* The paper is well-written and has extensive experiments across different AI text detectors, baselines, and datasets.\n\nWeaknesses:\n* The paper does not fully address the case where human-written text is incorrectly classified as AI-generated, affecting the overall reliability of detectors.\n* The evaluation of evasive soft prompts is missing, making it difficult to determine the effectiveness of the current prompts.\n* The paper's applications are limited, as it requires access to input embeddings of used LLMs and is sensitive to the used detector and LLMs.\n* No human evaluation of the text generated by the evasive soft prompts is conducted, so the actual resemblance to human-generated text remains unclear.\n\nOverall, this paper has merits and makes significant contributions to the field. However, it has some key weaknesses that may benefit from further revisions. The topic is interesting, and the motivation is strong, but addressing the concerns raised by the reviewers could further strengthen the paper."
            }
        },
        "id": "Cwmy56a4IV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vtC3sLXjDY",
        "replyto": "vtC3sLXjDY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4537/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592454,
        "cdate": 1696707592454,
        "tmdate": 1701465534087,
        "mdate": 1701465534087,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper explores methods for controlling sentence-level styles in abstractive summarization. The authors provide a clear description of their model, perform human evaluations to verify the summary quality, and support their claims with comprehensive experiments. However, reviewers have also raised concerns, including the lack of baseline comparisons with other methods, the insufficient sample size of 30 documents from the CNN/DM test set, the absence of details regarding human evaluation, the questionable reliability of ROUGE scores, among others."
            }
        },
        "id": "Mfoxhfkhs1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vscmppXqXE",
        "replyto": "vscmppXqXE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission424/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487304,
        "cdate": 1696707487304,
        "tmdate": 1701465398479,
        "mdate": 1701465398479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies how the weight space and underlying loss landscape of different models are interconnected. The authors have provided insightful findings as summarized well by one of the reviewers that \"They found that models fine-tuned on the same dataset cluster together in the weight space and models fine-tuned on the same task also form clusters though looser ones. Models fine-tuned on any language tasks seem to reside in a constrained region compared to random perturbations of pre-trained weights.\" Based on these analysis, the authors propose to create a better model by picking center of a region in the weight space, resulting in superior results. As summarized by the reviewers and myself, the pros and cons are:\n\n### Pros:\n1. This paper provides very novel and insightful empirical analysis as well as discussion on the weight space and loss landscape of different fine-tuned models. This is relatively new yet important direction for knowledge transfer.  \n2. \"The authors provide solid empirical evidence to support their claims. Through experiments and analyses, they demonstrate the existence of clustered regions in the weight space, the improved performance of models within these clusters, and the potential for generating high-performing models through traversal within these regions.\"\n\n### Cons:\n1. \"how to use these findings to improve the performance of the models and explain these findings.\"  \n2. Baseline results in Table 2 are poor.\n\nMost of the reviewers agree that this paper is sound and exciting. Even though that this paper does not demonstrate the practical application of the findings in a comprehensive way, I think this analysis itself is interesting and insightful enough to stand as a nice paper alone."
            }
        },
        "id": "Xrg8ncvsgq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vq4BnrPyPb",
        "replyto": "vq4BnrPyPb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission514/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489623,
        "cdate": 1696707489623,
        "tmdate": 1701465401771,
        "mdate": 1701465401771,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper examines the impact of number of annotators per instance on model performance for text classification. R2 and R3 see the topic of the paper as important and that the results have practical implications. Additionally, R3 sees the experimental setup as thorough for the particular tasks, datasets, and models used. However R1 indicates that one of their primary results may have already been demonstrated before, limiting the degree to which the paper makes a contribution to the literature on this topic. The authors address this in their rebuttal – there are some similarities in the finding but the scale of experimentation and the nature of the datasets used in this work differ sufficiently. The scope may potentially be limited, as the paper only looks at NLI, and R3 additionally describes some potential issues with the utility and generality of the findings. The discussion helped to clarify some of the concerns of the reviewers; while the experiments may be specific to the one dataset tested (ChaosNLI) the paper does give insight into the impact of annotator volume for ambiguous labeling tasks."
            }
        },
        "id": "FeHOBgnsMg",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vpkEJM9qYR",
        "replyto": "vpkEJM9qYR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4848/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599437,
        "cdate": 1696707599437,
        "tmdate": 1701465543314,
        "mdate": 1701465543314,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a new decoding method by extending the popular contrastive search with new regularization terms to improve the faithfulness-diversity tradeoff. The reviewers largely find the method to be new, simple, straightforward and well motivated. The reviewers also find the writing to be very clear. The reviewers have shared a bunch of concerns. Some of the concerns such as choice of probability measure, choice of hidden states over logits, decoding speed comparison were adequately answered by the authors. However, there are a number of concerns such as lack of human evaluation, sensitivity of the results to prompts, missing important evaluation metrics and hyperparameter search, which were unaddressed and could impact the assessment of this work. These should be addressed in the revision to maximize the impact of this work."
            }
        },
        "id": "4NHbowWENx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vooJHgn1Gm",
        "replyto": "vooJHgn1Gm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2558/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544774,
        "cdate": 1696707544774,
        "tmdate": 1701465468992,
        "mdate": 1701465468992,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explore how Large Language Models (LLMs) can be misused to generate misleading or false information that can affect Open-Domain Question Answering (ODQA) systems. Simulate different scenarios of misinformation generation and measure the impact on ODQA performance. Propose some defense strategies to detect and prevent misinformation from LLMs. Call for more research and collaboration to address the challenges of LLM-generated misinformation and to ensure the ethical use of these models."
            }
        },
        "id": "EivlFFHfdU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "voBhcwDyPt",
        "replyto": "voBhcwDyPt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2482/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543115,
        "cdate": 1696707543115,
        "tmdate": 1701465466465,
        "mdate": 1701465466465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a comprehensive benchmark for Arabic NLG comprising 40 datasets over 13 tasks. To create that they mostly rely on already publicly available datasets and they additionally create datasets for context switching and dialogue response generation. A leaderboard and an investigation of baseline performances on the benchmark are also included. The authors promote the benchmark as an important resource that can help boost research in Arabic NLG.\n\nReasons To Accept:\n- Introduction of a new, comprehensive, and well-constructed benchmark with a clear comparison to existing benchmarks \n- The paper provides baselines, analysis, and evaluation framework that addresses and highlights strengths and weaknesses in existing works across different generative tasks which is very beneficial to researchers in this field\n- This is a high-impact paper well-suited to a high-impact conference.\n-Useful resource for Arabic NLG. \n- Public access and diversity\n- Help with the reproducibility of research in Arabic NLG.\n\nReasons To Reject:\n- Similar to ARGEN. The design principles used in the papers are also mentioned in the ARGEN benchmark paper.\n\nIn my opinion the reason to reject is not fair, and the answer written by the authors is explicit:\n\"In the context of comparing Dolphin with ARGEN, as highlighted within lines 119-123 and Table 1, several significant distinctions become apparent. Specifically, we observe the following:\n- Dolphin boasts a significantly larger dataset pool (~ 3X larger). Precisely, Dolphin comprises 40 datasets compared to only 13 datasets in ARGEN. Hence, Dolphin offers a total of 27 totally new datasets.\n- Dolphin's reach also extends to a wider array of task clusters, encompassing 13 clusters as opposed to ARGEN's seven clusters. Dolphin introduces six novel tasks: Arabic text diacritization, dialogue response generation, data-to-text conversion, grammatical error correction, text rewriting, and question answering.\n- Dolphin's datasets are drawn exclusively from publicly available sources, while ARGEN involves several non-public datasets such as Zbib et al. [2012] and Song et al. [2014]. As such, Dolphin avoids issues ARGEN suffers from such as challenges with (i) public distribution of the data and (ii) ease of evaluation.\n- Dolphin uniquely offers a benchmark leaderboard, a feature absent in ARGEN, providing real-time performance tracking and a dynamic evaluation environment.\""
            }
        },
        "id": "OVo5cw17az",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vkEYzLIdLX",
        "replyto": "vkEYzLIdLX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4497/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707591375,
        "cdate": 1696707591375,
        "tmdate": 1701465533121,
        "mdate": 1701465533121,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper addresses the Aspect-Based Argument Mining (ABAM) task and proposes a Hierarchical Enhancement Framework (HEF) consisting of four modules for ABAM. This paper studies an important problem and it is well written. Experimental results show that the proposed model outperforms baseline models. The paper needs to add a relation work section. Experiments on the effectiveness of SMIA component could be added as well. This paper should also clearly explain how the three key challenges were addressed by the proposed components."
            }
        },
        "id": "cAQFoPLJfb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vjTnfxbkaL",
        "replyto": "vjTnfxbkaL",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission2923/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552251,
        "cdate": 1696707552251,
        "tmdate": 1701465481198,
        "mdate": 1701465481198,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a study on norm violations in live-streaming platforms. It introduces a new data set of moderated comments from Twitch and a taxonomy of 15 norm violation types. Models are trained that use context to identify norm violations that can improve overall moderation performance.\n\nThe reviewers are in agreement in their positive judgement of the paper. Overall, they consider the paper is well writen and appreciate the problem framing and importance of the work. The reviewers also appreciated the novelty of the work in dealing with live / synchroneuous chat and the insights gained into this problem, especially comparing it with async platform.\n\nThe data and annotation process are clear and it should form a solid basis for future work in this space.\n\nThe reviewers did not identify any substantial reason for rejection or area of improvement."
            }
        },
        "id": "ynh9Bom3iq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vgg3dKoyDH",
        "replyto": "vgg3dKoyDH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5077/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606436,
        "cdate": 1696707606436,
        "tmdate": 1701465549355,
        "mdate": 1701465549355,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper tries to improve the concept compositionally for contrastive VL models. The main novelty comes from the reliable negative example constructions from text scene graph. Overall, it is a solid paper."
            }
        },
        "id": "J94zsn2xpB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vgaJRhYVje",
        "replyto": "vgaJRhYVje",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission78/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478580,
        "cdate": 1696707478580,
        "tmdate": 1701465386345,
        "mdate": 1701465386345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a benchmark dataset for visually grounded commonsense reasoning that allows testing models’ predictions coupled with explanations from the model. They compare different model architectures (VLMs/LLMs/combinations of them) and outline a data augmentation/construction pipeline that improves performance of models that train on this new data. All reviewers agree that the benchmark is well constructed and validated and is a good contribution to the community to further vision-language research and evaluate VLN models to a better degree."
            }
        },
        "id": "rlCHDmzt0Q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vg55TCMjbC",
        "replyto": "vg55TCMjbC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1846/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528665,
        "cdate": 1696707528665,
        "tmdate": 1701465444335,
        "mdate": 1701465444335,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new dataset, MenatQA, for testing the time comprehension and reasoning abilities of large language models (LLMs). The reviewers appreciate the comprehensive experiments and analyses, and the proposed prompt designs. However, they suggest that the authors could further analyze when and why the model makes mistakes, the rationale behind a few design choices, limitations in technical innovations. The authors' rebuttal highlights the value of the work, provides some explanation for the the experimental design, and explains some limitations, but there are still some opaqueness in the analysis and also limited novelty."
            }
        },
        "id": "dsalx8OYRQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vexCLJO7vo",
        "replyto": "vexCLJO7vo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission509/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489512,
        "cdate": 1696707489512,
        "tmdate": 1701465401624,
        "mdate": 1701465401624,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the hallucination problem with a method more efficient than previous attempts that require multiple samples or external knowledge. Their uncertainty based method is said to resemble human focus in focusing on keywords, uncertain tokens in the context, and token properties. The reviewers agree that the paper is sound and exciting and mainly suggest minor revisions to improve the writing. The reviewers note the intuitive approach is valuable and performs well across diverse experimental settings. It appears that the contributions of this paper will be valuable to the community."
            }
        },
        "id": "OmLFd7tZ18",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vdLFYqupHA",
        "replyto": "vdLFYqupHA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1867/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529036,
        "cdate": 1696707529036,
        "tmdate": 1701465445270,
        "mdate": 1701465445270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses an important issue of factuality, and propose a novel approach to encode factual knowledge during pre-training. It convincingly shows the merits of their method on both in- and out-of-domain datasets. As suggested by reviewers, I hope that the authors include more qualitative analysis of FactKB in the next version of the paper."
            }
        },
        "id": "Iq1unL0mTf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vaKgq549Dy",
        "replyto": "vaKgq549Dy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4812/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598942,
        "cdate": 1696707598942,
        "tmdate": 1701465542467,
        "mdate": 1701465542467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper aims to analyze what makes Chain-of-Thought prompting (CoT) prompting of large language models (LLMs) effective. The authors explore how elements such as \"symbols\" (e.g., digits, entities) and \"patterns\" (e.g., equations, sentence structure) in CoT prompt affect model performance with counterfactual analysis (where symbols and patterns are manipulated). In general, the reviewers think the work studies an important problem of when/why CoT prompting works, conducts comprehensive experiments, and could be valuable for research on CoT mechanism. They still have remaining concerns such as the definitions of \"symbols\" and \"patterns\" being vague. There is also very related work such as Wang et al. (2022) and Ye et al. (2022) that has similar findings, from which the authors need to clarify the distinction (although the work appears concurrently on arXiv, the publication date is way earlier than the EMNLP deadline). Overall, I'd recommend accepting this work to Findings."
            }
        },
        "id": "ScUYV1ZEmK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "va7nzRsbA4",
        "replyto": "va7nzRsbA4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2679/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547425,
        "cdate": 1696707547425,
        "tmdate": 1701465473274,
        "mdate": 1701465473274,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper analyzes representations of color terms and phrases in text-only models, including the alignment between embedding representations and actual color space, as well as relative representations of two color descriptions. The paper finds that subjectivity and abstractness in color descriptions poses a significant challenge for text-only models. The study is well-motivated and very interesting, but there are two methodological concerns brought by a reviewer. \n\nFirst, that there is a possible confound that might be artificially lowering alignment quality -- specifically, that some color names don't imply they are being used as color names at all. The authors perform additional experiments to show that when only considering color names that actually include known color terms (e.g., \"yellow\"), alignments are slightly higher quality than those without known color terms. I encourage the authors to include the analysis in a final version of the paper.\n\nSecond, that while the data here was generated by people annotating color hex codes, there is no verification or calculation of agreement that collected descriptions accurately represent their paired hex codes. While I agree this analysis would be out of scope for a short paper, the lack of such analysis should be acknowledged in the paper limitations."
            }
        },
        "id": "OQonCXKu3Z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vWy66avGPR",
        "replyto": "vWy66avGPR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5199/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608877,
        "cdate": 1696707608877,
        "tmdate": 1701465552736,
        "mdate": 1701465552736,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This study proposes a dataset, which comprises of 20K+ tweet-reply pairs. To enhance contextual understanding, each reply incorporates the original tweet to which it was posted in response. Human annotators carefully labeled the dataset. This study also provides experimental results with and without contextual information. \n\nAmong reviewers there is a consensus about the dataset as it provided replies as context. However, there are concerns about the annotation process, utilization of context and so on. \n\nReviewer xHbP raised concern about the benefit of contextual information. Reviewer tdPF questioned about the skewed distribution issue in the dataset. While authors addressed the concern, however, reviewer still has concerns such as data distribution and data sharing, which reflected in the rebuttal.  Reviewer UEvi also raised the same concern regarding the data distribution."
            }
        },
        "id": "VoGbt5kwE6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vWol8k64op",
        "replyto": "vWol8k64op",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3511/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564167,
        "cdate": 1696707564167,
        "tmdate": 1701465499657,
        "mdate": 1701465499657,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes a defense against backdoor attacks on NLP models. They observe that triggers are typically unusually correlated with their target labels, and use this correlation to detect and filter out poisoned inputs. The method is shown to achieve very good results against certain families of attacks. The paper is clear, and the approach technically sound, well-motivated, and effective. The authors make use of original, sometimes debatable choices - like relying on “eyeball detection” rather than optimizing a detection threshold algorithmically - but make arguments in favor of those choices. \n\nThe paper still suffers from some limitations pointed out by reviewers: the approach relies on a certain amount of knowledge on the defender’s side (on the choice of attacks). I would add in that regard that it often makes sense to instead assume the attacker has knowledge of the defense, and will adapt the attack accordingly. Conceivably, an attack could explicitly encourage the backdoor-label correlation to be just sufficient for the attacker’s use case while evading detection, which could circumvent this defense. That said, the approach is still usable against currently existing attacks, adaptive evaluation on poisoning attacks is hard to implement, and I think the approach could find practical applications given the current state of the art."
            }
        },
        "id": "nqbDcsjSkK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vW3TFDUKWl",
        "replyto": "vW3TFDUKWl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1990/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531609,
        "cdate": 1696707531609,
        "tmdate": 1701465449658,
        "mdate": 1701465449658,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This short paper offers a focused exploration of the effectiveness of LLMs, specifically GPT-J 6B, when applied to the ScienceWorld domain. The study involves fine-tuning GPT-J for ScienceWorld and highlights an 3.3x enhancement in performance compared to DRRN, which is an RL-based method. \nAdditionally, the paper presents various analyses of factors influencing performance, including the effects of training data volume and context length.  It also offers a breakdown of the most common error types encountered during experimentation. However, two reviewers find the paper not exciting enough. Therefore, I recommend accepting the paper to findings."
            }
        },
        "id": "a5JDHhAXzs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vVrwnY76W1",
        "replyto": "vVrwnY76W1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4126/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583593,
        "cdate": 1696707583593,
        "tmdate": 1701465521077,
        "mdate": 1701465521077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper provides a comprehensive empirical study on multimodal model merging for transformer architectures. Three reviewers provided reviews for this paper and these reviews were in consensus. The reviewers appreciated the paper and found the empirical analysis to be interesting. One major feedback by the reviewers was to increase the number of datasets and tasks for evaluation. The authors responded well to this feedback by providing several new experiments in line with previous work that had already been cited. I really appreciate this diligence and effort put in by the authors. The new experiments solidify the findings and increase the impact of this study. With these improvements and clarifications addressed by the authors, reviewers found the work to be technically sound and complete."
            }
        },
        "id": "Q0xcXivwUY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vVdRgpC1Oh",
        "replyto": "vVdRgpC1Oh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3452/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563043,
        "cdate": 1696707563043,
        "tmdate": 1701465498139,
        "mdate": 1701465498139,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "There is consensus amongst the reviewers after the rebuttal period that the presented work is Sound and moderately exciting. It is clear that the methodology itself is exploring novel perspectives, and can shed further light on the internal workings by understand how abstraction of knowledge is represented in a model. Most of the weaknesses point towards a limited set of experimentation and exploration. However, being a short paper, the experiments necessarily have to be focused and directed towards a concrete hypothesis, which the work successful does. Overall, the paper has _good technical soundness_ and _moderate excitement_.\n\nThe following is a summary of the strengths, weaknesses and scores across the six reviews:\n\n**Strengths:**\n\n- Proposed methodology (NVIB applied to transformers) is more linguistically motivated towards robustness (**YN7E**, **LT26**, **gprF**, **2hkB**, **fPkp**, **bGq2**)\n- Experimental results back the efficacy of the proposed method well across several NLP tasks (**YN7E**, **LT26**, **2hkB**, **bGq2**)\n- The main results and overall contributions are well presented (**gprF**, **2hkB**)\n\n**Weaknesses:**\n\n- Design choices and technical contributions are not well motivated, and sufficient prior-knowledge is not communicated well (**LT26**, **bGq2**)\n\t- Authors agreed to add more details in the rebuttal, and alluded to the lack of space in a short paper for this.\n- Experimental setup is limited (e.g. characters only, single language, small alphabet size, specific task-tuned models) (**YN7E**, **gprF**, **fPkp**)\n\n**Scores in decreasing order of confidence:**\n\n|      | Soundness | Excitement | Reproducibility | Confidence |\n|------|-----------|------------|-----------------|------------|\n| YN7E | 3         | 2          | 4               | 3          |\n| gprF | 4         | 4          | 5               | 3          |\n| fPkp | 3         | 2          | 3               | 3          |\n| LT26 | 3         | 3          | 5               | 2          |\n| 2hkB | 3         | 3          | 4               | 2          |\n| bGq2 | 3         | 3          | 3               | 1          |"
            }
        },
        "id": "43V5EFQNwY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vU0KbvQ91x",
        "replyto": "vU0KbvQ91x",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3288/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559740,
        "cdate": 1696707559740,
        "tmdate": 1701465492922,
        "mdate": 1701465492922,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper targets a practical setting of IE on visually riched documents. Instead of pure text, the inputs can be pdf or some other visual based file. The paper is well motivated and organized, and the experiments show the effectiveness.After serious discussion and consideration, the reviewers' main concerns are about the huge improvements by iterative component, small datasets, and comparison with LLMs. The authors actively provide more evidence which we think has mostly solved the concerns. Nevertheless, some reviewers are not excited about the idea, suggesting the effectiveness yet not much insights.\n\nOverall, we appreciate both the efforts of reviewers and authors. This work is ready to publish, and the only concern is the novelty. We hope the authors can continue to improve the paper according to the comments and your response."
            }
        },
        "id": "99GoBPFuT6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vR1yERC0Wd",
        "replyto": "vR1yERC0Wd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission912/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499253,
        "cdate": 1696707499253,
        "tmdate": 1701465414646,
        "mdate": 1701465414646,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes symbol tuning, a novel fine-tuning method for LLMs that uses random symbols instead of natural language labels and instructions. The paper shows that symbol tuning boosts LLMs’ performance on in-context learning and algorithmic reasoning tasks, and enables them to override prior knowledge with contradictory information. The paper tests symbol tuning on various benchmarks.\nThe reviewers like the paper for its novelty, effectiveness, motivation, analysis  and comprehensive experiments. The reviewers  only mention some minor issues, as well as reproducibility problem due to the closed-source models. The reviewers recommend accepting the paper, as it offers a simple yet effective  technique for improving LLMs’ in-context learning and algorithmic reasoning abilities. The paper also opens new possibilities for creative uses of input-label pairs during in-context learning.\n\nPros:\nThe symbol tuning approach is simple, effective, and seems to complement instruction tuning.\nNovelty: The authors provide a novel and effective approach to fine-tuning models.\nRobustness: The methodology is supported with clear and logical analysis on the impact of symbol tuning in various contexts.\nStrong Motivation: The paper addresses the concern of prompt sensitivity in LLMs performance, providing a strong motivation for introducing symbol tuning.\nUsefulness to the NLP Community: The symbol tuning approach is beneficial for training LLMs using in-context examples.\nStrong Experiment Results: Comprehensive experiments were performed on a broad range of NLP tasks and with varied sizes of LLMs. \nDetailed Experiments: The details of the experiments are available, making the results convincing.\n\nCons:\n\nReproducibility Issue: Due to the models tested being closed-source, reproducibility could be an issue, although the process of symbol-tuning is described in detail.\nLimited Experimentation: The authors only experiment with one class of closed-source models which are inaccessible to most researchers. The effectiveness of symbol tuning on open-source LLMs such as Llama 1,2, is still unclear.\nLack of Evidence: While the authors hypothesize why symbol tuning allows models to perform better, there’s a lack of clear evidence backing these model’s performance.\n\nPlease note that a reviewer  mentioned template Issue: The template does not look like EMNLP 2023 template."
            }
        },
        "id": "BTpbJkgahz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vOX7Dfwo3v",
        "replyto": "vOX7Dfwo3v",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission162/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480730,
        "cdate": 1696707480730,
        "tmdate": 1701465389234,
        "mdate": 1701465389234,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a layer which composes the representations produced by the different layers of the encoder and outputs keys and values which are fed into the different layers of the decoder. The proposed layer is evaluated in the tasks of machine translation and semantic parsing.\n\nThe paper received mixed reviews. The reviewers found that that the paper has some merits: (i) the proposed approach is simple and quite general; (ii) the method appears to be empirically strong in the machine translation task; (iii) the considered problem is interesting and deserves further research. However, the reviewers also raised various concerns mainly about the lack of novelty, the lack of large baselines and about claims that appear in the paper and which are not well-justified. For instance, one reviewer stressed that there is no evidence that the proposed method indeed effectively combines semantic with syntactic information. There were also concerns about the presentation of the paper which gives the reader the impression that the proposed method is entirely novel and that there is no overlap with prior work.\n\nTaking author response into account, I think that the paper is interesting and that this line of investigation is worthwhile, but I also agree with some of the concerns that were raised in the reviews."
            }
        },
        "id": "SHu8n51W46",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vMpmabFTFw",
        "replyto": "vMpmabFTFw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1016/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502630,
        "cdate": 1696707502630,
        "tmdate": 1701465417924,
        "mdate": 1701465417924,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "> Overview\n\nThis paper presents a cognitive model of word integration in context, which makes predictions about word recognition times. It then proposes two linking functions (termed *shift model* and *variable model*) of how their predicted word recognition times affect observed neural responses, which they compare to a baseline (which does not use word recognition time information). They find that the variable model significantly outperforms the baseline.\n\n> Meta Review\n\nThe three reviewers and I agree that this paper’s analysis sheds light on the neural and temporal dynamics of spoken word recognition. They also agree that the paper’s methods are sound (modulo a robustness analysis the authors promised to address for camera ready). There is some disagreement, however, regarding how to interpret the paper’s theoretical contributions. While two of the reviewers appreciate the proposed cognitive model for word recognition in context, reviewer FAWT questions which kind of testable predictions it makes. Personally, I share some of reviewer FAWT’s scepticism about the proposed model (longer comment below). Regardless of these issues, however, I found this paper interesting and believe it might motivate more future work on the topic. \n\n> Further Comments\n\n\nI personally believe that: (i) this paper's topic does fit EMNLP; (ii) this paper's analysis makes a significant contribution in investigating the temporal dynamics of spoken word recognition; and (iii) this paper's contributions are not immediately clear from its text.\n\nRegarding (iii), I had questions similar to reviewer FAWT's, not being able to clearly situate this paper's contributions. At a first read of this paper, I believed that this paper’s main contribution was the two proposed linking functions (described in section 1.2) and the comparison of model fits to EEG data. After re-reading all reviews and author responses in detail, however, I now believe that the paper also claims its Bayesian cognitive model (in section 1.1) as a key contribution. If this is the case, I would expect (as noted by reviewer oXeZ) a longer discussion comparing this model with prior work modelling single-word and word-in-context recognition. Ideally, I would expect to even see an empirical comparison between how predictive of neural responses are the *word recognition times* output by this cognitive model vs prior works’. The proposed cognitive model (in particular, eqs. 1 and 2 here) also seems very similar to, e.g., the word recognition model from Smith and Levy (2010)—which was developed for analysing reading times—and it would be nice to see a discussion comparing them. Further, the *variable model* could be more carefully motivated in my opinion. In particular, I don't see why the word integration process would rely on clusters based on a word's required recognition time, instead of, e.g., varying smoothly as a function of the word's recognition time.\n\nFinally, I believe that reviewer FAWT’s question about how much of the model's performance is parameter-dependent becomes particularly relevant given the new results (in the authors’ response) that the proposed *variable model* (which predicts neural signals using clusters of word recognition times) is not significantly better than the *prior-variable model* (which predicts neural signals using clusters of surprisal values).\n\nSmith and Levy (2010). Fixation durations in first-pass reading reflect uncertainty about word identity. \n\n> Potential Typos\n\nI believe there might be a typo in eq. 5, as I don't see how word-level features would be incorporated into this equation."
            }
        },
        "id": "k7AawPpJTy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vLoSutEAJM",
        "replyto": "vLoSutEAJM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4037/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581772,
        "cdate": 1696707581772,
        "tmdate": 1701465518014,
        "mdate": 1701465518014,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose to tackle ambiguous open-domain questions by iteratively building a tree of clarifications, which holds interpretations for the original question. Then, the long-form answer is generated conditioned on these interpretations.\n\nReviewers all like the proposed approach for the problem, and are impressed with the improvements over the baselines.\nSome reviewers are favorable towards the ablations in the paper but reviewer UCkq mentions that analysis is missing w.r.t the different components of the system.\nReviewers also mention that the latency slowdown of using the proposed approach is not analyzed, and that the authors only experiment with one dataset.\n\nImprovement: I noted that you use SentenceBERT for ranking answer sentences. The specific research area is called Answer Sentence Selection, and you can find the state of the art below:\n\nSiddhant Garg, Thuy Vu, Alessandro Moschitti. TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection. AAAI 2020.\n\nIvano Lauriola and Alessandro Moschitti: \"Answer sentence selection using local and global context in transformer models\", ECIR 2021."
            }
        },
        "id": "aTDttbspvZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vDvFT7IX4O",
        "replyto": "vDvFT7IX4O",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3121/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556510,
        "cdate": 1696707556510,
        "tmdate": 1701465487740,
        "mdate": 1701465487740,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reasons Accept:\nThe reviewers agree that this a simple and linguistically motivated data augmentation approach that could be straightforwardly applied to extremely low-resource language pairs.\nReasons Reject:\nThe proposed approach heavily relies on the existence of a high resource language which share linguistic features which limits its applicability. There could have been some analysis of how relatedness affects viability of the approach. \nMetaReview\nThe authors submitted extensive rebuttal including new results which led to the increase in soundness from one reviewer from 3 to 4. All reviewers acknowledged rebuttal and there was some final discussion."
            }
        },
        "id": "seFwTLPMNT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "vBZ5eBdrgH",
        "replyto": "vBZ5eBdrgH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2513/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543681,
        "cdate": 1696707543681,
        "tmdate": 1701465467330,
        "mdate": 1701465467330,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a novel approach to unsupervised domain adaptation for both the dense retrieval model and rerank model in a two-stage retrieval system. Through meticulous experimentation, the authors validate the efficacy of their proposed method in domain adaptation. The reviewers' opinions on this work are not unanimously positive. While the author has addressed certain concerns raised by the reviewers, there are still some lingering apprehensions. zViP expressed concerns about the training time consumption of the method. The authors responded by providing a detailed comparison with previous methods and indicating the time consumption involved. Although the method does require a significant amount of time, zViP acknowledges that these costs are a reasonable investment. X5Zx raised concerns regarding method details, experimental details, and experimental repeatability. The authors provided a thorough response addressing these concerns. kWur recognizes the novelty of the method and provides suggestions for further revision of the paper. XVNc expressed concerns about the weaknesses in the comparison methods, the complexity of calculations leading to practical feasibility issues, and difficulties in reproducibility. The authors provided corresponding responses, but these concerns still remain."
            }
        },
        "id": "9BeSWT4HzZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "v9CVjuNlDI",
        "replyto": "v9CVjuNlDI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1780/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527066,
        "cdate": 1696707527066,
        "tmdate": 1701465441190,
        "mdate": 1701465441190,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper seeks to understand LLMs by applying a human test set based on Bloom's Taxonomy of Learning. Reviewers find that the approach has significant merit, but highlight weaknesses related to reporting information about the dataset, including annotator training and agreement, and robustness to instruction wording. These concerns appear to have been addressed in the discussion period."
            }
        },
        "id": "ztiYVCi7rm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "v8fRIzqeob",
        "replyto": "v8fRIzqeob",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3413/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562196,
        "cdate": 1696707562196,
        "tmdate": 1701465496692,
        "mdate": 1701465496692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The methodology, results, and presentation are sound. The relevance to linguistics and human cognition are slimmer."
            }
        },
        "id": "AxYdpNGA0E",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "v7JgI9dny2",
        "replyto": "v7JgI9dny2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4879/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707600175,
        "cdate": 1696707600175,
        "tmdate": 1701465543936,
        "mdate": 1701465543936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This manuscript seeks to deal with the confusion that arises from the terms (and research on) values, morals, and ethics. The manuscript surveys prior literature from outside of NLP on these concepts, then surveys NLP literature, and suggests future avenues for NLP.\n\nIn general, the reviewers are positive towards this manuscript and particularly appreciate:\n\n1. That there is confusion of the terms and this manuscript may provide a starting point to address such confusion\n2. The manuscript provides a comprehensive report on how the terms have been used\n3. The manuscript provides quantitive support for their recommendations and findings\n4. The manuscript highlights flaws in terminology in prior work\n\nHowever, the reviewers had the following contention:\n\n1. The manuscript would do well to consider additional related terms such as norms, beliefs, customs, behaviors, and ideologies.\n2. It can be hard to gain a sense of the bigger picture from how results are presented\n3. The selection of papers is narrow and does not take into consideration topics where the terms in question are implicit.\n4. It is unclear how to implement recommendations for common terminology in future work\n\nWrt. 1 both authors and reviewers agree that this manuscript can serve as a starting point for such considerations. To this effect, authors will reframe writing to make the bigger picture (2) clearer. While the authors do not address 3, the scope of addressing this is incredibly large and is in my opinion more appropriate for future work. Finally, the authors correctly identify that NLP researchers creating vocabularies may lead to further inaccuracies and limit opportunities for interdisciplinary collaboration.\n\n5193\n\nThis manuscript presents a study on practices and experiences of annotation on amazon mechanical turk. Towards the goal of understanding crowdwork, they review prior work, public discussions on reddit, and conduct a survey on MTurk. As a result they provide several recommendations based on their findings.\n\nReviewers are generally positive towards this manuscript, particularly as it provides evidence-based recommendations for best-practices for annotation.\n\nOf the concerns highlighted, only the concern that the manuscript applies to more fields than NLP is reasonable. While the reviewer is correct that best practices for annotation work is not only applicable to NLP, work in NLP very often requires human labelling and the paper is therefore also very applicable to the ACL community.\n\nA slight concern that I have: The book Ghost Work by Suri and Grey is missing from the references, which would further support and contextualize the claims and findings. Moreover, Turkopticon, the organization that has been working on crowd worker’s rights seems to have been neglected, and the findings from their work and their advocacy should be included in the final manuscript, should it be accepted."
            }
        },
        "id": "qMnoaPj9vc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "v6iM1bO78t",
        "replyto": "v6iM1bO78t",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5193/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608778,
        "cdate": 1696707608778,
        "tmdate": 1701465552420,
        "mdate": 1701465552420,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This short paper proposes a simple PCFG formalism, which produces each child independently in a PCFG production rule. This simplification scales more effectively to large number of nonterminals. Experimental results on four treebanks show significant improvements over low-rank PCFG with same number of nonterminals, and also better performance than other unsupervised parsing algorithms (e.g. C-PCFG, S-DIORA and ON). This paper also introduces a hardware-efficient implementation to speedup the computation with lower memory usage.  Despite the simplicity of this approach, as a short paper, the experimental results on both unsupervised parsing and language modeling should be beneficial to the community of unsupervised parsing."
            }
        },
        "id": "gVNIJ7P3pj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "v6hcCtzAWz",
        "replyto": "v6hcCtzAWz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5824/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618871,
        "cdate": 1696707618871,
        "tmdate": 1701465567809,
        "mdate": 1701465567809,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a three-stage interactive prompting approach to ensure that LLMs perform well in reasoning tasks in noisy contexts. The proposed method is simple and effective as shown in the experiments on several datasets. The contributions are somewhat slim though, the paper can benefit from having the evaluation across more tasks and having more error analyses."
            }
        },
        "id": "yG67BhadJ7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "v6VbokqzvP",
        "replyto": "v6VbokqzvP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5057/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606051,
        "cdate": 1696707606051,
        "tmdate": 1701465548804,
        "mdate": 1701465548804,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors investigate several methods to use a QE (sentence-level and word-level Quality Estimation) system to improve an APE (Automatic Post-Editing) system for machine translation.\nThey conclude a joint training of a model (dual-encoder single-decoder model with task-specific heads) with Nash-MTL (Mult Task Learning)\nperforms the best and obtains significant improvements in English-to-Marathi and English-to-German translation tasks.\n\nThe paper is well-written, and the experiments show the effectiveness of the proposed method.\nHowever, one reviewer has judged that the proposed method has low scientific novelty because it merely applies a known MTL method to a slight extension of a standard APE model."
            }
        },
        "id": "iUAjRr3vu3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "v2wbkddf52",
        "replyto": "v2wbkddf52",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5439/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613082,
        "cdate": 1696707613082,
        "tmdate": 1701465559438,
        "mdate": 1701465559438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper uses concepts from Tropical geometry to identify parameters for pruning from adapters. The paper is overall well-written considering the mathematical concepts that most of the NLP community is less likely to be familiar with. Reviewers overall appreciated the elegancy of the approach, and the results. One major question that immediately comes to mind is why prune the adapters which are already supposed to be relatively small, and not prune the full model? Based on the discussion (with reviewer pb9m), it seems like the answer is that the proposed approach relies on certain geometry assumptions that are specific to adapters. This should be better explained and discussed in the paper in order to properly define the scope of the introduced solution. Overall, while this limited scope slightly reduces the excitement, it could still be of interest to other researchers in the community. The soundness should be improved by the comments from the reviewers, specifically, explaining the assumptions/ constraints and why they are suitable for adapters, and comparing to other pruning baselines."
            }
        },
        "id": "DPnPXEMU8s",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "v15z3FzZGu",
        "replyto": "v15z3FzZGu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4733/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597338,
        "cdate": 1696707597338,
        "tmdate": 1701465540096,
        "mdate": 1701465540096,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors describe a novel approach (similar to masked language model) to self-learn rules that serve as interpretable knowledge to perform relation linking in knowledge base Question Answering. \nThe paper has some merits and the proposed solution is demonstrated to be effective on 2 datasets.\nThe main issue of the writing: the proposed method is not clearly described and the paper does not provide enough examples. This lack of clarity does not enable the reader to have a full understanding and obfuscate the quality of the work."
            }
        },
        "id": "5Xjc1ld2aI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uz89EXE540",
        "replyto": "uz89EXE540",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2887/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551549,
        "cdate": 1696707551549,
        "tmdate": 1701465480106,
        "mdate": 1701465480106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are in general agreement that this paper could be accepted in EMNLP-2023. The reviewers also raised some concerns about this submission. The authors have provided a rebuttal that appeared to alleviate some concerns. A suggestion was made to accept the paper as Main Conference. The reviewers' concerns are also suggested to be addressed in the final version."
            }
        },
        "id": "xOk5HV0xjZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uz4OrlHDA8",
        "replyto": "uz4OrlHDA8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3682/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567660,
        "cdate": 1696707567660,
        "tmdate": 1701465506544,
        "mdate": 1701465506544,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "All the reviewers agree that the paper contributes an extremely valuable multilingual dataset resource for the community as the dataset covers many low-resource languages and is curated carefully via transcription and translation with professional translators. They perform sufficient benchmark experiments and propose a better automatic metric system that better correlates with human ratings than the existing ones.\n\nAs mentioned by two other reviewers, and noticed by myself, the paper is slightly hard to read due to the authors' choice of presentation of information in the paper. For instance, the authors currently grouped their proposed metrics STATA under the experimental setup when their intention is to point out existing issues with existing automatic evaluation metrics and hence a need for better metrics (as discussed by the authors in their rebuttal). I believe the current paper structure may be one of the reasons why reviewers RmMM and qzSY have a difficult time evaluating the contributions of the proposed metric STATA, although it's much clearer now (to me) after the authors clarify the contributions of STATA in the rebuttal. \n\nAnother weakness of the paper is the missing (important) information about the translators and annotators, which should be added to the revised version of the paper. The authors did not give much details about them in the rebuttal. The paper could have been also made stronger by clarifying the choice of Russian as the zero-shot test language (as the authors did in the rebuttal and should add to the revised section 3.3 of the paper)."
            }
        },
        "id": "TOeVXCg9MK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uyl1O2LkAF",
        "replyto": "uyl1O2LkAF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission803/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496383,
        "cdate": 1696707496383,
        "tmdate": 1701465410990,
        "mdate": 1701465410990,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This study applies CoT to programming translation tasks. Initially, it generates explanations for the source code and then proceeds to generate the programming code. This approach has demonstrated performance improvements in multiple settings. Reviewers have expressed concerns about the reproducibility of experiments conducted solely with ChatGPT, and the authors have presented additional experiments during the rebuttal to address these concerns. On the other hand, the authors plan to compare with the open source SOTA model in the future."
            }
        },
        "id": "gzw8UuHLaZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uyUO80sbm0",
        "replyto": "uyUO80sbm0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission196/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481546,
        "cdate": 1696707481546,
        "tmdate": 1701465390766,
        "mdate": 1701465390766,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper in submission explores the connection between EEG (Electroencephalogram) signals and language, making it one of the first studies to do so in the field. The authors provide code for analyses, making the research reproducible. The study uses multiple baseline models for comparison and shows that their model outperforms others across different task conditions. A significant strength lies in emphasizing the multi-modal alignment between brain features and text descriptions, highlighting its practical relevance. The paper demonstrates superior accuracy in downstream tasks compared to non-alignment methods.\n\nHowever, the paper also has several shortcomings that raise questions about its overall contribution and clarity. One major concern is the near-100% accuracy reported for certain tasks, which seems implausible and is not adequately explained. There is also a lack of information regarding how EEG signals can be reliably interpreted to understand what a person is thinking or talking about. The study lacks a clear articulation of its contributions to our understanding of language processing in the brain or language representations in artificial models.\n\nThe methodological presentation in the paper is somewhat weak. It references three versions of their model (\"Ours-EEG,\" \"Ours-Text,\" and \"Ours-Multimodal\") without clearly explaining the differences in their inputs. Questions about the model architecture remain unanswered, reducing the clarity of the paper's contributions. Concerns are also raised about the soundness and novelty of the conclusions, particularly relating to the identification of brain areas and EEG frequency bands responding to positive and negative words. The description of the methodology used is considered too vague to affirm whether the conclusions are valid.\n\nThe paper could benefit from additional statistical analyses to support its arguments, as only qualitative interpretations are currently provided. The limitations of using EEG for locational interpretations are acknowledged but not thoroughly discussed. The paper also fails to answer some high-level questions critical to the field, such as how the continuous EEG data align with the discrete nature of language and how individual subjectivity in EEG signals is addressed.\n\nFurther, given that the paper is intended for an NLP (Natural Language Processing) conference, the authors should include explanations of physiological signals like EEG and MEG (Magnetoencephalography) for readers unfamiliar with these concepts. The paper is also advised to delve into why specific metrics like CCA (Canonical Correlation Analysis) and WD (Wasserstein Distance) were chosen over others like KL divergence or Euclidean distance.\n\nLastly, there are several mistakes in the paper, ranging from undefined acronyms to grammatical errors, that need correction before final submission.\n\nOverall, while the paper makes a pioneering attempt to link EEG with language and offers promising results, it suffers from issues related to clarity, methodology, and the soundness of its conclusions. I would suggest the author submit this work to journals instead of presented in EMNLP 2023."
            }
        },
        "id": "R0THHAWjqq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uxzlH5bLrJ",
        "replyto": "uxzlH5bLrJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission345/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485318,
        "cdate": 1696707485318,
        "tmdate": 1701465395895,
        "mdate": 1701465395895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces DemoSG to handle event extraction low-resource scenarios. DemoSG leverages BART and incorporates a 'demonstration' of the event type to extract, along with schema-based prompts that include event type and role labels. The BART decoder output is matched to trigger and argument mentions in the input text. The evaluation covers in-domain and domain adaptation settings using ACE05-EN and ACE05-EN+ datasets, demonstrating competitive or improved performance compared to state-of-the-art systems for low- and high-resource event extraction. \n\n\n\nThe reviewers raised considerable concerns but the reviewers could convince the reviewers and address many of their concerns.  Additionally, the reviewers also agree that the contribution is exciting and the results are reproducible."
            }
        },
        "id": "1Es5RiHFGL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ux826WlJtt",
        "replyto": "ux826WlJtt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1277/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509177,
        "cdate": 1696707509177,
        "tmdate": 1701465425968,
        "mdate": 1701465425968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes 'Look-Back', a decoding algorithm that leverages KL divergence to track the distribution between current and historical decoding steps in order to manage repetitions and topic drift. The paper is well written, clearly motivating the problem and explaining the proposed decoding algorithm, and demonstrating the performance of the new decoder via methodical evaluations against SimCTG. The experiments show through automatic and human evaluations the validity and advantage of the proposed Look-back decoding algorithm."
            }
        },
        "id": "sF2xslHqag",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uvbbsn4l6y",
        "replyto": "uvbbsn4l6y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5209/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609026,
        "cdate": 1696707609026,
        "tmdate": 1701465553049,
        "mdate": 1701465553049,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses the challenge of training LLMs when obtaining labeled training data is costly or unavailable. The method presented is both intuitive and supported by well-motivated design choices, enhancing its accessibility and promise. Importantly, the method yields substantial improvements in various reasoning tasks, demonstrating its relevance to the research community. Additionally, the paper is well-written and structured. The authors have addressed the concerns raised by the reviewers. Therefore, I recommend accepting the paper."
            }
        },
        "id": "tU7JFcPg5c",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uuUQraD4XX",
        "replyto": "uuUQraD4XX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4277/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586528,
        "cdate": 1696707586528,
        "tmdate": 1701465526393,
        "mdate": 1701465526393,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the limitations of existing code Language Models (LLMs) by proposing CodeT5+, a flexible encoder-decoder model capable of adapting to a wide range of downstream code tasks. The authors investigate the model's architecture, pretraining tasks, and computational resources, and demonstrate its effectiveness through comprehensive comparisons with other large models. The paper is well-received for its detailed analysis of existing LLM limitations, and the innovative training strategies implemented in CodeT5+.\n\nStrengths:\n* CodeT5+ introduces a new model architecture and training method for code LLMs, improving performance with fewer parameters.\n* The paper provides a clear summary of the limitations of existing work and offers practical solutions.\n* The authors present a detailed description of CodeT5+ and its components, including a computationally-efficient pretraining strategy.\n* The model demonstrates state-of-the-art performance on various code-related tasks.\n\nWeaknesses:\n* The paper lacks clarity on some details of the paper.\n* Some reviewers raised questions about the impact of the \"Compute-efficient Pretraining with Frozen Off-the-shelf LLMs\" method on * performance and the differences between encoder-decoder and decoder-only configurations.\n* The description of pretraining tasks, specifically \"Text-Code Contrastive Learning,\" could be improved for better understanding.\n\nOverall, the paper is well-executed and provides valuable insights into the limitations of existing code LLMs and the potential benefits of the proposed CodeT5+ model. The study is sound and generates excitement in the field by deepening the understanding of code LLMs and lowering barriers to existing research directions. However, some concerns need to be addressed, including the lack of clarity on pre-training data access and the need for more details on certain pretraining tasks."
            }
        },
        "id": "6OuwZZSLk4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uu6Oq7MN7g",
        "replyto": "uu6Oq7MN7g",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission851/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497650,
        "cdate": 1696707497650,
        "tmdate": 1701465412816,
        "mdate": 1701465412816,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "We had really abundant discussions regarding this work, which help clarify many important technique issues. Thank  reviewer Gx4y for the patience and hard work. Also thank the authors for detailed responses. Reviewer 6hiA resolved several math notation problems.\nBased on the comments of all reviewers and authors’ rebuttal and my own reading of this paper, I think this work proposes a novel and correct three-step semantic parsing approach, which works well on the COGS dataset, especially from the structural generalization perspective. \n\nThree minor suggestions for improving this paper:\n\n1)\tFor the first step, please give more explicit discussions on the one-word-one-concept assumption. For example, how does the assumption affect the proposed approach in the sense that how or whether it can be applied to other datasets, especially those in which the number of concepts are often larger than that of words. \n\n2)\tI have a feeling that more results and analysis can make this more influential, especially from the  structural generalization perspective. Please use the extra page properly. \n\n3)\tCan you add discussion about the following recent work (ACL-2023) in the next version? I think there is strong connection between your work and this one. Of course I understand the two approaches are different. \n\n> Compositional Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann | Alexander Koller | Ivan Titov"
            }
        },
        "id": "t7lMdQmwBy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "usnEi3Bfnt",
        "replyto": "usnEi3Bfnt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3030/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554697,
        "cdate": 1696707554697,
        "tmdate": 1701465484995,
        "mdate": 1701465484995,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Paper Topic And Main Contributions:\n* The paper proposes a question-aware global-to-local graph reasoning approach (GLGR) for multi-party dialogue reading comprehension (MDRC).\n* It introduces two types of graphs to facilitate reasoning. These graphs model global information across utterances and local information within individual utterances. A two-stage encoder network is employed to reason from the global level to the local level. \n* Experiments on two MDRC datasets (FriendsQA and Molweni) show that GLGR achieves significant improvements over BERT and ELECTRA baselines and state-of-the-art methods. The results show the contributions of global and local graph reasoning, as well as their integration via the two-stage reasoning process.\n\nReasons to accept:\n* The paper is well organized and written.\n* The two-stage reasoning framework going from global graph to local graph is intuitive and well-motivated for connecting coarse-grained and fine-grained semantics.\n* The paper includes detailed experiments and an ablation study to demonstrate the effectiveness of the proposed approach.\n* Generalization across models with different pretraining schemes (BERT and ELECTRA), which is promising.\n* The global-to-local reasoning method applied to both graphs results in improved performance in multi-party dialogue reading comprehension.\n\nReasons to reject:\n* The proposed graph construction heavily relies on external tools, such as co-reference, which may impact the stability of the approach. The technique proposed seems to include a lot of moving parts and engineering effort compared to the performance improvement over other baselines considered, which again impacts practical applicability. The authors explain in the rebuttal that their models are more lightweight than LLMs such as Llama.\n* The decrease in performance as context length increases is a concern for practical applicability of these models. As dialogues become longer, the relationships between utterances and entities can become complex and introduce additional noise, thereby limiting the applicability and generalization ability of the approach. The authors explain in the rebuttal that their model performs better than the baselines in this regard."
            }
        },
        "id": "R36VFJaoen",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "us7p0VsOhl",
        "replyto": "us7p0VsOhl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission895/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498662,
        "cdate": 1696707498662,
        "tmdate": 1701465413959,
        "mdate": 1701465413959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the existence and mitigation of hallucination in answers generated by LLMs in generative question answering in the medical domain. In particular, the authors first classify the hallucinated answers into 3 types and report their distribution in answers generated by 5 LLMs. Next, an iterative refinement method is proposed that reduces the amount of hallucination by first refining a generated background knowledge and then refining the generated answer based on the background knowledge. Both automatic and human evaluation results are reported, and the proposed method seems to reduce the amount of hallucinated answers.\n\nThe proposed method has been evaluated on top of Vicuna-7B and ChatGPT, showing improved results on 5 biomedical datasets.\n\nDuring the discussion period, most questions have been answered reasonably well. One of the reviewers complained about novelty, but the paper's focus on the biomedical domain with its unique challenges makes it different from previous work."
            }
        },
        "id": "86NAKrNeAe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "up8EYzyrKV",
        "replyto": "up8EYzyrKV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2424/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541837,
        "cdate": 1696707541837,
        "tmdate": 1701465464666,
        "mdate": 1701465464666,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main strength of the paper is that its contribution is important to processing related specifically to sign language, where there has been less work relative to spoken or written language. The reviewers also found the paper clear and well written. Room for improvement include adding the needed details for the method to be reproducible. Some reviewers also thought that the method should be tested on a larger available dataset."
            }
        },
        "id": "iAxTY1xST0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "unKIy4mpnn",
        "replyto": "unKIy4mpnn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission544/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490406,
        "cdate": 1696707490406,
        "tmdate": 1701465402845,
        "mdate": 1701465402845,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "All reviewers agree that this large-scale evaluation benchmark, which includes data and tasks for many under-resourced languages, will be useful for the multilingual NLP community as the current data and task coverage for these languages are scarce. In the rebuttal, it is made clearer what the notion of user-centric means (i.e., downstream tasks with a large number of end users), which justifies the selection of tasks including ASR and MT in the benchmark. The experimental setup, including the transformation of tasks into a seq-to-seq format and the few-shot prompting method, is also based on the recent standard practice of prompting language models. While one point of contention is about the choice of baselines, given the task format and the inclusion of multilingual models of different scales and different tokenization scheme, I believe the setup is sufficient. The authors also make salient their exact contributions to existing datasets by providing the table breakdown of their data contributions during the post-rebuttal discussion period, which hopefully will be included in the next revised version of the paper. One common weakness of the current paper raised by reviewers is the lack of in-depth analysis in Section 5; for instance, how the quality of the training splits of collected data (in a constrained setting of 8 hours) impacts the benchmarking evaluation and what is the reason for the poor performance of African languages."
            }
        },
        "id": "DSvLQzJHF0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ulqYwmcUnL",
        "replyto": "ulqYwmcUnL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission275/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483500,
        "cdate": 1696707483500,
        "tmdate": 1701465393404,
        "mdate": 1701465393404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Summary evaluation**: The authors propose a diffusion based visual story telling method to improve the quality and inference speed of the task. Two active reviewers have a consensus on the novelty of the proposed method and appreciate the comprehensive analyses and evaluations. Both reviewers had similar concerns related to the limited baseline, however the authors provided additional baselines to resolve the concern from the both reviewers. Also, the authors could provide comprehensive answers to the reviewers, so the most concerns were resolved.\n\n**Reviewer Xen1's reviews**: Reviewer Xen1 have been having a different opinion, but the reviewer haven't interacted at all and haven't even acknowledged the rebuttal by the authors. Also, it was pointed out that Xen1's summary of the paper contained exact copy from the paper which was not desirable.\n\n**Reviewers' recommendations**: After the in-depth discussions, two active reviewers made a consensus that the soundness is good or strong. Also, in terms of excitement, both reviewers had similar consensus which is good or strong. The reviewer Xen1's scores have been '2' on both criteria since the initial review, but Xen1 haven't joined the discussion about misunderstanding of the method. Therefore, I am downweighing the opinions from Xen1."
            }
        },
        "id": "iVQUbPAG3Z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ul47tFdRn6",
        "replyto": "ul47tFdRn6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5029/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605451,
        "cdate": 1696707605451,
        "tmdate": 1701465547862,
        "mdate": 1701465547862,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposed a method to represent multiple modalities, perform an extensive comparison with existing methods, and demonstrate competitive performance. However, this paper has limitations in a lack of clarity on the source of result improvements, the absence of error analysis, the need for more details on baseline results computation, and a perceived lack of novelty in the components used in the workflow."
            }
        },
        "id": "zVcLvJ2bxh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uhVJ3SLq80",
        "replyto": "uhVJ3SLq80",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1552/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517331,
        "cdate": 1696707517331,
        "tmdate": 1701465434169,
        "mdate": 1701465434169,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work examines the input (non-contextual) embedding structure of two families of multilingual LMs, finding that one (XLM-RoBERTa family) learns to separate languages (as one might guess) while the mT5 family unsupervisedly learns cross-lingual structure in the embedding space, as nearest neighbors tend to be related words in other languages. Reviewers praised it for being well-written and interesting, but had some concerns as to the lack of concrete takeaways, a somewhat small scope, and the importance of the results relative to existing knowledge of cross-linguality in language models. \n\nOverall, I think that it is a reasonable contribution for a short paper, but I would challenge the authors to engage with the question of what the importance of these results are for the broader goal of understanding the models–do they suggest an interesting next study? Are they related to an overall improved cross-lingual transfer behavior in mT5?"
            }
        },
        "id": "jg5ZEleKZs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uh5euNmL7t",
        "replyto": "uh5euNmL7t",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4386/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588713,
        "cdate": 1696707588713,
        "tmdate": 1701465530130,
        "mdate": 1701465530130,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposed to automatically create a new dataset, TOPDIAL, to improve the user experience via personalization in the target-oriented dialogue system. The data construction method is simple but effective. The dataset can be a good resource for the dialogue community.\n\nThe paper can be improved by clearly 1) highlighting the distinctiveness of the TOPDIAL dataset compared to existing datasets like DuRecDial 2.0 and 2) discussing the significance of the personalized aspect and its impact on the user experience."
            }
        },
        "id": "mfiSDP225Z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ufu4C0bTwB",
        "replyto": "ufu4C0bTwB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1553/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517424,
        "cdate": 1696707517424,
        "tmdate": 1701465434270,
        "mdate": 1701465434270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduce a sentence-level AI-generated text detection challenge by synthesizing a dataset that contains documents polished with LLM. It also proposes SeqXGPT, a method based on convolutional neural networks and self-attention networks by utilizing log probability lists of white-box Large Language Models (LLMs). The reviewers agree that the paper makes a novel contribution to the field, and the experiments clearly establish the effectiveness of the proposed approach. Reviewers also appreciated the additional results provided by the authors during rebuttal, which should be added in the main paper."
            }
        },
        "id": "7hVsrfYYY5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uemYdRTVvP",
        "replyto": "uemYdRTVvP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission175/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481068,
        "cdate": 1696707481068,
        "tmdate": 1701465389769,
        "mdate": 1701465389769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces \"DIFAIR\", a manually curated dataset designed to evaluate Transformer-based Masked Language Models (MLM) for gender bias. The dataset's creation involved data collection, cleaning, labeling using annotators, and establishing annotation guidelines. A new metric, GIS, is proposed to measure gender bias and assess whether a model retains gender knowledge or is fair in preserving gender information in relevant sentences. The paper highlights that while some models reduce bias, they may compromise gender knowledge, and vice versa. The research emphasizes the need for a balance between reducing bias and retaining essential gender information. However, the paper does not compare DIFAIR with other relevant datasets like CrowS-Pairs and StereoSet, nor does it compare the GIS metric with other gender bias evaluation metrics.\n\nThe paper's significant contributions include the introduction of the DIFAIR dataset, specifically designed to evaluate Transformer-based MLMs for gender bias. The proposed GIS metric offers a novel way to measure gender bias while ensuring that models retain essential gender knowledge. The paper's analysis provides valuable insights into the trade-off between reducing gender bias and preserving gender knowledge in models. The research addresses a crucial gap in the current understanding of model debiasing, emphasizing the importance of gender knowledge retention. Overall, the paper's contributions are seen as valuable for advancing fairness research in language models.\n\nSome weaknesses, listed below, have been discussed during the rebuttal phase.\nThe research does not evaluate the performance of recently proposed large language models, leaving a gap in understanding the dataset's applicability and relevance to state-of-the-art models. The paper's reproducibility is questionable, as there's no mention of making the code or data publicly available, which is essential for the broader research community. Additionally, the paper misses out on evaluating the performance of recent debiasing techniques."
            }
        },
        "id": "ZclnsOmWB9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "udl5f2seyU",
        "replyto": "udl5f2seyU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5469/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613601,
        "cdate": 1696707613601,
        "tmdate": 1701465560287,
        "mdate": 1701465560287,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "# Overview\n\nPrior work in psycholinguistics has shown that as language models become better (in terms of perplexity/cross-entropy) their surprisal estimate’s ability to predict reading times (RTs) improves (Goodkind et al. 2018; Wilcox et al. 2020).\nA few recent papers, however, show this trend reverses after a certain “language model quality threshold” (i.e., GPT-2 small’s surprisals have better predictive power than either GPT-2 XL or GPT-3; Oh et al. 2022; Oh and Schuller 2023; Shain et al. 2022).\nThis paper investigates when this trend-reversal occurs, by analysing how a model’s psychometric predictive power changes over training—using a number of language models with different sizes.\nThey find clear results suggesting that (under their experimental conditions) a language model’s psychometric predictive power peaks when they are exposed to roughly two billion training tokens.\n\n# Meta review\n\nThe reviewers generally agree that this paper is well-written, sound and makes an important contribution to the literature. Further, the fact that experiments are consistent across a range of datasets and RT measurements is both striking and reassuring. Personally, I believe that if these results are confirmed this could be a very impactful paper (but see my personal review below).\n\nThe reviewers also point out some issues with the paper. In particular, the paper does not try to address the question of “why two billion training tokens?”---i.e., it does not try to address the cognitive questions behind this observed trend. Further, the impact of model capacity on the observed trends seems unclear. Finally, the authors claim they did not include spillover effects in their analysis due to issues with regression model identifiability, but: (i) including spillover effects is common practice and these effects can be stronger than the main effects depending on the dataset (in e.g., Brown); (ii) I do not see why identifiability should be relevant to this particular study which focuses on delta log-likelihoods (a longer form justification could be useful). In my opinion, though, these issues are either addressable for camera ready, or not large enough to warrant rejection.\n\n# My Review\n\nFirst, I'd like to say that I agree with the positive points about the paper summarised above.\nHowever, I personally believe that this paper has a significant experimental flaw: it **does not seem to control for unigram log-probabilities** in its analyses.\n\nThis may be particularly problematic because:\n\n* It is well-known that unigram log-probabilities have a strong predictive power over reading times.\nBeyond the most recent confirmation of the effect (in Shain, 2023, which was released after EMNLP’s CFP), most prior work in this vein controlled for frequency effects (Goodkind et al. 2018; Wilcox et al. 2020, 2023; Oh et al. 2022; Shain et al. 2022).\nIn particular, a close look at Oh et al.’s (2022) results (comparing fig. 5 to 7 or fig. 6 to 8) shows that the predictability of unigram log-probability over RTs is larger than that of surprisal for these experimental conditions.\n\n* Figure 5 in Chang and Bergen (2022) shows that a language model’s distribution (and by proxy its surprisals) approximate the unigram distribution most closely after about 1000 training updates (also replicated in Meister et al. 2023).\n\nTogether, these results suggest that the effect demonstrated in this paper is that: transformers approximate unigram at roughly 1k training steps; and unigram log-probabilities predict RTs better than language model’s surprisals.\nIn conclusion, by not controlling for frequency effects this paper might be confounding these two trends.\n\nFurthermore, prior work (Goodkind et al. 2018; Wilcox et al. 2020) which showed that *improving language models improves their psychometric predictive power over reading times* has always controlled for frequency in their analysis.\nMore recent work which shows this trend stops when models get larger than GPT-2 small (Oh et al. 2022; Shain et al. 2022) also controlled for frequency.\nTo me, by not controlling for frequency, this paper might be investigating a different phenomenon.\nIn case this paper is accepted, I would recommend adding new experiments addressing this issue (or at least a disclaimer about this alternative interpretation of the results).\n\n# References\n\nShain et al. 2022. Large-Scale Evidence for Logarithmic Effects of Word Predictability on Reading Time\n\nShain. 2023. Word Frequency and Predictability Dissociate in Naturalistic Reading\n\nGoodkind et al. 2018. Predictive power of word surprisal for reading times is a linear function of language model quality\n\nWilcox et al. 2020. On the Predictive Power of Neural Language Models for Human Real-TimeComprehension Behavior \n\nWilcox et al. 2023. Testing the Predictions of Surprisal Theory in 11 Languages\n\nChang and Bergen. 2022. Word Acquisition in Neural Language Models\n\nMeister et al. 2023. A Natural Bias for Language Generation Models"
            }
        },
        "id": "IeIrJVRd5k",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "udiNCxGKLl",
        "replyto": "udiNCxGKLl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4585/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593593,
        "cdate": 1696707593593,
        "tmdate": 1701465535790,
        "mdate": 1701465535790,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes ExplainCPE, a new dataset focusing on the Chinese Pharmacist Examination. This dataset is unique in that it includes not only multiple-choice questions but also their corresponding textual explanations. Three prevalent GPT models (ChatGLM, ChatGPT, and GPT-4) are evaluated on this dataset. While reviewers appreciate the contribution of a new dataset and some preliminary evaluations, they also raise concerns regarding the quality of the dataset, the scope of the evaluations, and the paper's fit within the NLP application track. The quality of the dataset is not sufficiently validated. All reviewers agree that the creation of a new medical dataset specific to the Chinese Pharmacist Examination is a valuable contribution to the community. Questions have been raised regarding the correctness of the labels and explanations. The evaluation lacks a comparison with domain-specific LLMs, which would provide a better context for understanding the utility of the dataset. The paper should also provide more information on how the dataset was collected, which would help assess its reliability and usefulness. Considering both positive and important issues highlighted by the reviewers, this paper falls into the \"Borderline Sound\" category."
            }
        },
        "id": "kPb8yAk1mi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uckh15CSS1",
        "replyto": "uckh15CSS1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission539/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490286,
        "cdate": 1696707490286,
        "tmdate": 1701465402664,
        "mdate": 1701465402664,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new query-focused table summarization task. Unlike other tasks such as Table QA or Table-to-text, the proposed task requires both the adaptation to user queries and the necessary reasoning for summarization.\n\n* Reviewers all agree on the value of the dataset, especially commending the transparency and properness of the construction.\n* The authors also presented comprehensive experiments to contrast different SOTA methods for neighbor tasks (i.e., table QA, table summarization).\n* At the same time, however, the clarity on the proposed Refactor needs improvement. While some details are presented in the Appendix, the authors are expected to retain the main information in the body of the paper."
            }
        },
        "id": "5AGpiiJfxV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ubXaboYnzN",
        "replyto": "ubXaboYnzN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1118/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505330,
        "cdate": 1696707505330,
        "tmdate": 1701465420930,
        "mdate": 1701465420930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a recursive visual explanation method for step-by-step vision-language explanation generation. The authors have done a nice job of rebuttal. In general, all the reviewers are positive (or neural) about the paper, commenting that (1) the paper is well written, (2) experiments are sufficient, and (3) the few-shot self-training mechanism is interesting. One reviewer questioned about the novelty of the method, which the AC also agrees. Overall, the AC thinks that the merits of the paper outweigh the flaws, and is positive about the paper."
            }
        },
        "id": "Nr1kl7PAAH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uaZQ21cuzW",
        "replyto": "uaZQ21cuzW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission30/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477278,
        "cdate": 1696707477278,
        "tmdate": 1701465384774,
        "mdate": 1701465384774,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a dataset and an automatic summarization method to generate journalistic-style summaries from scientific papers. The method takes advantage of the discursive structure and metadata of the papers to generate the summaries. The method is compared with current LLM-based tools (Alpaca and ChatGPT) showing good results. The paper makes valuable contributions and is well presented."
            }
        },
        "id": "C7LJYLklVm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uZp3i8yEs4",
        "replyto": "uZp3i8yEs4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2154/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535740,
        "cdate": 1696707535740,
        "tmdate": 1701465455994,
        "mdate": 1701465455994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an Intelligent Tutoring System (ITS) framework called CLASS, trains a proof-of-concept ITS called SPOCK that aims to teach students biology, defines an evaluation protocol, and provides a qualitative evaluation of the built system with 4 experts demonstrating positive outcomes. \n\nAll reviewers agree that the paper has **a number of merits**. Specifically:\n1. *Motivation*: As reviewers 85Eg and g1rc point out, the paper makes valuable contributions to the AI in education domain and provides a new perspective on how to combine LLMs and ITS. Moreover, this work is grounded in the Learning Science principles.\n2. *Datasets construction*: Reviewers c1QD and g1rc highlight that the construction of scaffolding and conversational datasets for ITS is a valuable contribution.\n3. *Methodology*: The proposed framework and methodology is mentioned among the paper's strengths by all reviewers. Moreover, Reviewer g1rc points out that the paper provides an example use case of how to combine multiple LLMs to build an NLP-based application. \n4. *Use of domain experts for evaluation*: Finally, all reviewers also agree that qualitative evaluation using domain experts adds to the strengths of this submission.\n\nAt the same time, all reviewers have identified **several weaknesses** of this paper and areas for possible improvement, and posed a number of questions for the authors. The major weaknesses include:\n1. *Novelty*: Reviewer 85Eg points out that, while this paper makes a contribution to AI in education, it may lack in novelty when considered in the context of NLP applications.\n2. *Clarity of writing*: All reviewers have made suggestions on how the paper can be improved in terms of clarity of the points made. See their reviews and questions for more details.\n3. *Lack of datasets' quality evaluation*: This weakness is highlighted by Reviewer c1QD. In addition, Reviewer g1rc expresses further concerns regarding the evaluation approach taken by the authors. \n4. *Further limitations in the datasets construction*: As Reviewer c1QD points out, the framework dataset construction process was only performed using GPT-4 – a closed, commercial system with little transparency. As the reviewer proposes, testing the framework construction process using models beyond GPT-4 would ensure generalizability of the approach and provide a concrete, reproducible method for educators to build without using 3rd party models.\n5. *Lack of meaningful comparison and ablation studies*: Reviewer c1QD also points out that no baselines to compare SPOCK against were used in this work. Moreover, the reviewer suggests running some ablation studies.\n\nThe authors did a thorough job addressing reviewers' questions. They have also acknowledged reviewers' suggestions on how the paper can be further improved. These changes should be integrated in the revised version of the paper."
            }
        },
        "id": "Hn5HenpFIN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uUvlXyriM7",
        "replyto": "uUvlXyriM7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4423/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589383,
        "cdate": 1696707589383,
        "tmdate": 1701465531358,
        "mdate": 1701465531358,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a framework combining contrastive learning and high-level meta actions computed in different ways for embodied instruction following in the ALFRED benchmark. They show improved performance on the benchmark with their method and evaluate the ablated components of their system to assess the difference in performance. The authors amended and revised some flaws in the ablation studies pointed out by reviewers which has made and I urge them to consider other points raised as well, but overall this paper is a good contribution and would raise interesting discussions in the field."
            }
        },
        "id": "FdgbSgbFt4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uRIFDS3gtG",
        "replyto": "uRIFDS3gtG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2654/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546860,
        "cdate": 1696707546860,
        "tmdate": 1701465472321,
        "mdate": 1701465472321,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary:\n\nThe paper investigates the self-reinforcement effect in open-ended text generation, and proposes two new metrics for measuring this effect. They also analyze the effectiveness of the repetition penalty to mitigate this effect, and propose a simple forgetting mechanism to improve the penalty selection process. \n\nStrengths:\n\n 1. Proposes two new metrics for measuring self-reinforcement effect in text generation.\n 2. Propose a simple decoding strategy to mitigate the self-reinforcement effect to some extent. \n 3. Experiments showing potential effectiveness of the proposed decoding approach. \n\nWeaknesses: \n\n1. Writing and organization of the paper could be improved for clarity, based on reviewers questions and concerns. \n2. Additional in-depth analysis could help support its claims regarding the proposed forgetting mechanism."
            }
        },
        "id": "OFDaHO0o22",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uQfyuhhHBq",
        "replyto": "uQfyuhhHBq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2527/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543986,
        "cdate": 1696707543986,
        "tmdate": 1701465467913,
        "mdate": 1701465467913,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates the automatic generation of radiology reports from X-ray images (MIMIC-CXR). The authors' approach centers on optimizing the visual encoder based on anomaly semantics, deliberately excluding normal semantics to mitigate the impact of potential noisy signals.\n\nAll reviewers acknowledge the effectiveness of the proposed method in the radiology report generation domain, noting improvements relative to current state-of-the-art benchmarks. Comprehensive ablations and analyses are also presented. Some concern is expressed regarding how to determine abnormal vs. normal semantics (a naive heuristic is used which can be noisy itself)."
            }
        },
        "id": "3qHbLvOieX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uPz5a2NvrG",
        "replyto": "uPz5a2NvrG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission53/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477909,
        "cdate": 1696707477909,
        "tmdate": 1701465385479,
        "mdate": 1701465385479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Overall, The reviewers are very positive about the soundness and excitement of this work. Reviewers highlight the strong experimental results in particular regarding adversarial attacks, the good presentation, and the extensive experimental setting. As such, I recommend acceptance the main conference."
            }
        },
        "id": "dY0HTcIcoq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uEAFmlWYig",
        "replyto": "uEAFmlWYig",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4702/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707596505,
        "cdate": 1696707596505,
        "tmdate": 1701465538836,
        "mdate": 1701465538836,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Quality:\nPros with regards to quality are noted by two of the reviewers as follows:\n-\t“This paper adds depth to the discussion on NLI by thoroughly considering the subtask of medical contradiction detection. It provides SotA results and has exemplary writing, meaningful comparison with established literature, and excellent evaluation.”\n-\t“The paper is well-explained and evidence is provided for most assumptions and modeling decisions.”\n\nCons with regards to quality are noted by one reviewer as follows:\n-\t“Many of the proposed heuristics are crude (…),which cast doubt on the promising results in Table 3 with regard to the validity of the sentence-based evaluation. Determining contradictions in medical research is very context-sensitive, and even minor misalignment of the settings can make a detected pair useless. A more realistic test scenario is probably what is reported in section 5.3.3, with a precision of 0.375 (9/24).”\nThe authors rebut this point by stating “Our research falls under the umbrella of distant supervision where noisier sources of supervision are used to create larger training sets quickly. Although they are noisy, when coupled during fine-tuning the results are promising, as seen in Table 3.”\nThe reviewer further notes the mention of WordNet in the rebuttal but not in the paper, which exacerbates concerns around reproducibility. The authors note that the full experiments are fully reproducible via the shared code, i.e. “If you navigate to our code repo (bottom of Page 2), in the file \"snomed/snomedct_arr.ipynb\", in the function \"use_synonym_heuristic\", you will see the use of WordNet. All details can be found and reproduced via the code.”\nI feel that the point about reproducibility has been well addressed by having the code shared. The point about the crude heuristics is a more challenging one, however, the authors do a good job in pointing this out as a limitation of weak supervision and highlight the added value despite the presence of minor misalignments.\n\nClarity:\nThe reviewers disagree with regards to the clarity of the paper. Reviewer 1 states that the paper “has exemplary writing, meaningful comparison with established literature, and excellent evaluation”, while Reviewer 3 notes that “There are many scattered heuristics throughout the methods that also require referring back and forth with the appendix, making it very hard for the reader to track which is which in the end. English needs editing.”\n\nOriginality and Significance:\n-\t“The paper presents an innovative and relatively inexpensive method of data augmentation for detection of clinical contradiction by leveraging the existing SNOMED medical database.”\n-\t“a well-chosen, meaningful challenge in terms of matching what the NLI technical framework could be conveniently adapted to tackle in the medical domain.”\n\nOverall, I believe that the points raised by reviewer 3 have been meaningfully addressed by the authors, with reproducibility further guaranteed through the shared code."
            }
        },
        "id": "NNcpWlFhsw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uDMyRJw6ty",
        "replyto": "uDMyRJw6ty",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1074/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504433,
        "cdate": 1696707504433,
        "tmdate": 1701465419626,
        "mdate": 1701465419626,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a method, Vera, that estimates the plausibility of any given commonsense statement.  Vera is built by finetuning T5 on ~6M statements from two commonsense KBs and 19 commonsense QA datasets. The paper is well-written, and the author compares their method with several LLMs. VERA outputs a real-valued score in [0,1]. The idea of using Vera to detect errors made by ChatGPT is interesting. \n\nPros: \n\n- The idea of automatically estimating the plausibility of a natural language statement with respect to commonsense knowledge is important and will be useful for the community. \n- As agreed by all reviewers, the paper has promising results, especially Vera outperforming ChatGPT and GPT-4 in estimating plausibility. \n- Vera is able to detect erroneous commonsense statements from generative LMs (ChatGPT).  \n\nCons: \n- The authors show that Vera as a filter or critic can be useful for reinforcement-based QA models (Table 3). However, the Area Chair thinks there can be a knowledge leakage since Vera is finetuned on these datasets. It will be more useful if the authors show it on the Unseen dataset. \n- The authors do not explain why a real-valued score is used to estimate the plausibility. For example, it is difficult to understand the difference between 0.6 and 0.65 plausibility scores.  It would be interesting to see how the plausibility score relates to human understanding of commonsense plausibility. \n- The error (quantitative) analysis is missing. An analysis with respect to different common sense knowledge dimensions (physical, social, etc.) would make the paper stronger. \n\nThere are a few useful results and analyses in the Appendix of the paper. Especially the ablation study should be included in the paper. \n\nFinally, The AreaChair appreciate the authors' response to clarify the doubts of the reviewers."
            }
        },
        "id": "DqeTBD2o1r",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uBnIvIcAFx",
        "replyto": "uBnIvIcAFx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission222/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482194,
        "cdate": 1696707482194,
        "tmdate": 1701465391712,
        "mdate": 1701465391712,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes text-transport, a method to transform a causal estimate from a source to a target dataset. With this method, one can obtain causal estimates in related domains without the additional overhead of collecting new data. Two methods are proposed: one based on a binary classifier to distinguish between source and target domains and one based on prompting an LLM. The authors empirically show their framework's validity on three datasets and provide additional insights for the use case of hate speech detection. Comprehensive experiments and a thorough analysis of results support the paper's claims. The paper is well-written and exciting to read. \n\nThe reviewers have raised concerns about comparing with baseline methods and the rationale behind the proposed method. These concerns have been addressed by the authors in their rebuttals, which need to be incorporated into the final version of the paper."
            }
        },
        "id": "5lR5nKxWAy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "uB9ZnBCBX6",
        "replyto": "uB9ZnBCBX6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2442/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542284,
        "cdate": 1696707542284,
        "tmdate": 1701465465244,
        "mdate": 1701465465244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper is an empirical study of generative retrieval techniques, carried out at a scale that is beyond what has been previously studied. The overall finding is that generative techniques are effective at smaller scales, but more research is needed for scaling to much larger settings.\n\nReviewers found this paper to be well-written, a useful empirical study, and likely of value to parts of the community (\"could be valuable for the researchers in the field\"). \n\nThe main concerns among reviewers had to do with the diversity of results, as the primary focus was on a single metric on a single large scale dataset. In their rebuttal, the authors have helpfully provided additional results which include additional metrics and additional datasets, and this seems to have allayed the first concern at least. Nevertheless, two reviewers remained ambivalent in terms of excitement, even after taking the rebuttal into consideration. Unfortunately, one of these reviewer was somewhat confused about whether new experimental results could be considered as part of the rebuttal. The policy this year is a departure from past years, so it is understandable that they thought such revisions might be off limits, when in fact new results were within the scope for evaluation.\n\nAll reviewers agreed that this paper is worthy of publication. However, the one reviewer who was by far the most confident, who also engaged with the authors during the rebuttal, remained unconvinced to increase their excitement score above ambivalent. On the one hand, they suggest that \"this paper will be an excellent and comprehensive empirical study that will benefit the research field of generative retrieval\". On the other, they remain concerned that the paper is limited to generative retrieval based on only virtual tokens, and as such, \"the impact of this paper is still limited\", and they have not been convinced to raise their scores above 3.\n\nUltimately this paper is somewhat difficult to place. There seems to be consensus that this is a paper that will be interesting and useful to those who work on generative retrieval, despite being primarily an empirical study. That being said, the authors were unable to convince two reviewers to increase their scores, despite providing new results that seem to be precisely what these reviewers were seeking."
            }
        },
        "id": "rFCOrEUhd4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "u9gI4JlOSj",
        "replyto": "u9gI4JlOSj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2056/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533271,
        "cdate": 1696707533271,
        "tmdate": 1701465452106,
        "mdate": 1701465452106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores increasing the language coverage in multi-lingual seq2seq models. The authors propose a modular approach to handle this issue. \n\nOverall, the paper is sound: “The performance of the proposed approaches outperform its most similar precursor mT5” , and “The paper is well written”.  However, the paper could not inspire excitement. All reviewers agree that the paper is not presenting a strong contribution to the research community and mostly incremental. That said, the presented research is worth to be known by the community, as it could be useful for similar experiments."
            }
        },
        "id": "hmju9Onrlb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "u9Fvsy8Brx",
        "replyto": "u9Fvsy8Brx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission706/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494179,
        "cdate": 1696707494179,
        "tmdate": 1701465407941,
        "mdate": 1701465407941,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Paper uses RLHF techniques fine-tune LLaMA-13B to generate implicit toxic content (\"toxic while not using explicit toxic words\"). \"Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs.\" Next they train a RoBERTA classifier on a new human-labeled dataset (to be released) of such data and show it can detect such data with much better performance.\n\nOverall shows an interesting way to get around existing toxicity classifiers and is an important practical topic to highlight in the LLM space."
            }
        },
        "id": "p1N3ZuB3AI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "u69aCtohTC",
        "replyto": "u69aCtohTC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4922/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601725,
        "cdate": 1696707601725,
        "tmdate": 1701465544913,
        "mdate": 1701465544913,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Based on the reviews and my perusal of the manuscript, the work clearly states its goal---manually curating a benchmark dataset for visual commonsense QA based on ImageNet. The authors have also prepared quite a thorough comparison with existing ViComTe dataset in terms of *diversity*, *difficulty*, and *factuality* (correctness). They also evaluate LLMs and VaLMs (Vision-augmented LMs) on the curated ImageNetVC dataset under zero- and few-shot settings. The authors did not perform evaluation on instruction-tuned VLMs, as pointed out by  eD4G. They subsequently provided the results in the rebuttal. Overall, ImageNetVC appears more difficult, factual, and diverse. They also provide many interesting observations and analyses w.r.t. various aspects of the image, the effect of in-context learning, emergence of visual commonsense with model scale, model calibration for ICL, lack of prompt bias of ImageNetVC samples over ViComTe, etc. \n\n# Pros\n1. Overall, the authors seem to have tried their best to leave few questions unanswered in the manuscript.\n2. The overall goal and the aim and conclusions of the experiments seem mostly clear to the reviewers, especially after the rebuttal.\n3. This dataset could be significant in evaluating fundamental visual knowledge of LLM/VLMs, if not for more complex visual commonsense reasoning. According to the presented empirical human and automatic evaluation, ImageNetVC seems to be more challenging, yet factual and diverse dataset.\n\n# Cons (The authors may want to consider some of these points in the future iterations)\n1. The end product, the methods, and the evaluation are not particularly original. There is already ViComTe dataset with similar purpose. The zero- and few-shot evaluations of LLM and VLMs across various aspects is quite standard in NLP at this point. As pointed out by eD4G, the current manuscript claim this as novelty.\n2. It is unclear how A/B human evaluation was performed in fig. 4 (by category?). How do you get the notion of parallel instances across the datasets? Also, diversity is a global aspect of the dataset, unlike factuality and difficulty that can be evaluated per QA pair. More details are required here.\n3. The authors claim that PLMs may emerge to possess zero- and few-shot visual commonsense from 1.3B size onwards. However, such claim is made based on a sample size of only two. The authors also do not mention why it could be. Do they share any part of the pre-training dataset? That could lead to them having similar performance curves for visual commonsense. As far as I know, both at least share PILE as the pre-training dataset. More analysis is required.\n4. More qualitative examples should be added as mentioned by reviewer xCkP.\n5. The rationale and difference between the category- and instance-level filtering is not quite clear, even after rebuttal. Are you not looking at the all instances during category-level cross-checking? If so, why the instance-level cross-check once again? Can you elaborate on the distribution bias? Couldn't you have just done sample-level cross-check, followed by more annotation for categories with significantly fewer samples?"
            }
        },
        "id": "9z9zy4goLq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "u14dVx4rMW",
        "replyto": "u14dVx4rMW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1379/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511608,
        "cdate": 1696707511608,
        "tmdate": 1701465429110,
        "mdate": 1701465429110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper evaluates the zero-shot performance of ChatGPT on 20 NLP datasets. It seeks to understand the strengths and limitations of ChatGPT on different tasks, and compares its performance to GPT-3.5 as well as existing LLMs.\n\nReasons To Accept:\n- The paper covers a variety of NLP tasks and convincingly demonstrates that ChatGPT's current zero-shot performance is outperformed by existing few-shot/fine-tuned approaches.\n- The authors explore the efficacy of zero-shot Chain-of-Thought Prompting and show it can improve or degrade performance depending on the task\n- Comparison of performance: The paper compares ChatGPT's performance on tasks that require reasoning capabilities versus tasks that require sequence tagging, which provides insights into the types of tasks that are more challenging for ChatGPT.\n- Implications for future research: The paper discusses the implications of the findings for the development of more effective and efficient NLP models, which is valuable information for researchers working in this area.\n\n\nReasons To Reject:\n- Potential lack of novelty: The paper's Related Work section does not mention existing work (preprints) that also evaluate ChatGPT on NLP tasks, including one paper where there is overlap in datasets studied.\n- Limited scope: The paper excludes larger-scale datasets and more task categories due to the cost of ChatGPT, which might prevent further insights.\n- Limited generalizability: The paper only evaluates ChatGPT's zero-shot learning ability and does not consider its performance on tasks that require fine-tuning or transfer learning.\n- Lack of novelty: While the paper provides a comprehensive evaluation of ChatGPT's zero-shot learning ability, it does not introduce any new models or techniques.\n- Limited comparison: The paper only compares ChatGPT's performance with existing LLMs and does not compare it with other state-of-the-art models."
            }
        },
        "id": "yfYjv5jD9w",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "u03xn1COsO",
        "replyto": "u03xn1COsO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5530/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614629,
        "cdate": 1696707614629,
        "tmdate": 1701465561770,
        "mdate": 1701465561770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this work the authors extensively analyze the effect of sentence/document length in contrastive training of sentence/document encoders. They show that contrastive training, recently ubiqitous in training sentence encoders, introduces a substantial length bias. The authors propose to reduce the length bias of the resulting embedding space by means of training data augmentation, i.e., synthetic elongation of training sentences. The effectiveness of the proposed approach is evaluated on the BEIR benchmark, which encompasses 14 different IR tasks with \"documents\" of varying lengths (sentences, tweets, news stories, ...).  \n\nThe reviewers are generally appreciative of the analysis of length biases in representations of encoders trained with contrastive objectives and the proposed augmentation-based remedy. Concerns raised w.r.t. shown positional invariance (i.e., that length matters more than the position of the content in the text) and downstream NLU evaluation have been, in my opinion, convincingly addressed by the authors, both by means of reporting additional experiments and acceptable argumentation (e.g., that one would fine-tune vanilla LMs rather than sentence/text encoders for supervised NLU tasks). Some concerns, however, have not been fully addressed: question of direct comparison against the approach of Xiao et al. (ICLR 23), and that of performance on STS (unsupervised, similarity-based). \n\nThe work of Xiao et al in principle challenges the novelty of the contributions of this work -- as they also demonstrate that contrastive learning leads to overfitting to lengths observed in training data. I believe, however, that this work can be seen as concurrent/contemporary: it would still require that the authors compare against this work in the revised version of this work. The argument that authors offer against STS is that of limited length variance in the dataset, but I did not find this to be particularly convincing. STS results seem to show that LA(SER)^3, i.e., the augmentation strategy, seems to hurt the performance of short sentences. \n\nOverall, however, the contributions of this work outweigh the remaining concerns (which, however, I still believe should be remedied in the revised version of the paper)"
            }
        },
        "id": "RO08V6ZFfv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tueh30tKiv",
        "replyto": "tueh30tKiv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4865/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599820,
        "cdate": 1696707599820,
        "tmdate": 1701465543636,
        "mdate": 1701465543636,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper present a multilingual corpus for Name Entity Recognition tasks comprising 12 languages which allow for  a diverse typology and writing systems, and range from well-resourced to low-resourced ones. \nA fine grained taxonomy is defined leading to 33 entity types. \nThe objective of this corpus is to evaluate the robustness of NER system (LLM) given the different languages but also under noisy scenario. Word level and character level corruption strategies have been implemented on the test data. \nIt will be good to clarify the link between this paper and the presented to semeval."
            }
        },
        "id": "5Kb8ccOKig",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tquKyw04gE",
        "replyto": "tquKyw04gE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2639/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546414,
        "cdate": 1696707546414,
        "tmdate": 1701465471705,
        "mdate": 1701465471705,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents an interestingly clever approach for generating new knowledge connected to existing one. This is used to create a new benchmark to assess LLMs' abilities in reasoning about this knowledge and show that existing LLMs still struggle with the task by running experiments of 4 well known LLMs. Overall, all reviewers found the paper convincing and well written. Discussion was lively and constructive, and authors proved knowledgeable and willing to take into account reviewers’ suggestions for further improving their paper. The paper seems mature and timely interesting for the community. \n\n**Pros.**  \n\n-  the paper presents a smart method to generate (artificial) new knowledge, which could be particularly useful for evaluating LLMs \n\n- it tackle an important problem, i.e. reasoning about new knowledge; \n\n- it shows experimentally that the task is challenging also for top LLMs; \n\n- it proposes a new framing of a known task;  \n\n- it presents interesting error analyses.     \n\n**Cons.** \n\n- the paper does not clearly report data about comparability of performances against existing knowledge and across models. However, details were convincingly provided during the discussion. \n\n- some related works seems not accounted for (see reviews)"
            }
        },
        "id": "yEY17NPTCC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "toUPGCAMic",
        "replyto": "toUPGCAMic",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4944/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602050,
        "cdate": 1696707602050,
        "tmdate": 1701465545410,
        "mdate": 1701465545410,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes the task of location based visual question generation. They use GPT-4 to produce diverse engaging questions. And then share studies on both the data generation as well as solving the problem.\n\nThe reviewers find the paper exciting and sound after the rebuttal. As one reviewer indicates, there are flaws but it's nonetheless interesting. The authors should include several points raised by the reviewers so as to improve the paper: clarifications on the prompting and filtering, metrics and eval changes suggested by reviewers, and question classifier details to name a few."
            }
        },
        "id": "CbnFLnEBIu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tm5UxNFrlD",
        "replyto": "tm5UxNFrlD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2933/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552427,
        "cdate": 1696707552427,
        "tmdate": 1701465481755,
        "mdate": 1701465481755,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper aims to improve MRC-style NER by introducing a query-parallel design, which can extract multiple entity types at the same time and works well for both resource-rich and resource-limited settings. Comprehensive evaluation on multiple datasets under different settings demonstrates the effectiveness of the proposed method.\n\nStrengths:\n- This is a solid improvement for MRC-style NER\n- Comprehensive evaluation that demonstrates the effectiveness of the method especially under resource-limited settings\n\nWeaknesses:\n- As reviewer HR2U pointed out, there are many minor writing issues in the current paper that collectively makes it harder to read and follow\n- There are concerns over reproducibility, especially for a complex method like this. There is no indication of code release, either in the paper or in the rebuttal when the reviewer pointed this out as a weakness."
            }
        },
        "id": "9W6uHS1G1W",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tkY0l8mHii",
        "replyto": "tkY0l8mHii",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3119/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556511,
        "cdate": 1696707556511,
        "tmdate": 1701465487702,
        "mdate": 1701465487702,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a joint extraction method of entities and relation triples by combining set prediction networks (SPNs) and a new loss function that promotes global consistency. The effectiveness of the work is demonstrated on three biomedical datasets, where SOTA performance was achieved; however, their method didn't outperform baselines on ACE05. Through their experiments, the paper argues that this is because ACE05 doesn't have many relations per sentence while their method is effective when more relations are involved in each sentence. As pointed out by one of our reviewers, some commonly used datasets, such as ACE2004 and SciERC, were not included in the paper; many recent papers now use the Rel+ metric, which is also not presented in this paper.\n\nThe soundness of the work is well-perceived by all of the reviewers but reviewers split in terms of excitement. The general idea of joint extraction of entities and relations has been explored in the field for a long time, in training, in inference, or in both training and inference, so from that angle, the idea is not exciting enough. The novelty mainly resides only in the choice of SPN and the newly defined consistency loss function. The lack of analysis on other commonly used datasets and metrics is another limitation of the work."
            }
        },
        "id": "I5A7umZWL7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tfHJ9uLNlR",
        "replyto": "tfHJ9uLNlR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission497/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489252,
        "cdate": 1696707489252,
        "tmdate": 1701465401248,
        "mdate": 1701465401248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers found the dataset to be interesting, innovative, and valuable for future work. Reviewers generally found the experimentation to be thorough, although one reviewer pointed out that a few models were missing fine-tuning results. Two reviewers pointed out a missing citation to an ACL 2023 paper introducing a related (but sufficiently distinct) dataset, which the authors promised to add in the camera ready, addressing these concerns. One reviewer still had remaining concerns about possible data quality, given that the dataset was collected by crowdsourcing and a small number of data points were used in evaluation. However, in my opinion the manual checking of data quality by the authors and the human evaluations are likely sufficient to ensure that the dataset will be useful for future work."
            }
        },
        "id": "0JGyf0N0Li",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "te3pXuiVk3",
        "replyto": "te3pXuiVk3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission13/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476832,
        "cdate": 1696707476832,
        "tmdate": 1701465384000,
        "mdate": 1701465384000,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "While the paper presents promising results in terms of evaluation of engagingness, addressing concerns related to theoretical depth, prompt refinement, diverse experiments with additional models, translation quality in Chinese,  and novelty would strengthen its overall contribution to the field."
            }
        },
        "id": "HmbaaZ5MsM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tbRPPWDy76",
        "replyto": "tbRPPWDy76",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1182/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506810,
        "cdate": 1696707506810,
        "tmdate": 1701465422976,
        "mdate": 1701465422976,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "A new LLM is trained on a diverse set of financial datasets and obtains improved performance on target financial related NLP tasks. Reviewers overall appreciated the improved empirical results on financial datasets and the release of  a new larger LLM on this domain. However, there was less excitement due to the very focused scope (only financial domain) and the lack of novelty in the method or conclusions (helpfulness of diversity is already known).\n\nIn terms of soundess, overall most reviewers found the evaluation solid. However, it can definitely be improved. For instance, figure 1 is unclear (sides a and b look identical only with/without colors?... and overall this 2D visualization is more suitable as an interpretability tool but is not a proof to motivate a method on top). Also, reporting confidence intervals would be better (e.g. with bootsrapping). Reporting out of domain results would be helpful. Table 2 is not explained (I assume underline is for second best?)"
            }
        },
        "id": "gA28OEpFiB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tbHe97ENFD",
        "replyto": "tbHe97ENFD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2754/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548677,
        "cdate": 1696707548677,
        "tmdate": 1701465475118,
        "mdate": 1701465475118,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**quality, clarity, originality**\n\nThe work, as further evidenced by the deep discussion by both reviewers and authors, is of great quality and originality. The paper, further than just detecting LLM generated or not, works to identify which LLM generated the text. There are limitations that are highlighted through the reviews that the detection tool requires access to the full model, limiting its application as most commercial models are closed. The question on the perplexity calculation was addressed by authors and further provided updated tables (that should be put in the paper itself) as this provides for more clarity and also shared understanding. \n\nThe work would benefit for another round of proof reading as highlighted by a number of reviewers just to fix lagging issues. \n\n**Significance**\n\nThe work provides insight not just on detection but also on the LLMs themselves. This I think is significant for the field and the audience of this conference. \n\n**Meta review suggestions**\n\nIt would be a good idea for the authors to make further notes on how to get over the limitation of not having access to closed source models. For people who might want to build on your work, where should they look to be able to identify steps that could be taken with black box models? There is a large amount of literature on interpretability (even of black box models), could this be useful?"
            }
        },
        "id": "RozPAdIvIo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tauoKi9IWO",
        "replyto": "tauoKi9IWO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5411/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612507,
        "cdate": 1696707612507,
        "tmdate": 1701465558713,
        "mdate": 1701465558713,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the value of an intermediate model in an inter-training setting, which corresponds to a certain style of transfer learning, where the pretrained language model is being trained on different tasks before the task of interest. Experimental results on a wide variety of benchmarks show the importance of the size of target and source datasets, as well as the value of the base model. The paper ranks different models for tasks by the average performance gain under this inter-training setting.\n\nOverall, the reviewers seem compelled by the detailed empiricism of the work and present suggestions for future research such as extensions to other domains or replication in a scaled up setting. Some issues pointed out regarding interpretability of MSE scores and aggregation of scores seem to be addressed by the authors’ response.\n\nWe hope the authors include the suggested changes and clarifications in the next iteration of the paper."
            }
        },
        "id": "B23aiWFxJy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "taXJRZs43y",
        "replyto": "taXJRZs43y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission516/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489667,
        "cdate": 1696707489667,
        "tmdate": 1701465401782,
        "mdate": 1701465401782,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a method called UL2R, which aims to improve the performance of large language models, specifically PaLM, on various downstream NLP tasks. The technique involves continuing the training of a pre-trained language model using a small amount of additional steps with the UL2 training objective. The method claims to achieve significant improvements in performance, scaling, and emergent abilities while providing new prompting capabilities.\n\nStrengths:\n* The paper is well-written, with thorough experiments and convincing results.\n* UL2R significantly improves PaLM's performance on downstream NLP tasks and scaling, with minimal extra compute.\n* The technique enables emergent abilities at smaller scales and provides new prompting capabilities, such as bidirectional infilling.\n* The approach offers practical applicability and potential for further research in increasing training efficiency with modified training objectives.\n\nWeaknesses:\n* The paper's main limitation is the lack of novelty, as it continues a line of work on large language models and mixture of denoisers.\n* The method is only evaluated on a single model (PaLM), which raises questions about the generalizability of the gains.\n* Some reviewers found the presentation of innovation points to be vague and repetitive, making it harder to understand the exact contributions.\n\nOverall, this paper is considered strong and potentially transformative, offering valuable insights and practical benefits in improving the performance of large language models. However, further evaluation on other models and a clearer presentation of innovation points would strengthen the claims and generalizability of the method."
            }
        },
        "id": "GuZIEzZJpG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tZXaHWfsXB",
        "replyto": "tZXaHWfsXB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4436/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589473,
        "cdate": 1696707589473,
        "tmdate": 1701465531494,
        "mdate": 1701465531494,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a method for generating radiology reports by incorporating both the current radiology image and the patient's image from a previous visit. The proposed technique employs a two-stage training approach: initially, the model predicts disease observations and progressions, which are utilized to construct a disease progression graph. Subsequently, a decoder is trained in the second stage to formulate the report based on the data obtained from the first stage.\n\nReviewers largely concur that the structural modeling of spatial and temporal progression aspects in radiology images is both innovative and intuitive. Robust experiments demonstrate a marked improvement over existing baselines, and in-depth ablation studies have been conducted. Nonetheless, there are reservations concerning the complexity of the proposed method and its potential adaptability to datasets other than MIMIC-CXR."
            }
        },
        "id": "kUuh9ao7Vb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tSfZo6nSN1",
        "replyto": "tSfZo6nSN1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3105/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556221,
        "cdate": 1696707556221,
        "tmdate": 1701465487148,
        "mdate": 1701465487148,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper improves the performance of related work generation by introducing casual intervention that addresses specific confounds (e.g. sentence order and transitional languages) to transformer-based pretrained models. Experiments demonstrate that it works well compared to state-of-the-art models. Most reviewers thought the proposed approach is novel and interesting, and that the experiments are extensive in that it covers two datasets and there are various tests and analyses to measure the utility of each component. That said, there are some concerns that reviewers have identified:  (1) from the discussions it appears that the proposed casual intervention technique mostly benefits BERT-based models and not other pretrained models - while the authors had a reasonable speculation (owing to the differences in their pretraining objectives), this is a significant limitation that needs to be properly discussed in the paper; (2) reviewers would appreciate if the human evaluation is done more thoroughly; (3) the performance gain appears to be less than what it claims - authors should probably tone this down to avoid misleading the readers; and (4) reviewers thought certain parts of the paper are unclear or difficult to follow (e.g. key concepts and the baseline transformer encoder-decoder model needs more clarification)."
            }
        },
        "id": "dX00XSUXgb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tRYqTsaSyZ",
        "replyto": "tRYqTsaSyZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1204/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507327,
        "cdate": 1696707507327,
        "tmdate": 1701465423577,
        "mdate": 1701465423577,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper \"G-SPEED: General Text Editing with Sparse BERT and Mixture-of-Experts\" presents a lightweight text editing model called G-SPEED, which is composed of a tagging model and a generation model. The paper's main contributions include the proposal of G-SPEED, the unsupervised data clustering method, and the mixture-of-experts strategy for the tagging and generation models. The strengths of the paper include the well-written and easy-to-follow presentation of the proposed methods, the usefulness of the clustering method and the resulting editing data for advancing the field of text editing, and the impressive performance of the lightweight G-SPEED model. The paper also provides experimental results that demonstrate the effectiveness of G-SPEED, achieving state-of-the-art performance on some editing tasks. However, the reviewers also raise a number of concerns or suggestions to improve this paper. Overall, the technical novelty of the proposed techniques can be discussed further and enhanced. The paper does not provide a comprehensive analysis of the differences with prior work in the Editing Model subsection in Related Work. Secondly, the experimental setting and results analysis can be strengthened. Thirdly, the paper does not provide a comprehensive discussion of the pros and cons of LLM-based generative editing approaches, such as ChatGPT, compared to G-SPEED. Although G-SPEED is lightweight and powerful, it has limitations in generalizing to unseen editing tasks, whereas LLM-based approaches do not have such limitations. Finally, the paper should discuss the potential contamination between the data collected in an unsupervised manner and the test sets of the evaluation tasks, which raises concerns about the validity of the results."
            }
        },
        "id": "n4Jm7yF2uk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tQOncmMEVO",
        "replyto": "tQOncmMEVO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5215/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609246,
        "cdate": 1696707609246,
        "tmdate": 1701465553236,
        "mdate": 1701465553236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses an important and challenging research problem, conducts sound experiments, and introduces a novel approach leveraging in-context learning. However, the sensitivity to input prompts and potential lack of technical novelty should be addressed or clarified. Additionally, including a comparison with relevant baselines in the evaluation would enhance the paper's credibility and relevance."
            }
        },
        "id": "mp5fyQSHXu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tPJDg5G9SR",
        "replyto": "tPJDg5G9SR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3351/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561049,
        "cdate": 1696707561049,
        "tmdate": 1701465494852,
        "mdate": 1701465494852,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores a method for aspect term extraction that leverages ChatGPT to generates higher quality data to facilitate the predictions. The reviewers have several concerns for the paper regarding marginal performance improvement and additional experiments for the contribution of different steps in the pipeline. The rebuttal has provided further experiment results and clarified some details. The authors are encouraged to update the paper according to the rebuttal and discussion with the reviewers."
            }
        },
        "id": "SE2vzJv0ZJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tNN3ToWzCM",
        "replyto": "tNN3ToWzCM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1795/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527362,
        "cdate": 1696707527362,
        "tmdate": 1701465441833,
        "mdate": 1701465441833,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a framework for annotating data using both humans and LLMs. The core idea is to use the – cheaper – LLM to generate an annotation for examples where it has high certainty, and to use a human annotator on examples where the LLM has low certainty. \n\nThe paper discusses several different approaches to measuring uncertainty, and carry out experiments with each. Testing on three different tasks, the authors demonstrate that their approaches result in much stronger model performance after training on the produced data, when compared to a random allocation of examples between humans and LLMs. \n\nBeyond implications for dataset construction, this paper is also interesting purely for the proposals on how to assess LLM uncertainty."
            }
        },
        "id": "NBgW91tyah",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tL7hS11keH",
        "replyto": "tL7hS11keH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4148/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584181,
        "cdate": 1696707584181,
        "tmdate": 1701465522073,
        "mdate": 1701465522073,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper considers the task of of multi-defendant legal judgement prediction (LJP) task. This is an under-explored area in LJP literature. The authors introduce a new dataset resource and modelling approach. The dataset introduced in this paper contains a substantial number of instances based on real-world cases in China.  The modelling approach and baseline applies fusion in decoder architecture seq2seq model for an interesting hierarchical approach for extracting facts and relations from the case data.\n\nThere is reasonable agreement between the reviewers about the strength and excitement of this work and most questions seem to be well addressed by the authors. However, the reasons to accept listed by the reviewers seem to be fairly surface level. I think there are merits to the work, but the contributions are mostly collecting open data and applying a model architecture in an interesting way. I think the work has the potential to be used by other researchers studying LJP."
            }
        },
        "id": "NUCw5gQlUV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tJt1v8eugw",
        "replyto": "tJt1v8eugw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission496/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489229,
        "cdate": 1696707489229,
        "tmdate": 1701465401215,
        "mdate": 1701465401215,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors introduce a new dataset for interpreting indirect yes/no questions. A training set was constructed using distant supervision based on rules covering in 8 languages, and small test sets of human-annotated examples were created for 3 languages. They also run experiments to provide some baseline results using the data.\n\nThe reviewers are in agreement that the data collection seems reasonable and useful for the task, but they did not find it overall exciting beyond that."
            }
        },
        "id": "5Oi9CwIVaD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tEN5ONyUre",
        "replyto": "tEN5ONyUre",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4418/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589242,
        "cdate": 1696707589242,
        "tmdate": 1701465531028,
        "mdate": 1701465531028,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a method of token filtering for reducing latency when using the Fusion-in-Decoder model for retrieval-based tasks (e.g. open domain QA). All reviewers see the problem as well-motivated and the method as useful, with good experimental justification and analysis. R2 notes that the method may be somewhat incremental but is mostly favorable towards it. R3 is mainly concerned with the presentation, and thinks details in the paper could be better clarified. Additionally, they think more baselines could be added. The authors have responded to this by more formally defining the method proposed in the paper and clarifying various aspects e.g. how the extensive number of hyperparameters is selected, how token filtering is performed, etc.. The overall perception is that the paper is sound."
            }
        },
        "id": "fuWf1JRwAr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tCGyM6CpRI",
        "replyto": "tCGyM6CpRI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3740/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707571074,
        "cdate": 1696707571074,
        "tmdate": 1701465508127,
        "mdate": 1701465508127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes an approach for few-shot cross-domain NER. It mines templates for prompt generation depending on entity types. The type-dependent prompts are shown to improve the effectiveness over several baselines, in particular, FactMix.\n\nThe reviewers have found the work interesting. There is not extensive work on few-shot cross-domain NER. This work enriches the literature on this topic.\nThe reviewers have questioned about the comparison with few-shot NER and cross-domain NER. The authors replied that the comparison is irrelevant. Nevertheless, it would be interesting to compare the proposed method with the existing few-shot methods by considering cross-domain as a single domain. This would show how an in-domain approach can or cannot be generalized to cross-domain scenario.\n\nThe proposed prompt mining approach can also be compared to template mining in traditional NER studies based on rules. There is a large body of research work on template mining, which the paper does not refer to. It would also be possible to compare with such traditional approaches in the experiments."
            }
        },
        "id": "qV9vm5qleZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tCEtFcrq8n",
        "replyto": "tCEtFcrq8n",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2076/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533804,
        "cdate": 1696707533804,
        "tmdate": 1701465453030,
        "mdate": 1701465453030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper addresses the task of code retrieval, specifically targeting the problem of distinguishing functionally correct code from incorrect code. While previous work used lexical changes in this context, this paper proposes an approach called CIRL that introduces iterative perturbations into subtrees within ASTs that alter the semantics and structure. The authors use an RL-based approach to force a model to learn from the hard negative examples created using such iterative perturbations. Experimental results on three Python code datasets demonstrate the effectiveness of code interventions in code search and show improvements in MRR over multiple code intervention baselines.\n\nReviewers consider this paper **overall technically sound** and identify the following **strengths** in this work:\n1. *Approach*: Reviewer fqev highlights that the approach provides an intuitive and well-motivated improvement over previous approaches to the problem, and Reviewer Xw4P supports this view by adding that this approach could have implications for a wide variety of code-related tasks.\n2. *Novelty*: In addition, reviewers Xw4P and 4Eqj highlight that the proposed idea and method are novel.\n3. *Evaluation*: Reviewer fqev points out that the proposed approach is evaluated on multiple benchmarks, with further breakdown by code difficulty. In addition, Reviewer Xw4P notes that there are interesting code intervention baselines proposed in this work, which may be useful for the research community. Finally, Reviewer 4Eqj points out that authors conduct experiments based on recent popular LLMs, which could also be useful in future work.\n4. *Level of detail*: Reviewer fqev mentions that extensive implementation details are provided in the Appendix.\n\nAt the same time, all reviewers have identified **a number of weaknesses** of the current submission and posed some clarifying questions for the authors. Specifically:\n1. *Limitations to the proposed method*: As Reviewer fqev points out, the approach is evaluated only on Python data and with one code search method. In addition, the proposed approach may require a potentially complex implementation for each language (more so than the simpler lexical perturbation approach) but these aspects are not currently discussed in the paper. Reviewer 4Eqj agrees with this view and also suggests that the presented method be evaluated with other code search models and using other programming languages.\n2. *Weaknesses related to the experiments with LLMs*: Reviewer fqev identifies several weaknesses in these experiments – for more details, see their review.\n3. *Justification of certain methodological decisions*: Reviewer Xw4P poses a number of clarifying questions about implementation and methodological decisions – see their review. Reviewer 4Eqj supports this view by posing similar questions about the motivation behind using the RL framework.\n\nIn addition, all reviewers have provided further suggestions for improvements and identified some missing references. The authors did a good job addressing reviewers' feedback and acknowledged reviewers' suggestions. These will, hopefully, be reflected in the revised version of the paper."
            }
        },
        "id": "K1aUenNAt0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "tBtc4Ousge",
        "replyto": "tBtc4Ousge",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5228/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609344,
        "cdate": 1696707609344,
        "tmdate": 1701465553414,
        "mdate": 1701465553414,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a novel approach to use semantics phrase-level information for Neural Machine Translation. The phrases are extracted with Word Pair Encoding (similarly to Byte Pair Encoding) and are integrated using an Attentive Semantic Fusion (ASF) by extending the network input.\nThe paper is clear and well written. The experiments are sound and the results show the beneficial impact of the method (although sometimes, rather low quantitative improvements are obtained).\nIn the rebuttal, the authors address most of the reviewers' comments. Their answers are convincing and, if all modifications are made, should lead to an improved version of the paper for the camera ready. This is much appreaciated and strengthen the submission."
            }
        },
        "id": "E9aMbdqGGk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "t6p5LtTlqr",
        "replyto": "t6p5LtTlqr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3036/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554850,
        "cdate": 1696707554850,
        "tmdate": 1701465485225,
        "mdate": 1701465485225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a framework called DRAFT for few-shot topic classification using a dense retriever model. The model is shown to outperform large language models (LLMs) in experiments, but the novelty of the approach is questioned by the reviewers as it is similar to existing retrieval-augmented methods. The paper is well-written and provides detailed content and thorough experimental results, but the reviewers feel that more comparison with other methods is suggested. The dataset construction method is also questioned as it uses a multi-query retrieval algorithm, and more details are needed to make it more clear. The reviewers also suggest more discussion and comparison with existing methods is needed."
            }
        },
        "id": "wAbVm577Sk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "t42YUsyv3d",
        "replyto": "t42YUsyv3d",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission226/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482386,
        "cdate": 1696707482386,
        "tmdate": 1701465391997,
        "mdate": 1701465391997,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces the task of identifying sentence-level misinformation, as part of fake-news debunking. Due to the lack of an appropriate dataset, multi-instance learning is used as a form of weak supervision, and achieves state of the art results.\n\nOverall, reviewers (all of whom were quite confident) were Impressed by both the writing (\"well-articulated research\") and the research (\"faces a challenging problem in a creative but also effective way\").\n\nTwo reviewers raised nearly identical concerns -- the use of an outdated model as the basis for the method, when a newer approach (SentenceBERT) might have performed better, and the use of datasets that are known to be relatively easy. The authors have committed to including results with the first suggestion in their final version, but note that they are limited by the lack of a more suitable dataset for the second.\n\nAlthough these do seem like shortcomings, reviewers were nearly unanimous in this paper being strong on both soundness and excitement. As such, this seems like a clear accept (\"the contribution is timely and relevant\"). None were particularly effusive in their written praise (e.g., \"intriguing\", and an \"interesting stepping stone\", rather than transformative), but all rated it as strong on excitement, and wish to see it published."
            }
        },
        "id": "D8hwqGorvw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "t035Emm4Vt",
        "replyto": "t035Emm4Vt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1453/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707514687,
        "cdate": 1696707514687,
        "tmdate": 1701465431427,
        "mdate": 1701465431427,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposed a new dialogue task on a commercial video game, and conducted user study to analyze and evaluate the behavior of LLM-augmented dialogues and game-designers' writing. \n\nAll the reviewers agree that this paper is both sound and exciting. But they also all have some concerns around the task evaluation and dataset analyses. \n\nThe discussions around the study size (especially around the wording of \"large-scale\") during rebuttal and analysis of the statistical power should be included in the camera-ready. It would also be good if the authors could discuss how this specific task would shed light on general dialogue research beyond games."
            }
        },
        "id": "no0guMhSbd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "syj9VaxutQ",
        "replyto": "syj9VaxutQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3862/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578053,
        "cdate": 1696707578053,
        "tmdate": 1701465512143,
        "mdate": 1701465512143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers are agreed on the excitement, but are finding small errors and issues that need further clarification in the text. These should be cleared up in further drafts of this work."
            }
        },
        "id": "IfRNJI8w7f",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sxJU7X2ZG0",
        "replyto": "sxJU7X2ZG0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5523/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614473,
        "cdate": 1696707614473,
        "tmdate": 1701465561566,
        "mdate": 1701465561566,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In light of the significant recognition garnered by LLMs for their exceptional performance across various Natural Language Processing tasks, the exploration of prompt optimization emerges as a highly promising avenue for research. \nThis holds particular significance in the context of black-box LLMs such as ChatGPT.\nThis paper introduces a gradient-free prompt optimization technique, characterized by its robustness in mitigating the challenges posed by data domain shifts.\nAll reviewers have agreed that the proposed Generalized Prompt Optimization (GPO) framework is useful for improving generalization, especially under domain shifts.\nWhile some concerns have also been raised that the comparison is a bit weak. \nAnd during the rebuttal period, the authors have made efforts to address the concerns.\nGenerally, the merits of this paper outweigh its flaws."
            }
        },
        "id": "O1OErUfsV1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "svUOik2Xu1",
        "replyto": "svUOik2Xu1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission758/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495276,
        "cdate": 1696707495276,
        "tmdate": 1701465409438,
        "mdate": 1701465409438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work shows that LLMs can be used to synthesize data even when tasks cannot be solved directly by LLMs, particularly for problems with structured outputs. In this case, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in the task difficulty makes it possible to produce large-scale, high-quality data for complex tasks.  This leads to improved capabilities of smaller LLMs at structured prediction tasks, particularly IE. This is a simple but useful idea - and it has been properly investigated in the paper. The reviewers seem to like the paper as well."
            }
        },
        "id": "tKtPaqa0PG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "svSNikfCs1",
        "replyto": "svSNikfCs1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4142/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584012,
        "cdate": 1696707584012,
        "tmdate": 1701465521797,
        "mdate": 1701465521797,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper tackles the challenge of few-shot relation extraction by employing training-free methods, in contrast to traditional supervised learning. It achieves two key contributions: (1) it investigates the effectiveness of in-context learning for this problem; (2) it introduces a 3-step approach using typed chain-of-thought prompts. This method starts by extracting entity type information and then uses it in conjunction with the context to identify relations. Experimental results show that including entity type information enhances performance on the FewRel 1.0 and FewRel 2.0 validation sets. Reviewers unanimously agreed on the good soundness of the work (one reviewer also increased soundness score after rebuttal), and also shared several areas of improvement. First, there's a lack of error analysis, which could help justify the proposed CoT-ER approach over the baseline. Second, the selection of few-shot instances for reasoning varies widely across setups, and its contribution to performance is unclear. Third, only 100 samples are used from each dataset for evaluation. Thus, the results may not hold up when evaluated on the full test datasets. Finally, the paper does not provide a detailed analysis of the impact of the k value on the performance of the proposed method. k-NN search is a key component, a more thorough analysis of the impact of the k value on the experimental results is needed."
            }
        },
        "id": "MX35dfkaoT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sthusQGkef",
        "replyto": "sthusQGkef",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3752/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707571508,
        "cdate": 1696707571508,
        "tmdate": 1701465508679,
        "mdate": 1701465508679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Paper addresses the modality gap between speech and text representations when using pre-trained models for speech2text translation. It explores the use of boundary predictions (from CTC) in the shrinking process to reduce gap between speech sequence length and text sequence length. The paper is well-written and method proposed (as well experiments presented) are overall convincing. However the positioning related to previous approaches (to address modality gap for S2T translation) could be improved and this makes evaluation of the originality of the work difficult. \nMoreover, questions were raised about the accuracy of the CTC boundary prediction and the use of pre-trained forced aligners instead. Finally, the way additional alignment objectives are included is unclear and some improvement on this part would be needed."
            }
        },
        "id": "KzF4eZo3vd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "st5RaWdLTn",
        "replyto": "st5RaWdLTn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission618/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492268,
        "cdate": 1696707492268,
        "tmdate": 1701465405475,
        "mdate": 1701465405475,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The work demonstrated a novel and effective way of bypassing the toxicity detection models by appending positive words or sentences to the end of a toxic message. It showed that this sentence-level attack can be applied to multiple languages from different language families, and that the models’ confidence can be reduced by increasing the length of the added sequence.\n\nPros: \nIntroduced a concept of “To Each His Own” attack, which is based on the idea of separating the messages addressed to a human and an algorithm. The attack exploits the fact that the toxicity detection models are trained on sentence-level labels, and do not consider the context or the intention of the message.\n\nCons\nMay not be effective on toxicity detectors that use different definitions or criteria of toxicity"
            }
        },
        "id": "morQJ7rrHl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sriK75T3kd",
        "replyto": "sriK75T3kd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5417/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612590,
        "cdate": 1696707612590,
        "tmdate": 1701465558808,
        "mdate": 1701465558808,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new way to enhance the efficiency of multilingual neural machine translation (MNMT). First, it introduces an efficient alternative to language adapters, termed Language Specific Matrix Synthesis (LMS). Second, it presents Fuse Distillation (FD), which aims to distill all the knowledge encapsulated in language-specific modules into a single shared module. The experimental results show that LMS helps reduce the parameter count without compromising performance. Furthermore, FD reduces the parameter count while also improving MNMT, as it essentially serves as a regularization method.\n\nPros:\n- I agree with the reviewers regarding this work's innovative approach to achieving both efficiency and effectiveness in MNMT. The concepts presented are intriguing, and the results demonstrate notable improvements in both performance and efficiency.\n- The paper is well written, making it easy to comprehend and potentially straightforward to replicate. It includes comprehensive and thorough analyses.\n\nCons:\n- While there are minor details that require refinement in the final version, they have all been addressed well in the rebuttal phase."
            }
        },
        "id": "kaqYtaFh3l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "siiVduxdRz",
        "replyto": "siiVduxdRz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2444/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542349,
        "cdate": 1696707542349,
        "tmdate": 1701465465295,
        "mdate": 1701465465295,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies why improvements in throughput and FLOPs of recent efficient models and hardware architectures do not directly translate to reductions in model inference latency. The paper attributes this gap to the overhead imposed by some deep learning frameworks, to which it refers as \"framework tax.\" Overall, the reviewers unanimously rate this paper as strong both with regards to soundness and excitement.\n\nThe reviewers praise this paper for the broad insights it provides on a practically relevant topic. They also praise the novelty and systematic investigation, and note that this can be widely useful for the NLP community.\n\nMost of the reviewer criticism centers around extending the study to other accelerators, models (beyond encoder-only transformers), and other aspects such as to investigate different sequence lengths. While the present paper does not cover all possible experiment settings, it nevertheless has been praised for its broadness by the reviewers. This suggests that the identified gaps could motivate further work on this subject in the future. Moreover, the authors provide preliminary insights in their responses on some of the settings requested by the reviewers, such as decoder-only transformers, and regarding sequence length. These results could provide additional value to the submission."
            }
        },
        "id": "vVfFB3ttGG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sfkpJxeDzk",
        "replyto": "sfkpJxeDzk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4107/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583310,
        "cdate": 1696707583310,
        "tmdate": 1701465520615,
        "mdate": 1701465520615,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In general, reviews for this paper were mixed, with some reviewers feeling that the paper was more ready for publication than others.  Soundness was rated along a broad range from borderline to excellent, and excitement was rated as ambivalent in most cases.  Overall, the paper could benefit from stronger justification of its arguments and minor to moderate revisions, synthesizing the information (from authors) and feedback (from reviewers) that emerged during the discussion period with the manuscript.  As noted by the authors in their rebuttal, some of these revisions have already been made during the discussion period.\n\n**Summary of Reviewer Feedback and Discussion:**\n- **Reviewer UeN6** appreciated the insights arising from the paper's experiments, but felt that the experiments were not designed in such a way that they could demonstrate the manipulability of personality in more powerful LMs.  They also felt that the evaluation of this design was poorly justified and inadequately addressed biases that could be introduced during the experimental process, and they felt that the study's findings had limited reliability and generalizability.  They also raised one question pertaining to a figure.  In their rebuttal, the authors answered this question and clarified the intent of their research, which the reviewer had misunderstood.\n- **Reviewer 9sqR** felt that the insights from this paper would be meaningful and helpful for constructing personalized applications and that the experiments were detailed and comprehensive.  However, they felt that it was unclear whether different questionnaires would lead to different conclusions and that an analysis of ChatGPT should have been included.  They also felt that the proposed method could be more comprehensively compared with other personality prediction methods, and asked whether the authors' datasets would be publicly available.  The authors clarified that the datasets would be publicly available, and explained that they had selected open-source models and that ChatGPT had been released after their work started.  They noted that in the future they could also compare to more recent models, and mentioned some general updates that they'd also made to revise the paper.\n- **Reviewer RXZa** felt that the paper presented sound methods that were thoroughly analyzed and nicely motivated.  They appreciated the significant findings paired with detailed analyses, and noted that the contributed datasets will be important for the research community.  However, they pointed that the generalizability of the paper's findings to other types of computational models is known, and suggested further studying whether these findings were invariant to model type.  The authors reiterated from their statement to Reviewer 9sqR that further work is needed to asses generalizability, and also mentioned their general updates to the manuscript.\n- **Reviewer gkMG** felt that the study was interesting and may be useful to numerous applications, and also appreciated the new datasets introduced in the paper.  However, they raised many points requiring clarification, and some concerns regarding the models used in the paper and the datasets themselves.  Specifically, they felt that the selected models were too old, raising concerns that the paper's conclusions may not carry over to more recent models, and they were concerned whether adequate quality control was performed during the dataset development process.  The authors provided clarifying responses to Reviewer gkMG's questions, and reiterated earlier points made to other reviewers about their selection of open-source models and the need for further studies to assess broader generalizability.  They noted that no quality control was performed since there was no objective way to determine text relevance to the topic of personality.  Reviewer gkMG thanked them for their rebuttal and clarified that their interest in quality control mainly pertained to toxic language, and suggested that if no quality control was performed, then a qualitative analysis should be performed on a random sample of the data as a post-hoc quality evaluation.  The authors agreed that this would be helpful, and they promised to add this in the final version of the paper."
            }
        },
        "id": "5VE6O923vy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "se0YmUUfPs",
        "replyto": "se0YmUUfPs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3756/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707573917,
        "cdate": 1696707573917,
        "tmdate": 1701465508792,
        "mdate": 1701465508792,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Based on the reviews, the paper presents a multi-stage pipeline called WikiChat that combines retrieved data from Wikipedia with generative content from LLMs to produce factual and up-to-date conversation content. The reviewers generally agree on the soundness of the proposed method, its state-of-the-art performance, and the value of releasing the code and model to the community. However, there are some concerns and suggestions for improvement.\n\nReview 1 appreciates the effort invested in the research and finds the proposed method sound. The author suggests clarifying certain method/experiment/evaluation details and including an evaluation of system latency. Review 2 sees the proposed pipeline as rather engineering and lacking novelty, criticizing the non-scientific tone and bold claims in the introduction. They also make several questions about the definition of conversationality, comparison with the Wizard of Wikipedia system, response time, and quality of the user simulator. Additionally, they recommend rewriting the whole introduction. Review 3 acknowledges the complex nature of the pipeline and its effectiveness in improving factuality. However, they find it difficult to follow due to the scattered implementation details and the paper covering too many topics at once.\n\nOverall, the reviewers agree that the proposed method improves factuality and conversationality, and achieves state-of-the-art performance. However, improvements are needed in terms of clarity, organization, and presentation. The reviewers raise concerns about clarity in method/experiment/evaluation details, the definition of conversationality, and the comparison with existing systems. They also suggest including an evaluation of system latency and verifying the quality of the user simulator. The paper's presentation, including the introduction and organization of details, should be revised for better readability.\n\nIn terms of scores, the reviewers generally rate the soundness of the paper as good (3) and the excitement as ambivalent (3). While the paper has merits such as reporting state-of-the-art results, there are also weaknesses that may require further revision. However, no reviewer strongly objects to accepting the paper.\n\nIn summary, the paper presents an innovative chatbot pipeline that combines retrieved data and generative content for factual and up-to-date conversation. While the method shows promise and achieves state-of-the-art performance, improvements are needed in terms of clarity, organization, and presentation. The paper covers multiple topics and implementations, making it challenging to follow. However, with revisions addressing the reviewers' concerns, the paper can be much better.|meta review"
            }
        },
        "id": "45xuOKBdoL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sdC55K8cP0",
        "replyto": "sdC55K8cP0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2085/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534086,
        "cdate": 1696707534086,
        "tmdate": 1701465453452,
        "mdate": 1701465453452,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper deals with automated prompt construction for few-shot (parameter-efficient fine-tuning, not in-context) learning for instruction-tuned LLMs. The authors first demonstrate that instruction-tuned LLMs are pretty robust to the selection of instruction prompts for classification task, which then informs an automated approach to prompt selection, based on prompt retrieval (over a catalog of instructions on which the backbone model was initially (pre)trained) and strategy for generating candidates for answer choices (i.e., class lexicalizations), based on template-conditioned probabilities of tokens for a class (on the training data). Evaluation of Aut-Few on a dozen short-text classification datasets renders it effective (more on reviewer concerns raised on Aut-Few performance below). \n\nThere is some logical discrepancy between the initial study, which shows that instruction-tuned LLMs are robust to the choice of the prompt, and the rest of the paper that aims to automate prompt (task-description) selection/creation. The authors explain (in the rebuttal) that, despite the fact that instruction-tuned LLMs exhibit more robustness w.r.t. choice of task description than vanilla LLMs (i.e., not instruction-tuned), there is still variation, especially w.r.t. lexicalizations of classes. The finding of robustness of instruction-based LLMs is indeed somewhat at odds with the motivation for proposing Aut-Few, and the authors should address this in the structure of the paper.       \n\nThe main concern raised in the reviews is that of performance of Aut-Few in comparison to T-Few, which manually selects the prompts. The concern raised was that Aut-Few outperforms T-Few only on a minority of the evaluation datasets. The authors clarify the significance of their results in the rebuttal: on 3 datasets, Aut-Few is significantly better than T-Few, whereas on 1 dataset the opposite is true; on the remaining datasets the difference in performance between the two is not statistically significant. The crucial point here, that the reviewer may have missed, however, is that T-Few implies careful manual selection of the prompt, whereas Aut-Few removes the need for any manual effort.\n\nAlthough one could argue that neither the retrieval step nor class label generation component are greatly innovative -- i.e., they can be seen as relatively straightforward extensions of existing body of work on prompting LLMs -- they are meaningful and convincing. Moreover, the practical benefit of this work -- removal of the need for manual effort in prompt design for instruction-tuned LLMs -- is substantial."
            }
        },
        "id": "67Pxf9n8gN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "scAXKWMJR3",
        "replyto": "scAXKWMJR3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2225/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537322,
        "cdate": 1696707537322,
        "tmdate": 1701465458280,
        "mdate": 1701465458280,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers liked how the paper highlights current issues with existing text-to-sql benchmarks through experiments and human analysis. The paper might have created expectations for new benchmarks and evaluation metrics that it did not deliver, but also never claimed to. Other concerns have been addressed in an exemplary fashion by the authors during the rebuttal."
            }
        },
        "id": "m4XVkgwJF1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sbuO0s1r71",
        "replyto": "sbuO0s1r71",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4819/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599055,
        "cdate": 1696707599055,
        "tmdate": 1701465542676,
        "mdate": 1701465542676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper is concerned with knowledge graph enhancement to improve the coverage and precision; it specifically aims to improve entity information in 10 languages, and presents a large new dataset (35,000 multilingual names) as the WikiKGE-10 benchmark. The core idea of the approach is to combine previous approaches to this problem (leveraging machine translation, web search and LLMs) into their approach M-NTA \"Multi-source Naturalization, Translation and Alignment\". \nThe reviewers agree that the paper is well-written and sound; they particularly appreciate the crosslingual work and the evaluation, as well as the paper's clarity regarding limitations. Some more detailed points questions about the methods could be addressed in the rebuttal and should help to improve the final version of the paper."
            }
        },
        "id": "zH10RBFacX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sbLFUT4DaG",
        "replyto": "sbLFUT4DaG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4783/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598420,
        "cdate": 1696707598420,
        "tmdate": 1701465541628,
        "mdate": 1701465541628,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a memory augmentation technique to address OOD text classification. The memory is used during meta-learning to enhance the invariance of the representations. Experiments on semantic analysis and NLI show promising gain, and SotA result. The writing as a matter of presentational issue needs to be improved (fully agree with VVu8). The discussion with goM8 justified the reason behind the task selection adequately. The combination of memory and meta-learning is certainly well-justified (specially after reading the discussions).  An interesting work!"
            }
        },
        "id": "tVievXL8J8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sZGAxcUcNU",
        "replyto": "sZGAxcUcNU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4737/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597432,
        "cdate": 1696707597432,
        "tmdate": 1701465540373,
        "mdate": 1701465540373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers almost unanimously recommend acceptance with strong soundness and moderate excitement. The experimental setup and results are rigorous, the method well-motivated, and the results are strong. I recommend acceptance to the main conference."
            }
        },
        "id": "7ZBHmdhUYu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sZEAMUizsd",
        "replyto": "sZEAMUizsd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1923/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530134,
        "cdate": 1696707530134,
        "tmdate": 1701465447295,
        "mdate": 1701465447295,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces MetaL-Prompt, a prompt generation method that addresses the challenges of the lack of an automatic prompt generation method in existing language models as a service (LMaaS) and the need for large samples for prompt tuning. With this meta-learning based approach, a prompt generator model can generate prompts for a series of unseen tasks without additional training. \n\nPros:\n\nWell motivated with a timely and a very relevant research topic\n\nThe paper establishes a connection between the meta soft-prompt and the discrete demonstrations designed for a specific task\n\nYields comparable results without the need for fine-tuning and surpasses existing prompt generation methods when fine-tuned with considerable computing resources.\n\nThe idea of utilizing hidden states acquired from special tokens as prompts is interesting \n\nCons:\n\nThe authors have used GPT-2 large and XL .. these are both fairly weak and outdated models and significantly smaller in size compared to SoTA models today. In order to make this work more relevant and more convincing the authors need to show that their proposed method works even for large enough LLMs \n\nThe paper would benefit from some case studies to understand the effectiveness of the method \n\nThis method demonstrates clear advantages under low-resource conditions. But it would be good to understand how it performs under the more common high resource scenario where abundant samples are available (this point also has been raised by multiple reviewers)"
            }
        },
        "id": "YmK5wSLvwQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sYYRTVaG3n",
        "replyto": "sYYRTVaG3n",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3481/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563657,
        "cdate": 1696707563657,
        "tmdate": 1701465498909,
        "mdate": 1701465498909,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this work, the authors perform document-level translation using ChatGPT and GPT-4, and present a thorough comparison of these LLMs with state-of-the-art MT systems on a benchmark evaluating translation quality going from Chinese to English, English to German and Englishto Russian. They also probe which parts of the LLM training pipeline (fine-tuning, reward modeling, etc.) contribute most to its document-level translation capabilities and experiment with different discourse-aware prompts to identify what works best.\n\nAfter detailed discussions with the authors during the rebuttal phase, the reviewers are all unanimous in their ratings for soundness (4/4/4) and excitement (4/4/4). R3 pointed to many updates that the authors should make in their revised draft, including details of human evaluation criteria, citations to relevant references and to be more precise about the impact of different training techniques."
            }
        },
        "id": "iYk6F2mLru",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sXErPfdA7Q",
        "replyto": "sXErPfdA7Q",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2449/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542452,
        "cdate": 1696707542452,
        "tmdate": 1701465465488,
        "mdate": 1701465465488,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper explores analogical reasoning in large language models (LLMs), moving beyond word analogies to \"analogical structure abduction\". It introduces a benchmark, Scientific Analogical Reasoning (ScAR), with 400 analogies across 13 domains and 1600 concept mappings, supplemented with background knowledge. The study reveals LLMs, while adept at word analogies, often miss deeper structural connections, unlike humans. Using ScAR, LLMs show varied results, particularly when structures aren't directly provided. The paper's key contributions are the ScAR benchmark and highlighting the LLM-human cognition gap in analogy. It paves the way for future research in this area.\n\nThe paper's blend of a relevant topic, novel insights, a high-quality dataset, and comprehensive analysis makes it a worthy contribution to the field. In particular: \na) The authors have meticulously compiled a benchmark dataset that encompasses a diverse range of analogies spanning scientific and other domains. This dataset is enriched with consistent details about the structural essence of analogies and is supplemented with background knowledge and explanations. Such a comprehensive resource stands to benefit the broader community.\nb) A remarkable feature of the paper is its preliminary study highlighting the inadequacies of word analogy tasks in truly representing analogical reasoning capabilities. This perspective, not widely explored in previous works, offers a fresh lens to view analogical reasoning, urging the community to adopt a more holistic approach.\n\nHowever, the main weaknesses of the contribution are: paper's lack of novel techniques (despite the aforementioned novel insights), insufficient exploration of pre-training data, ambiguity in its linkage to human cognition, and methodological shortcomings."
            }
        },
        "id": "MGjT1HW59N",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sX4yqbYlRm",
        "replyto": "sX4yqbYlRm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission19/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476976,
        "cdate": 1696707476976,
        "tmdate": 1701465384273,
        "mdate": 1701465384273,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a novel method for text-video event extraction based on a three-stream multi-level event contrastive learning approach. In addition to text and visual appearance features, this work incorporates video motion features based on optical flows, significantly improving the proposed system from previous studies.\n\nThe reviewers' evaluations are primarily high. I don't find any critical issues in the reviews. \"Reasons to reject\" from reviewers look limited to minor issues and questions for clarification. The authors responded carefully to such reviewers' concerns and questions in the rebuttal.\n \nOne reviewer asked about related works on noise-robust event trigger extraction, the appropriateness of every 16-frame sampling, and the effectiveness of optical flow features against saliency-based features. The authors answered the questions with additional experimental results, showing that the proposed method outperforms different sampling approaches and saliency features.\n\nAnother reviewer was concerned about the novelty of optical flow features and motion representation. For these concerns, the authors explained that associating the optical flow features with the event triggers is novel, and the use of motion representation is also novel in text-video event extraction. The reviewer acknowledged the authors' response and raised the soundness score to 4.\n\nConsidering the reviews and discussions, the paper is well-written, showing that the proposed method is technically sound and effective for text-video event extraction."
            }
        },
        "id": "ylfRTuwGjj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sVSeGRCZT8",
        "replyto": "sVSeGRCZT8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1574/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518071,
        "cdate": 1696707518071,
        "tmdate": 1701465434883,
        "mdate": 1701465434883,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper aims to improve unsupervised sentence representations and efficiency by proposing a hierarchical contrasting learning method. Reviewers praised the proposed methods, the thorough experiments, and clarity in the paper. Through detailed back and forth discussions some reviewer complaints were addressed and their scores raised. However, reviewers still found the method's motivation to be lacking and the performance gains to be slight."
            }
        },
        "id": "sI6cOmnCRQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sTeoqvTH2j",
        "replyto": "sTeoqvTH2j",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2688/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547585,
        "cdate": 1696707547585,
        "tmdate": 1701465473456,
        "mdate": 1701465473456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes an approach for improving the diversity of question generation by retrieving templates from an external corpus and using them as guidance about the approximate syntactic structure of the generated question. The proposed RAST system is trained through RL, where in which the reward function is a combination of the consistency and diversity. \n\nExperimental results show that the proposed model outperforming baselines, in both automated and human evaluation, in terms of diversity while being comparable in terms of consistency."
            }
        },
        "id": "Ar5fo2V6Sy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sS02W7Sloj",
        "replyto": "sS02W7Sloj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3426/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562436,
        "cdate": 1696707562436,
        "tmdate": 1701465497048,
        "mdate": 1701465497048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes a method for detecting factual inconsistencies in long documents by applying Natural Language Inference (NLI) to chunks of the document. Additionally, the authors introduce a new dataset, ScreenEval, which is a dialogue-based dataset designed for factual inconsistency detection.\n\nThe method is simple, scalable, yet outperforms the baseline approaches. Furthermore, the new dataset is expected to be a valuable resource for the research community. Overall, I don't see any significant issues with the work, aside from minor points related to clarity."
            }
        },
        "id": "xZWdu9v3kP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sRHVpB7GE6",
        "replyto": "sRHVpB7GE6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2466/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542814,
        "cdate": 1696707542814,
        "tmdate": 1701465466051,
        "mdate": 1701465466051,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper aims to deal with few-shot relation extraction, especially to solve the problem of none-of-the-above (NOTA) relation classification. Specifically, this paper adopts an additional training objective to classify known and out-of-distribution instances, making the NOTA relations easier to identify. The structure of the article is well organized, and the presentation is relatively straightforward. The main problem of this paper is that the used datasets are limited, making the results not solid enough."
            }
        },
        "id": "dBLY404T0l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sQ1iTreITk",
        "replyto": "sQ1iTreITk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission982/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707501059,
        "cdate": 1696707501059,
        "tmdate": 1701465416735,
        "mdate": 1701465416735,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The submission presents a method for mapping from embedding space into a space defined by a set of human-interpretable concepts, using textual descriptions of each concept. Evaluation is performed via text classification. The initial reviews flagged some weaknesses in evaluation, which were addressed by further evaluations during the discussion period. In the end, all reviewers rated the paper as \"strong\" on excitement and either \"good\" or \"strong\" on soundness."
            }
        },
        "id": "lHu0obXf5C",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sPpft5DQJN",
        "replyto": "sPpft5DQJN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission148/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480358,
        "cdate": 1696707480358,
        "tmdate": 1701465388739,
        "mdate": 1701465388739,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper delves into the problem of optimizing deep clustering from an empirical risk minimization perspective, with a focus on utilizing the correlation between samples.\n\nThe reviewers have expressed varying opinions on this paper. After a thorough examination of the manuscript, I have identified several concerns that need to be addressed. These include limited novelty, poorly presented details (e.g., Deep Text Clustering, generalized supervision and generalized labels), the absence of SOTA baseline comparisons, and a lack of ablation study. Additionally, while the authors acknowledge some prior shortcomings in the Introduction, these issues should be more clearly elucidated and appropriately cited.\n\nTo address these concerns and enhance the quality, I recommend that the authors thoroughly revise their work and resubmit it for further evaluation."
            }
        },
        "id": "MkBqYsg9sV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sPB354cbmL",
        "replyto": "sPB354cbmL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission548/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490506,
        "cdate": 1696707490506,
        "tmdate": 1701465403239,
        "mdate": 1701465403239,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose a verifier for RAG models that verifies that the retrieved evidence is relevant to the query and that the generation is attributed in the evidence.\n\nThe reviewers agree that the tackled problem is important, and that the proposed approach is simple and effective.\n\nThe reviewers also note that using an LLM for verification for this specific task is relatively incremental, as using verifiers in NLP is well known. In addition, the paper should be edited for further clarification regarding specific questions raised by the reviewers."
            }
        },
        "id": "N1gYE2qwG4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sOngusZCsN",
        "replyto": "sOngusZCsN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1352/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510883,
        "cdate": 1696707510883,
        "tmdate": 1701465428079,
        "mdate": 1701465428079,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers highlight the novelty and exciting contributions of this paper. The paper is of good quality, both in terms of writing and analyses. There are some reasons to reject the paper, but they are mainly concerned with adding some clarifications or restructuring the paper.\n\nPaper Topic And Main Contributions:\n\nThis paper introduces a novel deductive approach called GeDe for solving math word problems (MWP) by generating operations iteratively using a multivariate directed acyclic graph (mDAG) structure. GeDe addresses issues related to repeated subtree generation and binary tree expression limitations. It achieves this by re-encoding the input at each reasoning step, introducing new intermediate quantities, and decoding to generate multivariate operations. The paper also presents a hierarchical beam search to enhance model performance. Experimental results on multiple datasets, including the newly released CMWPA dataset, demonstrate GeDe's effectiveness in achieving state-of-the-art performance with fewer parameters while properly handling N-ary operators.\n\nReasons To Accept:\n\n* Novelty of the Proposed Approach\n    * Successful extension of previous deduction approaches to N-ary operators, addressing a critical limitation in math word problem-solving.\n    * Devising a hierarchical beam search procedure for the decoding process, enhancing the model's efficiency and performance.\n    * Achievement of state-of-the-art performance while using fewer parameters, demonstrating the paper's contribution to optimization in the field.\n* New Data Set\n    * Collection and utilization of a dataset to showcase the effectiveness of the proposed GeDe approach, including dynamic quantity embeddings, re-encoder, and multivariate operation mechanisms.\n* Quality of the Writing\n    * Clear and well-structured writing\n* Quality of the Analyses\n    * Insightful ablation analysis.\n    * Promising results and insights from related experiments can potentially inform and benefit future research in the field of math word problem-solving.\n\nReasons To Reject:\n\n* The core of the paper lacks some details that should be included in there and not in the appendix.\n* The authors did not report the setting of random seeds or the average performance during multiple experiments.\n* The authors need to add more explanations of the results."
            }
        },
        "id": "6aAOzK7aOC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sOTbFCUrDj",
        "replyto": "sOTbFCUrDj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission695/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493931,
        "cdate": 1696707493931,
        "tmdate": 1701465407756,
        "mdate": 1701465407756,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Proposes Tuning-free Rule Accumulation (TRAN), a prompting framework for an LLM (gpt-3.5-turbo) to generate a self-managed set of rules that are used in future prompts to prevent similar mistakes. Relevant rules are retrieved via BM25.\n\nIt appears to improve performance over baselines on Q/A and text classification tasks. Some experiments are done in an 'online learning' fashion on the test set which is reflective of production use cases, making it hard to assess the lift over baselines in Table 1. It's recommended to clarify this. In Table 5, a fairer comparison is made with SALAM, however, comparing with other baselines would be more informative, especially given Table 1 suggests SALAM is not universally better than others and it is a new, less common baseline.\n\n## Pros\n- Mostly reproducible -- prompt templates provided\n- Good results on some QA and classification tasks (see caveat below)\n\n## Cons\n- Online rule accumulation is performed on the test set so it's not quite fair to other baselines that do not see the test set in Table 1 experiments. \n- Tested on limited types of tasks, unclear how to generalize to other generative tasks."
            }
        },
        "id": "x0o5Nkh49q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sM9NTLjsUh",
        "replyto": "sM9NTLjsUh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3882/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578377,
        "cdate": 1696707578377,
        "tmdate": 1701465512656,
        "mdate": 1701465512656,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel offline RL method for training chatbots with consistent persona.  Experiment results show that the method outperforms the BlenderBot3 baseline across several metrics, and is comparable to an alternative offline RL method.  The reviewers consider this an important topic and find the proposed method interesting, but also point out the the clarity of the paper could be improved.  The authors have provided extensive rebuttals to the reviews and the reviewers have acknowledged this.  Overall, the work looks sound and interesting, though the reviewers still have some reservations about it.|meta review"
            }
        },
        "id": "mK5JeJWCHn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sKdsBUAnts",
        "replyto": "sKdsBUAnts",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1186/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506899,
        "cdate": 1696707506899,
        "tmdate": 1701465423082,
        "mdate": 1701465423082,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Proposes an approach to enhance the performance of Retrieval Augmented Language Models, as it relates to outdated information and hallucination. The idea is to make the retrieved information related to the source text but also take future target text into account. Based on this, the authors propose an approach based on aggregating latent variables from a latent space as opposed to using the original text and develop the RegaVAE framework, which is based on built using VAE. Theoretical analysis is conducted and an upper bound for the objective is developed. Experiments reveal significant improvement in text generation quality and hallucination reduction. \n\nPLUS: \n\n- addresses two key challenges in Retrieval Augmneted Language models: selecting relevant retireval info and effectively aggregating retrieved info during generation. Appears to be novel. \n- theoretical analysis: upper bound on objective, which is optimizable. \n- ablation study and case study. \n- solid experimental study showing improvement over baselines on text generation quality and hallucination reduction. \n\nMINUS: \n\n- computational cost not analyzed. \n- a few additional experiments could add value, e.g., hyperparameter search."
            }
        },
        "id": "4L1AxqHLHz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sJb43ykK3o",
        "replyto": "sJb43ykK3o",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5178/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608651,
        "cdate": 1696707608651,
        "tmdate": 1701465552024,
        "mdate": 1701465552024,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new retrieval augmentation mechanism to improve the zero-shot generalization ability of language models. Most of reviewers have given positive scores, indicating that they appreciate the authors' work. Although there are specific concerns raised by each reviewer that need to be addressed for further improvement, the authors have adequately addressed the concerns raised by the reviewers, demonstrating their capability to further improve the paper. The authors should carefully consider and incorporate these suggestions to further enhance the quality of the work."
            }
        },
        "id": "4B2ZVaFHdg",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sJUCMYtgIK",
        "replyto": "sJUCMYtgIK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2195/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536668,
        "cdate": 1696707536668,
        "tmdate": 1701465457463,
        "mdate": 1701465457463,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes a new metric for identifying tasks for improving active instructing tuning. All reviews agree on the novelty of the proposed metric and usefulness of the study (for example task maps). Authors evaluate their method on large open-source LMs and two different instruction tuning dataset. Reviews also highlight some interesting follow-up questions and future research."
            }
        },
        "id": "HJYwl7vtnY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sGrYJQZMQo",
        "replyto": "sGrYJQZMQo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3231/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558656,
        "cdate": 1696707558656,
        "tmdate": 1701465491126,
        "mdate": 1701465491126,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agree that the paper presents a novel and interesting contribution and agree on a good soundness score. There were a couple of issues raised by the reviewers, that were also acknowledged by the authors, and for some I want to go a bit into more detail here.\n\nSome reviewers complained about the paper being hard to follow and too technical at some points. While I understand the position of the reviewer, the exact mathematical notation is necessary for such complex systems and it actually makes the paper stronger, although perhaps more difficult to follow for a wider audience. Nevertheless I would encourage the authors to add some more intuitive explanation for the most complex concepts. Maybe including a fully detailed example in the appendix might help?\n\nAnother reviewer also complained about older test sets being used in the evaluation. Regretfully this is an extended \"evil\" among our community and the authors defended their decision arguing comparison with previous work. In the rebuttal period the authors provided evidence of generalization to another language pair. These results should definitely be included in the final version to make the paper stronger.\n\nAll in all, the paper presents an interesting contribution. In its current form there are some issues in the presentation that have been pointed out by the reviewers, but all can be addressed in a final version of the paper, making use of the extra page of content or expanding the appendix if necessary."
            }
        },
        "id": "sIVOHVDUY4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sFtyaTTtap",
        "replyto": "sFtyaTTtap",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2837/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550490,
        "cdate": 1696707550490,
        "tmdate": 1701465478262,
        "mdate": 1701465478262,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces RefGPT, a method for generating truthful dialogue datasets using LLMs. Two reviewers pointed out that the truthfulness evaluation reported in the paper is based on automatic model-based evaluation with GPT-4, which cannot substitute for true human evaluation. The author response supplemented with additional human evaluation results and addressed the concern. One concern regarding the novelty -- improving LLM generation and reducing hallucination by providing additional textual information as conditional input is a common practice -- was not sufficiently addressed."
            }
        },
        "id": "pLUk2Fk7BL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sEvU6r8e7N",
        "replyto": "sEvU6r8e7N",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission734/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494687,
        "cdate": 1696707494687,
        "tmdate": 1701465408673,
        "mdate": 1701465408673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new dataset, DUNE (Dataset for Unified Editing), with the aim to propose a framework to evaluate in a comprehensive way the effectiveness of editing models. Moreover the authors presents retrieval-augmented methods for editing beyond knowledge triples, making the problem more practical. They also expand the definition of \"model editing\" to include a lot of changes (debiasing and rectifying reasoning errors for example). The experiments are well designed and conducted and the analysis  of the results demonstrate the effectiveness of retrieval enhancement in factual information editing, offering valuable insights for future research.\nThe paper would benefit from the addition of a discussion on the debiasing results and also of adding details on data quality and validation."
            }
        },
        "id": "vUWxoeIu1A",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sCxiD2Rx4l",
        "replyto": "sCxiD2Rx4l",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4314/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587261,
        "cdate": 1696707587261,
        "tmdate": 1701465527573,
        "mdate": 1701465527573,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper contributes a valuable new dataset of negotiation dialogues, a proposed new model for a negotiation agent, and automatic and human evaluation experiments that demonstrate superior performance over several baselines.  The reviewers especially appreciate the value of the newly created dataset and methodology, and is seen as the paper's main asset.  They also acknowledge the comprehensive evaluations and positive results in terms of both objective and subjective metrics.  On the other hand, the reviewers are less excited about the proposed approach, considering it to have only limited novelty.  The paper could be improved by pitching it more as a resource paper."
            }
        },
        "id": "iVcrJEJ4to",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sCu26OfxxZ",
        "replyto": "sCu26OfxxZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5321/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610871,
        "cdate": 1696707610871,
        "tmdate": 1701465556213,
        "mdate": 1701465556213,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper contributes a new dataset for the analysis of gender bias in LLMs, obtained by prompting GPT3.5-Turbo via in-context learning, and a method for moving from binary to graded bias annotation based on Best-Worst Scaling. As reviewer XADC summarizes, the resulting data is analyzed through 3 lenses: inter-annotator agreement, sample themes and baseline classification tasks, revlealing that annotators overall consider attacks on groups to be more biased than attacks on individuals, and that LLMs such as GPT4 cannot reason well about fine-grained gender bias classification.\n\nThe reviewers agree that the paper is overall sound, with adequate description of the methodology and support for the claims.\nThe paper and data leave open some questions around subjectivity in annotation and the impact of the gender of annotators, which are particularly important from a human-centered NLP perspective. However, the current scope of the work represents an interesting step forward in characterizing gender bias in NLP systems. \n\nI recommend incorporating the discussion of related work, limitations, motivation, and examples that were brought up in the thorough discussion with reviewers. As a minor point, it would be useful to preview the findings of the paper in the abstract, as it currently describes what was done, but leaves the reader wondering what was learned."
            }
        },
        "id": "2LnOHeeCvS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "sCtJmxhvJe",
        "replyto": "sCtJmxhvJe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5606/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615744,
        "cdate": 1696707615744,
        "tmdate": 1701465563504,
        "mdate": 1701465563504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a Hybrid Inverted Index that utilizes the embedding clusters and salient terms collaboratively to improve the quality and speed of dense retrieval.\nAll the reviewers have agreed that the paper has its merits in smoothly combining lexical and semantic features as an inverted index and bringing an instant speedup.\nHowever, common concerns have also been raised by the reviewers that either the most recent baselines are missing or the speed reports are not very complete, making the results not that convincing. \nDuring the rebuttal period, these concerns seem to be largely addressed."
            }
        },
        "id": "239xIr6MGZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "s7Vh8OIIm6",
        "replyto": "s7Vh8OIIm6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission132/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479995,
        "cdate": 1696707479995,
        "tmdate": 1701465388321,
        "mdate": 1701465388321,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a method to improve reasoning by combining chain-of-thought with self-verification, e.g., its generated rationale is paraphrased with masks and the language model fills in it, which can be verified against the original value. This method is shown to consistently improve 6 arithmetic tasks and 2 general tasks.\n\nReviewers agreed that the paper's contribution is in a new method for self-verification (yvPK, iNBL, hdXu), the proposed method can be used together with existing methods like self-consistency and PAL which leads to the best performance (yvPK), and the paper includes comprehensive experiments with many datasets, baselines, and backbone LMs (yvPK, hdXu).\n\nHowever, reviewers pointed out that there is insignificant gains, especially with self-consistency decoding and PAL, and on tasks beyond arithmetic tasks (hdXu). Also, they mentioned lack of analysis of the method (yvPK, iNBL, hdXu) and significant increase in cost (iNBL, hdXu) are weaknesses of the work. Authors are encouraged to address these issues in the final version of the paper if accepted."
            }
        },
        "id": "7NABS96jdV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "s4xIeYimGQ",
        "replyto": "s4xIeYimGQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission70/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478393,
        "cdate": 1696707478393,
        "tmdate": 1701465386110,
        "mdate": 1701465386110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper in question aims to evaluate the ability of ChatGPT to replace crowdsourced human workers in generating paraphrases for training intent classification models. \n\nPros: \n1. All the reviewers acknowledge that the paper addresses a relevant and important topic concerning the potential of ChatGPT as an alternative to traditional crowdsourcing methods for obtaining paraphrases.\n2. The study adopts a data collection methodology consistent with previous studies, providing a basis for comparison. The detailed analysis of generated paraphrases on parameters like diversity and validity adds credibility to the findings.\n3. There is a thorough examination of the limitations of the study, suggesting the authors' depth of understanding and potential areas of improvement.\n\nCons:\n1. Limited Scope: Reviewers also point out that the focus remains solely on paraphrase generation. A broader range of tasks would have offered more comprehensive insights.\n2. The omission of a comparison between ChatGPT and other open-source LLMs, like Falcon, Vicuna, is required by reviewers. I can see that the authors add new experiments with Falcon-7B-instruct and Falcon-40B-instruct during rebuttal. These new results should be added to the camera-ready version."
            }
        },
        "id": "zTs0VABAER",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "s1Lrw1HTcT",
        "replyto": "s1Lrw1HTcT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1578/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518164,
        "cdate": 1696707518164,
        "tmdate": 1701465435101,
        "mdate": 1701465435101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a multi-task knowledge interaction architecture to improve performance on the utterance infilling task.\n\nThe paper discusses some interesting points regarding incomplete utterance rewriting of multi-turn dialogues\nThe paper proposes an effective approach and is able to achieve state-of-the-art results.\nThe included experiments and ablation tests are informative.\n\nThe method has limited novelty according to some reviewers.\nSome of the results are mixed, while the ablation tests show a decrease when certain components are added.\nThe paper lacks comparsion to large language models, which may outperform this system, thereby decreasing its relevance to the current research directions."
            }
        },
        "id": "IEGS2dRp8J",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rzdqmUFVnv",
        "replyto": "rzdqmUFVnv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3907/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578885,
        "cdate": 1696707578885,
        "tmdate": 1701465513561,
        "mdate": 1701465513561,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes an approach to generate queries from a passage, and use them to pre-train a dense retriever. The approach is an alternative to using passage-passage pairs for pre-training. The experiments demonstrate the positive effects using such a pre-training approach.\n\nThe idea is interesting, although it is a straightforward extension to the exiting work. The experiments successfully demonstrate that this is a reasonable way to create training datasets for dense retriever.\n\nThe authors have answered most of the questions raised by the reviewers in their rebuttal."
            }
        },
        "id": "uUl6I2znYJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rzW3RouIXc",
        "replyto": "rzW3RouIXc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission96/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479126,
        "cdate": 1696707479126,
        "tmdate": 1701465387232,
        "mdate": 1701465387232,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces EGISES, a novel metric for evaluating personalization in summarization. Reviewers acknowledge the paper's theoretical contributions and its importance in addressing personalization in summarization, praising its clarity and organization. However, concerns include limited comparisons with classic metrics like BERTScore, computational complexity due to multiple JSD calculations, ambiguity in correlations between P-Acc and Acc, questions about the necessity of the metric compared to existing measures like ROUGE, limited dataset testing, absence of human judgment, and missing details. During discussions, there was a strong concern regarding the lack of human judgment and the fact that the metric was not compared with baseline metrics. While the authors have shown new results during the response period, it was not possible to check for soundness."
            }
        },
        "id": "L7Hb6yeDXK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rwpv2kCt4X",
        "replyto": "rwpv2kCt4X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1985/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531470,
        "cdate": 1696707531470,
        "tmdate": 1701465449262,
        "mdate": 1701465449262,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores leveraging a pretrained NLI model to assess the entailment relationship between generated text, and the prompt and the preceding text. While all reviewers appreciated the execution of the paper, some reviewers expressed concerns regarding the generalizability of the proposed method."
            }
        },
        "id": "zR9wyrZNXc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rwcTxeSsVI",
        "replyto": "rwcTxeSsVI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission626/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492460,
        "cdate": 1696707492460,
        "tmdate": 1701465405825,
        "mdate": 1701465405825,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper illustrates a new dataset, WikiWeb2M, designed to study multimodal webpage understanding. This dataset comprises 2 million webpages and retains the entirety of web content, rather than information tailored for specific use cases. The dataset facilitates the exploration of several generative modeling tasks that produce text outputs, including page description generation, section summarization, and contextual image captioning. To address challenges arising from long context, a novel attention mechanism named Prefix Global is proposed. This mechanism selects the most pertinent image and text content as global tokens, which then attend to the rest of the webpage for context. Experimental results indicate that annotations from WikiWeb2M enhance task performance compared to previous data. The studies also encompass comprehensive analyses concerning sequence length, input features, and model size, underscoring the dataset's significance and the efficacy of the Prefix Global method.\n\nThe paper's well-structured presentation, introduction of a comprehensive dataset, innovative attention mechanism, and robust experimental results make it a worthy contribution to the field. In particular: a) The paper introduces WikiWeb2M, a dedicated dataset for multimodal webpage understanding. This dataset is unique in its comprehensiveness, combining both image-caption pairs and extensive text articles, making it a valuable resource for a variety of generative tasks. b) The experiments conducted using the WikiWeb2M annotations show promising results. The paper also includes thorough analyses, such as ablation studies and efficiency analysis. Notably, the proposed method exhibits consistent performance, even as input sequence length increases, indicating its potential in handling long texts."
            }
        },
        "id": "ET6sBwM9qu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rwcLHjtUmn",
        "replyto": "rwcLHjtUmn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission161/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480673,
        "cdate": 1696707480673,
        "tmdate": 1701465389197,
        "mdate": 1701465389197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers share some similar concerns, especially in the lack of traditional baseline comparison. According to the rebuttal information provided by the authors, they have performed related experiments and will add these materials in the extra page if accepted. I therefore believe the authors have answered most of the reviewers' concerns."
            }
        },
        "id": "LmRPcyn2Rk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rwHOXIBFwq",
        "replyto": "rwHOXIBFwq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission797/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496173,
        "cdate": 1696707496173,
        "tmdate": 1701465410742,
        "mdate": 1701465410742,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a curriculum learning (CL) approach (especially a data scheduler) for effective training with text graph data.\n\nPros:\n1) The proposed curriculum learning method is overall novel and can generalize to other scenarios besides text graphs.\n2) The experiment on several node classification and link prediction datasets is sufficient.\n\nCons:\n1) In the main results (Table 2), some recent baselines cannot outperform the No-CL method, which is also mentioned by one reviewer. The authors' rebuttal cannot fully convince me because the most recent baseline CLNode performs worse than No-CL in almost all the datasets. I wonder whether the experimental setting is fair for the proposed method and all the basellines.\n2) The connection between the proposed CL method and text graph tasks should be described more clearly. Since this paper targets at a specific data type, the design of the proposed method should include some features that can deal with the core problems in text graph data."
            }
        },
        "id": "xoZ8zecUZb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rs78DlnUB8",
        "replyto": "rs78DlnUB8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4280/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586590,
        "cdate": 1696707586590,
        "tmdate": 1701465526408,
        "mdate": 1701465526408,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes generating rationales from bigger LLMs to help smaller LLMs come to the right decision. The discussion period raised several important weaknesses of the approach: it relies on the correctness of the final answer and doesn't have a human evaluation. Despite these weaknesses, the reviewers seem to agree that this approach is not only practically useful for creating interpretable distilled models but can also improve our understanding of what larger LLMs have to offer over smaller ones."
            }
        },
        "id": "TAoa7askB6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rq4UfmpRA9",
        "replyto": "rq4UfmpRA9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3322/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560486,
        "cdate": 1696707560486,
        "tmdate": 1701465493895,
        "mdate": 1701465493895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work introduces a covariance and variance optimization framework (OVO) to debias NER and RE tasks that is based on structural causal inference like a few other debiasing methods, but leads to more fine-grained adjustment than most of those. The technical novelty of this method is generally acknowledged by the reviewers. The paper originally missed important results on out-of-distribution evaluation, which is necessary for debiasing methods. There have also been a series of important comparative methods there were originally missed. The authors have provided some of those in the discussion phase, and the AC believe that these are essential to be included into the draft should the paper be accepted. In addition, there seems to be flaws identified regarding the SCM used behind the method, for which the discussion didn't reach consensus among the authors and the reviewers."
            }
        },
        "id": "EwXlWYmgDK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rn7Fn3CV7b",
        "replyto": "rn7Fn3CV7b",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3699/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567892,
        "cdate": 1696707567892,
        "tmdate": 1701465507030,
        "mdate": 1701465507030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper explores the feasibility of using LLM for data annotation, using the task of computational stance detection as a case study.  The authors find that while LLMs like Alpaca, Vicuna, and GPT-3.5 turbo achieve reasonable performance on a benchmark dataset for stance labeling, models are sensitive to variation along a number of dimensions (such as prompt and label) and show evidence of intrinsic bias.  They then test a multi-target multi-label setup to improve robustness in their annotation task, finding that training on additional machine-labeled examples improves out-of-domain and cross-target performance.\n\nOverall, the paper presents a solid experiment on the use of machine-labeled data.  An initial critique of the paper across reviewers is that the main focus of the paper is unclear in its current state.  Reviewer 5SF3 stated that they did not find that “this paper makes a focused contribution”, citing the fact that it seemed to run a number of experiments on using models to label stance without very much analysis on any given variation in approach.  While it isn’t clear what exact experiments the reviewer found unclearly motivated or analyzed, reviewer jYAZ similarly felt that section 4, which ran the finetuning experiments on RoBERTa, felt disconnected from the rest of the paper, as it suddenly seemed to be evaluating a new model on a wider range of datasets.  While I understood the purpose of section 4 to be testing the actual performance of models trained on data extracted from the multi-target multi-label annotation setup, the presentation of this section is unclear in its current form.  While the paper is a short paper, and thus, has stricter page constraints, ideally the paper would provide more motivation for a. why they are training the RoBERTa models and b. why they are examining performance across a variety of stance datasets, some of which are out-of-domain from the original training set of SemEval-16 Task-6 A.\n\nAnother issue with clarity that both reviewers KnpT and jYAZ identified was in the presentation of the results in Figures 2-4, which show the F1 of 3-class stance prediction using 3 different LLMs.  Reviewer KnpT, for example, found it frustrating to have to refer back to the text to understand what the figures were describing.  Reviewer jYAZ found it unclear what was meant by Specified Target A, Instruction Prompt A, … etc.   While the authors clarify in their rebuttal what each of the dimensions on the X-axis of figures 2-4 mean, these should ideally be included in some form in the paper (either in the main body or appendix).\n\nFinally, one issue that may affect the soundness of the paper is the issue of potential test data leakage in the pre-trained models used for annotation.  While the authors argue in their rebuttal that none of the open-source models explicitly include stance-detection corpora in their instruction-tuning process, there is still the potential for data leakage outside of instruction-tuning.  The potential presence of test data in the pretraining data for the LLMs performing annotation does raise concerns over the true robustness and extensibility of using LLM annotation.  The performance improvement of the finetuned-RoBERTa models on out-of-domain test sets, for example, may be attributed to the LLM annotation scheme reproducing the label distribution of data it should not have already seen.  While the authors argue that the instability between the vanilla instruction and the multi-target/multi-label settings provides some evidence against the potential data leakage issue, it’s hard to attribute the instability across different settings directly to the presence of data leakage, as LLMs are notoriously sensitive to how instructions are presented.  While there’s not a feasible approach to directly address the data leakage issue, a more convincing argument that the machine-annotation method is robust to out-of-domain data would be to repeat the section 4 analysis, taking each of the original “out-of-domain” datasets as the basis of their training corpora once and testing every other dataset against the model trained on the “leave-one-out” dataset."
            }
        },
        "id": "fiFsaz1LeQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rmhSMGjWPp",
        "replyto": "rmhSMGjWPp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2993/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553706,
        "cdate": 1696707553706,
        "tmdate": 1701465483598,
        "mdate": 1701465483598,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a dataset for open aspect-based summarization, a multi-document aspect-based summarization dataset with no fixed set of aspects. Authors recruit annotators to construct the resource from generic MDS datasets. Reviewers are satisfied with the quality of the constructed dataset and find it valuable for future research. Dataset analysis is also well presented.\nThat being said, the extraction and creation of aspect-based summaries out of generic summaries from MDS datasets raised questions among reviewers. Authors addressed much of the concerns in the rebuttal."
            }
        },
        "id": "4z8eKTqIP1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rjd8AqRyW3",
        "replyto": "rjd8AqRyW3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3514/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564258,
        "cdate": 1696707564258,
        "tmdate": 1701465499787,
        "mdate": 1701465499787,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work is a novel contribution to studying the formality of LLMs' outputs in a multilingual setting. The authors provide an in-depth analysis of formality for five different languages and highlight the differences, which is important for characterizing and evaluating the formality of the model outputs. However, I agree with reviewer 13h5 that the definitions of informality are not clear and consistent across languages, which can potentially affect the evaluation of formality (and possibly the reproducibility of the work). This weakness is further emphasized by the fact that there is only one native speaker annotating the non-English outputs (as pointed out by reviewers 4rv1 and emAM).\n\nBased on my personal reading of the paper, the reviewers' comments, and the post-rebuttal discussion, I believe that the setup is clear and well-designed to check the cohesion and formality variables. However, after AC-reviewers discussion, I think that the current version of the paper lacks some important in-depth analysis of the models' behaviors for more generalizable findings. For instance, how does the formality of text distributed in the pretraining corpora affect the formality-level bias and formality-level preservation? The paper briefly touches on this question (e.g., lines 521-533), but a careful empirical study is needed to determine whether the (aggregated) formality of text present in the pretraining corpora is a reasonably good predictor of the formality bias and preservation for any language. There must be (a few) causal factors that explain the contrasting behaviors across different model sizes and different languages. I don't think the current version of the paper can help answer \"For a new language, what kind of outputs could we reasonably expect if we use a formal/neutral/informal prompt?\" (in other words, generalizability of findings.)"
            }
        },
        "id": "OxM5CIa1ss",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rjDaTBwEBX",
        "replyto": "rjDaTBwEBX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4358/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588184,
        "cdate": 1696707588184,
        "tmdate": 1701465529024,
        "mdate": 1701465529024,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper creates a dataset MaXM which is a multilingual question answering benchmark. The dataset serves as a test-only mVQA dataset to validate the mVQA model performance. The reviewers have concern on the technical contribution from this paper and also request some detailed analysis on the proposed dataset. But overall, this dataset would be useful to the community as an evaluation set. The author has promised a release of this dataset in the paper."
            }
        },
        "id": "UcPQ3GauWi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rhGh8jLOPd",
        "replyto": "rhGh8jLOPd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2552/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544639,
        "cdate": 1696707544639,
        "tmdate": 1701465468778,
        "mdate": 1701465468778,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In their work “PEFTDebias : Capturing debiasing information using PEFTs“, the authors propose to use parameter-efficient fine-tuning (PEFT) for two-stage debiasing of language models. They evaluate several PEFT variants on BERT and four datasets across two bias dimensions (gender, race), and demonstrate that their methodology is effective for reducing biases. \n\nOverall, the reviewers appreciate this work (soundness, excitement). Given that this is a shortpaper submission, the scope is generally feasible. However, they also note that the claims would be stronger with results on more/ other base models and that more emphasis on the efficiency aspect (as suggested by the title of this work) would enrich the discussion."
            }
        },
        "id": "LCx15lRUel",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rgos321qpD",
        "replyto": "rgos321qpD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5232/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609442,
        "cdate": 1696707609442,
        "tmdate": 1701465553593,
        "mdate": 1701465553593,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces an innovative application of Byte Pair Encoding (BPE) in the realm of music scenarios. The proposed algorithm significantly enhances inference speed and result quality, only with a short preprocessing step. This research holds valuable insights for researchers and engineers engaged in symbolic music.\n\nThe reviewers have agreed on the innovation and impact of this research. By reducing the average token count by 50% while achieving improved performance and faster inference speed, the proposed method has the potential to reduce computational overhead in future studies on music generation and classification, making representation modeling more accessible. Furthermore, the paper is well-written, providing a comprehensive package with code, audio samples, and results on a dedicated website."
            }
        },
        "id": "cw9nI2gXDH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rgKfPzAF2j",
        "replyto": "rgKfPzAF2j",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1419/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512804,
        "cdate": 1696707512804,
        "tmdate": 1701465430379,
        "mdate": 1701465430379,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a new approach to knowledge-grounded dialogue generation, termed Efficient Latent Variable Modeling (ELVM). ELVM incorporates a query generator, followed by an off-the-shelf retriever, which remains fixed during the training process. Notably, ELVM aims to facilitate efficient and robust training without the requirement for annotated knowledge or explicit training of knowledge retrieval.\n\nWhile the utilization of variational techniques to enhance knowledge-intensive text generation tasks is not new, ELVM demonstrates its effectiveness in unsupervised joint training, leading to substantial performance enhancements compared to prior methods. However, despite the novelty of ELVM, the actual improvements over existing methods may appear relatively modest."
            }
        },
        "id": "aojaqlGFlA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rd0C4kD0o4",
        "replyto": "rd0C4kD0o4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1110/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505227,
        "cdate": 1696707505227,
        "tmdate": 1701465420752,
        "mdate": 1701465420752,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a multi-stage adaptation technique (unsupervised+contrastive+supervised) to improve quality of transfered models. All reviewers find the experiments sound and the technique well-explained. Only concern is raised by Reviewer j83o related to the novelty of the work as individual stages are not novel. I believe combining different techniques and demonstrating the benefit has great value for the community."
            }
        },
        "id": "G3CBt0yD2i",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rbaK24KnIO",
        "replyto": "rbaK24KnIO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4575/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593366,
        "cdate": 1696707593366,
        "tmdate": 1701465535367,
        "mdate": 1701465535367,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates method for model-driven reweighting of data for LLM pre-training, and propose a two phased learning method that leverages self-influence (SI) scores as an indicator of sample importance. This paper addressed an important research problem for large language models, and proposed a novel idea with good results on using gradient norm for weighting as comments by the reviewers. On the down side, given that this paper proposed a new method for LLM pretraining with data reweighting, it can be made stronger by experimenting on other LMs in addition to the T5 based models, and running evaluations on tasks that are beyond multilingual evaluations as commented by the reviewers. The computation overhead introduced by the proposed method may limit its usage in real-world large scaling training setting."
            }
        },
        "id": "yrndI0YQRB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rXn9WO4M2p",
        "replyto": "rXn9WO4M2p",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3054/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555228,
        "cdate": 1696707555228,
        "tmdate": 1701465485754,
        "mdate": 1701465485754,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Summary:** The paper proposes a new task: Entity-Centric Question Generation (ECQG) which aims at generating questions pertaining to specifically to an entity of interest (EOI). This is done using two modules: focus locating (to identify EOI), and a question verification module (to evaluate answerability of generated questions). Due to the lack of a task-specific dataset, the paper creates a dataset starting from SQuAD 2.0. Empirical and human evaluation shows that the proposed ECQG approach: GeneCONE outperforms both question-agnostic QG  and pretrained seq2seq baselines.\n\n**Approach and Methodology:** In the initial reviews some reviewers expressed concern on the limited novelty of the proposed solution, and the task being inspired from a previous work . In my opinion, the task setup is sufficiently distinct from previous works (with some possible real world applications) with the key novelty of the approach being restricted to plugging together existing solutions from previous ideas (focus locating and question verification) to get an end-to-end pipeline for entity-centric QG. \n\n**Empirical Evaluation:** The quantitative empirical evaluation in the paper is sound, with appropriate comparisons with baselines when needed (and shows that the approach beats the baselines). The dataset constructed in this paper may be an useful to the community. The main concerns regarding experiments is the limited human evaluation and discussions/qualitative studies performed to highlight some of the core motivations for the approach: centricity of generated questions on the entities, coherence evaluation of QA pairs, etc. The paper contains a small human evaluation study (over 100 samples) which needs to be made more extensive to strengthen the paper.\n\n**Recommendations for Improvement:** (i) Expand human evaluation study to make it more extensive, and clearly highlight analysis specific to entity-centric nature of GeneCone and improvements over baselines.\n\n(i) Add quantitative results (in addition to the qualitative results provided in author response) for benchmarking ECQG dataset with LLMs like ChatGPT, GPT-4, etc."
            }
        },
        "id": "AZoW8brtFp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rVsnAmxnR9",
        "replyto": "rVsnAmxnR9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2490/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543280,
        "cdate": 1696707543280,
        "tmdate": 1701465466802,
        "mdate": 1701465466802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper describes an active learning approach in the case of annotator disagreement. It takes the multihead approach that's now popular in passive learning, and experiment with both known and novel (eg Vote/Mix) data selection procedures. They find that active learning is effective, across three datasets.\n\nOn the positive side, this is a timely paper addressing an important problem, and the proposed approach is sensible and works reasonably well.\n\nOn the negative side, it is unfortunate that there is not a clear winner between group-level and individual-level annotation."
            }
        },
        "id": "gwBXxaDCOj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rTAIgZe3wo",
        "replyto": "rTAIgZe3wo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3161/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557271,
        "cdate": 1696707557271,
        "tmdate": 1701465488893,
        "mdate": 1701465488893,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces TrueTeacher, a method to enhance factual consistency evaluation in summarization by generating extensive training data. TrueTeacher utilizes various summarization models to create machine-generated summaries on a large scale and employs a 540B large language model to assess the factual consistency of these summaries. This approach can be seen as a distillation method, with the 540B LLM as the teacher model for a smaller student model (T5). Models trained using TrueTeacher and ANLI data achieve state-of-the-art performance on the TRUE benchmark. The authors demonstrate its effectiveness through a controlled analysis comparing TrueTeacher to other methods such as FactEdit, DocNLI, FactCC, and FalseSum, especially in out-of-domain evaluations.\nAdditionally, TrueTeacher is proven to be applicable and beneficial in multilingual settings. The paper also promises to release 1.4M generated data for the research community's benefit. Even though the paper is well-written and includes comprehensive ablation studies and human evaluations of model-generated summaries, its technical contributions are limited."
            }
        },
        "id": "hlYCKrjWUl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rRwPzcSFeL",
        "replyto": "rRwPzcSFeL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1539/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516924,
        "cdate": 1696707516924,
        "tmdate": 1701465433832,
        "mdate": 1701465433832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a method to automatically generating questions via LLMs and augment it as demonstrations for open domain multi hop reasoning. Their pipeline includes reasoning chain composition, duplication control and other aspects. \n\nStrengths:\nThe generation pipeline for ODMR datasets by LLMs’ self-generation and their method (SP-CoT) built upon the generated ODMR datasets. This yields good results on larger-scale (175B) and smaller-scale (13B) LLMs and high-quality intermediate reasoning steps.\n\nWeaknesses:\n- (Main concern) Comparison with (a) non-LLM approaches, and (b) with HotpotQA leaderboard systems, and (c) conceptual comparison with existing approaches with similar pipeline including decompositions (incl. recent open-domain QA with or without retrieval) ; would add depth to the paper. \n- Missing ablation studies of the pipeline. This will also strengthen the paper.\nPlease include the newly reported results on additional CoT baseline empirical studies in the next version."
            }
        },
        "id": "akVtfRLUtj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rQJAaOh4nr",
        "replyto": "rQJAaOh4nr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission434/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487631,
        "cdate": 1696707487631,
        "tmdate": 1701465398823,
        "mdate": 1701465398823,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper tackles the issue of answer scoring based on LM probabilities for common-sense related tasks. The main idea is to use a weighting sum of answer token probabilities, where the weights are derived by path-based scores calculated from ConceptNet. The answer set is also expanded to allow for better scoring. \n\nThe problem and solution are well-motivated. The main issue of obtaining answer probabilities via an aggregation of token probabilities is an important one. The proposed method of finding weights on the answer token probabilities is reasonable.  The experimental evidence is mostly sound with some minor caveats. The experiments were are all done on GPT-2 a relatively weaker model compared more recent ones (e.g. T5, UnifiedQA) of similar sizes. The paper uses ConceptNet as extra information during answer scoring. A tighter claim on answer token weighting would have been to include ConceptNet paths via other means in the model (e.g. as inputs to the model).  There are adequate analyses of the components in the paper. The writing is clear overall. \n\nThe work has significance within the scope of selection technique in common-sense QA tasks. The main contribution is in showing that weighting token probabilities helps for answer selection. This is a neat idea that is relatively simple to implement and use. Another plus for significance is that it adds to methods that tackle calibration of LM probabilities, which allow us to get more out of LMs when used as black-boxes.\n\nI am less excited about the broader significance of the work for the following reason. The overall idea seems similar to rescoring methods in generation tasks, where a base LM generates multiple candidates and some scoring method rescores the outputs, which can also be integrated into the beam search. It is not clear if there was some new challenge or twist to address in implementing the idea for answer selection."
            }
        },
        "id": "J3KLEhojVX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rMLnxh4oT5",
        "replyto": "rMLnxh4oT5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1256/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508721,
        "cdate": 1696707508721,
        "tmdate": 1701465425389,
        "mdate": 1701465425389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes VivesDebate-Speech, which augments the VivesDebate corpus with spoken argumentation in an attempt to leverage audio features for argument mining tasks. The size (about double the size of a comparable audio dataset) and versatility of the corpus have shown to be very interesting for the research community, it offers the chance for AM and speech processing communities to work together, and it highlights the benefit of integrating audio features in computation argumentation tasks.\n\nThe reviewers generally agreed that the paper is well written and clear. The most significant issue brought up in the reviews was the lack of detail in the description of both the resource and the experiments (which the reviewer also sees is an issue related to space restrictions). The concern was outlined in detail and the authors thoroughly addressed each point in the author response. I believe their response should be sufficient to dampen the concerns brought up."
            }
        },
        "id": "aeQQqC8RGB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rLx2eDYcMK",
        "replyto": "rLx2eDYcMK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission41/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477557,
        "cdate": 1696707477557,
        "tmdate": 1701465385013,
        "mdate": 1701465385013,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces GRENADE, a self-supervised learning (SSL) approach for text-augmented graphs that leverages both pre-trained language models (PLMs) and graph neural networks (GNNs). GRENADE proposes two novel SSL algorithms, Graph-centric Contrastive Learning and Graph-centric Knowledge Alignment. The former improves the representation of PLM and GNN encoding by incorporating neighborhood information, while the latter aligns the representations of PLM and GNN. The experiments conducted on various graph-specific downstream tasks demonstrate the effectiveness of GRENADE over baselines.\n\nThe strengths of the paper lie in its novel content and clear presentation. GRENADE introduces innovative SSL objectives that effectively leverage both PLM and GNN representations. The alignment module addresses a crucial need in multimodal learning, and the ablation results validate its effectiveness. The paper is easy to follow, making the proposed method accessible to the reader. Additionally, the extensive experiments provide substantial evidence of GRENADE's performance superiority over existing methods.\n\nWhile the paper presents several strengths, there are some areas that could be improved. It would be valuable to explore GRENADE's performance on non-homophilous graphs to assess the robustness of the proposed SSL objectives in diverse scenarios. The computational effort required by GRENADE is not discussed, which is an important consideration, particularly when relying on PLMs. Comparisons with graph-only SSL approaches like Auto-SSL could provide insights into the effectiveness of incorporating PLMs. Additionally, including results from Graph Transformer Networks in Table 2 would enable a comparison with other graph-based SSL methods like MLP and GraphSage."
            }
        },
        "id": "6sLx0qnkJL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rKjzOYrXKd",
        "replyto": "rKjzOYrXKd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2611/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545815,
        "cdate": 1696707545815,
        "tmdate": 1701465470737,
        "mdate": 1701465470737,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper establishes two sources of hallucinations in LLMs: memorization and statistical patterns. While reviewers see merit in the work, they also express reservation about whether the conclusions are sufficiently supported."
            }
        },
        "id": "0Y2k5M8VEt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rJhk7Fpnvh",
        "replyto": "rJhk7Fpnvh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3880/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578356,
        "cdate": 1696707578356,
        "tmdate": 1701465512622,
        "mdate": 1701465512622,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new method for aspect sentiment triplet extraction. The authors propose a new approach to enhance the supervision of encoder and decoder's hidden representations in Natural Language Generation-based methods for ASTE, integrating sequence tagging probabilities and semantic representations of labels. This leads to new SOTA results. The minor points for improvement include more discussion of the application possibilities of the proposed methods, as well as a comparison to more prior methods, both of which have been provided in the author response."
            }
        },
        "id": "wuvSdw9xNF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rJXYb7D4ck",
        "replyto": "rJXYb7D4ck",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4975/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707603203,
        "cdate": 1696707603203,
        "tmdate": 1701465546214,
        "mdate": 1701465546214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The work presents two valuable datasets on negation and role reversal and evaluate the performance of 22 models, showing that previous findings might have been skewed due to smaller test sets. \n\nThe three reviewers agree on their soundness and excitement scores, all on the upper bound, from good to strong/transformative, and the vast majority of comments and concerns are addressed on the rebuttal. These should be included in the final version of the manuscript."
            }
        },
        "id": "5KD0HcmGpT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rIc17Kziiq",
        "replyto": "rIc17Kziiq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4233/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585840,
        "cdate": 1696707585840,
        "tmdate": 1701465524936,
        "mdate": 1701465524936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary of the paper: The core idea presented in this paper is that for long-range language modeling and summarization, it is more critical to capture long-range dependencies early in the computation by using full attention in the bottom layers and sparse attention in the latter layers.\n\nPros:\n- The simplicity of the idea makes it easy to replicate\n- Their proposed setup works with pre-trained model\n\nCons:\n- **Time and memory experiment** \nEfficient transformers are useful because they reduce the compute and time complexity of transformers from quadratic in the sequence length to something less than quadratic in the sequence length. Since the proposed method uses fully attention as the first few layer the computational and memory requirement is $O(c_1L^2 + c_2Lk)$ where  L is the sequence length, k is the local attention block size, $c_1 + c_2$ is the total  number of transformer layers. So theoretically the proposed method is quadratic in sequence length. \n\nNow, it is true that even if theoretically $L^2$ will dominate at large $L$ we could still think that the whole computation will be done much faster at lower $L$. First this is a drawback of the method not reflected in the experiments/analysis. Second, this needs to be demonstrated by measuring the computational time and memory required rather than “attention cost”. The reason is that compute time will be throttled by the lower layer, something that is not reflected in the “attention cost” metric. Moreover this tradeoff needs to be better analyzed in the paper.\n\n- **Core idea of the paper and comparison to baselines**\nMultiple Sparse Attention Transformers [1, 2, 3, 4, 5, 6] have been proposed over the past 4-5 years. The idea presented in the paper is combining a few fully attending layers at the bottom with local attention layers at the top. The closest idea to the paper is using few fully connected tokens (i.e. tokens that have full attention) in each layer. This has been explored  in *several* papers [4, 5, 6] along with an analysis of their compute and memory requirement [7]. Given the relationship of this paper with such previous work, the lack of comparison to efficient transformers, which are similar in principle, is another major concern for this paper."
            }
        },
        "id": "NRzsV4jp94",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rI7ebWPRLr",
        "replyto": "rI7ebWPRLr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission826/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497002,
        "cdate": 1696707497002,
        "tmdate": 1701465411883,
        "mdate": 1701465411883,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper demonstrates a relationship between the norm of word embeddings, the extent to which a word's co-occurrence probabilities diverge from the unigram distribution (\"information gain\"), and the informativeness of the word with respect to several tasks. \n\nThe reviewers agreed that the research was sound. Opinions were generally positive on \"excitement.\" One concern, which I shared, was that the results pertain only to static word embeddings. During the discussion period, the authors added a follow-up experiment showing similar results for contextualized embeddings (presumably averaged?)."
            }
        },
        "id": "quV5XQiVUT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rHjZFQvj9k",
        "replyto": "rHjZFQvj9k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission901/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498775,
        "cdate": 1696707498775,
        "tmdate": 1701465414162,
        "mdate": 1701465414162,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a noteworthy addition to the field of table question answering (TQA) research. The authors present a new dataset and offer a comprehensive baseline evaluation of this innovative dataset, illustrating the substantial challenges it presents even to modern large language models. It stands out from other datasets, as many lack annotations for multi-step reasoning. CRT-QA incorporates a reasoning chain, delineating the step-by-step process required to derive answers from the table, which is a valuable feature in the current TQA landscape.\n\nWhile not primarily a modeling paper, this work also introduces an intriguing approach to solving the task, which can be readily applied to other TableQA datasets. Additionally, the authors compare their modeling approach with several other few-shot approaches, encompassing various reasoning types, and providing a thorough analysis.\n\nThe authors have effectively addressed the reviewers' concerns by promptly conducting additional experiments to furnish more evidence, thereby offering a more holistic understanding of their proposed model."
            }
        },
        "id": "ztTPiCRQQP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rG3QZA7JXV",
        "replyto": "rG3QZA7JXV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5033/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605604,
        "cdate": 1696707605604,
        "tmdate": 1701465548048,
        "mdate": 1701465548048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a two-stage framework PGIM that utilizes ChatGPT to generate auxiliary knowledge for Multimodal Named Entity Recognition (MNER). Experimental analysis and comparison are comprehensive. Important details, such as the annotation process and examples, are missing in the submitted version (provided in the rebuttal). The performance gains of the MSEA module are somehow slight."
            }
        },
        "id": "fC9eB2zBef",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rDuv0LGf3T",
        "replyto": "rDuv0LGf3T",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission389/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486271,
        "cdate": 1696707486271,
        "tmdate": 1701465397239,
        "mdate": 1701465397239,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper tackles multi-document summarisation, by introducing a novel idea that uses simplicial complex and sheaf graph attention to capture complex and diverse cross-document relationships. Reviewers all thought the paper is generally well-structured, the experiments are extensive, and the results are impressive. Some concerns are raised by the reviewers, and the two main ones that I can see are: (1) certain parts surrounding the methodology are difficult to follow, and they require more description (e.g. explaining the idea of simplicial complex) and motivation (e.g. why do we need simplicial complex?); and (2) the human evaluation results require some clarification - in particular its inter-annotator agreement and the conclusion drawn from the ChatGPT results. That said, I believe the contribution of the paper is significant, and that these concerns are presentational issues that could be fixed in the revision."
            }
        },
        "id": "ZeUXzaL7Ov",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rBrzSCruKl",
        "replyto": "rBrzSCruKl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4176/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584778,
        "cdate": 1696707584778,
        "tmdate": 1701465523138,
        "mdate": 1701465523138,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper focuses on the ability of LLMs to learn nover interpretations of word and phrases via in-context learning. Specifically, they introduce a new text-to-SQL semantic parsing  dataset called MAGNIFICo which contains diverse tokens and prompt settings. Theirexperiments indicate that LLMs do resonably well on this task but there is still room for improvement.\n\nOverall, reviewers agree that the task is interesting and the paper is well-written with analysis and experiments. While it does have some drawbacks ((1) only one task for studying a broader phenomenon, and (2) fewer data points for some settings), the contributions are above the threshold for acceptance."
            }
        },
        "id": "aWZbvNDIIb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "rBfVlElyVW",
        "replyto": "rBfVlElyVW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2169/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536021,
        "cdate": 1696707536021,
        "tmdate": 1701465456546,
        "mdate": 1701465456546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes to fine-tune pre-trained language models on raw HTML texts in order to improve their abilities to \"navigate\" webpages in raw HTML. To test this, the authors release a benchmark of 3 related HTML understanding tasks: two from previous work and a novel one.\n\nReviewers agree that the new benchmark is an important contribution, and that the proposed idea is simple and strongly effective, which is showcased with extensive experiments and analyses. One reviewer highlights that the model could potentially \"be deployed to many applications to help users with disabilities, such as visually impairments\".\n\nThe main concern shared by reviewers is that experiments did not include large language models, which have been shown to have great performances in several tasks. The authors argue that they follow a \"privacy-oriented on-device approach\" and they aim to deploy these models in personal devices where large models may not be able suitable. I believe this justification is valid, but perhaps needs to be strengthen in the paper. There are also some references missing from other related areas that the authors have promised to include and have already proposed how they will be described in the paper."
            }
        },
        "id": "2YpfNhVEKu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "r65IWQmsHF",
        "replyto": "r65IWQmsHF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2050/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533053,
        "cdate": 1696707533053,
        "tmdate": 1701465451781,
        "mdate": 1701465451781,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper leverages LLMs to significantly reduce the effort required for generating tests for K-12 students. Reviewers are highly positive about the approach and evaluation, giving high soundness scores (4). Concerns are raised regarding the lack of technical innovation (qWhv), missing related work (dxrP), and the limited range of the question types and options (CGAU).\n\nI recommend accepting this paper to the main conference. Overall, this is solid research that illustrates a very reasonable use case for LLMs. The concerns are minor and do not overshadow the contributions. However, the authors should take them seriously and make an effort to address them in the next version."
            }
        },
        "id": "HBIZLFODQk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "r3utB5u4zP",
        "replyto": "r3utB5u4zP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4876/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707600031,
        "cdate": 1696707600031,
        "tmdate": 1701465543815,
        "mdate": 1701465543815,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This study introduces a dataset and a method for analyzing generated text. The research delves into existing methods and identifies their limitations. Subsequently, the proposed method is evaluated on 15 LLMs.\n\nAll reviewers acknowledged the paper's timely contribution, which promises significant benefits for the community. Reviewer WEM6 expressed a concern about the name \"AI Detectability Index\", this could be taken into account while improving the paper.\n\nReviewer weMu emphasized the importance of understanding the method's generalizability to other languages. Including a discussion on this aspect would greatly benefit readers."
            }
        },
        "id": "W0u7kdfgqJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "r2z3qPltxs",
        "replyto": "r2z3qPltxs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2092/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534261,
        "cdate": 1696707534261,
        "tmdate": 1701465453746,
        "mdate": 1701465453746,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper is clear, well written, and presents significant results. It combines sound modeling, thorough linguistic understanding, and meaningful connections to human cognition. It is particularly well suited to represent this track."
            }
        },
        "id": "J2MiR1EqBE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qzYtTabDPY",
        "replyto": "qzYtTabDPY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3938/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579828,
        "cdate": 1696707579828,
        "tmdate": 1701465514794,
        "mdate": 1701465514794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers agree that this is a sound and exciting paper with good results and novelty. Ablations studies should be added to improve the paper."
            }
        },
        "id": "qgRpYyRs9h",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qyvabTsnWg",
        "replyto": "qyvabTsnWg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2645/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546621,
        "cdate": 1696707546621,
        "tmdate": 1701465472000,
        "mdate": 1701465472000,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Thank you to authors for their work and reviewers for their reviews. This paper builds a new data set in which participants describe their appraisals and emotions during the usage of products or services of personal significance.\n\nPros:\n- Reviewers agree that the data set, which aims to capture self-reported cognitive appraisals and emotions is novel in design and captures dimensions not present in current data sets\n- Reviewers agree that the data set collection is well-thought-out, with a pilot study to determine the best format and contextualization within prior work\n\nCons:\n- After reviewing the paper myself as well as the reviews and the author responses, I am missing some of the main motivation of the work. The authors state: \"Therefore, the lack of a product/service review dataset containing emotionally-rich variables such as cognitive appraisals and emotional intensity is a primary motivation for the proposed dataset.\" While reviewers (and I) agree that a dataset like this does not currently exist, that does not address why such a data set should exist. The only direct use case I can think of for studying emotions in product reviewers is to improve targeted advertising. If the authors have other use cases in mind, I would suggest adding them to the paper if it is accepted.\n- Emotion classification in general is an ethically complex topic: [1] outlines some of the debates regarding NLP specifically. I think the paper can more thoroughly address the ethical implications of their work, not only in the ethical implications section, but also in clarifying the limitations of their dataset throughout the work, for example, how post-hoc descriptions of emotions may differ from actual emotions. Given that the authors are planning to release the data, it would be useful to add more guidance on how future researchers should and should not use it to help prevent future researchers from making misinterpretations or inaccurate claims based on this data.\n- Reviewers note areas for clarification in the details of the data set. These look like minor clarifications to me, some of which are addressed in the appendix and could be moved to the main paper for a final version.\n\nOverall, I think this is a very interesting and well-collected data set and my concerns with the work are primarily about potential misuse.\n\n[1] Mohammad, Saif M. \"Ethics sheet for automatic emotion recognition and sentiment analysis.\" Computational Linguistics 48.2 (2022): 239-278"
            }
        },
        "id": "JFT4jxSYc5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qvftjm8DNC",
        "replyto": "qvftjm8DNC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2537/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544281,
        "cdate": 1696707544281,
        "tmdate": 1701465468284,
        "mdate": 1701465468284,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a new approach to multimodal NLU using multimodal large language models. Two reviewers see a lot of potential in the paper, one is more reserved, but their reviews are less substantiated. The authors addressed several of reviewers’ concerns."
            }
        },
        "id": "dSS8tFqXIh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qtZI5YDe5d",
        "replyto": "qtZI5YDe5d",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission458/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488317,
        "cdate": 1696707488317,
        "tmdate": 1701465399651,
        "mdate": 1701465399651,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses the problem of length bias in reward modelling in RLHF frameworks, which may lead to longer outputs even if they contain nonsensical tokens. It proposes Product-of-Experts framework to mitigate this bias using two novel expert models. The paper presents comprehensive experiments and analyses to support their claims. Further evidence and/ or discussion is needed to underline the frequency of the length bias problem and the effectiveness of this approach in addressing the length bias, as this is the main problem outlined in the paper. The presentation can be improved with more detailed explorations of specific topics as outlined by the reviewers for the revision."
            }
        },
        "id": "UC4xfd7qaE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qq6ctdUwCX",
        "replyto": "qq6ctdUwCX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5318/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610826,
        "cdate": 1696707610826,
        "tmdate": 1701465556000,
        "mdate": 1701465556000,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a framework that enables LLMs to proactively ask relevant questions to unveil more details in the image and evaluate the method on two visual QA datasets, OK-VQA and A-OKVQA. I think the idea explored in this paper is interesting, the performance gain given the complexity of additional modules seems somewhat limited, as pointed out by the reviewers. I am also curious whether this method can give further improvements on top of existing models on other tasks, as suggested by the reviewer QtV8"
            }
        },
        "id": "wo5cvoK4TV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qo17ZiVnH2",
        "replyto": "qo17ZiVnH2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2734/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548393,
        "cdate": 1696707548393,
        "tmdate": 1701465474724,
        "mdate": 1701465474724,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new approach to entity linking using generative models, by combining instruction tuning with a retrieval model. In doing so, they achieve improvements on both speed and linking metrics.\n\nReviewers were impressed by the experimental design and results (\"extensive, meticulously documented experimental results\"). The major concerns on soundness were with respect to lack of details regarding the effect of model size, and insufficient ablations. The second of these was resolved in the rebuttal. The first seems to still be somewhat of an issue, but the authors are limited somewhat by what models are available.\n\nAlthough one reviewer was only ambivalent in terms of excitement, the others were more positive, including the most confident reviewer. The reviewer that currently has an ambivalent rating raised many useful points, and believes that the resulting changes will greatly benefit the paper. No reviewers seemed to think this work would be especially transformative, but all seemed to think it worthy of publication."
            }
        },
        "id": "Lik0HomCBP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qnO9IRNA9d",
        "replyto": "qnO9IRNA9d",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission750/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495177,
        "cdate": 1696707495177,
        "tmdate": 1701465409283,
        "mdate": 1701465409283,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "We thank the author for their insightful paper. This work presents a dictionary-based mask-language modeling (MLMs) pretraining that improves cross-lingual transfer. It first motivates the training method based on a careful analysis of the multilingual internal structure of MLMs (e.g. XLM-R). It then introduces TCS and SCS, which expands masked-language modeling based on dictionary-based cross-lingual pairs and on-the-fly cross-lingual mined candidates. \n\nReasons to Accept\n- Clarity of argumentation (R1, R2)\n- Extensive empirical evidence that the introduced method leads to better cross-lingual transfer (R1, R2, R3)\n\nReasons to Reject: \n- Novelty of the results: Dictionary-based training for multilingual modeling has been explored in the past (e.g. Cao et al 2020; Wang et al 2022). The paper would benefit from a more detailed discussion and comparison with these papers (as pointed out by R3)\n\nSuggestions: \n- Tables 12 and 13 are tough to read without bolding the best scores. \n- One of the arguments in the paper is that monolingual corpora do not include any parallel data or embedded text and still XLM-R-like models are able to learn geometrically-aligned representations. However, e.g., Wikipedia is somehow parallel across languages (many documents are shared across languages). Additionally, many documents include code-switched text. The paper would benefit from a careful discussion of this point and how this could also explain the multilingual abilities of multilingual MLMs. \n\nMissing references\n- Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation, (Wang et al 2022)\n- First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT,  (Muller et al 2021)"
            }
        },
        "id": "aVO9N81bnx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qnHB2SMQLA",
        "replyto": "qnHB2SMQLA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2718/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548068,
        "cdate": 1696707548068,
        "tmdate": 1701465474281,
        "mdate": 1701465474281,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel approach that enables referring video object segmentation (R-VOS) models trained on clean text-video pairs to adapt to noisy speech as referring guidance and maintain robust performance. The authors achieve this by using Noise-Aware Semantic Adjustment for precise semantics extraction from noisy speech without using ASR and Semantic Jitter Suppression, which makes R-VOS models more robust to noisy queries. Experiments on three challenging benchmarks demonstrate the superiority of the proposed method compared to state-of-the-art approaches.\n\nThe reviewers praised the novelty of the proposed solution, many practical applications, clear writing and robust experimental analysis. The authors successfully answered all of the reviewers' questions during the rebuttal."
            }
        },
        "id": "tf6o9sSj9p",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qlwXv0oHJD",
        "replyto": "qlwXv0oHJD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission14/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476862,
        "cdate": 1696707476862,
        "tmdate": 1701465384119,
        "mdate": 1701465384119,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigated in the construction of a new instruction tuning dataset for logical chain-of-thought reasoning with GPT-4. The authors showed the competitive logical reasoning and general inference abilities with the proposed approach. This paper addressed an important research problem for instruction tuning of LLM with logical reasoning. The reviewers found the experiment results strong and convincing. On the down side, two reviewers expressed concerns on the experiment setting, related to choice of reasoning types, comparison to baseline models, and the potential data contamination. One reviewer commented on the lack of the evaluation on the quality of the proposed dataset. The authors were able to provide the human evaluation of the dataset which the author should consider add to the revised version of the paper. The paper presentation also has room for improvement as commented by two reviewers."
            }
        },
        "id": "FJuAozM94W",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qlCtkvgQJH",
        "replyto": "qlCtkvgQJH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2957/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552839,
        "cdate": 1696707552839,
        "tmdate": 1701465482322,
        "mdate": 1701465482322,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses the issue of \"pronoun-dropping\" in Chinese-English translation. \nA dataset with careful pro-drop annotation is created, consisting of four domains: talk, drama, movie, and vlog. \nThe paper also proposes a novel method to address the pro-drop problem, and the results show that it outperforms existing approaches. \n\nA few revisions are necessary, which are already provided in authors' response."
            }
        },
        "id": "rSu1jV47jY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qiV0mvkVyq",
        "replyto": "qiV0mvkVyq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2355/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540248,
        "cdate": 1696707540248,
        "tmdate": 1701465462453,
        "mdate": 1701465462453,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Focusing on the ACL Anthology from 1979 to 2022, this paper analyzes the correlational and causal effects of tasks, methods, datasets and metrics (TDMM) on other tasks. For example, the authors measure the causal effect of a change in the “BLEU” score appearing in papers on frequency shifts in the “machine translation” task. \n\nOverall, reviewers found the paper sound and exciting. A few reviewers noted that the findings were pretty well-known by the NLP community. However, the approach the authors took to get there is extremely thorough and sound: from TDMM Entity Extraction to regression analysis on the entity influence, to using state-of-the-art causal discovery and causal estimation approaches to estimate the final causal effects. Even if these relationships could have been highlighted by researchers in the NLP community a-priori, I’m guessing that those researchers would not be able to *quantify* the causal effects. \n\nThis seems like an interesting, well-developed, and extensive empirical pipeline that the rest of the community could have as an example for other large-scale data driven analyses with text and causal inference."
            }
        },
        "id": "fmFoEkVRh0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qhwYFIrSm7",
        "replyto": "qhwYFIrSm7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1285/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509366,
        "cdate": 1696707509366,
        "tmdate": 1701465426214,
        "mdate": 1701465426214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Overall Assessment: The paper titled \"Offensive Tweets with Homoglyphs: A Benchmark Dataset for Hate Speech Detection\" presents a novel dataset and associated experiments for hate speech detection in tweets containing homoglyphs. The reviewers have provided diverse opinions on the paper, which warrant careful consideration.\n\nScores: Based on the provided reviews, the average Soundness score is approximately 3 (Good, with some minor support needed). The average Excitement score is 3 (Ambivalent, with merits but key weaknesses). These scores indicate a paper with valuable contributions but room for improvement."
            }
        },
        "id": "2dsu2wHExd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qegD54EWAl",
        "replyto": "qegD54EWAl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2136/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535286,
        "cdate": 1696707535286,
        "tmdate": 1701465455302,
        "mdate": 1701465455302,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel counterfactual-based analysis framework to investigate the causal effects of factual knowledge injection on pre-trained language model performance within the pretrain-finetune paradigm. The authors challenge existing assumptions by demonstrating that the correctness of injected knowledge has only a limited impact on downstream performance. The findings provide valuable insights and contribute significantly to the understanding of the role of factual knowledge in language models. The reviewers are generally exciting about the insights of this paper. I would therefore recommend to accept this paper to the main conference."
            }
        },
        "id": "eQsevCoXl9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qae0FlfrG6",
        "replyto": "qae0FlfrG6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2525/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543939,
        "cdate": 1696707543939,
        "tmdate": 1701465467766,
        "mdate": 1701465467766,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel unsupervised dependency parsing approach based on LLM's attention heads. It offers a theoretical foundation for the method and empirical evaluations, revealing especially promising results for more challenging phenomena, such as long-distance subject-verb agreement. \n\nIn the case of this submission, the scores of the reviewers are unanimous. It is a **strong submission** -- both regarding soundness (score 4) and excitement (score 4). Among the strengths of the paper, the reviewers mentioned a lot of very important points:\n* solid theoretical foundations\n* simple and well-targeted method\n* great methodology, well-written and well-structured paper\n* detailed analysis helping to understand a particular issue\n* clear future work directions\n\nAfter the first round of reviews, the biggest weakness of the paper were unclear details (e.g., sentence generation, head selection). However, as reviewer 5gCw said in the discussion, *\"the authors went above and beyond in their response to answer [reviewer's] criticism\"*. As a result, the concerns of two reviewers eased after the rebuttal phase. They encourage the authors to clarify the details of the paper and improve smaller presentation issues for the camera-ready version."
            }
        },
        "id": "xR46iN7rjx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qZwsO1Qi3V",
        "replyto": "qZwsO1Qi3V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2568/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544973,
        "cdate": 1696707544973,
        "tmdate": 1701465469314,
        "mdate": 1701465469314,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a 2-step framework using large language models (LLMs) to augment aspect-based sentiment analysis (ABSA) datasets, aiming to improve performance. The framework involves prompt-based explanation generation and two explanation utilization strategies. \n\nReasons to Accept:\n- The proposed framework demonstrates a significant overall improvement in aspect-based sentiment analysis (ABSA) across multiple benchmark datasets.\n- The study features robust experiments that compare various approaches on the specific task, providing strong empirical evidence.\n-The novelty of leveraging large language models (LLMs) for data augmentation in ABSA\n-The study presents two effective training approaches, enhancing the overall quality of the proposed methods.\n-The methods are intuitive, and the extensive experiments offer a deep analysis from various perspectives, including text length, alternative text augmentation methods, robustness, and generalization.\n\nReasons to Reject:\n- The observed improvement achieved with the proposed framework is relatively limited when compared to other data augmentation baselines. Given the resource-intensive nature of the framework, this raises concerns about its cost-effectiveness.\n- Relying solely on unexamined and potentially biased language model-generated explanations may not be a reliable approach, as language models may not consistently provide accurate or comprehensive explanations. A closer investigation of LLM-generated explanations and the application of quality control measures are needed.\n- The paper lacks transparency regarding the snapshot version and parameters of the LLMs used (GPT-3.5 and MOSS), which affects reproducibility.\n- The absence of an error analysis to validate the claim is a significant drawback."
            }
        },
        "id": "2Dd0R3heks",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qWbCkbBN1P",
        "replyto": "qWbCkbBN1P",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2996/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553788,
        "cdate": 1696707553788,
        "tmdate": 1701465483829,
        "mdate": 1701465483829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers generally agreed that the paper presents a useful model for distantly supervised NER. The proposed prototype-based network, which incorporates multiple prototypes for each entity class, was praised for its ability to capture intra-class variance. The paper also introduces a denoised optimal transport algorithm to address issues of label noise.\nThe reviewers acknowledged the clear presentation, solid experiments, and extensive analyses performed in the paper. \n\nHowever, there were some criticisms and areas for improvement highlighted by the reviewers. One reviewer pointed out inconsistencies in the presented data and comparisons to previous work, potentially caused by the differences in encoders (via author rebuttal). Nevertheless, this should be discussed and presented in the paper properly (e.g., does the proposed method still achieve SOTA performance?) Another reviewer suggested comparing the results with more datasets and configurations to further validate the approach. There was also a call for error analysis and a high-level summary of the denoised optimal transport algorithm to aid readers unfamiliar with the topic."
            }
        },
        "id": "2tVcJhPWsQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qT4bw58Yl2",
        "replyto": "qT4bw58Yl2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission197/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481557,
        "cdate": 1696707481557,
        "tmdate": 1701465390765,
        "mdate": 1701465390765,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The main conclusions of the reviews and the post-rebuttal discussions:\n\n3/ 3 reviewers consider the paper sound (scores 5, 3, 3)\n3/ 3 reviewers find the paper exciting (scores 5, 3, 3)\n\nFrom reading the rebuttal and seeing the scores above, I find that the reviewers consider strong points for soundness the following:\n- This paper discusses an important topic for NLP research community, i.e., the discrepancy between the existing state of NLP research and the needed NLP directions from real-world applications\n- Well-designed and well-explained experimental and evaluation setup\n- The comparison of the types of questions asked by users to GPT versus the tasks defined in NLP datasets is novel and insightful, and it identifies gaps in user demands that the NLP literature can focus on.\n- Concrete insights are provided, like trends toward task diversity, open-endedness, and marginalized user groups. Challenges posed by real user needs are summarized, providing a potential roadmap for future work\n\nHowever, there are a few concerns expressed by the reviewers regarding the ethical implications to aligning LLMs with user needs, the difficulty of replicating the annotations given the reliance on GPT-4, and the focus on just two datasets might not reflect all real-world situations. \nThe reviewers respond to these concerns and plan to address them in the limitation section of their paper."
            }
        },
        "id": "vm7k3yD8OI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qS1ip2dGH0",
        "replyto": "qS1ip2dGH0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4451/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589782,
        "cdate": 1696707589782,
        "tmdate": 1701465531902,
        "mdate": 1701465531902,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper looks at the task of determining if a give phrase or text evokes visual imagery or not. They motivate this by stating that understanding the visualness of text can be a precursor step to help generation models  and retrieval models (focus on just the visual text). They curate a dataset of 3620 sentences for this task, and fine tune large vision language models like CLIP to identify visual text. \n\nThe reviewers were able to understand the contributions of the paper and 2 of 4 reviewers found it exciting, and all reviewers thought at least the major claims in the paper supported sufficiently (soundness=3). There were some clarifications regarding better ways of quantifying the visualness, and also corrections in the baseline formulations that the reviewers suggested. The authors have responded to those, and indicated they will correct their work based on the feedback. Overall the paper will improve and benefit from the feedback suggested by the reviewers."
            }
        },
        "id": "mYdssNYdmi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qRg3AxBDnN",
        "replyto": "qRg3AxBDnN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1857/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528897,
        "cdate": 1696707528897,
        "tmdate": 1701465445009,
        "mdate": 1701465445009,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper is a survey on how human feedback is used for learning in large language models. The authors present a literature review as well as a set of challenges and recommendations for the future. The paper seems like a good fit for the theme track.\n\nPros:\n* The literature review is thorough and the authors clearly describe their process for the review, including selection of papers, etc.\n* The paper is well-organized and the challenges and recommendations mentioned can be useful to other researchers\n\nCons:\n* The reviewers mention a few minor points related to clarity and wording, which can be improved for the camera-ready"
            }
        },
        "id": "kbjwGtm05e",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qRbhKhqp0b",
        "replyto": "qRbhKhqp0b",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4548/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592772,
        "cdate": 1696707592772,
        "tmdate": 1701465534367,
        "mdate": 1701465534367,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work tackles counter-narrative generation for hate speech on social media. Unlike existing works largely leveraging the hate speech itself, this work uses argumentative structures from the context. Results show that adding argumentative structures helps boost the results even in a few-shot setting.\n\nReviewers appreciated the importance of the task, effective use of argument information and the two datasets constructed for this study.\n\nAt the same time, reviewers are concerned about the limited details on the annotation process, the discussion on evaluation, and the limited experiments conducted to underpin the main claims of the paper. More discussions on the annotation and evaluation will be helpful to the community."
            }
        },
        "id": "5O4bMgrz6I",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qPfQq8c3kv",
        "replyto": "qPfQq8c3kv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5806/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618641,
        "cdate": 1696707618641,
        "tmdate": 1701465567466,
        "mdate": 1701465567466,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new metric for summarization, which does not require manual annotations. The reviewers agree about the soundness of the submission but are split between \"strong\" and \"ambivalent\" on excitement. There is some debate about the relationship between reference-free segmentation *evaluation* and segmentation *algorithms*, since the former seems to imply the latter. Another excitement issue is the degree of innovation over the Davies-Bouldin segmentation algorithm, which is the foundation for the approach."
            }
        },
        "id": "2hg2C4WQp0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qPIV6XQizX",
        "replyto": "qPIV6XQizX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4776/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598323,
        "cdate": 1696707598323,
        "tmdate": 1701465541350,
        "mdate": 1701465541350,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper builds a few-shot learning based model for multimodal NER.  There has been a very little attempts in this area, and hence it makes a contributions. There are certain aspects that need to be addressed: few shot setting for the NER; use of image captions and its impact on the overall model performance, and the use of ChatGPT in proper way."
            }
        },
        "id": "Xoii7K2tVC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qOOQW9DcpF",
        "replyto": "qOOQW9DcpF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1363/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511153,
        "cdate": 1696707511153,
        "tmdate": 1701465428513,
        "mdate": 1701465428513,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors studied the problem of uncertainty calibration on the summaries from PLM. This is an interesting problem to NLP community. \nThe authors investigate different probabilistic methods in addressing the problem. The experiments were on T5-base and results showed that these methods can improve the uncertainty quality. \nA weakness of this paper is that evaluation was carried out on T5-base. It is not an LLM in light of the current progress. Nevertheless, the results on T5-base look thorough."
            }
        },
        "id": "HsjKPVKa1Q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qMSG8S7zh0",
        "replyto": "qMSG8S7zh0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission651/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492971,
        "cdate": 1696707492971,
        "tmdate": 1701465406569,
        "mdate": 1701465406569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This is an interesting paper and will be useful in the relatively new Sign Language Understanding subfield. The new dataset the authors propose, especially integrating a new style of modality in Sign Language (handshapes) would be impactful for multimodal tasks other than Sign Language Understanding as well. \n\nPros:\n- New innovative dataset annotated and publicly available as part of this work with good potential for future work\n- Interesting step forward for Sign Language Recognition research\n\nCons:\n- Dataset might have noisy labels due to single annotator\n- Lack of Multimodal integration"
            }
        },
        "id": "8sLNO1VDoL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qJqJXpysnh",
        "replyto": "qJqJXpysnh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4466/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707590066,
        "cdate": 1696707590066,
        "tmdate": 1701465532288,
        "mdate": 1701465532288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a joint learning framework based on contrastive learning to extract and generate keyphrases. Concerns lie in the novelty over UniKeyPhrase and the motivation of the contrastive learning framework (partially addressed in the rebuttal). Comparison under the beam search setting is also needed."
            }
        },
        "id": "ev1Qo5VEJl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qGr17uesSx",
        "replyto": "qGr17uesSx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1000/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707501879,
        "cdate": 1696707501879,
        "tmdate": 1701465417359,
        "mdate": 1701465417359,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces LEXTREME, a new multilingual benchmark for evaluating natural language processing (NLP) models on legal and legislative text data. The paper chooses popular models and fine-tune them on the the final benchmark and showcase moderate performance indicating more work needs to be done to improve performance on this benchmark. The key contributions are:\n\nThe paper compiles 11 existing legal NLP datasets covering 24 languages from 8 language families into a unified benchmark.\nThe work provides baseline results using 5 popular multilingual encoder models (DistilBERT, MiniLM, mDeBERTa, XLM-R base/large) as well as monolingual models.\nOverall, this paper makes a valuable contribution to introducing a standardized multilingual benchmark for the legal domain, which has traditionally relied more on monolingual resources. The benchmarks and baselines could aid the future development and evaluation of multilingual legal NLP models.\n\nReasons To Accept:\n- The benchmark would be useful to the community to track multi-lingual progress in the NLP domain.\n- The paper highlights moderate performances of the baseline models and the underperformance of ChatGPT highlighting gaps to be filled in this domain.\n- This work brings together, in a single accessible space most of the open datasets in the legal domain. This leads to a new multilingual and multi-task benchmark for the legal domain. Furthermore, this is a living benchmark, with a public leaderboard and full access to code to run experiments.\n\n\nReasons To Reject:\n- The work does not provide any model, data, or evaluation novelty and is just a collection of other datasets and models that are already used by everyone in the community. \n- While standardizing the benchmark is a good contribution, the work is just incremental in that all the datasets are already available and people can already evaluate models on these datasets."
            }
        },
        "id": "emo5c6W6J9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qE5vtBMbCJ",
        "replyto": "qE5vtBMbCJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission754/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495214,
        "cdate": 1696707495214,
        "tmdate": 1701465409390,
        "mdate": 1701465409390,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main conclusions of the reviews and the post-rebuttal discussions:\n- 3/ 3 reviewers consider the paper sound (scores 4, 3, 3)\n- 2/ 3 reviewers find the paper exciting (scores 4, 2, 3)\n\nFront reading the rebuttal and seeing the scores above, I find that the reviewers consider strong points for soundness the following:\n- In-depth exploration of 3 RQs, on the math and problem-solving abilities of LLMs and the use of LLMs to identify errors in student solutions to math problems\n- especially RQ1 and RQ2 seem to contain valuable analysis, in particular for developing better tutors with LLMs\nConcerns regarding the use of just 1 annotator were clarified in the rebuttal as it is challenging to find an expert in both math and education."
            }
        },
        "id": "1NIT8GKsn5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "qDspFDJEHP",
        "replyto": "qDspFDJEHP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission889/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498508,
        "cdate": 1696707498508,
        "tmdate": 1701465413767,
        "mdate": 1701465413767,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces an innovative temporal question-answering task along with a dataset designed for semi-structured tables (info-boxes), which is a well-justified endeavor. The authors' commitment to elaborate more about the motivation behind this task, in response to reviewers' suggestions, is noteworthy. The paper's clarity in presentation and the comprehensive explanation of the data collection process are also notable strengths. Furthermore, the thorough and easily understandable future work section greatly enhances the paper's significance and its contribution to the research community.\n\nI believe that the paper is of high quality. However, I think that if the authors had suggested some modeling modifications to existing methods in order to solve the task, it would have been a great nominee for the best paper award."
            }
        },
        "id": "L1oy4PkuES",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "q8aTDcIXnO",
        "replyto": "q8aTDcIXnO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5195/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608885,
        "cdate": 1696707608885,
        "tmdate": 1701465552603,
        "mdate": 1701465552603,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses simultaneous machine translation (SiMT), which performs translation based on partial input and is useful for real-time translation. A trained SiMT is prone to suffer from the anticipation problem, i.e., the training loss encourages the SiMT to hallucinate. This paper addresses the problem by training from a wait-k non-autoregressive model, which tends to generate less anticipating samples (better aligned with the source order).\n\nOverall, the idea is analogous to using KD to address the multimodality issue of NAT -- I am not saying the paper is the same as KD-NAT, but they share the same spirit. \n\nReviewers generally find the idea makes sense and intuitively it may mitigate the anticipation problem of SiMT.\n\nHowever, the paper has a few issues. \n1. The paper lacks clarity and some formulations are sloppy. For example, p(y|x,y) is not sound.\n2. Some design choices are not clearly explained, such as why choosing CTC-NAT as the tailored reference. In the author response, the author suggest it's for efficiency concerns, but training efficiency is not analyzed in the paper.\n3. Experimental details are unclear, especially the RL part, which may hinder the replication of this paper."
            }
        },
        "id": "sDnQsHDutZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "q88QSsc75T",
        "replyto": "q88QSsc75T",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1007/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502266,
        "cdate": 1696707502266,
        "tmdate": 1701465417571,
        "mdate": 1701465417571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a multi-path voting technique called Dynamic Voting, specifically Confidence-based Dynamic Voting (CDV) and Percentage-based Dynamic Voting (PDV). The technique is designed to reduce the number of reasoning paths during multi-path voting while maintaining accuracy through early exit strategies. The authors evaluate CDV and PDV on various reasoning tasks under few-shot and zero-shot settings and provide comprehensive analyses.\n\nReviewers appreciate the innovative approach of using Dynamic Voting with early exit strategies to reduce computation while maintaining accuracy. This technique is seen as a valuable contribution to the field. The paper is commended for its comprehensive evaluation, including a wide range of datasets and settings. This thoroughness adds to the credibility of the proposed technique. The clarity of writing is acknowledged as a positive aspect of the paper, making it easy for readers to understand the proposed approach and its results. Some reviewers express concerns about the limited improvement achieved by the proposed method over existing approaches. In particular, the concerns centered around a minor improvement in terms of efficiency, and it is noted that the gains in terms of time or memory consumption may be limited, especially for larger batch sizes. \n\nThe paper presents an innovative approach to improving efficiency in LM decoding through Dynamic Voting with early exit strategies. The approach is novel and well-explained, however further clarification regarding the practical benefits, especially in terms of resource consumption, could strengthen the paper's contribution."
            }
        },
        "id": "hzGJNnbi8P",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "q7IvUsjEkb",
        "replyto": "q7IvUsjEkb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2770/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548982,
        "cdate": 1696707548982,
        "tmdate": 1701465475667,
        "mdate": 1701465475667,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a useful and intuitive concept of task-level thinking steps to help LLMs overcome any bias brought in by demonstration examples in in-context learning setups. It also proposes a framework to revise the thinking steps for confusing classes in classification tasks, by having one LLM correct another on hard examples. It provides comprehensive experiments with several relevant baselines, and demonstrates robustness to biased demonstrations and ability to generate chain-of-thought explanations without human involvement. \n\nConcerns include the generalisability of the prompt templates (and the manual effort in designing them) to other classification tasks, and the lack of sufficient baselines for more comprehensive comparisons. \n\nFor the authors — please include the cost analysis and a discussion on how to use the framework when the training sets are small in the paper. It would also be useful to open-source the code for future works on this effort."
            }
        },
        "id": "80D7EzzQLM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "q4oWkMHkQx",
        "replyto": "q4oWkMHkQx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3112/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556392,
        "cdate": 1696707556392,
        "tmdate": 1701465487494,
        "mdate": 1701465487494,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces the surgical fine-tuning method that only fine-tunes a selection of layers. The authors propose Fisher Information Matrix (FIM) score for the layer selection. They compare the proposed method with full model fine-tuning and show that their method achieves strong performance in certain tasks.\n\nAs mentioned by the reviewers, the paper presents a sound idea and executes it in a straightforward experimental setup. Additionally, the authors provide further analysis to understand the impact of linguistic features, and distributional shifts on the FIM and performance of their method. \n\nHowever, as reviewers noted and also acknowledged by the authors, the paper lacks important comparisons with several baselines. This includes both similar methods such as AutoFreeze to justify that the proposed method is advantageous in certain conditions, and also existing parameter-efficient fine-tuning methods such as LoRA. This comparison should include the performance metric as well as the potential benefits of using this method such as parameter or sample efficiency."
            }
        },
        "id": "2cO70yoYHy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "q0c1JTukWE",
        "replyto": "q0c1JTukWE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4258/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586231,
        "cdate": 1696707586231,
        "tmdate": 1701465525769,
        "mdate": 1701465525769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the task of repository-level code completion. They introduce a new dataset – RepoEval, and a new framework RepoCoder that incorporates a similarity-based retriever and a pre-trained code language model. \n\nThe paper is clear and well-written. The experiments are thorough. The proposed RepoCoder approach outperforms the in-file baseline. The work is solid and should be of interest to researchers working on code completion. \n\nThe question and concern regarding tuning the hyper-parameters on the test data should be addressed in the paper. It should be clarified whether the test data is used for finetuning, and even if it’s only done at the inference stage, it’s still problematic. Adding  a comparison with commercial applications as baselines would strengthen the paper."
            }
        },
        "id": "8UE21GEEdB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "q09vTY1Cqh",
        "replyto": "q09vTY1Cqh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2432/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542112,
        "cdate": 1696707542112,
        "tmdate": 1701465464950,
        "mdate": 1701465464950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper focuses on the use of influence scores to select important examples for fine-tuning transformer-based models. There is a consensus on the clarity and readability of the paper. The paper is well-written and easy to follow. Reviewers also agree that the paper receives praise for its extensive set of experiments, which includes both offline and online A/B tests, and using the techniques in a production environment is a strong aspect of the study. This practical orientation is seen as valuable to the NLP community. Besides, the proposed influence score techniques are seen as potentially valuable for practitioners. They can help identify subsets of training data that are most important for model performance, facilitating better annotation strategies.\n\nAlthough the reviewers do not have any major concerns, they question the relevance of the challenge mentioned in the paper's introduction, specifically regarding the selection of important examples for fine-tuning transformer-based models. They argue that this might not be a significant challenge anymore, particularly with larger models like LLMs. The paper should address this concern by providing more context and justification for this challenge. One reviewer expresses their concern about the limited scope of the study, as it focuses on a controlled setting using the SNLI dataset. They worry that the inferences drawn from this specific dataset may not generalize well to a wider range of problems, datasets, and experimental settings. The paper should acknowledge this limitation and discuss its implications.\n\nIn conclusion, the paper is commended for its clarity, experimental rigor, and potential utility for practitioners in the NLP community. However, it needs to address concerns related to the relevance of the challenge and the generalizability of its findings. Addressing these concerns and providing more context could strengthen the paper's overall contribution."
            }
        },
        "id": "iWNW9KB1nQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pyjppDCsq7",
        "replyto": "pyjppDCsq7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3680/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567616,
        "cdate": 1696707567616,
        "tmdate": 1701465506502,
        "mdate": 1701465506502,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reveiwers have reached consensus in their reviews. While the result are reasonable, they are not conclusive in setting out the advatages of AutoPlan, the presenation can be improved to highlight the value of the approach, i.e. exanding on Table 4 in the paper. The paper should be updated with the results in the rebutal."
            }
        },
        "id": "g6cspSJflk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pxscU6TidP",
        "replyto": "pxscU6TidP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3629/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566540,
        "cdate": 1696707566540,
        "tmdate": 1701465504695,
        "mdate": 1701465504695,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new Visual Grounding (VG) metric for VQA aiming to balance both faithfulness and plausibility, i.e. (i) if a model identifies question relevant objects in the scene and (ii) actually relies on the information contained in the relevant objects when producing the answer.\n\nThe majority of reviewers felt that the paper is well written with conclusions backed up by experiments. The new metrics give new insights that are not captured by existing ones, and can inspire the design and evaluation of new methods.\n\nThere were some concerns over writing and structuring of the paper, as well as concerns regarding some design decisions in the metric by reviewer 9Aq7, which the authors adequately addressed during discussion. Reviewer bzSZ raised several issues as well, in particular that the metric overlaps slightly with other investigations regarding metrics for grounding, which deserves additional comparisons. I believe the authors gave satisfactory responses regarding the differences in their method, but I would still strongly suggest adding a discussion wrt these other metrics. How can researchers decide which of these metrics to use or optimize their models for? Can we study the similarities and differences between these metrics systematically?"
            }
        },
        "id": "Vtxg3nx2de",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pvEkYbUPVW",
        "replyto": "pvEkYbUPVW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3075/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555727,
        "cdate": 1696707555727,
        "tmdate": 1701465486329,
        "mdate": 1701465486329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are generally in agreement about the quality of the idea and execution of the experiments in this paper. The one point of disagreement is one reviewer's concern about the unreliability of GPT-based metrics; however (as another reviewer and the authors point out) reliability is a challenge across many of today's evaluation practices (including human evaluations). The authors agreed to discuss this topic in more depth, particularly in the limitations section, which I think is sufficient, given the scope of the paper."
            }
        },
        "id": "sX0uNWXcPi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "puMfaHb1hY",
        "replyto": "puMfaHb1hY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4295/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586907,
        "cdate": 1696707586907,
        "tmdate": 1701465526919,
        "mdate": 1701465526919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposed a method to improve results in open-domain QA. Typically, open-domain QA is done by 1) retrieving relevant docs, 2) running a \"reader\" that reads the question + the retrieved documents and outputs the answer. The proposed method focuses on filtering out noisy retrieved documents. It allows the reader to output \"no answer\" if the retrieved document is not useful. \n\nThe model is implemented using FLAN-T5-XL-3B and using OPT-IML-MAX-1.3B. Results show performance gains on NQ, TQA, WebQ, SQuAD. \n\nReviewers found the result to be generally sound. \nSome reviewers complained about the choice of the base models, which I don't think is a fair criticism; 3B is a reasonable size + not every researcher can run the largest available model.\nOthers commented on the novelty of the work, and yes, the idea of giving the model the ability to say \"I don't know\" is not new. The details of how it is implemented are a little different from prior work, though.\n\n\nNit: I agree with R1 that using the word hallucination in this context is inaccurate. Hallucination usually means generating output not supported by the input. Here, the output is supported by one of the retrieved documents, it just happened that it was the wrong document."
            }
        },
        "id": "aWyVjesykx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "puLH3BEl93",
        "replyto": "puLH3BEl93",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5259/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609950,
        "cdate": 1696707609950,
        "tmdate": 1701465554388,
        "mdate": 1701465554388,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new task called Multi-Modal Perspective-based Educational Dialogue Summarization (MM-PerSumm), which focuses on summarizing a student-virtual tutor dialogue from three different perspectives – that of a student, a tutor, and generic. For example, a summary from the student’s perspective may focus on the student’s progress and outcomes, while that from the tutor’s perspective should focus on the tutoring strategy and efficacy. To that end, the authors introduce a novel dataset, CIMA-Summ, and build a novel multi-modal Image and Perspective-guided Dialogue Summarization (IP-Summ) model that accepts image and text as inputs and constructs a dialogue graph, capturing the intentions and actions of both the virtual tutor and the student. Experiments suggest that the model performs better than the baselines and holds potential in advancing AI-driven education.\n\nThe reviewers have identified the following **contributions and strengths** of this paper:\n1. *Novelty of the task*: Reviewers gdPt and q3T3 consider that the authors propose an innovative and important task, and that summarizing student-virtual tutor conversations from different perspectives is beneficial.\n2. *Dataset*: All reviewers find the extended CIMA dataset, augmented with multiple summaries from different perspectives, promising and potentially useful.\n3. *Model*: Finally, all reviewers also agree that the proposed model is interesting and seems to contribute to the summarization quality. Reviewer gdPt also mentions that the authors run ablation studies and provide comprehensive analysis.\n\nAt the same time, all reviewers have identified **weaknesses and areas for further improvement**. Specifically:\n1. *Technical rigor*: Reviewer uLLc points out that with the small dataset of 1,000 articles the training and evaluation splits matter greatly when reporting results, but the analysis of this seems to be missing from the paper. Given a relatively small number of articles, running experiments using cross-validation would be more appropriate than a single data split.\n2. *Limitations of the dataset*: Reviewer uLLc mentions that the student's response side appears to be rather template-based, which might not warrant the complicated graph-based approach to encode the information. Moreover, the dialogues included in the data are restricted in terms of the number of turns. Reviewer gdPt expresses further concerns regarding the dataset – for those, please refer to the original review.\n3. *Limited comparison of the results*: Reviewer uLLc highlights that the baselines considered in this paper date back to 2021 and more recent work has not been taken into account (the missing references are provided in the review).\n4. *Further analysis of the model and the results* is needed, as reviews, in particular from reviewers q3T3 and gdPt, suggest.\n\nIt is also clear from the reviewers' questions that many important technical details are missing in the paper. The authors did a thorough job answering the questions and even running additional experiments and analysis – all these need to be included in the revised version of the paper."
            }
        },
        "id": "jBNay3ikcI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ptcFuwr4YD",
        "replyto": "ptcFuwr4YD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5031/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605560,
        "cdate": 1696707605560,
        "tmdate": 1701465547969,
        "mdate": 1701465547969,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces an adaptive textual label noise learning framework, incorporating both an adaptive warm-up stage and a hybrid training stage that utilizes pre-trained models to address label noise. Additionally, it presents a dynamic early stopping method for the warm-up phase, evaluates the model's adaptability to various noise scenarios, and integrates generalization strategies in the hybrid training stage to optimize the utilization of noisy data. Experimental evidence demonstrates the framework's robust performance across different noise scenarios, including those with a noise range of up to 40% and mixed noise types.\n\nThere is a consensus among reviewers regarding the significance of the label noise problem in the fields of NLP and ML. Most reviews emphasize that the experiments demonstrate the robustness and effectiveness of the proposed method, particularly in synthetic noise scenarios. However, some concerns center around the simplicity of the tasks and the nonapparent computational complexity analysis of the method. It would be beneficial for the authors to offer further clarification regarding these computational complexities and the potential challenges stemming from the multitude of ad-hoc parameters and choices in future versions of the paper. Addressing these points would enhance the paper's applicability to real-world scenarios where label noise is prevalent and facilitate future iterations of this research."
            }
        },
        "id": "BK1LGyWJuu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "psv7operF8",
        "replyto": "psv7operF8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1440/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707513781,
        "cdate": 1696707513781,
        "tmdate": 1701465431263,
        "mdate": 1701465431263,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper focuses on personalized dialogue generation by leveraging external story documents. In particular, a retriever and a generator are jointly trained, which eliminates the need for annotated retrieval datasets. The retriever is utilized to extract top-k relevant stories.\n\nIn general, the reviewers agree that the paper is well-motivated and the proposed model is well-designed. However, reviewers have pointed out that more recent cutting-edge baselines and traditional knowledge-grounding baselines should be included. Reviewer d41o also pointed out that the model architecture should be described in more detail and the paper has some minor formatting issues to fix."
            }
        },
        "id": "9G0Nz1waeq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ppb7gyhc7k",
        "replyto": "ppb7gyhc7k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission396/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486508,
        "cdate": 1696707486508,
        "tmdate": 1701465397455,
        "mdate": 1701465397455,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper examines the hallucination problem by categorizing these by their category (acronym ambiguity, numeric nuisance, generated golem, virtual voice, geographic erratum, time wrap), degree (mild, moderate, alarming), and orientation (factual mirage or silver lining). They then provide two methods to mitigate these issues. The authors provide a sample of 75k examples of these hallucinations with 2k annotated samples. They also come up with an index (HVI) that allows them to rank LLMs by their likelihood of generating hallucinations. The reviewers agree that the paper is sound and are generally excited about the work. They provide several valid criticisms regarding the limitations of the HVI metric and somewhat simple mitigation strategies. However, this paper has a wide array of contributions that should be beneficial for the community. The fine-grained types of hallucination and annotations will probably be the most valuable component, though future work may build off of the other contributions. It is reasonable that the scope of a paper cannot include everything that the reviewers have brought up, though I think their suggestions will be a helpful guide for future work."
            }
        },
        "id": "V4A1sWrpON",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ppaIkXurvg",
        "replyto": "ppaIkXurvg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2112/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534766,
        "cdate": 1696707534766,
        "tmdate": 1701465454534,
        "mdate": 1701465454534,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers cite the motivation and clarity of the experimental design as strengths of the paper.\n\nThere were some concerns about limitations of this work, e.g., whether the resources/models used in the pipeline approach would be effective in another domain, and whether the evaluation used competitive baselines. Discussion with the authors led two reviewers to slightly increase their scores, though excitement scores remained borderline."
            }
        },
        "id": "o7DyLJdt96",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pnnab961TD",
        "replyto": "pnnab961TD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2716/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548016,
        "cdate": 1696707548016,
        "tmdate": 1701465474166,
        "mdate": 1701465474166,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All authors found merits in the submission, especially in the improvement of efficiency in reranking process. Although some reviewers pointed out the problem of failing to show FLOPs improvement explicitly, the authors described these information in the rebuttal. I strongly suggest the authors revise the manuscript per the comments of the reviewers."
            }
        },
        "id": "UeffRqERbn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pkZcvEYZEm",
        "replyto": "pkZcvEYZEm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3204/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558107,
        "cdate": 1696707558107,
        "tmdate": 1701465490213,
        "mdate": 1701465490213,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper conducts a comprehensive analysis of visual programming in the context of visual question answering (VQA), with a particular focus on ViperGPT. The key contributions include highlighting the power of question decomposition in VQA tasks, proposing an iterative prompting-based approach for visual question decomposition, and offering insights into the performance gain achieved by visual programming in ViperGPT. Overall, this paper offers valuable insights into the role of visual programming in VQA and presents an alternative approach to question decomposition. Both the AC and reviewers are excited about this work. With some improvements in organization and additional context in the revision, it can make a significant contribution to the field."
            }
        },
        "id": "S5TtOxB5uE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pk0OZZkYMP",
        "replyto": "pk0OZZkYMP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4481/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707590651,
        "cdate": 1696707590651,
        "tmdate": 1701465532747,
        "mdate": 1701465532747,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviews have a consensus on positive evaluations for both soundness and excitement.\n\nRegarding excitement, the paper is the first paper that introduces the task of novel relation detection as an out-of-scope (OOS) detection problem.  The core three methods of pseudo-OOS data generation are largely inspired by prior work (as described in L557-560), and thus the novelty on this aspect is relatively weak.\n\nWith respect to soundness, the proposed method is well motivated by problems with existing OOS detection methods and well tested by a comparison with various OOS detection baselines on two benchmark datasets.  The ablation study shows the contribution by each of the three data generation methods."
            }
        },
        "id": "ukKUtb0KiN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "piC2Dm47U1",
        "replyto": "piC2Dm47U1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1236/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508060,
        "cdate": 1696707508060,
        "tmdate": 1701465424734,
        "mdate": 1701465424734,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method using an LLM (GPT) to automatically remove “noise” from the target side of noisy parallel data (the MTNT dataset, en-fr and en-ja), to make the dataset more useful for testing the robustness of MT models. They compare the performance on 3 input types: monolingual (just the target sentence), bilingual (provide both source and target) and translation (provide just the source sentence). They focus the evaluation on 4 known types of “noise”: spelling/grammar errors, emojis, slang and profanities and compare again a rule-based correction tool. The best method depends on the noise type (e.g. bilingual working best for emojis/slang and the correction tool is best for spelling/grammar errors). They then test the new corpus C-MTNT on the MT task and show that the bilingual method leads to the greatest gain in performance when training MT models to produce clean target sentences from noisy source data. \n\nThe paper’s motivation was well appreciated by the reviewers and the experiments are interesting and useful for future research. The authors’ extensive rebuttal provided additional experimental details and justification some design choices, answering most of the reviewers’ comments. The paper was initially lacking some human evaluation which was provided during the rebuttal. I recommend including all this in the paper."
            }
        },
        "id": "o9ViVcLWzn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pi764D1Xrx",
        "replyto": "pi764D1Xrx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission932/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499637,
        "cdate": 1696707499637,
        "tmdate": 1701465415267,
        "mdate": 1701465415267,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The dataset introduced in the paper is an interesting contribution to the field of dialogue systems. It addresses a need for realistic, multi-user task-oriented dialogues and offers a comprehensive data collection process and insightful result analysis. This dataset is likely to aid researchers in advancing the development of dialogue systems for complex, real-world scenarios."
            }
        },
        "id": "jFSPhJkNtM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "phJtMADSdy",
        "replyto": "phJtMADSdy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1225/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507804,
        "cdate": 1696707507804,
        "tmdate": 1701465424193,
        "mdate": 1701465424193,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces SALT, a novel method utilizing unlikelihood training with human edits to enhance clinical text summarization. Reviewers find the concept of incorporating human edits well-motivated and appreciate the comprehensive experiments and ablations. However, concerns about presentation clarity in certain sections and the effectiveness of imitation edits are raised. Reviewers suggest broader comparisons with prior work, the adoption of established edit-based metrics, and a deeper exploration of alignment between AI-generated summaries and ground truth."
            }
        },
        "id": "XbuCeKIFTF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pgEIr2HY2E",
        "replyto": "pgEIr2HY2E",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4157/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584355,
        "cdate": 1696707584355,
        "tmdate": 1701465522344,
        "mdate": 1701465522344,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper provides a thorough analysis of ChatGPT's performance on extractive summarization and contrasts it with conventional fine-tuning techniques. The study is performed on multiple benchmark datasets.  The experimental results show that ChatGPT performs better on LLM-based assessment measures than existing supervised systems, but performs worse on extractive summarization tasks as measured by ROUGE scores. The authors also explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing chatGPT's performance on extractive summarization. Finally, the research reveals that an extract-then-generate pipeline with ChatGPT significantly outperforms abstractive baselines in terms of summary faithfulness. \n\nThe following pros were revealed by reviewers: \n1. It is the first attempt to extend the application of ChatGPT to extractive summarization and evaluate its performance.\n2. Investigation of the effectiveness of in-context learning and chain-of-thought reasoning for extractive summarization using ChatGPT.\n3. Experiments with the extract-then-generate framework that significantly outperforms the generated summary faithfulness compared to abstractive baselines. \n4. The paper is well-written and the experiments with ChatGPT for extractive summarization are well organized and explained.\n\nSome drawbacks were also noticed, such as using only one LLM and a limited number of documents in the experiments. The authors addressed all the comments and justified their choices."
            }
        },
        "id": "Wx8VGNddat",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pfeod9GPAw",
        "replyto": "pfeod9GPAw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission440/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487821,
        "cdate": 1696707487821,
        "tmdate": 1701465399030,
        "mdate": 1701465399030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies an interesting problem. It first shows the performance variations across large language models (LLMs) given the same set of prompts. It then adaptively optimizes the prompt for each LLM given a specific downstream task. The proposed method, MAPO, mainly consists of an SFT stage and an RL stage. The RL stage is composed of three main components: PPO, SFT approximation and maintaining generalization. The proposed method yields promising improvements.\n\nAs noted by the reviewers, the original submission had quite a few omissions such as ablations (SFT, temperature variations) as well as confusions such as the use of \"Human feedback\" while they actually just do model feedback; \"most effective candidate prompts that will produce outputs closely aligned with the ground-truth outputs, not with the outputs generated by the original prompt as initially stated\", etc. The authors have provided the required information in their detailed rebuttal. These should be added in the main paper / supplementary."
            }
        },
        "id": "QuGM4bimQo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "paUJOst3OE",
        "replyto": "paUJOst3OE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission373/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485912,
        "cdate": 1696707485912,
        "tmdate": 1701465396525,
        "mdate": 1701465396525,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates the use of calibrated models to address two trade-offs for dialogue systems that generate and execute code based on user input: (1) cost/speed vs. accuracy in annotation, and (2) usability vs. safety in task-oriented user interfaces. \n\n- The first question is addressed by showing the benefits of manually annotating only low confidence samples, although the experiments rely on a simulated annotator, which is always correct, thus limiting the generalizability of the results.\n\n- For the second question, safety is viewed as rejecting an unsuccessful program before executing it, while usability is not clearly defined but is operationalized as the percentage of inputs for which a program is executed. Here, the paper shows that confidence-based thresholding reduces the number of incorrect executions at the cost of coverage, and that rephrasing a user's query for low confidence outputs yields safer (ie higher confidence) outputs. This system is evaluated in a user study with 8 MTurk annotators.\n\nOverall, the paper represents an interesting focused contribution that is appropriate for a short paper.\n\nI will note that it was not very clear to me what is gained by looking at these two problems jointly in a single short paper. I would recommend either addressing that point more directly, or focusing the paper on the second study."
            }
        },
        "id": "GqLXsyz1vd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pYRCUypbuq",
        "replyto": "pYRCUypbuq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2025/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532512,
        "cdate": 1696707532512,
        "tmdate": 1701465451093,
        "mdate": 1701465451093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper applies Chain of Thought reasoning to personality detection using large language models. Their method is based on commonly used personality questionnaires (MBTI and Big 5), but re-structured for use with  LLMs in a multi-turn dialogue. The reviewers initially raised issues related to comparisons to previous work, which were addressed by subsequent experiments during the discussion period. The reviewers also questioned the framing of the task as Chain of Thought, given the lack of a concrete reasoning process. This point was discussed at length, and the authors promised to reconsider their framing. Overall, reviews are positive."
            }
        },
        "id": "JvnDsHCUir",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pW6xXXnCQu",
        "replyto": "pW6xXXnCQu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission21/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477045,
        "cdate": 1696707477045,
        "tmdate": 1701465384356,
        "mdate": 1701465384356,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents SPARROW, a multilingual benchmark dataset for evaluating cross-lingual sociopragmatic understanding.  SPARROW consists of 169 datasets for 64 different languages across 13 task types divided into six umbrella categories of sociopragmatic meaning.  Using a proposed SPARROW score, an unweighted average across dataset-specific metrics, the paper evaluates a number of LLMs in different settings (zero-shot for generative models, finetuning for encoder-only models) on their ability to perform sociopragmatic understanding tasks.  They found that overall, smaller, fully finetuned models surpassed larger generative models in the zero-shot setting.\n\nOverall, the paper was well-received by reviewers, who all appreciated the authors’ work in curating a comprehensive benchmark for sociopragmatic tasks across multiple languages and releasing it to the public.  In particular, reviewer fC9A highlights how useful the proposed benchmark would be to the broader NLP community in evaluating the capabilities of LLMs.  Reviewers B91m and GKU9 also appreciated the detailed set of experiments and analyses across various dimensions, such as the impact of finetuning and instruction tuning, performance across tasks and languages, and the impact of using translated prompts and inputs.  \n\nThe major concern highlighted by reviewers B91m and GKU9, however, is that for the larger models, the submitted paper only presents results in a zero-shot setting without exploring few-shot in-context learning.  Additionally, both reviewers raised questions about how robust the results of their analyses were to the choice of prompt, as in the original paper, prompts were tailored to each task rather than to each model tested.  However, both of these concerns were addressed during the rebuttal/discussion period, with the authors presenting new results with few-shot ICL and prompt variation.  If accepted, the paper would be strengthened with the inclusion of those new results in the camera-ready version.  One potential concern, however, is how substantially the conclusions about overall performance over different models may change with the full set of results, as so far, the authors have not presented ICL results for their best generative model (ChatGPT) or one of the strongest tasks for generative LLMs.\n\nThe other concern raised by reviewer fC9A about the paper is the lack of theoretical grounding in how tasks were chosen and categorized for the benchmark.  The authors clarify their use of the term “sociopragmatic” and the motivation behind their grouping of selected tasks in their rebuttal.  While the rebuttal does not fully address the issue of the lack of theoretical motivation in how tasks were selected and grouped, the motivation behind the paper would nevertheless be strengthened by the additional discussion about the definition of sociopragmatics and the interaction between language, identity, and interaction in the rebuttal."
            }
        },
        "id": "Hy8SKCKDnI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pQFgViJp77",
        "replyto": "pQFgViJp77",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2245/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537768,
        "cdate": 1696707537768,
        "tmdate": 1701465458765,
        "mdate": 1701465458765,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers generally agreed that the studies were well designed and the discussion was well written. Reviewer YiDy appreciated both. Reviewer YiDy also emphasized the timeliness of this analysis of different Human-AI Interaction paradigms. Reviewer DQTz appreciated the human-rated news dataset.\n\nReviewer YiDy had a valid concern that the guidance variant is highly constrained and it should be user-specified. The authors acknowledged it and suggested making edits to the discussion to reflect that. They also had a concern that subjectivity remains unaddressed in the work.The authors pointed to their use of TACT framework. Finally they had a concern about the scale of the study, and the authors pointed out that the same size is consistent with the prior work.\n\nReviewer Nsef also had similar concerns about lack of control in evaluation metrics as well as not allowing users to prompt/interact with the model. The authors also made the novelty relative to Cheng et al.’s wizard-of-oz framework, and pointed out that they used an actual AI-powered system for news text generation.\n\nReviewer DQTz had most notable concerns about the methodology including limited number of articles and task difficulty balance. The authors acknowledge it in the rebuttal."
            }
        },
        "id": "VJ1ur2A472",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pPiJykFn0K",
        "replyto": "pPiJykFn0K",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1992/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531635,
        "cdate": 1696707531635,
        "tmdate": 1701465449647,
        "mdate": 1701465449647,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies how the compression of LLMs affects their social biases. This kind of study is very timely as efficient versions of LLMs are becoming more widespread and may affect users in unpredictable ways. An interesting aspect of this study is that it ablates its results over model size and training steps, by comparing different model checkpoints. The findings are non-trivial, as the Authors find that quantisation and distillation can reduce bias. However, the results on the SEAT dataset remain mixed. Moreover, some conclusions regarding the effect of model size based on Pythia seem to be contradicted by BERT-style models. Both of these points warrant further discussion on possible explanations, but overall I recommend to accept this paper."
            }
        },
        "id": "UhT8plmFwz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pO7YD7PADN",
        "replyto": "pO7YD7PADN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3695/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567813,
        "cdate": 1696707567813,
        "tmdate": 1701465506805,
        "mdate": 1701465506805,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work investigates a unique and interesting application (supporting experimental scientists by assisting with protocol generation) and presents a new evaluation set and method for self-evaluation. Though there are valid concerns among reviewers that evaluating GPT-4 with GPT-4 will have misleading biases (and also questions about training set representation), these experiments and resources are likely to prove insightful to practitioners."
            }
        },
        "id": "ziMdDvQbTH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pMCRGmB7Rv",
        "replyto": "pMCRGmB7Rv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3418/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562272,
        "cdate": 1696707562272,
        "tmdate": 1701465496806,
        "mdate": 1701465496806,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This position paper presents a forward-looking study of LLMs over next-level NER research by introducing three new challenges including fine-grained, zero-shot based on labels, and retrieval for all instances for the query entity NER tasks. Reviewers appreciate that it presents insightful directions for future LLM and NER research. Reviewers share the concern and confusion about the paper contribution: while it is a position paper, it also provides a silver-annotated dataset but no baseline; the quality and usefulness of the silver-annotated dataset are uncertain; two out of three tasks are already explored in previous work. The authors provide effective responses to address these concerns. Overall, we strongly encourage the authors to incorporate the feedback to improve the paper."
            }
        },
        "id": "pRZhFZzBWY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pJwlMI7AYm",
        "replyto": "pJwlMI7AYm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3906/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578870,
        "cdate": 1696707578870,
        "tmdate": 1701465513511,
        "mdate": 1701465513511,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a novel approach called Cross-Lingual Prompting (CLP)  to improve zero-shot chain-of-thought reasoning across language. The experimental analysis show improvements over multilingual reasoning benchmarks MGSM and XCOPA.  The paper is well written and the approach and experiments are described well. \n\nThe draft can be improved by including more related literature work, adding discussion on few-shot prompting and including the analysis discussed in the rebuttal."
            }
        },
        "id": "bMJnyIjGgc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pHwLbEkB0J",
        "replyto": "pHwLbEkB0J",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2741/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548476,
        "cdate": 1696707548476,
        "tmdate": 1701465474799,
        "mdate": 1701465474799,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Solid contribution to the open-source large language models for low-resourced languages -- Finish in this case. Extensive analysis, translated evaluation set and great results demonstrating how LLMs could be used effectively for low-resource languages."
            }
        },
        "id": "nXlxy1plZW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pHrNmdzX2C",
        "replyto": "pHrNmdzX2C",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5840/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707619283,
        "cdate": 1696707619283,
        "tmdate": 1701465568081,
        "mdate": 1701465568081,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces \"FlowSUM,\" a novel variational encoder-decoder framework for Transformer-based summarization that solves two major problems: posterior collapse during training and a lack of semantic information in latent representations. To address these problems, the authors suggest a controlled alternate aggressive training (CAAT) technique with a better gate mechanism and use normalizing flows to enable flexible latent posterior modeling. According to experimental findings, FlowSUM greatly enhances the quality of generated summaries and speeds up knowledge distillation without dramatically lengthening inference times. \n\nPros:\n1. The paper is well-written\n2. The approach is novel\n3. Extensive experiments that show that the proposed methodology works fine\n4. Good ablation study \n\nThe authors addressed all comments (cons) that were raised by the reviewers in rebuttal."
            }
        },
        "id": "I9eWzKRLKw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pGlnFVmI4x",
        "replyto": "pGlnFVmI4x",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission427/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487403,
        "cdate": 1696707487403,
        "tmdate": 1701465398571,
        "mdate": 1701465398571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents an unsupervised method, based on prompting and LLM, for making \"indicative summaries\" from forum discussions. Sentences are clustered and cluster labels are defined, which serve as a guide for reading the forum. Different language models are tested and an evaluation with human annotators is made."
            }
        },
        "id": "u9lQBZOzUq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "pFTBsdZ1UM",
        "replyto": "pFTBsdZ1UM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3409/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562095,
        "cdate": 1696707562095,
        "tmdate": 1701465496515,
        "mdate": 1701465496515,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel learning strategy termed vision-language warm-up tasks for multimodal dialogue models (VLAW-MDM). Remarkably, this strategy avoids dependence on large pretraining or multi-task datasets and instead focuses on learning directly from the target data. The authors propose four distinct warm-up tasks. Furthermore, VLAW-MDM can autonomously generate captions for images, integrating them into the model's input to better contextualize visual information. Within constraints of limited data and smaller model configurations, experimental results indicate that VLAW-MDM offers comparable, and occasionally superior, performance against existing state-of-the-art models on the ImageChat benchmark.\n\n**Soundness Scores**: (3, 3, 3, 3)\nThe presented work exhibits moderate soundness. While the authors have a detailed ablation study for the four warm-up tasks, the experiments are confined to a single benchmark. Notably absent are a case study and error analysis. Additionally, the methodology section lacks a thorough exposition, potentially impeding reproducibility.\n\n**Excitement Scores**: (3, 3, 3, 3)\nThe excitment towards the paper is moderate. The majority of reviewers appreciate the novel direction of relying solely on target data for pretraining and recognize the uniqueness of the techniques introduced. However, the performance improvements observed are marginal. The general applicability of the proposed techniques remains questionable due to an absence of supporting justification. This gap makes it challenging to ascertain the broader utility of the introduced methods.\n\nIn summary, this work is **moderately sound and moderately exciting** ."
            }
        },
        "id": "n6e7ymmqc3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "p2P1Q4FpEB",
        "replyto": "p2P1Q4FpEB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2652/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546806,
        "cdate": 1696707546806,
        "tmdate": 1701465472272,
        "mdate": 1701465472272,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All the reviewers have agreed that the idea in this paper is simple yet effective.\n\nHowever, all the reviewers have concerns on whether the results are convincing enough or not. During the rebuttal period, the authors have addressed some of these concerns.\n\nGenerally, the merits of this paper outweigh its flaws."
            }
        },
        "id": "aUVrZgSCkY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "p0GyMJugcE",
        "replyto": "p0GyMJugcE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3219/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558385,
        "cdate": 1696707558385,
        "tmdate": 1701465490727,
        "mdate": 1701465490727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposed a new approach for extracting person names from the noisy texts. It used heuristic rules and labeling functions the enhance the labeling results. The experimental results show that the F1 scores are increased by 9% compared to the SOTA methods. The paper is well-organized and easy to follow, except for the writing. The baselines and settings are not very suitable. For example, only texting the performance with ChatGPT on the HTName. Conducting more experiments will make the paper more convincing."
            }
        },
        "id": "PFzwsIbMZr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oxZKOzePQX",
        "replyto": "oxZKOzePQX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5089/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606800,
        "cdate": 1696707606800,
        "tmdate": 1701465549722,
        "mdate": 1701465549722,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes XoT to improve math problem solving by LLMs by assembling a diverse group of reasoning paths with verification. Reviewers agree that while the proposed method is not exceptionally novel, it still presents a nice contribution to improving the reasoning capabilities of LLMs, and the experiments have demonstrated solid results. Based on the new results from the rebuttal, we strongly encourage authors to add experiments for more LLMs and reasoning tasks in their final version."
            }
        },
        "id": "aKVRcuROcV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "owc65ImkyU",
        "replyto": "owc65ImkyU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3449/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562955,
        "cdate": 1696707562955,
        "tmdate": 1701465498043,
        "mdate": 1701465498043,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The GeneraL-purpose EveNt benchmark (GLEN) is a notable initiative aimed at advancing event benchmarking. It achieves this by combining a substantial Wikidata-based event ontology, derived from DWD Overlay mappings, with a comprehensive corpus of event annotations. These annotations are created through the mapping of PropBank gold standard event annotations to ontology types, augmented by manual annotation for dev/test sets. As a result, GLEN introduces a novel dataset and presents an innovative event detection method, establishing the first baseline for this dataset. The associated research paper is well-written and effectively illustrated, but it would benefit from reorganizing the related work section to provide better context.\n\nStrengths:\n\n* Innovative Approach and Large-scale Corpus: GLEN introduces a novel dataset and event detection method, contributing to the field of event benchmarking.\n* Well-Written Paper: Oveall the paper is well written and easy to follow.\n\nWeaknesses:\n\n* Quality Assessment: GLEN lacks explicit quality assessment metrics for its dataset, such as a histogram showcasing the distribution of data types.\n* Limited Scope: The focus of GLEN is primarily on event detection, neglecting event roles, which might limit its applicability in certain contexts.\n* Typos and Structure of the Paper: The paper contains a few typographical errors that could affect the overall readability and keeps related work at the end making it hard for casual readers to follow the work."
            }
        },
        "id": "19okoimzRM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ovkb6woHvT",
        "replyto": "ovkb6woHvT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2185/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536478,
        "cdate": 1696707536478,
        "tmdate": 1701465457133,
        "mdate": 1701465457133,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "While the consensus is a high agreement on the novelty and excitement of the work, there is dissent on the soundness. Reviewers however do not raise fundamental concerns with the the underlying method, but with the coverage of the validation.  The authors address the concerns of two of the reviewers by providing updated numbers on impact of fine tuning, and possible drop in performance in quantized model with watermarks. The authors rebutal does not adress the issues of comparable benchmarks, these recommendations are needed to impove the evaluation."
            }
        },
        "id": "DQkcbReSSu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ouiQX2XWYc",
        "replyto": "ouiQX2XWYc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission792/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496011,
        "cdate": 1696707496011,
        "tmdate": 1701465410504,
        "mdate": 1701465410504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a two-stage hierarchical multimodal pretraining framework, named MedHMP, which is designed for Electronic Health Records (EHR) data. This framework aims to address the challenges of managing the hierarchical and heterogeneous nature of EHR data. By performing extensive experiments across diverse clinical benchmarks, the paper convincingly demonstrates that MedHMP outperforms existing baseline models. The hierarchical multimodal approach is well-suited for EHR data, offering a promising direction for predictive healthcare. The reviewers largely agree that the study provides strong support for its claims. Given that the majority of the reviewers found the work both sound and exciting, recommending a strong acceptance seems justified. However, the authors should pay close attention to the detailed questions and concerns raised by the reviewers in future revisions."
            }
        },
        "id": "2uAIetLqTD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oueo4cEgSJ",
        "replyto": "oueo4cEgSJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4083/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582721,
        "cdate": 1696707582721,
        "tmdate": 1701465519600,
        "mdate": 1701465519600,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose an approach for spatial reasoning over text to tackle Spatial Question Answering. The proposed solution disentangles the processes of information extraction and reasoning, and this is demonstrated to be effective on multiple Spatial QA datasets. \nThe paper has some merits, especially the experimental section which is very exhaustive. Still the paper can be largely improved, in particular it can be better organized and made more clear. \n\nFurthermore, the authors proposed three different solutions (a pipeline of linguistic modules, end two end-to-end systems based on pre-trained Language Models) which are tested only on 3 different datasets resulting in no clear best system (depending on the dataset the best solution changes). This is very problematic since it is hard to draw conclusions. What model should a practitioner use? I recommend the authors to try to take the best from each proposed solution and verify whether they can come up with individual model that works well in every setting."
            }
        },
        "id": "xn3u8UcQFA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "osox1GoFLS",
        "replyto": "osox1GoFLS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission153/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480452,
        "cdate": 1696707480452,
        "tmdate": 1701465388859,
        "mdate": 1701465388859,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a method for reducing hallucinations by additionally incorporating a classifier's probability to assess the correctness between the generated text so far during decoding. Reviewers agree that this method is useful, simple (in a positive way), and easy to apply. The idea is also well-motivated, and the authors have shown extensive results.\n\nA common drawback of this work, as raised by reviewers, is that the paper evaluates its method only on a single dataset, thereby raising questions about its generality. Additionally, there is a lack of comparison with other hallucination mitigation baselines."
            }
        },
        "id": "LSa7qlsUB4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oseYM8qxW4",
        "replyto": "oseYM8qxW4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3379/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561523,
        "cdate": 1696707561523,
        "tmdate": 1701465495648,
        "mdate": 1701465495648,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The problem of identifying psychological features was extensively studied, and while improvement is offered by the new method, all in all the result is relatively incremental. During review period, the paper under review has obtained substantial positive feedback, particularly for its high-quality writing and well-articulated methodology. It investigate into a central problem in Natural Language Processing (NLP), offering an innovative approach to predicting an author's personality based on their text. The authors excel in giving a comprehensive overview of methods for extracting personality-related features from text, highlighting the paper's significance in advancing the field.\n\nWhile the paper is largely commendable, there are areas identified for further refinement, which have minor impacts on its overall quality. For instance, the paper could benefit from greater detail to improve its reproducibility. Including specifics about the attention mechanism on numerical sequences and the input representation for the psychological feature encoder would make the work even more robust.\nReviewer 2's insights, though critical, offer constructive avenues for strengthening the paper. The presentation of BigFive and MBTI models could be clarified by consistently referring to them as datasets, unless a more elaborate explanation is provided. Additionally, the inclusion of details on train/validation/test splits and data statistics could significantly enhance understanding and replicability, while also addressing minor issues like typos and presentation inconsistencies.\n\nLastly, while it's noted that the research problem has been explored in prior works, the paper still makes a worthwhile, albeit incremental, contribution to the existing literature. The novelty and quality of the paper indicate that with a few refinements, it could serve as an impactful piece in the NLP community. Overall I would like to see this paper into the poster sessions."
            }
        },
        "id": "D61qleQXcx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "orefzVRWqV",
        "replyto": "orefzVRWqV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission231/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482473,
        "cdate": 1696707482473,
        "tmdate": 1701465392189,
        "mdate": 1701465392189,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an easy-to-follow approach called RoAST, which aims to improve the robustness of language models (LMs) from multiple perspectives. \nThis approach is unique as it considers both robustness on perturbed inputs and generalizable knowledge in pre-trained LMs. \nThe experimental setup is robust, including multiple baselines and thorough analysis, strengthening the results and conclusions. The paper's extensive experiments show the best weighted average performance in four robustness aspects, alongside a standard validation accuracy compared to other finetuning methods. Overall, RoAST has the potential to become a more robust and more generalized technique for various NLP tasks.\n\nFrom the reviewers, this paper only showed experiments using limited tasks and datasets, thus recommending the use of challenging benchmarks and non-BERT-based models like decoder-only structures.\nOverall, the paper is deemed to require further analysis to explain the difference in approaches, which could be more exciting to the community."
            }
        },
        "id": "jcnhjFVLJp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "orSVYeobMr",
        "replyto": "orSVYeobMr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2152/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535660,
        "cdate": 1696707535660,
        "tmdate": 1701465455862,
        "mdate": 1701465455862,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the significant challenge of mitigating the distribution shift caused by using synthetic images during inference in multimodal machine translation (MMT). The authors propose a method that feeds both synthetic and authentic images to the MMT model, reducing the gap between their representations through Optimal Transport theory and ensuring output distribution consistency using KL divergence. The paper's contributions include the introduction of synthetic images during training, an optimal transport-based training objective, and a prediction consistency loss. The paper's contributions seem promising, but further exploration and addressing potential limitations may enhance this paper, some concerns regarding novelty and robustness have been raised.\n\nPros:\n\n* Interesting Approach: The paper introduces an Interesting approach to addressing the dependence on authentic images in MMT. By leveraging principles like Optimal Transport and KL divergence, it proposes an Interesting technique to mitigate the distribution shift caused by synthetic images during inference.\n\t\t\n* Positive Results: The paper provides a comprehensive evaluation against various baseline models and datasets, including the Multi30K dataset. This thorough evaluation enhances the validity of the approach and demonstrates its effectiveness.\n\t\t\n* Clear Presentation: Reviewer 1 commends the clear writing and organization of the paper, making it easy to follow and understand.\n\n\nCons:\n\t\t\n* Novelty is limited: Reviewer 1 raises concerns about the novelty of the proposed method, noting that the objectives and consistency concepts are not entirely new and have been used in other visual-text models.\n\t\t\n* Potential Robustness problem: There are concerns about the robustness of the approach due to its heavy reliance on synthetic images during training. Reviewer 2 highlights potential sensitivity to the quality and diversity of synthetic images and the risk of overfitting to specific characteristics, limiting generalizability.\n\t\t\nIn summary, the paper offers an interesting approach to mitigate distribution shift in MMT caused by synthetic images during inference. It presents a thorough evaluation and demonstrates promising results. However, concerns regarding novelty and robustness have been raised."
            }
        },
        "id": "amCo8Gdca0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oqqmjw1BD1",
        "replyto": "oqqmjw1BD1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1423/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512971,
        "cdate": 1696707512971,
        "tmdate": 1701465430508,
        "mdate": 1701465430508,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers concur that the manuscript proposes an effective method to make LLMs private without hurting their general performance. To make the paper even more appealing, the authors are advised to address the following considerations: a) evaluate the cost-effectiveness of the proposed method when extensive neuron must be modified to ensure intricate privacy; b) provide guidance on determining an optimal batch size that balances preserving privacy and maintaining model performance."
            }
        },
        "id": "nz00D7XiBF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "onr6HrKxn0",
        "replyto": "onr6HrKxn0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission924/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499505,
        "cdate": 1696707499505,
        "tmdate": 1701465415009,
        "mdate": 1701465415009,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This is a short paper containing a blend of a (somewhat) systematic literature review and a position paper.\nThere is quite a gap between the scores of the reviewers that is mostly due to how to value these contributions, while they seem to agree about the \"facts\". \n\nIn my view, it would have been better to either write a more systematic literature review or a real position paper."
            }
        },
        "id": "FYuRujWS59",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "on3Wo4VODO",
        "replyto": "on3Wo4VODO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5265/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610088,
        "cdate": 1696707610088,
        "tmdate": 1701465554575,
        "mdate": 1701465554575,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates the transferability of several debiasing techniques across languages, when applied to multilingual models.\n\nThe reviewers appreciated that this work investigates debiasing in multilingual models, particularly since model debiasing work has centered on English. However, the reviewers felt that the paper would benefit from additional experiments (i.e., on a larger and more diverse set of languages), and more clarity, particular in the analysis."
            }
        },
        "id": "LsMpmzV8Y7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "olzuxDCxMZ",
        "replyto": "olzuxDCxMZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2927/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552334,
        "cdate": 1696707552334,
        "tmdate": 1701465481389,
        "mdate": 1701465481389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a model for attributing dialogue utterances in a novel to the characters in that novel, in particular handling cases without explicit speaker cues.  Overall, the reviewers agree this is a decent contribution with appropriate experimental support, but with somewhat limited innovative quality.  The comparison with ChatGPT is interesting, especially in cases where it is outperformed by the proposed SPC model.|meta"
            }
        },
        "id": "SklCifuaF5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "olEEp3Phda",
        "replyto": "olEEp3Phda",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1293/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509735,
        "cdate": 1696707509735,
        "tmdate": 1701465426533,
        "mdate": 1701465426533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers appreciated the dataset introduced by this paper, finding it \"wide-domain\", \"large-scale\", and created through an \"innovative data selection pipeline\". Reviewers generally agreed that the work is sound, with some concerns about possible data quality issues from reviewer k1U2 addressed in the author response. Reviewer 3agr gave a much lower excitement score than other reviewers, however I tend to agree with the other reviewers that the combination of model generations and human selection, and evaluation, will likely make this a high quality resource, and that the experiments done here look very thorough. \n\nTo address the points related to dataset discomfort raised by reviewer k1U2, I strongly recommend that the ​​authors ensure that details of their manual review process and use of the Lamda guidelines are clear in the paper, and also recommend (but less strongly) that they write a content warning for readers of the paper, and users of the dataset, about possible discomfort caused by some of the videos in the dataset."
            }
        },
        "id": "u6PNALFvXE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "okV4KG4kMg",
        "replyto": "okV4KG4kMg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2259/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538100,
        "cdate": 1696707538100,
        "tmdate": 1701465459394,
        "mdate": 1701465459394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces the technique of random entity quantization to knowledge base embeddings. The paper is also appealing as it questions the validity of existing methods. The experimental results, inclusive of additional experiments in the discussion period, are persuasive. We eagerly await the camera-ready version that incorporates feedback from the reviews and discussions!"
            }
        },
        "id": "9TkoCtrkvR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ojgwuBVokp",
        "replyto": "ojgwuBVokp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1678/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707523189,
        "cdate": 1696707523189,
        "tmdate": 1701465438084,
        "mdate": 1701465438084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposed a new system containing mostly pre-trained components for the TEACH benchmark of virtual mobile manipulation from dialog history or instructions.\n\nThe strengths highlighted by reviewers are:\n- Good relevance to robotics\n- Strong results of the proposed method\n- Convincing empirical evaluation on standard benchmarks and baselines (this was initially lacking, but got significantly improved during rebuttal with additional experiments)\n- Method requires minimal training.\n\nOutstanding weaknesses after rebuttal are:\n- Organization of the paper and confusion over how the many modules interact. The authors have attempted to address it."
            }
        },
        "id": "Q64lVSiDln",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ogh9vskMDH",
        "replyto": "ogh9vskMDH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission590/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491486,
        "cdate": 1696707491486,
        "tmdate": 1701465404385,
        "mdate": 1701465404385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "There is a consensus among the reviewers that the proposed text-to-SQL method is worthwhile, the experiments are thorough and the paper is generally well written and clear. The reviewers had concerns on the novelty of this work with respect to the existing techniques (and the subsequent comparison with these) and statistical significance tests. The authors have provided detailed response to these. Also, some questions were raised by the reviewers; the authors are encouraged to clarify those as well in the paper."
            }
        },
        "id": "kFwaVYh7EM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oeZiXoCHgq",
        "replyto": "oeZiXoCHgq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2903/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551928,
        "cdate": 1696707551928,
        "tmdate": 1701465480675,
        "mdate": 1701465480675,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper provides a framework for jointly distilling a teacher to multiple student networks. It presents a few challenges eg the potential bad influence of immature student models in the early training stage. The paper then proposes to dynamically detach gradients and employ mutual learning only during the second stage in a two-stage training scheme. The empirical results demonstrate the effectiveness of AIO-KD. The analyses of the proposed algorithm are interesting and insightful."
            }
        },
        "id": "hLvTarBcs0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "odPKQiL2X8",
        "replyto": "odPKQiL2X8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1483/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515439,
        "cdate": 1696707515439,
        "tmdate": 1701465432212,
        "mdate": 1701465432212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper innovatively integrates the cache language model (Cache-LM) approach with the Transformer architecture to address incoherence and hallucination in state-of-the-art LMs. Its key contributions encompass bridging the cache LM strategy with the softmax bottleneck of Transformers, introducing a novel contrastive learning objective to enhance the similarity between histories associated with the same output token, proposing a discriminative training criterion with a max-margin loss, and adapting Cache-LM for both Transformer decoder and encoder-decoder architectures.\nThe reviewers commend the paper for being well-motivated and presenting a clear rationale. They highlight the innovative nature and logical soundness of methods such as the contrastive learning objective. The paper's strength is further bolstered by comprehensive evaluations that demonstrate significant improvement across multiple tasks. Additionally, the versatility of the proposed technique is emphasized, noting its applicability to various architectures, including decoder-only and encoder-decoder. Finally, the paper's coherence, ease of understanding, and the inclusion of supplementary code are appreciated.\nThe consolidated concerns from reviewers primarily focus on the paper's baseline model choices, emphasizing its reliance on older models like GPT-2 and BART and suggesting the inclusion of newer models like LLaMA. Additionally, there's a strong call for a more pronounced distinction and deeper discussion regarding the differences between Cache-LM and pointer networks. Most reviewers also raised questions about the proposed techniques' effectiveness as language models become larger and more sophisticated. Furthermore, there are requests for clarification on certain technical aspects of the paper, especially around specific lines and equations. Lastly, concerns were highlighted about some assumptions in the paper, particularly regarding the definition of terms in the equations and the general perspective on token probabilities."
            }
        },
        "id": "X36yXHGSJR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oaNa4rNIpU",
        "replyto": "oaNa4rNIpU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3815/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707576309,
        "cdate": 1696707576309,
        "tmdate": 1701465510523,
        "mdate": 1701465510523,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Authors propose a method to efficiently adopt a small LM with the help of a larger LM by only having access to its probability outputs on a small development/validation set, without having need to the internal model weights. Results on English show the efficacy of the proposed method. From an efficiency perspective, the paper adds a lot of practical value to the literature while not solving all the issues in this space."
            }
        },
        "id": "7rIpErdPNL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oYs7h2dE2e",
        "replyto": "oYs7h2dE2e",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3049/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555157,
        "cdate": 1696707555157,
        "tmdate": 1701465485664,
        "mdate": 1701465485664,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a Transformer variant that maintains the distance between elements consistent across layers and heads. The paper achieves this by combining orthogonal attention, residual, and learnable weight. The method is validated on LRA and few small datasets.\n\nPros:\n- The paper proposes to combine orthogonal attention with residual networks and analytically show the Lipschitz continuity of their modified Transformer architecture. Which is an interesting idea.\n\nCons:\n- It is unclear why “preserving distance between a pair of tokens within a certain analytical bound” is actually useful or needed. This makes the objective of the paper unclear. \n- In the absence of clearly motivated objectives, it is unclear as to why Lipschitz continuity is required (as shown by the authors for their model).\n- Actual contribution of the paper is unclear\n- The reason behind a >5x improvement of speed while 50% more parameter is not clear and not analyzed in the paper."
            }
        },
        "id": "Z9jap3UbuC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oYRlrDN6uj",
        "replyto": "oYRlrDN6uj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5630/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616154,
        "cdate": 1696707616154,
        "tmdate": 1701465564104,
        "mdate": 1701465564104,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This study presents an approach for verbalizing structured data into concise template sentences, achieved by instructing Large Language Models (LLMs) to generate entity-agnostic templates while incorporating additional algorithmic parsing consistency checks.\n\n**Pros**:\n- The motivation behind the work and its contributions are clearly articulated.\n- The proposed method yields significant performance improvements in data-to-text generation tasks.\n- The authors use a diverse array of automated metrics for evaluation, enhancing the rigor and credibility of their results.\n\n**Cons**:\n- Two reviewers note that the paper's complexity is exacerbated by an abundance of mathematical expressions, which can obscure the paper's core message.\n- The method is complex and requires numerous LLM API calls."
            }
        },
        "id": "5TVtXs5iYA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oVJXUvXT9b",
        "replyto": "oVJXUvXT9b",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3767/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707574262,
        "cdate": 1696707574262,
        "tmdate": 1701465509150,
        "mdate": 1701465509150,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an approach for compositional edits to images in the CLEVR dataset. They also provide a dataset of editing instructions. Three reviewers provided reviews for this work that were in consensus. They found the paper to present a novel and interesting technique, they valued the dataset and found the results compelling. There were a few concerns noted in the reviews such as: the limited scope and generalizability to real world images. However, I must note that reviewers were still excited about the paper, and I consider these concerns to be addressable in future work. Based on the paper, reviews and discussion, I find this work sound and interesting for readers."
            }
        },
        "id": "M9VH6bGQb0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oVAod8GRI9",
        "replyto": "oVAod8GRI9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission344/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485300,
        "cdate": 1696707485300,
        "tmdate": 1701465395831,
        "mdate": 1701465395831,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors check the syntactic change in the UK parliament data from 1830 to 1990. They fine-tune a PLM to detect the syntactic change and also train their own POS tagger, which they compare to the fine-tuned PLM. The main contribution of the paper uses a hybrid tagger, a tagger that uses embeddings that have been shown to understand old English."
            }
        },
        "id": "xmOQoFdFnV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oTtA9uIlR8",
        "replyto": "oTtA9uIlR8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5070/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606312,
        "cdate": 1696707606312,
        "tmdate": 1701465549080,
        "mdate": 1701465549080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes to replace 20ms or 40ms-long discrete units used for generative spoken language modelling with 200ms-long continuous word-sized audio tokens. The paper explains the architectural choices needed to achieve this goal and reports that the model's performance is on par with the standard discrete-unit-based language model while being more memory efficient.\n\nThe reviewers praised clear writing, good motivation, explanation of the architectural choices, and sound experimental design. The reviewers raised several questions, which the authors answered during the rebuttal. The only comment that needs to be addressed in the camera-ready version is empirical inference speed measures, such as sec of generated audio per sec of inference."
            }
        },
        "id": "BysFGUhITQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oSYifZI06H",
        "replyto": "oSYifZI06H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission110/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479459,
        "cdate": 1696707479459,
        "tmdate": 1701465387607,
        "mdate": 1701465387607,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The content and results are interesting but there appear to be many missing references."
            }
        },
        "id": "GaUsI5AzWd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oOKU31j9Q6",
        "replyto": "oOKU31j9Q6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3519/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564401,
        "cdate": 1696707564401,
        "tmdate": 1701465499970,
        "mdate": 1701465499970,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors present a denoising framework for Commonsense Knowledge Graphs (CSKGs), which often contain spurious data. Leveraging semantic information, global rules, and local structures, this framework differentiates between authentic and noisy triples. Emphasizing link prediction across two datasets using Recall and AUC metrics, the evaluations show the value of this approach in enhancing data quality, which is further illustrated through zero-shot commonsense question-answering tasks.\n\nThis paper is technically sound and well-executed; however, the impact of the results is incremental and not particularly surprising to the community."
            }
        },
        "id": "zX7KdYFLvc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oEsuNpkA8d",
        "replyto": "oEsuNpkA8d",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2464/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542792,
        "cdate": 1696707542792,
        "tmdate": 1701465465998,
        "mdate": 1701465465998,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper explores the effectiveness of instruction fine-tuning in training language models for chatting, with the goal for open LLMs to become more similar in performance to ChatGPT or GPT-4 models. The paper collects a new dataset \"UltraChat\" that contains instructional conversations (Questions about the world, Creation and Writing, Assistance on Materials). The authors then use this dataset to fine-tune a new LLM based on LLaMA, and find that it achieves better performance than open LLMs not trained on such data. \nThe dataset is large and the reviewers judge it to be of high quality, and think that the semi-automatic data collection method is valuable for collecting similar such data in the future. \n\nThe reviewers' main concerns are about the cost of the dataset (and discussion of it; the authors address this to some extent in their rebuttal) and about a human evaluation for a subset of the data to obtain better ideas about dataset quality. For the rebuttal, the authors have conducted such a human evaluation at a small scale, so in my opinion the main concerns regarding the initial version of the paper can be addressed easily in the main paper; the one slighly lower score for this paper is based on a non-substantial review and can be disregarded."
            }
        },
        "id": "91m8q4NzbU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oEsYs3WRc3",
        "replyto": "oEsYs3WRc3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission905/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498966,
        "cdate": 1696707498966,
        "tmdate": 1701465414425,
        "mdate": 1701465414425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper focuses on enhancing fine-grained multimodal understanding through a weakly-supervised learning approach that incorporates visual relation annotations into multimodal pretraining. The method introduces a novel strategy, including verbalized scene graphs and masked relation classification, and demonstrates its effectiveness through extensive experiments on benchmark datasets. Despite some minor concerns, reviewers unanimously praised the soundness of this work with excitement. Overall, the paper makes a valuable contribution to the field of multimodal pretraining, and can benefit from addressing the limitations and providing a more extensive discussion of its applicability to various multimodal tasks in the camera ready version."
            }
        },
        "id": "FHs1UDP2k0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "oC5e8mAKAP",
        "replyto": "oC5e8mAKAP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3128/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556674,
        "cdate": 1696707556674,
        "tmdate": 1701465487971,
        "mdate": 1701465487971,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes an unsupervised approach to Grammatical Error Correction (GEC) based on method proposed in Yasunaga et al., (2021). The approach relies on using a large pre-trained language model (Flan T5-xxl) and proposes several novel techniques for synthetic data generation and enhancements to the critic component. Experiments on English and Chinese benchmarks demonstrate superior performance compared to unsupervised baselines and the LM-Critic pipeline.\n\nThe paper is clear, well-written, and demonstrates strong results on two languages. Two concerns raised in the reviews are the limited novelty (the paper is an extension of the Yasunaga paper), and the use of large backbone models. It is not clear whether the method can be effective when used with low-resource languages and smaller language models."
            }
        },
        "id": "FTswu9UYWq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "o9wco8bIVN",
        "replyto": "o9wco8bIVN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5844/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707619437,
        "cdate": 1696707619437,
        "tmdate": 1701465568259,
        "mdate": 1701465568259,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a new dataset for entity linking in scientific tables - a dataset of tables are annotated with links to the papers with code taxonomy using a custom annotation interface.  It appears this is the first work to construct such an entity linking dataset for tables in scientific papers.\n\nAs the authors discuss in the limitations section, the dataset only covers one domain (results tables in machine learning papers).  It would be nice to see more domains, but this still seems like an important contribution."
            }
        },
        "id": "fb9hTUsrto",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "o7SWorg8EM",
        "replyto": "o7SWorg8EM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3936/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579762,
        "cdate": 1696707579762,
        "tmdate": 1701465514766,
        "mdate": 1701465514766,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an investigation into potential biases and effects that may affect the performance of models for conversational recommendation.  A novel metric for quantifying selective bias is proposed, as well as a data augmentation method that addresses these biases.  Experiments on two datasets show how the proposed method can improve performance (in some cases).  The reviewers agree that overall, the paper is well-written and the motivation is clear.  There is some concern about the limited novelty of the work and that the evaluations focus on recommendation only and not on the conversational component, which was intentionally left for future work (according to the rebuttal).  This does however make for a weaker paper in a 'dialogue and interactive systems' track.|This paper presents an investigation into potential biases and effects that may affect the performance of models for conversational recommendation. A novel metric for quantifying selective bias is proposed, as well as a data augmentation method that addresses these biases. Experiments on two datasets show how the proposed method can improve performance (in some cases). The reviewers agree that overall, the paper is well-written and the motivation is clear. There is some concern about the limited novelty of the work and that the evaluations focus on recommendation only and not on the conversational component, which was intentionally left for future work (according to the rebuttal). This does however make for a weaker paper in a 'dialogue and interactive systems' track."
            }
        },
        "id": "UbCZ1o2BUk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "o7Cpy0nZZb",
        "replyto": "o7Cpy0nZZb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1044/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503703,
        "cdate": 1696707503703,
        "tmdate": 1701465418788,
        "mdate": 1701465418788,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes two pre-training approaches to improve opinion tree generation for aspect-based sentiment analysis. The proposed method is sound validated by experiments. However, the experiments have been performed only on one dataset (ACOS) while more extensive evaluation is needed. Morever, the reviews highlighted that lack of extensive explanations on why the performance can be improved with the proposed method."
            }
        },
        "id": "53vSDoB2tC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "o6D5yTpK8w",
        "replyto": "o6D5yTpK8w",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission372/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485833,
        "cdate": 1696707485833,
        "tmdate": 1701465396472,
        "mdate": 1701465396472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces DemaFormer, a novel architecture designed for temporal language grounding. Since simple attention mechanisms may not effectively capture the relationships between video segments and textual queries, this paper introduces an energy-based model framework to explicitly learn distributions between moments and queries. The Transformer-based architecture employs an exponential moving average with a trainable damping factor to efficiently encode input from moments and queries. In summary, the proposed method is reasonable with clear and strong motivation. The experiments are well-conducted. In rebuttal period, the author addressed several concerns from reviewers. In particular, authors have performed an extra comparison where the proposed method and baselines all employ the same extra features as [1]. Although there was no clear answer from the reviewers, we can conclude that the results obtained from the experiment largely dispelled the reviewers' concerns about the performance. The clarification and discussion in the rebuttal should be added to the revised version. This paper has a high possibility of being accepted to findings."
            }
        },
        "id": "U1JXiIvQ8r",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "o5bOK5a9qz",
        "replyto": "o5bOK5a9qz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5054/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606051,
        "cdate": 1696707606051,
        "tmdate": 1701465548627,
        "mdate": 1701465548627,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Paper Topic And Main Contributions:\n* This paper aims to address the problem in the factual probing task, which is that small changes to the prompt can lead to large changes in model output.\n* The authors propose to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time.\n* The augmentations include synonym substitution, back-translation and stop word filtering.\n\nReasons to accept:\n* The problem of answer variability depending on the prompt is important.\n* The test time prompt augmentation is an interesting solution to improve the effectiveness prompt without training. The proposed method is easy to generalize.\n\nReasons to reject:\n* The proposed augmentation method is not very novel. A similar approach has been proposed in \"Self-Consistency Improves Chain of Thought Reasoning in Language Models.\" The authors explain in the rebuttal: Wang et al. suggest generating multiple outputs from a *single* prompt and then taking the majority vote. In contrast, the proposed method involves generating outputs from *multiple* prompts and then taking the majority vote.\n* Diminishing improvements for larger models. The authors explain in the rebuttal that smaller LMs are important (environmental impact, financial constraints, hardware constraints when running LMs on edge devices, etc)."
            }
        },
        "id": "ahUTF1z2UD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "o5LeRFe7VS",
        "replyto": "o5LeRFe7VS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4268/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586384,
        "cdate": 1696707586384,
        "tmdate": 1701465526089,
        "mdate": 1701465526089,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents API-Bank, a benchmark for enhancing large language models' (LLMs) proficiency in using external tools and APIs. As mentioned by the reviewers, the paper make several contributions. For the evaluation of current LLMs, a dataset consisting of 73 tools on 314 dialogues (a total of 753 API calls) has been developed. It also includes multi-turn dialogue with multiple APIs being called.\nThe paper introduces a training dataset generator using a multi-agent approach, significantly reducing annotation costs while maintaining diversity and authenticity. The paper also proposes their model Lynx, which fine-tunes Alpaca model using their dataset.\n\nThe reviewers pointed towards some issues with the writing, along with missing details. For instance, the details of tool implementation are missing in this paper and so are some important components The distribution of API calls is missing. The authors have provided these details in the rebuttal, and it should be added to the main paper."
            }
        },
        "id": "glorBepr28",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "o2HBfgY20b",
        "replyto": "o2HBfgY20b",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2446/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542381,
        "cdate": 1696707542381,
        "tmdate": 1701465465399,
        "mdate": 1701465465399,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the relationship between mental health disorders and social media post emotion dynamics (tweets). The work highlights correlations between mental health diagnoses and linguistic cues about emotion dynamics. The reviewers are in agreement that the topic is interesting and that the work is generally sound. The reviewers' concerns seem to have been addressed during the discussion period."
            }
        },
        "id": "4FH0j8bar4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nwTqq0XW3w",
        "replyto": "nwTqq0XW3w",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4404/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589029,
        "cdate": 1696707589029,
        "tmdate": 1701465530685,
        "mdate": 1701465530685,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree the paper provides a thorough evaluation of static and contextualized embeddings in lexical semantic change detection.\nThey greatly approve the depth of analysis and the experimental setup, especially on the synthetic task.\n\nFor the camera-ready version, the authors should:\n1) Include detailed information about their datasets\n2) Add discussion about ethical concerns of using Reddit data\n3) Include explanation of the relation to experiments from Ethayarajh (2019)."
            }
        },
        "id": "Pw570w6dWY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nw6JxagUNG",
        "replyto": "nw6JxagUNG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4188/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585045,
        "cdate": 1696707585045,
        "tmdate": 1701465523562,
        "mdate": 1701465523562,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the problem of the inaccessibility of proprietary large language models and presents a method (inspired by but distinct from previous work) to distill knowledge from a large language model like ChatGPT or LLaMA to a smaller model requiring far less data and no extra human annotation. \n\nPros:\n\nPresents an innovative adversarial distillation framework that enables efficient knowledge transfer by modeling model differences between teacher models and student models. It combines data augmentation and feedback from the LLM to improve on \"harder\" instructions.\n\nExtensive experiments on multiple open generative and reasoning benchmarks demonstrate the superior performance of the Lion model.\n\nCons:\n\nMore ablation studies (some of them have been added by the authors during discussion/rebuttal)\n\nMor in-depth analysis/discussion on the model's training process and evaluation metrics\n\nThe original work  will also benefit from incorporating the additional discussion points/experiments conducted during rebuttal."
            }
        },
        "id": "due1eJPKPm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nucyYJZS5z",
        "replyto": "nucyYJZS5z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1646/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521622,
        "cdate": 1696707521622,
        "tmdate": 1701465437228,
        "mdate": 1701465437228,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the controllability of large language models on generation tasks on various benchmark tasks. Reviewers found this paper well written and easy to follow. Reviewers also found the experiments extensive and well designed, covering a wide range of controlled generation tasks. While this is an analytic paper with comprehensive evaluations, it can add significant value by including an in-depth discussion on why LLM struggled at fine-grained hard constraints and how to close the gaps, as also commented by one reviewer. The authors provided some detailed response on this of which the author should consider add to the revised version of the paper."
            }
        },
        "id": "zWHgKyhKS9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nuPp6jdCgg",
        "replyto": "nuPp6jdCgg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5283/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610346,
        "cdate": 1696707610346,
        "tmdate": 1701465555127,
        "mdate": 1701465555127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper explores the impact of disfluencies, particularly speech repairs, on the performance and understanding of LLMs. The authors created synthetic disfluencies to simulate live speech, which offers certain benefits, such as controlling for other variables in the acoustic environment. \n\nHowever,it would have been interesting to see how much the finding holds while incorporating natural disfluencies from other datasets like CallHome or Switchboard. Such an experiment could have enhanced the generalizability of the findings to real-world scenarios.\n\nOverall the reviewers’ highlighted some major concerns:\n\n– Absence of a thorough discussion on how the model's interpretation of disfluencies aligns with human understanding, which is claimed to be a major goal of the paper.\n\n– Absence of certain claimed information, such as results on statistical significance; or elaboration of results stating why certain types of disfluencies benefit or harm the model performance\n\n– Generalizibility of the findings of the study\n\nThe authors discussed some of these concerns and intend to address them in future study.\n\nWhile the reviews were mixed, the paper's content and findings hold some importance and will be of interest to the EMNLP audience. To strengthen the paper's quality, we recommend that the authors address these reviews, particularly focusing on filling in the missing information and enhancing the presentation of the results."
            }
        },
        "id": "ZpIcJLI53W",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nuLtpgr9l5",
        "replyto": "nuLtpgr9l5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5730/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617476,
        "cdate": 1696707617476,
        "tmdate": 1701465565856,
        "mdate": 1701465565856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This works proposes a method to embed watermarks into neural networks in order to protect their intellectual property. Employing contrastive learning, the authors create backdoors in the form of rare or combined words that can trigger specific labels at inference. Crucially, the proposed approach resists model finetuning on unknown datasets, as well as some watermark removal attacks that the authors evaluated.\n\nReviewers have all acknowledge the technical quality, clarity and readability of the paper, while opinions on its novelty and significance vary. On the one hand, the proposed method addresses a clear, well-motivated problem and achieves good results. On the other hand, the approach is restricted to non-generative language models, limiting its reach; and though it resists some common attacks in those evaluation settings, whether it withstands more aggressive or adaptive evaluation or a wider set of tasks, models and datasets is not certain. Questions of the novelty of the approach have also been raised with respect to the method and modality: overall, the authors have convincingly argued that their use of contrastive learning differs from other works, and leverages text-specific elements in backdoors."
            }
        },
        "id": "eTBjZIlU9Q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nsupkM0ppH",
        "replyto": "nsupkM0ppH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1576/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518135,
        "cdate": 1696707518135,
        "tmdate": 1701465435019,
        "mdate": 1701465435019,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a rule-based Bangla lemmatizer. It strips suffix markers and validates the predicted lemma against a dictionary.\n\nIt is somewhat unexpected to encounter a rule-based language analyzer in 2023. However, in comparison to prior research, this paper demonstrates relatively strong performance. The authors conducted comprehensive experiments and transparently outlined the study's limitations. Although it may not reach a broad audience, this paper deserves publication in some form."
            }
        },
        "id": "DeTMtvgoEE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "noUf45O1PX",
        "replyto": "noUf45O1PX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5312/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610746,
        "cdate": 1696707610746,
        "tmdate": 1701465555810,
        "mdate": 1701465555810,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work investigates how Large Language Models (LLMs) perform on a decision-making task that requires balancing exploration and exploitation. The authors show that LLMs’ behavior depends on the input prompts and the temperature settings, and that they can achieve human-like or superhuman results with simple adjustments. The authors suggest that LLM psychology studies should account for the variability of LLMs’ responses under different conditions.\n\nPros:\nDemonstrated that LLMs can achieve superhuman performance in terms of minimizing regret on the Horizon task, which can have implications for designing more efficient and effective decision-making systems based on LLMs.\n\nCons:\nInsufficient details and justifications for the experimental design and analysis"
            }
        },
        "id": "7amtPlm0fd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "noPuQXVx8Y",
        "replyto": "noPuQXVx8Y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4633/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594863,
        "cdate": 1696707594863,
        "tmdate": 1701465537246,
        "mdate": 1701465537246,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a method for fine-tuning large language models (LLMs) to use web search results more effectively for knowledge-intensive tasks. The paper creates a new dataset by querying search engines with self-instructions and labeling the results as informative or distracting using an entailment model. The paper then fine-tunes LLaMA-7B on this dataset to improve its ability to filter out noisy information and ground on trustworthy information. The paper evaluates the method on various tasks, such as instruction following and question answering, and shows significant improvement over the baseline. The paper also tests the method on a new corpus of questions based on recent articles, and achieves remarkable results."
            }
        },
        "id": "JPYnb9kqCj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "noIvPGG8P1",
        "replyto": "noIvPGG8P1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3531/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564687,
        "cdate": 1696707564687,
        "tmdate": 1701465500459,
        "mdate": 1701465500459,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Overall, the reviewers found this work to be sound, the proposed methodology to be novel, and the findings to be interesting. The impact of the paper also has practical/societal significance and may help to promote even more work in the area in the future. Some of the reviewers were not quite as excited because of the perceived limited methodological contribution (i.e., no new methods were introduced and existing ones were used), though I don’t see this as a pure methods paper so I don’t believe that should be heavily weighted. On the other hand, a reviewer points out that there are limited insights into how to address the problem that is pointed out. The authors make a note that they will address this further if the paper is accepted. Other comments regarding the framing of the paper (appears to promote using LLMs for reference letters, referring to sentiment as a kind of “style”) are valid concerns but also appear to be addressed sufficiently by the authors in their rebuttal. The paper may benefit from some additional writing improvements as recommended by the authors, but these appear to be mostly minor (yet still important to address!)."
            }
        },
        "id": "PwpyKwJs7Z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "noEKNSB8Zq",
        "replyto": "noEKNSB8Zq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5166/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608373,
        "cdate": 1696707608373,
        "tmdate": 1701465551753,
        "mdate": 1701465551753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses an emerging and vital issue in PLM. I agree with the reviewers ekhf and qUde. It addresses a pertinent issue in the field of data privacy with LLMs and presents an innovative privacy-preserving technique that performs well. The paper's clarity and the inclusion of techniques for synthetic data generation further enhance its value."
            }
        },
        "id": "qqZDhvyjCr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nntsSuRSPb",
        "replyto": "nntsSuRSPb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5083/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606507,
        "cdate": 1696707606507,
        "tmdate": 1701465549486,
        "mdate": 1701465549486,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper describes a comprehensive method to analyze biases in Social-IQ dataset, and proposed a new dataset DeSIQ using a simple yet effective perturbation method from which biases are reduced. \n\nReviewers appreciated the importance of the problem space, the well-structured content, and solid experiments. They raised two concerns: 1) The lack of diversity in the data construction method: The perturbation method represents just one application of existing debiasing methods; and 2) The lack of generalizability of the proposed dataset: DeSIQ derives from only Social-IQ."
            }
        },
        "id": "IHXmWbu2Db",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nmnPI4eNuh",
        "replyto": "nmnPI4eNuh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4790/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598576,
        "cdate": 1696707598576,
        "tmdate": 1701465541820,
        "mdate": 1701465541820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "the authors present an interesting approach to help bridge the gap between finetuning and prompt instructing of models, and show the effectiveness of their approach. The reviewers however agree on the lack of suffient rigour in the evaluation, especially wrt to alternative approaches to prompt tuning to better understand the gains and tradeoff with FinePrompt. The evaluation of GenBERT and SAE in the rebutal helps to address the main concern of a comparison with the finetuned models, and should be in the main text. \nStill there is suffient material to make the paper interesting.  The comments of reviewer wpwG should be addressed in the final text to better clarify the work better."
            }
        },
        "id": "sgz0kX4p7l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nmSvzxwfRZ",
        "replyto": "nmSvzxwfRZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1902/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529693,
        "cdate": 1696707529693,
        "tmdate": 1701465446476,
        "mdate": 1701465446476,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates the applicability of NLP for creating learning materials for second language learning; evaluation with language educators from North America.\n\nThere is consensus among the reviewers that this paper presents a valuable contribution. Reviewers especially appreciate the importance and practical application of the task (education), the focus on two under-resourced languages, and the thorough human evaluation. While reviewers (and the authors) acknowledge some mixed results/limitations of the work (e.g., there is work to do before the approach is fully usable/applicable), the reviewers nevertheless find the current work promising."
            }
        },
        "id": "nE19KmOkFf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "newk6aDMRi",
        "replyto": "newk6aDMRi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission863/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497872,
        "cdate": 1696707497872,
        "tmdate": 1701465412965,
        "mdate": 1701465412965,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Allies is a method for iteratively querying an LLM in a beam-search style. Specifically, they \n- use a query expansion method that retrieves relevant data and adds it to the prompt\n- for each retrieved expansion, score it and prune the low-scoring ones\n- repeat the above two steps until the score is > a certain threshold\n\nExpansion followed by pruning enables augmenting with missing information + being tolerant to wrong retrievals. The retriever is fine-tuned. The answerer and the scorer are both ChatGPT3.5-turbo.\n\nThe method is in the same category as methods like self-ask, retrieve-then-ask, and GENREAD. It has been implemented and evaluated on open-domain QA for the datasets NQ, TriviaQA, and WebQ. Results are consistently better compared to relevant baselines.\n\nReviewers 1 and 2 had complaints about paper writing, which can be addressed in the camera-ready version.\nReviewers 3 and 4 questions have been answered. \n\nThe method requires ~10x increase in number of API calls compared to some of the baselines in the same category. While expensive, it actually makes at interesting point about the fact that better results can be achieved if the user is willing to call the API many more times. \n\nOverall, the approach makes sense and the results are pretty positive."
            }
        },
        "id": "XQXKUaFjrq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "neRWI1hWyO",
        "replyto": "neRWI1hWyO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission200/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481661,
        "cdate": 1696707481661,
        "tmdate": 1701465390921,
        "mdate": 1701465390921,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviews provide scores of 3,4,4 for soundness and 4,4,4 for excitement.\n\nCondensing the reviews, strengths and weaknesses including the following were mentioned.\n\nStrengths:\n\n- addresses a crucial and timely issue for the field (R1, R2, R3)\n- interesting links to philosophy (R1, R2)\n\nWeaknesses:\n\n- no empirical evidence or validation (R1, R3)\n- high degree of abstraction limits accessibility (R1)\n- questions about mathematical soundness were originally raised by (R2), but their final soundness score is 4"
            }
        },
        "id": "NxLfa4tp5n",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nYgu408UIo",
        "replyto": "nYgu408UIo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2298/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538957,
        "cdate": 1696707538957,
        "tmdate": 1701465460521,
        "mdate": 1701465460521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper tackles decontextualization through the lens of question generation and question answering. Decontextualization is a relatively new but important task; the goal is to rewrite an extractive (short) text snippet such that it \"stands alone\". This work builds on prior work (a seq2seq system from Choi et al), and introduces a 3-step process with question generation, question answering, and rewriting. This framework, inspired by prior work in QUD, is compelling and useful in this task. This work involves an annotated dataset and sound evaluation of LLMs.\n\nThe reviewers pointed out several issues to be addressed; notably please include the answers in the author response to reviewer questions, such as new results re LLAMA2, additional human evaluation, additional error analysis, and other clarifications."
            }
        },
        "id": "fHcdhbDI0C",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nYbOG9EaxD",
        "replyto": "nYbOG9EaxD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4544/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592690,
        "cdate": 1696707592690,
        "tmdate": 1701465534269,
        "mdate": 1701465534269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposed to improve the reasoning capability of LLMs by giving them the access to a symbolic reasoner. A self-refinement mechanism instructs the LLM to refine the incorrect logical form, by prompting it with the erroneous logic form, the solver’s error message, and a set of demonstrations showing common error cases and their remedies. \nExperiment is conducted by transferring four existing synthetic natural language datasets back to symbolic sentences and solving it with a symbolic reasoner. The result shows significant improvement over standard and CoT settings for 3 GPT models.\n\nStrength:\n1. The three stage design and self-refinement are reasonable and effective, even though not completely novel.\n2. A thorough analysis on the performance of 4 tasks and 3 GPT models.\n\nWeakness:\n1. The datasets are synthetic and it is not clear how much impact the proposed approach has on more natural tasks such as Entailmentbank. \n2. It is not clear what possible applications can the FOL representation be applied to."
            }
        },
        "id": "EDGwVAUWju",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nWXMv949ZH",
        "replyto": "nWXMv949ZH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1989/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531585,
        "cdate": 1696707531585,
        "tmdate": 1701465449423,
        "mdate": 1701465449423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper evaluates the necessity of complex models for temporal knowledge graph reasoning and introduces a streamlined approach, SiMFy, that uses a multilayer perceptron (MLP) for modeling event structural dependencies and employs a fixed-frequency strategy for inference. SiMFy achieves comparable performance to SoTA approaches with faster convergence, reduced training time, and capturing structural dependency information for the TKG reasoning task.\n\nThis paper is technically sound, and the experiments are conducted appropriately, although the impact of the results is relatively incremental. As reviewers point out, experiments on larger datasets such as the GDELT dataset, will strengthen the paper. More importantly, as reviewer AXyf pointed out, the example of the Russia-Ukraine conflict is inappropriate from an academic perspective. Additionally, using TKG for predicting the future is less sensible. It would be more useful and appealing to identify similar event structures from the past for reference."
            }
        },
        "id": "FQVkeU64Yr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nTKRAgssvX",
        "replyto": "nTKRAgssvX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3354/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561070,
        "cdate": 1696707561070,
        "tmdate": 1701465494910,
        "mdate": 1701465494910,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a study of the impact of “translationese” on Cross Lingual Summarization (CLS), a problem relevant to the CLS area. An evaluation based on human annotators is presented, including Inter Annotator Agreement values. Some interesting conclusions are drawn from the work.\n\nSome unresolved questions remain: \n- Is Fleiss' Kappa appropriate for calculating IAA when using an ordinal scale (R3)?\n- Does it make sense to study the effect of translationese in a particular application, such as summarization, or does it make more sense to study it in Machine Translation methods in general (R4)?\n- It is important to better describe the annotation task that was performed (R1 and R2)."
            }
        },
        "id": "XFGiwC1x0d",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nT4S0wgrwp",
        "replyto": "nT4S0wgrwp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission49/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477821,
        "cdate": 1696707477821,
        "tmdate": 1701465385382,
        "mdate": 1701465385382,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a benchmark dataset for structural generalization in semantic parsing, as an extension to an earlier dataset focusing on lexical generalization. The benchmark is thoughtfully constructed, non-trivial, and useful for studying the theoretical linguistic capabilities of recent models. The reviewers found the paper also generally well-written.\n\nThe reviewers noted a few issues with the paper, most of which concern the presentation and can be addressed in the camera-ready version:\n- The similarities and differences between COGS and SLOG should be better highlighted and made accessible for readers not familiar with COGS.\n- Some of the included patterns seem at odds with human generalization capabilities (e.g. 4+ depth center-embedding clauses, inherently ambiguous sentences that are uniformly disambiguated). A more thorough discussion and justification of the choices made by the authors would be welcome.\n- A reviewer also wished an extension of the dataset to languages other than English. While I second this wish, it cannot be viewed as a flaw of the current paper."
            }
        },
        "id": "NTip6ghRCL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nSPsxWVe4k",
        "replyto": "nSPsxWVe4k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission815/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496629,
        "cdate": 1696707496629,
        "tmdate": 1701465411464,
        "mdate": 1701465411464,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper improves Transformers by adding \\lambda-layers, which perform a shift-reduce parser and enable the self-attention over the stack history (paring prefix) beside the surface tokens. Transformers with \\lambda-layers are jointly trained on strings with annotated (or parsed) trees. Experiments on both synthetic languages and various NL benchmarks show the generalization of their approach. All reviewers raise some questions about the clarity of this paper and some missing references related to shift-reduce stack parsing, recursive mechanism over attentions."
            }
        },
        "id": "BcBHrgraxd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nRB8VpeM7b",
        "replyto": "nRB8VpeM7b",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5258/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609964,
        "cdate": 1696707609964,
        "tmdate": 1701465554341,
        "mdate": 1701465554341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "First of all, thanks to the authors for working on \"fact-checking\" in a low-resourced language Hebrew. The problem itself is super important in the current scenario. The reviewers pointed out some shortcomings of the paper, which the authors also duly acknowledged. These include a detailed discussion on data collection, including recent relevant works on low-resourced languages, exemplifying the problem definition with relevant artifacts from the dataset, including a few simple baselines, etc., which I believe can be done easily. I would also request the authors to polish the paper in terms of language (reduce verbosity) and, if space permits, move critical discussions from the rebuttals/appendix to the main manuscript. I think the paper has most of the desired information for a resource paper, and the fact-checking dataset on Hebrew is a novel resource. I hope the authors would take the suggestions and cues from the reviewers to improve their paper."
            }
        },
        "id": "HR4S4ppsYD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nPzrjWrtlz",
        "replyto": "nPzrjWrtlz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3773/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707574822,
        "cdate": 1696707574822,
        "tmdate": 1701465509346,
        "mdate": 1701465509346,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces \"IndiSocialFT,\" a word embedding model developed using FastText for 20 Indian languages. This model is unique as it incorporates diverse textual characteristics from both structured and unstructured sources. The data used to train this embedding is derived from a variety of platforms, including 0.6 billion location-filtered tweets, Facebook posts and comments from notable Indian personalities, news media articles, and comments on popular video channels. The languages covered range from major ones like Hindi and Bengali to regional ones like Maithili and Angika, and also include English. The model's effectiveness is demonstrated through both intrinsic (word similarity) and extrinsic (text classification) evaluation methods. The results indicate that IndiSocialFT outperforms baseline models, including IndicFT and other FastText and TF-IDF based models, showcasing its potential for diverse linguistic applications in the Indian context.\n\nThe paper is appreciable for its extensive coverage of languages, surpassing similar previous studies. Specifically, IndiSocialFT addresses more languages than other renowned models like MuRIL and FastText. This comprehensive approach enhances the model's applicability and potential for various downstream tasks. Secondly, the evaluation methodology employed is robust and well-structured. The results from these evaluations are promising, with IndiSocialFT outperforming other pre-trained embeddings designed for Indian languages in several experiments.\n\nHowever, there's a notable lack of comprehensive evaluation across all languages covered in IndiSocialFT. Some datasets used for evaluation, like the word similarity task, are relatively small. Moreover, the performance difference between IndiSocialFT and other models in several experiments is marginal, necessitating significant statistical tests to validate any claims of improvement. The use of a news dataset from IndicGLUE for benchmarking is also questionable due to the already high baseline performance. The paper lacks detailed language-wise statistics, clarity on the amount of code-mixed data used, and a comparison with other significant models like IndicBERT and IndicBART. The paper seems to lack a clear explanation of why their model performs well."
            }
        },
        "id": "12PJ9z0QzB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nMjktU5AiP",
        "replyto": "nMjktU5AiP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3202/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558032,
        "cdate": 1696707558032,
        "tmdate": 1701465489989,
        "mdate": 1701465489989,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Quality, clarity, originality and significance\n\nTLDR: The paper proposes using LLMs, in particular, ChatGPT, to discover/label latent concepts in pre-trained language models. The LLM helps to find semantically richer annotations compared to human annotations. The resulting resource with annotated concepts is successfully used to perform interpretation analysis with both probing and neuron interpretation. The reviewers appreciate the importance and novelty of the studied problem and find the resulting resources and analysis framework facilitating future work in the area. The cons are about improving the clarity of the writing and the limitations of their work, which the authors have already readily acknowledged.\n\n\nPros:\n1. Significance 1 - all reviewers agree that the paper addresses an important problem - interpreting pre-trained language models.\n2. Originality - all reviewers appreciate the novelty of the contributions made in the paper -- using LLMs as annotators to label latent concepts and find the work can be used in other domains/pre-trained models/LLMs as well.\n3. Significance 2 - the released resource of annotated concepts will facilitate further research in this direction (VLTK, ZW2Y).\n4. Quality - VLTK finds the methodology to be  sound, combined with a rigorous evaluation.\n5. Clarity - VLTK also finds the writing to be clear and easy to follow.\n\nCons:\n1. Clarity - all reviewers find the neuron analysis hard to understand, but the authors already give more details in the rebuttal and indicate they'll include them in the final version as well.\n2. Clarity/quality - some reviewers identified certain limitations in the analysis performed in the paper, which could be addressed in the limitations section as well, and the authors acknowledge these already in their rebuttal -- using two very specific domains (zd3p), comparison of annotations with human-annotated concepts is based on a small sample size set (ZW2Y), the findings are based only on the use of ChatGPT as an annotator (ZW2Y)."
            }
        },
        "id": "A6bG5G4Ax9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nIuJXuSdhn",
        "replyto": "nIuJXuSdhn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1568/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517971,
        "cdate": 1696707517971,
        "tmdate": 1701465434762,
        "mdate": 1701465434762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper utilizes coarse-grained datasets for fine-grained NER with a small number of fine-grained annotations. The research questions have significant value and this paper gives the effective method. I suggest the authors give detailed descriptions for the method, and make precise revision accroding to comments of reviewers and responses."
            }
        },
        "id": "mvWFdwOpmK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nIp7wkMeMP",
        "replyto": "nIp7wkMeMP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1349/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510849,
        "cdate": 1696707510849,
        "tmdate": 1701465428075,
        "mdate": 1701465428075,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper analyzes important factors in crosslingual transfer of pre-trained LLMs. The paper studies three axes of variation using controlled studies. The reviewers agree about the neat experimental set up. Reviewers raised concerns about only English language experiments but this is addressed in the rebuttal."
            }
        },
        "id": "v2ybIxDR9e",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nI0X5IZOQA",
        "replyto": "nI0X5IZOQA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4734/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597341,
        "cdate": 1696707597341,
        "tmdate": 1701465540152,
        "mdate": 1701465540152,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Paper Topic And Main Contributions:\n* This paper proposes a method for math word problem-solving. It is a goal-driving method that starts from the root of the expression tree and decodes the tree layer by layer to the leaf nodes. \n* It extends this approach by 1. predicting all child nodes in the next layer in parallel and 2. using cross-attention to make the parallel prediction aware of each other. \n* The authors introduce a partial-tree evaluation metric to assess the capability of an MWP solver. \n* Experimental results demonstrate the effectiveness of the proposed approaches.\n\nReasons to accept:\n* The proposed method is interesting and sound. It combines goal-driving, MTree (a unified, unique representation of math equations), and cross-attention methods.\n* The proposed evaluation methods are valuable; they are more fine-grained than expression and result accuracy.\n\nReasons to reject:\n* The performance of the proposed approach on other datasets (such as Math, SVAMP, or ASDIV) is missing. The authords explain that the datasets they do use (Math23k and MAWPS) are used by many of the baselines. Experiments on MathQA and SVAMP will be added to the final version.\n* Comparison of performance with LLMs is missing. The authors address this point in the rebuttal.\n* Qualitative analysis of the unified tree structure is missing. The authors point to existing qualitiative analysis and will add further visualization."
            }
        },
        "id": "ICWdTeQmOC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nGFQ7IqOyg",
        "replyto": "nGFQ7IqOyg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3605/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566048,
        "cdate": 1696707566048,
        "tmdate": 1701465503299,
        "mdate": 1701465503299,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper focuses on the imbalance problem of N/A. The reviewers and the authors respectively give the detailed comments and responses. I suggest the authors fully consider the reviews and make precise revisions (especially the missing baseline)."
            }
        },
        "id": "Kqmi2i360J",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nGCwDjinT8",
        "replyto": "nGCwDjinT8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3771/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707574592,
        "cdate": 1696707574592,
        "tmdate": 1701465509349,
        "mdate": 1701465509349,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary:**\nThe paper presents two methods to improve local modelling to address the issue of local incongruity in the context of singing voice synthesis (SVS):\n1)\tA nearest neighbour local attention mechanism to enhancing local modelling by incorporating adjacent phoneme tokens' information.\n2)\tA phoneme-level local adaptive weights loss function, targeting the harder-to-synthesize parts of Mel-spectrograms.\nThe paper conducts experiments on Chinese pop songs and Hokkien Gezi Opera datasets to validate the effectiveness of the proposed methods.\n \n \n**Pros:**\n \n- The paper effectively summarizes prior work on SVS task.\n \n- The proposed methods are described clearly, and their effects are demonstrated by ablation experiments.\n \n- The proposed methods consistently outperform alternatives methods (N-Singer and adversarial methods) in both objective and subjective evaluation metrics.\n \n- The authors provide a detailed rebuttal addressing reviewers' questions and concerns, effectively clarifying their approach and results.\n \n \n**Cons:**\n- Reviewers 2 and 4 found the innovations presented in the paper to be relatively small and simplistic, mostly involving integrating local attention with global attention and adapting loss weights.\n \n- While the paper addresses concerns about adding a comparative analysis with N-Singer and adversarial training methods, it lacks a detailed analysis of local incongruity and comparison with existing methods (Reviewer 3, Reviewer 4).\n \n- The methods' effectiveness is primarily demonstrated on specific datasets (Chinese pop songs and Hokkien Gezi Opera), raising questions about the generalization of these methods to other languages, genres, and styles of singing (Reviewer 3).\n\n- The suggested dataset lacks both open-source availability and comprehensive explanations, which hinders its ability to facilitate progress in the field (Reviewer 1).\n\n\n\nReviewer 1: SqYD\nReviewer 2: cHLv\nReviewer 3: DiTD\nReviewer 4: knLR"
            }
        },
        "id": "mP4IlzfUsi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nFagtplIb8",
        "replyto": "nFagtplIb8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission475/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488653,
        "cdate": 1696707488653,
        "tmdate": 1701465400532,
        "mdate": 1701465400532,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper uses a wizard-of-oz experiment using two-hop questions from HotpotQA, to study how people's reliance on QA model predictions change when they have access to (in-)sufficient context knowledge.\n\nAll reviewers agree that the study is interesting and well-motivated, and that the experiments are well-designed and well-executed. \n\nMain concerns involve:\n1. Generalizability: Whether the findings generalize to other domains or QA tasks\n2. Implications: The study flags a phenonmenon but did not provide a solution \n\nThe authors sufficiently addressed these concerns in their rebuttals. I recommend accepting the papers to the Main track, and recommend the authors to revise the paper as they proposed in their rebuttals: add related work discussions, reflect on potential biases in human studies (e.g., whether they actually used the given information and whether their confidences are well-calibrated), and discuss the implications on futuer work."
            }
        },
        "id": "4yR4iSU9ux",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nE9aUYqz6k",
        "replyto": "nE9aUYqz6k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4362/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588247,
        "cdate": 1696707588247,
        "tmdate": 1701465529122,
        "mdate": 1701465529122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers find the task interesting and the paper well-written. However, the reviewers raised some issues with the size of the dataset used; the lack of discussion about architectural decisions; and some found the STEM-specific terminology difficult to understand. \n\nOverall, the reviewers find the soundness to be borderline, trending good, but are generally ambivalent about seeing the work published at EMNLP."
            }
        },
        "id": "n4Y9dDI1ld",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nC8WUrpWjG",
        "replyto": "nC8WUrpWjG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4773/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598254,
        "cdate": 1696707598254,
        "tmdate": 1701465541312,
        "mdate": 1701465541312,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Authors present the low-resource comparative opinion quintuple extraction by Data Augmentation with Prompting (DAP). Authors use a two-stage data augmentation strategy which first guides ChatGPT to generate triplet datasets and then employs transfer learning. \n\nOne of the 4 reviewers did not acknowledge the reading of the rebuttal.\nFor the soundness, reviewers give 2 (borderline), 3(good), 3(good), and 4(strong). For the excitement, 3 reviewers give 3 (ambivalent) and 1 gives 4 (strong). Some discussions authors did with the reviewer giving 2 were post-rebuttal. The response from authors has a reasonable explanation for the post-rebuttal question, as well as to the other questions. I have taken all these issues into account."
            }
        },
        "id": "s8nYnZgk3V",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "nC47EZVfAw",
        "replyto": "nC47EZVfAw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission634/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492611,
        "cdate": 1696707492611,
        "tmdate": 1701465406016,
        "mdate": 1701465406016,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new evaluation metric for visual storytelling, focusing on how references to objects and events are grounded in the sequence of images on which the story generation was conditioned. The proposed metric is used to evaluate methods applied to two visual storytelling datasets (including a third dataset in the author rebuttal). Evaluation shows that the metric correlates with human judgments of temporal and visual grounding."
            }
        },
        "id": "HhRLuXfO6D",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "n9y4IDFcCr",
        "replyto": "n9y4IDFcCr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3848/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707577714,
        "cdate": 1696707577714,
        "tmdate": 1701465511666,
        "mdate": 1701465511666,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses the problem of temporal adaptation for tweet classification and introduces the Variational Information Bottleneck for Evolutions (VIBE) model. The paper explores temporal adaptation in tweet classification, which is a valuable perspective for handling dynamic social media data. \n\nMost reviewers find the paper well-written, technically solid, and empirically convincing (MQvq and vJ1w). Reviewers have recognized the use of Information Bottleneck regularizers to distinguish past and future topics as an innovative approach to adapting to changing language features over time. The extensive experiments conducted on real-world social media data, along with ablation studies and case studies, are commended for demonstrating the effectiveness of the VIBE model. The clear and well-structured presentation of the paper, along with its readability, is appreciated by reviewers.\n\nReviewer QHXk suggests that the paper's presentation could be improved without specifying particular areas for enhancement. Reviewer vJ1w raises questions about the complexity of the VIBE model for text classification and suggests exploring the impact of data temporal differences on classification performance, especially in the context of larger language models (like GPT 3.5). More information is also asked on labels in each dataset and the choice of evaluation metrics. Additionally, conducting experiments on formal text datasets, such as news and reports, in addition to tweets, will be helpful (7trR).\n\nIn summary, the paper constitutes an innovative approach, comprehensive experiments, and a clear presentation. There are no major concerns with the paper's technical contributions. As suggested by reviewers, authors should consider including LLMs as a baseline, temporal differences in performance, and experiments on different text sources."
            }
        },
        "id": "qr2F5Lmz2L",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "n6qiOfZVYp",
        "replyto": "n6qiOfZVYp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2538/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544283,
        "cdate": 1696707544283,
        "tmdate": 1701465468324,
        "mdate": 1701465468324,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new benchmark for passage-level hallucination detection. The dataset consists of paragraphs generated by ChatGPT describing rare entities, which are annotated as being factual or not. It also presents a method for tackling the task, which essentially converts the paragraph into a question or list of entity requirements and tests whether the LM regenerates the original entity in response. Experiments show it outperforms previous approaches while running faster (the latter provided in the rebuttal). \n\nReviewers appreciated the dataset construction process (KooH/2PhM) and data curation (JYbz). Reviewers also appreciated the strong results of the new method. However, reviewers had some concerns about the data collection procedure regarding some missing details (KooH), lack of payment information or ethical statement (JYbz), and possible reproducibility issues with using a non-open-source model (2PhM). In the rebuttal, authors provided more information on the benchmark, point out they are using a stable ChatGPT version, and present some preliminary results with LLaMA. The proposed method seems a bit specific to the benchmark rather than being useful for detecting hallucination more broadly, but does work effectively. Overall, I think the new dataset is timely and covers an important if narrow kind of hallucination."
            }
        },
        "id": "mtbw6onOE9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "n20PghmZaD",
        "replyto": "n20PghmZaD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5468/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613546,
        "cdate": 1696707613546,
        "tmdate": 1701465560289,
        "mdate": 1701465560289,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents TOD-flow, a new approach to extract conversation flow graphs from dialogue dataset annotated with dialogue acts. The paper is well-written with helpful illustrating figures. The propose method shows its effectiveness in graph structure inference as well as two subtasks relevant to dialogues. The selection of baseline in evaluation was challenged by reviewers -- which the authors agreed to and provided further comparison during rebuttal."
            }
        },
        "id": "1XZKjNICcX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "n1Sx9ZjJRs",
        "replyto": "n1Sx9ZjJRs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2366/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540560,
        "cdate": 1696707540560,
        "tmdate": 1701465462877,
        "mdate": 1701465462877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an unsupervised Bayesian model designed for text segmentation and word discovery. They extend an existing method, with the idea that the regularity of the poetry's meter can be incorporated as a Bayesian prior. The proposed method shows good performance both on word discovery and text segmentation.\n\nThe reviewers concur that the authors have introduced a clever idea that is empirically justified. Personally, I love to see a new Bayesian model in 2023. While younger readers may face challenges in comprehending the approach, this should not pose a barrier to its acceptance."
            }
        },
        "id": "ZSs8yP2FVA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mx0ltXW10S",
        "replyto": "mx0ltXW10S",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3217/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558353,
        "cdate": 1696707558353,
        "tmdate": 1701465490684,
        "mdate": 1701465490684,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel technique (coined Knowledge Rumination) aimed at improving the performance of Pre-trained Language Models (PLMs) on knowledge-intensive tasks without relying on external knowledge sources. The approach elicits latent knowledge already embedded in PLMs via task-guided prompting and injects it back into the feed-forward network for knowledge consolidation. The paper presents experiments on various PLMs, including RoBERTa, DeBERTa, and GPT-3, showing improved performance on commonsense reasoning tasks and benchmark datasets. This highlights the potential for better exploiting the knowledge already present within PLMs to enhance their performance.\n\nAlthough the concept of extracting knowledge to enhance a PLM's performance, whether from external knowledge bases, external language models, or even the model itself, is not novel, this paper introduces a unique, innovative method to elicit knowledge already present in a PLM and to consolidate it to better address commonsense reasoning tasks.\n\nOverall, the authors conduct comprehensive experiments on multiple commonsense reasoning tasks and GLUE benchmarks, showcasing the effectiveness of the approach. The primary limitation might be attributed to the relatively modest performance improvement achieved by the proposed approach, even though this improvement is fairly consistent across all tasks and language models."
            }
        },
        "id": "dEOjAjY96s",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mvtjk1mlrq",
        "replyto": "mvtjk1mlrq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1415/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512737,
        "cdate": 1696707512737,
        "tmdate": 1701465430215,
        "mdate": 1701465430215,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a speculative decoding approach for LLMs by using a separate smaller encoder-decoder drafting model to quickly generate multiple tokens and then verify these tokens in parallel using a larger model. The paper is generally well written, has a clearly motivated idea, and is a part of an emerging research area.\n\nThe framework of speculative decoding has seen a resurgence in interest over the 18 months (as indicated by the authors in the comments below). One of the reviewers mentions several other speculative decoding works that have appeared in that time frame were not compared against. The authors provide additional extensive results in the rebuttal compared against these other works. The results suggest that their approach outperforms other existing approaches.\n\nTwo reviewers raised concerns around the memory/computational overhead of requiring two models (a drafting model and validation model) compared to the conventional approach. In the rebuttal, the reviewers provide additional results to show that Speculative Decoding requires more power but requires less energy due to the increased tokens per second of the draft-then-verify approach. Generally, I think this issue of how this approach scales to larger models is something that could be further explored. For instance, how would this comparison look under a setting where both the drafter and verification models could not both fit in GPU memory? My assumption is that IO for weight transfer would then become a limiting factor."
            }
        },
        "id": "bK6X3lTGEQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "muTWDq9bVs",
        "replyto": "muTWDq9bVs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4965/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602638,
        "cdate": 1696707602638,
        "tmdate": 1701465545978,
        "mdate": 1701465545978,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper addresses the problem of few-shot out-of-domain (OOD) intent detection in the specific setting of very few labelled in-domain (IND) samples and a large set of unlabelled (mixed OOD/IND) samples.  Two of the reviewers appreciate that the presented work addresses this practical setting and that the experimental results demonstrate the effectiveness of the proposed prototypical pseudo-labelling method.  The same reviewers also asked for a comparison with LLMs, which was addressed by the authors in their rebuttal by providing results for chatGPT, along with an analysis and an explanation how it was used.  The other reviewer expressed a more critical opinion that the paper lacked novelty and originality, which the authors addressed in the rebuttal by discussing the differences between their approach and the methods mentioned by this reviewer.|meta review"
            }
        },
        "id": "tRp7KGfCij",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mrD5HN7ZNR",
        "replyto": "mrD5HN7ZNR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1671/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707522997,
        "cdate": 1696707522997,
        "tmdate": 1701465438092,
        "mdate": 1701465438092,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a text-to-text framework for few-shot named entity recognition by adopting some cases for in-context learning. The proposed method is not novel enough but shows the effectiveness on some open-domain scenarios. The experimental results have demonstrated the effectiveness of the proposed method. Although some reviewers have raised some concerns about some statements in the experiment, all of them generally agree with the point of the proposed method."
            }
        },
        "id": "1XoFPRsY1E",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mrARDvuKi2",
        "replyto": "mrARDvuKi2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3891/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578534,
        "cdate": 1696707578534,
        "tmdate": 1701465513014,
        "mdate": 1701465513014,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes an end-to-end model for emotion-cause triplet extraction which is augmented with commonsense knowledge. The unification proposed in this paper aims to better handle error propagation. The generated and retrieved commonsense knowledge are integrated into the model with a novel gating mechanism. The results exhibit the effectiveness of the proposed model. The paper was missing some key details which the rebuttal helped addressing. The additional inference/training time footprint argument and the argument about its closeness to the existing approaches (i.e., UECA-Prompt) is rather unconvincing when we consider those numbers at large scale. The writing, in particular the introduction, needs further improvement to better position the problem."
            }
        },
        "id": "UuJ8f41eVK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mqnK19Dm80",
        "replyto": "mqnK19Dm80",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2879/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551373,
        "cdate": 1696707551373,
        "tmdate": 1701465479907,
        "mdate": 1701465479907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a prototypical-based framework that allows LLM to learn inherent interpretability by considering both token and sample-level features during the fine-tuning stage. The paper is clear, the model is well described, and experimental results show improvements in multi-class classification and NLI tasks and the quality of generated explanations. Two reviewers acknowledge the innovative aspect of the paper's approach, particularly the use of prototypical networks to improve interpretability. The method is seen as a novel way to achieve explanations for model predictions. Two reviewers raise concerns about the similarity between the proposed method and previous work, e.g. the proposed framework is similar to Friedrich 2021 (https://arxiv.org/abs/2110.02058), Koh 2020 (https://arxiv.org/abs/2007.04612), and Das 2022 (https://arxiv.org/abs/2204.05426). Clarification from similar approaches is needed."
            }
        },
        "id": "024Uxma8Bf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mpL9ikuYez",
        "replyto": "mpL9ikuYez",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission87/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478814,
        "cdate": 1696707478814,
        "tmdate": 1701465386886,
        "mdate": 1701465386886,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a new multilingual LLM, Struct-XLM - which understand the underlying structure and aligns the representations across languages. The approach uses RL mechanism to discover the structural information using a translation ranking task. \n\nThe paper proposes a novel mechanism to discover structural information and uses it bring alignment across langauges. This approach is quite useful for low resource languages. \nThe paper is motivated well with detailed explanation and experimentation to verify their claims\n\nAuthors should update the draft with the discussion and additional details shared during the rebuttal phase."
            }
        },
        "id": "afSqsYom99",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mnzjuOhkR2",
        "replyto": "mnzjuOhkR2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5094/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606893,
        "cdate": 1696707606893,
        "tmdate": 1701465549895,
        "mdate": 1701465549895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "There is a consensus among the reviewers that the paper's contribution to the field of few-shot learning is valuable. They also agree on the experiments being comprehensive and thorough. Two of the reviewers initially had concerns with regard to the clarity of the paper and the lack of statistical significance tests. Upon discussion, their concerns have partially been addressed. The authors clarified some points on contrastive learning and on how this work is positioned with respect to existing techniques in terms of novelty. Also, the authors are going to provide new results on the significance tests, and have promised to work hard on improving the writing and structure of the paper (which has been an issue raised in two of the reviews)."
            }
        },
        "id": "RBJHNhwgf4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mmlQICRJMc",
        "replyto": "mmlQICRJMc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1523/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516325,
        "cdate": 1696707516325,
        "tmdate": 1701465433368,
        "mdate": 1701465433368,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper advocates a novel method for evaluating QA models, especially those dialog-oriented: the authors propose an architecture, relying on Chat-GPT, to conduct an \"interview\" with a QA system. During the interview, Chat-GPT augments a pre-recorded sequence of questions (interview script) with generated hints and paraphrased Qs in order to nudge the system to arrive at a correct answer. This has several advantages: (1) it can reduce the cost of expensive human-based evaluation and (2) it can considerably improve our understanding of QA performance according to different criteria. \n\nAll the reviewers agree that the idea is novel and definitely interesting. Moreover, it is very impactful and this research would be immediately beneficial for the NLG community.\n\nThe paper is written very well and provides multiple insights on the proposed metrics, highlighting their meaning, interpreting the results on two datasets and discussing alignment with human judgement. \n\nTwo major issues have been raised by the reviewers. First, all the reviewers have given middle-low reproducibility scores and raised specific technical issues. Generally speaking, the authors provide a lot of technical details in the appendix -- yet, some questions remain.  I believe it is absolutely crucial for evaluation papers to arrive at maximum reproducibility, otherwise effective adoption of proposed metrics by the community is impossible. The authors, however, have very thoroughly clarified these issues in the response. One particular issue: the algorithm depends on the specific prompt template. The template is not presented in the paper -- but was revealed by the authors in the discussion. The omission of the template from the final version would considerably undermine reproducibility. (Figure 7 in the Appendix provides a very good idea of the template, yet it might be insufficient to reproduce the results.)\n\nSecond, reviewers uuh1 and dejG argue that the approach relies crucially on the properties of the interviewer (Chat-GPT). This might introduce multiple effects, from non-valid questions to biases etc. This is a more general problem that requires an in-depth discussion -- and the authors have provided some insights in the rebuttal, to be incorporated into the paper. I believe, however, that this issue is not a weakness per se, but rather a starting point for important further research."
            }
        },
        "id": "bT51iZhejX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mkEkfHveEL",
        "replyto": "mkEkfHveEL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1810/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527705,
        "cdate": 1696707527705,
        "tmdate": 1701465442611,
        "mdate": 1701465442611,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an original and valuable corpus for automatic essay scoring for French. The content and presentation of this paper is excellent, as reflected by three Soundness ratings of 5 and three Excitement ratings of 4. (Two reviewers rated its Reproducibility a 4 and one rated it a 5.) Other aspects of note are the paper's clarity, its comprehensive literature review, the quality of the corpus presented, as well as the inclusion of baseline results that set the stage for future work. None of the reviewers cited reasons to reject the paper, though all raised minor issues (e.g. grammatical errors, typos, unclear terminology) that were resolved during the author rebuttal period.\n\nIn light of this, only minor revisions, addressing reviewers’ comments and questions, need to be made to ensure this paper is camera ready."
            }
        },
        "id": "clq2VyOICm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mb35Pb69e8",
        "replyto": "mb35Pb69e8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4526/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592108,
        "cdate": 1696707592108,
        "tmdate": 1701465533689,
        "mdate": 1701465533689,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors built an interactive interface to collect annotations of simplifications made by humans and models. Using this data, they developed an evaluation metric called LENS-SALSA and trained models to predict word-level quality. All the reviewers unanimously commend the authors for their meticulous examination of edit types, their categorization, and the extensive analysis of the data collected by SALSA. \n\nHowever, the major flaw of the paper, as pointed out by the reviewers, is the excessive volume of information it presents while pushing substantial and pertinent content to the appendix due to space constraints. Reviewers express dissatisfaction with the paper's intricate structure and concur, as humorously noted by reviewer AAK9, that it requires simplification.\nI am confident that the authors have the capacity to implement revisions to the paper within the provided timeframe, including the extra page. My strong recommendation, however, is for them to earnestly consider the invaluable guidance from the reviewers regarding the paper's restructuring."
            }
        },
        "id": "jEiQdvewex",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mYniPxMGLL",
        "replyto": "mYniPxMGLL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4333/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587633,
        "cdate": 1696707587633,
        "tmdate": 1701465528238,
        "mdate": 1701465528238,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a method that accounts for annotators disagreement by grouping data based on demographic information about the annotators, for tasks where annotation is subjective (irony and hate speech detection).  The classifier aggreates confidence scores and predictions from models trained on annotation by annotators from different demographic groups. \n\nThis paper contributes to the growing literature on accounting for annotators disagreement. The claims are overall well-supported by the experiments, however, the presentation could be improved by clarifying what is meant by perspective and how that relates to demographic groups (reflecting for instance some of the points made in response to reviewer 17tq)."
            }
        },
        "id": "wGyYKIaYUz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mW5M8qkAxt",
        "replyto": "mW5M8qkAxt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3730/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707569961,
        "cdate": 1696707569961,
        "tmdate": 1701465507927,
        "mdate": 1701465507927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper explores the issue of toxicity in visual-language generative models (VLGMs) and introduces several key contributions, including the ToViLaG dataset, the WInToRe toxicity metric, and the SMIB detoxification method. Reviewers generally acknowledge the importance of the paper's contributions and its significance in the context of VLGM security and ethics. They also appreciated the comprehensive scope of this paper, as it introduces not just one component of toxicity reduction but all three components, from data to metric and method. This paper has the potential to serve as a foundation and facilitate future research in VLG toxicity.\n\nHowever, there are some recommendations made by the reviewers that are suggested to be incorporated into the final version. These recommendations include discussing related work more extensively, addressing necessary clarifications brought up during the rebuttal process (e.g., the transferability of detoxification methods from NLG to VLG), providing further discussion on the tradeoff between performance and detoxification, and justifying the degradation caused by SMIB."
            }
        },
        "id": "zIqtmisbUL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mU6C04mAJk",
        "replyto": "mU6C04mAJk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission441/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487849,
        "cdate": 1696707487849,
        "tmdate": 1701465399069,
        "mdate": 1701465399069,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper calls out the limitations of LLMs with in-context learning in performing the task of relation extraction, that they still lag behind supervised baselines such as fine-tuned BERT. It underlines a very valid concern regarding LLMs for this important problem. It outlines the limitations of these models and proposes an approach to mitigate them. The paper is very well-written and easy to follow. It provides a comprehensive survey of the related works in this space on four-widely used RE datasets. The community can benefit from this work for furthering the research in relation extraction using LLMs. The author responses are succinct and clarify most of the concerns of the reviews."
            }
        },
        "id": "OfbBfzYMVK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mTiHLHu3sP",
        "replyto": "mTiHLHu3sP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5046/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605803,
        "cdate": 1696707605803,
        "tmdate": 1701465548377,
        "mdate": 1701465548377,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a retrieval-augmented strategy to enhance story generation. In short, based on some metadata (e.g., plot, genre, mood), similar plots from the IMDb dataset are retrieved, and then relevant info from these plots is then used to revise an initially-generated story. All reviewers praise the idea's creativity and the evaluation conducted by experts. However, there are concerns about the method's novelty (there is perhaps too much complexity in the prompting strategy) and performance (improvements directly attributed from the paper's main ideas are small). The authors provided many new experiments and details in their rebuttal / discussion which assuaged other complaints about generalizability, which they are to be commended for, and I'd encourage the authors to include human evaluations in the next version of the paper for the new configurations they tried in their rebuttal. Overall, definitely a good inclusion to Findings!"
            }
        },
        "id": "YSF6WzFmyH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mRETTyZEJa",
        "replyto": "mRETTyZEJa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission731/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494625,
        "cdate": 1696707494625,
        "tmdate": 1701465408671,
        "mdate": 1701465408671,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a method to combine entity knowledge from KGs with language models for imporving the performance on the task of fake news detection, as measured on two benchmark data sets. Entity mentions are extracted from text and linked to a knowledge graph, then a subgraph is extracted and used in a graph-neural network, and its output combined with a representation of text obtained using a Transformer-based model.\n\nAll reviewers appreciated the results and the introduction of a new method that is effective on standard benchmark data sets. One reviewer also appreciated the ablation experiments, which provided additional insights in the results.\n\nTwo reviewers mentioned needing to add statistical significance results on the experiments and also having more baseline methods that use information from Knowledge graphs and a more detailed discussion on part work that did this. Doing so would have likely increased the soundness scores.\n\nAll reviewers acknowledged to reading the author response, but did not change their scores as a result of it."
            }
        },
        "id": "9QDji3jdb9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mQxqo1di63",
        "replyto": "mQxqo1di63",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission173/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481005,
        "cdate": 1696707481005,
        "tmdate": 1701465389560,
        "mdate": 1701465389560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Review Summary:\n\nThe paper under review investigates the evaluation and generation of loophole behavior in humans and large language models (LLMs). It conducts experiments comparing the performance of different LLMs to humans in assessing and generating behaviors related to compliance, non-compliance, and vulnerability in scenarios with power dynamics. The primary findings indicate disparities between LLMs and humans in understanding and generating these behaviors, with GPT-3.5 performing relatively well in vulnerability generation. The paper addresses a relevant topic, shedding light on the pragmatic reasoning abilities of LLMs and their implications for AI safety. However, there are some concerns regarding methodology, definition clarity, and depth of analysis.\n\nPros from Reviews:\n\n- Relevant Topic: The paper tackles a timely and important issue concerning the evaluation and generation of loophole behavior in language models, which is crucial for understanding the capabilities and limitations of these models.\n\n- Experimental Insight: The paper provides valuable insights into the differences between LLMs and humans when evaluating and generating behaviors in scenarios with power dynamics, using a well-structured experimental approach.\n\n- Implications for AI Safety: The research has implications for AI safety by highlighting the potential challenges posed by LLMs in understanding and generating vulnerable behaviors.\n\nCons from Reviews:\n\n- Methodology Concerns: Reviewer 01 raises valid concerns about the methodology, including the need for more details on human evaluation, potential cultural influences on humor and distress, and the lack of statistical analysis for interpreting results. These issues impact the robustness of the study's claims.\n\n- Definition Clarity: Reviewer 03 notes that the definition of 'loopholes' could be clearer and suggests a more in-depth exploration beyond superficial phenomena. A clearer definition and deeper analysis could enhance the paper's quality.\n\nAs AC, I have noted the large discrepancy between the soundness scores of the reviewers and asked for a discussion. Only one reviewer gave convincing arguments as to why the soundness is borderline. Looking at the depth of the review by Reviewer HszC and their arguments in comparison with the other reviews, I have to side with Reviewer HszC and agree that at this point there are open questions regarding the methodology which should be revised before the paper can be accepted. Nonetheless, I also agree with the reviewer that \"[t]he work tackles an interesting and challenging research question and is worth the attention of the NLP community\"."
            }
        },
        "id": "0IXWQsmaMI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mPaNp1eglz",
        "replyto": "mPaNp1eglz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4023/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581408,
        "cdate": 1696707581408,
        "tmdate": 1701465517210,
        "mdate": 1701465517210,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Reject"
            },
            "comment": {
                "value": "This manuscript provides an analysis and discussion on the state of fairness research made available through the ACL anthology. The authors find significant gaps as research predominantly is focused on gender bias and in English.\n\nAll three reviewers note the importance of this work, however two important concerns are raised which are not addressed by the authors in the author response:\n\n1. The organisation of the manuscript makes it hard to read and hard to draw out findings from the survey\n2. The keywords used to seed the paper selection may be too restrictive\n\nWhile both warrant answers and additional work, the lack of clear presentatin suggests that the manuscript can use another round of editing."
            }
        },
        "id": "splYr2Cit3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mNMwIwydgr",
        "replyto": "mNMwIwydgr",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission3993/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580805,
        "cdate": 1696707580805,
        "tmdate": 1701465516365,
        "mdate": 1701465516365,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper contributes a new task and dataset: comparing social norms across Chinese and American cultures. They build upon the Social Chemistry dataset to present a novel corpus of 3k social situations in the Chinese language and analyze cross-cultural norm differences. \n\nSeveral reviewers commented on the importance of this paper as the “first paper that addresses the cross-cultural social norm identification task” and “mitigat[ing] the huge US-centrism bias in NLP research”.\n\nThe author response was very thorough, causing at least one reviewer to increase their soundness score. Although I see this paper as primarily a task and dataset paper, the authors have also conducted many empirical experiments that show there is still room for models to improve on this task and dataset. For reviewers 7iE3 and nK7w reasons to reject as “some other LLMs” and “a simple tailored model”, these arguments seem to fall under the “The authors could also do [extra experiment X]” heuristic/shortcut discouraged in the reviewer guidelines: https://2023.aclweb.org/blog/review-acl23/#2-check-for-lazy-thinking"
            }
        },
        "id": "yKoYq7J8Nn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mN62FSvZVW",
        "replyto": "mN62FSvZVW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1921/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530061,
        "cdate": 1696707530061,
        "tmdate": 1701465447256,
        "mdate": 1701465447256,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper \"INSTRUCT EXCEL: A Benchmark for Evaluating Language Models in Spreadsheet Software\" has received positive feedback for its innovative benchmark dataset and the insights it provides into language model performance. However, concerns about evaluation criteria, limited scope, and the need for further details on dataset quality control and inter-annotator agreement need to be addressed. The average soundness score is strong (4), indicating a solid foundation, while the average excitement score is ambivalent (3), suggesting room for improvement."
            }
        },
        "id": "T3oDxUHlPB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mLlJavL0PB",
        "replyto": "mLlJavL0PB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4170/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584592,
        "cdate": 1696707584592,
        "tmdate": 1701465522836,
        "mdate": 1701465522836,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "After spirited discussion, the reviewers agree that this is a sound paper with a surprising and interesting result: you can use entropy to select with chain-of-thought rationales are effective for QA tasks. Where there is less agreement among the reviewers is how excited we should be about this. While this is a useful tool, the method seems to be dependent on some important details not adequately specified in the submission (but addressed in the author response period) and thus hard to replicate, and this would be a stronger paper if there were more emphasis/discussion on why this works. However, many practitioners will be immensely interested in the \"how\", which may be enough."
            }
        },
        "id": "GGIfVQQozY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mLJOMUwQyz",
        "replyto": "mLJOMUwQyz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5570/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615232,
        "cdate": 1696707615232,
        "tmdate": 1701465562564,
        "mdate": 1701465562564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers overall agree that the paper is sound and moderately exciting. While the paper has some flaws, these are mostly because it is difficult to experiment with MoEs if one does not have large computational resources. The initial results are very promising, and the paper is well-written and technically novel."
            }
        },
        "id": "yiNX9Qrukx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mKuH13Oq3x",
        "replyto": "mKuH13Oq3x",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission210/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481874,
        "cdate": 1696707481874,
        "tmdate": 1701465391228,
        "mdate": 1701465391228,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agree that this is an interesting and well-motivated study, and while there were some concerns raised about the lack of certain comparisons/evaluations and a lack of detail about the annotation process, discussion with the authors helped resolve these areas of concern, for the most part. They presented some follow up results and discussion, which should be included in the next draft of the paper (particularly the experiments regarding the different stages of the pipelined approach)."
            }
        },
        "id": "rx8ILo594D",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mJCXoiIeJU",
        "replyto": "mJCXoiIeJU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5125/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707607530,
        "cdate": 1696707607530,
        "tmdate": 1701465550675,
        "mdate": 1701465550675,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Thanks for your analysis. To me what you showed is kind of trivially true but there is a significant part of the research community who will find your results surprising. Therefore, I believe your paper can help to advance the state of the art in Question Answering."
            }
        },
        "id": "RAVcHzcoUy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mIsrzEjeG4",
        "replyto": "mIsrzEjeG4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1974/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531220,
        "cdate": 1696707531220,
        "tmdate": 1701465448952,
        "mdate": 1701465448952,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers agreed that the paper was sufficiently sound, but generally agreed that the scope of the paper was limited. Reviewers raised concerns about the evaluation task (only hallucination detection, rather than downstream performance on VLN after filtering augmented data, or instruction generation), small size of the test dataset, and limited type of hallucinations detected. However, reviewers also identified that the problem is important, and the method is simple (uses a small number of word-swapping rules, and random sentence appending, in combination with pre-training) but effective. This method, or extensions of it, may prove helpful in some downstream tasks. \n\nOne additional paper on hallucination detection for generation in VLN settings is Huang et al., Multi-Modal Discriminative Model for Vision-and-Language Navigation. SpLU-RoboNLP workshop, 2019, although it doesn't diminish the novelty of this paper as it keeps the language the same and corrupts paths, and doesn't use pre-training."
            }
        },
        "id": "IW6E6ArZYH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mGEfAu17Rk",
        "replyto": "mGEfAu17Rk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4161/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584414,
        "cdate": 1696707584414,
        "tmdate": 1701465522450,
        "mdate": 1701465522450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a simple prompting strategy for zero-shot classifying tweets according to COVID-19 conspiracy theories. Examples are augmented with a human-written or generated definition of the conspiracy theory being classified. The added definitions (especially human-written and generated ones similar to human-written ones) improve accuracy at the task. Reviewers generally thought the method is well-motivated and the paper is clear and well structured. One concern is the narrow set of experiments (one task and only using ChatGPT), but I think this lack of breadth is acceptable, although not ideal, for a short paper. The authors address reproducibility concerns over ChatGPT in the rebuttal. Reviewers did not give high excitement scores because the method is simple and the results are mostly unsurprising. However, that does not prevent the method from being useful or worth studying, and the paper does provide a focused contribution suitable for a short paper."
            }
        },
        "id": "okaH2aUG9i",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mERmlOPxPY",
        "replyto": "mERmlOPxPY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3394/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561806,
        "cdate": 1696707561806,
        "tmdate": 1701465495962,
        "mdate": 1701465495962,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a multimodal dataset of theory of mind inferences for event causality reasoning in social scenarios, which are set in a crime setting. The paper shows that theory-of-mind enhanced models achieve better performance on the task that models that do not perform such reasoning. \nThe reviewers agree that the presented task is an important and timely task, and that the dataset provides a new unique resource for this task; they also agree that the paper is presented very clearly, that the dataset is described well and that the evaluation is thorough. The concerns of the reviewers pertain to the generalizability of findings  / distributions of social inferences in the particular crime domain that was chosen to construct the dataset, but I think that the authors can address this in the discussion in the final version of the paper. The reviewers also had some questions regarding annotator agreement and a human evaluation, which are addressed well in the rebuttal, in my opinion."
            }
        },
        "id": "tj4NtBAM7o",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mDgLGrL6ze",
        "replyto": "mDgLGrL6ze",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission566/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490954,
        "cdate": 1696707490954,
        "tmdate": 1701465403701,
        "mdate": 1701465403701,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper conducts extensive experiments to compare a range of LLMs, datasets, and learning methods used for instruction-tuning LLMs in Chinese and investigates other factors including vocabulary and prompt language. The reviewers in general think the paper is well-written and is valuable for practitioners who plan to utilize these models and datasets, but feel less excited about it. I'd recommend accepting it into Findings."
            }
        },
        "id": "7JDIZe7cJu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mDPUF7ubAv",
        "replyto": "mDPUF7ubAv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission936/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499800,
        "cdate": 1696707499800,
        "tmdate": 1701465415411,
        "mdate": 1701465415411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Strengths**:\n\n1. Paper studies a critical problem of detecting hallucination in LLMs from an unanswerability perspective.\n\n2. 3 methods are proposed: (i) prompt engineering to check for unanswerability (ii) beam relaxation: checks for unanswerability cues in the generated candidates using cue phrase list. (iii) linear SVM classifier on first generated token to check for answerability.\n\n3. Experiments with three different LLMs, and three different datasets.\n\n**Weaknesses**:\n\n1. Proposed methods are very simple.\n\n2. Not clear to me even from the rebuttal as to how erasing experiments can help in performing interventions in that subspace in order to modify the model’s behavior.\n\n3. There is not enough analysis: The different approaches are not compared with each other. Also, it's not clear why few-shot settings do worse than zero-shot in many cases\n\n**Suggestions**:\n\n1. Please handle typos. There are too many.\n\n2. Explain motivation for erasing experiments more clearly.\n\n3. \"relatively poor performance in both NQ and MuSiQue compared to Squad\" -- please include your explanation in the main paper.\n\n4. Limitations section should include: (a) expts on QA with context only. (b) lack of comparison across methods. (c) Small cue phrase list (d) 3 datasets and 3 models. Need more expts to generalize to other datasets/models."
            }
        },
        "id": "OzHF89fG7q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mCnBRLJuhY",
        "replyto": "mCnBRLJuhY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5592/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615443,
        "cdate": 1696707615443,
        "tmdate": 1701465563011,
        "mdate": 1701465563011,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents an approach for debiasing VQA models by identifying features that are likely to have spurious correlations with labels---those that are have low information content but are still predictive of the labels---using methods motivated by causal theory (Average Treatment Effect and Rate Distortion), and mitigate those correlations by feature reweighting and additional losses. Experiments compare the proposed approaches with models that do not use any debiasing or use debiasing techniques leveraging knowledge of biases, including unimodal feature based debiasing and data augmentation. The proposed approaches do better than all these except those that use data augmentation, but the authors argue that data augmentation can often be expensive and requires knowledge of biases.\n\nThe reviewers recommended providing additional explanations and visualizations to further clarify how the methods work and the authors have agreed to do so.\n\nIn addition, I would encourage the authors to include\n1. a discussion on the limitations of targeted data augmentation methods for debiasing models, and in what scenarios _discovering_ spurious correlations would be practically more useful. It might help to include experiments on a wider range of distribution shifts to show the benefits of the proposed techniques over data augmentation. This would be helpful particularly given the large gap in performance between D-VQA and the proposed method.\n2. a discussion on how the proposed approach relates to work on slice discovery methods (e.g.: Eyuboglu et al., 2022).\n\n\nReference\n\nEyuboglu et al., 2022: Domino: Discovering Systematic Errors with Cross-Modal Embeddings, ICLR 2022"
            }
        },
        "id": "NvcatU0lru",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "mAxs9qiXbo",
        "replyto": "mAxs9qiXbo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1255/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508715,
        "cdate": 1696707508715,
        "tmdate": 1701465425299,
        "mdate": 1701465425299,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses emotion arcs using lexicon-based and machine learning approaches. The reviewers have praised the paper's originality addressing a topic that was largely unexplored in previous work. \n\nWhile one of the reviewers is very positive about the paper, the other two reviewers have identified several points that need improvement. For example, they have pointed to the lack of clarity in the evaluation and the description of experiments."
            }
        },
        "id": "ZnxqgoGUx5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "m1TV5K9Cvc",
        "replyto": "m1TV5K9Cvc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4703/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707596535,
        "cdate": 1696707596535,
        "tmdate": 1701465538887,
        "mdate": 1701465538887,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates privacy risks from large language models like ChatGPT and the New Bing. While these advanced LLMs can successfully refuse previous privacy extraction attacks, it remains vulnerable to the multi-step attack proposed in the paper. Experimental results demonstrates its effectiveness against ChatGPT and New Bing models. Overall, this paper poses potential privacy risks introduced by application-integrated LLMs.\n\nPros:\n\nAs recognized by all the reviewers this is a timely and well-motivated topic which investigates the privacy leakage issue of ChatGPT\n\nProposes a multi-step jailbreaking prompts to evade the ChatGPT privacy leakage defense mechanism.\nThe paper is well-written and understandable\n\nCons:\n\nLack of Technical innovation- The paper studies the effect of some of the jailbreaking prompts but there is no new technique or methodology as part of the paper’s contribution. There have been many prompt based attacks on LLMs and this seems to be yet another way. \n\nIt would have been more substantial technical contribution if the authors could have proposed some way of defending these attacks or had a more extensive study across different domains/datasets/PII/models or had a resource contribution (a benchmark or dataset that can help further research in this direction)\n\nThis paper can be categorized more as a case-study and as all 3 reviewers have pointed out, the study is also quite limited in terms of domains or types of PII or datasets and models. I feel just some privacy analysis on a single model and a single aspect of privacy (PII data extraction) can be a blog or article rather than a full-paper"
            }
        },
        "id": "TmEWRKbxZa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ls4Pfsl2jZ",
        "replyto": "ls4Pfsl2jZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission327/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484881,
        "cdate": 1696707484881,
        "tmdate": 1701465394928,
        "mdate": 1701465394928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents an approach for stance detection in social media, using Chain-of-Thought (COT) embeddings. The paper does not extract the label directly from the LM output. Instead, a BERT classifier is trained to use the LM output as an additional (or alternative) feature. Reviewers commented that the approach is simple and likely novel, and the idea could be relevant to a range of text classification tasks.\n\nThe main concerns were limited reproducibility because of the use of ChatGPT. Experimental comparison was also limited. Also, certain details such as data distribution were missing. Authors have provided additional experiments in the rebuttal for some of these concerns, and these should be added to the main paper."
            }
        },
        "id": "48zFoNsbYI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lqe06F5OiU",
        "replyto": "lqe06F5OiU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4713/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707596775,
        "cdate": 1696707596775,
        "tmdate": 1701465539295,
        "mdate": 1701465539295,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper studies how to identify and analyze the sources that journalists use in their news articles. The authors defined 16 types of sources and annotated 1304 news articles with the source type and the parts of the article referring to the source. The paper then develops and evaluates models to detect whether a sentence can be attributed to a source or not, and to retrieve the source. The paper also applies the model to a larger corpus of news articles to reveal insights about different reference patterns.\n\nOverall, the paper introduces a new task and contributes an important dataset for further research. All three reviewers appreciated the novelty and the potential impact of the work. However, they also provide some suggestions to further improve the readability of the paper which I would encourage the authors to consider while preparing the future version of the paper."
            }
        },
        "id": "KnedURgWPK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lpyU0zyEsS",
        "replyto": "lpyU0zyEsS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5018/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605007,
        "cdate": 1696707605007,
        "tmdate": 1701465547531,
        "mdate": 1701465547531,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a method to cluster news articles into key events using a large language model (LLM) and outlier detection. The paper uses the NELA dataset of news articles from various sources and domains. The paper first uses the LLM to generate event summaries from the articles and then remove irrelevant or noisy articles from the clusters. The authors evaluate the proposed method against topic models as baselines and shows that it produces more coherent and informative event clusters.\n\nI thank the reviewers for providing very constructive feedback and the authors for addressing concerns by executing additional experiments. Especially, the inclusion of BERTopic as an additional baseline is commendable. I would encourage the authors to consider other suggestions such as using another LLM such as Llama2 in addition to GPT3.5 model currently used."
            }
        },
        "id": "U6pTrb9bhs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lojtRAQOls",
        "replyto": "lojtRAQOls",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5373/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611756,
        "cdate": 1696707611756,
        "tmdate": 1701465557723,
        "mdate": 1701465557723,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agree that this is a solid paper which contains a comprehensive evaluation. The evaluation results indicate that the proposed method works well.  \n\nThe main reserve expressed by the reviewers is the limited novelty of the method. However, the authors' answer emphasise the fact that even though supervised contrastive learning has been used before, this is the first time when it is used as a representation learning tool. In response to questions from the reviewers, the authors have experimented with GPT-3.5-turbo in a zero shot setting for sentiment analysis. This information can be included in the paper if it is accepted. \n\nThe reviewers ask a few other questions related to methodology/technical details, which are successfully answered by the authors. In response to one of the reviewers' observation that there are no papers from 2023 in the related works section, the authors have conducted a quick survey of work from 2023 from venues other than ACL. This information can be added in the final version of the paper. \n\nOverall, the reviewers indicate that this is a strong and interesting paper."
            }
        },
        "id": "KJxHg1baJN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "llv2GnH5bD",
        "replyto": "llv2GnH5bD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1922/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530126,
        "cdate": 1696707530126,
        "tmdate": 1701465447331,
        "mdate": 1701465447331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents the Longtriever to efficiently model long text through intra-block and inter-block encoders, and then proposes a pre-trained LMAE task. This paper is well-presented, and the idea is interesting. Longtriever achieves superior performance on two dense retrieval datasets, MS MARCO and TREC. All reviewers were positive about this work, seeing it as solid work falling in an important area for the NLP community."
            }
        },
        "id": "nsVZyJ8uSg",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ljsGKc8cVR",
        "replyto": "ljsGKc8cVR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission624/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492369,
        "cdate": 1696707492369,
        "tmdate": 1701465405661,
        "mdate": 1701465405661,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Based on the idea that sentiment polarity of the specific target is not governed by its type but its context, authors propose a unified model for target-oriented multimodal sentiment classification, UnifiedTMSC. It is prompt-based language modelling and performers well on four datasets over two target types.\n\nReviewers give all good (3) in soundness; two ambivalent (3) and one strong (4) in excitement. Reviewers have some concerns about novelty (the use of prompts for language modeling is somehow not so exciting), clarity of details and reproducibility. However, this paper is well written and the comprehensive experiments support good experimental results."
            }
        },
        "id": "KKTlD178s6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ljjy0Sw5sx",
        "replyto": "ljjy0Sw5sx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2279/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538488,
        "cdate": 1696707538488,
        "tmdate": 1701465459825,
        "mdate": 1701465459825,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Jointly modeling of conversation semantics and strategies improves final model performance. The paper is well-written and well-motivated; however, it should incorporate suggested changes by our reviewer."
            }
        },
        "id": "cvBkSZRQjO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lhSLoOYLDv",
        "replyto": "lhSLoOYLDv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3870/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578246,
        "cdate": 1696707578246,
        "tmdate": 1701465512402,
        "mdate": 1701465512402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper is about a new method for sentence ordering. Although the motivation and the task are not the most \"exciting\" (sentence ordering as a proxy for discourse coherence), the reviewers agree on the novelty of the method which predicts sentences in parallel (as opposed to sequentially). One reviewer expressed some concern that the task is too simple which risks not pushing the method enough, but they all agree that the results are \"sound\" and with potential to be built upon by other researchers working on non-autoregressive generation."
            }
        },
        "id": "SSPtdSkpdt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ldtjC7TSJ5",
        "replyto": "ldtjC7TSJ5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4945/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602101,
        "cdate": 1696707602101,
        "tmdate": 1701465545400,
        "mdate": 1701465545400,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper evaluates recent LLMs (Language Model Metrics) as summarization metrics, using ChatGPT and GPT-4. It provides an extensive exploration of ChatGPT's capabilities as a metric and juxtaposes its strengths and weaknesses against alternative metrics such as ROUGE, BERTScore, and BARTScore. These evaluations are grounded in the SummEval benchmark dataset, which includes human annotations for various summarization models.\n\nWhile some reviewers expressed concerns about the use of a single dataset in the experiments, I believe it's important to note that this limitation is not a valid reason for rejection. This limitation arises due to the lack of alternative high-quality datasets for meta-evaluation in abstraction summarization.\n\nHowever, reviewers did raise legitimate concerns regarding missing experiments involving other (commercial) LLMs, the need for further human examination of LLM reasoning correctness, and practical guidance on utilizing the efficient framework. The authors have provided additional results in their rebuttals, which should be incorporated into the paper along with a comprehensive discussion to address these concerns."
            }
        },
        "id": "1g56z2xgu9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ldbYAF0ad0",
        "replyto": "ldbYAF0ad0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2660/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546968,
        "cdate": 1696707546968,
        "tmdate": 1701465472543,
        "mdate": 1701465472543,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary:**\nThis paper investigates the de-identification of clinical notes, with a particular focus on a cross-hospital setting.\nThe authors have constructed a corpus (which they intend to make available) comprising Electronic Health Records (EHRs) from three different hospitals. They have developed or fine-tuned various neural models to tackle the de-identification task. The authors have evaluated Chinese BERT-based models that have been fine-tuned using this corpus. They have conducted comparisons between same-hospital and cross-hospital settings.\nThe paper illustrates that, despite achieving high test set performance for \"within-hospital\" scenarios, there is a significant drop in performance in cross-hospital settings, even for relatively straightforward tasks such as identifying Personal Health Information (PHI) related to specific named entities (e.g., location, patient name, hospital name, patient ID, date/timestamps, patient/physician/hospital contact information, patient age/profession).\nIn addition, the authors have explored various domain generalization techniques to address this issue when training with a single hospital dataset is the only option. They found that only Stochastic Weight Averaging and text smoothing yield slight improvements.\n\n**Strengths:**\nThe main contribution of this work lies in the corpus constructed by the authors. While the corpus is not currently accessible, the authors have expressed their intention to release it upon the paper's acceptance.\nThe dataset holds significant value, given the scarcity of clinical corpora, especially those that encompass cross-hospital and non-English contexts. Furthermore, this work merits recognition for its role in highlighting the generalization gap observed in deep learning models.\n\n**Weaknesses:**\nReviewers have identified the following weaknesses in the paper:\n1. The techniques employed by the authors are standard. It is challenging to assess the significance of errors since errors in identifying age or the hospital are not equivalent to errors in identifying patient or doctor names.\n2. The authors opted to create their medical BERT-based model (HM-BERT) while disregarding existing pre-trained medical models. It would have been valuable to explore alternative models for comparison.\n3. The paper lacks an in-depth analysis of the aspects contributing to the generalization gap. For instance, there is insufficient exploration of the similarities and differences between the data from different hospitals that might explain the drop in model performance. It is possible that the document structure plays a role in this drop, and the proposed generalization techniques do not address structural differences.\n4. The paper's conclusion, stating that cross-hospital results are worse than in-hospital results, is expected and does not provide a novel insight.\n\n**Author-Reviewer discussion and acknowledgment:**\nDuring the rebuttal phase, the authors have responded to the questions and concerns raised by the reviewers by providing clarifications and outlining their planned improvements to the paper.\n\n**Conclusion:**\nThe paper is well-written and clear, offering valuable insights to the community, particularly in the context of medical NLP applications. However, reviewers recommend that the authors incorporate additional references and enhance the paper based on the points raised during the discussion phase. Additionally, the authors should address the identified typos."
            }
        },
        "id": "UBPNw9Mb7P",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lbtVebcVny",
        "replyto": "lbtVebcVny",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2395/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541338,
        "cdate": 1696707541338,
        "tmdate": 1701465463823,
        "mdate": 1701465463823,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers have a consensus that work is sound and exciting. The finetuning comparison as well as the clarrification in the review discussion should be included in further edits of the paper."
            }
        },
        "id": "gUBXPZTL3b",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lWlBAJTFOm",
        "replyto": "lWlBAJTFOm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5453/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613255,
        "cdate": 1696707613255,
        "tmdate": 1701465559712,
        "mdate": 1701465559712,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes and explores a lightweight approach to transfer factual knowledge across languages in multilingually pre-trained language models by using Language Representation Projection modules (LRP2). Overall, the paper presents a consistent takeaway that usage of LRP2 on multiple multilingual models shows improvement in the knowledge probing tasks using the mLAMA dataset. However, as pointed out by multiple reviewers, many important details are packed into the appendix and also into the discussion posts. The AC strongly recommends including it in the appendix and on the fifth page (in case of acceptance) given that the paper is pretty dense."
            }
        },
        "id": "wIsVEa1YUc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lVat423gKI",
        "replyto": "lVat423gKI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2295/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538941,
        "cdate": 1696707538941,
        "tmdate": 1701465460435,
        "mdate": 1701465460435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper applies structural priming, a method in psycholinguistics for studying the presence of linguistic structure in human language processing, to multilingual language models. A cross-lingual setup for priming is followed, testing multiple grammatical alterations. The results show that language model behaviour correlates with those of human subjects. The reviewers expressed some concerns about an overly broad interpretation of the results, however the study still makes a valuable contribution to the question studied."
            }
        },
        "id": "zHmqyCiqGS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lReh4LaP8f",
        "replyto": "lReh4LaP8f",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3911/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578977,
        "cdate": 1696707578977,
        "tmdate": 1701465513825,
        "mdate": 1701465513825,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an analysis of how objects may be implicitly associated with genders in image captioning models. The analysis recovers traditional gendered associations, e.g., lipstick with women. The paper also presents a metric that can be used to evaluate gender bias in object representations. I would suggest adding some discussion of the limitations of studying gender as a binary category and of autoamtic gender classification in general."
            }
        },
        "id": "0J3MyRuysj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lOPMuJSVz8",
        "replyto": "lOPMuJSVz8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3988/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580739,
        "cdate": 1696707580739,
        "tmdate": 1701465516249,
        "mdate": 1701465516249,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new framework that incorporates sub-graph structures into PLMs. In particular, their new framework, ReasoningLM uses subgraph-aware self-attention. Their experimental results show the effectiveness of the proposed method on multiple KBQA. While I agree with the concern about limited novelties noted by all of the reviewers (i.e. sparse masking of transformers has been explored in prior work), incorporating subgraphs for effective masking requires substantial work, and experimental results on KBQA are strong. Therefore, I recommend for acceptance."
            }
        },
        "id": "fiC8N20eZB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lKi1myznJe",
        "replyto": "lKi1myznJe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission984/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707501201,
        "cdate": 1696707501201,
        "tmdate": 1701465416890,
        "mdate": 1701465416890,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a new dataset for multi-party dialogue summarization in French. It consists of transcribed political debates, which are segmented into sub-topics and annotated with abstractive and extractives summaries. The paper reports model performance on this dataset for several existing summarization models and alignment approaches (for aligning summaries with original speech).  \n\nThe reviewers raise several questions about the annotation process which are not sufficiently explained in the paper, and raise concerns regarding use-policies of the underlying data (the authors address these satisfactorily in their rebuttal)."
            }
        },
        "id": "7QIiqDHqM0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lKPReKSJio",
        "replyto": "lKPReKSJio",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3618/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566339,
        "cdate": 1696707566339,
        "tmdate": 1701465504505,
        "mdate": 1701465504505,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a general strategy for decomposing a complicated task into a set of easier sub-tasks. Specifically, the proposed method represents each example with a vector consisting of a list of binary questions. By this decomposition, the proposed method not only improves prediction performance but also can generate more interpretable predictions (or be used as inputs to interpretable machine learning models). The reviewers commonly agreed that the proposed method is interesting and novel, although expressed their concerns about the experiment setup (e.g., baseline methods) as well as whether the method is generally applicable to real-world applications."
            }
        },
        "id": "DtGiE0HqDS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lCy3RwscMn",
        "replyto": "lCy3RwscMn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4427/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589462,
        "cdate": 1696707589462,
        "tmdate": 1701465531472,
        "mdate": 1701465531472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents ReXMiner, a new approach to zero-shot relation extraction from webpages that encodes relative shortest paths in the Document Object Model (DOM).  The sense of zero-shot here is following prior work (e.g. Lockard et. al.) in that the model is trained on one domain (e.g. NBA), and tested on webpages from another domain (e.g. Movies).\n\nReviewers appreciated the simplicity of the approach as a nice extension to MarkupLM, and the fact that the paper is relatively well written. \n However, some reviewers comments raised concerns about whether the contribution is incremental with respect to prior work to justify acceptance as a long paper.  Reviewers also indicated they would like to see a comparison to a baseline that uses LLMs.\n\nI have some concerns about the experiments, for example, what are the dev/test splits used to select hyperparameters - are these the same that were were used in prior work?  The ZeroShotCeres paper indicates they do not use a development set of pages from the target websites, whereas in Appendix A the paper mentions: \"As for the hyper-parameters, we select a subset of web pages from each web site in the SWDE dataset as validation set to find the best hyper-parameters.\", which seems to indicate that pages from the target website was included in the development set, so this seems like it may not be a fair comparison."
            }
        },
        "id": "nIDxjHO0MM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lC4vFCM2VA",
        "replyto": "lC4vFCM2VA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission57/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478021,
        "cdate": 1696707478021,
        "tmdate": 1701465385642,
        "mdate": 1701465385642,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper examines the interplay between narrative communication style and misinformative health content on user engagement on Twitter. Three reviewers, who reviewed the submission, highlighted the strengths of the paper. R1 appreciated its valuable dataset, R2 appreciated the analyses, and R3 appreciated the paper’s outcome in motivating this line of research. However, all the reviewers raised significant concerns about the paper, some of which were addressed during the rebuttal phase by the authors—and the reviewers accordingly updated their ratings. Particularly the reviewers commended the authors on the additional thematic analysis."
            }
        },
        "id": "jW2Aldpg0s",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "lBAc5JgyMI",
        "replyto": "lBAc5JgyMI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission76/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478527,
        "cdate": 1696707478527,
        "tmdate": 1701465386337,
        "mdate": 1701465386337,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a skip-computing model which achieved good results on basic benchmark dataset. It's a well written paper and demonstrate good empirical performance. \n\nPros:\n\n1. Review work is solid\n2. An effective method.\n3. Write well and easy to follow.\n\nCons:\n\n1. The comparison baseline is a bit limited. Despite during discussion period authors add many results and commit they will add to the final version, there is no guarantee this will be happening and the additional information could be integrated well into present materials.\n\n2. Authors are not proactive on adding real-time wall-clock results. Only after many rounds of discussion, they add some statistics compared to Transkimmer only. And it's only speed-up ratio but not wall-clock time which makes the results not verifiable.\n\n3. The comparing models are relatively small."
            }
        },
        "id": "xwK59djWur",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "l4eviuXtBd",
        "replyto": "l4eviuXtBd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2094/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534323,
        "cdate": 1696707534323,
        "tmdate": 1701465453796,
        "mdate": 1701465453796,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper examines the automatic identification of cognitive distortions with “diagnosis of thought” prompting. They use a three stage technique: first assessing the subjectivity of statements, second looking at statements that support or contradict the statements, and lastly an analysis of schemas, which means matching the statements to a cognitive distortion type. They show that using ChatGPT and GPT-4 they can improve distortion detection with this prompting approach. The two of the reviewers are not excited by this work, seemingly because of the novelty of the prompting method. The application area is exciting and does have potentially high impact. Reviewer eTZW makes some helpful suggestions for related work that should be expanded upon. The human evaluation and consultation with experts is valuable in this paper. I disagree with reviewers in that their perceived lack of novelty should not lead to a low soundness score, so I think this paper is appropriate for findings."
            }
        },
        "id": "Gtp9UhgJAZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kyHwalUpPu",
        "replyto": "kyHwalUpPu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2519/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543814,
        "cdate": 1696707543814,
        "tmdate": 1701465467530,
        "mdate": 1701465467530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree this work conducts a comprehensive investigation into an important topic of analysis, and yields insights into the changing nature of the knowledge acquisition and utilization gaps with model scale. While reviewers note challenges in deriving robust and generalizable findings without exploring many datasets, finetuning and probing methods, they do acknowledge the authors have thought deeply about the problem and experimental setup to make their findings as useful and possible."
            }
        },
        "id": "94bZtF4kuY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kuwz9k061u",
        "replyto": "kuwz9k061u",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1134/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505683,
        "cdate": 1696707505683,
        "tmdate": 1701465421418,
        "mdate": 1701465421418,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper addresses an unexplored topic in non-compositional expression generation. The proposed method has been properly evaluated in comparison with various baselines, demonstrating the effectiveness of the proposed approach. A common complaint about the paper is the lack of enough details for the experimental setup. While releasing the code may help in addressing the reproducibility of the results, the paper itself should also contain enough details for understanding the proposed approach and the evaluation settings. I strongly encourage the authors to address the reviewers' concerns in the next revision."
            }
        },
        "id": "TJejscNaBK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kuYRp78Qnp",
        "replyto": "kuYRp78Qnp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2178/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536292,
        "cdate": 1696707536292,
        "tmdate": 1701465456878,
        "mdate": 1701465456878,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In their long paper submission, “ROBBIE: Robust Bias Evaluation of Large Generative Language Models“, the authors propose a bias/ toxicity assessment benchmark consisting of 12 demographic axes and 6  bias and toxicity datasets (including 2 novel ones) using which they test 5 model families and 3 bias mitigation techniques.\n\nThe reviewers value the rich set of the experiments with high soundness scores, and also the transparent discussion of the limitations of this work. The main claims of this work are supported! (At the same time, the authors did not release the code (yet) which hinders reproducibility.)\nCompared to the soundness assessment of this work, the excitement of the reviewers is a bit more limited. This is, for instance, due to the practical impact being rather unclear."
            }
        },
        "id": "CaZQBOqx9o",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ktzudN7JmJ",
        "replyto": "ktzudN7JmJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1066/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504217,
        "cdate": 1696707504217,
        "tmdate": 1701465419248,
        "mdate": 1701465419248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents GenPPN, a generative post-processing components in dialogue systems to improve task completion. Experiments both through simulation and human evaluation on MultiWOZ dataset show improvement to the task completion performance of TOD system. The work is generally considered thorough, and the paper is well-written and easy to follow. One discussion point during rebuttal is around the high performance baseline Template NLG, which is sufficiently answered by author responses."
            }
        },
        "id": "SKjojXw3D7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kspXkK9PtA",
        "replyto": "kspXkK9PtA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5754/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617873,
        "cdate": 1696707617873,
        "tmdate": 1701465566390,
        "mdate": 1701465566390,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper contributes a new high-quality dataset for legal Information Extraction on legal wills derived from public domain court documents from two US states. It discusses preliminary empirical studies to investigate the capability of ChatGPT4 on the task.  Reviewers and authors exchanges during discussion helped clarify some of the critical issues highlighted in the reviews. Overall, the paper was found to be interesting and innovative. However, some revisions are needed to make it more convincing. For instance, discussing the differences between the wills from the two states, as mentioned in response to the reviewers, would yield valuable insights for other researchers.  Given that this is a short paper, it needs at least to better explain the importance and novelty of IE on legal wills and to better justify the design choices by providing additional details, as indicated by reviewers. \n\nHere below is a summary of the main strengths and weaknesses identified: \n\n**Pros**  \n\n- a new corpus for information extraction from legal wills; \n\n- demonstration  of the adequacy of the Gpt4 model for IE tasks; \n\n- focuson information extraction and linking, of special interest in legal profession; \n\n- comparison with other domains, interesting for highlighting shared challenges; \n\n- portability to other US states legal wills; \n\n- presence of error analysis. \n\n**Cons** \n\n- the dataset is limited, as it covers few states and few cases. Increasing its coverage will certainly provide stronger support to the claims;  \n\n- the importance and novelty, or difference from similar tasks in legal IE, is not evident. The paper should explain it more clearly; \n\n- details on the extracted and evaluated information categories seems to be missing, which leaves some of the choices unexplained."
            }
        },
        "id": "PdS2IHl5UU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kqm0SOisFq",
        "replyto": "kqm0SOisFq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5063/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606211,
        "cdate": 1696707606211,
        "tmdate": 1701465548995,
        "mdate": 1701465548995,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers agree on the soundness and excitement of this paper. The reviewers specifically call out the significance of the result by which sequence length in LLMs can be extended to much larger while also speeding up inference. The paper is well written with solid experimental results. The community will benefit from this work being published at EMNLP."
            }
        },
        "id": "3sVwFsjksC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kp1U6wBPXq",
        "replyto": "kp1U6wBPXq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1818/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527962,
        "cdate": 1696707527962,
        "tmdate": 1701465443087,
        "mdate": 1701465443087,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper evaluates and interprets neural language models using artificial data drawn from a language model learned from a PCFG. The PCFG is constructed using the state-split method over a large corpus. This method allows the definition for a closed-form expression for obtainable perplexity. \n\nAll reviewers bring up issues with using a PCFG to evaluate language models, questioning the claimed gains in interpretability and the degree of approximation of natural language. Despite a lively author-reviewer discussion period, there was no consensus among reviewers about the soundness and excitement of the work. The authors make some good arguments in their responses, and the manuscript can be strengthened by incorporating feedback from the author-reviewer discussion and expanding on the motivation for using a PCFG. On the positive side, reviewers praised the thorough experimental analysis and noted that the finding that causal and masked LMs show differences in their correlation with the true probabilities from the PCFG is quite interesting."
            }
        },
        "id": "AGYz6bnGKh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kj4MRgh2K5",
        "replyto": "kj4MRgh2K5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5707/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617177,
        "cdate": 1696707617177,
        "tmdate": 1701465565411,
        "mdate": 1701465565411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a method for reducing the cost of labeling data for document-level extraction tasks via active learning by training a model to propose candidate regions and having annotators perform a binary annotation task where they agree or disagree with the proposed extractions. R1 and R2 agree that the method is simple and sound, addresses an important research problem, and is sufficiently backed up by appropriate experimentation. The noted limitations of the work include: the datasets used are private, so reproducibility is an issue and the paper could benefit from experimentation on a public dataset; some important details (namely about the candidate selection model) could be added. The main concern from reviewer 3 is that potentially similar approaches have been proposed on other tasks e.g. named entity recognition. The authors provide a detailed response clarifying many of the reviewers concerns about limitations related to reproducibility, generalizability, and model details. In particular, the though the spirit of the method is similar to two-step sequence annotation, the problem under study is starkly different as such tasks (e.g. NER) still require observing partial sequences, while in visually complex document extraction the authors are able to reduce the problem to individual binary decisions. Overall the paper appears to be sound and of interest to those working on this topic."
            }
        },
        "id": "YX2DYMI099",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kgxtMJHe7w",
        "replyto": "kgxtMJHe7w",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2175/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536206,
        "cdate": 1696707536206,
        "tmdate": 1701465456714,
        "mdate": 1701465456714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents TRAVEL, a reinforcement learning-based approach to retrieve FAQs most relevant to the current conversational context of a user.\n\nOverall, reviewers are satisfied with the contributions. The paper can be improved by adding results on more datasets, currently, only one dataset is experimented with."
            }
        },
        "id": "1T4SLEWEv1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kdjSXbypKX",
        "replyto": "kdjSXbypKX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3621/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566350,
        "cdate": 1696707566350,
        "tmdate": 1701465504036,
        "mdate": 1701465504036,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the \"dialogue state continual learning\" problem. The authors present a novel and straightforward example-guided QA solution, achieving SOTA on the SGD dataset. The solution is built on three foundational ideas:\n\n1. **Reformulation of DST into QA (DST-EGQA)**: This approach considerably enhances continual learning performance by improving task consistency across domains.\n  \n2. **In-context Learning** by retrieving relevant in-context examples (from past domains) during both fine-tuning and testing. This method is akin to Memory-replay, given its exposure to past training examples. However, its performance (when combined with DST-EGQA as DST-EGQA + IC-DST-ret) doesn't measure up to Memory-replay (with a 50 turn-level example memory size). Despite this, the third pillar demonstrates that Memory techniques and In-context fine-tuning can collectively enhance the performance.\n  \n3. **Dialogue-level Memory-replay**: Sampling examples at the dialogue level proves superior to turn-level sampling. This is attributed to the richer dialogue state a dialogue offers compared to an individual turn.\n\nWhile the original paper has moments of ambiguity in presenting its main contributions and experiments, the authors managed to clarify these areas during the rebuttal phase, providing reviewers and AC with a clearer understanding of their innovations. Nonetheless, there are instances of overstatements in the paper, and I recommend the authors reevaluate and modify their claims and experimental presentation.\n\nRegarding the scores provided: Soundness scores are uniformly at (3, 3, 3), while excitement scores varied at (4, 4, 3). It's evident that all reviewers are enthused by the proposed solution due to its evident performance improvements in the experimental settings. Additionally, the simplicity of the proposed solutions is commendable.\n\nHowever, reviewers expressed concerns about:\n\n1. The single testing benchmark.\n2. Limiting their experimentation to a single model size (60M).\n3. A potentially skewed comparison with the Memory-replay method.\n\nAC's feedback on these issues:\n\n1. **Single Testing Benchmark**: Though multiple benchmarks typically provide a more comprehensive demonstration of a solution's generalizability, the SGD is arguably the most fitting benchmark for the proposed solution. This is because other dialogue benchmarks combine domains, which may not serve as the ideal \"proof-of-concept\" benchmark for this study. Authors might consider other benchmarks (like a sequence of mixed-domain tasks) to further fortify their experimental claims.\n\n2. **Model Size**: In their rebuttal, the authors mention testing larger models and observing similar findings to the 60M size. But without presenting these specific results, this concern remains unresolved.\n\n3. **Comparison with Memory-replay**: A reviewer pointed out a potential unfair in comparison with the Memory-replay method. However, after examining lines 333-335 of the paper, it appears the authors incorporated all past examples when using the Memory-replay method. Authors might want to clarify this for enhanced transparency.\n\nIn conclusion, this study comes across as a \"proof-of-concept\" endeavor, mostly justifying its contributions. I consider the work to be of **moderate sound and exciting**."
            }
        },
        "id": "Vz6YmanzMM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kda8szucLZ",
        "replyto": "kda8szucLZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4206/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585391,
        "cdate": 1696707585391,
        "tmdate": 1701465524225,
        "mdate": 1701465524225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces the task of Continual Generalized Intent Discovery (CGID), tackling existing limitations of Generalized Intent Discovery (GID).  Specifically, the paper proposes the Prototype-guided Learning with Replay and Distillation (PLRD) to address this task.  The reviewers appreciate the introduction of a new task that adds continual learning to the GID task, the proposed PLRD model for solving this task, and the experiments that demonstrate the model's effectiveness.  However, the work is considered to be somewhat lacking in novelty.|meta review"
            }
        },
        "id": "drolYz0ARJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kc2YhavobV",
        "replyto": "kc2YhavobV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1665/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707522687,
        "cdate": 1696707522687,
        "tmdate": 1701465437921,
        "mdate": 1701465437921,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a multilingual resource (6 languages including English) for claim span identification, created by their proposed methodology. The reviewers appreciate the importance of the task, the dataset creation methodology, and the experiments conducted. Most of their concerns are addressed by the authors' responses."
            }
        },
        "id": "btnPkZsZrX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kayoyzcsTa",
        "replyto": "kayoyzcsTa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3243/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558945,
        "cdate": 1696707558945,
        "tmdate": 1701465491726,
        "mdate": 1701465491726,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper describes the creation of a novel COVID-19 multilingual dataset that covers multiple middle-income countries. The dataset creation is sound and with high potential for impact given the lack of datasets of this nature.\n\nThe paper is primarily a dataset paper with benchmark experiments using off-the-shelf methods. The novel contributions on the experimental side are more limited, but the insights derived from these experiments as well as the benchmark performance scores are useful for future work, in addition to the key contribution of a dataset.\n\nI would suggest the authors to take into account reviewer comments in a further revision by toning down the correlation claims and by adding a discussion on per-country performance scores."
            }
        },
        "id": "cy5f7Qj4Pf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kZob2CsZXm",
        "replyto": "kZob2CsZXm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3905/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578789,
        "cdate": 1696707578789,
        "tmdate": 1701465513510,
        "mdate": 1701465513510,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces SynCSE, a contrastive learning framework for training sentence embeddings using synthetic data. It explores two variants: SynCSE-partial, which generates positive and hard negative examples from unlabeled sentences, and SynCSE-scratch, which creates sentences and their annotations from scratch. Experimental results show that both variants outperform unsupervised baselines and perform competitively with supervised models in most settings.\n\nReviewers have all found that the paper is sound and strong, therefore, I recommend acceptance of this paper."
            }
        },
        "id": "ahQDJ8Fuzm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kY7lpT8z1E",
        "replyto": "kY7lpT8z1E",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3893/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578601,
        "cdate": 1696707578601,
        "tmdate": 1701465513032,
        "mdate": 1701465513032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an analysis with the objective of comprehending biases within machine translation systems concerning names across various dimensions, including gender and racial/ethnic groups. The authors have also contributed a dataset to facilitate the examination of this issue. The paper offers an intriguing perspective on biases in machine translation systems, supported by a comprehensive experimental framework.\n\nThe authors have provided robust responses to the reviewers' critiques, particularly in addressing the concerns raised by Reviewer 2. I concur with Reviewer 1's point that name variant errors should not be equated with translation errors, even though both aspects are crucial to assess. I strongly encourage the authors to incorporate this suggestion into the paper, as articulated in their response to the reviewers.\n\nIn summary, this work contributes to our understanding of biases in machine translation systems. By addressing the reviewers' feedback, particularly the suggestion from Reviewer 1, the paper can further enhance its clarity and impact."
            }
        },
        "id": "SUb3f1IJ9m",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kXHDXPubz9",
        "replyto": "kXHDXPubz9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1919/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530050,
        "cdate": 1696707530050,
        "tmdate": 1701465447095,
        "mdate": 1701465447095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper explores prompt optimization for dialogue applications (focusing on open-domain chit chat).\n\n**Pros**: All reviewers agree the study is fairly comprehensive, and most mention that the problem is important/relevant. There are limited technical concerns overall, and most concerns raised appear to be resolved during the rebuttal.\n\n**Cons**: There is some disagreement among reviewers on the impact of the work. While reviewers all agree on the comprehensiveness of the study, some reviewers also feel there are issues in interpretation of the results, indicating a lack of generalizable takeaways and concerns on the proposed UID metric. Authors provide a detailed rebuttal with new results and highlight relevant takeaways from the current iteration - both of which are acknowledged by reviewers - but excitement is still marginal."
            }
        },
        "id": "dKREYJ999H",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kUNzgI1HxN",
        "replyto": "kUNzgI1HxN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2493/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543310,
        "cdate": 1696707543310,
        "tmdate": 1701465466844,
        "mdate": 1701465466844,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the challenge of effectively handling long inputs in summarization tasks by proposing PEGASUS-X, an extended version of PEGASUS, with a modified architecture and additional long-sequence pretraining. PEGASUS-X aims to perform state-of-the-art long-input summarization tasks while maintaining computational efficiency. It introduces a block-wise local Transformer architecture with staggered blocks and global tokens to balance performance and memory efficiency. PEGASUS-X demonstrates strong performance on datasets like GovReport and PubMed. However, the paper's limitation lies in its domain coverage, as it primarily focuses on specific datasets and broader tasks like meeting and narrative summarization tasks are not included in the study. Also,  the paper's empirical effectiveness compared to BART-LS shows marginal improvements."
            }
        },
        "id": "B0zckmKyV0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kQSlGF9lH6",
        "replyto": "kQSlGF9lH6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission412/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487000,
        "cdate": 1696707487000,
        "tmdate": 1701465398099,
        "mdate": 1701465398099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper describes the problem of taking transcribed Chinese text and performs a style transformation to remove disfluencies and grammatical errors, presenting an output that is more consistent with written style. To solve this problem they have prepared an open-source (non-commercial) corpus of annotated data based on an existing dataset, they have fine-tuned several existing models to establish baselines for this dataset, and have demonstrated the utility of the dataset in addressing a downstream machine translation task.\n\nMain contributions can be summarized as:\n\nA new curated dataset CS2W, a Chinese Spoken-to-Written style conversion dataset comprising 7,237 spoken sentences extracted from transcribed conversational texts with four types of conversion problems.\nThorough analysis of the dataset, and benchmark evaluation of the on spoken-to-written language conversion with five SoTA baseline models. The analysis led to the discovery of a new type of disfluency.\nAnnotation guidelines and methodology behind the dataset preparation are shared with satisfactory detail.\n\nReasons To Accept:\n- This paper sets a standard for addressing multiple types of errors in spoken transcription through one model, with an adequate-sized corpus for future academic work.The paper appears to be free of major methodological errors, demonstrates the utility of multiple large language models against the task, and follows up by demonstrating a substantial improvement on a downstream task.\n\nReasons To Reject:\n- There are some errors in visualization, some typos, and some missing citations, but none of these rise to the level of being reasons to reject this paper."
            }
        },
        "id": "obbvcACaUr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kOhxudaIEj",
        "replyto": "kOhxudaIEj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2803/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549750,
        "cdate": 1696707549750,
        "tmdate": 1701465476930,
        "mdate": 1701465476930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper conducts a comprehensive study on how several approaches work together for very low-resource languages including few-shot learning, parameter-efficient tuning, and translate-train methods. The paper finds that these methods could be complementary and offer recommendations on how to combine them together. The experiments are comprehensive as well."
            }
        },
        "id": "rY1aXaogTO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kNUglj7Kq1",
        "replyto": "kNUglj7Kq1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3648/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566973,
        "cdate": 1696707566973,
        "tmdate": 1701465505357,
        "mdate": 1701465505357,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper \"A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation\" presents work on examining how gender is translated from languages not having grammatical gender to languages having grammatical gender. To this end, the paper presents work on computing the gender bias in translated data. The paper also proposes a method for reducing the bias in translations. \n\nThe main criticism points mentioned by the reviewers refers to previous work, but also to the generalization of the results based on the used data set to general MT.\nIn support of the paper the reviewers mention the experimental setup, the presented details and the insights gained from the experiments."
            }
        },
        "id": "y6wqp5USgt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kNCHv0NZ69",
        "replyto": "kNCHv0NZ69",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5565/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615158,
        "cdate": 1696707615158,
        "tmdate": 1701465562464,
        "mdate": 1701465562464,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces DisCo, a semi-supervised learning framework designed for fine-tuning a set of smaller models derived from a pre-trained language model via knowledge distillation. The primary innovation is in the co-training technique, which encourages knowledge sharing amongst the student models based on varied views produced by differing distillation strategies and input augmentations. \n\nStrengths:\nThe paper's innovative approach with KD in scenarios with limited labeled data was seen as promising, particularly its results and methodology (QmSR). The results are found to be promising, especially on the classification task, and the proposed training schema is found to provide new insights for those in the knowledge distillation domain (vrX3). The introduction of an effective SSL method is deemed crucial (Reviewer VCv8).\n\nWeaknesses:\nConcerns were raised regarding the two-student approach, with questions surrounding its necessity and the complications it introduces in model selection due to the potential variance between student models (QmSR, vrX3). The datasets used for testing were scrutinized for their authenticity in representing \"low resource\" scenarios, and some claims, such as those in table 6, were seen as possibly overstated (QmSR). Some design choices like MSE were questioned and variance of some of the results seemed too large (vrX3)."
            }
        },
        "id": "nuES032gv7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kKX9X0tMRH",
        "replyto": "kKX9X0tMRH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5607/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615822,
        "cdate": 1696707615822,
        "tmdate": 1701465563576,
        "mdate": 1701465563576,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a new approach to using speech translation (where data is relatively abundant) for pre-training E2E spoken language understanding models for both intra and cross-lingual scenarios. While not a revolutionary novel idea, the approach improves performance on a series of existing tasks and benchmarks (setting SOTA performance levels on some), and the paper also proposes new benchmark datasets for speech summarization and low/zero-resource cross-lingual transfer. Authors also commit to releasing the code and models as open-sourced, which will create an interesting and relevant test-bed for future development. Authors responded well to reviewers (whose main criticism is the organization of the paper, and the presentation of results, sometimes asking for more detail, which the authors generally provide in the response), and commit to incorporating feedback and improve readability, which I think may even allow including the approach in the main conference, certainly in Findings."
            }
        },
        "id": "2VfjREcps0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kKKzd8SaMy",
        "replyto": "kKKzd8SaMy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1658/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707522325,
        "cdate": 1696707522325,
        "tmdate": 1701465437676,
        "mdate": 1701465437676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers have reached on a consensus that the paper is well-written and acknowledged the effectiveness of the proposed method by looking at the gains compared to the baselines. The reviewers initially pointed out some missing baselines and comparison studies as well as some missing analysis of robustness, latent space characteristics, and efficiency. The authors were able to provide more in-depth studies and data points during the discussion period which mostly addressed the major concerns from the reviewers.\n\nAs a result, all the reviewers agree that the paper is good or strong at soundness (3, 4, 3). In terms of excitement, all the reviewers have reached on a consensus that it's ambivalent (3)."
            }
        },
        "id": "kkwusl4D8X",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kIRIjRPgfR",
        "replyto": "kIRIjRPgfR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission223/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482254,
        "cdate": 1696707482254,
        "tmdate": 1701465391763,
        "mdate": 1701465391763,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Describes a method to generate instruction-tuning datasets from NLP datasets (e.g. on HuggingFace) in a more automatic fashion than prior work. In particular, the creation of templates for transforming inputs to instruction examples that are normally done by humans (e.g. PromptSource) is done by an LLM. The resulting large dataset (800k examples) is Dynosaur and is produced inexpensively ($12 querying ChatGPT). Base LLMs are fine-tuned on Dinosaur, and performance on Super-NI (ROUGE-L, automatic) and human evaluation are measured and outperforms competitors such as Alpaca. \n\nThe paper provides a cheap, and effective tool in the instruction-tuning dataset generation tool-box."
            }
        },
        "id": "GEdCMrFcXw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kFQrpCFanH",
        "replyto": "kFQrpCFanH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4681/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595841,
        "cdate": 1696707595841,
        "tmdate": 1701465538397,
        "mdate": 1701465538397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents an event essentiality task with respect to a goal. It introduces a dataset and benchmarks the performance of various existing models on this task. \n\nThe paper is has many positives. First, detecting essential events is well-motivated. Not all events in steps that lead to a goal are essential. The task stands on its own for event and goal related reasoning but can be useful for downstream applications as well. The dataset is a useful addition to the broad collection of event related datasets. The paper is written clearly (given the page limits of a short paper). The primary significance of this work is the potential for improving and evaluating event essentiality reasoning in language models using this resource.\n\nThe main weaknesses are that the downstream uses of the dataset or the task is unclear. This is indeed a challenge for many event related resources and not just this specific resource. In addition to the presentation and clarity questions raised by the reviewers, I have one additional concern. The definition of what makes something essential, while it sounds reasonable, can be hard to pin down in specific cases. There is not much discussion in the paper about this aspect."
            }
        },
        "id": "wQoRj1n6Xa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kEzI6OYXV4",
        "replyto": "kEzI6OYXV4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4462/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589980,
        "cdate": 1696707589980,
        "tmdate": 1701465532202,
        "mdate": 1701465532202,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Problem: This paper addresses the problem of complex question answering.\n\nSolution: It applies an intermediate step that decomposes the complex question into simple logical expressions. Their approach consists of an H-parser which: \n 1) generates an H-expression, hierarchically combining simple questions using symbolic operations like JOIN, UNION, etc.  \n 2) Afterward, they utilize a reader network to extract the answer for each subquestion.\n\n\nHere are the notable points of concern: \n\n- More extensive analysis of the factors contributing to the model's success. The authors elaborate on this, and I suggest they incorporate all these details in their revised paper. \n- How does the approach deal with errors from the parser? While the authors provide some statistics, the answer to this question is unclear. \n- Given the model's reliance on supervised data, expanding the model to tackle more diverse questions would require retraining the parser.\n- My addition to this list: The paper has no 2023 citations, which is odd. Please revise their draft with all the relevant work published this year. \n\n\nAll reviewers have assigned high soundness scores and overall buy the outcome. However, given the limited excitement about this work, I recommend the \"findings\" track."
            }
        },
        "id": "AOFzR4f0cf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kEhBOEsXXx",
        "replyto": "kEhBOEsXXx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2659/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546936,
        "cdate": 1696707546936,
        "tmdate": 1701465472524,
        "mdate": 1701465472524,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All four reviewers felt generally positively about the paper, citing the usefulness of the task, the interestingness of the idea, promising results, and wide comparison with other methods., and potential application to other tasks. One reviewer, jmUa, had a concern about presentation -- that the paper was only clear after reading it in entirely -- but other reviewers felt the paper was well-written. Reviewer W2zF had a concern about relevance of the task (parsing an image into latex code) to the conference, but seemed convinced by the author response and still felt positively about the paper. Overall this paper looks to make an interesting contribution on a well-motivated task, with thorough experiments and strong results."
            }
        },
        "id": "FW46Dp6w3I",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kEflZNzau4",
        "replyto": "kEflZNzau4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2693/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547709,
        "cdate": 1696707547709,
        "tmdate": 1701465473690,
        "mdate": 1701465473690,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper focuses on cross-lingual few-shot abusive language detection and addresses the scarcity of target language data by data augmentation and further pre-training for domain adaptation. Based on the existing vicinal risk minimization methods (SSMBA, MIXUP), the authors propose a new data augmentation method, MIXAG, that uses the angle between text representations to interpolate them. Furthermore, they test continue pretraining on a few unlabelled examples and compare it with the zero-shot baseline. \n\nAs mentioned by the reviewers, the authors introduce a novel data augmentation method MIXAG, and show its effectiveness with decent performance improvements in few-shot cross-lingual transfer especially in the multilingual scenario (Multilingual MIXUP). Although results from the multidomain setting (Figure 2) do not show superior performance, the authors contextualized these results in their rebuttal and pointed out the impact of their approach in multidomain and multilingual environments. \n\nI believe the additional content that the authors mentioned during the discussion period, such as XLM-R results and analysis on the representation of original and synthetic instances, will further improve the paper."
            }
        },
        "id": "ECS3cDC7pz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kEcDQzX3cI",
        "replyto": "kEcDQzX3cI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1039/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503577,
        "cdate": 1696707503577,
        "tmdate": 1701465418722,
        "mdate": 1701465418722,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a novel framework for distilling pre-trained language models, introducing a simultaneous approach to model compression and token pruning. This framework efficiently combines model compression with dynamic token-pruning, employing a reconstruction loss and alignment loss during knowledge distillation from a pre-trained teacher model. Through its experimentation on the GLUE benchmark, the work exhibits superior performance over contemporary methods like MiniLMv2 and TinyBERT. Notwithstanding its achievements, reviewers have raised concerns regarding the comprehensiveness of its experiments, reproducibility due to implementation ambiguity, and the increased training costs relative to the marginal improvements in inference speed.\n\nPros:\n\nIntroduction of a method combining model compression and token pruning, demonstrating enhanced performance on the GLUE benchmark.\n\nComprehensive ablation studies providing a deeper understanding of the proposed method's components.\n\nWell-structured paper that offers clarity and ease of reading.\n\n\nCons:\n\nInsufficient experimentation, especially concerning the compression of larger models which are becoming more prevalent.\n\nAbsence of clarity in implementation details and potential reproducibility issues, with concerns about non-deterministic inference processes.\n\nWhile the proposed method offers improvements, it comes at the cost of roughly doubling the training time with only minimal gains in inference speed."
            }
        },
        "id": "oV0GYZBk1e",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "kCzhhVMo4r",
        "replyto": "kCzhhVMo4r",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4937/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601924,
        "cdate": 1696707601924,
        "tmdate": 1701465545217,
        "mdate": 1701465545217,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new approach to social media stance detection based on speech act, toxicity, and moral features of tweets. In extensive experiments, they demonstrate that multi-task learning improves stance detection. \n\nThe following concerns have been raised by reviewers:\n* It does not seem that the proposed methods are truly novel. This is because lexical and neural features and MTL are quite common in NLP.\n * Lack of qualitative analysis and small-scale experiment\n * References and comparisons with prior works are missing. \n    - Empirical Analysis of Multi-Task Learning for Reducing Identity Bias in Toxic Comment Detection\n    - A multi-task model for emotion and offensive aided stance detection of climate change tweets.\n    - Exploring Multi-Task Multi-Lingual Learning of Transformer Models for Hate Speech and Offensive Speech Identification in Social Media\n    - Task Stance Detection with Sentiment and Stance Lexicons"
            }
        },
        "id": "Gem9aWKLVP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "k95cAni5Hk",
        "replyto": "k95cAni5Hk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3383/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561567,
        "cdate": 1696707561567,
        "tmdate": 1701465495639,
        "mdate": 1701465495639,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper provides a new (supervised) dataset for dialogue segmentation.\n\n**Pros**: All reviewers agree the dataset will be a useful resource to the community. Validation of the dataset is considered to be appropriate, using some human-verification used to verify the \"simple but effective\" annotation heuristic. Further, the motivations for the dataset are also well validated (the proposed supervisory signals add benefit). Indeed, reviewers note the benchmark experiments as being substantial and relevant to the community (3 benchmark datasets, 18 methods).\n\n**Cons**: One reviewer requests missing details, while another points out that deriving supervisory signals from an existing datasets annotations (i.e., proxies) can incur noise, which will be compounded with the noise from the annotation heuristic. Authors appear to address these concerns during the rebuttal period."
            }
        },
        "id": "o0j0uOkfcK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "k8rxolXsPE",
        "replyto": "k8rxolXsPE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5367/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611662,
        "cdate": 1696707611662,
        "tmdate": 1701465557544,
        "mdate": 1701465557544,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces ATFormer, a cost modeling and tensor program optimization framework for enhancing deep neural network efficiency. By framing the tuning problem as an optimization task in a hierarchical search space, ATFormer employs attention mechanisms to capture global and long-range dependencies in features. This model is assessed both traditionally and through transfer learning. \n\nMost reviewers asked for additional explanation/results related to code generation of the approach. Initially, reviewers interpreted the work to optimize only innermost loops, limiting its applicability. However, the authors provided extensive feedback and better explanation for the method (and example output) in the rebuttal and convinced the reviewers that the approach is a more general framework. I suggest that the authors try to refactor the paper to make this more clear and provide some more interesting examples upfront (taken from the rebuttal discussion) to highlight to what degree this work can perform optimization for tensor programs."
            }
        },
        "id": "p6O4dyeIET",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "k4QqDDoRyI",
        "replyto": "k4QqDDoRyI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission621/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492288,
        "cdate": 1696707492288,
        "tmdate": 1701465405617,
        "mdate": 1701465405617,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents mRedditSum, a new multimodal abstractive summarization dataset, and a novel cluster-based multi-stage summarization method. The dataset consists of 3,033 Reddit threads where a post solicits advice regarding an issue described with an image and text, and respective comments express diverse opinions. These threads are annotated with a human-written summary that captures both the text's essential information and the image's details. The paper also shows that the performance of summarization models, specifically the proposed CMS model, improves when visual information is incorporated.\n\nReviewers acknowledge the novelty and potential contribution of the mRedditSum dataset and the proposed cluster-based multi-stage summarization method to the field of multimodal abstractive summarization. The paper is well-written, with clear details about the proposed dataset and method. However, there are also concerns that may merit attention. These include a lack of detail on the total number of annotators and whether the same annotator generated the comment and full summary for the study. It is also pointed out the absence of an inter-annotator agreement for a few samples and a comparison with a text-only baseline model that supports longer sequence lengths.\n\nOverall, this paper proposes a potentially useful dataset for the research community, as well as a summarization model that surpasses SOTA. Authors are encouraged to address the reviewers' comments, in particular the concerns on the human annotation process, and their rebuttals into the final version if accepted."
            }
        },
        "id": "u38dVeIYHN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "k3i6PKlKY8",
        "replyto": "k3i6PKlKY8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2931/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552414,
        "cdate": 1696707552414,
        "tmdate": 1701465481563,
        "mdate": 1701465481563,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a dataset which requires models to resolve ambiguous definite descriptions (phrases that denote entities by describing the properties or roles that are unique to them within the relevant context, e.g. \"the pope\", \"john’s mother\", \"our king\"). The dataset has been constructed semi-automatically using 5 property-value pairs from from Wikidata. Their experiments indicate that LLMs still struggle to resolve this kind of ambiguity. \n\nThe paper tackles an interesting problem and makes a valuable contribution in terms of the dataset. It does have drawbacks -- there's no human baseline, the dataset has been constructed from only 5 property value pairs, and could have used stronger baselines (even if as upper bounds). However, I think the paper meets the criteria for a short paper, i.e., a short focused contribution which the community can build upon."
            }
        },
        "id": "IiXSazg9QO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "k2VHhq2LH9",
        "replyto": "k2VHhq2LH9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4256/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586222,
        "cdate": 1696707586222,
        "tmdate": 1701465525689,
        "mdate": 1701465525689,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "There is strong consensus that this work is both very exciting and very sound. The reviewers agree that the idea and its intuition is important and insightful and that the experimental results are excellent. As such, I recommend acceptance to the main conference."
            }
        },
        "id": "TJFNdqBwAP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jxgz7FEqWq",
        "replyto": "jxgz7FEqWq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5401/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612355,
        "cdate": 1696707612355,
        "tmdate": 1701465558497,
        "mdate": 1701465558497,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a novel framework for decoding in the morphological inflection task. Instead of standard left-to-right decoding, the model chooses at each step whether to generate a token on the left, on the right, or merge the left and right sequences. The approach leads to improvements over baselines and SoTA models in the 2022 and 2023 shared tasks on morphological inflection. The reviewers agree that the model is novel, reasonable, scalable to other languages, and yields better results than (more traditional) left-to-right decoders. Some reviewers note that the paper lacks ablation studies, comparison to other bidirectional models, and the model complexity is higher, but the authors carefully, neatly, and in a very detailed way addressed all of them.\nOverall, the paper provides a strong model for morphological inflection that might become a new SoTA. The paper should gain sufficient interest in the community of NLP/computational morphology.\nAlso, it's especially great to see that the paper authors care about time complexity and use dynamic programming!"
            }
        },
        "id": "X2oRRvSbe6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jw1iZfW5zN",
        "replyto": "jw1iZfW5zN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1166/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506407,
        "cdate": 1696707506407,
        "tmdate": 1701465422445,
        "mdate": 1701465422445,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Three reviewers provided feedback for this paper. They found the work interesting, the model capable and they appreciated the faster performance by the proposed approach. There was one glaring complaint from a reviewer, that had to do with fit to EMNLP. First, the reviewer felt that text conditioning the 3D generation was not sufficient to be accepted at a top language conference. Second, the reviewer felt that image conditioning was more relevant than text conditioning for this task. And third, they questioned the novelty. The authors provided a detailed rebuttal to these points and I agree with the authors views here. I find this work to be of interest to the EMNLP audience, particularly as multimodal AI is becoming increasingly popular at EMNLP. I think that text conditioned avatar generation / 3D generation is going to be very popular as a research topic going forward, and so I think this paper is very timely. I also disagree that the paper does not deliver on novelty of interest to the reader. Given the reviews, rebuttal and the above details, I recommend acceptance."
            }
        },
        "id": "qOvakdFXWp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jvTV8vSa3X",
        "replyto": "jvTV8vSa3X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1933/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530309,
        "cdate": 1696707530309,
        "tmdate": 1701465447608,
        "mdate": 1701465447608,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers agree on the 4:4 scores for soundness and excitements and I can only agree with those reviews. The paper provides a novel and original contribution that the field can benefit from. \n\nPros:\n- overall, the paper is clearly written, well structured and comprehensive\n- the contribution of the dataset is valuable to the community\n- idea and methodology are novel and very timely\n- the limitations and critical assessment is appreciated\n\nCons:\n- most of the criticism is raised with respect to inaccuracies within the paper which have been clarified in the rebuttal\n- without the data/code available during review, the reproducibility is limited\n\nOverall, I agree with the reviewers and agree that the paper is moderately sound and exciting. What is missing for 5:5 made apparent by the many (minor) ambiguities that raise the questions of all reviews."
            }
        },
        "id": "tlJCmRTq65",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jvNVmkGxiU",
        "replyto": "jvNVmkGxiU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2110/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534720,
        "cdate": 1696707534720,
        "tmdate": 1701465454319,
        "mdate": 1701465454319,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces an approach for denoising weakly supervised data, particularly through the introduction of a weighting mechanism for labeling functions and the Unsupervised Labeling Function (ULF) correction. Based on the discussions during the rebuttal phase, below is the summary of the main aspects of the work. \n\nReasons to Accept:\n\nClarity and Presentation: The paper is well-described and well-written. The clear and organized presentation enhances the overall understanding of the research, making it accessible to a broad audience.\n\nCompetitive Performance: The paper presents compelling results, demonstrating that, with the exception of one dataset, the proposed approach consistently outperforms other existing methods. This highlights the effectiveness and competitiveness of the proposed method.\n\nMotivated Research: The work is well-motivated and provides a clear rationale for the need to re-estimate the joint distribution between labeling functions and class labels. The experiments conducted effectively evaluate this motivation, strengthening the paper's credibility.\n\nExtensive Experimentation: The authors have conducted a comprehensive set of experiments, including feature-based and pre-trained models, to demonstrate the effectiveness of their method. This thorough experimentation adds significant value to the research. The appendices provide extra analyses and results.\n\nComparative Performance: The results consistently indicate that the proposed solution either performs comparably or surpasses current solutions and baselines. This underscores the competitiveness and utility of the proposed approach.\n\nReasons to Reject:\n\nDeeper Discussion Section Needed: While the paper excels in its description and presentation, it would significantly benefit from a more extensive discussion section. A deeper analysis of the results, including their implications and nuances, is essential to provide readers with a more comprehensive understanding of the research.\n\nFuture Directions and Next Steps: As a short paper, considering the inclusion of a section that outlines potential future research directions and next steps would be valuable. Identifying areas where the proposed approach could be extended, improved, or applied would enhance the paper's significance.\n\nIn conclusion, there is convergence on the reviews considering that the paper exhibits substantial promise with its clarity, strong presentation, innovative approach, motivated research, extensive experimentation, and competitive performance. To further elevate its contribution and overall quality, it is recommended to expand the discussion section and consider adding a section on future research directions."
            }
        },
        "id": "ZlBhDKroBC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "juUEOaH7bK",
        "replyto": "juUEOaH7bK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5184/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608738,
        "cdate": 1696707608738,
        "tmdate": 1701465552302,
        "mdate": 1701465552302,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The focus of this paper is on the issue of spurious correlation in NLU datasets. The argument presented is that biases or cues in NLU datasets can be exploited by NLP models, resulting in an overestimation of their capabilities. The paper is well-crafted and easily understood, with only minor notation revisions required. The proposed method is straightforward and straightforward to implement, and the experimental setup is relatively thorough. However, a few reviewers have expressed concerns about the validity of some claims based on the results. They claim that the results lack statistical significance tests, which undermines the credibility of the proposed method's impact. Also, some reviewers have observed that some of the accuracy scores might not be significantly difference from zero and that some conclusions drawn are based on a limited number of samples."
            }
        },
        "id": "7ymr1bcXdM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jsmV1WxXyb",
        "replyto": "jsmV1WxXyb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2934/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552458,
        "cdate": 1696707552458,
        "tmdate": 1701465481618,
        "mdate": 1701465481618,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process to solve complex reasoning tasks. The algorithm consists of an exploration process that decomposes the question into simpler sub-questions until the sub-questions are answered confidently. All the modules are implemented by prompting large language models. The proposed algorithm is evaluated on both language-only and multimodal tasks. \n\n\nBased on my reading of the reviews, the reviewers like the idea, the findings, and the writing of the work. \n\n\nHere are the important points raised in the reviews: \n(1) computational complexity of the proposed approach, which is addressed. \n(2) not evaluated on a diverse range of tasks (e.g., the game of 24 and StrategyQA): the author has agreed but has promised to address \n(3) Ablation on QA2H module: promised to include, though little shown in the rebuttal. \n\n\nThe ratings are high for this, and I support its acceptance. \nI worry that the authors have promised many changes to be incorporated in their revision (experiment on complexity, ablation  QA2H+hints, additional datasets, etc.), and I hope they keep their promise."
            }
        },
        "id": "Ecvdw05gWz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jqbhtSDPz7",
        "replyto": "jqbhtSDPz7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4458/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707590027,
        "cdate": 1696707590027,
        "tmdate": 1701465532181,
        "mdate": 1701465532181,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This study introduces a multifaceted ideology detection dataset comprising 12K English tweets. In addition to the dataset, the research presents benchmark results using various pre-trained language models.\n\nAll reviewers acknowledged the value of the dataset, though they had a few minor concerns. The authors addressed these concerns in their rebuttal. It would be beneficial to incorporate these responses into the paper to provide greater clarity for readers."
            }
        },
        "id": "sK95hutMJW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jqOymNqzuB",
        "replyto": "jqOymNqzuB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2436/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542150,
        "cdate": 1696707542150,
        "tmdate": 1701465464994,
        "mdate": 1701465464994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper argues that there is a subpace in the vector space of BERT token representations that encodes grammatical number, i.e. the information necessary for correctly predicting the form of the verb as regards the correct number agreement with the subject. The authors show that it is possible to find an orthonormal lower-dimensional basis for this subspace and then use the elements of this basis to erase or invert the information about grammatical number contained in token representations. The authors use iterative nullspace projection by Ravfogel et al. (2020) to compute the basis and measure the effect of their interventions using the dataset on number agreement compiled by Goldberg (2019). They show that their method is largely robust to the choice of hyperparameters (dimensionality of the subspace and the \"intensity of intervention\", essentially a general weight for the sum of weighted basis vectors), but that it is important to carefully choose the target layer and token depending on the structure of the sentence. The authors repeat their experiments on the MultiBERTs showing that their results are not limited to the uncased version of BERT base.\n\nWhile the paper is interesting and well written it seems that the novelty and experimental setup are somewhat limited (i.e. the excitement is somewhat limited)."
            }
        },
        "id": "dArxMwkq1T",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jqOIacThP3",
        "replyto": "jqOIacThP3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1084/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504630,
        "cdate": 1696707504630,
        "tmdate": 1701465419965,
        "mdate": 1701465419965,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces MUX-PLMs, a pre-trained language model designed to enhance throughput in multi-input multi-output scenarios. The approach uses data multiplexing to process multiple inputs simultaneously, promising improved inference speeds. Experiments focus primarily on the GLUE benchmark, with the paper arguing for both the novelty and effectiveness of MUX-PLMs. Three reviewers have provided their assessments.\n\nStrengths:\n\n1. Novel Approach: The authors' introduction of a MUX and DeMUX architecture in the realm of PLMs is noteworthy. This innovation aims to address the growing demand and computational cost challenges associated with large language models.\n2. Positive Results: Experiments indicate that MUX-PLMs, particularly under BERT and ELECTRA architectures, can maintain performance levels comparable to traditional PLMs while offering significant reductions in computational overhead.\n3. Clarity: The paper is well-structured, making the content easily accessible to readers. Several reviewers highlighted its readability.\n4. Broad Applicability: The proposed method is shown to work across different architectures like BERT and ELECTRA and can also be combined with other compression methods.\n\nWeaknesses:\n\n1. Insufficient Validation: The model's performance validation seems restricted, with emphasis on GLUE, NER, and POS tagging tasks. An exploration into more diverse and challenging tasks would have strengthened the study's credibility.\n2. Performance of Contextual MUX: The paper introduces the Contextual MUX module, but it reportedly does not outperform the original MUX module. Its absence in primary experiments and lackluster results question its utility.\n3. Comparison with SOTA: The method's performance, when juxtaposed with state-of-the-art model compression techniques (like MobileBERT and TinyBERT), is reportedly not competitive. Reviewers questioned the practical adoption of MUX-PLMs given these findings.\n4. Ambiguities & Lack of Depth: Several concerns were raised regarding ambiguities in the paper and a perceived lack of depth in specific sections. For instance, the method's uniqueness compared to the earlier TMUX version was questioned. A more profound exploration of the impact of different data within a batch in a multi-input setting would also have been beneficial.\n5. Training Cost & Flexibility: The potential increased training costs associated with MUX-PLMs and the need to pre-train different MUX-PLMs for different acceleration ratios could be a limitation compared to existing methods that fine-tune a single PLM.\n\nThe paper, with its focus on MUX-PLMs, offers a unique lens into improving language model throughput via data multiplexing. While the premise is exciting and there are promising results, there are notable gaps in validation, comparative analysis, and practical implications that should be addressed. Future revisions should consider expanding the experimental range, addressing ambiguities, and elucidating the tangible benefits of adopting MUX-PLMs over existing methods.|This paper presents MUX-PLMs, a pre-trained language model strategically crafted to optimize throughput in multi-input multi-output scenarios. By harnessing data multiplexing, this approach concurrently handles multiple inputs, offering the prospect of significantly enhanced inference speeds. The experimental evaluation primarily centers around the GLUE benchmark, demonstrating the novelty and efficacy of MUX-PLMs. The insights of three reviewers further enrich the paper's assessment."
            }
        },
        "id": "PGiSVzJ43A",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jph8GlHueb",
        "replyto": "jph8GlHueb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4398/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588968,
        "cdate": 1696707588968,
        "tmdate": 1701465530599,
        "mdate": 1701465530599,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper discusses the problem of few-shot classification for NLP by using black-box APIs. The setting is very relevant to the current state, where many foundational models are served by companies only through APIs without sharing their internals representations or model architecture details. The authors proposes to use a Transductive Learning approach through a Fisher-Rao regularizer.\nThe reviewers agree that the paper is timely with respect to the current state of the art. The reviewers agree that the proposed approach is interesting, the experiments are showing the efficacy of the appraoch and that in general  the paper can provide a positive impact to the NLP community."
            }
        },
        "id": "u9Ns9H9wZh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jp80nsryCF",
        "replyto": "jp80nsryCF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission903/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498867,
        "cdate": 1696707498867,
        "tmdate": 1701465414333,
        "mdate": 1701465414333,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents the first large scale evaluation framework for multilingual NLP tasks. Reviewers see merit in the proposal and mostly raise minor issues. Multilingual evaluation is a highly important topic in the field and the paper presents a sound account of the issue."
            }
        },
        "id": "bu0MqHBg6d",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jmopGajkFY",
        "replyto": "jmopGajkFY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5201/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608941,
        "cdate": 1696707608941,
        "tmdate": 1701465552813,
        "mdate": 1701465552813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This research focuses on out-of-context (OOC) mis-/disinformation on the Web and proposes a method to predict it using evidence and stances consisting of text and images. \n\nPros:\nMis-/disinformation identification is an urgent issue, and the effort is commendable. \nThe approach is based on a good understanding of the nature of the problem. \n\nCons:\nKey questions from reviewers have been addressed in the rebuttal, but these need to be properly reflected in the camera-ready version."
            }
        },
        "id": "wcmxUpMIGy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jkI9KGEFQz",
        "replyto": "jkI9KGEFQz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4557/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592917,
        "cdate": 1696707592917,
        "tmdate": 1701465534655,
        "mdate": 1701465534655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new dataset for training and evaluation of multimodal MT. The paper focuses on cases where the visual context can aid for a better translation, eg ambiguous cases or the cases where a word is dropped. The authors have addressed almost all critical points raised by the reviewers, although a better characterization of the cases where the visual modality provides help to improve the translation would be very helpful for the paper (beyond ambiguity). Furthermore, the paper proposes a multimodal MT system to leverage the visual modality and shows that it outperforms competitive baselines. Therefore, this paper seems a a good contribution to the conference and community."
            }
        },
        "id": "r5CaC94Kp8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jjSOGqLT2X",
        "replyto": "jjSOGqLT2X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission849/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497624,
        "cdate": 1696707497624,
        "tmdate": 1701465412730,
        "mdate": 1701465412730,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are unanimous that paper is sound and exciting. There is consensus on the quality of the evaluation and the proposed Flipped-VQA method as a sample effient means to train model and help understand their causal and temporal reasoning."
            }
        },
        "id": "2vtJAWZFem",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jhdVt7rC8k",
        "replyto": "jhdVt7rC8k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission376/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486017,
        "cdate": 1696707486017,
        "tmdate": 1701465396619,
        "mdate": 1701465396619,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This is an interesting short paper, proposing a memory mechanism to enhance the multihop question answering (MHQA) capabilities of transformer models.\n\nWhile it seems like reviewers agree the idea is interesting in general, review scores range from 2 to 4. I have read the reviews and rebuttals carefully, and have given the paper an in-depth read to try and give it a fair assessment.\n\n## Summary\nThis work introduces GEMFormer (Global Explicit Memory Transformer) to address the task of MHQA. One significant challenge of MHQA is the fact that information relevant for answering the question is scattered across long, disparate documents, which might not fit into the underlying transformer's context window; even when using models with large window sizes, the attention mechanism might lead to interference of local and global information in the token representations of relevant pieces of information, potentially harming model performance.\n\nThe proposed MHQA mechanism is very simple:\n- memory construction:\n  - Long background documents are split into segments fitting into the transformer's context window.\n  - the question is combined with each segment, and put through the transformer and LM head in a forward pass.\n  - segment tokens with either highest (top-k) or lowest (below a threshold) LM entropy are selected and copied into the global memory.\n- MHQA training\n  - each segment is combined with the question and global memory, and the MHQA task is trained in standard fashion.\n\nThe authors find that of the two proposed entropy-based memory selection methods, selecting low-entropy tokens works best, with the threshold tuned as a hyperparameter on the MHQA dataset.\n\nThe method is trained and tested on 3 MHQA datasets, HotpotQA, 2Wiki, and MuSiQue, with a RoBERTa-base model, and is able to outperform the memory-free baseline on each. While the main body does not contain other -- more relevant -- baselines, these are evaluated against in the appendix in order to make room for some ablation studies that provide some interesting insights into GEMFormer behaviour.\n\nThe models chosen for comparison in the appendix seem relevant and sensible, using transformer networks of roughly comparable sizes to GEMFormer. GEMFormer compares favourably overall, even though it is not able to outperform some of the more involved baselines.\n\n## Strengths:\n- the presented memory construction method is novel, simple, yet seems effective\n- the methodology is clearly presented and easy to follow\n- GEMFormer is evaluated on a variety of different datasets.\n- the conducted experiments are extensive, and seem to support the author's claims that (i) memory can help MHQA performance; and (ii) selecting low-entropy tokens is required to fill the memory with relevant information.\n\n## Weaknesses:\n- the entropy threshold for the (better performaing) low-entropy memory selection is a hyperparameter requiring per-dataset tuning.\n- results are consistently higher than the baseline, but not to a high degree\n- some parts, especially the discussion, can be hard to follow, as new concepts and ablations are introduced unexpectedly.\n\n## Conclusion\nIn my opinion, the authors' rebuttals to reviews were satisfactory, and cleared up relevant questions with convincing answers.\\\nI find myself agreeing with the authors that a per-dataset hyperparameter such as the proposed entropy threshold can be regarded as part of dataset-specific finetuning, which is very much standard -- I am not aware of an MHQA method that is able to be trained on one, and then performs its best on all other MHQA datasets.\\\nEven though no new state of the art is established (SOTA is not an acceptance criterion in itself), I believe the proposed method shows enough novelty with its simple yet effective approach to memory construction and utilisation for the MHQA task.\\\nOverall, the work seems sound and merits acceptance; excitement is dampened somewhat by the relatively small improvements achieved, but this is a subjective measure separate from soundness.\n\nWhile the writing could be slightly improved, such as better splitting of the discussion and ablation parts, as well as clarifying why low entropy works (low entropy = high certainty of tokens, conditioned on the question; changing \"least uncertain\" to \"most certain\" to avoid a potentially confusing double negative), this should be no problem for a camera-ready version, and the paper is otherwise very easy to follow."
            }
        },
        "id": "KGvM0GWVdo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jg2WCVrjhS",
        "replyto": "jg2WCVrjhS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3899/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578773,
        "cdate": 1696707578773,
        "tmdate": 1701465513387,
        "mdate": 1701465513387,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a chain of thought prompting-based approach for question generation over KBs in low-resource settings. The reviewers all believe the task is important, and it has been under-explored with LLMs in prior work. On the negative side, the improvements of the proposed method over competitors (e.g., Auto-CoT) are fairly small, which does call into question the method's generalizability. I would suggest that the authors consider adding a more thorough and well-documented human evaluation to the next version of their paper, as that would help determine its effectiveness more than simple (and flawed) string overlap-based methods. The authors were forthcoming with new results and details during the discussion period, which is much appreciated by both the reviewers and this AC. Overall, there are flaws with this paper, but I believe it will  still make a valuable contribution to EMNLP (either main conference or findings)."
            }
        },
        "id": "vgHwMSskA6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jfaJdk29k4",
        "replyto": "jfaJdk29k4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission920/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499361,
        "cdate": 1696707499361,
        "tmdate": 1701465414916,
        "mdate": 1701465414916,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work presents a datsaet and evaluations for the ability of neural MT systems and language models to translate idiomatic or non-idiomatic text which is ambiguous alone but disambiguated with context. The reviewers had somewhat ambivalent but overall positive evaluations of the work’s soundness and interest to the field; moderate concerns were raised both with the distinctness of the dataset from prior work and the validation of the LLM-assisted generations that comprise the dataset.\n\nThe work merited discussion among the AC and reviewers, and it was agreed that the initial concerns of the reviewers regarding scope were not at issue, and that there remain some concerns as to related work and dataset composition, as stated above. Overall, however, I’m happy to see the community leveraging strong generative models to help ask interesting questions about model behavior. However, concerns as to whether a dataset co-generated by GPT-4 will lead to, e.g., ChatGPT performing better than had the data come from an unrelated distribution are serious and under-explored; see, e.g., the AlpacaEval results that show that strong generative models like GPT-4 and Claude2 prefer their own family of models at a greater rate (https://tatsu-lab.github.io/alpaca_eval/). I encourage the authors to engage with this."
            }
        },
        "id": "ueJBsarV5Y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jcx5YIN3Sd",
        "replyto": "jcx5YIN3Sd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5276/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610221,
        "cdate": 1696707610221,
        "tmdate": 1701465554873,
        "mdate": 1701465554873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a challenging dataset to assess LMs' abilities in Theory of Mind (ToM). The reviewers find the paper well-written and clear, and they consider the approach novel, sound, and interesting. The authors have successfully addressed the concerns raised by the reviewers. I recommend accepting the paper."
            }
        },
        "id": "QaMvp5qnj0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jcqBLHFcYA",
        "replyto": "jcqBLHFcYA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission739/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494831,
        "cdate": 1696707494831,
        "tmdate": 1701465408906,
        "mdate": 1701465408906,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper delves into the realm of learning logic rules on knowledge graphs for reasoning and introduces the LATENTLOGIC model, designed to mine logic rules through controllable generation within the latent space.\n\nThe overall consensus among most PC members is that this paper demonstrates a good soundness but elicits mixed feelings in terms of excitement. Several noteworthy weaknesses have been highlighted by the reviewers, including the high complexity of the proposed model, the absence of the latest baselines for comparison, insufficient details concerning the implementation, and potential limitations associated with the choice of datasets.\n\nConsidering the good soundness and the identified concerns, this paper has the potential to be accepted as a findings paper. However, the authors are encouraged to diligently address all the concerns outlined in the reviews during their new revision, in order to enhance the overall quality and impact of this paper."
            }
        },
        "id": "5hJp9Jni9V",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jbicunmyXh",
        "replyto": "jbicunmyXh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission316/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484576,
        "cdate": 1696707484576,
        "tmdate": 1701465394615,
        "mdate": 1701465394615,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper is well-written, addresses an important and previously underexplored issue, presents a simple and effective method, conducts comprehensive experiments, and offers valuable contributions to the NLP community."
            }
        },
        "id": "LqKSsQ9MkR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jZXjHnzPyk",
        "replyto": "jZXjHnzPyk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission400/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486547,
        "cdate": 1696707486547,
        "tmdate": 1701465397542,
        "mdate": 1701465397542,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces RobustEmbed, a self-supervised method for learning robust sentence embeddings. The method applies dropout and adversarial perturbations to generate hard positives for contrastive learning. The method optimizes a loss function that encourages similarity between the original, dropout, and perturbed embeddings, while diverging the dropout and perturbed embeddings. The method shows improvement over baselines on various tasks as well, such as semantic textual similarity, transfer learning, and text classification. The paper is well written with a defined structure and provides high resilience against various adversarial attacks being comparable to other embeddings in representation and classification tasks at the same time."
            }
        },
        "id": "DC6j4stCRf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jWqkEB3wJP",
        "replyto": "jWqkEB3wJP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4792/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598607,
        "cdate": 1696707598607,
        "tmdate": 1701465542046,
        "mdate": 1701465542046,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Three reviewers evaluated the paper’s soundness with 4 (strong) and one with 3 (good) – overall, indicating that the paper on the most part provides sufficient support for all of its claims/arguments.  All four reviewers, however, were ambivalent about the paper’s excitement (3) – noting some ambiguity in the novelty of the contribution and potential limitations. Reviewer 9L3i  raised concerns about the limitation of not considering the diachronic aspects and more comprehensive model comparisons. Reviewer 2Gwe requested the authors to clarify the importance of their partisanship measure. 2Gwe also noted, the paper would be strengthened by providing key insights into linguistic features that indicate partisanship in the justice languages. Reviewer NGhD also notes that this is a topic that has been heavily studied and argues that the paper would benefit from highlighting new contributions. Relatedly, Reviewer HTaT notes that existing work has studied linguistic differences among (groups of) people with different political ideologies. An additional analysis examining the partisan differences in the language of justices would address such reviewers’ concerns. While the authors addressed some limitations with relevant citations and provided clarification to the reviewers’ questions, the paper will still benefit from additional analyses suggested by 2Gwe and 9L3i."
            }
        },
        "id": "dfWBBiVjSs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jWL2GhQw5D",
        "replyto": "jWL2GhQw5D",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1189/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506987,
        "cdate": 1696707506987,
        "tmdate": 1701465423202,
        "mdate": 1701465423202,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper focuses on using LLMs for automatic evaluation of attribution. Specifically, it makes three contributions: (a) a new NLI-inspired categorization of attribution types, (b) new datasets (simulated and real) for evaluating attribution, and (c) models for automatic evaluation of attribution.\n\nThis paper addresses an important problem and provides reasonable yet clearly imperfect solutions. Specifically, reviewers have concerns that their simulated data are biased and that their \"real\" test dataset is too small. NLP has had a fairly established precedent of using simulated data (especially for training) in settings where human data collection is costly. Typically, such data are indeed biased in certain ways, but are quite often still useful at least until a large costly human-curated dataset becomes available. The concern that their real/unsimulated evaluation set is small (~250 examples) is a more serious weakness.  Nonetheless, I think a fair bit of work has gone into the paper, and it will likely generate more followup work. Overall, I'd be inclined to see it accepted. I also hope that the authors will consider expanding the size of the evaluation set to make this paper stronger."
            }
        },
        "id": "gGNrdjzjmV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jVa7tFQw9N",
        "replyto": "jVa7tFQw9N",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1053/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503891,
        "cdate": 1696707503891,
        "tmdate": 1701465418982,
        "mdate": 1701465418982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper focuses on examining highlighting in metaphors, where certain aspects of a target domain are emphasized by the use of the metaphor.  The authors introduce a new NLP task of identifying the highlighted aspect in a metaphorical sentence.  Leveraging a source-target domain framework, the paper explores whether jointly predicting the highlighted aspect and the source domain in a metaphorical sentence improves predictive performance on both tasks.  They find that by performing continual learning, they are able to obtain performance improvements on both source domain and highlight prediction by allowing information to be shared.\n\nOn the whole, the reviewers thought the paper was sound and well-written.  One of the main issues in terms of soundness was brought up by reviewer onsm, who noted that the results lacked statistical significance tests despite the different models having similar accuracies across tasks.  The authors discussed including significance tests if they revise the paper but mentioned that their results may no longer be significant after testing.  This, along with the fact that the authors only compare their approach with a simple single-task baseline (noted by reviewers hYfy and u9Nz), does weaken the conclusions the paper draws on jointly predicting source domain and highlighting.  Another soundness critique by reviewer onsm is that the inter-annotator agreement for the dataset used was not acknowledged.  As stated in the rebuttal, agreement on the dataset is quite low, ranging from 0.42 to 0.65, suggesting there may be issues with the dataset used.  However, given that the dataset used is from Gordon et al. (2015), the interannotator agreement does not appear to be an explicit soundness issue with this paper directly.  The paper, however, could be strengthened by acknowledging issues with the single dataset used in the limitations section.\n\nOn the other hand, while the paper was well-received in terms of soundness, there is some disagreement over how substantial a contribution it made with its multitask experiments.  While reviewer onsm appreciates that the paper runs experiments examining the difference between single and multitask learning for a metaphor interpretation task, reviewers hYfy and u9Nz regard that comparison as a weakness of the paper.  As reviewer u9Nz notes, the multitask learning experiment “mainly follows an ideological approach that has been used for a long time in the NLP community”.  As a result, the methodological approaches and results in the paper are not surprising, despite operating in a new task setup.\n\nOverall, while the paper is well-written and most of the reported technical details in the experiments are sound, there remain questions about how exciting of a contribution the paper makes.  This is especially the case, given that many of the issues in the paper are due to the limited availability of existing high-quality datasets for metaphor highlighting.  Thus, work that can be done to follow up on the observations and analyses in this paper may be limited."
            }
        },
        "id": "E0wTbpzvE8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jUrRIcedTN",
        "replyto": "jUrRIcedTN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2973/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553210,
        "cdate": 1696707553210,
        "tmdate": 1701465482869,
        "mdate": 1701465482869,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In complex test environments where the environment content is distinctive from the example environments, traditional prompting methods suffer from lack of generalization. \nThis paper proposes establishing and maintaining a memory-based trajectory library that records state-action tuples and other information. The main idea is composed of two phrases: memory formation and memory refinement. In the memory formation phase, human trajectories are summarized into state-action tuples by LLMs and clustered with goal labels and observation labels. In the refinement phase, LDM2 explores the tree structure to retrieve the relevant information, and later LDM2 is finetuned by adding high-reward state-action tuples.\n\nPros:\n\nIntuitive and interesting idea of using memory for dynamic prompting to boost decision making ability of LLMs\n \nUnlike fixed or hand-crafted prompts, the proposed dynamic memory offers a natural and self-updating prompting approach, encouraging LLMs to improve their decisions based on environmental feedback.\n\nThe experimental results demonstrate the effectiveness of the proposed method, with a good improvement over baselines \n\nCons:\n\nIn realistic applications of this, the computational cost can be quite high (as pointed out by most of the reviewers)\n\n\nAgain for many realistic applications, getting the expert human behavior trajectories might be quite difficult or  very expensive. Referring to the authors response on this (link), it is important to understand how effective the model would be if only a few example trajectories are provided for a test environment. Some more experimental analysis and ablations are needed to establish this. \n\n\nWhat are the advantages of the proposed method compared to other memory-based frameworks for reinforce language agents, such as Reflexion [1]? Besides, it is necessary to add baselines about other state-of-the-art memory-based frameworks to demonstrate this."
            }
        },
        "id": "R0UDE1LHBi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jUkDEaE0fK",
        "replyto": "jUkDEaE0fK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3207/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558186,
        "cdate": 1696707558186,
        "tmdate": 1701465490342,
        "mdate": 1701465490342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a method for self-training of small language models to improve their rationalization capabilities.  They propose to use NLI to evaluate explanation plausibility. Figure 2 establishes that plausible explanations are correlated with correct predictions; if plausible explanations *cause* correct predictions, then training on them to produce more plausible explanations should led to better self-rationalization models. The paper uses this observation to filter model predictions for use in the self-training stage.  Experiments on FEB with a UnifiedQA model show stronger accuracy across the board with the proposed technique.\n\nReviewers liked the use of off-the-shelf models to improve rationalization. They praised the overall clarity of the paper.\n\nI agree with these strengths and think this is a solid piece of research. However, the differences from STaR don't seem that substantial: the main difference is that ground truth labels are not needed and a different filtering criterion (plausibility) is used. I do believe the authors (in the response to JA6T) that it's hard to properly compare the techniques, so it's more of a conceptual similarity.  However, I would say that this paper should be cited more prominently in the present paper (right now it appears to be \"nocited\" and only appears in the citations).\n\nJA6T also raises a valid point about causation being derived from data that shows correlation. However, the ultimate test of this is in the final end-to-end results, so I don't see this as a critical soundness weakness. It should be clarified in the paper, though."
            }
        },
        "id": "RvWKa1lSz5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jUgBvYwc50",
        "replyto": "jUgBvYwc50",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2423/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541848,
        "cdate": 1696707541848,
        "tmdate": 1701465464603,
        "mdate": 1701465464603,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors present a new benchmark dataset that is built from human-chatbot interactions, which are different from the social media data that many toxicity detectors are trained on. The reviewers agree that this is an important problem in detecting toxic inputs to chatbots. They agree mostly that the dataset will be valuable and includes interesting jailbreaking prompts. They also show that models perform better when trained on their dataset. Although there is interest in the jailbreaking prompts, the reviewers note that they account for less than 4% of the data. They also note a lack of comparison to stronger models. They also note some previous work such as ConvAbuse that should be mentioned in the paper. Many of the smaller concerns have been addressed by the authors in their rebuttal including the analysis of prompts, robustness, and discussion of ConvAbuse or can be addressed in the camera ready, such as the writing improvements."
            }
        },
        "id": "PPwUfLogpk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jTiJPDv82w",
        "replyto": "jTiJPDv82w",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission4980/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707603397,
        "cdate": 1696707603397,
        "tmdate": 1701465546399,
        "mdate": 1701465546399,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Most reviewers agree that the current manuscript proposes a RL-based approach that leverages dissimilarity score between the authentic and generated completions as a reward signal. To enhance the paper attractive, the authors are encouraged to address several points: a) clearly articulate that the primary objective is to reduce memorization during pre-training phase in fine-tuning phase. 2) add more quantitative details in KL-divergence and UnLearning methods in a self-contained manner; 3) incorporate additional unlearning baselines if feasible. Otherwise, justifying why differential privacy methods cannot be applicable to pre-training of LLMs would provide more clarity."
            }
        },
        "id": "7ZYFzQWogC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jSu7hAIZM0",
        "replyto": "jSu7hAIZM0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4789/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598538,
        "cdate": 1696707598538,
        "tmdate": 1701465541806,
        "mdate": 1701465541806,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper’s main novel contribution is a new dataset of structured career records and textual historical records for around 10K career officials from the Ming dynasty. They also provide annotations for “civilian officials with military power.” \n\nOne of the challenges of a paper like this is they introduce a novel dataset, novel task (predicting “civilian officials with military power”), and a new method for the new task and dataset (specifically modeling both the structured and unstructured text using graph neural networks). Several reviewers seemed to struggle to unpack these three contributions by noting things like “the experiments were conducted on only one dataset” (Reviewer ZgKy) and “the proposed method appears to be primarily built upon existing research, and it lacks sufficient innovation” (Reviewer zST9). I think these critiques would have been mitigated if the authors had  framed this paper as primarily being a dataset contribution.\n\nIf the paper is primarily a dataset contribution paper, the dataset seems novel and important given the bias towards English-only datasets (this is in classical Chinese). As reviewer qxZu hinted at in their comment “Is the base language model used to encode historical text actually trained on ancient mandarin?”, this dataset could potentially be an valuable source for training or evaluating of models for classical Chinese. \n\nHowever, I would extend the concern reviewer qxZU had “Why is military power selected as a special attribute for representation learning?” The predictive task seems interesting, although a little esoteric. \n\nOverall, the dataset contribution is sound and exciting, even if the proposed task (predicting “civilian officials with military power”) and the methods approach with graph neural networks are a bit less exciting."
            }
        },
        "id": "tNly0yarmY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jQozdfjJSZ",
        "replyto": "jQozdfjJSZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2452/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542487,
        "cdate": 1696707542487,
        "tmdate": 1701465465549,
        "mdate": 1701465465549,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper identifies the limitation of prior work on generating diversity prosody (pitch and duration) patterns, and presents a plug-and-play prosody diversifying module (PDM) to effectively tackle the issue. All reviewers agree that the authors address an important issue in speech synthesis, present a novel DPP-based solution that is well-designed and thoroughly evaluated when combined with various TTS backbones. \n\nThe authors have presented many additional studies and samples during the rebuttal period, which sufficiently addressed reviewers concern on ablation studies. These results would be valuable to the readers and I suggest authors to incorporate them into the paper."
            }
        },
        "id": "4BKw4QKFoW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jQcShOpcfM",
        "replyto": "jQcShOpcfM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission474/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488637,
        "cdate": 1696707488637,
        "tmdate": 1701465400258,
        "mdate": 1701465400258,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes learning which tokens to upweight in the training of language models, in order to adapt the model to new sources of information. They propose Context-aware Meta-learned Loss Scaling (CaMeLS), which uses an auxiliary language model to learn how to reweight the language modeling loss for each token. \n\nReviewers recognize the novelty of this work, and find the method well motivated. Some raise questions about the efficiency of the approach which the author response addresses. More interestingly, a suggested baseline by a reviewer is implemented by the authors and compared to their own results, which still highlights the effectiveness of CaMeLs. Some proposed baselines by the reviewers are also compared with in the author response. Overall the reviewers seem satisfied with the author response.\n\nWe hope the authors include the new results and suggested changes in the next iteration of the paper."
            }
        },
        "id": "wBu2CpAPyd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jPrl18r4RA",
        "replyto": "jPrl18r4RA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4887/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707600436,
        "cdate": 1696707600436,
        "tmdate": 1701465544007,
        "mdate": 1701465544007,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agreed on the importance of this novel resources, automatically constructed, for the enthymeme detection and reconstruction tasks. However, they also highlighted some drawbacks to be addressed (i.e., simplification of the task, lack of deeper discussion on the selection and ranking of the enthymemes). The reviewers appreciated the author rebuttal where some issues have been clarified."
            }
        },
        "id": "i6SyZyByPN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jMwvnqKTBG",
        "replyto": "jMwvnqKTBG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2792/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549528,
        "cdate": 1696707549528,
        "tmdate": 1701465476635,
        "mdate": 1701465476635,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Based on the reviews and updating scores provided, the paper titled \"Gaussian Diffusion into a Dialogue Generation Framework” primarily showcases integration of Gaussian diffusion and dropout on memory latents within a dialogue generation framework, specifically the CVAE model, to alleviate the impact of isotropic Gaussian priors in VAEs and their resulting posterior collapse. This approach shows promising results on open-domain dialogue generation datasets.\n\n## Pros:\n\n- Proposes a novel approach to handle the posterior collapse problem and the limitation of Gaussian distribution on prior, which could enhance the diversity of generated responses.\n- The proposed model shows potential competitive results against some existing generation methods without relying on large-scale dialog pre-training.\n- There is a clear and strong motivation behind the research, with the results demonstrating effectiveness of the proposed model.\n\n## Cons:\n\n- One of the main criticisms revolves around the clarity and thoroughness of the proposed method's explanation. Detailed descriptions and insights into the co-training process of diffusion latents with CVAE are missing, blurring potential readers' understanding.\n- Integration of the diffusion process with the CVAE training is not clearly explained.\n- Information is lacking on the dynamics governing the diffusion process and the parameter tuning process.\n- There is a lack of comparative analysis on inference speed and parameter size against baselines.\n- Experimental validation of the paper's claims could be more robust, with the inclusing of more recent baseline models for evaluation.\n- Evaluation methods of the system are deemed outdated and insufficient, and fail to show if the model can actually generate fluent, diverse dialogues.\n- As per the third reviewer, more in-depth analysis on the influence of the diffusion process on the latent space is needed.\n- There are detailed concerns about the memory dropout method and questions on its effectiveness in regards to the posterior collapse problem."
            }
        },
        "id": "o6IDyNeQ7j",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jLmSsybvkR",
        "replyto": "jLmSsybvkR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission964/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500353,
        "cdate": 1696707500353,
        "tmdate": 1701465416207,
        "mdate": 1701465416207,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This study investigates the use of multimodality in LLM and how retrieving (inputting) multimodal information contributes to it. \n\nPros:\nIt is a useful paper with a good description of the current situation surrounding multimodal LLM, including background knowledge.\nA comprehensive study is well done. \n\nCons:\nAs reviewer 4Ruw points out, the scope of multimodality covered by this paper needs clarification."
            }
        },
        "id": "DrkerPobhl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jLEnVo0RW3",
        "replyto": "jLEnVo0RW3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2507/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543567,
        "cdate": 1696707543567,
        "tmdate": 1701465467163,
        "mdate": 1701465467163,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a method for detoxifying language models by identifying and reversing toxification trajectories in the latent vector space. The main premise is to leverage toxic samples in a generative model to determine the toxic direction. By moving in the opposite direction, the model's output can be detoxified. This method's efficacy is gauged against other detoxification methods through both automatic and human evaluation. For most reviewers, the paper presents an innovative approach to language detoxification with a well-defined mathematical foundation. It introduces a potentially groundbreaking method, However, there are concerns about the clarity of its objectives, the evaluation process, computational costs, and the limitation to GPT-2. These factors should be addressed for a more comprehensive contribution. A reviewer expressed appreciation for the effort and dedication in providing experimental results. They found clarity in the response provided, specifically noting the differences between toxic and detoxified continuations from Table 3. While both content types were coherent, This reviewer raised concerns regarding the use of perplexity as a measure due to its observed low correlation with manual assessments in many generation tasks. However, they acknowledged that this might be a broader scientific discussion rather than a direct critique of the presented work. In conclusion, the reviewer encouraged the addition of two-step pipeline experiments and more prompt examples (including from other baseline models) to enhance comprehension for both annotators and readers.\nAnother reviewer acknowledges the author's clarifications but raises further concerns about the fundamental understanding of the task. While both language detoxification (preventing toxic language generation) and text detoxification (reducing toxicity of existing text) aim for a non-toxic output, they operate differently. The central question is whether detoxifying the \"language\" from which tokens are sampled affects the relevance of the generated text compared to the original Language Model's output. The reviewer posits that perhaps only certain toxic words need detoxifying while retaining content relevance. They believe that the evaluation methodology for this task might not be clearly established."
            }
        },
        "id": "Ul7LT9vGtD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jImeNRfAy2",
        "replyto": "jImeNRfAy2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission738/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494821,
        "cdate": 1696707494821,
        "tmdate": 1701465408862,
        "mdate": 1701465408862,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a novel framework for interactive text generation and introduces a new task benchmark. The reviewers agree that this framework and benchmark are useful for interactive text generation. The paper also proposes an interactive text-editing model and provides a thorough analysis of the proposed method. One area for improvement raised by the reviewers is that some of the important technical details are buried in the appendix. It would be beneficial to include these in the main body of the paper, a suggestion with which the authors have agreed, given the extra page."
            }
        },
        "id": "QephaVpQD2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "jAf0gd0ez4",
        "replyto": "jAf0gd0ez4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1983/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531421,
        "cdate": 1696707531421,
        "tmdate": 1701465449198,
        "mdate": 1701465449198,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper provides empirical evidence that knowledge distillation and label smoothing are not equivalent, contradicting previous work which has argued that KD could be interpreted as LS. The reviewers are generally in agreement that the empirical results are interesting and convincing, and that the experimental setup is well-executed. However R1 believes that the paper could benefit from a theoretical treatment to support the empirical evidence in order to be more convincing, as well as addressing more of the literature on the topic. Additionally, the generality of the results could be improved by incorporating data with different modalities e.g. vision. Overall though, the reception is quite positive and the paper would make a good contribution to the conference."
            }
        },
        "id": "utc5fZrJHm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "j9e3WVc49w",
        "replyto": "j9e3WVc49w",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3813/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707576119,
        "cdate": 1696707576119,
        "tmdate": 1701465510479,
        "mdate": 1701465510479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The content and results are clear and a great fit for this track. The focus on cognitive aspects is particularly relevant."
            }
        },
        "id": "RD08w5XPRa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "j9E9xLlTmB",
        "replyto": "j9E9xLlTmB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2715/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547986,
        "cdate": 1696707547986,
        "tmdate": 1701465474002,
        "mdate": 1701465474002,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors proposed a two-step scoring method for multiple choice reasoning tasks, based on the process of elimination.\nThe 2 out of 3 reviewers are selected strong soundness, and reproducibility is enough."
            }
        },
        "id": "sJVJd9Pcua",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "j6g3qwoQKU",
        "replyto": "j6g3qwoQKU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission182/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481241,
        "cdate": 1696707481241,
        "tmdate": 1701465390130,
        "mdate": 1701465390130,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers agreed that this paper presents an innovative approach to temporal knowledge graph completion. The method's combination of Allen intervals, neurosymbolic scoring, interpretable rule mining were recognized as strong contributions. Sufficient detail was provided to be able to reproduce the results. Several reviewers pointed out issues with presentation, writing, and clarity that could be improved. While the absolute performance of the proposed method for KGC in isolation does not represent a new SotA result, time interval prediction does improve over prior models and the authors propose an ensembling approach to improve KGC quality. During rebuttal discussion, the authors provided ancillary results that helped to quantify improvements and provide details to support their conclusions. The ideas in this paper and its goal of predicting both time intervals and tail entities are likely to generate significant interest for a small community interested in TKG modeling."
            }
        },
        "id": "yOzULJVrOP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "j61Sx05QRj",
        "replyto": "j61Sx05QRj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3658/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567144,
        "cdate": 1696707567144,
        "tmdate": 1701465505664,
        "mdate": 1701465505664,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper gives a focused contribution to the field of contrastive learning of sentence embeddings. By emphasizing hard negative samples during the contrastive learning process, the method improves on a contrastive learning approach called SimCSE.\n\nThe reviewers raised some questions regarding qualitative and quantitative analysis of the results, which the authors have satisfactorily addressed in their rebuttal."
            }
        },
        "id": "pFihnLokZI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "j48JCRagwR",
        "replyto": "j48JCRagwR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission802/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496281,
        "cdate": 1696707496281,
        "tmdate": 1701465410930,
        "mdate": 1701465410930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper under consideration introduces \"The Vault,\" a novel dataset designed for training large language models to comprehend and generate code. It consists of 41 million high-quality code-text pairs extracted from 10 prevalent programming languages, surpassing existing datasets in both quality and scale. The authors employ innovative data cleaning techniques, combining rule-based methods with deep learning classifiers, and open-source the toolkit used for dataset creation and quality assurance. The paper provides detailed experiments showcasing the dataset's advantages and is well-written and easy to follow.\n\n\n**Reasons to Accept:**\n1. The paper presents a significantly larger and higher-quality dataset compared to existing alternatives, with empirical evidence demonstrating its superiority in downstream tasks.\n2. The authors' data cleaning methodology is innovative and effectively filters out inconsistent code-text pairs.\n3. The open-sourcing of the toolkit for dataset creation aligns with the principles of open research and encourages community engagement.\n4. The paper's clarity and comprehensive experiments contribute to the understanding of the new dataset's capabilities.\n\n\n**Reasons to Reject:**\n1. The dataset's limitation to 10 programming languages may restrict its appeal to researchers interested in less prevalent languages.\n2. The evaluation of code generation relies on small models, potentially limiting insights into its performance with larger-scale models.\n3. The paper's evaluation of the dataset across three tasks using only one model for each task may not provide a holistic assessment of its adaptability across different architectures.\n4. The dataset, while high-quality, is smaller in scale compared to some raw datasets, potentially limiting its effectiveness in applications requiring larger volumes of data.\n\nThe paper appears to be sound, with reviewers generally agreeing on its methodological rigor and contributions. However, excitement levels are mixed, with some reviewers expressing reservations about the novelty and impact of the work."
            }
        },
        "id": "o6M1tVQqoR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "j2bP0STpw7",
        "replyto": "j2bP0STpw7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3799/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707575640,
        "cdate": 1696707575640,
        "tmdate": 1701465509969,
        "mdate": 1701465509969,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces the SDOH-NLI dataset for recognizing social and behavioral determinants of health (SDOH) from clinical notes. The authors frame the problem as a natural language inference (NLI) task and also provide a baseline performance using multiple models. Reviews point to the relevance and novelty of the work, but also highlight several key concerns such as data imbalance, redundancy, and issues related to the study's design and methodology. All reviewers have provided a \"Good\" soundness score but are \"Ambivalent\" regarding their excitement. All reviewers agree on the paper's relevance in addressing an important issue in healthcare, specifically the extraction of SDOH factors from clinical notes. This could potentially aid in improving healthcare equity and decision-making. Reviewers 2 and 3 indicate a lack of detail about the annotation guidelines, data collection, and the utility of the dataset to stakeholders. Reviewer 2 expresses concern about the lack of discussion on navigating privacy and regulatory constraints in data collection, which is crucial given the sensitive nature of healthcare data. Given the dataset's potential value and the paper's overall soundness, the paper is recommended to be accepted into Findings. Authors are encouraged to address the listed concerns to enhance the paper's quality and impact."
            }
        },
        "id": "2NKnIsjAub",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iytcEQ5I5v",
        "replyto": "iytcEQ5I5v",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3835/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707577096,
        "cdate": 1696707577096,
        "tmdate": 1701465511245,
        "mdate": 1701465511245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a new multimodal dataset for distress analysis. The social media posts have been annoated for the distress level and the cause. The construction of the dataset is sound and might be a useful resource for the community. The methodology is mostly sound and it might need to clarify some aspects (e.g. metric definitions, detailed experimental settings and model ablation, distress vs hate speech). However, the reviewers did not find the work particularly exciting."
            }
        },
        "id": "9P07sGX4kb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ix6h7Bkq62",
        "replyto": "ix6h7Bkq62",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5791/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618495,
        "cdate": 1696707618495,
        "tmdate": 1701465567205,
        "mdate": 1701465567205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers on the whole agree on the soundess of the work, and it's excitement. While the points raised by reviewer GhLs on `Human writing data & its distribution` are correct, the paper's focus is on whether or not detectors can identify the data from generators they have not seen data for previously.  Admitedly while this is a closely related question - a person with a new wrting style is akin to a new generator, the authors have followed the methodology of recent work in the area.  It would of been good the challenge some of these practices as the authors know their limitations."
            }
        },
        "id": "pQKuXoUnUf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iw4zUlc5OF",
        "replyto": "iw4zUlc5OF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3623/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566407,
        "cdate": 1696707566407,
        "tmdate": 1701465504294,
        "mdate": 1701465504294,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a survey on the problem of out-of-distribution generalization in natural language processing. The authors propose a novel taxonomy to categorize OOD challenges, describe multiple approaches in a structured way and also discuss recent large language models in the context of OOD generalization. In particular, the paper stimulates further discussion about the robustness of NLP models and the need for more accurate measures of their performance. Thus, it could be very helpful and impactful for researchers working on that topic.\n\nPros / Strengths:\n- Survey addresses a crucial problem in NLP\n- Well-written und well-structured\n- Novel taxonomy is helpful in understanding the problem\n- A wide range of tasks, datasets, and evaluation metrics related to OOD generalization are covered in the paper\n- Stimulates further discussion about the robustness of NLP models and how to measure their performance\n\nCons / Weaknesses:\n- Some concepts are not defined clearly enough\n- There is no quantitiative analysis of methods on benchmark datasets\n\nAction items for improving the paper:\n- Concepts around recent LLMs need to be defined more clearly\n- Limitations of current OOD datasets and evaluation practices should be discussed in more detail\n- Practical insights and recommended ways of dealing with different OOD issues should be better highlighted throughout the paper"
            }
        },
        "id": "xr8mgN3UGk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ivSJdhcuTi",
        "replyto": "ivSJdhcuTi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1585/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518485,
        "cdate": 1696707518485,
        "tmdate": 1701465435418,
        "mdate": 1701465435418,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Reject"
            },
            "comment": {
                "value": "The authors create a very interesting instruction dataset combining real GitHub data with ChatGPT expanded samples. While the discussion and motivation of the dataset creation method is very interesting and well-describe, the only evaluations they offer to demonstrate the value of this finetuning dataset are automatic GPT-4 based evaluations. The authors justify this criterion by citing a paper claiming that LLMs are effective at evaluating the quality of generated code, but this paper only compares the LLM generated evaluations to sequence similarity based metrics which are extremely problematic especially for code generation. See for example arxiv:2201.12901 figure 7 and arXiv:2108.07732 figure 10. At a minimum, showing a correlation between the automatic LLM evaluation and a code execution-based evaluation like HumanEval, MBPP, DataScienceProblems would be necessary to make a claim that LLM-based evaluation is effective. Even so, such evidence is only indirect and is not sufficient to make empirical claims."
            }
        },
        "id": "4PobuFr23P",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "islVqaCzfa",
        "replyto": "islVqaCzfa",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission261/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483280,
        "cdate": 1696707483280,
        "tmdate": 1701465393146,
        "mdate": 1701465393146,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "To assess the robustness of Large Language Models on multi-hop arithmetic reasoning tasks, the manuscript proposes a systematic approach that encompasses both lexical and semantic perturbations. All reviewers agree that the comparative analysis of distinct prompting methods, coupled with various proposed perturbations, offers significant insights. To further improve the quality of current draft, reviewers suggest 1) expanding to other reasoning task for wider generalization, 2) investigating different perturbations such as styles and orders, and 3) incorporating human evaluations as a supplement to the automated assessments. As some of these points are addressed during rebuttal, it is encouraged to properly integrate them into the revised manuscript."
            }
        },
        "id": "05lmBLfy7x",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "inN4TdboJX",
        "replyto": "inN4TdboJX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3919/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579237,
        "cdate": 1696707579237,
        "tmdate": 1701465514128,
        "mdate": 1701465514128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a knowledge-enriched discrete diffusion model for event schema induction. Further, the authors use an OpenAI LLM  to generate training data. The technologies used here are combined in an interesting and novel way, and the paper robustly demonstrates that the strategy leads to improved performance on several benchmarks. \n\nThe model and results presented are interesting. As identified by multiple reviewers, the main flaw of this paper is clarity – the original version left out many details, and important sections were underspecified. This has been partially addressed by authors in responses – i.e., the authors have added motivation for their choice to prompt through code generation. When the paper is updated to reflect the rebuttals, it will be in a much stronger state."
            }
        },
        "id": "Qspuw5fVXt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "in5xvBrMHv",
        "replyto": "in5xvBrMHv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1605/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707519583,
        "cdate": 1696707519583,
        "tmdate": 1701465435882,
        "mdate": 1701465435882,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores empathetic response generation in natural language processing, introducing the Emotion-Semantic Correlation Model (ESCM) to leverage dynamic emotion-semantic correlations. It conducts experiments on the EMPATHETIC-DIALOGUES dataset, comparing ESCM with state-of-the-art baselines and addressing ethical considerations."
            }
        },
        "id": "KADJByrOtH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ilCMZV0Qdl",
        "replyto": "ilCMZV0Qdl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission382/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486138,
        "cdate": 1696707486138,
        "tmdate": 1701465396988,
        "mdate": 1701465396988,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents examines alignment of LLMs with humans on the task of natural language inference. Thorough experiments on many models indicate that LLMs fail to adequately capture human disagreement. Concerns about the paper include the lack of actionable insights and the framing of the paper, which were addressed in the lengthy rebuttals."
            }
        },
        "id": "fiIkywUAD6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iipuAqcPGL",
        "replyto": "iipuAqcPGL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5338/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611206,
        "cdate": 1696707611206,
        "tmdate": 1701465556539,
        "mdate": 1701465556539,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Paper proposes an efficiency improvement method for long-range transformers. While the reviewers found the proposed method interesting, they also highlighted the complexity of it and questioned whether the conducted experiments were enough to explain the complexity of the proposal."
            }
        },
        "id": "EN6MEBWqKh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iiWP7khhwP",
        "replyto": "iiWP7khhwP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3424/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562397,
        "cdate": 1696707562397,
        "tmdate": 1701465496974,
        "mdate": 1701465496974,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes an interesting two-pass method for promting LLMs for translation between related languages. \n\nThe proposed method yields better performance in terms of automatic evaluation scores than previous prompting methods over several language pairs from different families. \n\nThe method may be an inspiration for future work on translation between non-English or low-resourced languages using LLMs.\n\nThe revisions arised during authors' response should be applied in the final version."
            }
        },
        "id": "ayqMV8YXyd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ii9ZoryPH2",
        "replyto": "ii9ZoryPH2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1308/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510052,
        "cdate": 1696707510052,
        "tmdate": 1701465426892,
        "mdate": 1701465426892,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes two methods to improve simplification of medical texts: unlikelihood loss during training and reranked beam search during decoding. Both techniques intend to  to optimise for readability by relying on flesch-kincaid grade level, which is a metric based on average sentence and word lengths.\n\nReviewers recognise that the proposed method is straightforward to understand. In addition, the authors compare their approach against strong baselines, and show improvements mostly based on automatic metrics (flesch-kincaid included), as well as with some human evaluation.\n\nOne of the main concerns is related to the suitability of flesch-kincaid, traditionally a document-level metric, for assessing the readability of words/phrases and partial sentences. Previous work has pointed out that it is easy to fool this metric (https://aclanthology.org/2021.gem-1.1/) and that it does not agree with judgements of simplicity at the sentence level (https://aclanthology.org/2021.cl-4.28/). However, it is difficult to disregard the results obtained by the authors' experiments, where their proposed model does improve over the BART-XSUM baseline, even using human judgements (despite low inter-annotator agreements, which is common in subjective tasks). In their rebuttal, the authors also point out that their method allows for less conservative outputs, which is an interesting finding that they are recommended to include in the paper."
            }
        },
        "id": "fRSOwRtovM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ifuvyCdLro",
        "replyto": "ifuvyCdLro",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2419/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541750,
        "cdate": 1696707541750,
        "tmdate": 1701465464254,
        "mdate": 1701465464254,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper is commended for addressing an interesting and important problem of language model backbone replacement for personalized downstream tasks in an on-device, non-stationary scenario. The clear separation between cloud and edge is acknowledged as crucial for the efficiency and privacy of ML models on low-powered devices, making this a relevant and timely topic. Reviewers appreciate that the proposed method achieves over 1000x computation reduction in FLOPs for back-propagation while outperforming comparable transfer learning methods. Additionally, the paper is recognized for conducting extensive experiments with various models and datasets, as well as including an ablation study. \n\nHowever, reviewers raise concerns about the clarity and presentation of the paper. They note that the problem definition is not clear, and the paper lacks a clear, concise summary of the training procedure. There are concerns about the lack of certain experimental details. Reviewers suggest reporting memory usage for each method and providing actual running times in addition to FLOPs computation. I believe the inclusion of these details would strengthen the paper's experimental evidence.\n\nOverall, the paper addresses an important and interesting problem with significant computational and performance improvements, but it needs to address concerns related to clarity, presentation and experimental details. Addressing these issues would enhance the paper's quality."
            }
        },
        "id": "hLHZ5QUUi2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iaxdEnxgju",
        "replyto": "iaxdEnxgju",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5511/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614387,
        "cdate": 1696707614387,
        "tmdate": 1701465561385,
        "mdate": 1701465561385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers agree on the quality of the evaluation and find the work exciting. Authors address most of the concerns/questions (if not all) raised by reviews and provide additional experiments and ablations. I recommend authors to improve the explanation of the evaluation procedure (i.e. how it differs from the setting used by ATTEMPT and MPT + single task fine-tuning) in the camera ready version."
            }
        },
        "id": "TwR8Wu5kj7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iammae3CbG",
        "replyto": "iammae3CbG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2856/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550878,
        "cdate": 1696707550878,
        "tmdate": 1701465478951,
        "mdate": 1701465478951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper conducts an interesting study on the model robustness problem of NER. It introduces V-information for measuring the difficulty of individual examples and entire datasets and context-entity information margin (CEIM) for assessing the difference in difficulty between entity- and context-level instances.\nthe paper found that lexical information of entities is easier to learn compared to contextual information, which makes models learn shortcuts from entity strings. The paper argues that the difference in learning difficulty between entity and context information should be reduced to de-bias the model.  In general, this paper represents a well-executed and systematic exploration. Its main limitation lies in the lack of novelty in the conclusions it draws and the proposed solutions, as they align closely with widely accepted findings within the field."
            }
        },
        "id": "jCV8HbWF2L",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iWVpissNEP",
        "replyto": "iWVpissNEP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5313/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610787,
        "cdate": 1696707610787,
        "tmdate": 1701465555884,
        "mdate": 1701465555884,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes a method to group languages in multilingual tasks, e.g., text classification and sequence labeling, by leveraging the gradient similarity. The languages are clustered according to the metric, and a model is trained for each cluster.\n\nStrengths\n* The proposed method is well-motivated with comprehensive analyses on the impact of topical distributions and representation in layers.\n* Experiments were carried out on various tasks with consistent gains especially targeting low-resource language settings.\n\nWeaknesses\n* The gains are small, and they have not experimented on other tasks, such as natural language understanding.\n* Some clarity issues in the current manuscript, which could be addressed in the revised one without changing the conclusion."
            }
        },
        "id": "gBzi4Yjt8h",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iVINvItqhb",
        "replyto": "iVINvItqhb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3241/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558909,
        "cdate": 1696707558909,
        "tmdate": 1701465491568,
        "mdate": 1701465491568,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses multilingual audio-visual speech recognition and introduces a method for feature extraction and language classification loss optimization. Experiments are conducted on a multilingual dataset, MuAViC.\n\nPros:\n- Innovative approach through fine-tuning with language prompts.\n- Demonstrates robustness in low-resource languages and noisy environments.\n- Addresses language imbalance with a weighted objective function.\n- Comprehensive experiments on a challenging dataset.\n- Potential impact on reducing language-specific models.\n\nCons:\n- Issues with clarity, including unclear components and errors in the text.\n- Inconsistent baseline numbers and unclear source for references.\n- The value and impact of the prompt component are not adequately explained.\n- Lack of an in-depth ablation study.\n- No results for the scenario where the language is unknown during inference.\n- Details about dataset language distribution are missing.\n- Could explore the integration of recent advancements in multimodal learning.\n- Focuses primarily on the MuAViC dataset; generalization across diverse data sources is not explored.\n\n\nOverall, the paper's strengths lie in its innovative approach, robustness, and potential impact on multilingual speech recognition. However, there are areas of improvement related to clarity, experimental methodology, and further exploration of the proposed method's capabilities."
            }
        },
        "id": "yG61z1J4ae",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iRIj0OvFG1",
        "replyto": "iRIj0OvFG1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission138/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480161,
        "cdate": 1696707480161,
        "tmdate": 1701465388569,
        "mdate": 1701465388569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors describe an interesting and well-executed study. The reviewers agree that the quality of the experiments is high and report no issues with clarity. The main concerns relate to the originality of the approach compared to previous work by the same authors, as well as the significance of the results compared to past experiments and baselines. The author response does a good job alleviating some of these concerns and showcasing the novelty and value of the proposed approach. In my view, the main question at this point is whether this paper should be published in its current form or if there should be a bigger paper that combines this approach with the one proposed in prior work and provides comparison to enhanced baselines.\n\nQuality:\n\nPros:\n\n-\t“This paper presents a straightforward method to make use of prior images for radiology report generation. The authors present compelling results on public datasets, outperforming other recent baselines.”\n\n-\t“Comprehensive ablation experiments on respective contribution of prior data, as well as SA dropping”\n\nCons:\n\n-\t“As the image encoder was trained for multi-label prediction, it may not represent the long tail of rare diseases as well.”\n\nOriginality:\n\nPros:\n\n-\t“This shift from traditional image-level features to anatomical representations presents a new perspective and can potentially enhance the accuracy and clinical relevance of automated reports.”\n\n-\t“The introduction of sentence-anatomy dropout as a training strategy allows the report generator model to predict sentences corresponding to specific anatomical regions. This fine-grained controllability in report generation is crucial for clinicians who require targeted and interpretable reports, and it addresses a significant limitation of previous automated reporting systems.”\n\nCons:\n\n-\t“The paper appears to combine ideas from existing works rather than introducing a novel structure. It is unclear whether the reported performance improvement is solely attributable to this paper or results from the combination of ideas from the previous work (Anonymous, 2023). This ambiguity makes it challenging to ascertain the specific contributions of this paper and may raise concerns about the novelty and originality of the approach.” In their response, the authors highlight that there are methodological changes that lead to improvements on the most significant metrics, as well as to some disadvantages shown in the ablation study. I agree with the point raised by the reviewer that a combination between the two studies into one paper will likely provide better value to the community.\n\n-\t“The paper introduces Longitudinal Representation Learning as an extension of Karwande et al. (2022) by using Faster R-CNN (Ren et al., 2015). This approach does not seem to offer a significant advancement beyond the existing methods, potentially limiting its contribution to the NLP community.” The authors respond that: “Our novelty lies in proposing a region-level mechanism for the use of prior images as context for text report generation (to the best of our knowledge, we are the first to do so). Since we are inputting the visual tokens to a large Transformer model (and training end-to-end), for practical reasons we chose to use a more computationally efficient method than Karwande et al, employing an MLP rather than GCN, and performing unified region detection and encoding in Faster R-CNN (instead of cropping the RoIs from the CXR and encoding them separately).” Based on my understanding as a metareviewer, the contribution here is mainly in the computational efficiency of the new approach, which I believe is an important advancement.\n\nSignificance:\n\nThe main issue here relates to the fact that the comparison to some baselines may not be fair since they were not provided the additional data that the main method utilizes.\n\n-\t“Unclear if the comparisons against existing methods are entirely fair, given possible use of prior and SA-based inputs not available to such methods”. Another reviewer notes: “For sure that if the baseline never sees anat. region-sentence pairs in training, then it will not generate sentence properly if asking for region descriptions. This experiment relating to hallucination can be better designed.” The authors respond that: “We agree that including sentence/anatomy annotations using the method proposed in this paper or alternative supervision strategies, could improve other existing methods, and hope this paper provides evidence to try this.” The question about fairness of comparison remains – why not compare to these previous methods by providing them with the additional data as well?\n\nClarity:\n\nNo major concerns reported in the reviews."
            }
        },
        "id": "Bl1lBntQWP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iRISsJCzTA",
        "replyto": "iRISsJCzTA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3961/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580115,
        "cdate": 1696707580115,
        "tmdate": 1701465515388,
        "mdate": 1701465515388,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "*Summary*: This work evaluates the zero-shot capabilities of ChatGPT and GPT-4 on multi-party conversation (MPC) tasks. Five tasks including emotion detection, accurately detecting the addresses in a conversation, speaker identification, response selection and generation have been explored. They find that including speaker and addressee information in the prompt yields performance improvements.\n\n*Evaluation*:  R1 and R3 rated this work as medium to strong on soundness (3/4), while excitement ratings were lower (2/3). R2 rated this low on soundness (2) and stated two main concerns: A) Since training data of ChatGPT, GPT-4 is unknown, we cannot be sure there is no test leakage. B) No in-depth analysis of failure cases has been presented. Regarding A), this is a broader concern that holds for all closed model evaluations and not specific to this work. B) is indeed a valid concern. Given this is a first investigation of the capabilities of ChatGPT/GPT-4 on MPC tasks, demanding an in-depth analysis of the failure cases (along with anecdotal examples in an Appendix) is warranted and would have been useful for the reader."
            }
        },
        "id": "pZkcuJSnNl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iO5YOddOyG",
        "replyto": "iO5YOddOyG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1508/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516056,
        "cdate": 1696707516056,
        "tmdate": 1701465433002,
        "mdate": 1701465433002,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper undertakes an exploration of the universal geometry within embeddings through independent principal analysis (ICA). The study demonstrates that embeddings can be effectively represented in lower dimensions using ICA-transformed axes, encompassing different languages, algorithms, and modalities.\n\nOverall, the reviewers generally find the paper sound. All reviewers agree that the paper is well-written and extensively supported by experimental results, enhancing its credibility and potential usefulness for NLP model design. In light of these merits, the consensus among reviewers is to accept the paper for publication."
            }
        },
        "id": "Z80NN6RSSa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iMnwXQemEr",
        "replyto": "iMnwXQemEr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission898/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498708,
        "cdate": 1696707498708,
        "tmdate": 1701465414188,
        "mdate": 1701465414188,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a lightweight framework to utilize text for training speech processing models. In particular, it adds a latent synthesizer module that maps text into speech embedding for training a shared backbone model that takes this embedding as input. Two implementations (fixed projection and diffusion) of the latent synthesizers are presented. Effectiveness of the approach is verified on speech recognition (Librispeech) and spoken language understanding (SLURP and STOP).\n\nAll reviewers find the approach novel and can be easily reproduced. The proposed approach also demonstrates strong performance over supervised baselines and other semi-supervised baselines on multiple speech processing tasks. \n\nThe main caveat of the manuscript is the lack of comparison with semi-supervised baselines that utilizes text data (only compared with TTS data augm. (Laptev et al., 2020)). While I agree with the authors in the rebuttal that SpeechUT may not be a suitable baseline because it is built upon HuBERT utilizing unpaired speech, there are still prior studies such as [1,2]. Specifically, [1] trains a text-to-encoder (TTE) model that predicts ASR embeddings from text and uses that for training shared components. This is highly related to the proposed method, but I acknowledge that proposed methods does have novelty by leveraging diffusion model.\n\n[1] Hayashi, Tomoki, et al. \"Back-translation-style data augmentation for end-to-end ASR.\" 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018.\n[2] Liu, Alexander H., Hung-yi Lee, and Lin-shan Lee. \"Adversarial training of end-to-end speech recognition using a criticizing language model.\" ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019."
            }
        },
        "id": "oUAtbl6Smb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iLTNcB3601",
        "replyto": "iLTNcB3601",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission94/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479028,
        "cdate": 1696707479028,
        "tmdate": 1701465387182,
        "mdate": 1701465387182,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this paper, the authors use a named entity recognition model trained on a novel annotated dataset to analyze how property renters in New York City describe the locations of their listings on Airbnb. The authors connect this linguistic study with a spatial analysis of gentrification in New York. They also contextualize their findings using theories of “critical toponymy” from social science. \n\nOverall, the review process converged to a consensus on this paper. The paper gained high soundness and excitement scores from all reviewers. Reviewers also offered very positive qualitative feedback.\n\nPositives which emerged from the review process:\n- All reviewers assigned a score of 4 for soundness and a 4 for excitement. One praised the paper for being “clear and well-written,” and “properly contextualizing” a computational analysis. Another noted that “the paper is well-written and I really enjoyed reading it.” A third described the paper as a “great example of high quality computational social science.”\n\nNegatives which emerged from the review process: \n- While the review process did surface some issues surrounding soundness in the original draft, these were addressed during the rebuttal period. Reviewer QGQw asked for missing details about KDE parameters, the NER tagging scheme, the outlier detection procedures and the F1 score for a specific model. Reviewer AMJs also asked for missing information about the correlation statistic. However, the authors provided detailed answers to these questions, and both reviewer QGQw and reviewer AMJs replied to the rebuttal to say that their concerns had been addressed. Each mentioned they still had a positive assessment of the paper after reading the rebuttal.\n\n- Reviewer B1cM also noted that one limitation of this work is that it focuses on a single city (New York). The authors replied that this was due to practical considerations, and that their paper offers a proof-of-concept which might be applied to other cities in the future. They noted that annotated data for other cities could improve their analysis. B1cM read this rebuttal and did not change their already high score."
            }
        },
        "id": "DNJeovZE3k",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iIpnncYQZb",
        "replyto": "iIpnncYQZb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1158/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506291,
        "cdate": 1696707506291,
        "tmdate": 1701465422101,
        "mdate": 1701465422101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "A pipeline for the topic of knowledge selection. However, some reviewers think the author still has to address some of the points in the final version."
            }
        },
        "id": "88hecBl7qb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iIoHir5Hyg",
        "replyto": "iIoHir5Hyg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4926/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601924,
        "cdate": 1696707601924,
        "tmdate": 1701465545017,
        "mdate": 1701465545017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper is the clear and concise description of the proposed method, However, there are some insufficient works, such as evaluation metrics for summary redundancy/coherence,  human evaluation, etc. \nMoreover, more advanced datasets and flexible tasks are needed to contribute the development of extractive summarization, other than CNN/DM or MDS. \nTherefore, I would suggest the paper accepted as Findings in the conference."
            }
        },
        "id": "qoCwNHHpvJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iHb4MOMyOd",
        "replyto": "iHb4MOMyOd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3818/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707576364,
        "cdate": 1696707576364,
        "tmdate": 1701465510621,
        "mdate": 1701465510621,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an approach for combining LM generated and retrieved support passages in open-domain QA to balance coverage and faithfulness. The paper solves an important problem, and the reviewers all agreed that the approach is sound and that the paper is well-written. The reviewers were moderately excited about this work because the improvements over baselines were small particularly given that the approach requires additional training.\n\nI think this is solid work, and while the improvements are small, it does provide an interesting solution for the coverage-faithfulness tradeoff by combining retrieval and generation."
            }
        },
        "id": "9EctEtpBgm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iEACF99lQz",
        "replyto": "iEACF99lQz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1157/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506206,
        "cdate": 1696707506206,
        "tmdate": 1701465422141,
        "mdate": 1701465422141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces BMTPT to address the challenge of improving the performance of PTLMs on target tasks through task-specific prompt fine-tuning. BMTPT takes into consideration the correlation among source tasks to enhance the transfer to target tasks. Particle-based Variational Inference and Stein Variational Gradient Descent (SVGD) are utilized to solve the multi-task prompt tuning problem. Specifically, BMTPT aims to learn a global posterior approximation for source tasks, facilitating efficient transfer to downstream target tasks. Experimental results on standard benchmark NLP tasks demonstrate the effectiveness and parameter efficiency of the proposed Bayesian multi-task transfer learning approach, outperforming existing methods by capturing task correlations and leveraging this knowledge to improve target task performance.\n\n\nThe majority of reviewers accept that the soundness of this work is moderate and the excitement level is decent. Reviewer AbeE has some concerns but the reviewers also provided substantive responses. However, the reviewer did not engage in discussions and I found responses by the authors at least relevant."
            }
        },
        "id": "sSk2vBTQUk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iDZQG9aUGH",
        "replyto": "iDZQG9aUGH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5334/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611014,
        "cdate": 1696707611014,
        "tmdate": 1701465556493,
        "mdate": 1701465556493,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a solution for handling missing information in knowledge bases while answering questions over them (KBQA). Particularly when natural language queries translated into SPARQL fail to execute, parts of the query are translated back to natural language and an off-the-shelf textual QA system is used to find the missing information. The reviewers all agreed that the approach is well-motivated and that the paper is well-written. The authors addressed the reviewers' concern regarding missing error analysis. One outstanding concern is regarding the specificity of the approach to handle missing _temporal_ information alone, and the authors argues that this kind of information represents a significant bottleneck in KBQA. Overall, I think this is a strong paper."
            }
        },
        "id": "MEKuLAhQXE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iDQBP0cvzX",
        "replyto": "iDQBP0cvzX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3236/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558810,
        "cdate": 1696707558810,
        "tmdate": 1701465491404,
        "mdate": 1701465491404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper seems to introduce a new and interesting task on text fact transfer with promising applications. The proposed model seems intuitive and technically sound. The evaluation seems exhaustive with multiple real world datasets and a reasonable set of metrics. The results are competitive with clear writing. All reviewers find this paper to be mostly technically sound with moderately high excitement. Reviewers shared some concerns on sensitivity to slight variations of prompt, choice of factuality metrics, discussion on applications where style and meaning can be disentangled reasonably, and also discussion on case studies to further illustrate the effectiveness of the methods. Reviewers were mostly happy with the author's responses. Authors should include these changes in the revision, which could significantly increase the impact of this work."
            }
        },
        "id": "P0iUfUXw8c",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iDBUssVu5Z",
        "replyto": "iDBUssVu5Z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2258/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538053,
        "cdate": 1696707538053,
        "tmdate": 1701465459293,
        "mdate": 1701465459293,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an interactive image translation method that generates source and target language tokens jointly as a multi-task problem using cross-lingual and cross-modal attention mechanisms.\nThe reviewers agree with the clarity of the paper and the effectiveness of the proposed method.\nThere was a discussion on the novelty of the approach, but it seems to be resolved by the authors’ rebuttal.\n\nHere is a list of the pros and cons of this paper.\n* Pros\n- Clear and well-written (jT2T, gHG5)\n- Effectiveness against existing methods (DKy3, jT2T, gHG5)\n* Cons\n- Some essential technical details should be improved (DKy3)\n- More analyses should be conducted (e.g., visualization of attentions) (jT2T)"
            }
        },
        "id": "qoMawZA8so",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iCNoSVJl2y",
        "replyto": "iCNoSVJl2y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission945/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499987,
        "cdate": 1696707499987,
        "tmdate": 1701465415716,
        "mdate": 1701465415716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a training-free memory selector to improve memory efficiency in Transformer-XL models, which includes a ranking-based method to select the top-k memory tokens. The proposed method, TRAMS, is demonstrated to be effective in improving memory efficiency through experiments. \n\nStrengths:\n* The paper is well-structured and clearly explains the proposed method.\n* TRAMS is a novel and effective memory-selection algorithm that is easy to integrate with existing transformer-based models.\n* The experimental setup is comprehensive and the authors compare their method with state-of-the-art techniques.\n\nWeaknesses:\n* There is a lack of detailed discussion on the results and their implications.\n* The paper would benefit from more experimental results and theoretical justifications to support the claims.\n* Incomplete content makes it difficult to understand the full context and impact of the paper.\n\nOverall speaking, this paper gives some insightful notes on the method and has clear goodness. However, details and many contents need to be enhanced."
            }
        },
        "id": "PYJwPtf5fv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iCLJHkE5s1",
        "replyto": "iCLJHkE5s1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5220/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609325,
        "cdate": 1696707609325,
        "tmdate": 1701465553341,
        "mdate": 1701465553341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Although the reviewers acknowledge the intuitive masking approach, improved efficiency, and novelty, they also suggest improvements such as conducting multilingual evaluation, expanding comparisons to other diffusion models, and providing more details on the reverse process and target ambiguity."
            }
        },
        "id": "Btb9ANwTR2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iBv0M8WrFi",
        "replyto": "iBv0M8WrFi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4628/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594715,
        "cdate": 1696707594715,
        "tmdate": 1701465537066,
        "mdate": 1701465537066,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an extensive and in-depth investigation into the influence of pre-training, fine-tuning, model-modality, and OOD scoring functions across a wide range of document OOD detection tasks. To enhance OOD detection performance, the authors introduce a spatial-aware adapter. The paper's well-structured presentation effectively communicates the central concepts, and the experiments conducted are comprehensive.\n\nAll reviewers agree that this paper offers a thorough and extensive empirical analysis of document OOD detection, yielding valuable insights substantiated by robust experimental results. In light of this, the reviewers find the paper sound and express enthusiasm for the practical implications of the proposed add-on spatial-aware adapter and the depth of the analysis.\n\nAs a result, the final decision for the paper is acceptance."
            }
        },
        "id": "QgULyzVY0X",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "iAeDYlEXrM",
        "replyto": "iAeDYlEXrM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2280/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538492,
        "cdate": 1696707538492,
        "tmdate": 1701465459828,
        "mdate": 1701465459828,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a method that mixes LLM-style prompting and seq2seq encoder-decoder methods in NMT. The result allows for combining terminology integration, few-shot translation and domain adaptation.\n\nPros: reviewers have relatively high opinions and raise valid but non-critical concerns.\n\nCons: the authors fail to address the raised concerns.\n\nAs a conclusion, the contribution is exciting, but there are major unaddressed questions with the paper: namely, that the terminology metric relies on the same terminology as the method (and thus additionally penalizes methods that do not use this terminology), no comparison to existing LLMs is given."
            }
        },
        "id": "DKsGaegKqz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "i7ifZu49kW",
        "replyto": "i7ifZu49kW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3276/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559573,
        "cdate": 1696707559573,
        "tmdate": 1701465492672,
        "mdate": 1701465492672,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the harms of anthropomorphism in dialog systems. The authors studied cognitive factors that promote anthropomorphism, discussed relevant linguistic and psychological features causing this problem and made recommendations for reducing such problems. Reviewers agree that the paper has sufficient support for its claim and would be a great reference for future work in related topics."
            }
        },
        "id": "dJNHT3ZQDS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "i65hZUPwuQ",
        "replyto": "i65hZUPwuQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission84/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478752,
        "cdate": 1696707478752,
        "tmdate": 1701465386732,
        "mdate": 1701465386732,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the difference between a models behaviour and query representation. They highlight the difference between these two paradims and create a taxonomy of clases of disagreement between the two methods. \n\nBoth aRIY and M8PW praise the work, stating that the problem is interesting, the taxonomy is valuable and there are substantial empirical results, especially for a short paper. M8PW does have some qualms with the presentation, for which they made ample suggestions. The author response makes me (as well as the reviewer) confident that the authors will improve this for the camera ready version of the paper. M8PW furhermore points out that there is little hyperparameter search on the probe side, I believe the authors succesfully rebutted this in their response. The third reviewer, WxBN, is less enthusiastic about the work, but provides no arguments for their main stated weaknesses. Given the fact that the other reviews are more elaborate and interactive with the authors, I don't think the lower scores of WxBN should stand in the way of publication at EMNLP."
            }
        },
        "id": "XWlE0iReno",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "i1KSRMVlST",
        "replyto": "i1KSRMVlST",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4060/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582257,
        "cdate": 1696707582257,
        "tmdate": 1701465518627,
        "mdate": 1701465518627,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Originality**\n\nThis paper is very original and proposes evaluating ASR systems in fairly novel way. It enables this evaluation by presenting a dataset with fine-grained annotations for noise types (both background and \"speaker\" noise).\n\n**Significance** \n\nThe topic of explainability in ASR, which this paper is among the first to address, is very important. The resource that the authors have created can easily be used in conjunction with deletion, insertion and substitution errors to help diagnose ASR errors in the presence of specific source noises or speech characteristics. However, it does not appear that automatic classification of text errors using ChatGPT is feasible, so there may be limits the extent to which this error analysis can scale to other ASR systems.\n\n**Clarity**\n\nThe paper is generally well structured, and most details are explained. Most reviewers agreed the paper was well-written, however, as the meta-reviewer, I did want to inform the authors of a number of grammatical, spelling or other errors that I found while reading the paper, and which I found made it slightly difficult follow at times, but which are easily fixable.\n\n(1). \"Our motivation for proposing a KEBAP, which contains both aspects, is detailed below.\" -- Which two aspects? It was not clear.\n\n(2) \"Since benchmarks measure performance with quantitative metrics, it is crucial **to fine-grain** characteristics for a more detailed diagnosis\" -- Perhaps there was a missing word here? it is crucial to **examine**?\n\n(3) \"Hence, to solve the explainability issue, we must define **the** error type criteria that consider both the speech- and text-level and create benchmarks to achieve human-level explainability.\"  -- remove **the**\n\n(4) GEC acronym in 2.3 is not defined until the subsequent section \n\n(4) \"we selectively compose **a** text-level error types dataset by human evaluation.\" \n\nStep 1: Build Text-Level Error Corpus (An example would be nice for clarification)\n\n(5) \"we request the recording participants to incorporate characteristics\" --> \"we request that recording participants incorporate characteristics\" \n\n(6) The x-axes in figures 4. and 5. use a different order of text-level errors. It makes it hard to interpret those plots. Furthermore, they are called correlation distributions, but I think they are just 2-d histograms with counts of errors. The text should be updated to reflect this.  \n\n**Pros:**\n   - An exciting new way to evaluate and compare ASR systems\n   - A new data resource to enable ASR model comparison using different noise types (background and speech)\n   - Generally well-written save for the items mentioned in the previous section\n   - The noise portion of the dataset is easily applicable in the analysis of new models, as demonstrated by the rapid feedback from the authors.\n\n**Cons:**\n   - Almost none\n   - The proposed method currently relies on human annotation of text errors as far as I can tell which limit applicability of the text errors to other models."
            }
        },
        "id": "JshPWx2cvI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "i17SCD0YDI",
        "replyto": "i17SCD0YDI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission843/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497527,
        "cdate": 1696707497527,
        "tmdate": 1701465412538,
        "mdate": 1701465412538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper moves the pareto frontier for the quality-latency tradeoff in simultaneous machine translation by proposing to separate the adaptive policy model from the translation model and introducing a supervision signal based on statistical divergence.\n\nThe reviewers agree that this work is technically sound, well written, and exciting for the community."
            }
        },
        "id": "M4b4JGUcby",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "i0vMIpaEn4",
        "replyto": "i0vMIpaEn4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3235/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558728,
        "cdate": 1696707558728,
        "tmdate": 1701465491303,
        "mdate": 1701465491303,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies active learning methods for selecting demonstrations to prompt large language models. Four simple approaches are analyzed: uncertainty, diversity, similarity, and random sampling. Results on GPT and OPT models suggest that similarity to the test example is best.\n\nPros:\n- The problem that the paper is solving is important and relevant to the community.\n- Thorough and extensive analysis of the topic\n- The findings of the evaluations in the paper are relevant to the practitioners.\n\nCons:\n- Novelty of the findings in the paper. Some of the findings are confirming already known results.\n- Some of the findings are contradictory across different metrics."
            }
        },
        "id": "SiCMN9U8BA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "i0RfSS9CUU",
        "replyto": "i0RfSS9CUU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4001/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580991,
        "cdate": 1696707580991,
        "tmdate": 1701465516666,
        "mdate": 1701465516666,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces, a novel approach to improving cross-lingual transfer in pretrained multilingual models. The method leverages multilingual adapters trained on different granularities (sentence-level and document-level) and combines them using AdapterFusion. These adapters are pretrained using contrastive learning on a newly created multilingual entity-aligned dataset. The paper demonstrates the effectiveness of their approach through extensive experiments, achieving state-of-the-art results on various cross-lingual tasks.\n\nReviewers raised several concerns, including the need for clearer delineation of contributions, a desire to decouple the impacts of document-level adapters and contrastive learning, and a request for more detailed explanations of experimental choices, particularly regarding low-resource languages.\nThe authors responded to the concerns by providing clearer explanations of their contributions, emphasizing the non-trivial nature of fusion, and explaining the challenges of decoupling factors tied to the pretraining objective. They addressed the request for more clarity on the impact of experimental choices and offered additional data regarding low-resource languages.\n\nOverall, while the reviewers had concerns about the clarity of contributions and the impact of individual factors, the authors' responses provided reasonable justifications and clarifications."
            }
        },
        "id": "FhGJTKDf3d",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hyBwGem8OS",
        "replyto": "hyBwGem8OS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5525/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614528,
        "cdate": 1696707614528,
        "tmdate": 1701465561631,
        "mdate": 1701465561631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In this paper, the authors propose a patient-centered medical dialogue model named PlugMed, which leverages prompt learning from large language models (LLM). PlugMed is equipped with two key modules: a prompt generation (PG) module and a response ranking (RR) module. These components work in tandem to enhance LLMs' dialogue strategies, with the primary goal of improving response specificity.\nThe experimental results presented in the paper demonstrate that PlugMed outperforms other large language model-based methods by achieving more accurate dialogue intents.\n\nHowever, I agree with one of the reviewers that this paper primarily combines established components into a unified system and lacks novelty. Another shortcoming of the paper is the absence of standard evaluation metrics commonly used for dialogue generation tasks, such as ROUGE or BLEU. These metrics are essential for assessing response quality and should be included to strengthen the paper's empirical findings."
            }
        },
        "id": "lTfE8kkomR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hxExXDMwcc",
        "replyto": "hxExXDMwcc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission433/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487651,
        "cdate": 1696707487651,
        "tmdate": 1701465398798,
        "mdate": 1701465398798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces comprehensive new data sets for code translation between many different programming languages.\n\nPros:\n- Valuable new data sets, high number of programming languages covered. Much larger than other data sets in the space.\n- Detailed experiments involving LLMs and code translation using these new data sets.\n- Inclusion of even \"niche\" programming languages like Fortran, for which code translation can be useful as part of modernization of legacy systems.\n- Reproducibility and extensibility: Datasets, code, experimental results, and model checkpoints are openly available, enabling further research down the line.\n- Well structured and easy to follow.\n\nCons:\n- Some concerns about missing comparisons with other data sets, and unclarity around the selection of the models that were chosen for initially evaluating the benchmark on. This can be addressed in a minor revision.\n\nThe main focus of this paper is the creation of the new data sets, not the experimental results on various models, so it's not a problem that the tables with the detailed experimental results only appear in the appendix, given that the top-level analysis of these experiments appears in the main body of the text."
            }
        },
        "id": "KIWEq47VLE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hv3VpXDIh8",
        "replyto": "hv3VpXDIh8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2331/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539639,
        "cdate": 1696707539639,
        "tmdate": 1701465461421,
        "mdate": 1701465461421,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a novel approach to incorporate global discourse structures for event coreference resolution, in particular, for cross-document event coreference resolution, the approach will conduct text compression on two documents except for two focus sentences that contain event mentions under scrutiny. Reviewers expressed excitements on this approach, especially on applying text compression to enable effective discourse analysis on the long meta document combining two original documents."
            }
        },
        "id": "KbHdmu0TEk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "htulPWUheU",
        "replyto": "htulPWUheU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1793/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527287,
        "cdate": 1696707527287,
        "tmdate": 1701465441736,
        "mdate": 1701465441736,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Interesting post-hoc refinement technique, potentially extendable to other knowledge-grounded generative tasks. It also contains important checks for source-faithfulness or provenance."
            }
        },
        "id": "EzumRRPswK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hsptWISmi6",
        "replyto": "hsptWISmi6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission660/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493182,
        "cdate": 1696707493182,
        "tmdate": 1701465406786,
        "mdate": 1701465406786,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper deals with the knowledge editing problem, or how to update existing knowledge in language models.  Unlike past approaches, which update parameters, this paper approaches this purely through in-context learning: showing examples of edited facts and QA pairs reflecting the updated knowledge (or not, in the case of \"retaining\" examples). Results focus on the COUNTERFACT dataset, with some additional evaluation on open-ended generation in the Appendix.\n\nThis is an interesting approach with solid results on COUNTERFACT. The reviewers liked the simplicity of the approach and found the paper to be well-written.\n\nfQT9 brings up the fact that this may specialize the added knowledge to a certain format, and not update it when the model is used in an open-ended generation fashion. This is shown in Appendix D.2, but not evaluated systematically.\n\ni8Yq brings up narrowness of the benchmarks, and fQT9 also wonders about the ability to inject multiple facts.  I think that more broadly, some of the reviewers' questions point to a concern that this paper is missing the point about knowledge editing a bit. The ability to use a fact in context to make simple inferences doesn't really look like knowledge editing.  It has more in common with past studies on knowledge conflicts ( https://arxiv.org/pdf/2109.05052.pdf , https://arxiv.org/pdf/2210.13701.pdf , etc.), which investigate where this kind of in-context knowledge clashes with parametric knowledge.\n\nIn simple fact-editing, the present paper shows that the model successfully navigates these conflicts. However, in more complex settings, they may more seriously impact this method than those that update the parameters.\n\nAs a result, I think that this paper is sound in that the results it presents are technically correct, but I have some lingering uncertainty about whether this approach to the broader research questions makes sense."
            }
        },
        "id": "xpKBlICuuq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hsjQHAM8MV",
        "replyto": "hsjQHAM8MV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission774/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495623,
        "cdate": 1696707495623,
        "tmdate": 1701465409917,
        "mdate": 1701465409917,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates methods for in-context-learning (ICL) sample selection for entity extraction from scientific documents and compares it against a fully supervised baseline. The reviewers acknowledge that the motivation for the work is important and that the experiments demonstrate the key contribution, that ICL is useful for low-resource settings even though a fully-supervised solution still outperforms ICL (which is to be expected). \n\nThe majority of the issues identified by the authors are suggestions for further research (e.g. considering more NLP tasks) or changes to the experimental setup that don't affect the conclusions of the work (e.g. using a stronger supervised baseline). The remaining issues seem to have been successfully addressed by the authors."
            }
        },
        "id": "pDveM54DE3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hpUNou0UaJ",
        "replyto": "hpUNou0UaJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2862/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551036,
        "cdate": 1696707551036,
        "tmdate": 1701465479200,
        "mdate": 1701465479200,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper brings the EDIS dataset that can evaluate the image retrieval task in news domain. This news domain challenges the model with more common sense knowledge and also fine-grained entity understanding. The reviewer raises the concern of the lack of novelty in proposing methods, which I partly agree with but the main novelty of the dataset paper will be mainly evaluated based on the data. Methodology would be considered more as a bonus.\n\nOverall I feel that this dataset can be helpful for the community given that the authors promise on releasing dataset."
            }
        },
        "id": "NNt4kJx1OA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hoO5anfnRk",
        "replyto": "hoO5anfnRk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1723/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707525282,
        "cdate": 1696707525282,
        "tmdate": 1701465439557,
        "mdate": 1701465439557,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a toxicity mitigation method that takes into account the changing nature of the text. The premise of the adaptable mitigation technique is commendable. The main contribution of the work is a reduction in inference time by 43% and minimizing computational requirements. The contribution is rather incremental but there are no critical issues that are unresolved after the rebuttal. I would suggest the authors to add the additional experiments reported in the final version of the paper."
            }
        },
        "id": "OqQQyvvym1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hn0B3jTlwE",
        "replyto": "hn0B3jTlwE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3613/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566212,
        "cdate": 1696707566212,
        "tmdate": 1701465503766,
        "mdate": 1701465503766,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "There is a consensus that this work is sound and exciting. The reviewers highlight that the method and paper are easy to understand and yield a straightforward inference improvement at negligible performance degradation. While some concerns have been raised regarding the evaluation in terms of ROUGE, the authors provide enough evidence of strong performance across other metrics."
            }
        },
        "id": "wOfW4UA7Hl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hmOwOZWzYE",
        "replyto": "hmOwOZWzYE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission294/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483956,
        "cdate": 1696707483956,
        "tmdate": 1701465393932,
        "mdate": 1701465393932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers were generally on the same page that the current scope of the work is somewhat narrow (on a synthetic task and on the naturalistic but synthetically-generated ProofWriter task), yet the overall findings about being able to mechanistically recover the implicit tree-structured reasoning from the model is valuable enough to be of interest. Unlike most explainability work that tries to obtain natural language multi-step explanations, the mechanistic interpretability angle of this work is novel in the multi-step reasoning space and provides useful insights.\n\nThe authors adequately addressed many of the concerns raised during the review period. The findings seem valuable to the community, even if currently somewhat narrow in scope. The reviewers have suggested specific other datasets (e.g., Musique) to push this line of work further."
            }
        },
        "id": "QIpWivTKuM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hlqIu07ics",
        "replyto": "hlqIu07ics",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4046/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581870,
        "cdate": 1696707581870,
        "tmdate": 1701465517909,
        "mdate": 1701465517909,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates the robustness of machine translation (MT) metrics in the face of adversarial attacks. The primary contributions are the exploration of adversarial attacks that do not change translation semantics and the evaluation of MT metrics' responses to these attacks, particularly focusing on BERTScore, BLEURT, and COMET.\n\nBased on the average scores, the paper is considered sound and has potential, but it is not without weaknesses. It falls short in terms of excitement, mainly due to perceived incremental novelty and the need for revisions.\nIn conclusion, while the paper addresses an important issue in the field of MT metrics, it should strive to provide stronger evidence for its claims, highlight its unique contributions, consider expanding the scope, and address the reviewers' concerns for a higher chance of acceptance."
            }
        },
        "id": "ySUtzwcWC8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hl6TVdQjeh",
        "replyto": "hl6TVdQjeh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3240/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558878,
        "cdate": 1696707558878,
        "tmdate": 1701465491483,
        "mdate": 1701465491483,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose a method to add time awareness to conversational models. The reviewers generally like the approach and the importance of the problem. However, the reviewers also point out that improvements using this method are only found in smaller models, that the data used is proprietary, and that the contribution may be too narrow. The authors respond to most of the reviewers concerns and I think this would be a good Findings paper."
            }
        },
        "id": "jACpsffLxD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hjEnagXGYV",
        "replyto": "hjEnagXGYV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5620/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616014,
        "cdate": 1696707616014,
        "tmdate": 1701465563983,
        "mdate": 1701465563983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a framework for AI-generated explanations to help improve content moderation quality, and they evaluate that framework with a large-scale crowdsourced user study to demonstrate that the framework helps moderators think more carefully during content moderation decisions. \n\nReviewers all agreed that this was a well-designed study and a well-written paper that makes a focused and interesting contribution to the NLP community and to work on content moderation and online safety, through the framework and the empirical results.\n\nReviewers wanted to see clearer explanations of the choice of the moderation framework (i.e., using a prescriptive framework, rather than a more descriptive approach to identifying bias) and the background of the participants in the study (i.e., that they were not themselves moderators). In addition, more discussion of the Tversky and Kahneman framework and more contextualization of what makes for difficult items to annotate would be useful for readers."
            }
        },
        "id": "1XWcvgOwks",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hiJ2hzwghq",
        "replyto": "hiJ2hzwghq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2087/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534162,
        "cdate": 1696707534162,
        "tmdate": 1701465453479,
        "mdate": 1701465453479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a thorough examination of the information loss within the language module of CLIP, aiming to understand its impact on language processing. To facilitate their investigation, the researchers introduce two novel datasets, namely CompPrompts and ControlledImCaps. The compelling analysis leads them to a finding: the text encoders in CLIP act as performance bottlenecks, potentially affecting the model's overall language capabilities.\n\n\nThis is a strong paper, in its observation, experiments and the benchmarks it provides."
            }
        },
        "id": "dl6jbg08eh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hgF8In32gL",
        "replyto": "hgF8In32gL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2005/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531944,
        "cdate": 1696707531944,
        "tmdate": 1701465450336,
        "mdate": 1701465450336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper explores the role of non-compositionality in sentiment within phrases, aiming to understand how sentiment analysis systems handle phrases whose sentiment doesn't align with expectations. Utilizing a sample of 500 phrases from the SST dataset, the authors conduct a rigorous experiment involving syntactic replacements and human annotations. The annotation process employs an online survey system, Qualtrics, and yields a good inter-annotator agreement score. The final dataset comprises 259 rated phrases, which are then analyzed for their non-compositionality ratings.\n\nThe findings reveal that although most phrases adhere to the principle of compositionality in terms of sentiment, exceptions exist. These are typically the result of figures of speech, common sense reasoning, and discourse relations. The paper not only offers valuable insights into the compositionality of sentiment labels but also provides a useful dataset for further research. Written clearly and thoughtfully, the paper presents a novel perspective for evaluating existing sentiment analysis methods.\n\nOverall, despite minor concerns of reviewers, I would still suggest the paper get accepted."
            }
        },
        "id": "YDTt8LeqWs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hfmmVWJecp",
        "replyto": "hfmmVWJecp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5845/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707619520,
        "cdate": 1696707619520,
        "tmdate": 1701465568372,
        "mdate": 1701465568372,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces Multi-level Prompt Tuning (MPrompt), a method for enhancing the fine-tuning of pre-trained language models (PLMs) for reading comprehension tasks. MPrompt leverages dynamically generated prompts, including domain-specific and context-specific prompts, to improve PLM performance. It demonstrates superior results compared to baseline methods in various MRC datasets and investigates zero-shot and few-shot adaptation scenarios.\n\nReviewers raised concerns about MPrompt's generalization ability, efficiency, and novelty. They questioned how MPrompt handles new domains, its efficiency compared to baselines, and whether it offers a genuinely novel contribution.  The authors conducted new experiments to showcase MPrompt's generalization ability in zero-shot and few-shot scenarios, compared its efficiency with baselines, and provided evidence supporting the multi-level prompt approach as a valuable contribution. They also acknowledged and addressed the need for further case studies and visualization in the paper, indicating a willingness to improve their work based on reviewer feedback.\n\n**Note:** The paper appears to be closely related to the works https://arxiv.org/pdf/2304.08467.pdf and https://arxiv.org/pdf/2212.10315.pdf, which focus on compressing prompts into soft-prompts. While the papers share similarities in utilizing soft prompts, it would be beneficial for the authors to reference these works and emphasize the distinctions in their approach, particularly in the context of reading comprehension tasks and the utilization of multi-level prompts."
            }
        },
        "id": "UEJY74MMXN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hfZKiBh4zS",
        "replyto": "hfZKiBh4zS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1803/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527549,
        "cdate": 1696707527549,
        "tmdate": 1701465442272,
        "mdate": 1701465442272,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a foundational dataset that combines machine document comprehension with human eye movement data for visually-rich documents. Most reviewers have expressed considerable enthusiasm for this novel dataset, recognizing its significance in enhancing the comprehension of visually-rich documents, a notion substantiated by the performance improvements showcased in the paper's experiments.\n\nHowever, the reviewers have raised pertinent issues that we encourage the authors to address:\n\n1. The experimental procedure is notably time-intensive and demanding in terms of labor. It would be beneficial for the authors to explicitly acknowledge this as a limitation of their paper. Moreover, providing insights and recommendations for future research and researchers embarking on similar endeavors could be a valuable addition.\n\n2. The issue of agreement among annotators warrants attention. While the paper elucidates the methodology employed to determine the final annotations, it is equally important to declare the level of agreement among the annotators. For instance, when instructing annotators to rate a sample on a scale of 1 to 5, it should be noted whether the average score of 3 results from all annotators assigning a score of 3 or if it arises from a split where half the annotators rate it as 5 and the other half as 1. Reporting such discrepancies in annotators' judgments is crucial.\n\n3. To address the queries posed by the reviewers, it is advisable to consider incorporating additional experiments into the paper, as suggested by reviewer hWn3. Expanding the experimental scope could help provide comprehensive answers to these questions and further bolster the paper's contributions."
            }
        },
        "id": "HrkQ3dzXZx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hdxMdgKddK",
        "replyto": "hdxMdgKddK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5782/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618419,
        "cdate": 1696707618419,
        "tmdate": 1701465567038,
        "mdate": 1701465567038,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Jailbreaking vulnerabilities of LLMs and prompt injections is a known issue. This work launches a systemic and global competition to exploit the prompt vulnerability. This paper details the setting of competition, and shows the key findings. The contribution of the study also includes a data set containing more than 600K adversarial prompts, taxonomy of different prompt attack, and strategy to develop adversarial prompts. Through 10 challenges on 3 LLMs (GPT-3, ChatGPT, FLAN-T5-XXL), the authors show that prompt hacking is possible, with 9/10 challenges solved and highly effective (54% success rate among submissions).\n\nPros:\n\nThe competition itself is well thought out and conducted and is extremely relevant and timely. \n\n\nThis is an excellent study on the adversarial vulnerability of LLM. As more LLMs are being deployed, this paper focuses on a real risk in the LLM era. The setting of the competition is a good balance between realistic attack and no increased harm.\n\nUnanimously all reviewers have acknowledged the importance and relevance of this paper\n\nSome specific doubts or comments were raised by the reviewers which have been addressed by the authors - the paper would benefit from including those points to clarify things better"
            }
        },
        "id": "A4IrlGHBqN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hcDE6sOEfu",
        "replyto": "hcDE6sOEfu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4754/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597828,
        "cdate": 1696707597828,
        "tmdate": 1701465540764,
        "mdate": 1701465540764,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes a selective prediction technique that discourages LLMs from predicting an answer when unsure. Towards this goal, an LLM is fine-tuned on task-specific data and multiple answers are sampled (some correct, some wrong) from the fine-tuned model. A classifier is trained to discriminate between correct and wrong answers, and the model is tuned to refrain from predicting wrong answers. The model incurs low inference costs and is also fairly sample-efficient during training.\n\nAll the reviewers have rated this work as good or higher on soundness. Concerns about whether the proposed technique is effective\nfor larger LLMs was addressed by the authors in their rebuttal. While inference costs are minimal and this is a desired feature in many use-cases, training efficiency does take a bit of a hit using the proposed technique with sampling multiple answers and task-specific finetuning of the model."
            }
        },
        "id": "1VXJMCRaQb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "haPIkA8aOk",
        "replyto": "haPIkA8aOk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission309/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484407,
        "cdate": 1696707484407,
        "tmdate": 1701465394449,
        "mdate": 1701465394449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The work proposes a modular MMT framework that is able to use both dense and sparse MoE modules, which brings together the benefits of both sparse and dense MMT models. The authors do various experiments in different settings to compare to as baselines and compare across various language pairs and resource settings. The authors and reviewers had good discussion and reviewers are in consensus for accept, in particular the authors added additional critical baselines. The authors do note differences to existing work such as NLLB in terms of amount of data, which is a reasonable point in terms of performance."
            }
        },
        "id": "WbOVv4wWQx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hXXyBtlo4D",
        "replyto": "hXXyBtlo4D",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2339/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539811,
        "cdate": 1696707539811,
        "tmdate": 1701465461783,
        "mdate": 1701465461783,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a method to use world knowledge from LLMs to improve active object detection and tracking in ego-centric videos.\n\nStrengths:\n- Paper is well written (MyHn)\n- Interesting idea (h4o5)\n- Results and analysis (MyHn, 5jwp)\n- Detailed limitations section (MyHn)\n- Generality of method (h4o5)\n\nThe concerns raised by the reviewers were mostly comments to provide constructive feedback, or mostly addressed during rebuttal (reviewers h4o5, 5jwp). No serious outstanding soundness concerns are identified and there is overall consensus on the soundness of the paper."
            }
        },
        "id": "gx2ds6MhNc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hWNsvpWfhy",
        "replyto": "hWNsvpWfhy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4977/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707603271,
        "cdate": 1696707603271,
        "tmdate": 1701465546268,
        "mdate": 1701465546268,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The experiments in the paper is sound. All reviewers agree the results support the claim about generalization in the paper.\n\nHowever, there are a few concerns. Reviewer uYzG raises a valid concern that the improvement is not as significant. Reviewer TJ6o and NTTU raises concern on the applicability and computational cost of the approach. Overall, the authors should revise the paper to discuss whether the marginal improvement is worth the extra effort.\n\nThe writing needs some more work. There are several bogus claims and statements.\n\nFor example, as pointed out by Reviewer NTTU, in line 187,\n\n> ... CHILD-TUNING_D only optimizes an unchanged sub-net during fine-tuning and ignores the update of other parameters, which may degrade the model’s performance on downstream tasks.\n\nThe sentence is too defensive that it's simple vacuous, but at the same time, the statement is likely blaming the wrong thing.\n\nAnother example is the problem stated in the Introduction:\n\n> (1) hysteresis in sub-net updating: the sub-net preference is estimated with the model parameters in previous iterations and may be incompatible with the current update step; and (2) insufficient utility of training data: FI estimation requires cumulative gradients through multiple mini-batches, so these methods cannot fit in situations with data scarcity.\n\nThe description is imprecise, and the paper does not validate these problems before going about solving them. Again, we do not know if the paper is blaming the right things.\n\nOverall, the paper has merit, but I would highly recommend the authors review and improve the wording in their revision."
            }
        },
        "id": "FlfIDpAi5q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hUWrmo7nNh",
        "replyto": "hUWrmo7nNh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3657/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567121,
        "cdate": 1696707567121,
        "tmdate": 1701465505652,
        "mdate": 1701465505652,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new task (and corpus) ClozEx which focuses on generating explanations for cloze question in language assessment with a focus on focus on English as a Second Language, ESL, learners. Various models have been fine-tuned to generate explanations for the questions. Extensive evaluations have been conducted in order to answer to two questions: (i) Do LLMs Explain Cloze Questions Well?, (ii) Are Automatic Metrics Reliable in ClozEx?.  The experiments are solid and the analysis interesting.\nSome extra information could be provided concerning the annotators and the bias management."
            }
        },
        "id": "3FDR630ABv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hTLIAYTi5w",
        "replyto": "hTLIAYTi5w",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2543/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544470,
        "cdate": 1696707544470,
        "tmdate": 1701465468514,
        "mdate": 1701465468514,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes rhetorical parallelism detection (RPD) as a novel NLP task in which the objective is to identify sets of textual spans in natural language that constitute a parallelism as defined by rhetoreticians. To this end, the authors formalize the task and several plausible evaluation metrics, offer a novel expert-annotated dataset of naturally-occurring parallelisms in Latin and Chinese texts, identify and suggest solutions to important technical challenges (e.g., which tagging scheme to use), and report the performance of a diverse array of baseline models, the results of which show both substantial evidence of learning but also much potential for improvement, suggesting that this is a tractable but challenging task.\n\nIn general, the reviewers found RPD and the paper to be very interesting, well-motivated and clearly presented. The scope of the paper is quite wide, including a novel task and formalization, a large expert-annotated resource, and extensive modeling.\n\nOne of the main concerns brought up is the low IAA scores. These scores raise doubts about the quality of the dataset, the construct validity for the task, and how much space there is for improvement on a baseline for this task. The authors respond echoing the concerns and elaborating the possible reasons behind this; however, they suggest that releasing the dataset to the community will provide the opportunity for refinement. \n\nAdditional questions or need for clarification brought up by reviewers were extensively addressed in the author response. I would expect any camera-ready version to incorporate these details and clarifications."
            }
        },
        "id": "Rhu1ltNjGH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hRJZIsC9VU",
        "replyto": "hRJZIsC9VU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4901/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601126,
        "cdate": 1696707601126,
        "tmdate": 1701465544343,
        "mdate": 1701465544343,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper makes an argument for eschewing probing tasks in favor of using behavioral evaluations (“indicator tasks”) without a trained probe component for understanding the linguistic structure of neural models, largely upon the rationale that linear probes show random chance after linear concept erasure, while behavioral evaluations demonstrate that the model still behaves as if it has some knowledge of the linguistic property of interest even after linear concept erasure. The overall claim, as I understand it from the paper and author discussion, is that the simplicity of behavioral evaluations is preferable over changing the probing methodology to add complexity through controls or sample efficiency arguments.\n\nThis paper warranted discussion among the reviewers and AC. I think that the methodological problems of understanding unsupervised learned structure are nuanced, and the reviewers seem to believe this work contributes to the discussion thereof. However, I agree with reviewer VEsr that considerable research has been done in this discussion that this work does not engage with properly. The issue seems to be in whether the experiments in this paper demonstrate an issue not already handled by other advancements in analysis methodology.\n\nFrom my own perspective, I’d ask the authors to consider discussing whether it really is the case that one of probing or indicator tasks must take precedence over the other; they seem just to be testing rather different hypotheses about the model – one about linear extractivity of interesting properties, the other about behavior, so the fact that they lead to differing results isn’t necessarily a huge methodological problem."
            }
        },
        "id": "svr0r2dJNw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hPr1QC623H",
        "replyto": "hPr1QC623H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2562/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544855,
        "cdate": 1696707544855,
        "tmdate": 1701465469122,
        "mdate": 1701465469122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work investigates the effect of compressions such as pruning or quantization on the parametric knowledge learned by different types and sizes of pre-trained language models. \n\nAs the reviewer mentioned, the paper presents a solid study on compression of LLMs by focusing on the preservation of knowledge which has been somehow overlooked in existing studies in the compression context. Furthermore, the findings are helpful for the researchers and practitioners for both further compression studies and the use of such methods. Finally, the presentation of the findings demonstrates a nice example and makes the paper easier to follow.  \n\nHowever, the reviewers also noted that the paper has limited contribution to the existing literature. An important part of the paper focuses attention and the feedforward modules. More fine-grained analysis such as layers and attention heads are suggested. Furthermore, the issues such as varying model size in different model families (note that as mentioned in LLM-int8 some LLMs larger than 6B face a stronger quantization challenge), and separate analyses for different attention modules remain to be investigated within the context of this paper."
            }
        },
        "id": "UsuJLvCM79",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hNSbSaD1WC",
        "replyto": "hNSbSaD1WC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2161/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535893,
        "cdate": 1696707535893,
        "tmdate": 1701465456251,
        "mdate": 1701465456251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All three reviewers provided positive assessments, consistently giving a score of 4 for both soundness and excitement. They consistently agreed that this paper is thorough, well-written, and of clear relevance to the field.\n\nThe strengths and weaknesses described by the reviews may be summarized as follows.\n\nStrengths:\n- valid and thorough experimental design (R1, R2, R3)\n- well-written (R2, R3)\n- clear and practical implications for the field (R1)\n\nWeaknesses:\n- only tested on one language (English) (R1) and limited exploration of the space of possible prompting strategies (R3)"
            }
        },
        "id": "Qec5NWy8nM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hMqRphmoM9",
        "replyto": "hMqRphmoM9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1108/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505158,
        "cdate": 1696707505158,
        "tmdate": 1701465420699,
        "mdate": 1701465420699,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces LLMs that are able to perform multiple text editing tasks, while beating strong baselines, using fewer parameters and being faster than existing models. The training approach is based on fine-tuing Flan-T5 models of different sizes using a large set of instructions.\n\nReviewers praised the extensiveness of the experiments presented, comparing their models with several benchmarks, which allow to trust the conclusions related to performance. Other strengths of the paper include an analysis of the performance of the models in adjancent tasks and in composite tasks, including human assessments on output samples.\n\nThe main concern raised by reviewers is related to the novelty of the proposed approach compared to existing work (e.g. PEER), and the fact that fine-tuning LLMs is a common technique in the area. The authors' rebuttal has clarified where the novelty of their work lays, which includes practical reasons (e.g. inference speed, model size, etc.). Authors are also encouraged to include clarifications regarding \"generalisability\" since \"adjacent tasks\" would be more suitable than \"out-of-domain\", as they have also acknowledged."
            }
        },
        "id": "DoAu2Va2Qx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hInB4JIQ5P",
        "replyto": "hInB4JIQ5P",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission381/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486134,
        "cdate": 1696707486134,
        "tmdate": 1701465396862,
        "mdate": 1701465396862,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper is a study on a multimodal out-of-distribution detection task, where it addresses the problem using a combination of text and image modalities with the help of a vision language model. With the help of large language models, we are able to imagine some of the image descriptions. These descriptions are able to provide more fine-grained information for the CLIP model to discern. The model also requires an object detection module to understand the objects inside the image. \n\nThe experimental results show that the proposed method can achieve better scores for OOD detection for multi-modal aligning. However, as reviewer qf8F pointed out: Utilizing a third-party model to create descriptors seems to dilute the advantages of the model. The proposed method is indeed a pipeline, consisting of several modules together. This can greatly increase the complexity of the problem. Such a drawback makes the method a bit limited in terms of novelty.\n\nBased on the current limitation, we would recommend maybe accepting to Findings."
            }
        },
        "id": "8VT2j6g4tF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hGUu750pcx",
        "replyto": "hGUu750pcx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission885/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498423,
        "cdate": 1696707498423,
        "tmdate": 1701465413680,
        "mdate": 1701465413680,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "For the task of knowledge graph prediction, this paper argues that a generative LM can be used to capture relevant graph neighborhood information. The paper presents a decent number of experimental results, with generally strong performance. Reviews were mixed, but overall general consensus on sufficient soundness. Reviews were contextualized wrt general PC guidelines about common pitfalls. While a number of typos and clarifications need to be addressed, the authors provided lengthy feedback, which suggests they will make these revisions. Even accounting for this being a short paper, I recommend the next revision, camera-ready or not, be restructured so that the main experimental take-aways are front-and-center."
            }
        },
        "id": "c27oIM8gHn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hEglNMGeqj",
        "replyto": "hEglNMGeqj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5392/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612109,
        "cdate": 1696707612109,
        "tmdate": 1701465558322,
        "mdate": 1701465558322,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work explores parameter-efficient fine-tuning (PEFT) in the context of active learning (AL). The authors show that PEFT methods outperform full fine-tuning in low-resource conditions, and the performance gain with PEFT methods extends to AL settings. \n\n1. As the reviewer mentioned, the authors present a large set of experiments that includes 4 PEFT methods, 5 AL sampling strategies, and 4 classification datasets. Experimental results are sound and support the authors' claims. \n\n2. Analysis of forgetting dynamics gives insights into why PEFT methods perform differently in AL scenarios.\n\nFurthermore, the authors provide clarification regarding their experimental setup. Overall, the benefits of PEFTs in AL is an important angle, and the paper addresses such benefits. I believe the authors will take reviewers' suggestions and points into account, such as providing the absolute performance metrics of PEFTs and FFT in AL settings similar to Table 1."
            }
        },
        "id": "Q4t5InfdKM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hEWgNQF1TM",
        "replyto": "hEWgNQF1TM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission531/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490028,
        "cdate": 1696707490028,
        "tmdate": 1701465402441,
        "mdate": 1701465402441,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a new evaluation method, DeltaScore, for evaluating nuanced aspects of story generation models. The authors propose using perturbation techniques to assess the quality of a story's specific aspect (e.g., fluency), suggesting that the susceptibility of a story to certain perturbations (e.g., the introduction of typos) correlates with the story's excellence in that aspect. The quality of an aspect is measured by calculating the likelihood difference between pre- and post-perturbation states using pre-trained language models (LMs).\n\nThe reviewers generally appreciated the comprehensive experiments and comparisons with baseline methodologies. They also acknowledged that the paper was well-written and presented, with effective use of figures and tables. The simplicity and effectiveness of the proposed method were also noted as strengths.\n\nHowever, some concerns were raised about the lack of intuition regarding the connection between perturbation-based evaluation and the model working mechanism or human evaluation. Reviewers suggested that a more thorough discussion about why this method is useful would be beneficial. The authors were also asked about the possibility of bias in the perturbation results from ChatGPT, and how it selected related words or stories for the Relatedness domain or revised commonsense output. One of the reviewers also noted that the quality of perturbation outputs was not validated.\n\nIn response to these concerns, the authors provided thorough rebuttals. They discussed their intuition behind the perturbation-based method and presented additional experimental results to support their claims. They also clarified the mechanisms of their perturbation methods and promised to include more details in their revisions. The authors acknowledged the need for rigorous validation of perturbation outputs and committed to performing error analyses in the future.\n\nOverall, the reviewers found the study to be generally strong and sound, providing sufficient support for its claims. They were excited about the paper and believed it could deepen the understanding of certain phenomena or lower the barriers to an existing research direction. However, they also acknowledged that there were areas for improvement and additional work needed, particularly in terms of validating the quality of perturbation outputs and clarifying the perturbation methods used."
            }
        },
        "id": "iJa2XR07o9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hDzfqmLrol",
        "replyto": "hDzfqmLrol",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission300/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484138,
        "cdate": 1696707484138,
        "tmdate": 1701465394186,
        "mdate": 1701465394186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Summary:**\nThis paper presents a new benchmark in the field of multimodal learning for multimodal classification, leveraging on game data. The data is collected from four vastly different video games with a total of eight classification tasks, each tasks involves information from three modalities: tabular, text and visual data. \nThe authors conduct benchmarks involving different unimodal and multimodal models, including several well-known ones. Additionally, the authors propose a new multimodal classification approach, called MugNet, which models sample similarity in multimodal data using graph neural networks (GNNs).\nThe authors execute an exhaustive evaluation to compare the proposed approach against competitive baselines. The results demonstrate the effectiveness of their approach. This work might facilitate research on multimodal learning.\n\n**Strengths:**\nThe reviewers acknowledge the following strengths in this contribution:\n1. The paper introduces a valuable new dataset for addressing multimodal classification challenges, with the authors committed to making the dataset publicly available.\n2. The rarity of a multimodal classification dataset derived from the domain of video games enhances its significance within the research community.\n3. The authors propose an innovative multimodal learning approach that leverages within-modality graph neural networks to model sample similarity and subsequently aggregates modality-specific representations.\n4. Comprehensive data and feature analyses are included in the paper, illustrating the indispensability of all modalities for solving the tasks.\n5. The authors conduct benchmarking on the dataset using various well-established unimodal and multimodal models, highlighting how the dataset and benchmark can facilitate comprehensive evaluations of multimodal classification models, encompassing performance and performance-speed tradeoffs.\n6. The experimental results offer valuable insights for guiding future research in this domain.\n\n**Weaknesses:**\nReviewers share certain concerns with this paper. The dataset lacks clear motivation, and critical details necessary for comprehending the datasets and tasks are absent. The proposed approach is solely evaluated on the new dataset, and its effectiveness against existing tasks and baseline models considered in the paper remains unclear. It might be more persuasive to showcase the improvement in benchmarks for real-world tasks achieved by employing a model trained on this dataset. Additionally, some of the tasks in the experiments can already be solved nearly perfectly by the benchmarked models\n\n**Author-Reviewer discussion and acknowledgment:**\nThe authors provided clarifications addressing the concerns raised by the reviewers and have delineated the improvements to be made, during the rebuttal response and discussion phase. All reviewers have responded and acknowledged the authors' arguments.\n\n**Conclusion:**\nThe paper is well-structured, but reviewers suggest that the authors rectify the identified typos. Furthermore, reviewers recommend that the authors include additional references."
            }
        },
        "id": "DrFqzwzWH8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hCsppacsqS",
        "replyto": "hCsppacsqS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission430/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487495,
        "cdate": 1696707487495,
        "tmdate": 1701465398631,
        "mdate": 1701465398631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "There was a consensus among the reviewers that this position paper outlines an important problem (models training on publicly available evaluation sets) and proposes a viable mitigation strategy. It would be a clear benefit to the EMNLP conference program and is a clear accept."
            }
        },
        "id": "Rjk1g8gq9a",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "hA8h2KtSv2",
        "replyto": "hA8h2KtSv2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1021/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502720,
        "cdate": 1696707502720,
        "tmdate": 1701465418014,
        "mdate": 1701465418014,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a conditional computational graph based on the importance of the tokens. Only a pre-set number of important tokens would be sent to heavy modules and thus the inference would be much faster. \n\nGood parts:\n\n1. The idea of using a light route and a heavy route with a dynamic routing mechanism is novel and interesting.\n\n2. Experiments are solid to justify the claim.\n\n3. The introduction motivates well.\n\nFew potential issues:\n\n1. Most experiments are performed on TPUs which limits its accessibility to mostly Googlers.\n\n2. Description of method is mixed with many FLOP analysis, which is hard to digest. It's not very easy to understand what's the main proposed components and leave the FLOP analysis a separate session."
            }
        },
        "id": "QoB8VTjePG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "h96N32OkAx",
        "replyto": "h96N32OkAx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission216/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482058,
        "cdate": 1696707482058,
        "tmdate": 1701465391466,
        "mdate": 1701465391466,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a method for improving soft labels by aligning annotator confidence levels using a Bayesian approach. It also presents a novel dataset for soft label research in NLP.\n\n*Reasons To Accept:*\n1. Challenges conventional practice by utilizing uncertain annotations for soft labels, maximizing data utilization.\n2. Addresses a recognized limitation in data annotation for single-label classification.\n3. Innovative dataset creation supports future research.\n\n*Reasons To Reject:*\n1. Lack of dataset source accessibility raises concerns about evidence and validation.\n2. Methodology could benefit from simplification for better clarity.\n\n**Summary:**\n\nThe paper introduces an innovative method for generating soft labels, addressing a significant issue in the field of data annotation for single-label classification tasks. It challenges conventional practices and provides valuable contributions. However, concerns regarding dataset accessibility, method complexity, and comparison against more recent baselines need attention. The reviewers' confidence levels vary, indicating some uncertainty, but overall, the paper shows promise and potential for acceptance with revisions."
            }
        },
        "id": "fyJhSt1FG4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "h5gum6ximf",
        "replyto": "h5gum6ximf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3851/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707577789,
        "cdate": 1696707577789,
        "tmdate": 1701465511720,
        "mdate": 1701465511720,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents DiSTRICT, a method to improve DST performance through in-context tuning with relevant retrieved training examples. This is the first work to apply in-context tuning method to the dialogue state tracking task. The selection of baselines are appropriate and the conducted experiments are generally sufficient to demonstrate the effectiveness of the method, while the domain generalizability can be further verified through more detailed analysis, which the authors addressed in the rebuttal."
            }
        },
        "id": "Xwj6brtOJJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "h4NNcIZUHT",
        "replyto": "h4NNcIZUHT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission258/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483162,
        "cdate": 1696707483162,
        "tmdate": 1701465393006,
        "mdate": 1701465393006,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper compiles a large corpus of over 1,900 tales from 27 diverse cultures across six continents, and conducts language and statistical analyses to study how human values, morals, and gender biases are expressed in folk tales across cultures. Three reviewers reviewed the submission, highlighted the strengths of the paper in terms of novelty (R1, R3), data contributions (R1, R2), presentation and readability (R2, R3), and findings (R1, R2, R3). During the rebuttal phase, the authors also responded that they would address the reviewers’ concerns through engagement with prior literature and additional analyses."
            }
        },
        "id": "9W2ezZO9Fv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "h1nUUpmvpf",
        "replyto": "h1nUUpmvpf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5838/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707619194,
        "cdate": 1696707619194,
        "tmdate": 1701465568091,
        "mdate": 1701465568091,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a method for annotating (and improving) semantic parsing with labels from non-programmers, using text-to-SQL. They enable these non-expert annotations by having annotators judge program execution, rather than programs directly. They introduce the APEL framework, which searches for synthetic inputs that lead to different outputs on candidate programs, and then elicit correctness judgments that can be used to fine-tune parsers. They find in a case study that their method can reach the same annotation accuracy as annotations using experts.\n\nThe reviewers are in agreement that the paper is well-written, with an interesting, intuitive, and novel framework and methodology. The reviewers also express that the paper features clear presentation of implementation details, extensive analyses, and sensible validation of the approach. Some questions and concerns were raised, but were largely addressed in discussion. Overall I judge that this is a strong submission and a worthy contribution to the conference."
            }
        },
        "id": "zG3u5RiGLZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "h1YhUpPKEq",
        "replyto": "h1YhUpPKEq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4583/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593534,
        "cdate": 1696707593534,
        "tmdate": 1701465535690,
        "mdate": 1701465535690,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposed a semantic parser to parse natural language to first-order logic for commonsense problems. Applying this semantic parser to smaller models resolved much better results compared to LLMs (with or without CoT).\n\n**Pros**: The paper is very well-written and easy to follow. It also provided in-depth analyses of their methods and LLM performance. The performance improvement from smaller models is also important compared to LLMs. All the reviewers appreciate these aspects of the paper.\n\n**Cons**: It's unclear how to apply this method to more \"natural language\" tasks, as mentioned by 1NfW. Moreover, adding more discussion in the related work section about the comparison to Toolformer and other work in the mathematical domain of semantic parsers is going to be beneficial for the readers (HxCp, iSNf).\n\nOverall, it's a very well-written, solid, and sound paper. The direction of combining neural models and traditional solvers is exciting to solve certain problems in commonsense domains."
            }
        },
        "id": "5bM0k7THAV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "h00GHjWDEp",
        "replyto": "h00GHjWDEp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5260/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609957,
        "cdate": 1696707609957,
        "tmdate": 1701465554388,
        "mdate": 1701465554388,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a non-autoregressive streaming transformer for simultaneous translation, which addresses two current problems: \"non-monotonicity problem\" and \"source-info leakage\" .  Alignment are specially considered for setting the latency loss and translation loss, which is important in the simultaneous translation setting. The results show substantial improvement over existing research. Reviewers suggest more analysis into the model behaviors."
            }
        },
        "id": "kANlzHZPYp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gzRBs4gIbz",
        "replyto": "gzRBs4gIbz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission677/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493532,
        "cdate": 1696707493532,
        "tmdate": 1701465407329,
        "mdate": 1701465407329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a black-box prompt tuning method for CLIP finetuning. The authors have added additional results and have done a nice job during the rebuttal. After rebuttal, two reviewers are happy about the paper, while one reviewer still shows concerns about accepting the paper.\n\nOne one hand, reviewers agree that the problem studied in this paper is interesting, and results are comprehensive. During rebuttal, additional results are added. There are a large attention recently towards black-box settings in language and vision-language domains and the proposed paper outperforms previous performance on black-box CLIP image classification setting, thus, reviewers recommend to accept the paper. \n\nOn the other hand, some reviewers think that the newly added experiments in the rebuttal derive different conclusions from the experiments in the main paper, i.e., whether a longer prompt works. Besides, a more thorough analysis of the upper bound is needed. Therefore, rejection is recommended. Also, some reviewer commented that the black-box tuning setting is kind of weird, where learnable prompts can be injected, and the feature of the model can be obtained."
            }
        },
        "id": "Sb6N0rXn5K",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gybvlVXT6z",
        "replyto": "gybvlVXT6z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1844/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528644,
        "cdate": 1696707528644,
        "tmdate": 1701465444269,
        "mdate": 1701465444269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main contribution of this work is creating a taxonomy of methods to estimate the \"transferability\" of Pretrained Language Models (PLMs) based on whether they require certain inputs, are task-agnostic, take into consideration the dynamics of the features during fine-tuning, and/or are training-free. However, the properties of the surveyed methods are not always adequately expounded. Moreover, the Authors provide a comparison of their performance on tasks from GLUE. However, the selection of PLMs is limited to variants of BERT and RoBERTa encoders; hence, it is hard to determine if the findings generalise to auto-regressive and encoder-decoder PLMs. Finally, despite computer vision being prominent in the abstract and introduction, no studies have been carried out in this domain. Hence, there is a mismatch between the motivation and the experimental setup. Overall, this paper might be valuable as a summary of previous work on transferability, but its novelty is somewhat limited, as it mostly compares existing methods in a standard NLP benchmark. This submission might be considered for acceptance to Findings."
            }
        },
        "id": "NtfIDZWCsZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gslZifaE3t",
        "replyto": "gslZifaE3t",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1680/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707523398,
        "cdate": 1696707523398,
        "tmdate": 1701465438298,
        "mdate": 1701465438298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper trains a monolingual MLM (XLM-R architecture) from scratch specifically for Vietnamese social media text. The data was scraped from popular Vietnamese social media sites and results on downstream tasks show that it outperforms other available models. \n\nThe reviewers agree that the paper, data and models can be impactful for Vietnamese social media language understanding. The authors have provided some missing experimental results during the response period while other concerns regarding further training existing models on the newly collected data have been punted to future work."
            }
        },
        "id": "EHeMStjYIV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gqkg54QNDY",
        "replyto": "gqkg54QNDY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission518/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489701,
        "cdate": 1696707489701,
        "tmdate": 1701465401929,
        "mdate": 1701465401929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Summary:**\nThis paper introduces a new dataset called the Linguistic Concept Learning benchmark (abbreviated as Licon) designed to investigate concept learning capabilities. The Licon dataset comprises three components: Licon-A, which includes basic concepts for concept learning and novel concepts for evaluation; Licon-I, similar to Licon-A but with images; and Licon-T, collected from real-world questions to assess concept learning and reasoning. The diverse information types are intended to facilitate the learning of abstract concepts that cannot be solely represented through visual information. The primary objectives of this benchmark dataset are concept classification, attribute prediction, and concept relationship recognition. Additionally, the paper proposes an entailment-based concept learning method (EnC) that leverages box embeddings and graph neural networks to model relationships between concepts, demonstrating its effectiveness on the Licon dataset.\n\n**Strengths:**\nThe reviewers unanimously acknowledge the strengths of this work. This paper introduces the Licon dataset, a novel concept learning dataset that stands out for its diversity, controllability, and challenge. The motivation behind incorporating both the visual and linguistic worlds into concept learning is well-founded. Language, unlike visual-only representations, can describe complex relationships between abstract concepts. The proposed concept learning method captures relationships through entailment and contradiction quantification. Notably, the evaluation largely consists of zero-shot tasks, involving multiple-choice questions related to novel concepts not included in the training set. Finally, the tasks defined in this paper have the potential to serve as instructions for fine-tuning or evaluating Large Language Models (LLMs), enabling LLMs to mimic human learning processes and potentially enhancing their generalization capabilities.\n\n**Weaknesses:**\nThe Licon dataset is overly focused on specific types of concept learning and lacks scalable expansion options. Additionally, specific sections of the dataset, such as Licon-A, which pertains to hierarchical concepts, do not distinctly exhibit hierarchical characteristics, and the description of the Licon-T dataset is somewhat limited. One of the reviewers expressed confusion regarding the usage of the Licon-T dataset, as it does not appear to differentiate between basic and novel concepts. Regarding the Licon-I dataset, it may be considered somewhat redundant, as the primary theme of this paper revolves around learning concepts from linguistic cues. Lastly, the claimed difficulty level of the dataset is not readily discernible, as none of the experiments effectively demonstrate it.\n\n**Author-Reviewer discussion and acknowledgment:**\nReviewers raised various questions and concerns, to which the authors provided responses during the rebuttal and discussion phase. They outlined improvements and engaged in further discussions, particularly with one of the reviewers. Two of the reviewers have responded and acknowledged the authors' arguments.\n\n**Conclusion:**\nThe paper is well-written; reviewers suggest that the authors rectify just a few typos."
            }
        },
        "id": "ye09RiQuFt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "goH9e5Vd44",
        "replyto": "goH9e5Vd44",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1431/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707513379,
        "cdate": 1696707513379,
        "tmdate": 1701465430765,
        "mdate": 1701465430765,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an approach for \"interrogating\" models by way of a conversational dialogue system.  A backend system supports several interpretability \"operations\", for things like running attribution methods, finding counterfactual explanations for an example, constructing adversarial inputs, rationalization, and more.  The implementation largely follows the ideas in TalkToModel, though the supported operations in the dialogue systems are different, and in some cases specialized to NLP.  It presents a fine-tuned model for interpreting user queries, essentially an intents-and-slots-style semantic parsing task to identify which interpretability operation to call. Performance of the parser is evaluated.\n\nThe paper then conducts end-to-end evaluation on helpfulness and simulatability.  Three datasets are used in its case study: DailyDialog for dialogue act classification, BoolQ (QA), and OLID (hate speech detection). Results are presented on parsing user intents, subjective ratings of system helpfulness, and simulatability, focused on evaluating different interpretation operations.\n\nThe reviewers found this paper interesting and well-written, with a comprehensive set of experiments. The idea of democratizing interpretability methods through techniques like this is very intriguing! However, there are a few significant issues with the work.\n\nVaJL brings up another crucial question: \"What is the intended use case for InterroLang?\" The paper claims that the system is \"user-centered\". But there is very little discussion of why a dialogue system is needed or whether this is actually necessary or sufficient for the needs of its intended users. Given the underlying discrete set of operations that are supported, why not simply have a drop-down menu? I can see the argument that the dialogue system makes it more friendly to use, but does this actually have benefits? That is, are users without knowledge of the underlying operations able to use it \"zero-shot\" or do they need extensive documentation (e.g., a description of what operations are supported)? The trace in the appendix shows a fairly smooth interaction, but questions like \"can you do an adversarial attack\" or \"what would be the counterfactual for this instance\" do not seem like something a user would come up with on their own unless they knew what the model could do.\n\nRelated to this, another crucial issue is the lack of comparison to other systems.  The only real baselines are presented in Table 2 for the semantic parsing task (the nearest neighbors model, and weaker pre-trained models). Notably, the user study does not do A/B testing of the present system versus another possible system.  To justify the use of a dialogue system, I would expect to see a study where users are presented with an ablation of the system or some other kind of dataset visualizer to understand (a) are they able to do the tasks as effectively?  (b) Do they prefer the dialogue system to, say, a drop-down menu?\n\nThese two points (use case, baselines) were discussed privately and reviewers were left with lingering uncertainty about work, which is reflected in the revised ratings.\n\nFurthermore, 4uXs points out that many of the user study participants are authors themselves. I believe this is also a weakness of the user study; although the tool may be intended for relative experts in machine learning, this is still not a representative sample."
            }
        },
        "id": "qLvsyYhljz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gmVEVn0Qi5",
        "replyto": "gmVEVn0Qi5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3732/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707570612,
        "cdate": 1696707570612,
        "tmdate": 1701465508037,
        "mdate": 1701465508037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper is well-written and proposes an important research question. The experiment results show that their method is able to solve this problem."
            }
        },
        "id": "1F2Fzgayyn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "glxrubmH91",
        "replyto": "glxrubmH91",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3306/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560139,
        "cdate": 1696707560139,
        "tmdate": 1701465493508,
        "mdate": 1701465493508,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new language model (GeoLM) that is pre-trained using masked language modelling and a contrastive objective on descriptions of geospatial locations and information with the goal of enhancing geospatial abilities of the model. This is evaluated using a suite of downstream tasks including toponym recognition, toponym disambiguation, geospatial relation extraction and geo-entity tagging and shows improvements over baselines and existing methods.\n\nAll reviewers agree on multiple aspects, including that the paper is well written and motivated, that it addresses a relevant problem and highlight that the results are good in comparison with relevant baselines. Evaluating on four downstream tasks gives robustness to the results.\n\nThere were some questions about some details of the work (e.g. trie based matching, evaluation setup) that were addressed well in the author response. Ablation experiments were also mentioned by two of the reviewers, and the response provided results on this aspect as well.  This leads to all reviewers agreeing that the paper methodology and results are sound.\n\nOverall, the authors should consider including the details provided in the discussion in their camera ready version of the work. Beyond this, there are few pending comments from the reviewers."
            }
        },
        "id": "2lxBwuLwF5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gkQo3CoPLd",
        "replyto": "gkQo3CoPLd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4724/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597187,
        "cdate": 1696707597187,
        "tmdate": 1701465539713,
        "mdate": 1701465539713,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "I provide here an analysis of three reviews for the paper. The reviews present both positive and negative aspects of the paper, leading to a comprehensive evaluation of its quality.\n\nReasons to Accept:\n\nImportance of the Research Problem: The paper addresses an important issue in the field of large language models (LLMs) by investigating their susceptibility to hallucinations when confronted with invalid questions. This problem is significant as it highlights the need for improving the reliability of LLMs.\n\nDiverse LLM Evaluation: The paper tests a wide range of LLMs, which strengthens its claims and provides a comprehensive evaluation of the problem.\n\nReasonable Data Generation Approach: The use of templates and DBpedia predicates to generate invalid questions is considered a reasonable approach, providing a basis for creating a valuable dataset for future research.\nClarity and Accessibility: Reviewers appreciate the paper's clear and understandable presentation, making it accessible to a broad audience.\n\nReproducibility: The paper's methodology is deemed reproducible, enhancing the credibility of the research.\n\nReasons to Reject:\n\nLack of Dataset Release Clarity: Reviewers express concerns about the unclear status of dataset release. While the dataset can potentially be generated with the provided code, clarity on dataset availability would facilitate quicker experimentation.\n\nPoor Correlation Between Automatic and Human Evaluation: The paper reports a poor correlation between automatic and human evaluation, which raises questions about the utility of the dataset. Suggestions are made to explore better automatic metrics for evaluation.\n\nLimited Question Generation Diversity: The simplicity of the question generation approach, where entities are filled into fixed templates, and the use of only one template per question category, are seen as limitations that may introduce bias into the evaluation.\n\nExclusion of Larger LLMs: The paper focuses on smaller LLMs for evaluation and does not include experiments on larger models like GPT-3.5 or GPT-4, which could provide more insights into the problem. Additional Metrics for Hallucination Evaluation: Reviewers suggest incorporating specific metrics tailored for hallucination evaluation to provide more valuable insights into LLM performance."
            }
        },
        "id": "dsH0PTxGsh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gjrs5oF8TC",
        "replyto": "gjrs5oF8TC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4970/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602982,
        "cdate": 1696707602982,
        "tmdate": 1701465546073,
        "mdate": 1701465546073,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a novel method known as Cross-Modal Conceptualization in Bottleneck Models (XCBs), aimed at enhancing traditional Concept Bottleneck Models (CBMs). XCBs leverage textual descriptions accompanying images to automate the concept induction process, eliminating the need for manual annotation. The authors present a mechanism for aligning concepts between visual and textual data.\n\nIn general, all reviewers agree that this work is well-motivated and sound. The reviewers concur that it holds significant implications for the field of interpretable machine learning. Therefore, the final decision for the paper is acceptance."
            }
        },
        "id": "1Z0u8Eq2dk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ghF1EB6APx",
        "replyto": "ghF1EB6APx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3959/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580083,
        "cdate": 1696707580083,
        "tmdate": 1701465515249,
        "mdate": 1701465515249,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a survey of recent work in multi-modal fact checking and a framework for categorizing the sub-tasks and existing datasets. All reviewers agreed that a survey of this kind would be useful for the community. The paper covers an extensive set of previous work and the reviewers mostly agree that the coverage is mostly comprehensive.\n\nSeveral of the objections come from disagreements about the importance of evaluating the performance of existing models on the benchmark datasets. However, the authors argue in their rebuttal that this is in line with previous surveys that have been published in the field. \n\nA second objection raised by the reviewers is in the structure and organization of the paper, and more specifically, the links between specific benchmark datasets and the sub-tasks they are useful for. However, these objections are not well fleshed out in the reviewer comments. The authors have proposed a reorganization of the paper to improve its structure, as well as an extensive update to Table 1 to include more information about the rationale and process behind each dataset."
            }
        },
        "id": "oUtDSnJF7P",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ggTNeg2fem",
        "replyto": "ggTNeg2fem",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1708/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707524522,
        "cdate": 1696707524522,
        "tmdate": 1701465439067,
        "mdate": 1701465439067,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents an empirical study of parameter-efficient fine-tuning (PEFT) for large language models (LLMs). The authors address three representative open-source LLMs, exploring various adapters like Prefix-Tuning, Series Adapter, LoRA, and Parallel adapter. They delve into the optimum placement of these adapters, focusing on Attention-only, MLP-only, or both, while also evaluating them concerning hyperparameters like LoRA rank and bottleneck size.\n\nAll reviewers felt that the technical implementation of the paper was solid and extensive.\n\nSeveral additional points of concern were raised by reviewers and adequately addressed by the authors:\nGeneralization Concerns: The paper's findings were based on two tasks (math and common sense reasoning), raising concerns about the generalization of the results to other tasks like question-answering, summarization, and open-ended generation. During the rebuttal, the authors responded with additional results for the SciQ (scientific understanding) dataset.\n\nNeeds Additional Analysis on Adapter Placement: Some reviewers wanted more experiments showing the impact of how adapter placement impacted performed in terms of MLP-only, Attention-only, and both on additional models/datasets/tasks. During the rebuttal, the authors provided additional results in this direction.\n\nFinally, one point the authors may consider is providing clarity regarding the distinctive features and utility of the LLM-Adapter framework in comparison to existing solutions. Essentially, does this work need to include the framework as a main contribution or can it simply be a solid survey/benchmark paper of existing PEFT methods applied in many different configurations and settings."
            }
        },
        "id": "qtk96gAAux",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gdUBK65fwn",
        "replyto": "gdUBK65fwn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1420/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512792,
        "cdate": 1696707512792,
        "tmdate": 1701465430393,
        "mdate": 1701465430393,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces PROTEGE, a novel technique for question generation over web articles. The basic concept is to condition generation on a prompt, either keyword-based (e.g. an entity in the article) or phrase-based (e.g. a sentence in the article). Cross-attention to this prompt is then used during generation, and different prompts can be selected to induce diversity. The authors combine this with a hill-climbing submodule that allows control over the tradeoff between diversity and fidelity. \n\nThe experiments in the paper test the model using common QA datasets. The authors demonstrate that both fidelity and – especially – question diversity benefits from their approach. That is, given an article, the questions generated by PROTEGE are significantly better at covering the aspects of the article compared to prior work. The reliability of these findings are supplemented with human experiments, providing additional backing.\n\nThe paper is well-written and clearly presented, and the empirical findings are a convincing demonstration of PROTEGE's efficacy.|This paper presents a method called PROTEGE that does a two-step process for question generation from text sources.  First, a set of diverse questions is generated, using a pair of encoders one of the document and the other for the \"prompt signals\".  Next, there is a submodular objective function defined by a pair of quantities meant to control the trade-off between diversity and fidelity, and this optimized and hill-climbed when arriving at the final set of questions.\n\nThe authors could have done a slightly better job motivating this work.  One of the reviewers noted the lack of practical application.  However, this work not only pertains to the claimed area of constructing QA datasets, but also to the related—and very practical—areas of building better QA systems, and/or evaluating them.  This came out, albeit in a slightly oblique way, in the rebuttal to reviewer ypQJ.  Nevertheless, I believe this paper warrants inclusion in the conference."
            }
        },
        "id": "xqFCBV3o8l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gd8TxhKoLv",
        "replyto": "gd8TxhKoLv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3371/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561348,
        "cdate": 1696707561348,
        "tmdate": 1701465495250,
        "mdate": 1701465495250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Review Summary:\nReviewers agree that the paper advances multilingual medical text simplification, creates a new dataset, and evaluates various pre-trained language models in zero-shot and fine-tuned settings are positive contributions. Some reasons to reject this paper are stated including not using language models specifically trained for the medical-domain, poor justification of approach, poor human evaluation quality, degradation of performance on certain languages, and lack of detailed information regarding experiments. In addition, since this paper presents a generative approach for the medical domain, it raises additional ethics consideration including the intellectual property regulations, annotators compensation, etc\n\nReasons to Accept:\n(1) The paper makes significant contributions to multilingual medical text simplification, introducing the MULTICOCHRANE dataset for four Indo-European languages.\n(2) It presents a novel approach to creating the dataset, including MC-CLEAN and MC-NOISY subsets, which can be valuable resources for further research.\n(3) The systematic evaluation of simplification models in both zero-shot and fine-tuned settings across multiple languages provides insights into model performance and challenges.\n(4) The inclusion of human assessments alongside automatic evaluations, and the analysis of their correlation, enhances the understanding of model performance.\n(5) The paper identifies differences in model performance when simplifying to different languages, shedding light on potential quality degradation.\n(6) The release of MULTICOCHRANE, model outputs, and human judgments promotes future research in multilingual medical text simplification.\n\nReasons to Reject:\n(1) While there are no strong reasons to reject the paper, some suggestions for improvement include addressing the limitation of uneven language distribution in the dataset, exploring the use of the simplify-and-translate approach for comparison, and considering more context-aware approaches instead of sentence-level simplification. Additionally, providing more experimental details and experiments with domain-specific language models could enhance the paper's quality.\n\nThe author rebuttal have tried to address these issues."
            }
        },
        "id": "rJoxcWqP3y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gccSE5vDZ7",
        "replyto": "gccSE5vDZ7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2153/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535667,
        "cdate": 1696707535667,
        "tmdate": 1701465455919,
        "mdate": 1701465455919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper advances the state of the art in argument mining by showing that a pipelined approach, with ensembling, can outperform current end-to-end systems.\n\nReviewers were mostly ambivalent about this paper in terms of excitement, largely because the main contribution seems to be experimental results showing the advantages of a less-commonly used approach to this task. Reviewers also noted that improvements from using ensembles was not surprising or innovative, and that were results were only provided for a single dataset. However, they reported that the code to reproduce these experiments would be useful, and that these results would inform future work.\n\nIn terms of soundness, some initial concerns were raised, but reviewers seem largely satisfied with the authors' response, including an update that would correct for multiple comparisons. The one exception to this is Reviewer 1CYs's concerns about computational costs, but this seems like a somewhat secondary concern."
            }
        },
        "id": "e1HXR5Qg86",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gZykO63OUh",
        "replyto": "gZykO63OUh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission748/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495106,
        "cdate": 1696707495106,
        "tmdate": 1701465409211,
        "mdate": 1701465409211,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This study introduces a multilingual legal document summarization dataset, comprising 3,122 judgment-summary pairs in both English and Hindi. In conjunction with the dataset, the authors presented benchmark results using various models.\n\nAll reviewers recognized the value of the dataset for this novel domain. Reviewer egHn raised several concerns, many of which have been addressed. However, the ethics statement remains unclear. To enhance clarity, the authors should elaborate (i) the implications of the models’ output, and (ii) the consent required for such data collection (as also mentioned by Reviewer eZYq). When releasing the dataset, it's imperative to ensure license terms accompany the released package. Concerning Q2 from Reviewer eZYq, providing a quantitative value would be more informative for readers. \n\nAddressing Reviewer XjyQ's point on \"some quantitative measures\" is crucial to verify the dataset's quality. Although this concern was responded to in the rebuttal, such details should be incorporated into the paper."
            }
        },
        "id": "sOXVVpePBS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gZhvtIRu7i",
        "replyto": "gZhvtIRu7i",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1970/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531125,
        "cdate": 1696707531125,
        "tmdate": 1701465448792,
        "mdate": 1701465448792,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a rewrite-retrieve-read approach as opposed to a retrieve-read approach in order to refine the retrieval query to be tailored toward the needed knowledge that should be retrieved from the knowledge source. The rewriter model is learned using reinforcement learning where the reward is based on the end prediction from the language model. Evaluation on three open-domain QA datasets and 1 multi-choice dataset reveals the effectiveness of the method.\n\nThe reviewers acknowledged that the contribution is in proposal of a new method that is shown to be effective (ksRg, GP32, rmM6), experiments and analysis are thorough (GP32, rmM6), and the paper is well-written (GP32).\n\nReviewers raised concerns on heavy reliance on close-sourced models such as web API (GP32, JBWk) and qualitative analysis (rewriting seems minor based on examples) (GP32). Also, reviewers pointed out lack of details reported in the paper (ksRg) and lack of comparison to prior work (rmM6, JBWk). These concerns are sufficiently addressed by authors during the discussion period. Authors are encouraged to add details and discussion in the final version of the paper if accepted."
            }
        },
        "id": "Z6yHb7iFOy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gXq1cwkUZc",
        "replyto": "gXq1cwkUZc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2590/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545422,
        "cdate": 1696707545422,
        "tmdate": 1701465470080,
        "mdate": 1701465470080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a data augmentation approach using pre-trained LLMs, where they generate borderline (challenging) examples using a model GPT3, then prompting the model again to re-label the examples for correcting potential labeling errors. They show experimentally that augmenting this kind of data leads to improved student models, including in challenging few and zero-shot settings. The paper is well written and organized, with extensive analyses and ablation studies supporting the claims of the paper, and demonstrating the validity of the proposed data augmentation approach across various classification tasks."
            }
        },
        "id": "4V245WWffR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gWWjz9NBo9",
        "replyto": "gWWjz9NBo9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3410/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562102,
        "cdate": 1696707562102,
        "tmdate": 1701465496551,
        "mdate": 1701465496551,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents an investigation into the use of large language models (LLMs) as evaluators for figure captions, specifically scientific ones. Using the SCICAP-EVAL dataset, a human evaluation dataset that contains human judgments for 3,600 scientific figure captions, both original and machine-made, for 600 arXiv figures, the authors find that GPT-4, used as a zero-shot evaluator, outperforms all other models and even surpasses assessments made by computer science undergraduates. \n\nThe reviewers have shared diverse views on the paper. Overall, the reviewers acknowledge the paper's merits in investigating an interesting problem, providing a new dataset (SCICAP-EVAL), and demonstrating the effectiveness of GPT-4 as a caption evaluator. The use of GPT-4 as an evaluator, especially in the zero-shot setting, is noteworthy. However, the reviewers also express concerns: One primary concern is the lack of novel techniques in the paper, and the overall approach is seen as more of an experimental report.  There's also concern about the methodology discussed in the paper on figure captioning, which could potentially be misleading as the model doesn't utilize visual input (i.e., the figure) to perform this task. Comparison between the GPT-4 scores and undergraduate students' evaluations is another area of concern due to differing scoring scales and a lack of detail about the experimental design and the rationale behind certain choices. The authors are urged to provide more clarity on these aspects.\n\nIn conclusion, while the paper provides valuable insights into the use of LLMs as evaluators for figure captions, there are points raised by the reviewers that need to be addressed, particularly in terms of technical contributions and experimental details. The paper could significantly benefit from another round of revision to address these concerns and questions. However, the reviewers are generally in agreement that the paper has merits and potential contributions to the filed."
            }
        },
        "id": "mPI08VPzUC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gVTtkPJbRq",
        "replyto": "gVTtkPJbRq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4570/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593250,
        "cdate": 1696707593250,
        "tmdate": 1701465535133,
        "mdate": 1701465535133,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a benchmark to assess human-perceived coherence (through crowd annotators on Amazon Mechanical Turk) in generated texts, which contains 500 automatically generated texts. The authors  discuss linguistic aspects of coherence including its deep theoretical background. They present two alternative annotation methods – holistic – the annotators were asked to provide a global coherence score for the whole text, and incremental – the annotators scored each sentence also providing a reason for the detected incoherence cases. The results show that the incremental method guarantees higher agreement among annotators. \nApart from the annotation procedures, the authors complemented the paper with further experiments organised into two subtasks  - Coherence scoring and coherence reasoning evaluating different language models using the resulting benchmark - encoder models (BERT and DeBERTa) and generative models (Flan T5 and GPT3). Here, the results were also better for the incremental setting. The results also show that performance of large language models seem to be only a small improvement over the lower-sized models.\nStrong sides (pros): (1) definitely, an interesting topic for the community; (2) novel benchmark for coherence assessment which can be used in further studies, and also for texts generated with any model; (3) the presented methodology is sound; (4) the paper is clearly and well-written, it is self-contained.\nI would not say that there are cons, but rather some flaws in the paper. They were all pointed in reviewers’ comments and include typos, formatting, etc."
            }
        },
        "id": "XOJ1w1awR4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gUKVyjoQBG",
        "replyto": "gUKVyjoQBG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission664/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493208,
        "cdate": 1696707493208,
        "tmdate": 1701465406856,
        "mdate": 1701465406856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The manuscript introduces a new approach to video QA by aligning visual and textual at different levels to accomplish the task. Reviewers and authors engaged in a productive discussion around strength and weaknesses of the approach and some details of the claims. The idea of aligning textual and visual information is novel and most important aspect of the manuscript. Some of the baselines highlighted by the reviewers were not in the original submission and but authors have produced comparison and used the arguments in responses to the reviewers. While this helped with the discussion, a more comprehensive literature survey would have been better in the submission. One of the main advantages of the proposed approach is the claim of using small pre-training data compared with the state of the art to achieve the same performance. However, this claim needs to be adjusted to refer only to the extra pre-training data. Technically, encoders are pre-trained with additional data before being used.\nIn summary authors have identified a better approach to leverage the information in a video to answer question. However while we see some improvements over state of the art, this is not consistent potentially highlighting the need for further experimentation to improve the approach."
            }
        },
        "id": "uKeqqeuXfT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gQeZoe2j3v",
        "replyto": "gQeZoe2j3v",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3375/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561437,
        "cdate": 1696707561437,
        "tmdate": 1701465495454,
        "mdate": 1701465495454,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a method based on LLM-based generation of rationales to boost generalizability of hate speech detection models. Extensive experimentation shows the effectiveness of the proposed model, which both reviewers and myself deem as being novel and exciting.\n\nI appreciate the authors for the rebuttal responses and I would urge them to incorporate the new experiments reported there (GPT2 and T5) in a further revision of the paper."
            }
        },
        "id": "moTCuM2LEs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gQUDsNE3Lh",
        "replyto": "gQUDsNE3Lh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3165/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557357,
        "cdate": 1696707557357,
        "tmdate": 1701465489084,
        "mdate": 1701465489084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes ReLM, a framework for predicting chemical reactions. ReLM combines the capability of language models as well as graph neural networks to provide answer candidates, followed by the confidence score strategy. All reviewers agreed on the well-motivated approach for combining LM and GNN, as well as postiive experiment results."
            }
        },
        "id": "f5UwuJTz1P",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gJZqSRfV21",
        "replyto": "gJZqSRfV21",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3712/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707568089,
        "cdate": 1696707568089,
        "tmdate": 1701465507351,
        "mdate": 1701465507351,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents detailed evaluation criteria, new evaluation data, and an evaluation of multiple outputs for the QUD discourse parsing task, an emerging area of computational discourse modeling. Reviewers generally agree that the quality of the paper is high, that the level of detail is a strong point, and that there are novel contributions here, while their views on the excitement and potential of this work moving forward are more mixed. All reviewers assign positive soundness scores, and two reviewers give a strong excitement score. Considering the relatively unexplored state of QUD research at this point, I would like to see more papers in this area get a chance to develop the framework further, and especially the contribution of new data could be substantial at this point. If accepted, I would ideally like to see this presented as a poster to encourage discussion and allow interested researchers to find out more about the data and how they can use it to develop these ideas further."
            }
        },
        "id": "2i7rtpk7K8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gJXydPLBkt",
        "replyto": "gJXydPLBkt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2206/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536970,
        "cdate": 1696707536970,
        "tmdate": 1701465457852,
        "mdate": 1701465457852,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary:** The paper proposes to incorporate an additional module: Pluggable Reward-Driven Contextual Adapter (PRCA) in the retrieval-augmented generation setting (RAG/ReQA). PRCA is plugged between the retriever and the generator: each of the latter two modules are frozen, and only PRCA is trained (PRCA is itself based on a generative LM backbone) using a reinforcement learning approach to refine the retrieved content so as to optimize and provide a better context as prompt for the generator. Specifically, PRCA first extracts a relevant context from the retrieved passages, and then uses a ROUGE-L score to reward/penalize parameter updates while also maintaining similarity to the original parameters. The approach is empirically evaluated on three QA datasets, and beats baselines by significant margins.\n\nAll reviewers acknowledged the novelty of the solution proposed in the problem: also highlighting it's practicality to suit real world industry use-cases where the LLM cannot be trained/only API access calls are available. The claims in the paper are supported well empirically with evaluation over 3 datasets. Based on clarifications provided in the author rebuttal, most major concerns raised by the reviewers have been mitigated.\n\n**Recommendations for improvement:** (i) One of the concerns on the paper is around the usage of only automatic evaluation (GPT-4) and no human evaluation to support the claims. The author response has partially addressed this by providing a small pilot study on 100 examples to compare human and automatic evaluation (and achieved parity in the late 90%s). I would encourage the authors to expand on this study, and add it to the main paper to strengthen claims of the paper.\n\n(ii) The authors should add all details from the author response clarifying several of reviewer JYC9's concerns around missing details in the paper, and adding significance tests to the empirical results."
            }
        },
        "id": "sdFmME8sAM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gI11vXg1W4",
        "replyto": "gI11vXg1W4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3464/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563258,
        "cdate": 1696707563258,
        "tmdate": 1701465498415,
        "mdate": 1701465498415,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper is focused on developing resources and methods for complex retrieval tasks. Towards that end, they have created a new dataset which involves long text queries based on real users trying to remember the identity of particular books (e.g., describing the cover, along with vague details about the author or story). The paper also presents a new method for this task, based on decomposing the query, and compares against classical IR baselines.\n\nReviewers found this paper to be interesting, novel, and easy to follow. The new dataset was seen as potentially useful to the community. \n\nThe two main concerns were that 1) the methodology was somewhat ad hoc, and left room for improvement, both in terms of rigor and sophistication, and 2) limitations with the evaluation, including the use of only a single dataset, lack of ablations, and lack of more recent baselines for comparison. In general, it seems like there is a concern that although the authors have demonstrated an effective method, it is unclear exactly why it works, or how well it will generalize.\n\nIn their rebuttal, the authors have included additional results, for both additional baselines and additional datasets. Although reviewers have acknowledged the rebuttal, they have unfortunately not specifically engaged with the new results provided by the authors. Given the difficulty of assessing out of context results provided in a rebuttal, this is not terribly surprising, and potentially reveals a problem with encouraging authors to include new experimental results in their rebuttal.\n\nOverall, this seems like a case where this paper could benefit from an additional round of revisions before publication. However, all reviewers rated this paper as good or better on soundness, so they nevertheless felt it was worthy of being accepted, even if the paper's limitations tempered their excitement."
            }
        },
        "id": "8eoh5mwy7z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gGmccVXoy2",
        "replyto": "gGmccVXoy2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2483/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543134,
        "cdate": 1696707543134,
        "tmdate": 1701465466484,
        "mdate": 1701465466484,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This manuscript seeks to deal with the confusion that arises from the terms (and research on) values, morals, and ethics. The manuscript surveys prior literature from outside of NLP on these concepts, then surveys NLP literature, and suggests future avenues for NLP.\n\nIn general, the reviewers are positive towards this manuscript and particularly appreciate:\n\n1. That there is confusion of the terms and this manuscript may provide a starting point to address such confusion\n2. The manuscript provides a comprehensive report on how the terms have been used\n3. The manuscript provides quantitive support for their recommendations and findings\n4. The manuscript highlights flaws in terminology in prior work\n\nHowever, the reviewers had the following contention:\n\n1. The manuscript would do well to consider additional related terms such as norms, beliefs, customs, behaviors, and ideologies.\n2. It can be hard to gain a sense of the bigger picture from how results are presented\n3. The selection of papers is narrow and does not take into consideration topics where the terms in question are implicit.\n4. It is unclear how to implement recommendations for common terminology in future work\n\nWrt. 1 both authors and reviewers agree that this manuscript can serve as a starting point for such considerations. To this effect, authors will reframe writing to make the bigger picture (2) clearer. While the authors do not address 3, the scope of addressing this is incredibly large and is in my opinion more appropriate for future work. Finally, the authors correctly identify that NLP researchers creating vocabularies may lead to further inaccuracies and limit opportunities for interdisciplinary collaboration."
            }
        },
        "id": "0yZx7jE9yy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gBI7thSo0X",
        "replyto": "gBI7thSo0X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5465/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613483,
        "cdate": 1696707613483,
        "tmdate": 1701465560176,
        "mdate": 1701465560176,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper verifies the chain-of-thoughts prompting style in the context of text-to-SQL. Two enhancements are introduced to the original CoT, providing interesting insights. The reviewers agree that the experimental results are promising. The authors have provided detailed answers to the questions raised by the reviewers (+ new results), addressing their concerns with respect to statistical significance of the results, the impact of different choices, and the novelty of the proposal. Reviewer DzXr has kept the initial judgement for the \"possibility of missing or misunderstanding some points\" while the concerns of reviewer Cceb are either not backed up with details or addressed in the rebuttal."
            }
        },
        "id": "sI1P2Hb9b3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "gAzBhetShk",
        "replyto": "gAzBhetShk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4741/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597436,
        "cdate": 1696707597436,
        "tmdate": 1701465540411,
        "mdate": 1701465540411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a unified normal form for the class of tree-adjoining languages, the two-level representation leads to efficient algorithms to parse all those formalisms. Typically, they introduce new definitions of semiring-weighted two-level formalisms, propose new stringsum  and allsum algorithms, and prove the correctness and efficiency of their algorithms. Those formalisms and findings are beneficial to the community of tree-adjoining languages, and may lead to some potential applications."
            }
        },
        "id": "8RZ0Ca4B3t",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "g8OqNaz6dY",
        "replyto": "g8OqNaz6dY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4629/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594866,
        "cdate": 1696707594866,
        "tmdate": 1701465537069,
        "mdate": 1701465537069,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a novel approach to improve the commonsensical output of pre-trained language models (PLTMs). The research focuses on (1) the creation of a reference-free evaluator called the O-score to assess the commonsense nature of generated text and (2) guides the extension of a controllable generation method named NADO, aiming to steer PLTMs towards producing more commonsensical text.\n\nPros:\n1. Introduces a new metric, the O-score, to evaluate the commonsensicality of generated text without requiring reference sentences.\n2. Extends the NADO method to focus on controllable commonsense generation. The proposed method is model-agnostic. \n3. The paper is well-structured and comprehensive in its experimentation, and the results surpass baselines.\n\nCons:\n1. The initial manuscript misses out on comparing with more diverse and strong LLMs such as instruction-tuned LLMs. However, I can see these experiments are added during rebuttal. I hope the author will add these new results in the final version and provide more details discussions. For example, the reply to QA of Reviewer DEwh should have more narrative to explain the results and comparisons. \n2. The paper does not clarify how the generated commonsense outputs could be useful for downstream applications, which could attest to the method's significance. The related discussion in rebuttal should be included in the final version."
            }
        },
        "id": "DIf0bCVoI1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "g84UrdUwBA",
        "replyto": "g84UrdUwBA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4325/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587500,
        "cdate": 1696707587500,
        "tmdate": 1701465527950,
        "mdate": 1701465527950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors proposes a behavior cloning-based approach for text games which simulates supervised approaches without the need for ground truth demonstrations.\nThe 2 out of 3 reviewers selected \"Good\" for soundness (1 selected \"Strong\")"
            }
        },
        "id": "hCREEKrHP6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "g4FAvRcSuf",
        "replyto": "g4FAvRcSuf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4385/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588697,
        "cdate": 1696707588697,
        "tmdate": 1701465530048,
        "mdate": 1701465530048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies model calibration of LLMs trained with RLHF based on existing observation that RLHF worsens the calibration of model, i.e. the confidence of model predictions less correlates with accuracy. Due to lack of log probabilities of closed models, authors propose to use verbalized probabilities. They also show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Per reviewers' requests, authors added experiments with llama2 and llama2-RLHF models. I think this paper studies a very important issue -- model calibration in the LLM regime, and provides timely and interesting studies regarding how to measure model calibration for closed models. I encourage authors to add more results of open-sourced models in the camera-ready and include more evidence on \"RLHF worsens the calibration\"."
            }
        },
        "id": "m4ZqdZoH0i",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "g3faCfrwm7",
        "replyto": "g3faCfrwm7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4756/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597899,
        "cdate": 1696707597899,
        "tmdate": 1701465540793,
        "mdate": 1701465540793,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a comprehensive study on adapting pre-trained text-to-text models for long text sequences. It explores three key aspects: model architecture, modeling objectives, and pretraining corpus. The authors recommend replacing full attention with pooling-augmented blockwise attention, using a T5-style denoising objective with short and long spans, and pretraining with randomly concatenated passages from the C4 corpus. These adjustments result in a strong long-context language model that excels in long-text summarization and QA tasks. The work requires further study on the claim that \"randomly concatenating passages improves over long documents\"."
            }
        },
        "id": "bLKotPMaru",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "g3VOQpuqlF",
        "replyto": "g3VOQpuqlF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission468/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488464,
        "cdate": 1696707488464,
        "tmdate": 1701465400037,
        "mdate": 1701465400037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents xDial-Eval, an evaluation benchmark for multilingual open-domain dialogue systems. In addition to its contribution as a new evaluation resource, the paper also shared insights into applying LLMs to automatic dialogue evaluation. \n\nTwo reviewers pointed out the need for human evaluation on the automatic machine translation quality during the construction of the multilingual dataset, to which the authors' rebuttal provided some further details on the conducted human evaluation results. However, given these new results were missing from the original paper submission, and somewhat in contradict with the description under section 3, \nthey are not considered as sufficient to override to the original review criticism. \n\nOne of the reviews was not taken into consideration for the AC recommendation due to the lack of details and no reviewer response during rebuttal period.|meta review"
            }
        },
        "id": "3sQlVmgnIA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "g1Q9Uu8lCp",
        "replyto": "g1Q9Uu8lCp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission137/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480097,
        "cdate": 1696707480097,
        "tmdate": 1701465388456,
        "mdate": 1701465388456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Strengths**:\n\n1. Critical problem of selecting a representative subset of in-context demonstrations that can effectively prompt different test instances in a speciﬁc task. \n\n2. high quality and diversity are stressed together for good quality examples. \n\n3. two-stage Determinantal Point Process (DPP) method is simple and effective. results show a significant advantage in inference time and token usage compared to the retrieval-based method.\n\n\n**Weaknesses**:\n\n\n1. The proportion of the selected subset should be explored, and for the unselected samples it needs to further investigate that whether there are useful samples that are dropped falsely. Although the authors show results for GPT2-XL in rebuttal, they should include such analysis for other models also.\n\n2. Missing latest baselines: comparisons with Xiaonan Li et al, ACL 2023; Zhang et al., 2022a.\n\n3. Code is not made public.\n\n**Suggestions**:\n\n\n1. C(10,4)=210 4-shot subset quality evaluation and checking rank of selected subset should be done at least for a few samples.\n\n2. Good to add a frame diagram to describe the entire process of our method (Algorithm 1).\n\n3. Please address above weaknesses also in the revised draft."
            }
        },
        "id": "G7qgIE3uDf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "g1LLeiHX0P",
        "replyto": "g1LLeiHX0P",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2871/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551205,
        "cdate": 1696707551205,
        "tmdate": 1701465479514,
        "mdate": 1701465479514,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper studies the inconsistencies problem in LLMs. It compares two methodologies: up-scaling the LLM, and the usage of retrieval databases. With an improved version of the ParaRel benchmark the results show that retrieval augmentation performs better and is more efficient. However, even with the retrieval augmentation approach, there is no full consistency. extensive analysis drills down on the reasons behind this finding along various dimensions: consistency of the retrieval, correlation between retrieval consistency and prediction consistency, dependency of the reader on term frequency in the retrieved results, etc.\n\nStrength:\n1. Introduce an  improved benchmark (ParaRel) by considering consistency.\n2. The approach is well formalized.\n3. The result  is  abundant and convincing.\n\nWeakness:\n1. The results that retrieval augmentation improves performance is not much of a surprise.\n2. The result is restricted to only one dataset\n3. There is no result for the combination of higher parameter count and retrieval-based enhancement"
            }
        },
        "id": "AJYTyEURs5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "g0wzziJSmN",
        "replyto": "g0wzziJSmN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3498/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563965,
        "cdate": 1696707563965,
        "tmdate": 1701465499297,
        "mdate": 1701465499297,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors introduce ViPE (Visualise Pretty much Everything), an innovative suite of text-to-image models that emphasize the depiction of figurative and non-literal expressions. Key outcomes of ViPE encompass the lyrics-based pretraining corpus named 'LyricCanvas' and an assortment of pretrained text-to-image foundational models.\n\nThere is a unanimous consensus among reviewers that ViPE is both exciting and original. It ventures into the visualization of figurative language, a domain that, though crucial, remains largely uncharted, and does so with a fresh methodology."
            }
        },
        "id": "R3SZ0jKcTX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "g04NBFnIxb",
        "replyto": "g04NBFnIxb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3580/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565621,
        "cdate": 1696707565621,
        "tmdate": 1701465502310,
        "mdate": 1701465502310,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The objective of this paper is to devise an automated annotation approach aimed at ameliorating the false negative predicament in document-level relation extraction (DocRED) dataset with minimal human intervention. DocGNRE leverages a substantial language model for annotating relations between entities and incorporates Natural Language Inference (NLI) models to derive entailment scores, thus assessing the quality of the relation triples generated by GPT. Overall, this paper is robust. However, the authors are encouraged to enhance clarity in experimental specifications and certain notations in the revised version."
            }
        },
        "id": "Uo8kDkFLxW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fzb2sxexWN",
        "replyto": "fzb2sxexWN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4955/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602223,
        "cdate": 1696707602223,
        "tmdate": 1701465545661,
        "mdate": 1701465545661,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a novel dataset, MathDial, featuring 3k tutoring dialogues centered around multi-step math reasoning problems. The dataset was collected using a framework where real teachers interact with a Large Language Model (LLM) scaffolded to represent common student errors. The authors propose a taxonomy of teacher moves that promote student learning opportunities, and demonstrate the potential of MathDial to fine-tune language models into more effective tutors. The paper is submitted for consideration in the Resources and Evaluation and Dialogue and Interactive Systems tracks.\n\nReviewers largely agreed on the significant value of the proposed dataset to the community, particularly in the context of dialogue-based tutoring systems. They appreciated the paper's novelty, the potential impact of the work, and the depth of the analysis provided. Several reviewers pointed out that the dataset can fill a crucial gap in studying effective dialogue tutors at scale, and it could encourage more work in this area. However, the reviewers also raised several concerns and questions which were tried to address in rebuttal. These include the role of the LLM as a student, the realism and quality of the collected dialogues, and the clarity of the experimental details. The consensus among the reviewers was that while being an interesting field of study, the paper could significantly benefit from addressing these concerns and questions in the final version. The authors are strongly encouraged to take these comments into consideration to further improve their paper.\n\nOverall, this submission is seen as an important contribution to the field of dialogue tutoring and has the potential to stimulate further research in this area. In addition, I encourage the authors to address the reviewers' feedback to further strengthen the paper."
            }
        },
        "id": "0k7DVHH2VD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fyza2OQ9NI",
        "replyto": "fyza2OQ9NI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1011/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502395,
        "cdate": 1696707502395,
        "tmdate": 1701465417716,
        "mdate": 1701465417716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper examines how LLMs respond to epistemic markers of certainty and uncertainty and develops a wide range of different modifiers based on linguistic theory, which are inserted into QA prompts. The authors conduct experiments which turn out to contradict the hypotheses/expectations (i.e., LLMs are robust to the modifiers and the certainty of the modifiers increases accuracy and vice versa). The reviewers find this paper very intriguing to read and that analyzing the behavior of LLMs w.r.t. epistemic markers of uncertainty and specific linguistic phenomena in general is valuable to the community and the paper has conducted comprehensive experiments and analysis. Most concerns have been addressed in the rebuttal and the authors promised to incorporate the feedback into the revision. Overall, I'd recommend accepting this paper to the main conference."
            }
        },
        "id": "Z2Pt8qwk9r",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fxotfo1j8T",
        "replyto": "fxotfo1j8T",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission809/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496473,
        "cdate": 1696707496473,
        "tmdate": 1701465411304,
        "mdate": 1701465411304,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "*Summary*: This work investigates different prompting strategies to improve ChatGPT's performance on machine translation tasks. The authors c\nonsider the impact of decoding temperature, inserting task/domain-specific information to the prompts, and the effect of few-shot\nand chain-of-thought prompting on translation quality on two (FLORES, WMT) benchmarks.\n\n*Evaluation*: R1 and R3 have rated this work as 3 or higher on soundness (3/4), while R2 has rated this as a 2. R2's main concerns are that 1) the investigation in this work is too empirical without any other solid backing and 2) the content is a bit thin with the focus on identifying good prompting strategies to improve ChatGPT's translation capabilities. Regarding 1), well-conducted empirical analyses do have a place in EMNLP. With regards to 2), the authors have offered many natural dimensions (adding task/domain information, few-shot setting) along which prompting might be carried out for translation. The CoT-based prompting could have been made more competitive; it is not surprising that just adding \"step by step\" to the prompt does not help."
            }
        },
        "id": "EXIBzbgleX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fxdvWG4rJe",
        "replyto": "fxdvWG4rJe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3860/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578001,
        "cdate": 1696707578001,
        "tmdate": 1701465512074,
        "mdate": 1701465512074,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper examines the implicit discourse relation recognition (IDRR) task, proposing a generative model that exploits instruction learning, chain-of-thought, and in-context learning. Overall, the paper does not have serious flaws. While one reviewer found this work exciting, the remaining two reviewers were not particularly excited about it.\n\nStrengths:\n\nThe use of instruction learning and chain-of-thought for the IDRR task is new and interesting.\n\nExtensive experiments were conducted.\n\nWeaknesses:\n\nThe reviewers agreed that some parts of the paper were confusing (e.g., instruction learning).\n\nThere was missing related work (e.g., the Long and Webber 2022 paper).\n\nThe empirical contribution of this paper could be strengthened. For instance, there wasn't any empirical analysis of the results (e.g., the seemingly small improvements). In addition, the claims could be strengthened through experiments with different models and different model sizes. The rebuttal did not provide additional empirical results or any analysis of the results. \n\nThe reviewers did not change their scores after reading the rebuttal and believed the paper could benefit from another round of revision."
            }
        },
        "id": "DCBTk19wH6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fwmZinFwgX",
        "replyto": "fwmZinFwgX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1895/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529598,
        "cdate": 1696707529598,
        "tmdate": 1701465446139,
        "mdate": 1701465446139,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reasons to Accept:\n(1) The paper addresses a neglected aspect of abstractive text summarization (ATS) by focusing on content addition, which is essential for actual text simplification.\n(2) The exploration of the Question Under Discussion (QUD) framework for generating elaborations in ATS is an innovative and well-executed idea.\n(3) The paper is well-structured and thoroughly explains its concepts, making it easy to follow.\n\nReasons to Reject:\n(1) There are no strong reasons to reject the paper, but it could benefit from discussing the risk of hallucinations in elaborations, especially given that some elaborations involve explanations of concepts. While this might be a topic for future work or limitations, addressing it would add depth to the paper.\n\nThe authors have provided reasonable responses regarding hallucinations and discussions on focus and topic structure."
            }
        },
        "id": "S1wv9Jc9sr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fwA8iKyIlk",
        "replyto": "fwA8iKyIlk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2095/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534344,
        "cdate": 1696707534344,
        "tmdate": 1701465453837,
        "mdate": 1701465453837,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper conducts the first multi-faceted human evaluation on entity-centric summarization, which reveals detailed insights into model behavior. And, all reviewers have expressed positive opinions about this paper. Therefore, I suggest this paper accepted as a short paper in the conference."
            }
        },
        "id": "p8oHrlLVN6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fvGJOVkm0b",
        "replyto": "fvGJOVkm0b",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4341/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587804,
        "cdate": 1696707587804,
        "tmdate": 1701465528436,
        "mdate": 1701465528436,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a comprehensive benchmark for Scientific Document Representations evaluation. It comprises  25 tasks out of which 11 are new. A detailed analysis of the generalizability of state-of-the-art scientific document representation models is presented along with a new method called Multi-Format embedding, where the authors  utilize different embedding based on different formats which shows better performance as compared to other state-of-the-art models."
            }
        },
        "id": "aq4ntVvz95",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ft0c1K3492",
        "replyto": "ft0c1K3492",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5182/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608710,
        "cdate": 1696707608710,
        "tmdate": 1701465552230,
        "mdate": 1701465552230,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a well-thought-out user study about how model miscalibration affect the user's trust over multiple trials. All reviewers agree that the paper is well-written, the research question is important, and that the experimental setup of using bets to measure trust is novel. Reviewer 4Q7E recommended beta-distribution based computational trust models as baselines, which is addressed by the authors. The only downside of this paper is the generalizability of results from the simplistic setting, but the authors made reasonable efforts to improve that by controlling for the user's prior knowledge. This paper presents a good blueprint for future work studying the temporal effect of calibration on trust in AI.\n\nFor related work, I recommend including HCI literature on the impact of feedback on user trust, e.g., No Explainability without Accountability: An Empirical Study of Explanations and Feedback in Interactive ML by Smith-Renner et.al."
            }
        },
        "id": "kIho22nksl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fsGowIsscZ",
        "replyto": "fsGowIsscZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1340/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510686,
        "cdate": 1696707510686,
        "tmdate": 1701465427813,
        "mdate": 1701465427813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose a new method for black-box model adversarial attacks. They leverage a generative model to use data sampled from the target tasks to obtain transferable adversarial features. In the experiments, the authors show a high attack success rate across multiple tasks. The paper is well-written and easy to follow. \nAt the same time, the authors might miss some detailed descriptions of their methods and results."
            }
        },
        "id": "vWM6qPzJcP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fqWbXPX99P",
        "replyto": "fqWbXPX99P",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission565/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490928,
        "cdate": 1696707490928,
        "tmdate": 1701465403642,
        "mdate": 1701465403642,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a new supervised opinion summarization model for a large number of reviews. A new sampling strategy based on sentiment analysis and contrastive information evaluation is introduced for the selection of high-quality review subsets from a large input set.  The study suggests a two-stage process for carrying out the task: training using MLE to generate the initial summaries and then refining the model with contrastive learning to assign a higher probability to better candidate summaries. The results of the experiments demonstrate that the suggested method outperforms the robust supervised opinion summarization baselines. \n\nPros:\n1. New sampling strategy for selecting the best candidates for review summarization\n2. A novel two-stage training scheme \n3. The experimental results, including human evaluation using best-worst scaling, show the superiority of the proposed system compared to the strong supervised opinion summarization baselines.\n\nCons:\nSome reviewers noticed missing comparisons with LLMs designed for long inputs. The authors ran experiments and reported the results in their rebuttals. \nThe authors responded to every reviewer with detailed letters. Two reviewers increased their scores."
            }
        },
        "id": "crcBrwIEoV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fqKoLPfCba",
        "replyto": "fqKoLPfCba",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission184/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481289,
        "cdate": 1696707481289,
        "tmdate": 1701465390152,
        "mdate": 1701465390152,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL) into modeling long document topic segmentation. The experimental results show their model based on longformer outperforms the existing SOTA methods. \n\nAll three reviewers believe that the paper has a good structure and clear organization, and the proposed solution has a certain novelty. However, there are certain concerns about motivation, increasing the cost and impact of using TSSP and CSSL, and the details still need to be further improved. I suggest the author refine the paper based on the valuable feedback from the reviewer, whether accepted or not."
            }
        },
        "id": "gkY1mcllnM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fonxcS8gqM",
        "replyto": "fonxcS8gqM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission927/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499609,
        "cdate": 1696707499609,
        "tmdate": 1701465415183,
        "mdate": 1701465415183,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors tackle the problem of multi-hop common sense reasoning in dialogue and propose a dataset and a model. The reviewers appreciated the dataset, the clarity of the work, and the results achieved. They did raise a few concerns such as the fact that the novelty lies in the combination of known methods or some missing details but the authors adequately address them. Therefore I recommend accepting this paper."
            }
        },
        "id": "Fp0kqvQ2ro",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "flkXLt9WKn",
        "replyto": "flkXLt9WKn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5657/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616472,
        "cdate": 1696707616472,
        "tmdate": 1701465564638,
        "mdate": 1701465564638,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces topic-informed dialogue summarisation, which pre-computes the topic distribution in a dialogue and incorporates this information (in the form of soft prompt / latent embeddings) in the summarisation process. Reviewers thought the idea of incorporating topic distribution, particularly for dialogues, is well-motivated and the proposed approach is interesting. Moreover, reviewers also thought quantitatively the topic-informed summariser appears to perform very well. That said, there are some minor but important concerns that dampens the excitement of reviewers: (1) the writing for certain parts in the method description can be improved (GoNj and KN1n); (2) the paper would benefit from more analyses to bring out more insights (GoNj); (3) the utility/applicability of the approach for longer conversations warrants at least a discussion, if not more results (KN1n, Fkky); and (4) the paper might want to provide some results to validate the assumption that the topic distribution in the ground truth summary is similar to that of the dialogue."
            }
        },
        "id": "VOPuZEoJis",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fkmSyrSjnq",
        "replyto": "fkmSyrSjnq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2758/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548794,
        "cdate": 1696707548794,
        "tmdate": 1701465475379,
        "mdate": 1701465475379,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper combines fundamental linguistic and cognitive questions with sound modeling. It is well situated in previous literature and makes insightful and sound contributions."
            }
        },
        "id": "YibrreGbwM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fkAKjbRvxj",
        "replyto": "fkAKjbRvxj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3278/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559575,
        "cdate": 1696707559575,
        "tmdate": 1701465492679,
        "mdate": 1701465492679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel approach to data augmentation for relation extraction, consisting of three key steps: causal term identification, relation expansion, and controlled editing. Generally speaking, this is a solid data augmentation work for relation extraction. While the paper employs terms such as \"intervention,\" \"counterfactuals,\" and \"causal,\" it is my viewpoint that the method proposed in this paper has a relatively weak connection to causal theory and does not operate within a causal theory framework. Therefore, I recommend that the authors carefully reconsider this aspect, make necessary revisions to the paper, and provide a clearer elucidation of the relationship between their method and causal theory, or employ more appropriate descriptions."
            }
        },
        "id": "ZU0HEsMAwu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fi90p5364y",
        "replyto": "fi90p5364y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1075/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504509,
        "cdate": 1696707504509,
        "tmdate": 1701465419697,
        "mdate": 1701465419697,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors proposed a new modeling formulation that examines and addresses the domain dependency issue in political bias detection models. The authors tried to remove dependency on external knowledge sources. Reviewers have concerns regarding comparison with baselines. \n\nAll the reviewers agree that the paper provides sufficient support for the major claims.  \n\nHowever, the contributions are marginal. Comparisons are not comprehensive (Reviewers Nv3C, twSB). Some terms need to be defined. An additional revision would be helpful to make the paper more readable (Reviewer Nv3C, b9Zn)."
            }
        },
        "id": "OJXv1HbAGV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fhEkqMyvb0",
        "replyto": "fhEkqMyvb0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3294/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559833,
        "cdate": 1696707559833,
        "tmdate": 1701465493092,
        "mdate": 1701465493092,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel task and dataset, enhancing the semantic textual similarity (STS) by introducing conditional elements. Even in the era of large-scale language models, predicting textual similarity remain vital, with applications like retrieval-augmented generation. We look forward a refined camera-ready version and updated dataset that takes into account feedback from reviews and discussions with reviewer L1HG."
            }
        },
        "id": "9bwC67Q7Fm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fgQ7JQoBIM",
        "replyto": "fgQ7JQoBIM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4360/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588222,
        "cdate": 1696707588222,
        "tmdate": 1701465529138,
        "mdate": 1701465529138,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work focuses on the compositional reasoning ability of LLMs and makes contributions in both evaluation and method. To evaluate the compositional gap with 2-hop questions is reasonable. I am also curious about such a gap for 3-hop questions, and the performance of self-ask prompting for 3-hop questions and implicit multi-hop questions. I also agree that decomposing compositional questions and using search engines are not new ideas. The novelty of self-ask prompting is somewhat incremental. Overall, this paper presents a decent work, even though it is not that groundbreaking."
            }
        },
        "id": "zcEGtc3jRr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "feiAVaSXdb",
        "replyto": "feiAVaSXdb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission89/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478884,
        "cdate": 1696707478884,
        "tmdate": 1701465387057,
        "mdate": 1701465387057,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper \"Cross-lingual Transfer Can Worsen Bias in Sentiment Analysis\" presents work on the effect of transfer learning on sentiment analysis in non-English languages and the import of biases from one languages to another. \n\nThe arguments against accepting the paper refer to details of the experimental setup. \nThe arguments for accepting the paper refer to the task as a whole, the depth and extent of the experiments and the results presented."
            }
        },
        "id": "VbmEj7a3Zy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fbbbbfhAxC",
        "replyto": "fbbbbfhAxC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1197/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507140,
        "cdate": 1696707507140,
        "tmdate": 1701465423399,
        "mdate": 1701465423399,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper is focused on the topic of unsupervised candidate answer extraction for QA. It introduces a novel approach using the DMR model. The reviewers largely appreciate the paper's innovation, clarity, and contributions (the DMR model and annotated datasets). The reviewers offered many constructive comments, and there were major questions regarding better understanding the experimental results and the significance of the proposed approach on downstream QA tasks. Overall this is an interesting work that is worth sharing with the community."
            }
        },
        "id": "ukkMGpU39J",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fXyoHAVffT",
        "replyto": "fXyoHAVffT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4050/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581971,
        "cdate": 1696707581971,
        "tmdate": 1701465518210,
        "mdate": 1701465518210,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper advances the state of the art for rumor detection using novel methodology that is backed with strong evidence. The reviewers appreciated the novelty of the work along with the experimentation. The authors are recommended to address the reviewers' concerns in the camera ready, if the paper is accepted."
            }
        },
        "id": "y6YENsAfUc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fRpif5Sflc",
        "replyto": "fRpif5Sflc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2962/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552973,
        "cdate": 1696707552973,
        "tmdate": 1701465482467,
        "mdate": 1701465482467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers judged this work to be sound technically and also reproducible. The paper is well written and clear. The study itself was found to be creative and novel, with important results. The major limitation is the small size of the dataset and the fact that the VLM used this work would respond with \"N/A\" is a large number of trials."
            }
        },
        "id": "5DBtcC955y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fOoZipX9z3",
        "replyto": "fOoZipX9z3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4763/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598017,
        "cdate": 1696707598017,
        "tmdate": 1701465541044,
        "mdate": 1701465541044,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This submission offers a straightforward short dataset paper, which collects 3,447 articles from two state-backed fake news websites (WarOnFakes and Reliable Recent News) using the WordPress API. The authors also perform rudimentary exploratory analysis on the dataset, by analyzing topics and article frequencies. Additionally, they analyze the dataset by comparing the distributions of LIWC words on the fake news sites to the New York Times, and counting the most frequent n-grams by month. \n\nAll reviewers offered positive assessments of the paper, although the reviews were fairly brief (perhaps because the paper was very straightforward). All three reviewers awarded an excitement score of 4, and seemed enthusiastic about the work. One noted that the submission offered an “interesting corpus” which would “offer a wide range of research possibilities.” They also pointed out that the paper “fits the short paper track very well.” Another suggested that “I think the dataset can be very valuable” and that they “found the analysis interesting, although more from a sociological, cultural perspective than from an NLP point of view.”\n\nBecause all reviewers were excited about the contribution, the only questions surrounding this submission have to do with soundness. One reviewer awarded a soundness score of 4, and the others awarded a soundness score of 3.  However, looking more closely, the reviews did not surface any major issues surrounding soundness. Therefore, there do not appear to be concerns about the soundness of this work.\n \n- Reviewer Bm7V initially awarded a soundness score of 3, but changed it to 4 after the authors answered a question about tokenization during the rebuttal period.\n- Reviewer q5Fz awarded a 3 for soundness, but did not list any concerns about soundness in their reasons to reject. Instead, they suggested that the paper might be improved if the authors include a discussion of prior scholarly study of propaganda. \n- Finally, reviewer Rv3e awarded a soundness score of 3, and said they were concerned about fine-tuning during the neural topic modeling analysis. However, based on their comment, it seemed as if they actually may have been concerned about how the authors performed hyperparameter tuning. The authors replied to Rv3e with information about hyperparameter selection, which seemed to satisfy Rv3e. Rv3e thanked the authors for the additional details but did not change their score."
            }
        },
        "id": "6YWgo1ubYg",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fONyQKyvsY",
        "replyto": "fONyQKyvsY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3624/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566419,
        "cdate": 1696707566419,
        "tmdate": 1701465504387,
        "mdate": 1701465504387,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper describes a methodology for instruction finetuning an LLM for the domain of materials science, including the release of a new benchmark for this domain. While reviewers agreed that the application area and new benchmark were interesting, they weren't very excited about the novelty of the approach itself compared with previous work, and felt that the paper was in some places unclear or that details were missing. The authors should do a careful pass to revise the manuscript for clarity and inclusion of details following the reviewers' feedback in subsequent drafts."
            }
        },
        "id": "6zFwf0ZbBJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fNlSVIsbIT",
        "replyto": "fNlSVIsbIT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4171/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584618,
        "cdate": 1696707584618,
        "tmdate": 1701465522877,
        "mdate": 1701465522877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents an editing mechanism which first prompts a PLM to extract style class of a given text, which is then used along with word-level editing for style transfer. The motivation is claimed to be a better more control during the style transfer phase, in contrast with conventional prompt-based style transfer solutions. On 3 style-transfer benchmarks, the paper outperforms previous SotA. The discussion with xk33 and PLiK clarified some of the missing steps and better contextualised the contribution of the work compared to previous sentence-editing literature. I encourage the authors to take these suggestions on board."
            }
        },
        "id": "WcmWhW7aM7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fNi7eet4Qc",
        "replyto": "fNi7eet4Qc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3965/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580160,
        "cdate": 1696707580160,
        "tmdate": 1701465515440,
        "mdate": 1701465515440,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "There is a consensus among the reviewers and most questions have been answered in the author response."
            }
        },
        "id": "vCVALljPRp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fM7x9Lvb9r",
        "replyto": "fM7x9Lvb9r",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3742/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707571342,
        "cdate": 1696707571342,
        "tmdate": 1701465508225,
        "mdate": 1701465508225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper surveys a large number of codeswitching data sets regarding their representativeness and puts forth the position that progress in developing systems that correctly handle real-world codeswitching may be caused by failures to accurately represent the codeswitching communities and to clearly report information regarding representativeness (demographics, etc.) of the subjects and annotators.\n\nOne point of confusion in the reviewing process was what the author’s intentions were for this paper. While the paper does not explicitly identify itself as a position paper, it does explicitly state it is not aiming for a comprehensive survey [091-093]. However, the reviewers generally identified it as a survey paper, and I think most readers would come away with that impression. The authors clarified in the response period that they believe this is a position paper, and not a survey.\n\nWhy does this point matter? There appears to be some confusion about what the main contribution of the paper is, and it is not easy to determine from the paper itself, judging from the reviews.\n\nAs reviewer sZBB points out, if we take this paper as a position paper, the position is not well-supported by the paper itself:\n\n> The main reason to reject this paper is that some of the main claims are not backed up with sufficient analysis and/or citations. In particular, I believe that the main claim of the paper is that a lack of representativeness in training and evaluation data leads to down-stream systems which are not fit for purpose (see lines 85, 694). However, there is no evidence in the paper which backs this up (empirical work here would be really good). \n\nAs reviewer 65nB states, “empirical evidence for the main motivation is limited.” Reviewer 5PvD seems to believe this is a survey paper and that essentially no clear position is put forward.\n\nIf there is a central position, perhaps it is best summarized in the abstract:\n> we argue that the lack of representative CSW data collection and preparation procedures could lead to this drawback.\n\nHowever, as the reviewers point out, while this hypothesis is floated, there is no clear evidence given to support it. Significant issues of representation are raised, but the empirical evidence given mostly has to do with system adaptation issues. Systems adapted to improve on one population or data type show performance reductions in another, etc.\n\nThe structure of the paper could be improved to make it clearer what its goals are and focus more on their specifics (see reasons to reject from Reviewer sZBB for more detail).\n\nStrong claims about the progress of codeswitching systems are made, for example in the abstract “there is still not much progress in building successful CSW systems”. This is a controversial claim that is only supported in the paper with the limited evidence mentioned above. The general question of adapting models to work well on different domains, genres, registers, types of input, etc., goes far beyond codeswitching, and it is not at all clear that creating and using more representative data will address this.\n\nAnother concern raised by reviewers is the limited contribution beyond existing work (see Reviewer sZBB in particular), given recent significant surveys.\n\nOverall, all reviewers were in agreement that portions of this paper represented significant contributions, however, the paper’s structure makes it difficult to understand the paper’s primary contribution and distinguish it from recent work that is cited throughout.\n\nMinor notes:\n\n1. Reviewer 5PvD raises the question of whether a survey paper is appropriate for this venue; it is absolutely appropriate, as survey papers and position papers are explicitly called out as welcomed in the call for papers. I believe regardless of the exact framing of the paper, this content is absolutely welcome and encouraged at this venue.\n\n2. While the reviewers did not point this out, I would like to make sure the authors are aware that the collection and reporting of demographic data about the speakers and annotators in corpora is often restricted by research ethics boards. For example, some Institutional Research Boards (IRBs) in the US do not allow this data to be recorded, and if it is recorded, they may not allow it to be released along with the data. In my personal experience with IRBs, as the annotators are not the subjects of research, I have never been able to convince an ethics board to allow the recording or release of this information."
            }
        },
        "id": "G6CppLVLhl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fLJVvFGFEE",
        "replyto": "fLJVvFGFEE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2075/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533766,
        "cdate": 1696707533766,
        "tmdate": 1701465452880,
        "mdate": 1701465452880,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a dynamic routing approach for Sparse Mixture-of-Experts (SMoE) training and inference. This method aims to mitigate the limitations of existing SMoE approaches by generating the router's parameters through a combination of a fixed hypernetwork and trainable embeddings. \n\nGenerally, the reviewers appreciated the novelty of the proposed approach and that experimental results were given for both pre-training and fine-tuning settings. The results indicate that the approach appears to outperform existing routing strategies in the SMoE regime. Some concerns were raised by reviewers around the size of models evaluated, to which the authors provided additional results. These results indicate that the proposed approach also works on larger models (at least in the architectural family). The paper would be even stronger if additional Transformer architectures were explored. \n\nThere was also concerns related to communication overhead/amount of computation of the approach raised by one reviewer. It is not clear to me why this approach would suffer additional communication overhead compared to relevant existing SMoE approaches. The authors state clearly in the rebuttal that it is a fair comparison in terms of number of experts used across the different approaches.\n\nFinally, the authors also did a good job in the rebuttal of providing additional analysis to argue why the representational power of HyperRouter could be considered better than prior work using dropout."
            }
        },
        "id": "jXIpUCIfua",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fL8AKDvELp",
        "replyto": "fL8AKDvELp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1522/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516308,
        "cdate": 1696707516308,
        "tmdate": 1701465433236,
        "mdate": 1701465433236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main contribution of this paper consists in the construction of a new manually annotated dataset and a benchmark for NER in Bengali for medical language processing. The experimental evaluation, though limited, appears to be sufficient for assessing quality and justifying the effort.  Reviewers and authors engaged in a constructive discussion, and the authors' responses clarified numerous points and show willingness to incorporate reviewers' suggestions into the next version of the paper. Reviewers finally converged on Soundness and two of found the paper exciting. Overall, this short paper can indeed be of great interest to the EMNLP community. \n\n**Pros** \n\n- coverage of a non-English, less-resourced language (Bengali); \n\n- coverage of a significant domain, biomedicine; \n\n- coverage of ordinary language usage, various linguistic styles, and regional dialects. \n\n- creation of the largest dataset available for Bengali, with and evaluation against SOTA for Bengali; \n\n- findings appear to be generalizable to other dialogical, informal contexts; \n\n- the paper is easy to follow and understand. \n\n \n\n**Cons** \n\n- as this paper focuses on a data resource, it lacks essential details regarding the annotation process and dataset composition properties, as pointed out by Reviewers V5Uk and NwnB. Nevertheless, the authors have shown awareness of these missing details, which will be included in the revised version; \n\n- The significance of the performance differences among the various models is not self-evident and would need a more thorough investigation. Authors however performed an extra test and provided the results in the discussion, which clarifies the point and should be included in the paper or appendix; \n\n- some background references need to be acknowledged (see reviewers' suggestions);"
            }
        },
        "id": "a8xExLdYME",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fK6N4R6TpF",
        "replyto": "fK6N4R6TpF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2061/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533382,
        "cdate": 1696707533382,
        "tmdate": 1701465452250,
        "mdate": 1701465452250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a disentanglement-based headline generation model for social media that balances content (i.e. capturing the main topic of the document) and contextual features (i.e. producing \"eye-catchy\" headlines). Overall, reviewers are quite positive about this work and I feel that the paper's strengths (e.g. sound and effective model, introduced dataset, application beyond social media) outweigh its weaknesses (e.g. lack of details about the proposed dataset, limited scope of the experiments). It should be noted that some of these concerns were discussed and, I believe, addressed by the author's responses. \n\nI would also like to see the potential ethical issues related to this work thoroughly discussed in the paper (i.e. misuse of the model for click-bait (#R zt61), sourcing of the REDBook dataset (#R r8GS and miJc))."
            }
        },
        "id": "VPrSQ0qv5K",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fG6zH1LBHE",
        "replyto": "fG6zH1LBHE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2421/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541746,
        "cdate": 1696707541746,
        "tmdate": 1701465464494,
        "mdate": 1701465464494,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Paper Topic And Main Contributions:\n* Introduction of a novel semantic parser for Wikidata, accompanied by a new dataset called WikiWebQuestions. This dataset contains semantic parses over Wikidata, providing a valuable resource for training and evaluating semantic parsers in the context of Wikidata.\n* The proposed semantic parser, WikiSP, is developed by fine-tuning LLama with Alpaca instruction tuning set and modified SPARQL queries. \n* Extensive evaluation on the proposed WikiWebQuestions dataset offers insights into the performance of the semantic parser. By highlighting different categories of errors, the evaluation provides a roadmap for future research and the development of improved techniques in semantic parsing for Wikidata.\n* In addition to WikiSP, the paper introduces a shallow integration of WikiSP with GPT3. The semantic parser initially responds to user queries, and its outputs are subsequently supplemented and backed up with responses from GPT3, providing a two-step approach to generate more comprehensive and accurate answers.\n\nReasons to accept:\n* Introduction of a new dataset, WikiWeb-Questions, which is a high-quality knowledge base question answering benchmark for Wikidata.\n* The paper presents the first effective few-shot sequence-to-sequence semantic parser for Wikidata.\n* The proposed methodology reduces the hallucination of large language models like GPT-3 by grounding it with a semantic parser for Wikidata.\n\nReasons to reject:\n* This paper only conducts experiments on a single dataset (self-collected), but other KBQA datasets exist (e.g., WebQuestions, GrailQA). The authors address this in the rebuttal by adding experiments on QALD-7. They also clarify that they focus on Wikidata."
            }
        },
        "id": "7t9HX5MBxk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fEuslEGN0j",
        "replyto": "fEuslEGN0j",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4618/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594503,
        "cdate": 1696707594503,
        "tmdate": 1701465536861,
        "mdate": 1701465536861,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a vision-language black-box adversarial attack method called Sparse Multimodal Attack (SparseMA). The SparseMA discretizes the input image and performs attacks in the combined space of image patches and input tokens. The effectiveness of SparseMA has been evaluated in comparison with three baseline methods across three distinct datasets. Compared to a baseline that conducts attacks separately in image and text spaces, their approach demonstrates superior performance.\nIn summary, all reviewers agreed that this paper focuses on black-box multimodal attacks, which is an interesting and timely research topic. The paper is well-written with a clear description of how the framework  and algorithmic are constructed. Their approach performs attacks in the joint space of image patches and text tokens, and achieves superior performance against a baseline that performs attacks separately in image space and text space. However, as three reviewers have pointed out, the effectiveness of the proposed method has not been sufficiently demonstrated, so ｍore experimental evaluations are needed to validate the contribution of this work. Specifically, the strong and significant differences with past adversarial works and the visibility of the perturbations, etc. There is a possibility that the paper will be accepted to findings."
            }
        },
        "id": "9VcLl3Hqsa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "fCvJrponuK",
        "replyto": "fCvJrponuK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4846/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599363,
        "cdate": 1696707599363,
        "tmdate": 1701465543258,
        "mdate": 1701465543258,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores the use of LLMs for zero-shot semantic parsing where the main idea is to adapt the formulation of the task (semantic parsing) into an (extractive or abstractive) QA problem, which is a very natural thing to do. The LLMs also need the capability to figure out when a specific slot is missing and the authors show how it is possible to teach the model (i.e., tune it) that functionality via using synthetic negative examples. All the reviewers have detected a good deal of strengths of the paper, and I align with the overall sentiment of the reviewers.\n\nThe results are reported on a single dataset: MTOP, and I agree that the main issue of the current work is the fact that it evalutes on a single dataset, a wider coverage of other related semantic parsing problems and datasets would make the main findings more generalisable and more impactful eventually."
            }
        },
        "id": "DFGQ0wQ3rh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "f7eqyX0nJP",
        "replyto": "f7eqyX0nJP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission830/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497085,
        "cdate": 1696707497085,
        "tmdate": 1701465412051,
        "mdate": 1701465412051,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The proposed method reformulate the query and retrieval the documents as references for better related work generation. The method is smart and useful. I personally respect the three reviewers' opinion, and suggest the paper accepted as Findings in the conference."
            }
        },
        "id": "VN8tV9mPC6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "f6S1411OlZ",
        "replyto": "f6S1411OlZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3455/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563097,
        "cdate": 1696707563097,
        "tmdate": 1701465498160,
        "mdate": 1701465498160,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces the use of planning in the form of question-answer pairs to guide visual story generation for more natural and grounded stories. On one hand, reviewers found that (1) the idea is interesting and well explained. The setup is clear and results on both automatic and human evaluations are good and well discussed. On the other hand, two reviewers commented that the impact of generating blueprints seems quite marginal as seen in Tables 2 and 3 (especially for fluency and coherence metrics). The authors did not agree on this, and argue that the model achieves better performance in terms of interestingness and grounding than VP-BART."
            }
        },
        "id": "0xId6Mfyf6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "f42iMss8J3",
        "replyto": "f42iMss8J3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3076/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555740,
        "cdate": 1696707555740,
        "tmdate": 1701465486365,
        "mdate": 1701465486365,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper provides a clear and compelling motivation to address a key challenge in GEC. \nIt presents interesting auxiliary tasks that could inspire further exploration. \nThe authors acknowledged that the proposed method does not have much technical novelty.\nHowever, the authors also indicate that the novelty of this paper comes from dataset ordering for training.\nThe empirical recommendation regarding training order is a valuable contribution, offering practical guidance to researchers and practitioners in the English GEC community on a topic that has not been previously investigated. \nThe paper also achieves strong results with a smaller model and includes a simple and reproducible method. \nThe experimental results show that the proposed method outperforms the current state-of-the-art performance.\nThe potential weakness of this paper is the lack of a thorough analysis and definitive methodology for determining the training order. \nFor example, the authors mention noisy datasets should be placed earlier in the training sequence than clean ones. Still, no concrete way to classify datasets as loud or clean is out of focus in this paper. \n\nOverall, there seem to be some contributions to the community."
            }
        },
        "id": "AHYlLB2DEn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "f34v92a86l",
        "replyto": "f34v92a86l",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3113/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556419,
        "cdate": 1696707556419,
        "tmdate": 1701465487583,
        "mdate": 1701465487583,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers felt very positively about this paper, viewing the proposed benchmark (evaluating VLMs on particular linguistic constructions) as useful and challenging, with interesting and insightful analysis. Once concern raised by a few reviewers is the relatively limited number of pretrained models evaluated on this dataset; however the author response adequately addressed this by including a few more (generative) models. Given that the paper is already very clearly written, it should be easy for the authors to present these extra results and the promised additional qualitative analysis in the extra page in any camera-ready version to make the paper even stronger."
            }
        },
        "id": "L1nSi8SPwX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "f1y1tG5pAE",
        "replyto": "f1y1tG5pAE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4152/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584268,
        "cdate": 1696707584268,
        "tmdate": 1701465522101,
        "mdate": 1701465522101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "On the whole, reviewers thought that this was an interesting paper. They appreciated the sociolinguistic motivations for the focus/approach, and the ways that the proposed context representations could enrich our understanding of communities, beyond previous literature. \n\nThe reviewers had a few concerns about the methodological choices and paper framing. They felt that these concerns could be addressed via some writing changes, especially since the authors started to provided clarifications in their response. I’ve highlighted some main suggestions, and I encourage the authors to attend to these suggestions — as well as others that the reviewers raised — as they revise their paper.\n* Reviewers found the choice of focusing on intensifiers somewhat arbitrary. It would be great if they could elaborate on/foreground the point briefly made, that controlling for grammatical variation is important, and if they could include the analysis with the full set of markers presently included in their response to reviewers. In particular, reviewers feel that including the additional analyses would greatly improve the soundness of the paper.\n* Reviewers note that the additional explanatory power offered by the proposed representations is a strength of the paper, and ought to be emphasized. For instance, one reviewer suggests emphasizing S6, relating stance to behavior, over S5 — in the interests of showing the reader examples of _how_ the representations could be used in other analyses. \n* More work could be done to validate the representations, especially with human judgements."
            }
        },
        "id": "phG9BrwLjC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "f10SqktqkF",
        "replyto": "f10SqktqkF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4202/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585315,
        "cdate": 1696707585315,
        "tmdate": 1701465524032,
        "mdate": 1701465524032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the OOD problem in generation (machine translation and response generation in dialogue). The paper propose an evaluation setting, called LOFTER, and show that their proposed method outperforms competitive baselines. In the reviews, There have been concerns about (i) comparison with stronger baselines on established OOD benchmarks on the classification task, (ii) the validity and relevance of the OOD problem in the LLM era, (iii) the marginal improvement of the proposed method in some settings. The authors have addressed these concerns, and by revising the paper according to the discussions, the paper seems to be ready for publication."
            }
        },
        "id": "E8ARdhFz8p",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "f05z3XqUeu",
        "replyto": "f05z3XqUeu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2046/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532967,
        "cdate": 1696707532967,
        "tmdate": 1701465451684,
        "mdate": 1701465451684,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a new approach for hypernym discovery in Chinese and a new dataset for evaluation. \n\nReviewers mrHG and rkKb call out the dataset as an important contribution and all three reviewers acknowledge the performance improvements that the proposed method achieves.\n\nThe majority of issues raised by the reviewers come down to missing details in the manuscript, and other presentation issues. From the extensive and constructive discussions (especially with Reviewers mrHG and rkKb) it appears that the authors were able to address all these concerns, either with additional experiments or with clarifications. An additional issue raised by Reviewer rkKb regarding a missing simpler baseline was similarly addressed by the authors. \n\nWhile the revisions required by the authors would be relatively extensive, the underlying soundness of the work is high, as evidenced by the increased scores by the reviewers."
            }
        },
        "id": "X1j1lye0gz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eyuTFB2CBM",
        "replyto": "eyuTFB2CBM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3709/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707568042,
        "cdate": 1696707568042,
        "tmdate": 1701465507215,
        "mdate": 1701465507215,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Given the growing dependence in society on LMs, assurance of their safety is a pressing question for our field. The authors tackle this problem with an automated red-teaming methodology, providing a coherent means for generating tests and an intriguing sets of results of how LMs profile across the safety landscape defined by their work.\n\nThe reviewers recognize the importance of the studied problem, the clarity of the author's work in addressing it, and reasonable agreement on the level of rigor/thoroughness. Multiple reviewers highlight concerns with the clarity/exposition on targeted bootstrapping, which I second based on my reading, and the author's rebuttal partially alleviates. I would recommend this be a clear area of focus if the paper is accepted. Additionally, two reviewers raise questions on the substance/importance of the findings, which I agree with, though I believe is partially blunted by the paper's substantial contributions methodologically. \n\nPut together, I feel the paper makes meaningful contributions to an important area, but could be strengthened both in terms of technical presentation (i.e. targeted bootstrapping), questioning of the core premise (i.e. clarifying if there is anything sacrificed by so exclusively focusing on automated evaluations), and relevance (i.e. how much do the findings really change what we know and don't know as a field about LM safety)."
            }
        },
        "id": "bH4dNdK1q7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ewedHtUI5X",
        "replyto": "ewedHtUI5X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2458/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542687,
        "cdate": 1696707542687,
        "tmdate": 1701465465885,
        "mdate": 1701465465885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a method called Diagonal Attention Pooling, an unsupervised way to improve sentence embedding quality. The reviewers converge toward excitement, posing only minor issues regarding clarification and suggestions for future work."
            }
        },
        "id": "HHhLkHBI9n",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ev8dLLwScW",
        "replyto": "ev8dLLwScW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1619/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707520437,
        "cdate": 1696707520437,
        "tmdate": 1701465436333,
        "mdate": 1701465436333,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposed a co-prediction prompt tuning method for the entity typing task. The method could correct the noisy labels through masked token prediction. The experimental results on three datasets showed that the proposed model mostly outperformed existing work.\n\nStrength: The proposed \"co-predict\" mechanism is interesting and somewhat novel. The whole paper is well-organized and clear.\n\nWeakness: 1) Actually, BERT is also based on the masked token prediction. So it is not very clear whether BERT can actually correct human-annotated labels, which should be investigated further. 2) co-prediction prompt is similar to contrastive learning. The authors should explain their differences through illustration or experimental comparisons. 3) More experimental analyses/discussions are needed."
            }
        },
        "id": "uOHdjjshpr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "euYA3EmI0e",
        "replyto": "euYA3EmI0e",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission639/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492693,
        "cdate": 1696707492693,
        "tmdate": 1701465406067,
        "mdate": 1701465406067,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper \"Co^2PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning\" presents work on debiasing through prompting  and testing this method based on three down-stream tasks.\n\nPoints mentioned for rejecting the paper refer to the experimental setup. \nArguments for the paper is the clearly written and detailed information given, but also the approach taken."
            }
        },
        "id": "jBHAGMeWiZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "erorKQYQ7P",
        "replyto": "erorKQYQ7P",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4378/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588525,
        "cdate": 1696707588525,
        "tmdate": 1701465529782,
        "mdate": 1701465529782,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a new benchmark for evaluating the robustness of OIE systems to distributional drifts surfaced as syntactic structure variations of semantically similar statements. \n\nThe reviewers unanimously acknowledge the importance of the proposed resource, the quality of the dataset construction and the in-depth analysis of the experiments. Reviewers 59er and 2rx1 raised some issues and posed some interesting questions that were successfully addressed by the authors and the clarifications will further enhance the manuscript."
            }
        },
        "id": "JCoGKsAjGo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eiHT1VAs4K",
        "replyto": "eiHT1VAs4K",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5016/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707604931,
        "cdate": 1696707604931,
        "tmdate": 1701465547446,
        "mdate": 1701465547446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work presents a dataset of Wikipedia editor discussions for policy detection and stance detection in three languages, along with models for the two tasks and a joint task. \n\nReviewers appreciated addressing the important issue of content moderation with a focus on transparency. Also, the multilingual dataset and models released will be useful for future research. \n\nThe reviewers share a sentiment that a deeper analysis of the dataset from a linguistic perspective can be beneficial, shedding light on linguistic characteristics of different classes and confirming the quality of the dataset in the process. Also, the focus on the Wikipedia editor discussions can be a weakness as it is hard to confirm generalizability of the findings."
            }
        },
        "id": "gojimNCjOj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eiFRPhpsW6",
        "replyto": "eiFRPhpsW6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3474/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563493,
        "cdate": 1696707563493,
        "tmdate": 1701465498737,
        "mdate": 1701465498737,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper works on Early-exit based strategy to accelerate the model. Three main improvements: 1) shallow-deep module 2) synchronized parallel decoding 3) threshold estimation without valid dataset.\n\nPros:\n\n1. Well organized and extensive experiments to validate the methods.\n\n2. Provide a very good overview over Early-exit based methods. \n\n3. Observations on the existing early-exit methods are insightful. It could potentially benefit others to further improve this line of reserach.\n\n4. The threshold estimation is novel.\n\nCons:\n\nSection 5.2 is not well written. There is no math formula or enough English explanation on how the parallel decoding works. Figure 4 itself is not enough for readers to quickly understand."
            }
        },
        "id": "ru1yyt9O4p",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eeP1y7zPQ7",
        "replyto": "eeP1y7zPQ7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4296/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586916,
        "cdate": 1696707586916,
        "tmdate": 1701465526944,
        "mdate": 1701465526944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a survey on end-to-end task-oriented dialogue systems. The survey stands out due to the following features:\n\n1. It is the first survey to comprehensively summarize efforts in this field.\n2. It introduces a new taxonomy: modular ETOD and Fully ETOD.\n3. The inclusion of resources such as a public website and leadership board.\n4. A discussion on frontier trends in the field.\n\nThe soundness scores were recorded as (4, 4, 3). All reviewers found the paper well-written and easy to follow. The retrospection and analysis of prior work are both detailed and comprehensive. The fresh taxonomy is lucid, and the provision of resources like a public website and leadership board can potentially enhance visibility, transparency, and traction in the domain.\n\nThe excitement scores were (4, 3, 3). Despite its strengths, the work has a few areas of improvement:\n\n1. The challenges associated with evaluations are not sufficiently addressed.\n2. While Large Language Models (LLMs) are gaining prominence in NLP research, their significance isn't thoroughly discussed.\n\nIn conclusion, this work is **Sound and Moderately Exciting**."
            }
        },
        "id": "CL2DpNg4Tf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "edwSiVzFpU",
        "replyto": "edwSiVzFpU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1240/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508191,
        "cdate": 1696707508191,
        "tmdate": 1701465424830,
        "mdate": 1701465424830,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper improves upon prior work that aggregates over multiple chain-of-thought outputs to generate a final answer. The idea is to use all reasoning chains and have the LLM use all of them as context to predict the answer. Experiments on seven datasets confirm the effectiveness of this approach. The reviewers found the paper convincing and well-written."
            }
        },
        "id": "VS5tlSvyop",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ebSOK1nV2r",
        "replyto": "ebSOK1nV2r",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1505/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515970,
        "cdate": 1696707515970,
        "tmdate": 1701465432807,
        "mdate": 1701465432807,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a metric that attempts to identify errors in generated text using LLaMA-7B. The authors show that their metric works across a variety of tasks and also validate with human judgments.\n\nReviewers uTpq and p5F9 were both impressed with the idea of using GPT-4 to help fine-tune LLaMA. Reviewer uTpq also points out that there's very nice work to show the alignment with human judgments. In the rebuttal the authors allayed some of the issues that the reviewers had by explicitly stating that they will clarify the pipeline and include the refinement table."
            }
        },
        "id": "9VVW9Dj5L8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eaUi1mcvrM",
        "replyto": "eaUi1mcvrM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2729/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548301,
        "cdate": 1696707548301,
        "tmdate": 1701465474569,
        "mdate": 1701465474569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a multi-level contrastive learning framework for script-based character understanding. They utilized summary of the script and built a summary-conversation contrastive loss. Experiments are exhaustive and the results are interesting. There are some aspects that need further attention: novelty of the proposed contrastive loss based model; and detailed qualitative explanation."
            }
        },
        "id": "y8oa4IO665",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eXV8sdO5HL",
        "replyto": "eXV8sdO5HL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2288/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538757,
        "cdate": 1696707538757,
        "tmdate": 1701465460165,
        "mdate": 1701465460165,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a data augmentation method for Korean by blending lexical and functional morphemes together to generate new forms.\n\nThe reviewers found the proposed method intuitive from a linguistic point of view and acknowledged its effectiveness on several tasks. In the initial round of reviews, the reviewers identified various weaknesses, ranging from the impact of the chosen morphological analyzer (mecab-ko) over general comprehension issues to lack of comparison with existing methods. The authors were able to engage in discussions with all three reviewers and provided additional results that, if included in the final version, would make the paper significantly stronger. All reviewers increased their scores as a result of the discussion with the authors."
            }
        },
        "id": "Pa1APUgTpV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eWW0KQhsHe",
        "replyto": "eWW0KQhsHe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission672/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493419,
        "cdate": 1696707493419,
        "tmdate": 1701465407183,
        "mdate": 1701465407183,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new method for automatically evaluating and predicting the winning stance in professional argumentative debates. The key problem it addresses is performing debate analysis and evaluation on complex natural language arguments, which have been relatively unexplored. \n\nThe reviewers agree that the [hybrid] approach is very interesting and the results are strong. In addition, the paper was well-written and clear, with a generally strong literature review. The authors provide a clear error analysis which helps support the results and conclusions nicely. The hybrid approach is also technically sound and might provide inspiration for additional NLP tasks."
            }
        },
        "id": "CXhpFUPOli",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eTDs4UY52h",
        "replyto": "eTDs4UY52h",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission71/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478420,
        "cdate": 1696707478420,
        "tmdate": 1701465386117,
        "mdate": 1701465386117,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a new model which uses Hierarchical context for the task of MDS. The hierarchical schema is designed not only for encoder, where one reviewer pointed with limited novelty, but also for decoder which is interesting. Based on the performance, almost all the reviewers agree that the method competes well with the best-performing models in the field, including those with additional MDS pre-training or with more model parameters. Therefore, I would suggest the paper as Findings in the conference, or  Main conference is also acceptable."
            }
        },
        "id": "RrRmwOT8U6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eSM4RWpuJF",
        "replyto": "eSM4RWpuJF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2631/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546321,
        "cdate": 1696707546321,
        "tmdate": 1701465471483,
        "mdate": 1701465471483,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper looks at the task of generalization and  domain adaptation borrowing ideas from previous work (Universal Domain Adaptation) from computer vision, and applying them to NLP datasets. The paper presents benchmark tasks, alternate metrics, and empirically evaluate the ideas of Universal Domain Adaptation on the benchmarks and find them to transfer well.\n\nThe reviewers have rated the paper strong on soundness but excitement scores vary. The authors address some of the concerns of reviewers in terms of limited baselines, and In particular reviewers have noted several limitations of the work: namely that it is confined to classification tasks, limited model size/architecture, and is missing some zero-shot baselines from large language models which would have been appropriate for this topic. Nevertheless the experiments are mostly sound even if missing interesting comparisons."
            }
        },
        "id": "xXc6pgA9lE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eNu9odz1sz",
        "replyto": "eNu9odz1sz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2750/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548593,
        "cdate": 1696707548593,
        "tmdate": 1701465474981,
        "mdate": 1701465474981,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an approach to align large language models (LLMs) to individual users by modelling the user's demographics, ideology, and past opinions. The key idea is to go beyond just using demographics or ideology and to also leverage the user's previous stance on issues as a richer signal for personalization. The authors conduct experiments on the OpinionQA dataset using different LLM variants and show that modelling all factors - demographics, ideology, and past opinions - leads to accuracy gains in predicting user responses.\n\nThe reviewers raise important points about the generalizability of the findings beyond this specific dataset and whether other user factors could further enhance personalization. In their rebuttal, the authors provide satisfactory responses, conducting additional experiments with BERT to demonstrate consistent trends. The authors also thoughtfully discuss how their approach provides a scaffolding for generating personalized responses in future work, albeit requiring large-scale user studies.\n\nReviewer 2 suggests encoder-based models may be sufficient for the task framed as classification, while the authors clarify the benefits of studying decoder models and few-shot prompting. Reviewer 3 recommends rejection based on anonymization rules, which is an important issue that needs resolution.\n\nOverall, this appears to be a novel contribution demonstrating the utility of modelling opinions in addition to standard user factors for alignment. Furthermore, the paper tackles an important problem of personalizing large language models (LLMs) to individual users. Aligning LLMs to unique user perspectives, beyond broad demographic or ideological groups, can enable more accurate and natural conversational AI. \n\nI think the core idea is sound and I agree with the reviewers that the experiments are reproducible, and results are likely generalizable beyond this dataset based on additional analyses.  The core contribution is demonstrating that modelling a user's past opinions is a critical signal, on par with or even more informative than standard factors like demographics and ideologies. This insight could reframe debates in social sciences on whether ideologies alone capture individual variance. The authors thoroughly analyze different prompting combinations on multiple LLMs using the OpinionQA dataset. The results clearly show gains from adding past opinions to user profiles, with the full model reaching up to 7% higher accuracy. Additional BERT experiments confirm the consistency of these trends.\n\nI think the paper is technically sound, with reproducible experiments and sufficient analysis, especially since the framing and motivation situate the work well within active discussions on model personalization in the NLP community.\n\nIn conclusion, the work provides valuable insights into modelling different user factors, laying the groundwork for personalized applications."
            }
        },
        "id": "JAZKkgDMJD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eHqrdft1wn",
        "replyto": "eHqrdft1wn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission818/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496718,
        "cdate": 1696707496718,
        "tmdate": 1701465411569,
        "mdate": 1701465411569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Work is dedicated to end-to-end speech translation and proposes a framework (CCSRD) that disentangle linguistic content from non-linguistic content in the speech representations. The motivation for learning such disentangled representations is clear and method proposed is interesting. The method is evaluated on a speech2text translation task (MustC) and it achieves an improvement compared to baselines. However, reviewers highlighted the lack of detailed analysis of the disentanglement module and not enough comparisons with other approaches. Moreover, it is unfortunate that the work is focusing only on speech-to-text translation task and does not use the ‘non-content representation’ during inference as well (for instance for a S2S task or a speaker id task or emotion recognition task). As the non-content representation is unused, the paper sounds a bit incomplete and i’d recommend only an acceptance to findings."
            }
        },
        "id": "RLjkdEtIoY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eGNwWBfqqs",
        "replyto": "eGNwWBfqqs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3160/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557241,
        "cdate": 1696707557241,
        "tmdate": 1701465488784,
        "mdate": 1701465488784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper has got mixed reviews and has also resulted in quite a long and detailed rebuttal.\nThe content of the paper is, no doubt, interesting and forms a solid short paper which can spur a lot of discussions during the conference (which is always encouraging). Despite having some flaws in the experiment setup, this paper should still make it to the conference.\n\nNow coming to the ethical side of things, the paper has also raised some ethical concerns, and the ethics meta-reviewer has listed down a set of points that the paper must address before it can be published. I remind the authors to carefully take all these into account to prepare the camera-ready version."
            }
        },
        "id": "MvrPmsQBIO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eFnBXtZXIH",
        "replyto": "eFnBXtZXIH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5742/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617720,
        "cdate": 1696707617720,
        "tmdate": 1701465566134,
        "mdate": 1701465566134,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a two-stage contrastive learning based slot labeling approach to solve the language-domain-task adaptation challenge   for task-oriented dialogue system development. Reviewers agree that the work is well-motivated and achieves significant improvements in low-resource language scenarios. Several questions regarding the evaluation on truly low-resource languages, and comparison with seq2seq model were sufficiently addressed during rebuttal by authors."
            }
        },
        "id": "Lauime6Jgf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eEV5S2EIp9",
        "replyto": "eEV5S2EIp9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2177/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536285,
        "cdate": 1696707536285,
        "tmdate": 1701465456794,
        "mdate": 1701465456794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a new method for personalized dialogue generation called MIRACLE, which disentangles complex personalities into three attributes (language style, attitude, and mental characteristic) and utilizes an energy-based model(EBM) for generation. The paper also proposes two new loss terms that can make personalized text generation better (aspect classification loss and attribute distance loss). \n\nThe paper has demonstrated many merits but can still be largely improved by addressing the following issues:\n1. The observed improvement in coherence is incremental.\n2. An in-depth analysis of the synthetic data. Do these data have sufficient diversity? Can they capture the characteristics of human dialogues well?\n3. It would be nice to add the results of LLama, will the proposed approach still be much more efficient than LLama?\n4. Authors are encouraged to release their code, model checkpoints, and datasets generated.|meta review"
            }
        },
        "id": "49fUxIqRcx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eCXfUq3RDf",
        "replyto": "eCXfUq3RDf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1807/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527593,
        "cdate": 1696707527593,
        "tmdate": 1701465442394,
        "mdate": 1701465442394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores the concept of automatic glossing. The authors leverage not only the morpheme sequence of the target sentence but also its translation to determine the appropriate glossing. The authors introduce a latent variable Conditional Random Field (CRF) model, incorporating traditional symbol-based features. The authors place particular emphasis on addressing low-resource scenarios. They assert that their CRF model outperforms neural models.\n\nThis paper is well-written and the experiments were consistent with the research objectives. While it would not reach a large audience, this paper is worthy of presentation at the main conference."
            }
        },
        "id": "YakhFsGxpw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "eBUgomB8uo",
        "replyto": "eBUgomB8uo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3489/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563815,
        "cdate": 1696707563815,
        "tmdate": 1701465499100,
        "mdate": 1701465499100,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Pros:\n- Several of the results are strong.\n- The method is simple and intuitive, a divide-and-conquer algorithm integrated into a Transformer.\n- The approach is well motivated for the specific problem addressed, that of handling regular languages.\n- The discussion is in depth and enjoyable to read.\n\nCons:\n- The motivation of handling regular languages is somewhat niche.\n- It is not clear why one would use a Transformer variant rather than an RNN or other existing model with a stronger inductive bias for regular languages."
            }
        },
        "id": "lWSDcyTk9Y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "e8wYLib8HC",
        "replyto": "e8wYLib8HC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission410/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486857,
        "cdate": 1696707486857,
        "tmdate": 1701465397942,
        "mdate": 1701465397942,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new approach utilizing LLMs for the generation and refinement of query rewrites, addressing a critical limitation inherent in human-generated rewrites – the absence of contextual information required for optimal retrieval in conversational search. Leveraging the instruction-following capabilities of LLMs, this research delineates a set of highly desirable attributes for query rewrites.\nConsidering that LLMs can face challenges when confronted with intricate tasks, this paper also introduces a \"rewrite-then-edit\" process.\n\nAlthough it is essential to underscore the potential impact of this method on the field of conversational search, some reviewers have expressed concerns about the paper's novelty."
            }
        },
        "id": "JsNSHyPGG9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "e8jvAr4Aaj",
        "replyto": "e8jvAr4Aaj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4177/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584786,
        "cdate": 1696707584786,
        "tmdate": 1701465523183,
        "mdate": 1701465523183,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Authors presents a detailed analysis of performance of several state-of-the-art Large Language Models on Mental Health analysis tasks, especially mental health disorder identification and cause behind the result). Authors perform thorough evaluations under various prompting strategies, human and automated evaluations, with explanations evaluated by human experts. Most reviewers agree about the technical soundness of the paper and at least 3 found it exciting. Some reviewers found issues withe evaluations, expert choice, and lack of solutions to the cause of errors presented. Most of the comments have been address by the response.\n\nI agree that, the \"why\" behind inaccurate predictions and \"incorrect reasoning\" (either qualitatively or quantitatively) is interesting. I will recommend authors to include this in the camera-ready."
            }
        },
        "id": "avkE7R30oB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "e5UzmaR8EE",
        "replyto": "e5UzmaR8EE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1028/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503090,
        "cdate": 1696707503090,
        "tmdate": 1701465418225,
        "mdate": 1701465418225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a novel framework called Automated Explainable Student Response Assessment (AERA), which is capable of distilling ChatGPT as a reasoning teacher to fine-tune a smaller language model, generating score rationales, and improving the interpretability on student answer assessment tasks. The proposed method consists of three steps: (1) generating rationales using various prompt templates with ChatGPT, (2) refining output explanations, and (3) fine-tuning a Long T5 model on these refined explanations and labels. In addition, as part of this process, the quality of rationales is enhanced with a rationale refinement module. The authors run extensive experiments, which show that a smaller language model can surpass ChatGPT in terms of assessment performance while generating more accurate rationales to explain the assessment decisions. Specifically, the proposed method improves the overall Quadratic Weighted Kappa (QWK) score by 11% compared to ChatGPT alone, and a human evaluation confirms that the rationales generated by the new method are of comparable quality to those produced by ChatGPT.\n\nAll reviewers highlight **a number of strengths and merits** of this paper:\n1. *Motivation*: Reviewer JJVy mentions that the paper focuses on an important and a long-standing application of NLP. In addition, Reviewer PxdN believes that the proposed framework has the potential to influence the education field by making student answer assessments more efficient, consistent, and interpretable.\n2. *Methodology*: Reviewer JJVy finds the proposed distillation method simple but effective and the rationale refinement module interesting. Reviewer FgJs agrees with this view.\n3. *Evaluation*: Reviewers JJVy and FgJs also find ablation studies useful and informative. \n4. *Clarity of writing and presentation* is listed among the reasons to accept by Reviewer PxdN.\n\nAt the same time, reviewers identify **further weaknesses of the current submission** and areas for improvement: \n1. *Concerns about the choice of the dataset* are expressed by reviewers JJVy and PxdN (please see their reviews for more details). While the ideal dataset for this task may not yet exist, these concerns should be addressed in the revision.\n2. *Concerns about the experimental setup*: Reviewer JJVy has posed a number of questions about the setup and comparison of the proposed model to the baselines. The authors have tried to address some of these concerns – these answers should be incorporated into the revision. For instance, there does seem to be a trade-off between the performance of the models and their ability to provide rationales, and while it is the latter that was the main focus of this work, this trade-off should be addressed in the paper. This may require comparison to more baselines. In addition, presence of spurious correlations, which contribute to certain baselines' performance, is currently hypothesized in the paper – this needs to be supported by stronger evidence and possibly additional experiments. Please also see further suggestions on the experimental setup from Reviewer PxdN.\n3. *Concerns about the use of ChatGPT* are expressed, to various extent, by all reviewers. Reviewer FgJs, in particular, suggests running experiments with open-source LLMs for replicability and reproducibility. The question of generalizability of the findings is also raised by more than one reviewer.\n4. *Other suggestions on the presentation of evaluation results* (such as inclusion of significance testing) are expressed by Reviewer FgJs.\n5. *Missing references* are also suggested by Reviewer JJVy."
            }
        },
        "id": "2RVJZ2qqre",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "e4m1Gu6rVP",
        "replyto": "e4m1Gu6rVP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1034/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503371,
        "cdate": 1696707503371,
        "tmdate": 1701465418476,
        "mdate": 1701465418476,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses two issues with the training data used in Grammatical Error Correction (GEC) – namely, errors in annotation and the lack of annotation diversity. The paper proposes an approach that uses a weighting schema both on the token and sentence level to address these issues. Experiments on English benchmarks with the frameworks of seq2seq and seq2edit modeling paradigms demonstrate improvements over the baseline models. \n\nThe proposed method is interesting and the experimental results demonstrate the utility of the approach. The paper can be significantly strengthened by presenting experimental results on other languages. The paper should also make it clear that the obtained results are not SOTA. If larger models (such as T5-xxl) cannot be used due to their computational cost, the paper should nonetheless include those results (along with the SOTA results on the benchmarks)."
            }
        },
        "id": "ZVrCW5hqtd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "e4dXIBRQ9u",
        "replyto": "e4dXIBRQ9u",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission934/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499704,
        "cdate": 1696707499704,
        "tmdate": 1701465415326,
        "mdate": 1701465415326,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper under consideration presents an innovative approach to understanding and generating grapheme-color synesthetic experiences. It introduces a comprehensive annotation framework and a novel model for this purpose. Overall, the work is well-structured, motivated, and addresses an important topic in synesthesia research. It makes significant contributions to the field, but there are also some potential concerns that need to be addressed.\n\nPros:\n\n- The paper introduces a novel annotation framework that includes sensory modalities, cues, and stimuli, providing a holistic understanding of synesthesia.\n- The proposed model effectively captures the interdependencies between synesthetic elements, enhancing comprehension accuracy.\n- The expansion of the dataset through social media data collection contributes to a more diverse and robust dataset.\n- The experimental results clearly demonstrate the superiority of the proposed model over baseline methods, providing a comprehensive view of its performance.\n\nCons:\n\n- The comprehensive annotation framework may introduce complexity and subjectivity. The paper lacks information on how inter-annotator disagreements were resolved and how consistency was ensured.\n- The study focuses on Chinese synesthesia, and the generalizability of the proposed framework to other languages with different structures is unclear. Moreover, the title does not specify that the study focusses on Chinese only\n- The paper acknowledges that understanding synesthesia involves cognitive efforts. A comparison of the proposed model's performance to human performance in identifying and generating synesthetic elements is missing.\n- Section 4.3 could be improved for better readability, and the authors should clarify their plans for making the dataset publicly available to foster research.\n- While the paper is well-structured, the use of T5 for synesthesia generation might be considered less innovative in comparison to other generative models.\n\nIn summary, the paper makes notable contributions to synesthesia research with its comprehensive framework and model, but it should address concerns related to subjectivity in annotation, cross-lingual adaptability, and comparisons to human performance. Additionally, improvements in readability and dataset availability are recommended.\n\nIn the rebuttals, the authors were able to provide access to the data and explain many of the issues raised by the reviewers, yet the paper would need some substantial edits."
            }
        },
        "id": "fz20f9YF5N",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "e3gXrvjGys",
        "replyto": "e3gXrvjGys",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission350/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485438,
        "cdate": 1696707485438,
        "tmdate": 1701465396058,
        "mdate": 1701465396058,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper targets the problem of large action space and the delayed reward problem when RL is used for conditional text generation. A thorough evaluation is conducted. A common concern of reviewers is of human evaluation which authors agree to include in the final version of the paper. All the other experiments using different metrics and comparisons with other baselines are provided during rebuttal."
            }
        },
        "id": "iXNMmVb5PU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "e3eIqCPCT9",
        "replyto": "e3eIqCPCT9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5721/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617375,
        "cdate": 1696707617375,
        "tmdate": 1701465565671,
        "mdate": 1701465565671,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree that the notion of domain privacy of general-purpose language models introduced in this paper is a novel and interesting idea.  Also the solid experimental design for evaluating privacy leakage and the results are appreciated.  One important concern emerging from the reviews and discussion is that the authors should take care to clearly formulate the scope of what the work is aiming to do.  The current title suggests too wide a scope to be justified by the work, and therefore the alternative title brought forward by the authors themselves in their rebuttal seems reasonable."
            }
        },
        "id": "rczPuVQgCt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "e2B31gDhnj",
        "replyto": "e2B31gDhnj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3697/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567829,
        "cdate": 1696707567829,
        "tmdate": 1701465506970,
        "mdate": 1701465506970,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Most reviewers noted the paper was well written, intuitive and novel/interesting. All of the reviewers had concerns about various experimental design choices not being justified, and one reviewer cited concerns about unfairly comparing the results, but rated the overall soundness of the paper as \"good\". Nonetheless, after the rebuttal, all of the reviewers found the paper to be adequately sound (\"Good\")."
            }
        },
        "id": "WkDKVsjXV4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dxCviFd7rj",
        "replyto": "dxCviFd7rj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1231/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507926,
        "cdate": 1696707507926,
        "tmdate": 1701465424502,
        "mdate": 1701465424502,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes an unsupervised approach for scientific abstract segmentation, and all the reviewers agree its soundness and give comprehensive reviews. The authors also response to the questions in detail. In addition, reviewers also have some concerns, e.g., efficiency. I strongly suggest that the authors make precise revisions based on the comments of the reviewers."
            }
        },
        "id": "eYLuGL1xGi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dwj886NUqy",
        "replyto": "dwj886NUqy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission989/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707501415,
        "cdate": 1696707501415,
        "tmdate": 1701465417040,
        "mdate": 1701465417040,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper explores structured pruning techniques for encoder-decoder transformer-based pre-trained models and uncovers novel insights into pruning these models.\n\n**Pros**:\n- The approach is well-grounded, as it begins with preliminary experiments to assess the significance of each module in terms of performance and inference speedup.\n- The proposed approach outperforms merely applying previous methods without thoughtful adaptation.\n- The experiments conducted are thorough and comprehensive, encompassing a range of tasks, including standard fine-tuning tasks and instruction tuning.\n- In summary, I believe this work lays a strong foundation for further investigation into effective pruning approaches for encoder-decoder models.\n\n**Cons**:\n- Some reviewers express concerns regarding the novelty of the work, as it primarily involves making straightforward adaptations to prior research like CoFi."
            }
        },
        "id": "vs8GAkEOPO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dwGKBFXiy2",
        "replyto": "dwGKBFXiy2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2627/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546248,
        "cdate": 1696707546248,
        "tmdate": 1701465471298,
        "mdate": 1701465471298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "As a reviewer indicates The work proposes a technique named Generative Boosting Training (GBT) where additional synthetic data is generated for difficult cases (where model makes mistake in prediction) on the task of paraphrase identification (PI). Authors propose to augment paraphrase sentences that are different in text but have similar semantic meaning (referred to as IBH1) and sentences that have similar text but different semantic meaning (referred to as IBH0). The work shows improvement from on 2 PI datasets, PPQ and LCQMC.\n\n\nThe main reasons to accept the paper are the following ones:\n\n\tThe paper is well written and easy to follow.\n\tAll the source codes will be made publicly available to support reproducible research.\n   \tThe method seems promising in term of the efficiency, which is superior to the adversarial methods in this view.\n    \t Good motivation to solve the paraphrase identification, with current limitations, detailed examples and vivid diagrams\n    \tSolid experiment on both BERT and GPT, reaching good performance or even state-of-the-art compared to the baseline models.\n    \tExplore the detailed of such data augmented boosting training, including but no limited to, boosting time, boosting ratio, different language models, etc.\n    \tGBT is efficient since it can be completed 1-hour on GPU.\n\tThe GBT technique shows improvementand could be applied generally to other tasks as well. The negative and positive paraphrase augmentation seems like a good mix of balanced data augmentation (though we don’t know how much just adding IBH1 and just adding IBH0 independently contribute to improvement). The propose technique seems like a good choice for the PI task and some of the analyses (Figure 2 for boosting interval and ratio, table 5 for why to use GBT for hard examples) highlight interesting takeaways about GBT.\n\n\nThe main reasons to reject the paper are the following ones:\n\n   \t The soundness of mimicking the human learning process should be further discussed in detail. \n   \t More error ratio information and the corresponding examples should be analysed to support the motivation of the idea.\n\tThe advanced data augmentation methods in comparison are from 2018-2020. It is unclear that whether the paper is comparing with the stat-of-the-art counterparts.\n\tIt is not sure that whether part of the performance improvement can be attributed to the learned language ability from the seq2seq model via generated data (like the language model distillation), but not the GBT.\n\t There are potentially numerous baselines as data augmentation for hard examples has several work. Given the closeness with this proposed work of using paraphrases (both negative and positive), some of baselines are necessary for comparison with GBT, especially counterfactual data-augmentation techniques as GBT uses (negative paraphrases in DA i.e., IBH0) \n    \tSome other, even more simpler baselines could be lower learning rate and training for more number of epochs. This would even strengthen the claims of GBT if improvements are significant.\n   \t The ablation study in table 5 seemed more like good baselines which is good to have as it shows GBT is more effective when applied to only hard examples. A better ablation study could be giving just positive paraphrases (IBH1) and just negative paraphrases (IBH0)\n    \tSome of the important technical details are unclear. For example, which datasets and how was the paraphraser trained to generate candidate sentences for selecting IBH0 and IBH1. In line 268-277, more details would be needed as to how and where the 50K examples were selected from.\n    \tThe text in line 293-295 makes the above point a little bit more unclear. It would be difficult for readers to understand and evaluate – “we manually observed the generated examples and find the results acceptable.”\n   \n\nTwo reviewers are quite positive about the paper, one reviewer less, even after reading the answers provided by authors. \n\nIn sum,  despite the reasons to reject, the paper is well written, the results are reproducible, the techniques are interesting and the methods show improvements. Some clarifications should added if finally accepted."
            }
        },
        "id": "VxobdkojBc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dvDi1Oc2y7",
        "replyto": "dvDi1Oc2y7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission652/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493008,
        "cdate": 1696707493008,
        "tmdate": 1701465406595,
        "mdate": 1701465406595,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents an approach to address biases in semi-supervised learning within the context of crisis detection from tweets. While the paper has interesting ideas, there are also areas of concern that warrant attention before it can be considered for publication.\nReasons to Accept:\n* \t\tSolid Debiasing Method: The paper introduces a debiasing method, MemoryBank, which offers competitive performance in comparison to baselines. The simplicity of this approach is a merit, as it presents a viable solution for addressing biases in semi-supervised learning scenarios.\n* \t\tInteresting Domain Application: The choice of crisis detection from tweets as the application domain is noteworthy. This context offers a unique and compelling real-world problem, which adds value to the paper's contributions.\n* \t\tExploration of Semi-Supervised Learning in Imbalanced Data: The paper discusses semi-supervised learning with imbalanced datasets, shedding light on how the quantity of pseudo-labels can introduce bias. This exploration is interesting, given the practical challenges of handling imbalanced data in real-world applications.\n* \t\tComprehensive Experimental Evaluation: The paper includes a set of experiments that contributes to the understanding of the proposed method's effectiveness.\nReasons to Reject:\n* \t\tGeneralizability Concerns: The paper raises concerns about generalizability due to the use of small test and validation sets. It is crucial to demonstrate that the proposed method is effective beyond the specific dataset and domain tested.\n* \t\tLack of Cross-Domain Testing: While the method shows promise within the crisis detection domain, its applicability to other domains remains unexplored. A broader assessment of its effectiveness in diverse domains would strengthen the paper's contributions.\n* \t\tComparative Analysis Needs Strengthening: The comparative analysis between the proposed MemoryBank method and existing approaches appears to lack depth. A more thorough exploration of prior approaches for addressing imbalanced data and semi-supervised learning is essential to provide a comprehensive understanding of the proposed method's advantages and limitations.\n* \t\tNeeds Deeper Discussion of the Method: The MemoryBank method has lots of aspects very similar to standard under-sampling techniques with modifications to the training loop. The paper should clearly establish how MemoryBank significantly advances the state of the art in addressing bias in semi-supervised learning.\n* \t\tContradictions in Dataset Choice: The paper's choice of a balanced dataset appears contradictory to its main motivation, which is to address biases in semi-supervised models when classes are imbalanced. Using a more imbalanced dataset would better support the paper's claims and motivations. More discussions are needed on this aspect.\nIn conclusion, while the paper presents an interesting approach to address biases in semi-supervised learning within the crisis detection domain, there are concerns related to generalizability, comparative analysis, and the novelty of the approach that need to be addressed. All those aspects were discussed during the rebuttal phase and there was convergence on the discussions. I encourage the authors to carefully revise the paper considering the aspects discussed with the reviewers."
            }
        },
        "id": "VgvwBU7LFw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "du1t38uXPA",
        "replyto": "du1t38uXPA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2815/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550047,
        "cdate": 1696707550047,
        "tmdate": 1701465477470,
        "mdate": 1701465477470,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper describes the creation of a spoiler-to-episode matching dataset. Different from previous datasets which focused on spoiler detection as a binary task, this dataset offers the possibility to match datasets with specific episodes. This makes a novel and valuable contribution to the task of spoiler detection.\n\nI would suggest fleshing out the paper with extra details as suggested by reviewers, adding more evaluation metrics (precision and recall) and more details on how the data was sampled.\n\nI would also suggest to make the contribution of a novel dataset (not just a new task) more explicit in the introduction.\n\nDoes the current paper title really convey the new task / dataset that it proposes?"
            }
        },
        "id": "dBKztYPd30",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "drG2ScCe4C",
        "replyto": "drG2ScCe4C",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission325/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484798,
        "cdate": 1696707484798,
        "tmdate": 1701465394932,
        "mdate": 1701465394932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper delves into the efficacy of large language models (LLMs) in hate speech detection. Through a comprehensive analysis of two prominent LLMs (GPT 3.5 and text-davinci) across distinct datasets, the authors evaluate their performance and the impact of various context cues, such as explanations and targeted communities. The research unveils the models' inherent challenges in default configurations and introduces tailored prompt strategies for performance optimization. \n\nThe advantages of the paper are: 1) addresses an important and timely issue of detecting hate speech using large language models. 2) Presented comprehensive analysis over two commercially available LLMs 3) Identifies the vulnerabilities and limitations such as vulnerability to adversarial attacks. 4) Provide a good benchmark for follow up research\n\nSome concerns were also raised regarding 1) The definition of hate speech is not clear and result in ambiguity in the interpretation of the results 2) the technical detail of the approach is not clearly described (e.g. data preprocessing techniques, hyper-parameters ) 3) The experiment is only limited to two LLMs that form a same family (GPT3) ); In addition 4) the paper needs significant reformatting"
            }
        },
        "id": "SPLiFAgvAL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dq25TkeI1W",
        "replyto": "dq25TkeI1W",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2016/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532271,
        "cdate": 1696707532271,
        "tmdate": 1701465450907,
        "mdate": 1701465450907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper studies the use of machine-generated dialog-trajectory pairs for training symbolic plan prediction models in embodied trajectory-from-dialog tasks. The paper proposes a method to generate this data from the simulator, and finds that a plan-prediction transformer model trained on this data is on par with human-generated data.\n\nSome strengths cited by individual reviewers:\n“The paper solves an important problem of data scarcity in embodied AI research”. \n“Solid engineering work.”\nThe synthetic data generation pipeline is general with some modification\n“Thorough experiments and convincing results”\n\nOutstanding concerns after rebuttal:\n- Only looked at training transformers, did not try LLM prompting. My opinion: although LLM prompting is a very popular approach and deserves being looked at, it is also its own moving target that this paper did not aim to study.\n- Section describing data-generation was hard to read for reviewer zTkK, who is not in the area. Constructive feedback was discussed\n\nOverall, there is consensus that the paper is sound and has addressed the research question it sought to study. Reviewer YKo6 gave a soundness score of 4 with the highest confidence, while the other reviewers scored it as 3.\n\nExcitement scores are 4 from reviewer YKo6 (the most confident and well-matched reviewer), and 3 from everyone else."
            }
        },
        "id": "HHrzagqfcl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dpS5VxAwuF",
        "replyto": "dpS5VxAwuF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3930/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579622,
        "cdate": 1696707579622,
        "tmdate": 1701465514526,
        "mdate": 1701465514526,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a simple-to-complex progressive approach for event argument extraction and the complexities of events are determined by the probability of their generated arguments in the first step. The approach achieves better extraction performance but the improvements are relatively marginal."
            }
        },
        "id": "B0PZwNBgXB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dp9jTeKXec",
        "replyto": "dp9jTeKXec",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1424/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512912,
        "cdate": 1696707512912,
        "tmdate": 1701465430605,
        "mdate": 1701465430605,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The focus of the paper is the creation of a multimodal dataset for metaphor understanding. It’s main contributions are:  \n\n- a Chinese multimodal (images-texts) metaphor database from advertisements, annotated with metaphors, source and target domains of metaphors, and conveyed sentiments.  \n\n- a 4 domain lexicon; \n\n- benchmark results for metaphor detection, source and target identification, and conveyed sentiment identification.   \n\nReviewers generally appreciated the work but found some important weaknesses in the paper. The authors' responses were accurate and detailed, providing clarifications and the required details for many issues raised. While the required revisions are not substantial, they are indeed necessary (please refer to the Cons. section for specifics); anyways, the work is worthwhile.  \n\n**Pros** \n\n- the paper describes the creation of a high-quality Chinese multimodal metaphor dataset, which constitutes a valuable resource for future researchers in the field of multimodal metaphor understanding; \n\n- The extraction of data from creative advertisements and artistic works is an interestingly efficient method for creating useful data for metaphor understanding and yields valuable insights; \n\n-  the domain classification approach followed enhances comprehension of metaphorical expressions across diverse data forms, and seems to align with human cognitive patterns for problem-solving; \n\n- The paper is well-written, the argumentation easy to follow and understand; \n\n- It presents interesting benchmark results;  \n\n- the work entails substantial effort and resources. \n\n**Cons**  \n\n-  the paper leaves doubts about the choice of the level of granularity for identifying source and target domains. Some discussion seems to be needed in the paper, to help the reader understand the choices made; \n\n- the description of the annotation process lacks details esp. on the annotators’ background, apparently for anonymity reasons. It is recommended to include this in the next version of the paper; \n\n- a detailed analysis or discussion of the dataset and the annotation results is missing: e.g. analysis and discussion of disagreements, discussion on the observed properties of the multimodal metaphors; genre specific observations; \n\n- Most appendix material is in Chinese-only. Translation in English is required."
            }
        },
        "id": "yOm5rbrsYi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dnQI76LKQy",
        "replyto": "dnQI76LKQy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission584/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491315,
        "cdate": 1696707491315,
        "tmdate": 1701465404163,
        "mdate": 1701465404163,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel method for building pre-trained language models that are capable of handling markup language-based text and understanding the complex interplay between HTML structure, text and visual appearance, and layout. The idea behind the new approach is grounded in Gestalt laws of perception, which describe how humans perceive similar objects to form a group and how closely situated objects are also perceived to form a coherent group. The paper hypothesizes that this way of pre-training improves the model's ability to understand web page content, and the experiments confirm that this helps improve its performance on downstream tasks.\n\nAll reviewers agree that the paper has **a number of strengths and merits**. Specifically:\n1. *Novelty and theoretical as well as practical appeal of the proposed idea*: This strength is highlighted by all reviewers.\n2. *Overall clarity of writing*: Reviewer W8C9 points out that the paper is clear, well organized and well written.\n3. *Reproducibility* is noted by all reviewers, and Reviewer W8C9 lists the fact that the authors plan to publish the code and the model among their reasons to accept. \n4. In addition, Reviewer bTQ3 points out that *ablation studies* suggest that the proposed method is crucial to the improvement of the final performance of the model.\n\nAt the same time, all reviewers also identified **some weaknesses** and further areas for improvement, including:\n1. *Lack of meaningful comparison and further model's analysis*: Reviewer 6rWs points out that comparison of the loss or perplexity of the proposed model to MarkupLM or RoBERTa on which the model is based is missing. In addition, Reviewer 6rWs has further feedback on the comparison to GPT 3.5.\n2. *Evaluation and analysis of the results*: Reviewer 6rWs points out that it would be desirable for the paper to show improvements on a broader scale and using some more contemporary models. Furthermore, Reviewer W8C9 proposes improvements in terms of ablation studies and error analysis. Finally, Reviewer bTQ3 considers the performance gain to be relatively small compared to MarkupLM.\n3. *Further methodological explanations*: Reviewer 6rWs also suggests that the paper should provide more detailed explanation and / or examples of Gestalt principles.\n4. *Implementation details*: Finally, while formulated not as clear weaknesses, Reviewer W8C9 makes suggestions for further improvement and clarification of certain implementation details – see their review.\n\nThe authors did a good job addressing the concerns expressed and the questions posed by the reviewers. Most suggestions made by the reviewers have been acknowledged by the authors and seem to be likely to be integrated in the revised version of the paper."
            }
        },
        "id": "rIKPQFLO8J",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dnIfD7RJLU",
        "replyto": "dnIfD7RJLU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission437/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487680,
        "cdate": 1696707487680,
        "tmdate": 1701465398898,
        "mdate": 1701465398898,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary (adapted from Reviewer 9oC8): The paper proposes language identification datasets and models, specifically focused on covering low-resource languages and covering 1665 languages, which is an improvement over previous works.\n\nAs reviewers pointed out, there are a number of things missing from the paper to fully understand the experiments and resource construction. For example, the paper does not actually contain a precise list of the sources for dataset construction, and the decision rule was not clearly explained. These are well-addressed in the authors’ response.\n\nThere are substantial risks of dataset mislabeling and contamination involved in creating LID datasets. The authors address some of these issues in the paper, and others are addressed in the response period. Particularly, the “reasons to reject” raised by Reviewer 6ooP all received clear responses, and I do not believe that they represent soundness issues with the paper, although they do reflect some information that should have been provided in the original submission.\n\nThe authors must ensure that any clarifications and information they provided in the response period are added to the paper in revision, as well as the formatting requests (the appendix results are unreadable).\n\nThe paper includes a substantial analysis section which explores the performance of the models and discusses some quirks of the assembled data. This is a very useful contribution on top of the rest of the paper."
            }
        },
        "id": "FaP8vSoNx2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dl4e3EBz5j",
        "replyto": "dl4e3EBz5j",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2582/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545236,
        "cdate": 1696707545236,
        "tmdate": 1701465469832,
        "mdate": 1701465469832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose a method, LENS (fiLter-thEN-Search), for selecting and ordering 'support' examples used as part of an in-context learning (ICL) prompt in LLMs in an effort to maximize performance. Specifically, LENS, first filters the dataset to obtain informative in-context examples using a newly developed InfoScore metric followed by a diversity-guided example search is used to iteratively refine selected 'support' example permutations. Experimental results on eight widely used text classification datasets show that LENS significantly outperforms several ICL example selection baselines, with additional experiments addressing questions regarding sensitivity to example ordering, selection generalization across LLMs, the value of ground truth quality in support examples, and algorithm hyperparameter sensitivity.\n\n== Quality == \nOverall, the reviewers believe that this submission is of solid quality. Specifically, the method is conceptually well motivated and there are extensive experimental results in terms of baseline methods and additional experiments to better understand the dynamics of the proposed method. However, there were some recommendations regarding stronger experiments including larger foundational models (including instruction-tuned) additional recent applicable baseline methods, and tasks beyond text classification. Overall, the experiments support the stated premises and result in interesting findings to motivate further work.\n\n== Clarity == Overall, the paper is well-organized and easy to understand with the appendices adding clarity. All of the reviewers were confident that they would be able to reproduce the empirical results and though the evidence as presented was convincing. The only conceptual clarification was the use of the term \"support examples\" (which are specific to learning discriminative functions) vs. \"core set\" (which align more closely with representing example distributions) -- a technical distinction, but worth considering the precision of the methodological claims. Also, while not a concern of the reviewers, discussion of theoretical implications and contextualization of the algorithmic approximations would increase overall understanding.\n\n== Originality == While there has been work in selecting examples for ICL, the reviewers believed the specific approach to be novel and pragmatic (especially when considering algorithmic simplifications). This is a new area and there is the right balance of good conceptual motivation and heuristics to create a feasible implementation.\n\n== Significance == As stated above, this is a increasingly widely-studied area (example selection in ICL) where small improvements can have large impact. Thus, there is definitely potential for impact, minimally as a baseline for future work. That being said, there were several concerns that the reviewers thought may hinder wide adoption including only using GPT-2 scale models (thus, without instruction-tuning and knowing that these results scale to SotA settings), only performing static selection (which is a natural area for future research with a multi-stage method), and questions regarding computational resources needed to perform this in practice. Thus, while the reviewers concur that the method is likely useful, it isn't clear what the impact will be with stronger LLMs (and if this ends up being a negative result, the impact will clearly be limited)."
            }
        },
        "id": "ESz9YtMmUB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "djmjglxOZ7",
        "replyto": "djmjglxOZ7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission471/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488560,
        "cdate": 1696707488560,
        "tmdate": 1701465400245,
        "mdate": 1701465400245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces new dataset for hate speech instigation. Reviewers raised concern about the missing literature, annotations, and modeling. Authors in their rebuttal have provided reasonable explanation for these points and how these could be addressed."
            }
        },
        "id": "72wxkujm2x",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "diUHb3jt3j",
        "replyto": "diUHb3jt3j",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2485/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543191,
        "cdate": 1696707543191,
        "tmdate": 1701465466631,
        "mdate": 1701465466631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper focuses on abstractive OIE where the relation may not explicitly appear in the text. The research is meaningful and interesting. Most of reviewers express their concerns about the experiments. The experiments are not sufficient and need further improvements according to the reviewers' comments."
            }
        },
        "id": "U47HE0qsKa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "diItUQ1idA",
        "replyto": "diItUQ1idA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4711/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707596696,
        "cdate": 1696707596696,
        "tmdate": 1701465539265,
        "mdate": 1701465539265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper under review presents a novel approach to implicit hate speech detection in conversational contexts. It incorporates the author's historical context, social context, and conversational context, comparing its methodology with various baselines and state-of-the-art (SOTA) methods. Additionally, the paper extends the annotation scheme for six widely used hate speech datasets through crowdsourced annotations. While there are some concerns regarding the quality of these annotations, the paper's clarity, thorough evaluation, and the superiority of the CoSyn model over baselines make it a valuable addition to the field, with potential significance for future research in implicit hate speech detection.\n\nThree reviewers (ArYr, 4YGb, and cZN1) provided their feedback and insights on the paper. All reviewers acknowledge the paper's contributions to implicit hate speech detection and its comparative analysis against baselines and state-of-the-art methods. ArYr commends the paper's precise problem formulation, transparent explanations of methods, and comprehensive evaluation on multiple datasets. 4YGb highlights the success of the proposed model, CoSyn, in outperforming baselines across six conversational hate speech datasets and the availability of code for result reproducibility. cZN1 praises the paper's technical soundness, particularly using hyperbolic learning to model network properties and the interesting qualitative analysis providing interpretations.\nReviewer Concerns and Suggestions:\n\nSome concerns, although addressable, have been raised by the reviewers as well. ArYr recommends adding an error analysis section and providing examples of where the model systematically fails for both explicit and implicit cases. 4YGb notes that the paper lacks a clear explanation of the compelling reasons for designing the proposed model. cZN1 asked for clarity regarding the use of Fourier transforms and the addition of a complexity analysis or a way to assess the computational load of the proposed model compared with baselines.\n\nIn summary, while the paper receives recognition for its contributions and strengths, the authors should address concerns related to error analysis and computational complexity to enhance the paper's impact. Future paper readers will also appreciate some changes in the manuscript to better motivate the inclusion of specific design elements like Fourier transform."
            }
        },
        "id": "FWk6B2foqk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "di1Foopybz",
        "replyto": "di1Foopybz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission820/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496810,
        "cdate": 1696707496810,
        "tmdate": 1701465411634,
        "mdate": 1701465411634,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a new metric for the evaluation of grammatical error correction (GEC) that takes advantage of multiple references and operates on chunk level.\n\nThe reviewers agree that this is a well-written paper and contains extensive experiments and analyses. The proposed technique is relatively simple but effective. However, the reviewers raised some questions about the proposed methods, in particular the minor improvements compared to existing metrics, the generalizability of the independence assumptions, and the behavior of the proposed method in cases of non-local errors."
            }
        },
        "id": "t65dYb2p30",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ddldNozhnM",
        "replyto": "ddldNozhnM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2554/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544679,
        "cdate": 1696707544679,
        "tmdate": 1701465468819,
        "mdate": 1701465468819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper \"Responsible AI Considerations in Text Summarization Research: A Review of Current Practices\" presents a survey of how ethical issues are dealt with in the context of automatic summarization. \n\nThe main criticism voiced by the reviews refers to the narrow scope of the paper and the lack in connection between aspects of the paper. \nPoints in favour of the paper are the details of the survey and the data set created for this paper."
            }
        },
        "id": "7aJGyUZoOP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dcYt9ByOOK",
        "replyto": "dcYt9ByOOK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3326/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560537,
        "cdate": 1696707560537,
        "tmdate": 1701465494045,
        "mdate": 1701465494045,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes to improve speech translation by using a \"fused\" representation consisting of a combination of the original input speech and output produced by an ASR system. The reviewers found that the motivation for the approach was clear, with thorough experiments that supported the paper's claims (strong soundness scores). However the reviewers also all indicated that they are ambivalent about the excitement from this paper, in large part because of the complexity of the resulting system. There were also some issues with writing and clarity, but the authors largely addressed these in the rebuttal period."
            }
        },
        "id": "Htscd3d7zE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dbRZyDxYlL",
        "replyto": "dbRZyDxYlL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission486/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488956,
        "cdate": 1696707488956,
        "tmdate": 1701465400805,
        "mdate": 1701465400805,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The work presents a novel method of keeping generating stories while keeping narrative order in mind through a two step pretraining and finetuning with RL+optimal transport rewards. All the reviewers agree that the method is sound and though the experimental setup is rather weak and can be improved. I would encourage the authors to update the paper with such results as they have stated in their rebuttal to reviewer bja9.\n\n(Also a minor suggestion to edit the title Narrative Order Aware Story Generation via a Bidirectional Pretrained Model and Optimal Transport Reward)"
            }
        },
        "id": "YmKyDwNq1R",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "daGbpBMkoy",
        "replyto": "daGbpBMkoy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3386/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561640,
        "cdate": 1696707561640,
        "tmdate": 1701465495794,
        "mdate": 1701465495794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary: This paper proposes to verify claims by generating intermediate questions. Google's search results are used to answer these questions, and the answers are then used to produce an answer and an explanation. The paper compares different prompts, such as CoT and Self-Ask. The main contribution of the paper is an improved prompt format called FOLK, which involves finding prompts for first-order-logic-style predicates in the claim and generating follow-up questions based on those predicates. The results show that the FOLK approach improves performance, especially for more complex questions. \n\nStrength: This paper presents a straightforward yet promising prompting method for fact-checking. The paper is well-written and easy to follow. Additional experimental results in the author's response further indicate that the presence of the FOL forms helps LLMs combine knowledge in multi-hop cases. Most of the reviewers agree that this is a solid contribution but the excitement is relatively limited. \n\nWeaknesses: I don't think there are any major weaknesses -- most of the weaknesses have been addressed during the discussion phase. The authors should take the experimental results during rebuttal into account when preparing for the final version of the paper."
            }
        },
        "id": "vbsdQmKRJC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dZWiI6A09u",
        "replyto": "dZWiI6A09u",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2342/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539911,
        "cdate": 1696707539911,
        "tmdate": 1701465461929,
        "mdate": 1701465461929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper points at weaknesses in the retriever-reader framework for open domain conversational QA, and suggests a retriever-reranker-reader approach that shows benefits on two benchmark datasets. \nWhile the approach is not very novel (there were previous works that suggested reranking in similar settings, e.g. as pointed by reviewer 3), the experiments are sound and demonstrate appealing results as agreed by all reviewers. \nGiven the above, the paper may be accepted to findings."
            }
        },
        "id": "rQh5EqWvTn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dVfeS1pp2e",
        "replyto": "dVfeS1pp2e",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission3427/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562480,
        "cdate": 1696707562480,
        "tmdate": 1701465497092,
        "mdate": 1701465497092,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose a method which determines the optimal number of tokens (k) which should be displayed to humans based on feature importance attribution scores. The authors perform their experiments on six explainability methods (SHAP, LIME, vanilla-grad, int-grad, the latter two in the plain & gradXinput form) on one model (DistilBERT) and one dataset (eSNLI). The authors show that using their dynamic method for determining k leads to better agreement and avoids the sentence-length bias.\n\nThe reviewers agree that the paper is well written, the gap is well motivated and the problem is relevant (Mndj, HqLk, Cz9e). Furthermore, the reviewers commend an interesting analysis which produces valuable insights and answers a relevant question (Mndj, HqLk, EnSP, Cz9e).\n\nThere are relatively few criticisms of the paper in the reviews, with two reviewers even noting they cannot find any reason to reject the paper (Cz9e, HqLk) barring perhaps the narrow experimental scope (Cz9e). The main criticisms were omission of some related work (EnSP9), unclarity within an equation (EnSP) and a request for a few more illustrative examples (Mndj). All of these criticisms were addressed in the discussion period, and the authors have either resolved them through comments or committed to include additional material given an extra page."
            }
        },
        "id": "RNZ55L7dVs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dVOXsyVcik",
        "replyto": "dVOXsyVcik",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3819/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707576488,
        "cdate": 1696707576488,
        "tmdate": 1701465510691,
        "mdate": 1701465510691,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The problem statement, solution, methods, and experiments are clearly explained. The manuscript is well-written and it is very easy to follow. The research topic is interesting and although the individual methods are not novel, the way the methods are combined together, the overall solution is interesting. \n\nHowever, one of the major contributions of the work is to build a model that can be adapted to the dynamic of the data, however, the dataset they used does not completely capture the capabilities of the framework they are proposing. The datasets are collected in different years and they have different genres certainly, they are different in nature, but one cannot compare the nature of a static dataset in a specific era of time with the nature of the data stream. \n\nIn addition, another concern is the way sentiment words are added to the seed words with positive and negative polarity. Sentiment is such a subjective phenomenon and it is very hard to add words based on a threshold since words in different contexts could mean differently. This matter is also mentioned by one of the reviewers.\n\nLastly, all the baseline and other methods that are compared with this framework are static and that is not a fair comparison because these models are static."
            }
        },
        "id": "JE5auBK9zV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dRlYuG3bj7",
        "replyto": "dRlYuG3bj7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1329/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510405,
        "cdate": 1696707510405,
        "tmdate": 1701465427470,
        "mdate": 1701465427470,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This research paper introduces a novel approach to tackle few-shot relation extraction using a hypernetwork-based decoupling method. The model comprises three key components: an encoder, responsible for extracting representations of relations and instances; a network generator, which produces the initial relation classifiers; and the generated classifiers themselves, used to predict relations for each query instance. By employing a two-step training strategy and a class-agnostic aligner, the proposed model effectively addresses the problem of overfitting and demonstrates enhanced generalization capabilities."
            }
        },
        "id": "8mC7clZuUy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dQxLtay1M3",
        "replyto": "dQxLtay1M3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5694/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617030,
        "cdate": 1696707617030,
        "tmdate": 1701465565241,
        "mdate": 1701465565241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an application of relation embeddings as a method for finding relation chains between concepts in the ConceptNet knowledge graph, implemented also for finding analogies, the task on which the method is evaluated.\n\nThe reviewers agree that the paper presents a novel method in a clear way, with some expressing excitement at its elegant nature and impressive results over the benchmarks. All concerns were addressed by the author responses, which were acknowledged by the reviewers. If accepted, I recommend the authors evaluate the benchmarks on at least one more competing method as proposed by reviewer zL7T, and add a short discussion featuring examples of nearest-neighbor words to alleviate the first concern raised by reviewer S1H7.\n\nAs a postscript, although I am not an official reviewer and have not carefully read the paper, but on a quick skim I noticed that there is no reference to literature on multi-hop relations and models, e.g. https://aclanthology.org/2020.acl-main.412/ (and much preceding work in ACL and non-ACL venues). This seems fairly relevant, and I hope the authors can look into this literature as well."
            }
        },
        "id": "xUx1S0MJBy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dJ5yzTX4rZ",
        "replyto": "dJ5yzTX4rZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1068/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504313,
        "cdate": 1696707504313,
        "tmdate": 1701465419341,
        "mdate": 1701465419341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Proposes a continued pretraining scheme that tries to incorporate temporal (date) information into representations in an effort to better generalize on down-stream tasks where the year is given in addition to input text. The method involves using off-the-shelf parsers to guide masking in BERT-like MLM. While performance appears to improve on selected downstream tasks (TwiNER and Poliaff) compared to baselines such as continued pretraining with prepending the date to the input text. \n\nOne concern that was raised was that the work depends on a well-working parser. This happens to work well for well written news, but may suffer for less well-formed text. It is unclear how general this can be beyond well-written text. Furthermore, a more comprehensive evaluation involving more tasks would be more convincing."
            }
        },
        "id": "2UbWjkXtiA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dHHumVX2XV",
        "replyto": "dHHumVX2XV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5264/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610045,
        "cdate": 1696707610045,
        "tmdate": 1701465554579,
        "mdate": 1701465554579,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a hybrid method to improve graph-based parsing in realistic contexts. A number of experimental choices could be explained and motivated better. However, the proposal is considered valid and the task relevant."
            }
        },
        "id": "Naisg7sctb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dFvwxdSj0B",
        "replyto": "dFvwxdSj0B",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission609/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491979,
        "cdate": 1696707491979,
        "tmdate": 1701465405078,
        "mdate": 1701465405078,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method to detect and analyze news media storms, which are periods of intense and focused coverage of a certain event, issue, or topic. The paper uses a large corpus of news articles from various sources and a neural network model to measure the similarity between articles. The paper then clusters the articles based on their similarity, time, and named entities to identify the media storms. The paper also examines the characteristics and dynamics of the media storms, such as their topics, duration, diversity, and inter-media influence. Overall, the paper makes contribution towards proposing a scalable news similarity detection model and the availability of the resulting dataset can help subsequent future works on media storms.\n\nI thank the reviewers for providing useful suggestions and the authors for providing relevant responses to the concerns raised. I encourage the authors to incorporate these discussions in the future versions of the paper, such as, providing the rationale for using particular portions of the documents for similarity computation or for the specific choice of parameters for determining media storm, apart from addressing the confusions surrounding Table 1 and Figure 5."
            }
        },
        "id": "dDtgKi1GIx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dFlGP1l65l",
        "replyto": "dFlGP1l65l",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4721/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597140,
        "cdate": 1696707597140,
        "tmdate": 1701465539633,
        "mdate": 1701465539633,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a framework operationalizing empathic similarity (i.e., similarity between personal stories based on empathic resonance). It contributes 1) a dataset of personal stories annotated with features from the framework and pairs of stories annotated with empathic similarity scores, 2) a model fine-tuned to compute empathic similarity of story pairs, and 3) a user study showing that participants empathize more with model-retrieved stories.\n\nThere is strong consensus from the reviewers that this paper presents a valuable contribution. They appreciate that the paper draws on work outside NLP (e.g., social psychology) to engage meaningfully with empathy and its expression in narratives, and find the concepts and task introduced to be new and interesting. They find the study to be well-scoped and the methodology sound, and appreciate the concepts’ and dataset’s likely utility not only for researchers working directly on empathy-related questions, but also for researchers working on sentiment, summarization, and other tasks that would benefit from engagement with the expression of personal experiences."
            }
        },
        "id": "bCw7H99bNm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "dABNxI5c1X",
        "replyto": "dABNxI5c1X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission85/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478765,
        "cdate": 1696707478765,
        "tmdate": 1701465386851,
        "mdate": 1701465386851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a novel approach for automatically capturing dependencies between event arguments within and across events using graphical networks. Additionally, the incorporation of an event memory and retrieval module during inference adds innovation to the proposed architecture. The experimental results, evaluated on ACE05, RAMS, and WikiEvents datasets, demonstrate the significant advantages of this approach.\n\nTwo out of three reviews agree on the good soundness of the work; one reviewer raised concerns over the clarify of the presentation of technical details, which I think is clarified in authors' rebuttal. Overall, I don't see obvious improvement areas in terms of soundness. Reviews also point out that the idea of constructing intra-event and inter-event dependency graphs is novel and well-motivated; however, the performance gains over prior methods appear relatively small based on the results presented."
            }
        },
        "id": "hPwSxKflMV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "d94iPelgSD",
        "replyto": "d94iPelgSD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2717/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548062,
        "cdate": 1696707548062,
        "tmdate": 1701465474164,
        "mdate": 1701465474164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces an innovative approach called Tree Prompting, which aims to enhance the performance of Language Models (LMs) without the need for fine-tuning. In this method, Tree Prompting operates in a manner akin to a decision tree. Each node of the decision tree is associated with generated labels derived from individual prompts, serving as the split features. Furthermore, the authors present various extensions of this novel method, including the incorporation of human-written instruction prompts, dynamic prompts, the use of Neural Network (NN) prompting as split features, and the implementation of a tree ensemble technique. However, it would be better if the authors could provide more insights into the failures of baselines."
            }
        },
        "id": "tAL8q6ub60",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "d0zla3M3LI",
        "replyto": "d0zla3M3LI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2058/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533299,
        "cdate": 1696707533299,
        "tmdate": 1701465452127,
        "mdate": 1701465452127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper aims to optimize the evidence retrieval process by utilizing feedback from the claim verifier. The retriever is trained on both ground-truth evidence and retrieved evidence. Fact verification is an important task and the proposed approach is intuitive and effective. The experiments showed good improvement on FEVER. Reviewers raised issues about unclear presentation in some parts of the paper, missing references, and weak ablation studies. One reviewer had a concern about lack of excitement. Overall, I think this paper presents a solid approach to an important task with some convincing results, especially considering it is a short paper. I hope the reviewers’ comments on the weaknesses will help improve the quality of the next version of the paper."
            }
        },
        "id": "DgGrpj02ta",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "d0qmGnKfXa",
        "replyto": "d0qmGnKfXa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1520/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516258,
        "cdate": 1696707516258,
        "tmdate": 1701465433233,
        "mdate": 1701465433233,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new DA approach with diverse queries and sources of supervision to progressively train a generalizable dense retrieval. Although there are varying opinions among the reviewers, the majority have given positive scores. Reviewers have raised concerns on comparison with larger model or potential model collapse when enhancing  with negative samples, or model complexity. The rebuttal has addressed these concerns. These suggestions by reviewers have contributed to strengthening this paper. Reviewers have high excitement score to this paper."
            }
        },
        "id": "NpajTBvTJM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "d00kbjbYv2",
        "replyto": "d00kbjbYv2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2131/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535273,
        "cdate": 1696707535273,
        "tmdate": 1701465455261,
        "mdate": 1701465455261,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "TLDR: The paper makes an impportant contribution proposing an automatic method for identifying shortcut reasoning; it can quantify the severity of the shortcut, and discover unknown shortcut reasoning. The concerns raised by the reviewrs are mainly about limitations of the work and a better comparison with related work. There are also some clarification points. The authors address well all raised concerns in their rebuttal and provide additional results to further support their claims.\n\nFollowing is a summary of the pros and cons identified by the reviewers in order of importance:\n\nPros:\n1. Significance/Originality - all reviewers agree that the paper makes an important contribution with an automatic method for identifying shortcut reasoning; it addresses important limitations of related work on shortcut discovery (iQTs), the method discovers interesting dataset patterns (X4Zv), could benefit future research on shortcut reasoning (xMKL).\n2. Quality - the proposed method is conceptually straightforward and reasonable (X4Zv).\n3. Clarity - the presentation and writing of this paper are very concise and clear (iQTs, xMKL).\n\nCons:\n1. Clarity - some evidence might not support the claim, e.g., is only one example enough to say performing well on IID (xMKL).\n2. Clarity - include a more comprehensive discussion of the limitations of the method - the need for OOD dataset (X4Zv), computational cost (X4Zv), discovering only a certain type of shortcut reasoning (xMKL).\n3. CLarity - a more comprehensive comparison with related work in terms of evaluation (iQTs), and dataset usage (X4Zv)."
            }
        },
        "id": "Ky33C4iRAW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "czxX6jjpVJ",
        "replyto": "czxX6jjpVJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1692/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707523758,
        "cdate": 1696707523758,
        "tmdate": 1701465438606,
        "mdate": 1701465438606,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The novelty of the proposed pipeline is questioned, with references to prior works exploring similar approaches. The paper needs to clarify its unique contributions and distinguish itself from existing research. In addition, there is a lack of human evaluation. However, reviewers acknowlege the contributions made by the approach outlined in the paper. Given the reviewers, the author rebuttal and following discusssion, I recommend the paper be accepted to Findings track."
            }
        },
        "id": "hxo3POtFEh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cw6v58yo6s",
        "replyto": "cw6v58yo6s",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission130/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479967,
        "cdate": 1696707479967,
        "tmdate": 1701465388214,
        "mdate": 1701465388214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes an interesting idea such as jointly learning empathy intent with empathy for achieving better results for empathy classification in utterances. \n\nThe paper is well-written and the results are solid, however, it is not clear how this model is effective in helping the empathic generation. The results of jointly learning two well-balanced tags on the same text content are always better than single training. \n\nThe annotation is well-designed and the results of kappa between two annotators are high. It would be great if the authors provided the count for each category.\n\nIf accepted I suggest the reviewers' comments should be considered."
            }
        },
        "id": "oXFbYCyQ6P",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cvRvFj3Pyv",
        "replyto": "cvRvFj3Pyv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2425/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541924,
        "cdate": 1696707541924,
        "tmdate": 1701465464712,
        "mdate": 1701465464712,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper tackles zero-shot cross-domain slot filling, a challenging but meaningful setting. It proposes a novel adaptive end-to-end metric learning method that includes context-aware soft label representations and slot-level contrastive learning. Experiment results on the SNIPS dataset as well as under cross-dataset settings (from rebuttal) show competitive performance of the proposed method.\n\nStrengths:\n- Innovative, well-motivated method designs\n- Clear and stylish writing\n- Competitive empirical results\n\nWeaknesses:\n- As the reviewers pointed out, the clarity in several places, including motivation, could still be improved. the authors are strongly encouraged to revise accordingly in the future revision."
            }
        },
        "id": "EBDh0gJmKr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "csBtifBXKo",
        "replyto": "csBtifBXKo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5879/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707620455,
        "cdate": 1696707620455,
        "tmdate": 1701465568833,
        "mdate": 1701465568833,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a new task named schema-adaptable knowledge graph construction to enable the model to handle emerging knowledge without retraining the IE models. This work is applicable in real-world scenarios. The authors have studied changes in the schema in three dimensions: horizontal, vertical, and hybrid, and they have built task-specific benchmarks using existing datasets.\n\nThe proposed model is evaluated on three IE tasks (NER, RE, and EE) across three datasets and compared to four baselines, achieving similar scores as the baselines. However, the t-test indicates that the results are statistically significant with a p-value < 5%.\n\nPaper is easy to follow with typos and grammar style errors."
            }
        },
        "id": "F23DPfI2NI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "crfQrbxWAK",
        "replyto": "crfQrbxWAK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission542/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490394,
        "cdate": 1696707490394,
        "tmdate": 1701465402835,
        "mdate": 1701465402835,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary (from reviewer v7Sy): The paper presents a dataset for the assessment of readability for languages that belong to the central Philippine subgroup. The languages covered have not been treated before. The paper briefly presents the features taken into account, the experimentation setting and baseline developed leading to promising results. The author(s) make use of traditional widely used linguistic features plus the previously introduced cross-lingual N-gram overlap feature and introduce the hierarchical cross-lingual modeling for automatic readability assessment (ARA).\n\nThe reviews acknowledged the value of the resource that was developed for this paper, and its unique position in both the languages it covers and the domain (ARA). Many of the indigenous Philippine languages lack significant text resources altogether, let alone resources in ARA.\n\nThe discussion period was very productive. While the reviewers pointed out a number of missing or not understood statistics and other pieces of information regarding the corpus, there was also some confusion about why the specific modeling approach was used. The authors’ responses helped clear all of this up to the point where any revisions required to the paper represent missing or not fully explained information, but not methodological flaws or soundness issues that should lead to rejection. However, the authors must carefully address all questions raised in revisions if the paper is accepted.\n\nIt is worth noting that this is a paper that primarily provides a resource, and does so for a set of less-resourced languages. Papers of this type often receive medium or low enthusiasm scores, despite the significant positive impact that publishing them in popular venues can have."
            }
        },
        "id": "RguY93AwVk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cooAE3hYUC",
        "replyto": "cooAE3hYUC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission549/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490554,
        "cdate": 1696707490554,
        "tmdate": 1701465403084,
        "mdate": 1701465403084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigated the potential of using LLMs (GPT-3) for KB completion. The paper is easy to understand and provides a timely analysis of LLMs' capabilities on KG completion tasks. Findings presented in the paper may be of interest to a broader community. The authors clearly justify their study based on previous studies that do not pay attention to the long tail when extending KBs. The authors also provide a study per relation and use a different prompting. Overall, it is a solid short paper presenting interesting findings."
            }
        },
        "id": "RcVq3NQ3UE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cmQj1FdsOJ",
        "replyto": "cmQj1FdsOJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1382/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511709,
        "cdate": 1696707511709,
        "tmdate": 1701465429287,
        "mdate": 1701465429287,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors present a new table-to-text generation dataset along with a model and a credible evaluations. Their human annotator results are well constructed with reasonable reliability. The authors also seem to have allayed some of the concerns from reviewers in their rebuttal.\n\nI encourage the authors to follow reviewer  5oq8's advice and suggestions to increase the clarity of writing in the paper. There's"
            }
        },
        "id": "50o8N5A2Au",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "clxLDVanxO",
        "replyto": "clxLDVanxO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1567/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517958,
        "cdate": 1696707517958,
        "tmdate": 1701465434718,
        "mdate": 1701465434718,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the problem of evaluating the knowledge extracted from LLMs. The proposed evaluation metrics consist of four intrinsic aspects (Factuality, Relevance, Coherence, and Informativeness) and two extrinsic aspects (Helpfulness and Validity). The authors propose an automatic metric for each aspect and evaluate generated knowledge from 3 LLMs (Flan-T5, Llama and ChatGPT) and text from retrieval system (DPR) across using these metrics. Their results indicate that LLM generate text are less factual but more relevant than retrieved text, and that the higher relevance is ultimately important for downstream tasks.\nThe reviewers appreciate the importance of verifying the information generated by LLMs and the interesting analysis provided by the paper. \n\nThe reviewers initially had some concerns specifically with respect to the validity / efficacy of the metrics, especially the informativity metric. However, these concerns seem to have been largely resolved in the discussion with the authors such that it seems likely that the authors can address the concerns in the revision of the paper."
            }
        },
        "id": "1Ludon90zC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "clTPP37Rpu",
        "replyto": "clTPP37Rpu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3090/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555900,
        "cdate": 1696707555900,
        "tmdate": 1701465486643,
        "mdate": 1701465486643,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Summary:**\nThe authors introduce CONIC10K, a challenging dataset consisting of mathematics problems focused on conic sections in Chinese senior high school education. The dataset encompasses a set of problems having different levels of reasoning complexity, all requiring a basic knowledge limited to conic sections. For each problem, the authors give a formal representation, outline the reasoning steps, and present the final solution. Experimental results across multiple models demonstrate that even state-of-the-art large language models like GPT-4 exhibit mediocre performance when confronted with intricate reasoning tasks.\n\n**Strengths:**\nThe reviewers are in consensus on the paper's strengths, which can be summarized as follows:\n1. The paper introduces a 10,000-problem dataset on conic sections, encompassing a wide range of problems with different levels of reasoning complexity. This dataset serves as an interesting benchmark, shedding light on the limitations of Large Language Models (LLMs) such as GPT-4.\n2. The paper includes a comprehensive performance analysis of several state-of-the-art language models across subcategories of the 10,000-problem dataset.\n3. An additional noteworthy aspect is that the senior high school education content serves as a quality checker.\n\n**Weaknesses:**\nReviewers are in consensus that the paper lacks insights into data quality analysis, especially given the unavailability of the dataset for sample examination. Questions also arise regarding the reliability of the benchmark's evaluation protocol. Additionally, the study lacks experiments investigating the relationship between semantic parsing and mathQA, as existing models excel in semantic parsing, diminishing the dataset's utility as a semantic parsing benchmark. Furthermore, it would be beneficial to explore the effectiveness of adding more demonstrations. Lastly, the paper lacks results regarding self-consistency, which involve sampling multiple reasoning paths and conducting a majority vote on predictions.\n\n**Author-Reviewer discussion and acknowledgment:**\nReviewers presented comments and questions, all of which the authors addressed in their rebuttal response. All reviewers acknowledged the authors' rebuttal.\n\n**Conclusion:**\nThe paper is well-written and the idea is intuitive. However, the reviewers recommend that the authors make further improvements to enhance the paper and incorporate missing references into the bibliography."
            }
        },
        "id": "U9AdvzPHNK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ckKuQDW2RZ",
        "replyto": "ckKuQDW2RZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5362/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611653,
        "cdate": 1696707611653,
        "tmdate": 1701465557356,
        "mdate": 1701465557356,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an efficient inference method that ranks the spans of tokens in the context, and removes the least important ones. The reviewers appreciate the importance of the problem, the simplicity, novelty and theoretical foundation of the solution, and the writing quality. Nonetheless, there were some concerns about missing references and comparisons, and missing ablations, though many of them did not include a specific missing comparison, so they can be discounted. There were also practical gains given the method overhead. The rebuttal seems to have addressed most of the concerns raised."
            }
        },
        "id": "w9ofgYuJSC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cjbdRN8Yxy",
        "replyto": "cjbdRN8Yxy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1117/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505313,
        "cdate": 1696707505313,
        "tmdate": 1701465420864,
        "mdate": 1701465420864,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Summary:** The paper proposes a novel depth-wise graph neural network (DepWiGNN) for multi-hop spatial reasoning in text. The paper claims that DepWiGNN can mitigate the over-smoothing issue faced by GNNs through aggregating information over the depth dimension of the graph. The proposed Transformed Product Rule (TPR) mechanism enables convenient memory updating and retrieval operations through simple arithmetic operations, rather than relying on additional neural layers. Empirical evaluation is performed on two multi-hop spatial reasoning datasets where DepWiGNN outperforms three GNN based baselines.\n\nOverall the paper is well written and easy to follow (with detailed explanation for specific design choices). The empirical results are sound and show that their approach achieves strong gains over multiple strong baselines on two multi-hop spatial reasoning benchmarks. The main concern raised by the reviewers is the limited empirical evidence in the paper pertaining to the main claim of their approach mitigating over-smoothing faced by classical GNNs. Through the rebuttal and reviewer discussion, the authors have added comparison with a stronger over-smoothing mitigating baseline GCNII, as well as expanding the results for the GraphConv model to 10 layers. The results show that the DepWiGNN approach avoids over-smoothing, however the strong empirical performance cannot be solely attributed to mitigating this issue. The paper's empirical claim needs to be strengthened by highlighting that baseline methods suffer from a bottleneck when aggregating longer paths, and how DepWiGNN avoids this.\n\n**Recommendations for Improvement:** (i) Restructure the prose writing to mitigate the concern about the methodology of the paper lacking motivation for each individual design choice of the approach.\n\n(ii) Add details around the time and space overhead complexity of the BFS to the paper.\n\n(iii) Add additional experiments that highlight that baseline methods suffer from a bottleneck when aggregating longer paths, and how DepWiGNN avoids this."
            }
        },
        "id": "EhtxCdSFtG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ci6cexmrmD",
        "replyto": "ci6cexmrmD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3253/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559167,
        "cdate": 1696707559167,
        "tmdate": 1701465492200,
        "mdate": 1701465492200,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a new method, TK-KNN, for acquiring semi-supervised data for the task of intent detection. The reviewers agree that the problem itself is quite important, that the proposed method is a meritorious contribution towards solving the problem of acquiring labeled data for intent classification, and that the experimental results appear to be sound. Each reviewer has some different individual concerns; R1 about the theoretical motivation behind the method, which the authors try to clarify; R2 about novelty, as top-k KNN + contrastive learning is already a well established technique on other tasks; and R3 about the experimental setup, particularly that more ablations could be useful. Of these, the most salient concern is about novelty: the authors note that the particular combination of top-k KNN + contrastive learning has not been studied for semi-supervised learning on the task of intent classification, and is therefore still a good contribution, while R2 holds that these are well established techniques and it already makes sense that they would perform well in this setting. The study appears to be overall well executed, and though the method itself may not be particularly exciting from a novelty perspective, its application to the task of intent classification with extreme number of labels is still a potentially useful contribution."
            }
        },
        "id": "m31mBzxc2c",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "chCrhE2kl4",
        "replyto": "chCrhE2kl4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1073/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504415,
        "cdate": 1696707504415,
        "tmdate": 1701465419544,
        "mdate": 1701465419544,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper addresses sentiment analysis for code-switched data, emphasizing the challenges posed by mixed languages. The authors propose a late fusion model that combines the outputs of two fine-tuned transformers and demonstrates improved performance on English-Spanish and English-Hindi datasets.\n\nReasons to accept:\nThe paper demonstrates a clear improvement on the dataset used for sentiment analysis.\nThe approach is presented in a clear and direct manner, making it easy to understand and reproduce.\nThe methodology is reproducible, utilizing well-known and publicly available data with proper hyperparameter discussion.\nThe proposed approach not only outperforms the baseline but is also well-justified for the problem at hand.\n\nReasons to reject:\n- Insufficient empirical evidence to support the effectiveness of the proposed model, with a lack of experiments involving additional models and diverse datasets.\n- Limited focus on a single dataset and comparison against a single 2020 model, raising concerns about generalizability.\n-Lack of clarity in explaining the methodology, particularly in terms of how the models are fine-tuned before fusion.\n-The paper is perceived as an incremental traditional sentiment analysis work without introducing novel solutions that leverage context for problem-solving.\n-The advantage of not using ensemble techniques is not adequately explained.\n-Contradictory statements in the literature review regarding data availability and transformer performance with less training data."
            }
        },
        "id": "z2iaDJOG4r",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cgmlfA1sPl",
        "replyto": "cgmlfA1sPl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2628/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546262,
        "cdate": 1696707546262,
        "tmdate": 1701465471339,
        "mdate": 1701465471339,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "improvement of large language models (LLMs) without fine-tuning or annotated data. The framework uses a two-stage process: first, it generates and stores high-confidence examples from an unlabeled dataset as external memory; second, it recalls and uses the most relevant memory examples to assist the LLM in answering questions. The paper shows that MoT can significantly improve ChatGPT’s performance on various reasoning tasks, such as arithmetic, commonsense, factual, and natural language inference. The paper also demonstrates the importance of each component of MoT and its generalizability to different LLMs and methods."
            }
        },
        "id": "wF5dCsd3JB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cWw5FfVhvl",
        "replyto": "cWw5FfVhvl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission470/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488542,
        "cdate": 1696707488542,
        "tmdate": 1701465400005,
        "mdate": 1701465400005,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a methodology for a specific type of link prediction tasks, namely link prediction on graphs consisting of unseen entities (but with seen relations). The problem may have some relevance to data mining and knowledge management communities, whereas the significance for EMNLP is not clear (the only, weak link mentioned in the intro is that KGs are an ingredient in NLP tasks).\n\nReviewers found the paper generally sound, though excitement was limited. In particular, the problem was considered narrow, and several related works were not discussed. It appears the paper might be better suited for a data mining or knowledge management venue.\n\nFurther to the reviewer comments, I see three issues with this paper:\n - The reported problem appears strange - it is true that unseen entities are a problem in link prediction, but the formulation in this paper, of predicting on a completely disjoint graph, seems to misunderstand the problem: In the domain that the authors investigate (Wikidata-style knowledge), common general-world entities like countries, big cities, big companies definitely do re-occur between train and test sets. The challenge, instead, are organic entities that are only occuring a few times, but an (arbitrary?) hard split of the dense organic KG does not shed light on them.\n - The datasets are insufficiently explained, and seem extremely cherrypicked. There is no discussion of relations and instances to be predicted, and how the artificial splits were generated. MRR values around 0.8 represent outstanding ranking scores, and make me believe that the datasets consist of very strange instances, and are not indicative of realistic use cases. Instead of ranking metrics, it would be more helpful to report absolute or relative recall gains at a certain precision, e.g., how many, or what percentage of triples can the approach add to the KG, while maintaining 80% precision?\n - In addition to the strong absolute results (MRR), the relative gains (+15/+44% over SoA) are also at a level that is not easy to believe (extraordinary results require extraordinary evidence). The discussion provides zero case studies, and the metrics also aggregate across (unexplained) relations, thereby making it impossible to check them for plausibility."
            }
        },
        "id": "2imEJo8FfC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cVAHzYRVUO",
        "replyto": "cVAHzYRVUO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2642/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546498,
        "cdate": 1696707546498,
        "tmdate": 1701465471870,
        "mdate": 1701465471870,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper describes two new datasets for dynamic stance annotation in two languages, Dutch and Catalan. It's shown that a cross-lingual approach to stance detection helps performance for Dutch, for which there is only a smaller dataset available.\n\nThe reviewers were generally in agreement that the use of annotations of both static and dynamic stance to show relations on different levels of analysis was a strong aspect of the paper, and a framework that could be useful in future work. The cross linguistic approach was also very well received. Further the paper offers a useful resource to the community and provides strong baseline results. \n\nThe reasons to reject that concern me most (which are not resolved in the rebuttal/discussion period) include:\n* the suggestion that the paper may be more suited for LREC rather than a conference focusing on empirical analyses\n* the unclear presentation of the thesis of the paper (which was discussed in the rebuttal, but I do see the point the reviewer makes)\n\nI feel this paper is strong and could be a useful resource to the community. I do also feel that the authors should address the issues brought up by reviewers and adapt the paper accordingly before acceptance."
            }
        },
        "id": "hChVah4Ryb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cOxL1tlSQw",
        "replyto": "cOxL1tlSQw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3323/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560520,
        "cdate": 1696707560520,
        "tmdate": 1701465493953,
        "mdate": 1701465493953,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The research makes a good contribution for empathetic quality in LLMs text. The paper presents a comprehensive automatic and human evaluations. The qualitative examples provided further illustrate the impact of the proposed improvements on empathetic dialogue."
            }
        },
        "id": "3IyYRRo4U7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cMMxJxzYkZ",
        "replyto": "cMMxJxzYkZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2876/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551310,
        "cdate": 1696707551310,
        "tmdate": 1701465479685,
        "mdate": 1701465479685,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a study leveraging GPT-4 to analyze the patterns of dissent within the Federal Open Market Committee (FOMC) on the subject of inflation. The paper addresses an important research problem, i.e., understanding the dynamics within FOMC, and employs a novel approach using a state-of-the-art model, GPT-4, which is noteworthy. The paper presents an annotated dataset that can be valuable for a variety of applications, including economics, finance, and NLP research. The most glaring weakness is the lack of quality measures or sanity checks to ensure the reliability of the analysis. Given the high-stakes nature of the topic, this is crucial for both academic and policy implications. The research primarily differs from the previous work by the transition from GPT-3 to GPT-4. This could be seen as an incremental step, diminishing the paper's overall impact. In light of the identified merits and shortcomings, the collective judgment leans toward accepting the paper for the \"Findings\" section."
            }
        },
        "id": "sWTdXje2nE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cI6oe7i5mj",
        "replyto": "cI6oe7i5mj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4850/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599498,
        "cdate": 1696707599498,
        "tmdate": 1701465543340,
        "mdate": 1701465543340,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a 4-bit labeling scheme for projective dependency parsing as sequence labeling and extends it to a 7-bit labeling scheme for non-projective dependency parsing.\n\nAll the reviewers are positive, believing that the proposed scheme is simpler and more intuitive than previous schemes, the experimental results are good, and the paper is well-written. They also provide some suggestions for improving the paper, e.g., comparison of speed, comparison with other types of parsers, and clarification of section 3, which the authors have promised to address in the camera-ready."
            }
        },
        "id": "UvUrh19YwY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cFsfgaEMlw",
        "replyto": "cFsfgaEMlw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1554/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517434,
        "cdate": 1696707517434,
        "tmdate": 1701465434322,
        "mdate": 1701465434322,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper at pointer delves into situational grasp within LLMs, emphasizing the ChatGPT model. Critical facets of the study include:\n\n- The introduction of a synthetic environment akin to TextGame (as cited in one of the reviews), aimed at examining models' capability to chronicle and enumerate environment states.\n\n- An extensive evaluation of ChatGPT's limitations in maintaining consistent environment states over extended periods attributed to transient in-context memory and proneness to hallucinated updates.\n\n- Detailed error analyses reveal how LLMs transition from one state to another, encompassing correct and incorrect state transitions.\n\n**Reasons to Accept**\n\n- The paper uncovers the inherent limitations of LLMs, particularly ChatGPT, in tracking situational changes, emphasizing the challenges of LLMs in emulating human-like contextual comprehension and interactions.\n- The synthetic environment introduced offers a novel paradigm to evaluate the situational understanding of LLMs. This environment sheds light on the intricate dynamics governing model performance, presenting invaluable insights into the nuances of state transitions.\n- An in-depth discussion, spanning the risks associated with an unnecessary dependency on ChatGPT's conversational prowess to the broader importance of designing agents that align with societal norms and policies, lends a holistic perspective to the paper.\n\n**Reasons to Reject**\n\n- Reproducibility Concerns: The paper needs to include essential implementation details of the synthetic environment. Furthermore, there needs to be more clarity about the release of necessary prompts and the step instructions' design, making it challenging for others to replicate the results and potentially question the study's validity.\n- Limited Scope: The research predominantly concentrates on ChatGPT, neglecting a broader examination of other chat models. This limited focus restricts the paper's applicability and generalizability in the broader context of chatbot situational understanding.\n- Evaluation Metrics Ambiguity: The difference between specific evaluation metrics, specifically Step-EM and State-EM, must be articulated, leading to potential misunderstandings about the study's conclusions."
            }
        },
        "id": "eNIZfrHIcE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cFXHe1mW7V",
        "replyto": "cFXHe1mW7V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1823/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528191,
        "cdate": 1696707528191,
        "tmdate": 1701465443329,
        "mdate": 1701465443329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper tackles the challenge of extracting question-answering dialogue data from customer service chatbot logs. The authors propose an extension of the existing 1-1 question-answer pair extraction to a more versatile N-N setting. Furthermore, the paper introduces a suite of generative and discriminative tagging methods, employing both end-to-end and two-stage architectures. These methods are evaluated on five customer service datasets. An in-depth analysis of the extracted QA pairs reveals valuable insights into the dialogue structure, including information-seeking, clarification, barge-in, and elaboration. The proposed models are subjected to testing on dialogues from diverse domains and languages. While reviewers acknowledged that the paper does not present groundbreaking techniques, it undeniably constitutes a valuable contribution to the field."
            }
        },
        "id": "l7wYIlWdsG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cD9blNBYF2",
        "replyto": "cD9blNBYF2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2233/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537573,
        "cdate": 1696707537573,
        "tmdate": 1701465458511,
        "mdate": 1701465458511,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper provides a new dataset for misinformation detection and mitigation in French and English. The authors show that GPT-4 outperforms existing methods and that GPT-4 and RoBERTa show large differences in what they fail to detect. They further show that training GPT-4 with an uncertainty-based method that excludes examples that are difficult to classify further improves performance. The dataset and code will be publicly released and should be useful for the community. Some of the weaknesses regarding the explanations and prompts were sufficiently addressed by the authors in the rebuttal and the results and analysis would be great to add to the paper. One reviewer had concerns about the number of different LLMs tested. The authors responded with some insight into other models and I believe that this is appropriate for the scope of this work. Overall, the work appears to be sound and exciting."
            }
        },
        "id": "XMENculNco",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cCJGuKJYG8",
        "replyto": "cCJGuKJYG8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5600/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615601,
        "cdate": 1696707615601,
        "tmdate": 1701465563401,
        "mdate": 1701465563401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper tackles an underexplored problem in the NLP community, namely the grammatical error correction (GEC) for Arabic language. The paper proposes two Transformer-based models and demonstrated state-of-the-art performance on multiple shared task datasets. The reviewers all agreed on the soundness and excitement of this work."
            }
        },
        "id": "QYCr3X20un",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "cBhzqp8WlV",
        "replyto": "cBhzqp8WlV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission201/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481783,
        "cdate": 1696707481783,
        "tmdate": 1701465390937,
        "mdate": 1701465390937,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper under review introduces a novel approach that applies Inverse Reinforcement Learning (IRL) to text summarization tasks. The main contributions of the paper are the introduction of IRL for text summarization, the design of sub-rewards, and empirical validation on datasets like CNN/Dailymail and Wikihow. While reviewers generally acknowledge the novelty of applying IRL to text summarization and the extensive empirical evidence to support its effectiveness, concerns were raised about the novelty of sub-rewards, the computational efficiency, and some missing details."
            }
        },
        "id": "kDB1VJPfAU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "c2xBtTNceS",
        "replyto": "c2xBtTNceS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1981/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531390,
        "cdate": 1696707531390,
        "tmdate": 1701465449184,
        "mdate": 1701465449184,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces MM-Reasoner, a framework designed for knowledge-based Visual Question Answering (VQA) that harnesses the strengths of multiple models and tools. Specifically, when presented with an image and a question, MM-Reasoner engages vision APIs, an LLM, and a vision-language model in tandem to address the query.\n\nAll reviewers commend the innovative approach of amalgamating the capabilities of various models and plugins. The method demonstrates commendable empirical results on the OKVQA benchmark. However, some reservations exist. Two reviewers note that the framework is intricate, necessitating intensive engineering of the pipeline. They also highlight concerns about the assumed availability and access to a multitude of tools/plugins, such as APIs."
            }
        },
        "id": "KtV7CmtZZv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "c27QqxALfo",
        "replyto": "c27QqxALfo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2242/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537685,
        "cdate": 1696707537685,
        "tmdate": 1701465458719,
        "mdate": 1701465458719,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper looks at combining discrete speech units (automatically extracted with a self-supervised speech model that quantises its input) with text. The proposed approach is applied to spoken language understanding tasks. The reviewers commented positively on the writing of the paper and the thoroughness of the analysis (good soundness scores). But some of the reviewers were concerned about limitations in the experiments, e.g. in the type of downstream tasks used (ambivalent and mediocre excitement)."
            }
        },
        "id": "uVaTnoyriN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "c0utj9Q4YY",
        "replyto": "c0utj9Q4YY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission822/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496870,
        "cdate": 1696707496870,
        "tmdate": 1701465411816,
        "mdate": 1701465411816,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Hallucinations are a major concern in text generated by current generative models. The paper proposes a benchmark for evaluating hallucinations. The dataset contains hallucinations in response to queries from HotpotQA, OpenDialKG, CNN-Dailyail). The core claim of the paper is that current instruction-tuned models do not reliably detect hallucinations, but that knowledge retrieval and CoT reasoning can help to improve the results. \nThe dataset is large, including 35 000 samples. The paper contains an analysis of what kinds of hallucinations occur. \n\nThe authors engaged in in-depth discussion with reviewers 1 and 3, during which many of their concerns could be addressed; the discussion resulted in several new analyses and improvements to presentation which can be incorporated in the final paper."
            }
        },
        "id": "3qxzKUsAHK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bxsrykzSnq",
        "replyto": "bxsrykzSnq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission947/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500102,
        "cdate": 1696707500102,
        "tmdate": 1701465415820,
        "mdate": 1701465415820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introducing CACN offers a well-justified and innovative approach that builds upon prior research. The method for developing CLAN is transparent and easily replicable. There is commendable depth in evaluating CACN's performance, ensuring robustness using various metrics and models. The manuscript is praised for its clarity, flow, and cohesion. The topic of claim normalization, especially in the context of social media texts, is seen as both intriguing and significant. A notable highlight is the curated dataset, CLAN, which has potential value for future research if made public. The paper's LLM-based approach, with chain-of-thought prompting and reverse check-worthiness, is robust with well-supported results. Section 7, especially the error analysis and human evaluation, stands out for its quality. It is recommended that the authors consider feedback to further refine their contributions."
            }
        },
        "id": "BdgJKQFNSb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bxltAqTJe2",
        "replyto": "bxltAqTJe2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2979/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553341,
        "cdate": 1696707553341,
        "tmdate": 1701465483066,
        "mdate": 1701465483066,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposed generating citations with LLMs as a means to enhance factual correctness and verifiability. A benchmark for citation evaluation is created which uses automatic metrics for fluency, correctness, and citation quality. Besides the core topic addressed by the paper, the experimental setup and analysis are very well presented. In general, it is a nice contribution to the field."
            }
        },
        "id": "MRPqVkVQjd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bxFwIn0wZ0",
        "replyto": "bxFwIn0wZ0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4339/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587706,
        "cdate": 1696707587706,
        "tmdate": 1701465528291,
        "mdate": 1701465528291,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an approach to mitigate bias in hate speech detection approaches by automatically finding likely non-causal correlates.\n\nOverall strengths include the focus on a current and important problem, a well-motivated approach with a potentially novel \"spuriousness\" measure, and good performance across multiple datasets at least for the metrics shown. Weaknesses include: choice of evaluation metrics, lack of validation for non-causal confounds/sentence-level spuriousness, and a lack of clear definition of the task (hate speech detection)."
            }
        },
        "id": "gKjDPAKnUN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bvl3p6JUlv",
        "replyto": "bvl3p6JUlv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3186/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557723,
        "cdate": 1696707557723,
        "tmdate": 1701465489519,
        "mdate": 1701465489519,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes the use of a contrastive learning based strategy to boost performance on hate speech detection with a focus on both implicit and explicit instances. Experiments show the overall potential of the proposed solution to improve performance over other baselines.\n\nThe experiments are overall sound and the problem tackled is an important and challenging one, with one of the weaker points being that the proposed solution is somewhat incremental. Another aspect that would have been good to see is some more depth in the experimentation, possibly with ablation experiments to demonstrate where the improvement is coming from, e.g. whether it is from the use of contrastive learning.\n\nWhile this is somewhat understandable from a short paper, I appreciate the extensive rebuttal with new results addressing the above limitations and I would appreciate the authors to incorporate these in a further revision."
            }
        },
        "id": "Cf3M2BMnF9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bt9Ho2FMxd",
        "replyto": "bt9Ho2FMxd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3209/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558230,
        "cdate": 1696707558230,
        "tmdate": 1701465490389,
        "mdate": 1701465490389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper finds that \"translate-test\" could be a stronger method than what people previously expected. The reviewers appreciate the comprehensive analysis on various classification tasks and the paper is well written. The paper could be further improved by provide more theoretical justification of the results. Moreover, it would be useful to clarify the languages included in the task in the paper. It might be interesting to understand whether this conclusion still holds for very under-represented languages other than those studied in the paper, or if it still holds for tasks other than classification."
            }
        },
        "id": "01wRjq9erz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bqaW5sGZOq",
        "replyto": "bqaW5sGZOq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2090/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534190,
        "cdate": 1696707534190,
        "tmdate": 1701465453631,
        "mdate": 1701465453631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agreed that the novel resource introduced in this paper is an important contribution for the community. However, they also highlighted some drawbacks like the lack of further details on the annotation process and the consistency between human annotators, and the limited discussion and comparison with the related work. Some ethical concerns where also raised about annotators' privacy. The reviewers appreciated the author rebuttal which clarifies most of the issues."
            }
        },
        "id": "sI7Mip5qLS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bpArUWbkUF",
        "replyto": "bpArUWbkUF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2762/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548850,
        "cdate": 1696707548850,
        "tmdate": 1701465475454,
        "mdate": 1701465475454,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores the question of whether scanpaths (from eye-tracking data) can improve various NLU tasks when the scanpaths used are fully synthetic, based on well known models of human scanpath data. The results show that the answer is yes, though the improvements diminish - unsurprisingly - as the amount of training data increases.\n\nOverall this is a very interesting result, and my opinion as AC is that the authors have addressed the majority of the critical concerns brought up by reviewers. My only additional concern is that this paper does not really fit in \"human centered NLP\" as the point of this paper is exactly to remove humans from the process, not figuring out how to center humans. So I would suggest moving tracks."
            }
        },
        "id": "cP5VLiofhT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bmeKrAzRqz",
        "replyto": "bmeKrAzRqz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2786/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549391,
        "cdate": 1696707549391,
        "tmdate": 1701465476405,
        "mdate": 1701465476405,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigated ChatGPT's morphological capabilities by prompting the generation of inflected or derived forms of nonce words. The findings indicate that ChatGPT has not yet reached human-level performance. Additionally, the creation of the dataset covering four languages is a contribution to the research community.\n\nTesting only a single closed model through a web API is undeniably disappointing. However, as the authors aptly pointed out in their rebuttal, there was understandably no alternative available. I believe this paper deserves publication in some form."
            }
        },
        "id": "8cfLi6kp7S",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bkGVmCE3UJ",
        "replyto": "bkGVmCE3UJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5556/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615053,
        "cdate": 1696707615053,
        "tmdate": 1701465562276,
        "mdate": 1701465562276,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents leverages in-context learning to adapt language models from the source domain to the target domain in the absence of target labels.  The paper presents various prompting and training strategies and shows on two tasks (Sentiment Analysis and NER) that the proposed approach works very effectively compared to existing baselines. The use of the ICL is very well-motivated and carefully done.  The authors are strongly advised to further polish the writing and reflect on the points (which are now clarified based on the discussion phase)."
            }
        },
        "id": "KYt4GQD8ev",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bgskDuMqcz",
        "replyto": "bgskDuMqcz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5768/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618249,
        "cdate": 1696707618249,
        "tmdate": 1701465566711,
        "mdate": 1701465566711,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper tries to connect continuous and discrete perturbations. Particularly, this paper suggests an algorithm to find a continuous perturbation given discrete perturbation within some error bound and further suggests a regressor (PerturbScorer) that emits matching continuous perturbation’s norm-bound given original sentence S, and discrete perturbation P(S).\n\nThe responses from the reviewers are somewhat unanimous. This paper brings an interesting and novel direction (and thus high excitement scores), however, there are concerns regarding clarity, generalizability, and the method’s application in a broader perspective (resulting in insufficient soundness score).\n\nI too agree that this is an interesting direction, but also agree with other reviewers that the manuscript lacks clarity and that the broader application should be addressed further. Authors do respond that this can be useful in multi-modal settings and help explain previous papers. However, it seems that there is a nontrivial step to show this usefulness on top of what the paper already shows. To be more precise with some examples, can this algorithm tell why some continuous perturbations worked and some did not? Addressing the concerns reviewers brought up and resolving ambiguities will significantly enhance the paper's value to the NLP community.\n\n(Some more detailed examples that the paper can improve on)\n\n- (Figure 1) I was not able to understand Figure 1 at all by just reading the paper. I was only able to understand this after looking back and forth at the comments the authors made in the rebuttal. Even then, the interpretation of it was not very straightforward.\n- Correlation is somewhat overused. For example, the paper studies the correlation between the effect of discrete and continuous perturbations and also names the output of the PerturbScorer to be outputting correlation.\n- Authors claim that this method can help multi-modal setups. However, they also respond in their rebuttal that PerturbScorer is not very effective in generalizing prediction when overall statistics (word lengths) are different. This result was on a very similar task but with different sentence-length statistics. In a truly multi-modal setup, would the proposed method be helpful? + Why is it very useful to connect two modalities in the first place was not very clear to me in the paper. Delivering those motivations clearly could also help enhance the method’s impact in the community.\n- As many reviewers pointed out, some of the statements are strong but unwarranted in a theoretical manner. e.g.) \"proving\" is a strong word in 418-423: \"...proving that we can successfully find norm balls ...\". How about changing this to a softer sentence? For example, one could say \"Our experiments display, that under XYZ kind of perturbations, the proposed method can successfully find matching continuous perturbations..\""
            }
        },
        "id": "WpPwGeZZ2Y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bflAMCWJh8",
        "replyto": "bflAMCWJh8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission79/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478622,
        "cdate": 1696707478622,
        "tmdate": 1701465386380,
        "mdate": 1701465386380,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an alternative way to use LLMs for conversational IR. It generates relevance scores for candidate passages, and use the scores to instruct dense retrievers. Although there is some similarity between this method and the previous utilizations of LLM for generating supervision signals for dense retrievers, this study provides an alternative and interesting method.\n\nThe authors have answered most of the questions of the reviewers in their rebuttal. Overall, this is a valuable piece of work that enriches the current literature on conversational IR."
            }
        },
        "id": "BuVdoMNLxA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bdgUPZhF9b",
        "replyto": "bdgUPZhF9b",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission37/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477459,
        "cdate": 1696707477459,
        "tmdate": 1701465384910,
        "mdate": 1701465384910,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This is a position paper arguing that characteristics of evaluators (i.e., demographic characteristics) are especially important for generation of language such as humour, irony, and sarcasm.\n\nOverall, this paper makes a valuable contribution. Reviewers agree that the paper’s core claim is well-motivated, and that it addresses a critical issue (lack of transparency with regards to human evaluators) and raises important awareness on the issue. Reviewers’ concerns are generally well-addressed by the authors’ responses; one reviewer suggests that the work would be strengthened by a list of concrete takeaways, which the authors provide in their response, while another reviewer’s concern about the scope of the literature review is addressed by an extended response detailing the review methodology.\n\nFinally, one reviewer raises concerns that the position paper requires an empirical experiment in order to justify its claim (i.e., that evaluators’ demographic characteristics impact outcomes), on the grounds that the paper’s analysis — which provides examples showing that different demographic characteristics impact interpretations of humour, irony, and sarcasm, and further shows that most papers in the meta-analysis do not mention demographic characteristics — is not convincing. While I agree that such an empirical experiment might be interesting, I see it it as a different analysis (for another paper, perhaps) and not one that is required for this paper. While it’s perhaps true that a study that does not report demographics (as is the case for most of the studies examined by this paper) may still have taken demographics into account or may have deemed demographics irrelevant to its analysis, the burden is on such a study to report this thinking, and not on readers to assume that it was present — therefore, in my view this paper’s analysis showing a distinct lack of reporting in the literature, as well as examples illustrating that demographics are indeed often relevant, seems to justify its claim."
            }
        },
        "id": "bELsAReOob",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bc2xgl7oGf",
        "replyto": "bc2xgl7oGf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2053/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533100,
        "cdate": 1696707533100,
        "tmdate": 1701465451922,
        "mdate": 1701465451922,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work explores using dissimilarity measures for model interpretability, including findings that these measures can give insights about model generalization. All reviewers appreciated the execution and motivation of this work, and it would be a good short paper at EMNLP."
            }
        },
        "id": "ye3qE4dtJ2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bZel7wM6fN",
        "replyto": "bZel7wM6fN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4690/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707596341,
        "cdate": 1696707596341,
        "tmdate": 1701465538645,
        "mdate": 1701465538645,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper under review offers a novel method, INGENIOUS, focused on the efficient selection of a subset of data for pretraining language models. This approach ensures performance comparable to training on the entire dataset. Leveraging a submodular maximization problem with a similarity-based facility location, the technique identifies representative samples from a broader corpus. When assessed, models pretrained on these subsets showcased remarkable efficiency, often matching or even surpassing models trained on full datasets in downstream tasks. The research stands out due to its rigorous experimental design, innovation in methodology, and addressing a critical issue of training efficiency. While there were concerns related to the clarity of specific results, the main focus on dataset size over other modalities, and the breadth of model validation, it is important to note that some of these criticisms may not directly align with the paper's primary focus.\n\nPros:\n1. The INGENIOUS approach represents a significant advancement in efficient language model pretraining by extracting crucial information subsets.\n2. Extensive experimental design appreciated by multiple reviewers, encompassing various architectures, sampling methods, and evaluations beyond just GLUE scores.\n3. The research offers profound insights into a pivotal area, potentially paving the way for future investigations.\n\nCons:\n1. Some ambiguities in results, especially related to the advantages of subsetting over full datasets.\n2. Results on smaller models may not generalize to larger models"
            }
        },
        "id": "xjSD8CmhLA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bWXIut4pNM",
        "replyto": "bWXIut4pNM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3983/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580659,
        "cdate": 1696707580659,
        "tmdate": 1701465516148,
        "mdate": 1701465516148,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper extends State-Space Models to many longer benchmarks, and proposed a slightly adjusted way to make it more efficient. It's a clearly written paper. Its main merit is to further reassure the efficacy of SSMs. \n\npros\n\n1. A solid justification for SSM to certain longer text documentation.\n\n2. Well written and easy to follow.\n\ncons\n\n1. The proposed adjustment of model is not significant. None reviewer finds it novel enough such that the reviewer won't to champion it.\n\n2. To make it a really comprehensive study-style paper on efficacy of SSMs on long document, the analysis is not enough and it lacks many other references to compare with."
            }
        },
        "id": "6DCwudd4zT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bVO1sWgnTx",
        "replyto": "bVO1sWgnTx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4225/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585697,
        "cdate": 1696707585697,
        "tmdate": 1701465524595,
        "mdate": 1701465524595,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "An interesting coupling of knowledge retriever and dialog generator to improve the quality of the retrieved knowledge to be incorporated in the generated dialog response. The paper is sound and shows clear evidence of the efficiency of the proposed method."
            }
        },
        "id": "SxR4wuLNg3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bSfBgrmabV",
        "replyto": "bSfBgrmabV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1364/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511178,
        "cdate": 1696707511178,
        "tmdate": 1701465428538,
        "mdate": 1701465428538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work analyzes how existing metaphor identification datasets can be gamed by fully ignoring the context in which the potential metaphorical expression occurs. \n\nThe reviewers ratings range from 3-4 with respect to soundness and excitement, and raised several critical issues which the authors tackle in their rebuttal and generally commit themselves to include in the final version (e.g. claim clarification, adding statistical analyses, etc.). As one reviewer notes, the work lacks a human baseline which would greatly improve the work. This, however, implies great additional work and the authors deemed this out of the scope of the short paper."
            }
        },
        "id": "ux1GOijM7g",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bQLMv4v0Gc",
        "replyto": "bQLMv4v0Gc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4555/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592900,
        "cdate": 1696707592900,
        "tmdate": 1701465534547,
        "mdate": 1701465534547,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper focuses on the problems of integrating multiple feedback modules (including frozen LMs and external tools) for improving reasoning in LLMs. \n\nThe reviewers agree that the problem is interesting and the method is effective. The one major weakness is that the ablation studies suggest that some modules have small to no effect on the final results. Overall, this is above the threshold for acceptance."
            }
        },
        "id": "yHteM2T3Xi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bNeDLx5O6w",
        "replyto": "bNeDLx5O6w",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2497/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543402,
        "cdate": 1696707543402,
        "tmdate": 1701465466993,
        "mdate": 1701465466993,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel Pathological Graph-driven Cross-modal Alignment (PCGA) model for the generation of brain CT reports. The authors first construct a pathological graph that encodes detailed medical knowledge, which is then integrated into node features via graph embedding and updating, resulting in fine-grained visual representations. The model aligns these learned node features with corresponding word embeddings through cross-modal contrastive learning to enhance report generation. Extensive experiments on the BCT-CHR dataset show the model's superior performance in generating clinically accurate reports.\n\nMain Contributions:\n\nThe authors propose a new framework to capture detailed domain knowledge from a Pathological Graph and align fine-grained visual and textual features of pathology.\nThe paper introduces the concept of decoupling cranial tissues and lesions via a Pathological Graph in the medical report generation area, a first in the field. This approach can handle the fine-grained alignment between long-text reports and multiple scans.\nThe authors validate the proposed model comprehensively on the BCT-CHR dataset, demonstrating that their method outperforms previous works in medical report generation.\nReasons for Acceptance:\n\nThe paper is well-written, with clear figures and examples that make it easy for readers to understand the ideas, methods, and results.\nThe use of a graph-assisted multimodal training approach is novel and interesting. The knowledge graph, automatically constructed from the report/paragraph of text, allows for intra-attribute classification and inter-correlation alignment.\nThe novel pathological graph decouples cranial tissues and lesions, enhancing the model's interpretability while also yielding benefits as demonstrated by the experiments.\nThe paper demonstrates the potential of incorporating domain knowledge as a specialized graph for medical report generation from images.\nThe model was extensively compared against several state-of-the-art models, showing superior performance."
            }
        },
        "id": "0mg8ohE7d7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "bB32QLrpu4",
        "replyto": "bB32QLrpu4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1463/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707514929,
        "cdate": 1696707514929,
        "tmdate": 1701465431707,
        "mdate": 1701465431707,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a behavioural evaluation framework for MT. The method first masks a pair of aligned words or phrases in the source and the reference translation and fills them in with ChatGPT. The evaluation test: the model passes the test if (i) the original translation quality is good, (ii) the translation quality of the new pair is similar to the old quality. For English and Chinese, the authors show that the new sentences, i.e. the ones used for evaluation, are of good quality. The authors test six MT systems.\n\nThe reviewers feel rather ambivalent about the paper. While they acknowledged that the test set could help finding MT errors without additional parallel data, they all had the same concerns. Firstly, it is not clear what to take out of the evaluation results: (i) often the models do not pass the tests because of an error outside of the modified span, (ii) many words/phrases cannot be translated in isolation (e.g., verbs often require collocations with prepositions). Secondly, the core of the method is using ChatGPT to translate: while this might work for the single language pair considered in the paper (English-Chinese), it is largely not applicable for other languages. While the authors replied very extensively and were extremely proactive during discussion, I believe these concerns are (i) valid and (ii) not addressed (e.g., mentioning the use of ChatGPT in the limitation section does not revoke the concern)."
            }
        },
        "id": "McP2N3fPGV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "b7gtyaaM2y",
        "replyto": "b7gtyaaM2y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3608/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566126,
        "cdate": 1696707566126,
        "tmdate": 1701465503483,
        "mdate": 1701465503483,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary: The authors of the paper focus on verifying factual claims in open domains, specifically when evidence needs to be retrieved from a knowledge base consisting of text and tables. Their proposed approach involves extracting tables and the associated cells using a two-step pipeline. Firstly, they use a pre-trained tabular model to compute a score over encodings of a table's rows and columns to extract tables from selected documents. Secondly, they build an evidence graph using the selected tables and evidence sentences to select the cells. The authors demonstrate the effectiveness of their system by evaluating it on the FEVEROUS dataset.\n\nStrengths: All the reviewers unanimously agree that this is a solid contribution with interesting implications. The approach used in this study is simple yet effective, yielding substantial improvements over previous models. The comprehensive experimentation conducted in this study greatly contributed to understanding the mechanism of the proposed method.\n\nWeaknesses: I don't think there are any major weaknesses -- some of the weaknesses have been addressed during the discussion phase. The authors should take the feedback into account when preparing for the final version of the paper. For example, concerns about the baseline raised by Reviewer cEAj have not been addressed in full."
            }
        },
        "id": "tCwThajloM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "b7ZJcAkjC3",
        "replyto": "b7ZJcAkjC3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5245/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609627,
        "cdate": 1696707609627,
        "tmdate": 1701465553954,
        "mdate": 1701465553954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers saw both strengths and weaknesses in the submitted version of the paper. The authors have provided a rebuttal that appeared to alleviate some concerns. After some discussion, the overall verdict was leaning toward an acceptance of this submission. However, there are still some issues to be addressed as raised by Reviewer uG9t. So, I am leaning toward a Borderline Sound on this submission."
            }
        },
        "id": "grXEyoivOt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "b6e1wV03hy",
        "replyto": "b6e1wV03hy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3059/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555314,
        "cdate": 1696707555314,
        "tmdate": 1701465485930,
        "mdate": 1701465485930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper studied the lifelong temporal knowledge graph reasoning problem. The authors first constructed datasets for this task and proposed a model under the framework of RL. \n\nAll reviewers agreed that the lifelong TKG task is important and novel. Nevertheless, the reviewers also raised some concerns on the experiments setup and comparison. Most of these concerns were addressed by the authors during the rebuttal phase."
            }
        },
        "id": "iSX6zcWAF5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "b6JnUJxOpN",
        "replyto": "b6JnUJxOpN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission520/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489763,
        "cdate": 1696707489763,
        "tmdate": 1701465402019,
        "mdate": 1701465402019,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Although the reviewers overall acknowledge several contributions of the paper, such as 1) detailed experiments and ablation study,  2) improvements over the baselines, and 3) analyses of under-explored factors that influence keyphrase generation performance, there are concerns such as the scalability of the method to longer texts, as well as further clarification on the results obtained."
            }
        },
        "id": "2H6lWAZdov",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "b5pbHYNJnX",
        "replyto": "b5pbHYNJnX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2361/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540416,
        "cdate": 1696707540416,
        "tmdate": 1701465462714,
        "mdate": 1701465462714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary:**\nThe authors argue that current evaluations of entity linking (EL) systems lack practical significance due to their reliance on coarse metrics and vulnerability to artifacts and biases. This underscores the necessity for more comprehensive evaluation methods. Accordingly, the paper presents an innovative evaluation system that includes a detailed error analysis, encompassing various indicators. Furthermore, the paper highlights the limitations of traditional benchmarks by introducing two new ones, Wiki-Fair and News-Fair. These newly introduced benchmarks have been utilized to  re-evaluate existing entity linking systems and models. \n\n**Strengths:**\nThe reviewers agree on the paper's strengths, which are as follows:\nThe author address a widespread phenomenon in current entity linking evaluations that has been previously overlooked and insufficiently discussed. This paper systematically analyzes the limitations of mainstream benchmarks, offering a comprehensive examination of research in this field.\nThe paper presents a novel attempt to create fair evaluations for NER/NEL, introducing new benchmarks and metrics that encompass false negatives and false positives, and disambiguation error rates.\n\n**Weaknesses:**\nThe paper deviates from common terminology in the field, as NERL and NEL are recognized task names, rather than ER and EL. Additionally, discussing NER/NEL without considering the broader context of relation extraction and slot filling is unusual in current discourse. The proposed benchmarks appear relatively small, with limited coverage of entity categories. Some concerns regarding benchmark limitations, such as entity type imbalance, are not adequately addressed. Consequently, it remains unclear which overall metric should be adopted in fair evaluations, and there is a lack of evaluation results on separate benchmarks. Lastly, certain portions of the text is less reader-friendly. \n\n**Author-Reviewer discussion and acknowledgment:**\nReviewers provided multiple comments and questions, all of which the authors addressed in their rebuttal response. The authors effectively explain their thought process. All the reviewers acknowledged the authors' rebuttal.\n\n**Conclusion:**\nThe paper is generally well-written and well-illustrated. It provides sufficient details to reproduce most of the work. However, reviewers recommend that the authors include missing references in the bibliography, and some minor improvements in the writing of certain details would enhance the overall quality of the paper."
            }
        },
        "id": "uOuPhylBaE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "b3lGS64ZZK",
        "replyto": "b3lGS64ZZK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3100/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556105,
        "cdate": 1696707556105,
        "tmdate": 1701465486950,
        "mdate": 1701465486950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper reports on a new resource of Chinese middle school essays annotated for three tasks: discourse coherence (global coherence score), topic sentence extraction (identification of the main sentence per paragraph), and, discourse relation recognition (logical relations between sentences). The paper also reports on several experiments validating the corpus using diverse approaches.\n\nUnfortunately, resource papers are not particularly \"exciting\" as reflected in the lukewarm scores by the reviewers. There seems to be a consensus that the paper needs some re-wording, more in-depth description, and potentially better argumentation, all of which is feasible within the time frame for the final version. In addition, two of the reviewers took issue with what is reported about the corpus construction. Details concerning basic descriptive statistics of the resource are missing and information about how the data was obtained or preprocessed is also missing, e.g, were the essays anonymized? (this last concern is not coming from the reviewers, but from me after perusing the paper). Last, reviewer2's suggestion about highlighting the correlation between the tasks--although somewhat severely worded--would indeed make for a stronger paper. The authors do point to some correlations but structure their discussion per task which reads a bit fragmented. \n\nAll in all, I believe this is a valuable resource for the community and that most of the issues can be fixed by rewriting (as opposed to running experiments), hence my recommendation."
            }
        },
        "id": "iy6PUAOgkO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "b1XS87j323",
        "replyto": "b1XS87j323",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1430/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707513216,
        "cdate": 1696707513216,
        "tmdate": 1701465430777,
        "mdate": 1701465430777,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors consider using reinforcement learning to improve the continual learning for named entity recognition, by using RL to control parameter tuning in a knowledge distillation module for memory preservation, so that new entities can be learned while existing knowledge is also maintained. Results show that the method is effective, and the same method can also be used for other tasks. The content is nicely organized and nicely written.\n\nIt would be a nice addition if the authors can discuss further the motivation behind the model design, and give stronger evidence in the stability and significance of the results. The authors promised to release code for RL, which can be useful."
            }
        },
        "id": "EVoTeGiMjC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "b1J3WplfgM",
        "replyto": "b1J3WplfgM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2583/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545263,
        "cdate": 1696707545263,
        "tmdate": 1701465469897,
        "mdate": 1701465469897,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers mostly agree that the paper is well-written and easy to follow. Also, most reviewers reach a consensus that the proposed method is novel and acknowledge that the method achieves new SoTA results. The reviewers also asked more analysis on the hyper-parameters, additional baselines, ablation studies on each parts and missing references. The authors mostly addressed the concerns raised by the reviewers except for the reviewer 9rh6. Reviewer 9rh6 could not provide further feedback or comments on the authors' rebuttal. Given the discussion, we weight more on the other reviewers' scores.\n\nAs a result, the reviewers mostly agree that the paper is 'strong' on the soundness. ([4,4,4], *3 - 9rh6*). In terms of excitement, two reviewers give 'Strong' rating, one 'Ambivalent' and one 'Mediocre'. Therefore, the excitement score is more towards 'strong'."
            }
        },
        "id": "b3Lc2MPpp1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "b07c10sXzN",
        "replyto": "b07c10sXzN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2637/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546391,
        "cdate": 1696707546391,
        "tmdate": 1701465471641,
        "mdate": 1701465471641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies how modeling annotator individuality creates more accurate classifier as well as how to remedy the subsequent privacy concerns. on All reviewers reach consensus that this paper is sound and exciting. The authors engaged in an in-depth discussion with reviewer aGsZ on this paper's implications and made convincing arguments about its claims. The paper is well-written and easy to follow, and potentially an impactful contribution to the community."
            }
        },
        "id": "htgiaa4gUD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ayzVnzaUzB",
        "replyto": "ayzVnzaUzB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5030/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605527,
        "cdate": 1696707605527,
        "tmdate": 1701465547843,
        "mdate": 1701465547843,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "the authors focus on decoding for structured problems using probabilistic models, using a lazy-k decoding algorithm to accommodate global constraints while maintaining efficiency. In the discussion the authors added a proof which further enhanced the soundness. While the experiments are conducted on information extraction tasks (the authors agreed to make the title more specific), it has potential of being useful for more tasks.\n\nThere are two things to improve, including discussions on search algorithms for traditional probabilistic models, and more discussions on the tradeoff between decoding effectiveness and model size."
            }
        },
        "id": "jkClaHZuic",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ayoGdkXi4V",
        "replyto": "ayoGdkXi4V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1790/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527245,
        "cdate": 1696707527245,
        "tmdate": 1701465441662,
        "mdate": 1701465441662,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper compares prompt-based fine-tuning with standard fine-tuning for 5 text classification tasks from Urdu and Roman Urdu languages. These datasets cover emotion detection and offensive language detection tasks. The paper uses 3 pretrained LLMs and shows that prompt-based fine-tuning outperforms standard fine-tuning when using a few examples (4, 8, 16, 32 per class). \n\nAs reviewers mentioned, this work presents a valuable contribution focusing on Urdu as a low-resource language. During the discussion period, the authors highlighted that the presented comparison (prompt-based fine-tuning vs. standard fine-tuning) is important when fine-tuning with a very small amount of data which is in line with the paper's focus on Urdu language. In this context, experimental results are sound and consistent across 5 datasets and 3 LLMs.  \n\nAddressing feedback from the reviewers, the authors provided additional experimental results during the discussion period and acknowledged the suggestions from reviewers. Concretely, incorporating the additional results, rephrasing the main claim, adding further details on the dataset and fine-tuning hyperparameters will further improve the paper."
            }
        },
        "id": "WoWPBS63PD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "asYObzj0IT",
        "replyto": "asYObzj0IT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5348/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611400,
        "cdate": 1696707611400,
        "tmdate": 1701465556981,
        "mdate": 1701465556981,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviews have a consensus on slightly positive soundness and excitement.\n\nRegarding soundness, the empirical observations show the proposed method improves two state-of-the-art baselines in the CodRED task, demonstrating the effectiveness of the proposed method.  The technical description on the path construction process is comprehensive and clear.\n\nWith respect to excitement, the idea of employing bridging entities for reasoning paths is novel.  However, its practical impact is obscure."
            }
        },
        "id": "8PS0lJ3HH1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aolJqJ50ZA",
        "replyto": "aolJqJ50ZA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2816/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550072,
        "cdate": 1696707550072,
        "tmdate": 1701465477521,
        "mdate": 1701465477521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The manuscript introduces a personalized distillation method to extract knowledge from closed-source to open-source Large Language Models. Unlike conventional distillation that forces student’s prior closely to follow teacher’s prior, the student LLM tackles given tasks first, then the teacher LLM adaptively offers refinement to improve the student’s own prior. The experimental results show that the personalized distillation outperforms the conventional distillation with only one third of the data. All reviewers concur that the paper proposes a smart idea, and the experimental results are solid for code generation domain. To further improve the clarity, incorporating friendly comparisons against traditional distillation and RLHF elucidates a clearer distinction across various approaches."
            }
        },
        "id": "vxJHL4KzaK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "alxWMBcNVN",
        "replyto": "alxWMBcNVN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1928/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530252,
        "cdate": 1696707530252,
        "tmdate": 1701465447494,
        "mdate": 1701465447494,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an extensive analysis of temporal reasoning capabilities of LLMs. Most reviewers agreed that the paper presented a clear and thorough analysis of an interesting and important category of reasoning. The authors responded to reviewers' questions with very thorough responses and additional results that inspire confidence that they will update the next draft to include these additional requested details and results, which will make the paper even stronger. Main concerns raised were that the paper doesn't include an analysis of a 10B+ parameter LM, and that it should include evaluation on the TRACIE dataset. The missing TRACIE eval was due to a misunderstanding where the authors believed that there was no public test set. However, they claimed to be running the experiments and plan to include these results in their next draft. The authors also justify the lack of 10B+ model by citing the cost, which I think is reasonable; while including larger models would be nice, they are not required for the soundness of the paper, and I believe the current set of results will already be of interest to the community."
            }
        },
        "id": "Myn6oXBwtP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "akJUrevmwI",
        "replyto": "akJUrevmwI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5090/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606799,
        "cdate": 1696707606799,
        "tmdate": 1701465549757,
        "mdate": 1701465549757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces SciFix, a novel scientific error correction system optimized for the bio and med domains, utilizing Large Language Models (LLMs) GPT3.5. By leveraging factual claim verification datasets, the research creates a unique factual error correction dataset. This is achieved by transforming correct claims into false ones using LLMs and subsequently annotating them with explanations. Furthermore, a domain-adapted T5 language model is employed, which incorporates claim-aware decoding to ensure accurate error correction. Notably, this approach surpasses existing benchmarks, even outperforming GPT3.5 in factual error correction tasks. The study encompasses multiple datasets, including SciFact, SciFact-Open, and CovidFact, and delves into the correlation between automatic metrics and human evaluations.\n\nThe paper appears solid and worthy of publication. Several reviewers have mentioned that its technical novelty is marginal. Another concern raised by a reviewer and ethics char is the description of hiring annotators, but this can be addressed easily by authors."
            }
        },
        "id": "EFHppdIvTf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ak6PQPmmEK",
        "replyto": "ak6PQPmmEK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission299/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484121,
        "cdate": 1696707484121,
        "tmdate": 1701465394128,
        "mdate": 1701465394128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The work suggests to additionally feed semantic graphs that represent input documents as part of the prompt when answering multi-hop questions. The authors show that on two multi-hop QA datasets, feeding the semantic graphs helps the model to predict more faithful reasoning chains and more accurate answers. The work is sound as agreed by all reviewers (3,4,3) and relatively exciting (4,4,3). I also find it appealing as it introduces a simple way to use structure in LLMs with CoT reasoning, that shows some empirical benefit. Given the above, I think it should be accepted to the main conference or Findings."
            }
        },
        "id": "pXR13fBJ8Z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ajzFrKT3U7",
        "replyto": "ajzFrKT3U7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission594/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491623,
        "cdate": 1696707491623,
        "tmdate": 1701465404666,
        "mdate": 1701465404666,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a new method for generating reliable translations of lexicons categorized into meaningful groups. The approach involves expanding the lexicon with synonyms and conceptually related terms using word embeddings and nearest neighbor estimation. The expanded lexicon is then filtered based on style scores assigned by pre-trained language models. The authors use this framework to study politeness across multiple languages, exploring the variations in the importance of lexical categories due to cultural differences. For studying politeness, they also create a corpus annotated with level of politeness on a 5 point scale.\n\nBased on the reviews and the subsequent discussion during the discussion period, the significant aspects shared by reviewers include:\n\n* The task is interesting and eventual applications (e.g. fine-tuning chatbots appropriately for different segments of society) is a reason to accept\n* The paper is well motivated and generally well written. Issues that have been raised wrt coherence and missing discussion points have been addressed in the author response\n* The presented framework is simple yet effective, which is a strong aspect of the paper\n* A nice contribution to the community with the code and the multilingual politeness dataset\n\nA shared issue brought up was that the experiments were not extensive enough to warrant a long paper. It should be noted that the author response addressed this well by providing new/additional analyses and results which should be included in the camera-ready, and make the paper stronger."
            }
        },
        "id": "LOBFaO2fgS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aipbZ5obaz",
        "replyto": "aipbZ5obaz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1147/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505970,
        "cdate": 1696707505970,
        "tmdate": 1701465421755,
        "mdate": 1701465421755,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper formulates event causality extraction as a template-based conditional generation problem to capture interactions between the cause and effect event. All the reviewers agree that this paper provides the sound method and makes significant contributions. The authors also give the detailed responses to the questions. The reviewers suggest some valuable methods for improving the paper. It would be nice if the authors could take them into consideration."
            }
        },
        "id": "FPSod03pIn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ai6kjD6cyX",
        "replyto": "ai6kjD6cyX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission485/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488941,
        "cdate": 1696707488941,
        "tmdate": 1701465400757,
        "mdate": 1701465400757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents JASMINE, a collection of Arabic GPT models trained on a diverse dataset encompassing both standard and dialectic Arabic. The paper includes a comprehensive description of the training procedures, datasets, and evaluation on various tasks, including human evaluations. The JASMINE models generally outperform existing models like AraGPT2 and mGPT, with only a few exceptions. The authors also discuss the inherent biases and shortcomings of the models.\n\nMain Contributions:\n\nThe authors introduce the JASMINE models, a suite of decoder transformer models for Arabic, trained on a diverse range of data from both standard and dialectic Arabic.\nThe paper provides a comprehensive description of the training procedures, datasets used, and evaluation on various tasks, including human evaluations.\nThe JASMINE models generally outperform existing models like AraGPT2 and mGPT, advancing research on Arabic generative models.\nThe authors also discuss the inherent biases and shortcomings of the models, providing a candid assessment of their performance.\n\nReasons for Acceptance:\n\nThe paper presents a major contribution to the field of Arabic generative models, advancing recent advancements of language modeling and in-context learning beyond the English language.\nThe JASMINE models generally outperform existing models and are tested on a diverse test set, demonstrating their robustness.\nThe comprehensive details and examples provided in the paper enhance its clarity and applicability.\nThe availability of these models and benchmarks will significantly advance research on Arabic generative models."
            }
        },
        "id": "iQp9gjWKT8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ahVTS392C3",
        "replyto": "ahVTS392C3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4730/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597259,
        "cdate": 1696707597259,
        "tmdate": 1701465539906,
        "mdate": 1701465539906,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new task of generating a \"catalogue\" for literature review given reference papers. For this task, authors provide a dataset, evaluation measures, and evaluated performances of baseline models. \nReviewers agree on the novelty of the proposed task, and the value of the proposed dataset for further research in this field.\nReviewers also question the choice of baselines, data size, and the validity of the proposed measure, which are addressed by the authors. That being said, constructing a high-quality human-annotated test set is suggested."
            }
        },
        "id": "c2HQeYApDy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aeLyo8GAco",
        "replyto": "aeLyo8GAco",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2341/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539938,
        "cdate": 1696707539938,
        "tmdate": 1701465461847,
        "mdate": 1701465461847,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The study introduces the novel Multi-CoT Consistent Knowledge Distillation (MCC-KD) method, aimed at the efficient distillation of reasoning capabilities from Large Language Models (LLMs) to smaller ones, addressing the challenge of maintaining both diversity and consistency in rationales. This method innovatively generates multiple rationales for each query and emphasizes consistent predictions by reducing the bidirectional KL-divergence. Through extensive experimentation across various model architectures and scales, the authors have demonstrated the superior performance of MCC-KD, particularly emphasizing its strength in both in-distribution and out-of-distribution datasets.\n\nPros:\n\nIntroduction of the MCC-KD approach, aiming to enhance reasoning capabilities in smaller models through CoT prompting.\n\nDemonstrated superior performance in mathematical and commonsense reasoning tasks, suggesting real-world applicability.\n\nAddresses an imperative issue in chain-of-thought distillation, introducing diversity and consistency in the generated rationales from larger models.\n\nCons:\n\n\nSomewhat confusing presentation and organization of experimental results, combined with a lack of profound analysis and comparative evaluations, which could mislead interpretations.\n\nLack of some baselines (partially addressed in rebuttal)."
            }
        },
        "id": "E2aM6969hH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ae6MmBuX6k",
        "replyto": "ae6MmBuX6k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission615/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492160,
        "cdate": 1696707492160,
        "tmdate": 1701465405368,
        "mdate": 1701465405368,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper does a thorough investigation of the ability of LLMs to understand AAE as compared to SAE. This is a very timely piece of work, as models get more intertwined with lives globally. Discrepancies in quality of service must be investigated, and this paper contributes to this soundly. In addition, they develop a resource for this evaluation. It would be useful to release more details of this such as annotator demographics, compensation etc in a datasheet.\nI would also urge the authors to reflect a little however on the definition and usage of the word ‘bias’ in the paper and try to ground it in experienced harms. As noted by a lot of recent literature, the interchangeable or disconnected usage of these words can dilute the impact and utility of a work, and given the thoroughness in this paper, just some additional reflection of the concepts used and framed would amplify its message."
            }
        },
        "id": "OxwzrEPw3O",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "adjZtG9bDM",
        "replyto": "adjZtG9bDM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4487/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707590856,
        "cdate": 1696707590856,
        "tmdate": 1701465532875,
        "mdate": 1701465532875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents two new methods for frame selection for Text-to-Video Retrieval (TVR). The paper has some merits. For example, reviewers agreed the proposed methods are effective as they show gains over baselines, and some reviewers enjoyed the clarity of the paper. However, reviewers also raised some concerns on the experimental settings and incremental novelty. Overall, AC agrees that the paper is a good source of experimental findings and methods of improvements in TVR."
            }
        },
        "id": "jtuLQyZegL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "adIeh9ZsfC",
        "replyto": "adIeh9ZsfC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3190/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557809,
        "cdate": 1696707557809,
        "tmdate": 1701465489690,
        "mdate": 1701465489690,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The majority of the reviewers agree that the work is both sound and exciting. I don't find the objection raised by the third reviewer convincing. The fact that a paper was published on Arxiv in May 2023 should not necessarily diminish the novelty of this work."
            }
        },
        "id": "BKcaXJUa18",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aaMwMjrDz0",
        "replyto": "aaMwMjrDz0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4145/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584084,
        "cdate": 1696707584084,
        "tmdate": 1701465521855,
        "mdate": 1701465521855,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper provides an empirical analysis of task performance disparities between multilingual task-oriented dialogue (ToD) systems, demonstrating that these disparities depend on the nature of the task at hand, the underlying pre-trained language model, the target language, and the amount of annotated data.  The findings offer valuable guidelines for future research to effectively expand ToD systems across languages.  Two of the reviewers express a concern that only four languages were covered, and that therefore the findings may not generalise to other languages.  The authors addressed this to some extent in their rebuttal by pointing out that the four languages were from different families, emphasising their linguistic variation."
            }
        },
        "id": "ExQ9ojKqSN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aY4avQ0ItI",
        "replyto": "aY4avQ0ItI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2180/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536366,
        "cdate": 1696707536366,
        "tmdate": 1701465456951,
        "mdate": 1701465456951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper applies a multiple sequence alignment (MSA) transformer designed to model protein sequences to the task of automatic phonological reconstruction. The authors find that their approach outperforms the best performing models from recent shared tasks on protoform reconstruction and cognate reflex prediction for a large number of language families. They extend their work with a qualitative error analysis, a discussion of zero-shot reconstruction, and a description of examples exploring how specific diachronic sound changes were learned by their models. \n\nThe reviewers appreciated the novelty of the idea of adapting a method from computational biology to a linguistic task. They also noted the improvements in performance over prior work reported for two distinct tasks with three different accuracy metrics. One reviewer had a very positive evaluation of this paper, finding no reasons to reject. Two reviewers, however, questioned the significance and novelty of the contribution.  Both reviewers seem to have misunderstood parts of the paper, despite the paper's overall clarity, and the author rebuttal addressed their concerns very thoroughly. Despite the middling scores from these two reviewers, the paper is clear and offers a substantial contribution that is entirely appropriate for this track."
            }
        },
        "id": "CLPQqDVA64",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aXi6UwdygV",
        "replyto": "aXi6UwdygV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1370/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511334,
        "cdate": 1696707511334,
        "tmdate": 1701465428742,
        "mdate": 1701465428742,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a distant supervision method for training MRC models using a contrastive loss, with the primary contribution being a model confidence-aware sampling strategy for obtaining negative samples to make training more effective. The reviewers all agreed that the proposed method is sound, that the results are strong, and that the paper is well-written. They recommended that the authors include further analysis of the trained model's errors and sensitivity to hyperparameter choices."
            }
        },
        "id": "OoqhtyJVR4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aVqGqTyky7",
        "replyto": "aVqGqTyky7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1296/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509863,
        "cdate": 1696707509863,
        "tmdate": 1701465426612,
        "mdate": 1701465426612,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work presents IPA (Inference-Time Policy Adapters), a novel way of adapting LLMs to various generation tasks by \"tailoring\" their output distribution through task-specific training of other (smaller) LMs via reinforcement learning (RL).\n\nThe output probability for the next token prediction is modelled as the normalized product of the original and adapter LM probabilities. Crucially, during training, the LLM is frozen, and only the adapter is updated with the RL process.\n\nThe efficacy of IPA is demonstrated on 5 generation tasks using various SOTA LLMs as base models, and smaller variants or distilled LMs as adapters. For each task relevant RL rewards are devised, IPA is trained, and automatic and human evaluation is carried out, against a variety of strong baselines.\\\nIPA proves to be effective on all tested tasks, notably sometimes even when a nominally less capable model (GPT2) is used to tailor a strong LLM (GPT3).\n\nSome ablations wrt. reward modelling are presented (Appendix), showing that IPA is somewhat robust to the precise reward, yet performs best when the full proposed rewards are used.\n\n## Strengths\nReviewers agree the paper is sound (4, 3, 3) and exciting (4, 4, 3), and I agree with the positive aspects mentioned by reviewers.\n- The paper is very well written and easy to follow; it is clearly structured, and presents the proposed novelties in a concise and understandable manner.\n- The topic of adapting (extremely) large LLMs is very current, and the authors propose a novel method that seems both easy to implement and train, and effective on a variety of tasks.\n- The conducted experiments are extensive and seem to support the author's claims; the use of human evaluation for the presented generation tasks is commendable and largely supports the results of automatic metrics.\n\n## Weaknesses\nConcerns were raised wrt. to various aspects of the proposed method. The authors provided extensive rebuttals and addressed, in my opinion, all relevant criticism in a satisfactory manner.\n- Availability of smaller versions of base LLMs might be an issue; however, it is demonstrated that (i) often smaller versions are indeed available; (ii) distilled versions can be derived relatively cost-efficient; (iii) from the paper's results, even not directly related models might be viable candidates for adapter policies.\n- Some of the chosen benchmarks/metrics were questioned; I find myself agreeing with the authors that the these are standard and relevant.\n- The need for task-specific rewards was criticised. I again have to agree with the authors that such task-specific rewards are inherent to the RL process. While the choice of specific rewards in the paper could have been better motivated in the description of each task to make the it more understandable, the individual rewards are (in my opinion) sensible and justified by the tasks and relevant related work; clarification is an easy one-line fix per task for a camera ready version.\n- Related, it was pointed out that directly optimising a reward that is also the test evaluation metric is potentially problematic; however, experiments are extensive enough to show improvements persist for metrics that are not used for reward modelling, as well as for human evaluation.\n\n## Conclusion\nThe authors present a strong piece of work that is well-motivated, relevant, and extensively evaluated. In my opinion, IPA constitutes a valuable addition to the growing toolkit of \"parameter-efficient\" methods for LLM adaptation. In particular, IPA is applicable in settings where the underlying LM's parameters are not accessible, and only its output distribution is observable.\n\nThe efficacy of the proposed approach is demonstrated clearly on 5 different generative tasks, through automatic and human evaluation.\\\nI only have three small points of criticism, mostly wrt. evaluation:\n- The human evaluation results of 4.2 (Lexically Constrained Generation) seem surprisingly weak compared to the clear improvements on automatic metrics. Looking at the Mechanical Turk question template I wonder if a 3-point Likert scale and the measured aspects are the right tool here, as even the base GPT-3 achieves 2.60 on the \"Overall\" question, and even higher on \"Quality\" and \"Plausibility\". Still, IPA improves over these base model scores.\n- Task 4.5 (Knowledge-grounded Dialog) is the only one that was not put to a human evaluation; there might be reasons, which could briefly be pointed out.\n- The title \"[...] without Fine-tuning\" might be construed to imply that no training is happening at all (e.g., that IPA is purely inference-based). This is obviously not the case, as the adapter policy is indeed trained (one could say with task-specific fine-tuning) via reinforcement learning. However, the abstract clarifies the true nature of the approach.\n\nNone of these points seem severe enough to not merit acceptance, and the work in itself should make a valuable contribution to the main conference."
            }
        },
        "id": "YKaMUEKnak",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aVejMt2gYN",
        "replyto": "aVejMt2gYN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3647/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566972,
        "cdate": 1696707566972,
        "tmdate": 1701465505244,
        "mdate": 1701465505244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses the problem of spurious correlations in weakly supervised semantic parsing with a filtering approach based on \"denotation similarity\" approximated by how often two programs output the same results on a set of inputs. \n\nTwo out of three reviewers acknowledged the rebuttal.\n\nAll reviewers value the simple but effective method (on NLVR, Execution-Based Filtering improves accuracy and consistency, and on WTQ it improves accuracy), and give relatively high soundness and excitement scores (3.3 on average for each).  Still, the shortcomings mentioned by reviewers are quite serious, such the fear that the complexity may outweigh the benefits, and that the method may not extend well to more complex tasks, as well as the need for additional comparisons with methods using pre-trained language models, self-consistent chain-of-thought prompting etc. The latter of these is partly addressed in the rebuttal."
            }
        },
        "id": "Cr2IAnrDuO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aUn1BAzo7q",
        "replyto": "aUn1BAzo7q",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3969/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580292,
        "cdate": 1696707580292,
        "tmdate": 1701465515662,
        "mdate": 1701465515662,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper extends the task of NER with newer categories, named taxonomy expansion. The difficulty is how to train the NER model with two disjoint, partially labeled training sets. To this end, the paper proposed a new approach that employs a modified loss function to capture the relatedness between the original labels with the new ones. The experimental results on six datasets show the proposed methods could cover more domains and types. The paper is clear and easy to follow. The cross-annotation idea is interesting."
            }
        },
        "id": "oq4eFY3a4l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aURCCzSuhc",
        "replyto": "aURCCzSuhc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4408/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589169,
        "cdate": 1696707589169,
        "tmdate": 1701465530783,
        "mdate": 1701465530783,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Strengths**:\n\n1. The paper adds to line of work that improves PLMs for entity-centric data by also including topic awareness. The model design is novel. \n\n2. Model is evaluated thoroughly on two representative KEPLMs, and with four entity-centric tasks.\n\n3. Paper is written well. Code is released. Authors promise to release model checkpoints.\n\n\n**Weaknesses**:\n\n\n1. Gains in performance are quite small yet costly to achieve. Without a significance test, it is hard to say that the proposed approach outperforms significantly the approaches available in the literature.\n\n2. Motivation for hope of significant gains by inclusion of topic entities is weak. Just adding topic entities seems incremental.\n\n**Suggestions**:\n\n\n1. Please add discussion on added pre-training overhead comparison to the main draft.\n\n2. Please also include the LUKE-base+KÉPLET w/o Entity Identification and LUKE-base+KÉPLET w/o Entity Fusion ablations to the main draft.\n\n3. Please add extra references as pointed out by reviewers."
            }
        },
        "id": "5ZPsvhaFN5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aRlH9AkiEA",
        "replyto": "aRlH9AkiEA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2625/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546170,
        "cdate": 1696707546170,
        "tmdate": 1701465471164,
        "mdate": 1701465471164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper tackles zero-shot relation extraction and proposes an interesting summarize-and-ask strategy to use LLMs for this challenging task (reviewer 83on gave a good summary of the proposed method). Extensive experiments show fairly strong zero-shot relation extraction performance on multiple standard datasets.\n\nStrengths:\n- The main idea is intuitively plausible and may generalizable to other tasks\n- The empirical performance, as far as I could tell, is fairly strong for zero-shot RE\n- The proposed method doesn't need manually written templates, which is a nice and unique property\n- The writing is clear and easy to follow\n\nWeakness\n- Added cost for calling LLMs many more times\n- There's an inadequate discussion on the uncertainty part, which is quite important. the authors are encouraged to strengthen this part in the revision."
            }
        },
        "id": "dLavzF01Z7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aPZ7AjA5YV",
        "replyto": "aPZ7AjA5YV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission377/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486033,
        "cdate": 1696707486033,
        "tmdate": 1701465396679,
        "mdate": 1701465396679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a study comparing human and model rationales in both contrastive and non-contrastive settings. The results show that model-based rationales computed in both settings align equally well with human rationales, contradicting a claim in literature that contrastive explanations are more natural for humans than non-contrastive explanations. Overall, the reviewers were impressed by the carefully conducted study on several datasets, architectures, and post-hoc explanation methods. The insights from the study are informative to the community, whereas the dataset used in this paper will be publicly released as a unified benchmark fostering future research. However, the reviewers posted some questions about the justification for the conducted experimental settings and speculated results for alternative settings which the authors answered in the rebuttal."
            }
        },
        "id": "O7TeUaO1Ky",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aP5f7cgY1M",
        "replyto": "aP5f7cgY1M",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3370/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696708176138,
        "cdate": 1696708176138,
        "tmdate": 1701465495248,
        "mdate": 1701465495248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary:**\nThe authors introduce the EtiCor dataset, comprising 35,000 manually annotated text samples labeled for determining whether the text adheres to acceptable localized etiquette or not. Specifically, this corpus encompasses etiquettes from five major world regions plus Europe: India, Latin America, Japan-Korea-Taiwan, Middle-East-Africa, North America, and Europe. The dataset was constructed by scraping websites that provide information about regional etiquettes, tourist guide points, pamphlets, etiquette information channels, tweets, and books on etiquettes. While the corpus is currently in English, the long-term goal is to make it multilingual. The authors employ this dataset as a testbed for evaluating Large Language Models (LLMs) such as Delphi, Falcon-40B, and GPT3.5, introducing a novel Etiquette Sensitivity task. In this task, given a statement about etiquette, the goal is to predict whether the statement is appropriate for a specific region. This research reveals how LLMs exhibit biases toward Western societies while overlooking many aspects of other cultures. Additionally, the authors fine-tune four distinct BERT binary classifiers, reporting modest improvements over the LLMs evaluated in a zero-shot setting.\n\n**Strengths:**\nThe reviewers are in agreement regarding the strengths of the paper:\n1. The authors introduce a novel corpus focusing on etiquettes from five distinct global regions, featuring 35,000 meticulously curated labels.\n2. Building upon this dataset, the authors propose a new task, 'Etiquette Sensitivity,' aimed at assessing the appropriateness of a social norm within a given geographical context.\n3. The paper delves into an analysis of potential biases within Large Language Models (LLMs) concerning social etiquettes, with the overarching aim of promoting responsible and inclusive AI systems.\n4. The dataset is not only valuable for NLP applications but also for fostering cross-cultural understanding and promoting a more equitable, empathetic, and responsible society.\n5. Additionally, one reviewer highlights the interesting findings related to GPT3.5's responses to Middle East and Africa (MEA) social norms, where the model either refuses to process or categorizes them as gender-specific hate. This outcome could motivate an extensive multidisciplinary qualitative analysis.\n\n**Weaknesses:**\nReviewers have identified the following weaknesses in the paper:\n1. The paper lacks clarity in articulating the specific research gap addressed by the new etiquette corpus, as it may not be evident considering the existence of other resources for evaluating model responses in terms of ethical and moral integrity.\n2. The methodology used to create the data is presented without the necessary level of detail.\n3. The paper does not adequately describe the approach taken to query models for their knowledge of social etiquette.\n4. Some of the results reported by the authors are not well-described, and the paper is not consistently self-contained. For instance, the results related to Falcon40B lack sufficient clarity, and there are discrepancies in the reported accuracy of BERT experiments between the validation and test sets. Furthermore, the BERT experiments do not employ a ten-fold cross-validation approach for the fine-tuned system.\n\n**Author-Reviewer discussion and acknowledgment:**\nThe authors gave clarifications in response to the concerns raised by the reviewers and have outlined the planned improvements to be made during the rebuttal response and discussion phase. All reviewers have responded and acknowledged the authors' arguments\n\n**Conclusion:** \nThe topic presented in this work is interesting, and the paper is well-written and easy to follow. However, reviewers suggest that the authors address the identified typos. Furthermore, reviewers recommend that the authors incorporate additional references and enhance the paper based on the points raised during the discussion phase."
            }
        },
        "id": "ew9omdFswQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aNFWz8zubu",
        "replyto": "aNFWz8zubu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5150/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608095,
        "cdate": 1696707608095,
        "tmdate": 1701465551262,
        "mdate": 1701465551262,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a benchmark for evaluating LLMs' ability to understand converse relations that are less common during training. More specifically it pairs text description and triples defined in formal language, while inverting the conventional meaning of the relations. \nFew-shot learning experiments conducted on multiple LLMs ( GPT, Claude, and Flan-T5)  confirm that counter intuitively the problem gets worse with larger LLMs.\n\nStrength:\nThe paper reveals an important issue with LLMs: the reliance on superficial correlations when dealing with some formal-language oriented tasks.\n\nWeakness:\nThe scope of the study is limited to specific type of data-- relationship triple ↔ text conversion."
            }
        },
        "id": "jXZmP1amji",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aN8zkE15Nx",
        "replyto": "aN8zkE15Nx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission876/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498219,
        "cdate": 1696707498219,
        "tmdate": 1701465413293,
        "mdate": 1701465413293,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors have contributed to an important and challenging area of program repair, and have raised the important issue of the lack of data in the area. Their evaluations create an artificial scarcity of data which can be critiqued, but their evaluations demonstrating the value of their method are comprehensive. Some writing and figure captions can be clarified to improve presentation and readability."
            }
        },
        "id": "HPRrJ0H98q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aLkknJNdl6",
        "replyto": "aLkknJNdl6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission972/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500649,
        "cdate": 1696707500649,
        "tmdate": 1701465416513,
        "mdate": 1701465416513,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a novel approach to multi-scenario multi-domain dialogue summarization, introducing the MP4 pre-trained model with a multi-stage pre-training strategy to enhance pre-training and fine-tuning alignment. Furthermore, the paper contributes the LCM3DS corpus, featuring multi-scenario multi-domain dialogues and annotated summaries, thereby enriching training data quality for this task. The study encompasses the collection of 20 dialogue datasets, automatic annotation using ChatGPT, and multi-stage pre-training involving domain-aware pre-training, task-oriented pre-training, and fine-tuning of BART. Experimental results on public datasets (SAMSUM, DIALOGSUM, and TWEETSUMM) confirm the effectiveness of this proposed approach, emphasizing the importance of both the data and pre-training methodology. However, as the reviewers pointed out, the novelty contributions were insignificant since several previous papers have explored domain-adaptive post-training."
            }
        },
        "id": "PtpTKR0O2C",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aJILUuANbs",
        "replyto": "aJILUuANbs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1206/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507355,
        "cdate": 1696707507355,
        "tmdate": 1701465423692,
        "mdate": 1701465423692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores the usage of adapters  to support of language models for unseen low-resource languages during training time. This is done by a proposed architecture called ZGUL that is an ensemble of  ensembles of adapters. This method also claim to use linguistical related source languages to perform this. Overall the method is sound, and tackles a high impact problem. The authors also include a strong ablation study as well as a quantitative analysis. On the reject side, there are some issues that the authors need to tackle (using the correct linguistic family classifications). There are also room for future work as, how to include new scripts, and testing it on other tasks, like generative tasks."
            }
        },
        "id": "jkjroUpvJW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aIp5EZeO3f",
        "replyto": "aIp5EZeO3f",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3958/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580061,
        "cdate": 1696707580061,
        "tmdate": 1701465515227,
        "mdate": 1701465515227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Overall, this paper addresses the topic of LMs powering agents with clarity, providing a structured framework* for how to reason about it.\n\nAs the reviewers all note, the paper is well-written and provides good conceptual clarity. I am perhaps a bit more critical than the reviewers here on how much the paper has sharpened our view on the agent paradigm; I would encourage the authors to be a bit more incisive about how things should evolve and so forth (e.g. most of the guidance in the paper is not terribly sharp/fairly restrained). With that said, this paper is a good contribution to our field and hopefully will help the growth of this subarea!\n\n* Remark: I am not sure I would call the paper a vision paper/it seems more like a framework (whereas I would expect a vision paper to make more speculation about the future/take a stronger stand on how the future should evolve), but that's fine/a minor nit."
            }
        },
        "id": "amLLqnnnvv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aICXDsoH3O",
        "replyto": "aICXDsoH3O",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4902/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601247,
        "cdate": 1696707601247,
        "tmdate": 1701465544449,
        "mdate": 1701465544449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a method for logic-guided fine-grained address recognition method, where address hierarchy has been utilized as the logic rule and apply this in a probabilistic manner to improve the accuracy. Automatic extraction of addresses from conversation is a nice step. The paper needs revision to address the following issues: ablation and qualitative analysis to demonstrate the justification of hierarchy relationships of addresses; ablation by including logic rules to each of the baselines."
            }
        },
        "id": "6I7s4b49kR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aFIx8T43LU",
        "replyto": "aFIx8T43LU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2614/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545910,
        "cdate": 1696707545910,
        "tmdate": 1701465470899,
        "mdate": 1701465470899,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes FISH-DIP to use fisher information to highlight those task-specific parameters and then tune these highlighted parameters. In fact, using fisher information to highlight important parameters is widely used in continual learning, which is not a novel method. Considering that the paper is well organized and clearly presented, it is appropriate for this paper to be accepted to the main conference or findings"
            }
        },
        "id": "jdraInUEGt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aE7feUD7o7",
        "replyto": "aE7feUD7o7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4963/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602830,
        "cdate": 1696707602830,
        "tmdate": 1701465545899,
        "mdate": 1701465545899,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an innovative approach to enhance the capabilities of Large Language Models (LLMs) by introducing the CREATOR framework, which allows LLMs to create their own tools using documentation and code realization. The results on MATH and TabMWP benchmarks, as well as the introduction of the Creation Challenge dataset, highlight the potential of this approach to revolutionize problem-solving paradigms. The reviewers concur that the method is intriguing and valuable in the context of LLMs as agents. However, concerns have been raised about the experimental setting and its ability to support the claims made in the paper. In light of these concerns, we recommend accepting the paper to Findings."
            }
        },
        "id": "tud0GG7t9n",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aCHq10rQiH",
        "replyto": "aCHq10rQiH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission783/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495821,
        "cdate": 1696707495821,
        "tmdate": 1701465410164,
        "mdate": 1701465410164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Pros:\n- A strong theoretical result that RNN LMs can simulate probabilistic Turing machines.\n- All reviewers agree the paper is exceptionally well-written, especially for a theory paper.\n\nCons:\n- The result is incremental, extending an existing finding to a probabilistic rather than weighted model."
            }
        },
        "id": "h4eFfsecJl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aBvwASLqMg",
        "replyto": "aBvwASLqMg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5597/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615528,
        "cdate": 1696707615528,
        "tmdate": 1701465563204,
        "mdate": 1701465563204,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a mechanistic interpretation of Transformer-based LMs on arithmetic-based questions using a causal mediation analysis. They identify the subset of parameters responsible for specific predictions. \n\nThe main contribution of the paper is the causal mediation analysis (CMA) on arithmetic. \n\nIn pros, reviewers praise that the paper is well written, provides a better understanding of the mathematical capabilities of transformers / interesting insights, with findings that are \"interesting enough to stand on their own\".\n\nThe main cons / weakness is the limited applicability of an analysis only performed on arithmetic / the limited scope, and that this does not lead to any insights to improve transformers with. There are a few worries on that the claims are a little too bold, which could be adjusted by the authors should the paper be accepted.\n\nAll in all, the reviewers were (almost) unanimous in that the paper is strong in soundness (3/4) and strong in excitement (3/4), and the only (weak) reason to reject it would be its limited scope to arithmetic."
            }
        },
        "id": "1F9oMnHOBW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "aB3Hwh4UzP",
        "replyto": "aB3Hwh4UzP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1848/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528696,
        "cdate": 1696707528696,
        "tmdate": 1701465444452,
        "mdate": 1701465444452,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this work the authors address the ambiguity of mapping from natural language queries into SQL queries: a reasonable case in which the NL utterance can have more than one interpretation in terms of SQL queries. To study this phenomenon and measure to which extent current NL-to-SQL models can generate all viable interpretations within the top-k candidates, the authors create the new AmbiQT dataset in which two valid SQLs are written for each NL query. \n\nThey proceed to show that, due to the token-level diversity of the beam search in SotA models,   these models fail when the two valid SQLs exhibit mutual structural diversity. To address this, the authors propose a meaningful a structural decoding algorithm that nagivates the space of SQL syntax, dubbed LogicalBeam. LogicalBeam is shown to perform comparably or better to SotA on standard NL-to-SQL evaluation datasets (on SPIDER, reported in the submission; and KaggleDBQA, additionally reported in the rebuttal) and drastically better on the new AmbiQT, i.e., in the setting for which it was primarily designed. \n\nAll of the reviewers identify the new dataset and the algorithm as valuable contribution. The few concerns, primarily about the performance of LogicalBeam on the non-ambiguous NL-to-SQL datasets have been successfully resolved in the author-reviewer discussion."
            }
        },
        "id": "DWo0NQKd54",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "a0yFO9gKc5",
        "replyto": "a0yFO9gKc5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3251/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559138,
        "cdate": 1696707559138,
        "tmdate": 1701465492060,
        "mdate": 1701465492060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a non-autoregressive seq2edit approach for text editing in order to improve inference speed. The novel approach is based on connectionist temporal classification, traditionally used in ASR. The method is evaluated in two tasks: grammatical error correction and sentence fusion.\n\nReviewers agree that the method is effective, clearly described and straightforward to apply. There is also a solid evaluation procedure, comparing the approach against multiple existing models. Evaluation is also cardied out in other two languages other than English with satisfying results. Finally, the paper also includes a through analysis of the robustness of the proposed model across various scenarios and its ability to generate more flexible outputs.\n\nA main concern is related to the novelty of the approach, since similar ones have been proposed for machine translation. The authors have clarified how their apporach differns, and are encouraged to highlight these differences in the paper. Some concerns were raised regarding details of certain model components, which were clarified during rebuttal. There is also a concern that the paper is dense, and some suggestions were provided for improving the clarity. Authors are encouraged to include these recommendations and clarifications in the final version."
            }
        },
        "id": "FWBFjFVlza",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZsuPbCxPnA",
        "replyto": "ZsuPbCxPnA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5822/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618817,
        "cdate": 1696707618817,
        "tmdate": 1701465567718,
        "mdate": 1701465567718,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers have praised this papers' technical contribution to the field and originality. They agree that the paper advances the state-of-the-art in machine translation particularly concerning the well-studied phenomenon of translationese. \n\nFinally, the reviewers also praised the paper style, writing, and clarity."
            }
        },
        "id": "ZOKHUIdkx9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZskD7TlNVZ",
        "replyto": "ZskD7TlNVZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1078/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504531,
        "cdate": 1696707504531,
        "tmdate": 1701465419818,
        "mdate": 1701465419818,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "As a reviewer indicates this paper defined a novel benchmark, called NLI4CT, for clinical trial reasoning and inference tasks. This benchmark includes two tasks. This paper also released a new corpus with 2400 expert annotated entailment relations. This paper tested and compared 7 SOTA NLI models.\n\nThe main reasons to accept the paper are the following ones:\n\n\n\tExtracting the information from clinical trials report and inference on the patient eligibility is an important task to facilitate patient recruitment for clinical trials. The problem is very significant for clinical and translational research. The dataset released by this paper will be very valuable to the medical informatics community.\n\n    NLI4CT Resource itself. NLP needs good resources annotated by experts.\n\n    Addressing Biomedical NLI Challenges: CTRs are technical and complex documents challenging natural language inference. The paper highlights the importance of addressing challenges related to numerical reasoning, biomedical NLI, and processing long texts.\n\n    Baseline Performance Comparison: The paper presents test results with 6 state-of-the-art NLI models on the NLI4CT corpus. These results highlight the current limitations of existing NLI models, especially regarding numerical reasoning. The performance comparison is a benchmark for future improvements and advancements in the field.\n\n    Availability of Resources: The authors provide public access to their NLI4CT corpus, a competition leaderboard, a website, and the code necessary to replicate the baseline experiments.\n\t\n    The paper is easy to follow, with plenty of examples and detailed explanations.\n\n    The categorizations in NLI4CT allow researchers to do more fine-grained analysis (Results Vs. Intervention, Single Vs. Comparison).\n\n\nThe main reasons to reject the paper are the following ones:\n\n \n\tSome references are missing in the paper. \n\n\tThe corpus compilation and its annotation is not fully described. The paper should address potential biases and limitations in the annotation process.\n\n    \tPotential Biases: The article should address any potential biases introduced during the data collection and annotation. Biases could impact the generalizability and reliability of the findings.\n\n    \tThe list of limitations acknowledged by the authors: the models developed are not fit for direct medical application; the evaluation also lacks an interventional study to verify if the models learn the underlying causal structure of the tasks; and the dataset contains fewer instances than other published biomedical NLI datasets.\n\n\tThe paper does not report the performance of several recent best-performing biomedical models including PubMedBERT and BioLinkBERT and therefore lack of more domain-specific models evaluation.\n\nDespite the reasons to reject the paper, reviewers agree that the paper is sound and strong. This resource would help make advances in the field.\n\nIf the paper is accepted, the final version should be elaborated taking into account the comment by reviewers and by the ethics committee."
            }
        },
        "id": "ICpwoXCLu4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Zlm7F7g9FK",
        "replyto": "Zlm7F7g9FK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission493/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489142,
        "cdate": 1696707489142,
        "tmdate": 1701465401079,
        "mdate": 1701465401079,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel approach to policy planning for persuasion dialogue using Open-Loop MCTS without any model training.  Experiments on the PersuasionForGood dataset as well as an interactive human evaluation demonstrate that the proposed GDP-ZERO model outperforms ChatGPT.  The reviewers agree this is an interesting new approach to persuasion dialogue and appreciate the extensive experiments and results, further detailed in an elaborate appendix.  Some concerns were raised about a lack of comparisons with other methods in the experiments, which were subsequently addressed in the rebuttal, largely to the reviewers' satisfaction.  The other main concern raised was the fact that evaluations were carried out on one dataset only.  In response, the authors pointed practical limitations of evaluations in other datasets/domains, because these require hypothetical scenarios to be followed by the subjects.  This is understandable, but also leaves us with the question to what extent the proposed approach is applicable to and effective in other genres of task-oriented dialogue, which is what the title seems to suggest."
            }
        },
        "id": "v9nXwu7v9B",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZkR2bWvRpZ",
        "replyto": "ZkR2bWvRpZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1262/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508794,
        "cdate": 1696707508794,
        "tmdate": 1701465425455,
        "mdate": 1701465425455,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work addresses the task of multi-modal math problem solving. They augment a T5 model with a VQ-VAE image tokenizer and train the model on 3 math datasets which also include images and tables. Their model shows good performance on these datasets and also performs well when fine-tuned and evaluated on 2 other datasets.\n\nThe reviewers mostly agree this work is exciting. The initial reviews noted that some of the claims made are not accurate - particularly the depiction of existing works and reporting of the results from prior works. The authors have clarified this in the subsequent rebuttal and have promised to tone down the claims of state of the art. The authors should also report results from other works including some references that should have been included in their tables and are missing. One reviewer has given a lower score stating that the technical details were lacking and the work may not be reproducible but the authors promise to make their code available and have included information in the supplement.\n\nOverall the contribution is good for a short paper. The authors should incorporate the feedback from the reviewers' comments to improve the paper and in particular should tone down the claims and represent results from existing works clearly."
            }
        },
        "id": "lwsBrugx2h",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZjfclaOF7M",
        "replyto": "ZjfclaOF7M",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1999/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531791,
        "cdate": 1696707531791,
        "tmdate": 1701465450143,
        "mdate": 1701465450143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new task for image captioning which requires a model to generate captions based on (1) a 360-degree picture, (2) a controlling query. This task is novel and interesting to the community. However, reviewers have raised some concerns as well: (1) It is not clear the benefit of using a 360-degree picture instead of a standard picture, (2) The experiments section can be improved.\n\nSpecifically, reviewer 4vda has provided several good suggestions to improve the quality of the paper, including more baselines and more challenging examples."
            }
        },
        "id": "CzIO2XlBnl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZjWkQz9qXn",
        "replyto": "ZjWkQz9qXn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4979/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707603354,
        "cdate": 1696707603354,
        "tmdate": 1701465546397,
        "mdate": 1701465546397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This submission studies the generative retrieval and focuses on the problem of document indexing to handle new documents properly. In particular, this submission proposes a new framework with a reparameterization mechanism to combine two modules of semantic indexing and generation. Extensive experimental results on both public and industrial datasets validate the effectiveness of the proposed method. Active discussion and additional experiments addressed most concerns from reviewers and further demonstrated the rationality of the proposed method."
            }
        },
        "id": "0UmMOQwMPl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZhZFUOV5hb",
        "replyto": "ZhZFUOV5hb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission939/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499817,
        "cdate": 1696707499817,
        "tmdate": 1701465415516,
        "mdate": 1701465415516,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers find the paper sound and exciting (3+ for both).  They note the novelty of the task and the clearness of task definition.  A noted concern is the broad applicability of this research methodology to outside of the legal domain, as well as other legal jurisdictions than the area studied."
            }
        },
        "id": "HVAFmiEfrX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZgJSDBU3px",
        "replyto": "ZgJSDBU3px",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission256/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483068,
        "cdate": 1696707483068,
        "tmdate": 1701465392906,
        "mdate": 1701465392906,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The proposed methodology is novel, interesting and also timely (because it involves application of instruction-tuned LLMs for table-QA). The reviewers have pointed out to the absence of several meaningful baselines/ablations, which the authors have acknowledged and promised to include in the revised version of the paper. Overall, the suggestions of the reviewers, if incorporated, will make the paper a good contribution in EMNLP. I would strongly encourage the authors to add the suggested baselines and the analysis (e.g., the analysis on detailed breakdown of accuracy by table size and question complexity)."
            }
        },
        "id": "KRaYWR0BhO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ze2IIzaSF3",
        "replyto": "Ze2IIzaSF3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4363/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588295,
        "cdate": 1696707588295,
        "tmdate": 1701465529298,
        "mdate": 1701465529298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Based on the provided reviews, there are several positive aspects of the paper. Review 1 highlights the contribution of exploring DocGD from a causal perspective, presenting the causal relationships among variables for the first time. Additionally, the paper proposes a causally-complete dataset construction strategy and a causally-perturbed pre-training strategy. Review 2 also emphasizes the novelty of the proposed methods for creating causally-complete datasets and causally-perturbed pre-training. The paper achieves outperformed results on benchmark datasets and demonstrates effectiveness in different scenarios.\n\nHowever, Review 1 mentions concerns about the data quality being constrained by the dialogue inpainter and paraphrase model, as well as potential lack of robust generalizability due to the singular nature of data sources. Review 2 suggests that the paper needs more evidence to explain how the pre-training works for Document-Grounded Dialogue task and suggests comparisons with other existing pre-training methods.\n\nOverall, the soundness and excitement scores from the three reviews range from 3 to 4, indicating the paper provides sufficient support for its major claims/arguments, has some merits, and reports state-of-the-art results. However, there are key weaknesses and areas for improvement, such as providing more evidence and comparisons with other methods.\n\nThe contributions and strong motivation of the paper make it a valuable addition to the field. However, additional evidence and clearer explanations are needed to address concerns and improve the clarity of the paper."
            }
        },
        "id": "Aroy08DCMU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZZ3PL3qT9f",
        "replyto": "ZZ3PL3qT9f",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2668/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547145,
        "cdate": 1696707547145,
        "tmdate": 1701465472782,
        "mdate": 1701465472782,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "I concur with the other reviewers and believe this paper's idea is valuable; I recommend acceptance."
            }
        },
        "id": "P4Xs6BGDpv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZWpJFq6RRU",
        "replyto": "ZWpJFq6RRU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2428/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542036,
        "cdate": 1696707542036,
        "tmdate": 1701465464832,
        "mdate": 1701465464832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a new benchmark (dataset and evaluations) for OIE fact linking. The main focus of the benchmark is the different aspects of OIE linking (seen vs unseen vs polysemous entities), as well as out-of-KG detection.\n\nAll three reviewers find the proposed benchmark as valuable resource and, as Reviewer kopq points out, the dataset curation process itself is an interesting contribution. The lack of extensive comparison to other existing datasets was brought up as an issue, but this has been addressed by the authors and should be included in the revised paper. \n\nAll three viewers pointed to the lack of strong baselines as an issue, with Reviewer WyCA explicitly calling out the lack of models that used the context sentence as input. While I agree with the authors that the benchmark itself and not the baselines are the main contribution of the paper, I think that using the strongest baselines possible would better demonstrate the usefulness of the resource. In response to Reviewer WyCA, the authors have included an additional baseline (that uses the context sentence) and this - in part - addresses the reviewers' main concern. The issues regarding the novelty (presumably of the baselines) raised by reviewer PQB3 were not backed up with further clarifications and were not considered further."
            }
        },
        "id": "V325zUha1w",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZVy8L79f5f",
        "replyto": "ZVy8L79f5f",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1550/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517347,
        "cdate": 1696707517347,
        "tmdate": 1701465434161,
        "mdate": 1701465434161,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are in general agreement that this paper could be accepted in EMNLP-2023. The reviewers also raised some issues with its presentation and motivation. The authors have provided a rebuttal that appeared to alleviate some concerns. A suggestion was made to accept the paper as the main conference or findings. The reviewers' concerns are also suggested to be addressed in the final version."
            }
        },
        "id": "kD4mewDHmV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZTM90jlGAm",
        "replyto": "ZTM90jlGAm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2805/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549822,
        "cdate": 1696707549822,
        "tmdate": 1701465477002,
        "mdate": 1701465477002,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores stereotypical biases present in Stable Diffusion; specifically, tendencies for \"person\" to be aligned with \"white, male person\" and for the sexualization of women of colour. Reviewers agree that this is important and relevant work, and novel in its inclusion of various national/cultural identities and non-binary gender. \n\nThe main weakness of the paper according to the reviewers is in the use of cosine similarity of CLIP embeddings as the primary evaluation metric. CLIP embeddings themselves encode certain societal biases, and so the question is whether the results presented in the paper are due to bias in Stable Diffusion or bias in the evaluation metric.  In response, the authors have conducted a 100-sample manual annotation study, which demonstrates that cosine similarity and human annotation are in high agreement. They also offer to include examples in the revised version of pairs ranked as “very similar” and “very dissimilar”, as a visual (qualitative) demonstration of what the cosine similarity is measuring. \n\nWhile I agree with the concerns raised by Reviewer m6ZP, I also share the opinion of Reviewer 4Bgr that a thorough discussion of any potential limitations of CLIP cosine similarities, along with the manual annotation study and the examples, may be sufficient for the current work. I also encourage the authors to make the other changes suggested by the reviewers, including an expanded discussion on bias mitigation techniques (both existing solutions, such as Fair Diffusion and Safe Latent Diffusion, and potential solutions and recommendations as outlined in the reviews), and revising problematic wording where necessary. \n\nPros:\n- Interesting work on an important research topic\n- Considers bias along under-studied dimensions\n- Small annotation study shows good agreement with human perceptions of similarity\n\nCons:\n- Possible bias in the evaluation metric muddies the interpretation\n- Limited comparison against existing “de-biased” models"
            }
        },
        "id": "wqdhFlwYag",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZT3yJWAsrq",
        "replyto": "ZT3yJWAsrq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission83/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478716,
        "cdate": 1696707478716,
        "tmdate": 1701465386696,
        "mdate": 1701465386696,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new benchmark for Dutch including a variety of nine tasks and introduces the use of relative error reduction (instead of mean scores) relative to a baseline to make future evaluations on this benchmark (more) comparable.\n\nReasons To Accept:\n- The work presented in this paper is relevant to the further development of Dutch and multi-lingual LMs. The benchmark is well constructed, tested quite extensibly with existing LMs and the results reveal directions for further research. The paper is well written and has a logical structure.\n- The paper includes extensive experimentation and meaningful analysis of existing models according to the tasks.\n- The inclusion of the Abusive Language Detection is of particular importance, as it is not included in GLUE or similar benchmarks.\n\nReasons To Reject:\n- The work might be significant only to researchers working on Dutch, but even then other researchers might find interesting ideas in this work to apply to other languages."
            }
        },
        "id": "bLgaOFFeSx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZSHcpMXWxX",
        "replyto": "ZSHcpMXWxX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1954/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530730,
        "cdate": 1696707530730,
        "tmdate": 1701465448263,
        "mdate": 1701465448263,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Three reviewers provided feedback for this paper. There was a high degree of consensus amongst reviewers in the overall reviews, soundness and excitement. They found the method interesting and effective. They also found the work novel. One reviewer complained about missing baselines but failed to provide the specific ones they had in mind. This reviewer also asked the authors to compare their work to GPT-4 which is unfair in my opinion, given that the multimodal version of GPT-4 isnt even publicly available. Other complaints included a request for more experiments (with BLIP2) and missing related work. Overall the authors have done a good job at addressing these concerns and overall I am satisfied with the paper given the reviews and rebuttal."
            }
        },
        "id": "EU1PFslCBS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZQrRDCxfhW",
        "replyto": "ZQrRDCxfhW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1252/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508549,
        "cdate": 1696707508549,
        "tmdate": 1701465425193,
        "mdate": 1701465425193,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper develops metrics to evaluate citations from generative search engines. They perform a human evaluation to determine the efficacy of four such search engines, and find that many statements of these search engines are not supported by citations. The released annotations will be useful for future research.\n\nThe primary concern following the discussion period is the lack of a method to quantify support of individual claims within a sentence, as mentioned by reviewer qPBo. There are additionally remarks from reviewer hMXQ about how the search engines may change; however, even if they change, the metrics can be used to evaluate their progress over time."
            }
        },
        "id": "PFb9AzONyN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZQV5iRPAua",
        "replyto": "ZQV5iRPAua",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1219/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507684,
        "cdate": 1696707507684,
        "tmdate": 1701465423944,
        "mdate": 1701465423944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces an approach, named DisCal, to improve generated summaries' abstractiveness (as determined by n-gram overlap) without compromising their informativeness (as determined by ROUGE). \nDisCal offers various pseudo summaries to the student model under two supervisions: 1. the optimal pseudo summary is chosen for sequence-level distillation based on its abstractiveness and informativeness; 2. their rankings are used to guarantee that summaries with higher rankings receive higher prediction scores from the student model. \n\nThe reviewers noticed the following pros:\n1. DisCal superiority over other distilled models.\n2. Extensive experiments and ablation study. \n3. The paper is well-structured and clearly written. \n\nAccording to the changed scores, I understand that the reviewers were highly satisfied with the authors' answers in the rebuttal."
            }
        },
        "id": "12XsAqIiLd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZNQh02cCxt",
        "replyto": "ZNQh02cCxt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission666/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493227,
        "cdate": 1696707493227,
        "tmdate": 1701465406903,
        "mdate": 1701465406903,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the surface structure bias issue in unsupervised sentence embedding models, particularly in contrastive learning methods. The authors propose a two-fold solution: first, they augment the data to counteract the bias, and second, they use a recall loss to preserve word semantics and avoid forgetting. This approach aims to improve the similarity assessment between sentences with different surface structures and sentences with semantic differences.\nAfter author rebuttal, the reviewers updated their original views. In general, they agree that the paper is well-written, it's clearly motivated, and the empirical comparisons are comprehensive and sound. The reasons to reject are minors and the authors provided detailed comments that can added to the camera-ready version to enhance the paper."
            }
        },
        "id": "6rweAGpZ8q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZJua8VeHCh",
        "replyto": "ZJua8VeHCh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2755/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548715,
        "cdate": 1696707548715,
        "tmdate": 1701465475164,
        "mdate": 1701465475164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Positives of the paper include a well motivated problem and novel datasets being produced. Improvements in the writing could make the paper stronger, particularly in the results section and the notation. Reviewers also expressed desire to see results expressed in terms of time being spent learning. It is also not clear whether the results would generalize to more realistic datasets and scenarios."
            }
        },
        "id": "oTnNYKFjN8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZJrEDp19kC",
        "replyto": "ZJrEDp19kC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4388/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588785,
        "cdate": 1696707588785,
        "tmdate": 1701465530212,
        "mdate": 1701465530212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose a dataset for QA where questions have many answers. The authors focus the evaluation on robustness where clusters of similar questions are considered.\n\nThe reviewers like the robustness focus and the way the dataset is constructed.\n\nOn the other hand some reviewers mention that the baselines might be weak, although in my mind using LLMs and other smaller capacity supervised models from the literature makes sense. Reviewers also mention that the paper does not deal with all aspects of robustness, and to that authors replied that the paper focuses on compositionally. Finally, reviewers ask for an error analysis and several specific clarifications that were raised as questions."
            }
        },
        "id": "6FHaIywERK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZF8Ye9xWZc",
        "replyto": "ZF8Ye9xWZc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2579/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545160,
        "cdate": 1696707545160,
        "tmdate": 1701465469658,
        "mdate": 1701465469658,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This is an empirically sound work which proposes an approach for test-time adaptation of language-guided classifiers. In contrast to prior works, this paper uses EM to learn some parameters on unlabeled test examples to enable better aggregation. During the discussion period there were some questions as to (1) novelty compared to existing data-programming methods and (2) whether this method would extend to LLM-based models. The authors ran extensive experiments during the rebuttal period to answer (2), but questions regarding the methodological contribution above existing work remain."
            }
        },
        "id": "FEmuDkUmE3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZE6fN4OO18",
        "replyto": "ZE6fN4OO18",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3923/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579412,
        "cdate": 1696707579412,
        "tmdate": 1701465514292,
        "mdate": 1701465514292,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "I concur with the other reviewers and believe this paper's idea is valuable; I recommend acceptance."
            }
        },
        "id": "LcaMp11GA2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZBi4ijmOzs",
        "replyto": "ZBi4ijmOzs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4565/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593109,
        "cdate": 1696707593109,
        "tmdate": 1701465534933,
        "mdate": 1701465534933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers generally found that this was an interesting and fairly sound paper: The topic of focus is important, and the proposed counterspeech taxonomy is a useful addition that other researchers in this area could adopt/build on. \n\nThe main concern the reviewers had was with the lack of comparison to other frameworks for counterspeech. I think that addressing this concern, as the authors have started to do in their responses, is important, because it would better situate their work in this area. The meta-reviewer’s opinion is that the choice to work from an argument-mining perspective is super interesting and compelling; comparisons to other taxonomies would have the additional benefit of clarify why that’s the case. (Per one of the reviewers: how does the new taxonomy “add knowledge to what we already knew?”) I suggest that the authors heed our suggestions to expand their discussion on this point (alongside other reviewers’ comments)."
            }
        },
        "id": "nskDceaBaM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZB66oX17CQ",
        "replyto": "ZB66oX17CQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2636/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546372,
        "cdate": 1696707546372,
        "tmdate": 1701465471528,
        "mdate": 1701465471528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes JointMatch which addresses the challenges (pseudo-label bias and error accumulation) in semi-supervised text classification (SSTC). \n\nJointMatch combines three existing techniques (cross-labeling, per-class adaptive thresholding, and weighted (dis) agreement update) in order to mitigate two important problems in SSTC: model bias towards easy-to-learn classes and error accumulation. Experiments on multiple datasets indicate the potential efficacy of JointMatch.\n\nThere were worries that the proposed method is only a combination of existing work and not novel for that reason, however, the holistic combination of previous methods and experimental analyses of Jointmatch were recognized as innovative by multiple reviewers. Additionally, the authors added additional results: recent semi-supervised learning models as baselines and experiments of JointMatch on larger label sizes (>10). \n\nI recommend authors reflect new experiment results and clarifications in the manuscript."
            }
        },
        "id": "Q6lYPSYuuN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ZAHyZ3CBds",
        "replyto": "ZAHyZ3CBds",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2880/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551393,
        "cdate": 1696707551393,
        "tmdate": 1701465479874,
        "mdate": 1701465479874,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper develops a new approach for tackling the challenge of predicting hashtags for tweets in real-time social media by leveraging temporal adaptation. The method utilizes a constructed datastore to retrieve similar tweets, providing informative context and allowing the model to predict hashtags for new tweets without the need for re-training. The evaluation employs Twitter data partitioned into weekly segments, offering comprehensive insights into their approach's effectiveness. While this paper addresses the crucial issue of adaptability in handling evolving social media data, there are some aspects that could benefit from further attention and refinement, as mentioned in the second review.\n\nThe reviewers acknowledge the merits of this work and the authors provided additional experiments to address some of the raised concerns. The reviewer agree that the work is sound and has a decent excitement level."
            }
        },
        "id": "hzf8NmNqUh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Z8p4FX15fa",
        "replyto": "Z8p4FX15fa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4302/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587038,
        "cdate": 1696707587038,
        "tmdate": 1701465527210,
        "mdate": 1701465527210,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces PEERSUM, an innovative dataset for generating meta-reviews of scientific papers, and RAMMER, a multi-task meta-review generator model. PEERSUM includes features such as explicit inter-document relationships and conflicting information, enhancing the realism in multi-document summarization. RAMMER utilizes sparse attention and multi-task learning, showing promise compared to baseline models. The paper's strengths include a rigorous experimental setup, code and data sharing commitments, and the potential for advancing meta-review generation. However, reviewers unanimously call for stronger results, real-world use cases, clearer model explanations, and improved analysis of hyper-parameters and auxiliary tasks, indicating areas for enhancement in the paper's overall impact."
            }
        },
        "id": "5jhhzEezHI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Z7O1kA3pjB",
        "replyto": "Z7O1kA3pjB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2469/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542867,
        "cdate": 1696707542867,
        "tmdate": 1701465466177,
        "mdate": 1701465466177,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers all agree that the paper draws attention to a timely and important question: which (potentially copyrighted) books appear in GPT-3.5/4's training data, and to what extend has GPT-3.5/4 memorized these works? Understanding the degree of memorization has important implications for researchers in the humanities who wish to use these models. The reviews also see broader applicability of this work beyond the humanities, contributing to a deeper understanding of the models. The reviewers agree that the experiments are well designed and implemented.\n\nThe reviewers raise several concerns, which the authors convincingly address in their rebuttal. Specifically, they explain why the name cloze  task is appropriate, why measuring the perplexity of the prompt is infeasible, and why fine tuning is less relevant in this situation."
            }
        },
        "id": "pUBuHcALw4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Z65Wq2dvxB",
        "replyto": "Z65Wq2dvxB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission650/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492952,
        "cdate": 1696707492952,
        "tmdate": 1701465406479,
        "mdate": 1701465406479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors develop prompts to shed light on LLM’s access to explicit grammar principles. The authors evaluate on Icelandic, German, and Spanish. All reviewers appreciated the motivation of this work, but noted that it could benefit from clarifying its exposition."
            }
        },
        "id": "2Rw09wysM7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Z5VthlliRt",
        "replyto": "Z5VthlliRt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3003/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554003,
        "cdate": 1696707554003,
        "tmdate": 1701465484042,
        "mdate": 1701465484042,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents CITEBENCH, a novel benchmark for citation text generation. The authors propose an approach for merging and harmonizing diverse datasets, ensuring a consistent evaluation of models across different tasks and domains. The work also includes an exhaustive examination of several robust baselines, offering insights into their adaptability across different datasets. The paper is well-received with some minor concerns raised by the reviewers. The reviewers praised the novelty of the approach, the clarity of presentation, and the thoroughness of the experiments described. The reviewers also highlighted the potential of this work to become a reference benchmark in the field of Citation Text Generation.\n\nThe paper introduces a novel dataset for Citation Text Generation and provides a clever formalization of the task that allows the unification of several existing resources and the standardization of the evaluation of the task. The paper contributes significantly to the field by providing an exhaustive set of baselines for future work, a standardized evaluation methodology, and a comprehensive analysis of the results. The quality of the paper is high, with clear explanations, well-made figures, and solid experiments. On the other hand, one reviewer raised concerns about the clarity of task-specific modifications when populating the new benchmark from different existing benchmarks. There were questions about the novelty of the problem formulation and a perceived lack of depth in the analysis of existing studies, which was tried to answer in rebuttal. A minor weakness identified by a reviewer is the absence of validation done to check if the labeling of the benchmark or the generated texts actually reflects how humans would label them.\n\nGiven the novelty and potential impact of the work, as well as the overall positive feedback from the reviewers, the paper is recommended for acceptance. However, the authors are strongly encouraged to address the reviewers' concerns and suggestions in the final version of the paper."
            }
        },
        "id": "WFuYALH4B4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Z2JBNkaJ7k",
        "replyto": "Z2JBNkaJ7k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1005/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502134,
        "cdate": 1696707502134,
        "tmdate": 1701465417514,
        "mdate": 1701465417514,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Summary:**\nThis paper proposes a large, machine-generated dataset, called VIPHY, aimed at evaluating Vision-language Models (VLMs) performance related to object color, size and spatial relationships. The result shows limitations of existing VLMs especially in spatial relation reasoning tasks. The automatic pipeline extracts object names and subtypes from existing captioning dataset, such as Visual Genome and ADE20K.\nThe authors evaluate the results on multiple state-of-the-art methods on both zero-shot and fine-tuning settings. Experiments probing the vision language model in their benchmark show a huge gap between model and human performance across all the tasks. In addition to vision-language model, the authors explore training a language only model on captioning dataset which achieve superior results in inferring size and spatial knowledge than VL models.\n\n**Strengths:**\nThe reviewers agree on the following strengths:\n1. The authors have introduced a substantial dataset that holds the potential to become a valuable resource for the community.\n2. Their work presents an innovative pipeline for the automatic construction of a visual physics commonsense dataset, leveraging diverse tools such as a parser, depth estimation, vision language models, and bounding box areas. Building upon these foundations, the contribution introduces a novel task: estimating attributes like size, color, and space from images.\n3. The authors provide empirical evidence demonstrating that vision language models perform considerably worse in VIPHY compared to humans. This underscores the imperative need to address the common understanding of visible physics in such models within the NLP community.\n4. Lastly, the authors have thoughtfully included both source code and data with their submission. It would be worthy if these resources could be made publicly available upon the paper's publication.\n\n**Weaknesses:**\nReviewers have identified the following weaknesses in the paper:\n1. The work lacks the inclusion of recent models, potentially limiting the comprehensiveness of the assessment. The absence of stronger models like OFA and Unified-IO harms the robustness of the analysis. Moreover, this absence weakens the claim that existing Visual Language Models (VLMs) struggle to effectively integrate physical knowledge. Actually, it would be more intriguing for the language community to gain insights into the capabilities of Large Language Models (LLMs) in the domain of physics commonsense understanding.\n2. The paper does not clearly articulate how utilizing a larger, “potentially noisier” dataset adds insights to the community. In fact, previous benchmarks have reached similar conclusions and analyses using smaller, manually curated datasets.\n3. Reviewers have emphasized the absence of human validation in the automated process, which may introduce errors for several reasons. Additionally, the paper has limited coverage of \"visible commonsense\" understanding.\n4. The existence of CapBERT highlights that the dataset may be prone to significant text biases. This means that visual understanding may not be necessary for size and spatial questions, leading doubt on the claim that the dataset effectively tests visible physics commonsense knowledge.\n\n**Author-Reviewer discussion and acknowledgment:**\nReviewers raised various questions and concerns to which the authors responded during the rebuttal and discussion phase, outlining planned improvements and engaging in further discussions with the reviewers. All reviewers have responded and acknowledged the authors' arguments, while still pointing out some weaknesses in the paper.\n\n**Conclusion:**\nThe writing is clear, and the paper is well-written and easy to follow. However, reviewers suggest that the authors address the identified typos. Furthermore, reviewers recommend that the authors include additional references and improve the paper based on the points raised during the discussion phase."
            }
        },
        "id": "rUMnPnzeVN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Z1wGHeHBrk",
        "replyto": "Z1wGHeHBrk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4520/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592028,
        "cdate": 1696707592028,
        "tmdate": 1701465533769,
        "mdate": 1701465533769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors compare two types of data augmentation for the task of detecting Alzheimer's signs and symptoms from clinical notes. They showed that adding in the two augmented datasets (silver and gold) led to an improvement in performance. During the discussion, the authors asserted that the novelty of their work lies mainly in the application area and in the label-to-data direction of generation.\n\nThere are comments from numerous reviewers that point to a lack of excitement due to the paper's novelty primarily lying in the application of LLMs to a specific task. While this may get more interest in a medical informatics community (as some reviewers state), it largely does not effect the soundness of the work, and the work seems reasonably connected to NLP. The re-framing that the authors are working on, as well as further discussion of how their findings may apply to other tasks, could make this work more interesting to the NLP community."
            }
        },
        "id": "hqUmctzSOr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Yzi6LM20E2",
        "replyto": "Yzi6LM20E2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3826/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707576671,
        "cdate": 1696707576671,
        "tmdate": 1701465510896,
        "mdate": 1701465510896,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper takes inspiration from dual process theories in cognitive psychology, where humans use quick heuristic thinking to make decisions, then slower analytic thinking to rationalize them. While they use a simple approach (like a reverse CoT), it leads to valuable insights on their new benchmark \"Tiered Propara\". To understand these empirical gains, they visualize the Faithful Attention within the LLM, clearly and effectively displaying how their approach aids LLMs in focusing on the correct content. \n\nLimitations:\nWhile the simple approach presented in this paper works for the two benchmarks, fine-tuning and in-context learning strategies seem to be tailored for datasets where key information is localized, allowing for shifting model attention to a smaller chunk of key information. Thus, it remains to be empirically verified if this approach generalizes to other datasets where the key information is not localized. Further, errors in can be cascaded in this reverse CoT reasoning and this issue must be discussed in more detail.\n\nOverall, the strengths outweigh the limitations. It would be valuable to include the reviewer's suggestions and rebuttal content in the camera ready."
            }
        },
        "id": "VC53NT7N0y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Yz4VKLeZMG",
        "replyto": "Yz4VKLeZMG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4139/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583911,
        "cdate": 1696707583911,
        "tmdate": 1701465521657,
        "mdate": 1701465521657,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a framework for selecting relevant (key-)frames and corresponding captions, generated from videos that will constitute a summary of the video. The authors formulate the task as two main challenges: (1) the method of selecting the frames from video segments (solved using iterative and simultaneous approaches), and (2) evaluation of the task (aligned keyframe matching score for frames and METEOR for caption). The authors also contribute a convincing new dataset by adding additional attributes to the keyframes of existing data. The proposed baselines are sound and insightful, setting up a rewarding avenue for future research. The paper is overall well written, and authors commit to updating the paper in response to a meaningful discussion with reviewers."
            }
        },
        "id": "Ddmrc2C2JI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YvzA0hFCF3",
        "replyto": "YvzA0hFCF3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2443/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542326,
        "cdate": 1696707542326,
        "tmdate": 1701465465288,
        "mdate": 1701465465288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The detailed and careful design of learning objectives in the proposed KASDG framework tailored to specific objectives is a key contribution of the paper. \n\nAs one reviewer pointed out, the paper was missing several important references (RAG, ATLAS, and arguably DTR). While the author provided some comparison (both empirical and theoretical) during rebuttal, these put the thoroughness of the paper's literature review questionable.|meta review"
            }
        },
        "id": "VtwBgOCj8L",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Yt4QAWQJ2o",
        "replyto": "Yt4QAWQJ2o",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1897/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529614,
        "cdate": 1696707529614,
        "tmdate": 1701465446219,
        "mdate": 1701465446219,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper \"Copyright Violations and Large Language Models\" presents work on examining LLMs for copyright violations. To this end, LLMs are probed for copyrighted material, such as books, but also code. The results indicate that some LLMs might violate copyright laws in the US.\n\nThe main criticism mentioned are a lack of a clear definition and the degree of novelty. \nBut the task in itself is deemed important and findings interesting enough to merit a publication."
            }
        },
        "id": "V5iMr5gzKL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YokfK5VOoz",
        "replyto": "YokfK5VOoz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5571/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615229,
        "cdate": 1696707615229,
        "tmdate": 1701465562636,
        "mdate": 1701465562636,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Paper presents a detailed study and analysis of different sub-word segmentation algorithms and their impact on convergence, generalization and efficiency. Reviewers unanimously found the paper to be sound and exciting with few editorial and experiemental design issues."
            }
        },
        "id": "auVZYhqt1U",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YoKptDpMtt",
        "replyto": "YoKptDpMtt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1047/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503847,
        "cdate": 1696707503847,
        "tmdate": 1701465418937,
        "mdate": 1701465418937,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a novel method termed Symbolic Planning and Code-generation (SPC) that is applied to the OneCommon task. This task-oriented dialogue system hinges on a blend of neural (GPT-4 Large Language Model) and symbolic techniques to comprehend human interactions and formulate responses. SPC reads human utterances, generates code from them, and uses the code in a symbolic space for planning, subsequently reverting to natural language for responses. The evaluations indicate that SPC considerably surpasses the state-of-the-art method in human evaluations, especially when partnered with skilled humans. However, there are concerns regarding the clarity of the methodology, the system's broader applicability, and its novelty relative to prior research. Further elaboration on the process and addressing the cited limitations would bolster the paper's impact. After the rebuttal, it seems most concerns regarding soundness have been addressed, but reviewers are generally not very excited about this work. The AC feels that it would be a nice work to appear in the conference without all the additional materials and discussions."
            }
        },
        "id": "MHcfYdsD9w",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ynxo6lene2",
        "replyto": "Ynxo6lene2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2138/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535349,
        "cdate": 1696707535349,
        "tmdate": 1701465455382,
        "mdate": 1701465455382,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a framework for analyzing joint encodings in LMs, and evidence for the existing of these for certain labels and tasks. The reviewers identified missing baselines as well as additional experiments (e.g. different model sizes); but these have all been provided by the authors in the rebuttal. It is well written, and has enough significance (especially after the rebuttal). The leftover cons would be the question of validity of a 2-layer probe (this is a larger discussion within probing), the lack of discussion of implications, and that some analyses are merely speculative. I would say the first one is beyond the scope of this paper, the discussion of implications could be added, and speculative analyses could be seen as interesting avenues for future work."
            }
        },
        "id": "n3OFOQWu8i",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YllS5zEzVq",
        "replyto": "YllS5zEzVq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3436/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562695,
        "cdate": 1696707562695,
        "tmdate": 1701465497529,
        "mdate": 1701465497529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The work proposes a universal prompting approach for zero-shot inference by leveraging unlabeled data to construct in-context learning examples.\n\n**Pro**: \n- The reviewers all agree that improving zero-shot prompting is an important research topic, especially in the case where labeled data is not available.\n- The authors conduct a large suite of experiments and show that the proposed approach consistently improves model performance by a large margin across various types of tasks. The evaluation and analysis are very through and exhaustive.\n- The paper is well written and easy to follow. \n\n**Con**:\n- Some reviewers concern that paper only conducts experiments on close-sourced models and it’s not clear if it transfers to open-sourced models."
            }
        },
        "id": "kWgZ12zXkD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YivRtscaFW",
        "replyto": "YivRtscaFW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3034/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554786,
        "cdate": 1696707554786,
        "tmdate": 1701465485111,
        "mdate": 1701465485111,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel information retrieval corpus as well as a list of stop words for Somali, thus addressing a gap in NLP research for a low-resource language. While the excitement and potential impact of this work deserves praise, some details are vague or missing, e.g. the total number of queries in the dataset, statistics regarding the length of the documents and queries, etc. These concerns were nevertheless addressed by the authors in their rebuttals, and reviewers acknowledged these responses as satisfactory. There is, however, substantiated concern over style and presentation, suggesting the paper would benefit from the input of a native speaker or at minimum, an automatic grammar checker like Grammarly.\n\nIn summary, the revisions required to address paper content concerns can be accomplished quite readily, as indicated by fruitful discussions during the rebuttal period. The bulk of revisions would likely be dedicated to ensuring the writing is clear and up to standard."
            }
        },
        "id": "nxF0NaT8o5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YiITnAhKQd",
        "replyto": "YiITnAhKQd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3537/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564800,
        "cdate": 1696707564800,
        "tmdate": 1701465500696,
        "mdate": 1701465500696,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary:\nThe reviewers consider the paper to present a simple and effective approach for detecting AI generated text. The main reviewers concerns however revolve around the models used in this paper (GPT-2 and GPT-3.5 turbo) for baseline comparisons, which makes log likelihood and similar measures unreliable for detecting machine-generated text. The absence of XSum, SQuAD, and WritingPrompts datasets, originally used to evaluate DetectGPT, raises questions about the comparability of the methods. Additionally, the study's heuristic approach may not work well when the generator and revision models are significantly different, like PALM or Claude models. Lastly, the study's binary classification approach may not address the challenge of accurately detecting machine-generated text, as it can be a difficult problem requiring higher accuracy. This could be considered a limitation of the approach.\n\nThe reviewers however seem to all agree that although the paper is strong it is not so exciting. I have read the reviewers comments and the authors rebuttal comments. Two of three reviewers conclude that the paper is a sound paper with soundness score of 4 while one gives a review of 3. All reviewers on the other hand give an ambivalent score on excitement. All reviewers commented on acknowledgement of rebuttals and made appropriate changes.\n\nReasons to Accept:\n(1) The proposed method addresses the important task of detecting machine-generated text and presents promising results on various datasets.\n(2) The approach is novel, simple, and effective, offering a valuable contribution to the field.\n(3) The experiments involve multiple datasets and source models, demonstrating good generalization.\n\nReasons to Reject:\n(1) Baseline comparisons might be flawed due to the use of different source models for generating text and calculating log likelihood, making the comparison less meaningful.\n(2) The reliance on appendices for crucial information and the limitation of testing revision only with ChatGPT may hinder the paper's comprehensiveness."
            }
        },
        "id": "9TWcr2vpsL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Yg5uDwWQti",
        "replyto": "Yg5uDwWQti",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission881/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498322,
        "cdate": 1696707498322,
        "tmdate": 1701465413485,
        "mdate": 1701465413485,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a significance test for model-based metrics. This topic is crucial in NLP given that many experiments are evaluated using machine learning models which can errors in the evaluations, and  affect the confidence intervals that could lead to wrong conclusions. \nThe paper is clear and well written. While the experiments are conducted only on few tasks and binary classification tasks, a generalization of the formula is proposed that could applied to multi-class classification tasks."
            }
        },
        "id": "6ey6WUxv2j",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YbCkHTqZGn",
        "replyto": "YbCkHTqZGn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission834/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497202,
        "cdate": 1696707497202,
        "tmdate": 1701465412250,
        "mdate": 1701465412250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces SMRC^2, a new method for representation learning for scientific literature. The paper also introduces a new bio-sci dataset for evaluating systems in this field. SMRC^2 is demonstrated to be an effective strategy, useful for downstream tasks.\n\nSMRC^2 incorporates a *Wasserstein constraint*, which the authors empirically demonstrate to perform considerably better than alternatives; this is likely to have implications beyond SMRC^2 itself. \n\nThe paper initially lacked details, including specifications of some model components as well as comparisons to recent baselines; the authors have been diligent in addressing these in their responses, which when incorporated into the main text will much improve the paper. The research presented in this paper is of high quality, but initially held back by a lack of clarity. Two reviewers raised their soundness scores in response to the rebuttals, mainly because additional details around implementation and baselines were provided."
            }
        },
        "id": "nTXxS7Em0F",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YaxyQwG2TP",
        "replyto": "YaxyQwG2TP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission669/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493330,
        "cdate": 1696707493330,
        "tmdate": 1701465407059,
        "mdate": 1701465407059,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper conducts a large multilingual analysis of the relationship between large language model quality and its ability to predict human reading behavior. They study a diverse set of languages and find a correlation between LLM quality and ability to predict reading times (measured in an eye tracking corpus) in most languages. For a short paper, this paper makes a large contribution, and all reviewers agree that the topic is exciting. Reviewer 2 had a concern about the proof sketch; I think the authors addressed this well during the discussion period. The other reviewers flag some smaller issues, which also appear to have been addressed."
            }
        },
        "id": "GXI3HV8x3D",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YZJ3oewPcu",
        "replyto": "YZJ3oewPcu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5634/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616217,
        "cdate": 1696707616217,
        "tmdate": 1701465564239,
        "mdate": 1701465564239,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper provides an empirical analysis of the robustness of fine-tuned LMs and MLMs to perturbations applied to input text.\n\nReviewers agree that while there is a good breadth of experiments, there’s less in terms of ablations and understanding what causes the differences in robustness (beyond speculation in the responses), which affects the level of excitement."
            }
        },
        "id": "f96ctr45vC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YWbEDZh5ga",
        "replyto": "YWbEDZh5ga",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission319/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484666,
        "cdate": 1696707484666,
        "tmdate": 1701465394649,
        "mdate": 1701465394649,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes metrics and subsequent mitigation strategies for constraint violations during utterance-to-api semantic parsing, focusing on LLMs and the in-context learning paradigm.\n\n**Pros**: Reviewers agree on the relevance of the problem, and further, agree the paper provides valuable insights to the community, citing the proposed metrics, analysis, and mitigation strategies as having the potential to advance research.\n\n**Cons**: Reviewers agree on a lack of generalizability and limited scope, citing limited datasets and application to only one learning paradigm. Author's rebuttal emphasizes a lack of available resources, and the importance of this particular paradigm, respectively. Reviewers acknowledge this argument, but the concerns remain. In the end, no reviewer finds the work particularly exciting - there is no champion."
            }
        },
        "id": "2EganKzQMF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YTo9KGNZ3U",
        "replyto": "YTo9KGNZ3U",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4419/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589282,
        "cdate": 1696707589282,
        "tmdate": 1701465531114,
        "mdate": 1701465531114,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes HyperGraph neural network for joint entity and relation extraction to alleviate the error propagation in marker-based pipeline models, and capture higher-order interactions between multiple entities and relations. For all the reviewers, the most concern is its novelty and contribution. Comparing with prior methods, there are indeed something new in the proposed method. It would be nice if the authors could make precise revisions according to their responses."
            }
        },
        "id": "EqQGwhOwGL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YSWLs0G5va",
        "replyto": "YSWLs0G5va",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4093/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583010,
        "cdate": 1696707583010,
        "tmdate": 1701465520030,
        "mdate": 1701465520030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper innovatively combines feature diversity sampling, low-rank matrix completion in tensor models, and improving entity disambiguation with limited labeled data.\n\nThe proposed method for entity linking is clearly explained, and the experimental results support the paper's claims. However, the experimental section primarily focuses on a specific dataset, which somewhat limits its ability to fully demonstrate the method's applicability in different scenarios."
            }
        },
        "id": "FwriZQxvMR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YQzgk43sFB",
        "replyto": "YQzgk43sFB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1812/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527756,
        "cdate": 1696707527756,
        "tmdate": 1701465442776,
        "mdate": 1701465442776,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an interesting idea of topic-based prompt learning for sense passage retrieval. The idea is well motivated. The proposed model incorporates well the idea. The experiments demonstrate the strength of the proposed approach against a single prompt.\n\nThe reviewers have found the idea new and interesting. The authors have answered most of the questions raised in the reviews in their rebuttal. It is expected that some of the replies will be added into the final version.\n\nOverall, the paper proposes a new and interesting idea for topic-dependent prompt. The paper may inspire other researchers in the future."
            }
        },
        "id": "9DOCvZEZKV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YO2VZBcinK",
        "replyto": "YO2VZBcinK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission674/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493453,
        "cdate": 1696707493453,
        "tmdate": 1701465407273,
        "mdate": 1701465407273,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper evaluates the performance of large language models (LLMs) on machine translation (MT) and automatic speech recognition (ASR) for different languages and dialects. The paper uses six LLMs from Google, Meta, and OpenAI, and tests them on 30 language varieties across 7 languages per task.. The paper measures the performance disparity between major/standard and minor dialects, and analyzes the factors that influence it. The paper finds that linguistic similarity to well-resourced language varieties is more important than socio-economic indicators or training data size. The paper also discusses the challenges and opportunities for improving dialectal NLP and hence has merits for promoting research for dialects."
            }
        },
        "id": "TSPw9PNsME",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YN6FtojPxD",
        "replyto": "YN6FtojPxD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2078/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533872,
        "cdate": 1696707533872,
        "tmdate": 1701465453053,
        "mdate": 1701465453053,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper delves into the problem of sample-relation guided confidence calibration on tabular data and introduces the RECAL framework, a post-training confidence calibration approach utilizing graph neural networks to model sample relations.\n\nThe overall consensus among most PC members is that this paper demonstrates good soundness but elicits mixed feelings regarding its level of excitement. After thoroughly reviewing the paper and the accompanying reviews, several notable concerns have emerged. These include:\n1. Lack of Comparisons: The paper would benefit from providing comparisons with existing approaches or techniques to contextualize its contributions better.\n2. Reproducibility: It is better to provide the source code for better reproducibility.\n\nIn my view, while the reviewers generally acknowledge the soundness of this paper, its excitement appears somewhat ambiguous. Consequently, I recommend that the authors carefully address these concerns during their revisions to strengthen the overall quality and impact."
            }
        },
        "id": "F7o7sWt9VW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YMlYb8cWgE",
        "replyto": "YMlYb8cWgE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission789/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495969,
        "cdate": 1696707495969,
        "tmdate": 1701465410415,
        "mdate": 1701465410415,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a translation alignment objective to increase the performance of multilingual vision/language models. The reviewers all agree that the experiments in the paper are comprehensive and sound. However, the main complaint is the lack of novelty because the paper seems to combine several existing approaches together. Given that the paper conducts comprehensive study of these approaches on multilingual vision-language setting, this paper could still have value for readers interested in this direction."
            }
        },
        "id": "KmGgR3HNWU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YKKcbwztwH",
        "replyto": "YKKcbwztwH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1544/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517014,
        "cdate": 1696707517014,
        "tmdate": 1701465433924,
        "mdate": 1701465433924,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper analyzes the problem of the presence of words or n-grams contained in the references of the training set in the automatically generated summaries. Four different corpora for summarization and several models for automatic summarization are used in the study. Different conclusions are presented, which are interesting for the improvement of the results in this area. Even with the widespread use of pre-trained language models to generate summaries, it still seems to be necessary to have corpora such as the ones analyzed for fine tuning and improving the summaries generated."
            }
        },
        "id": "XNQGoGgLec",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YJMUVwLcEi",
        "replyto": "YJMUVwLcEi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission293/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483937,
        "cdate": 1696707483937,
        "tmdate": 1701465393917,
        "mdate": 1701465393917,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new LLM benchmark called JEEBench, which comprises of pre-engineering mathematics, physics, and chemistry problems derived from the highly competitive IIT JEE-Advanced exam, testing LLM's reasoning and in-depth domain knowledge. The paper also introduces an innovative method for effective response selection by applying confidence-thresholding over self-consistency. The authors have identified and detailed the common failure modes of GPT-4, such as errors in algebraic manipulation, difficulty in translating abstract concepts into mathematical equations, and failure to retrieve relevant domain-specific concepts.\n\nIn the era of LLM, a comprehensive benchmark is very necessary for NLP community. This paper focuses on IIT JEE-Advanced exam, which is both hard and broad enough to test different capabilities of LLM.\n\nAll reviewers found the paper strong and exciting."
            }
        },
        "id": "1WCeBsCzSz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YHWXlESeS8",
        "replyto": "YHWXlESeS8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4005/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581062,
        "cdate": 1696707581062,
        "tmdate": 1701465516792,
        "mdate": 1701465516792,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper received mixed scores of 4,3,2 (soundness) and 4,4,2 (excitement).\n\nStrengths and weaknesses including the following were mentioned prominently:\n\nStrengths:\n- conceptual contribution to LLM evaluation (R1, R2, R3)\n- well-written (R1)\n\nWeaknesses:\n- leaves unclear how the proposed framework can be implemented (R1, R2, R3)\n- limited insights into relation to existing evaluation approaches (R1, R2)\n- framework was criticized as too general to be useful (R3)\n- may be reducible to PAC learnability (R3)\n\nR3 provides a low soundness scores and has the following major concerns: (1) the work is very preliminary, (2) without concrete instantiation of evaluators, it remains unclear if the framework can be used for nontrivial theoretical analysis, (3) the  definition of pseudo-intelligence may be linked to PAC learnability.\nI do not find (1), as explicated in the review, to be a major concern given the goals of this track.\nThe authors pushed back against the concerns (2) and (3), and I found their explanations to be convincing. In conclusion, I do not see major concerns against the soundness of this proposal for a theoretical framework for evaluation -- noting that it is certainly a first step, with substantial future work needed to work towards implementation."
            }
        },
        "id": "eAEPiZI8Tv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YGYaxZVsJK",
        "replyto": "YGYaxZVsJK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4954/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602197,
        "cdate": 1696707602197,
        "tmdate": 1701465545658,
        "mdate": 1701465545658,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes an encoder-decoder network to compositionally encode by explicitly factoring in constituency parses for sentences. Results are positive on a number of textual similarity and inference tasks. The reviews converge in their soundness and excitement scores."
            }
        },
        "id": "BfVpViDg6k",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YGUUT6CkbB",
        "replyto": "YGUUT6CkbB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2214/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537144,
        "cdate": 1696707537144,
        "tmdate": 1701465458039,
        "mdate": 1701465458039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All the reviewers agree on the fact that the paper provides a valuable contribution to the domain. In particular, the proposed entailment dataset extracted from Wikipedia will be useful for textual entailment recognition research, to complement existing resources. The experiments are solid, and provide interesting insights for research on RTE.\nThe authors have addressed the reviewers' concern in the rebuttal phase (in particular the fact of clarifying the contributions of the paper, as well as some technical aspects and justification of choices in the experimental setting), providing detailed descriptions of the unclear aspects in the paper."
            }
        },
        "id": "1oc3fmKsFY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "YGK9cd0bHz",
        "replyto": "YGK9cd0bHz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission99/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479218,
        "cdate": 1696707479218,
        "tmdate": 1701465387347,
        "mdate": 1701465387347,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a novel data augmentation technique for textual data, named GDA, which yields diverse and faithful samples. The methodology consists of inducing a grammar extended with slot information and diversifying it through rule manipulations. The results are especially promising in low-resource settings. Another strength of the paper is that it highlights its own limitations, as it mostly benefits tasks with NER-taggable tokens. However, the choice of CFG as the underlying grammar is questionable, as it restricts the space of sentences that can be successfully modelled by GDA. Another issue that remains not fully addressed is how to avoid error propagation in the proposed pipeline (e.g., incorrect NER slot predictions when these are not available in the data). To make the results more convincing, the Authors should compare the performance of DA baselines on datasets where slot annotation is available, possibly tone down their claims on reducing semantic errors which need more quantitative evidence, and add significance testing to the reported figures."
            }
        },
        "id": "jqmJfO9Upv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Y7kK2HcxDK",
        "replyto": "Y7kK2HcxDK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2988/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553515,
        "cdate": 1696707553515,
        "tmdate": 1701465483419,
        "mdate": 1701465483419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper deals with approaches based on multimodal datasets in order to propose sequential hierarchical classification. The hierarchical model can give results at different abstraction levels based on dedicated use cases. This paper highlights how the proposed system is effective in classifying multimodal tweets. All reviewers appreciated the soundness and excitement of this paper. This paper has great potential for acceptance for presentation at main conference.\n\nHere is a summary of the major points in the reviews.\n\nReasons To Accept:\n- The motivations to exploit social media and multimodality in disaster assessment, management and analysis of the severity of the damage and humanitarian sub-tasks in real time are convincing.\n- The proposed system is very interesting to deal with crisis events like natural disasters. \n- The new solid augmented dataset for the multi modal task is constructed.\n- The nice pipeline for putting together existing tools for a common goal which has the four subtasks is proposed.\n- Experiments are conducted with several state-of-the-art text and image embeddings.\n- The experiment result shows the proposed multimodal-processing approach marginally improve performance.\n\nReasons To Reject:\n- The used tequniques are not really original (but the paper proposes an attractive combination of up-to-date methods well evaluated)"
            }
        },
        "id": "FitJ7IO5YV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Y7Wx7usMtc",
        "replyto": "Y7Wx7usMtc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2156/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535782,
        "cdate": 1696707535782,
        "tmdate": 1701465456074,
        "mdate": 1701465456074,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary:**\nThe paper describes the impact of continuous updates to blackbox APIs for toxicity detention, often commercially available, like the Perspective API, on research reproducibility. The authors conduct a rigorous analysis comparing  different models and methods aimed at addressing toxicity, They emphasize issues related to the instability in research result due to irregular and frequently unannounced API updates. The contribution reveals major changes in high-profile benchmarking datasets for large language models such as RealToxicityPrompts dataset and UDDIA. Furthermore, the authors provide a set of concise recommendations for stakeholders to better navigate this line of research in terms of reproducibility.\n\n**Strengths:** \nThe reviewers unanimously agree on the paper's strengths, which are consistent across all three reviews. Here, we'll highlight the points initially outlined by the first review, which also apply to the other two reviews.\n\n- The paper pointed out an important problem of using Black-Box APIs for Toxicity Evaluation. This is an important finding because authors’ joint usage of outdated and fresh scores prevents a fair comparison of different techniques over time and leads authors to biased conclusions.\n- Supported by detailed data analysis and illustrations, the author clearly presents how REALTOXICITYPROMPTS distribution changes over time. And presented the impact of API changes on rankings of model risk, living benchmarks, and reproducibility of research contributions.\n- Authors provided several good recommendations on resolving the problem to both API maintainers and authors.\n\n**Weaknesses:** \nReviewers are in consensus concerning weaknesses. In particular, the actual experiments all focus on a single API – Perspective. As a result, the potential audience for this paper could be limited. Additionally, the first reviewer emphasizes that having an API that’s evolving overtime is definitely a good thing for the community and provides more accurate evaluation results. Furthermore, numerous existing static benchmarks or evaluation datasets are already overused by researchers. Consequently, this resulting in overfitting on these static benchmarks and evaluation datasets in subsequent studies. \n\n**Author-Reviewer discussion and acknowledgment:**\nThe authors have provided clarifications regarding the reviewers' concerns and have outlined how the paper will be enhanced in response. All the reviewers have taken into account the arguments and points emphasized by the authors in their rebuttal.\n\n**Conclusion:**\nThe paper is well-structured, but the reviewers recommend that the authors include additional references. Furthermore, the reviewers suggest that the authors carefully review the paper to enhance the clarity of the English presentation and rectify the identified typos."
            }
        },
        "id": "P5Ft9Fw8D4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Y6w2prqvjM",
        "replyto": "Y6w2prqvjM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2118/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534920,
        "cdate": 1696707534920,
        "tmdate": 1701465454727,
        "mdate": 1701465454727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper tackles the task of Implicit Sense-labeled Connective Recognition (ISCR) as a text-generation task by proposing an encoder-decoder model to generate both the connective and their sense labels while previous work has been addressing the problem as a classification task. The authors examined the relation between connectives and their corresponding sense labels and presented promising results of the proposed generation-based approach to the task. During the rebuttal period, the authors provided further details to address reviewers’ comments and questions regarding the implications of label distribution and hierarchical label classification task. \n\nThe paper would benefit from including results from level 2 labels as well as relevant data insights that can contribute to a better understanding of the results of the task in relation to the proposed approach, as mentioned by the reviewers (jhFv and mtYh)."
            }
        },
        "id": "OV9JVcK4TT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Y30NTg87od",
        "replyto": "Y30NTg87od",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2542/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544398,
        "cdate": 1696707544398,
        "tmdate": 1701465468509,
        "mdate": 1701465468509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In this paper, the authors propose a visual-textual knowledge graphs (VTKGs) benchmark datasets, where the triplet itself is explained using images and provides a detailed description of the meaning of entities and relationships. Meanwhile, they propose a knowledge graph representation learning method named VISTA that incorporates the visual and textual representations of entities and relations using entity relation encoding. The visual features are extracted by ViT-Base, and the textual features are extracted by BERT. Experimental results on four real-world datasets demonstrate that VISTA outperforms 10 different SOTA methods. As the reviewers mentioned, the paper writing should be improved and the authors should add the new results mentioned in rebuttal stage to revised version."
            }
        },
        "id": "0XhbJ1D9Ku",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Y2wUa9n7sr",
        "replyto": "Y2wUa9n7sr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2860/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551007,
        "cdate": 1696707551007,
        "tmdate": 1701465479166,
        "mdate": 1701465479166,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a corpus of cognate pairs and borrowed words for 5 Romance languages, the largest known corpus of its kind, with potential impact for the field of historical and computational linguistics. The content and presentation of this paper are of good quality, as reflected by its Soundness (three 4s, one 3), Excitement (two 4s, one 3), and Reproducibility scores (two 4s, one 5). In particular, the paper was praised for its literature review, the quality of the corpus presented (One reviewer commended the “human-on-the-loop process”), and sound experimental results that present a solid benchmark for the automatic detection of cognates. Only one reviewer cited reasons to reject the paper, questioning some aspects of the data collection methodology (e.g. the removal of special characters that indicate pronunciation). These concerns, alongside general comments and questions from all reviewers, were resolved during the author rebuttal period to reviewer satisfaction.\nIn light of this, only minor revisions, addressing reviewers’ comments and questions, need to be made to ensure this paper is camera ready."
            }
        },
        "id": "IhAvtz575X",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Y28GzovPql",
        "replyto": "Y28GzovPql",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2781/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549294,
        "cdate": 1696707549294,
        "tmdate": 1701465476143,
        "mdate": 1701465476143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces the idea of instructive dialogue summarisation, which combines dialogue QA, general summarisation and query-focused summarisation into a single task. All reviewers thought: (1) this idea is novel and well-motivated; (2) the paper is well-written and easy to understand; (3) the experiments are extensive and thorough, including multiple datasets, summarisation models, automatic and human evaluation; and (4) the approach of training data generation via LLM is intuitive, creative, and impactful. There are no major concerns raised, although the paper could benefit from the following minor revisions: (1) clarifying the task (instructive dialogue summarisation) better, in particular how it is related/different to query-focused summarisation; (2) discuss why QMSum is not included as one of its datasets (seeing it's very relevant to the task); and (3) discuss or provide preliminary results on the impact of using dialogues vs. references for generating training data."
            }
        },
        "id": "Y4Aaar19Yt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Y13EvAJlhQ",
        "replyto": "Y13EvAJlhQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission218/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482102,
        "cdate": 1696707482102,
        "tmdate": 1701465391536,
        "mdate": 1701465391536,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "When I look at this paper, I see some flaws and statements that will not translate to real hardware and personally, I find it a bit misleading. However, the overall response from the reviewers is strongly in factor of acceptance both in soundness and excitement, and I will not overrule their decision. As such, I recommend acceptance to the main conference."
            }
        },
        "id": "84XXI2a4kZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Y0PN9Eic8T",
        "replyto": "Y0PN9Eic8T",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4251/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586066,
        "cdate": 1696707586066,
        "tmdate": 1701465525518,
        "mdate": 1701465525518,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper reframes the text classification tasks, particularly in the zero-shot and few-shot settings, as matching class descriptions. They show strong performance on a number of tasks using many baselines. They also extend the Model-Agnostoc Meta Learning algorithm (MAML) in this context and show improved performance.\n\nThe reviewers noted several limitations with respect to evaluations only on tasks of limited token length (so affects length of class descriptions), focusing on tasks with semantic classes, and also using language models with fewer parameters. The authors have addressed some of these points in the rebuttal. Getting final soundness scores of (4,3,5) and excitement scores of (4,3,3)"
            }
        },
        "id": "h1r4yQJxRZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Xyy1p1IGvn",
        "replyto": "Xyy1p1IGvn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4778/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598339,
        "cdate": 1696707598339,
        "tmdate": 1701465541480,
        "mdate": 1701465541480,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a comprehensive benchmark for evaluating large language models on legal judgment prediction using the CAIL dataset. The work designs prompt-based solutions leveraging LLMs to map case facts to charges. An information retrieval system is incorporated to provide similar cases as context. The experiments analyze model performance under various settings like open vs multiple choice questions and zero-shot vs few-shot learning.  \n\nReviewer 1 highlighted the rigorous comparative analysis of multiple state-of-the-art LLMs as a strength. However, they felt using supervised learning baselines from existing legal judgment prediction literature would make the evaluation more compelling. They also questioned the lack of analysis around critical issues like fairness, expert evaluation, explainability and hallucination risks. In the rebuttal, the authors provide additional experiments with BERT-based models as baselines and acknowledge the importance of the highlighted issues for future work.\n\nReviewer 2 found the clean experimental design and straightforward performance comparison between models to be merits. A concern raised was whether the charge labeling task evaluated is a practical legal application. The authors clarify in the rebuttal that legal judgment prediction serves as a proxy task requiring multifaceted legal abilities, though other complex applications like case report writing could also be considered. The reviewer also sought more clarity on the implications of the \"paradox\" finding. The authors explain this points to the importance of retrieval quality and future research directions.\n\nReviewer 3 recognized the comprehensive analysis of prompting strategies as a strength. They suggested comparing results to general domain prompting and highlighting legal-specific challenges. The rebuttal provides examples of legal reasoning difficulties like charges with overlapping facts and penalties. The reviewer also asked about tokenizer differences which the authors analyze by correlating sequence length and performance.\n\nI agree with the reviewers that the paper is beset with  some of these limitations:\n- Lack of comparison to supervised baselines on LJP (Review 1). The authors provide additional experiments with BERT-based models in the rebuttal.\n- Unclear implications/applications of the \"paradox\" finding (Review 2). The authors clarify the takeaways for future work.\n- Overlap with general domain prompting effects without highlighting legal-specific challenges (Review 3). The rebuttal provides more legal case analysis.\n\n\nSummarily, the reviewers recognize this work provides one of the first comprehensive LLM benchmarks tailored for the legal domain, which is a novel contribution. While comparisons to supervised learning baselines are lacking, the authors provide additional experiments in the rebuttal indicating LLMs can outperform BERT-based models under low resource settings. They also acknowledge the importance of analyzing issues like fairness and plan to investigate them in future work.\n\nThe remarks about practical applications and implications of findings help highlight areas where the paper could be strengthened, but the core utility of extensive LLM analysis for an under-explored domain remains. The authors have made efforts in their responses to provide more legal case insights and clarify the takeaways.\n\nConsidering the authors' effort in addressing some of these issues in the rebuttal, I would encourage them to carry out thorough revisions to enrich the legal analysis, highlight unique challenges, and crystallize the implications of findings that could make this a stronger contribution."
            }
        },
        "id": "elm4Hyux2Y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Xyb8Qh6vxU",
        "replyto": "Xyb8Qh6vxU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5203/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608953,
        "cdate": 1696707608953,
        "tmdate": 1701465552849,
        "mdate": 1701465552849,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes augmenting training data for extractive QA by incorporating unanswerable questions, created by swapping antonyms and entities found in answerable questions. Experiments, including ablation studies, demonstrate that the proposed approach enhances performance on SQuAD (as discussed in the paper) and TidyQA (as mentioned in the rebuttal). The paper is well-structured and easy to follow. Its findings, including experimental setting, can be of interest to the QA community."
            }
        },
        "id": "2RftEbj60o",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XySq36VD0U",
        "replyto": "XySq36VD0U",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1042/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503614,
        "cdate": 1696707503614,
        "tmdate": 1701465418763,
        "mdate": 1701465418763,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a metric for evaluating dialogue appropriateness, but reviewers have expressed several concerns. First, there is a notable lack of empirical comparison with existing dialogue appropriateness metrics, despite listing them in the related work section. The reported correlations between six systems are system-wide, with no exploration of dialogue or turn-level correlations. Additionally, the metric focuses on pragmatic appropriateness while ignoring semantic appropriateness."
            }
        },
        "id": "SEcYLCaCso",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XySNnzF9Ir",
        "replyto": "XySNnzF9Ir",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5387/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612265,
        "cdate": 1696707612265,
        "tmdate": 1701465558189,
        "mdate": 1701465558189,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a multi-party conversation generation model that is speaker-aware and works when speaker annotation is missing.\n\nReviewers appreciated the realistic setting where speaker labels could be missing. There was strong consensus on the soundness of the paper.\n\nSome of the critical suggestions include:\n- Better details on the human evaluation setup.\n- Improved description of the graph-neural network.\n- Enhanced discussion on complexity analysis.\n- Baselines suggested by reviewers.\n\nThese changes should be incorporated in the revision as promised by the authors."
            }
        },
        "id": "gWhz6b39sR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XtlquCY7qs",
        "replyto": "XtlquCY7qs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1312/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510188,
        "cdate": 1696707510188,
        "tmdate": 1701465427148,
        "mdate": 1701465427148,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a method for table understanding in the few-shot setting using prompt and graph-based approaches. The main contributions of the paper include the application of prompt learning, specifically soft prompt or prefix tuning, in the context of table understanding, as well as the use of graph network pre-training to transform table values into a tabular graph. The paper presents experimental results on four benchmark datasets, demonstrating the effectiveness of the proposed method in classifying cell and table types. However, there are also several weaknesses and reasons. One major weakness is the lack of comparisons to state-of-the-art methods, making it difficult to assess the performance of the proposed method relative to other approaches. Additionally, the experiments are limited in scope, with missing combinations of datasets, baselines, and tasks.  \n\nOverall, while the paper presents some interesting and novel approaches to table understanding, there are several aspects where it falls short of expectations. The authors are encouraged to address these weaknesses in their work to improve its overall impact."
            }
        },
        "id": "VbSdbzuplv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Xt1JbFofwP",
        "replyto": "Xt1JbFofwP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2389/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541189,
        "cdate": 1696707541189,
        "tmdate": 1701465463620,
        "mdate": 1701465463620,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper discusses a generative retrieval approach that uses a learned, dynamic lexical identifier for the retrieval candidates. The proposed model, named Generative retrieval via LExical iNdex learning (GLENT), exploits a dynamic lexical identifier using a two-phase index learning strategy. The work is novel and interesting. It had some drawbacks, that were clearly identified by the reviewers and that were well addressed in the rebuttal."
            }
        },
        "id": "wm3kkQAbIG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Xqhdpk0Qrj",
        "replyto": "Xqhdpk0Qrj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3130/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556684,
        "cdate": 1696707556684,
        "tmdate": 1701465488064,
        "mdate": 1701465488064,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses an essential yet under-explored problem in Dialogue State Tracking: minimizing the amount of turn-level annotation. The proposed active learning approach at the turn level, which is based on maximum entropy, yields performance comparable to training with a complete (and considerably larger) training corpus. Both the ablation studies and case analyses offer valuable insights. A notable limitation of this research is the additional computational cost it introduces. While the authors have present the computation study in their rebuttal, I recommend incorporating the analysis into the camera-ready version of the paper.\n\nAll reviewers agree that the paper is solid and offers value to the dialogue field."
            }
        },
        "id": "L5lLf12dEC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XpK2LCt8iM",
        "replyto": "XpK2LCt8iM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission954/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500171,
        "cdate": 1696707500171,
        "tmdate": 1701465415960,
        "mdate": 1701465415960,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers are in consensus that the work is quite sound but the excitement level was low. The author responses went a long way to improve the soundness scores overall and addressing the reasons to reject from the reviewers.\n\nWhile the overall assessment is higher than the numerical scores assigned by the reviewers, this is mainly due to the extra information that was provided during the author response, so it is very important that all of the extra experimental results and clarifications are included as part of the paper by the authors including the missing references to previous work. The additional context provided by the information in the author response is a big reason for the final evaluation of this submission in this meta-review."
            }
        },
        "id": "ryj61kwhPn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XmS9J3Lvip",
        "replyto": "XmS9J3Lvip",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5852/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707619692,
        "cdate": 1696707619692,
        "tmdate": 1701465568434,
        "mdate": 1701465568434,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a decoding intervention framework for Grammatical Error Correction (GEC), where they employ an external critic to improve the choice of the next token. They investigate 2 types of critics – pre-trained LMs and a target-side Grammatical Error Detection (GED) critic that requires training. Experimental results on English and Chinese benchmarks are presented that show improvements over the baseline approaches and demonstrate performance competitive with SOTA models.\n\nThe paper is thorough and the experiments demonstrate the effectiveness of the approach. \n\nWhile the study is interesting, and the experiments are thorough, the main issue pointed out in the reviews is the lack of novelty. In particular,  one of the two critic components -- using a language model to improve GEC performance -- has been studied before. \n\nThe reviewers also express concern regarding the utility of the approach in practical applications, given that adding a critic increases the model complexity and thus is expected to increase decoding time. It would also be useful to see, given these complexity concerns,  whether increasing the GEC model size would have a similar effect, as adding the critic component."
            }
        },
        "id": "Am4M8wqlKT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XlIrJUKTgS",
        "replyto": "XlIrJUKTgS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5637/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616235,
        "cdate": 1696707616235,
        "tmdate": 1701465564274,
        "mdate": 1701465564274,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers find that this paper offers valuable contributions by exploring the potential of LLMs in code-tracing question generation for programming education, backed by a robust methodology and insightful analysis. It addresses a novel application area with interdisciplinary relevance, making it a strong candidate for acceptance in an NLP venue.\n\nThe authors raised some concerns with the review submitted by Reviewer 1, which does not seem to be of good quality (the scores are low, but the actual reasons to reject are missing). Moreover, the reviewers highlight reasons to reject such as lack of personalization and narrow scope (Java programming), which have more to do with the limitations of the paper (potential future work) than with the soundness of the described methodology.\n\nPaper Topic And Main Contributions:\n\nThis paper investigates the potential of using large language models (LLMs), specifically GPT-4, to automatically generate code-tracing questions for introductory programming courses. The study addresses the challenge of time-consuming and limited scalability in manually creating such questions and explores whether LLMs can provide a solution. The authors employ GPT-4 to generate questions based on code snippets and descriptions, comparing them to human-authored questions using various evaluation metrics. Notably, the results reveal that GPT-4 can produce questions of comparable quality to those crafted by humans, with over 50% being indistinguishable from human-authored questions. The contributions of this research include a high-quality dataset of both human and LLM-generated code-tracing questions, a rigorous evaluation methodology, insights into the potential of LLMs for educational content generation, and a foundation for considering their integration into learning platforms. In essence, this paper offers a promising solution to the challenge of automating code-tracing question generation in computer science education, providing a valuable dataset and evaluation framework in the process.\n\nReasons To Accept:\n\n* Novelty and Benefits for Educational Applications\n    * The paper introduces a novel application of LLMs in code-tracing question generation, offering a unique perspective distinct from previous NLP work.\n    * The generated data, when released, has the potential to bring significant benefits to the field of programming education and dialogue systems, addressing an important educational need.\n* Quality of the Methodology\n    * The authors have constructed a high-quality dataset tailored to the code-tracing scenario, which can serve as a valuable resource for future research.\n    * The introduction of a rigorous evaluation methodology, including human ratings and textual similarity metrics, sets a standard for assessing LLM-generated content, contributing to the field's evaluation practices.\n* Quality of the Analyses\n    * The paper provides insightful analysis, delving into the promise and limitations of LLMs in generating code-tracing questions, offering valuable insights into quality, diversity, and discernibility.\n\nReasons To Reject:\n\n* The study solely focuses on GPT-4 for code-tracing question generation and does not incorporate other LLMs, limiting the exploration of potential alternatives and comparative analysis.\n* The dataset used in the study consists of only 176 question pairs from limited sources, which raises concerns about the statistical robustness and generalizability of the findings. A larger and more diverse dataset is needed for a more reliable evaluation.\n* The metrics used for evaluating code-tracing questions are not comprehensive enough, and the setting of these metrics is somewhat subjective, lacking an automatic evaluation component."
            }
        },
        "id": "nvYCk2SbfI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XkexLrJDss",
        "replyto": "XkexLrJDss",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2812/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549937,
        "cdate": 1696707549937,
        "tmdate": 1701465477294,
        "mdate": 1701465477294,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a prefix-based method for efficiently updating general-purpose text representations, reducing computational costs compared to fine-tuning or multi-tasking. It uses task-specific prefixes to adapt the model for specific tasks while maintaining the same general text representation. Experimental results demonstrate its effectiveness across various tasks, outperforming or matching fine-tuning and multi-tasking while requiring fewer computational resources. \n\nThe problem addressed in this paper is intriguing, and the utilization of prefixes appears to be effective on the datasets used in the experiments. However, a notable question raised by the paper is the extent of computational savings achieved by the proposed method. While the paper frequently emphasizes computational efficiency, it lacks clear measurements or comparisons in this regard."
            }
        },
        "id": "kQX9tpuYRs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XjwNxSE0v8",
        "replyto": "XjwNxSE0v8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4383/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588634,
        "cdate": 1696707588634,
        "tmdate": 1701465529980,
        "mdate": 1701465529980,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes MeTaCo-XMT, a framework based on model agnostic meta-learning (MAML) for cross-lingual learning, focusing on data selection strategies that construct the meta tasks for MAML. Overall, the empirical results in the paper show that MeTaCo-XMT has better results than comparable frameworks, and some interesting takeaways are that using syntactic distance is important rather than semantic one. However, some questions on the limitations of experimenting only on mBERT and XLMR, and the data availability for each language on different tasks.  Based on the reviews, the AC acknowledges that the analysis and results across multiple tasks are interesting, but recommends improvement in presenting the results, especially in resolving the extremely small fonts used in multiple results tables."
            }
        },
        "id": "yoJXNPeQk1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XhR6ebeEXo",
        "replyto": "XhR6ebeEXo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1771/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707526882,
        "cdate": 1696707526882,
        "tmdate": 1701465440902,
        "mdate": 1701465440902,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Paper proposes a \"simple but effective\" method to add visual knowledge into vanilla dialogue data. Two new visually augmented datasets are provided as an outcome of the method.\n\n**Pros**: Most reviewers agree the contributions of the work, and in particular, the datasets provided by the work will be a good resource to the community. One reviewer comments explicitly on the experimental study, citing components (e.g., ablation and human evaluation) that provided confidence in takeaways and potential real-world application. Multiple reviewers find the work to be fairly exciting/interesting.\n\n**Cons**: One reviewer has concerns about the presence of visual biases - given that the dataset is automatically constructed. Another provides examples of experiments that would improve the work. Authors make some promises to ease concerns, but leave some challenges as future work. Reviewers appear to generally consider these technical concerns as minor after rebuttal. \n\nWith regards to novelty, there is some disagreement. While two reviewers find the work to be exciting/interesting - one of which takes care to highlight many of the strengths of the work - another reviewer finds the approach to be more incremental, emphasizing that the reliance on larger data sources may not be a significant contribution. Given the disagreement, I looked into the rebuttals and some paper details as well. From a methodological standpoint, I agree \"larger data source\" is not a significant contribution. *But,* some novelty is also owed to the use of refined visual knowledge (entity + turn level). Indeed, this refinement in approach leads to two new datasets which are well positioned as novel by the authors (Table 1)."
            }
        },
        "id": "D5kCbVTkjo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Xd2A31vcLd",
        "replyto": "Xd2A31vcLd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5864/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707620203,
        "cdate": 1696707620203,
        "tmdate": 1701465568652,
        "mdate": 1701465568652,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Inspired by conceptual metaphor theory, this paper presents a new model for metaphor detection, (specifically for pairs of words in context), using a Siamese network and contrastive loss. The reviewers were impressed with the quality of writing and the quality of the experiments (\"comprehensive and clear\").\n\nIn terms of scores, reviewers were balanced between good and strong / ambivalent and strong. However, the authors provided extensive responses in their rebuttal, including updated language and promised changes, which largely seem to mitigate the (mostly minor) concerns. In addition, the most favorable reviewer was the most engaged with the authors during the discussion.\n\nAlthough I am not convinced that the relatively poor performance of ChatGPT can be taken as conclusive (given the number of degrees of freedom in such a setup), that part is not particularly important for this paper to succeed. Rather, it can stand on its strong experimental results (relative to other methods), the quality of its writing, and its contributions to an interesting area."
            }
        },
        "id": "3SyCBgQBmp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XclSRY9Wp8",
        "replyto": "XclSRY9Wp8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission431/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487523,
        "cdate": 1696707487523,
        "tmdate": 1701465398733,
        "mdate": 1701465398733,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a rule distillation framework for relation extraction and comprises three components: (a) a reasoning rule generator, (b) a compound rule compiler to combine multiple rules, and (c) a relation extractor trained on both supervised and rule-labeled data. The proposed method is evaluated on sentence-level relation extraction in a low-resource setting.\n\nThe paper is moderately well-written and represents an incremental advancement in existing rule-based weak supervised methods. However, it lacks some references and semi-supervised baselines. Following discussions, the model was subsequently evaluated on two new datasets, Chemprot and Re-TACRED, as the original dataset TACRED was found to contain incorrect labels. The results indicate a slight performance improvement compared to the current baselines, and this could be further strengthened through statistical significance testing."
            }
        },
        "id": "6DUfagX0mn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XcNXOVhNlN",
        "replyto": "XcNXOVhNlN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission761/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495354,
        "cdate": 1696707495354,
        "tmdate": 1701465409614,
        "mdate": 1701465409614,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Three reviewers provided feedback for this paper and their reviews were in consensus. Overall, reviewers found the work to be novel and significant, particularly the new network architecture. They appreciated the results, i.e. the high scores on multiple benchmarks, as well as the ablation analysis provided by the authors. The main complaints included the lack of extensive qualitative examples and a clear comparison/clarification to recent related work. There was also a concern that the method does not work well for small objects, but I think this isnt a strong concern since it is unreasonable to expect a single paper to address all concerns within a problem. Overall, post rebuttal, reviewers continue to be happy with this paper and fairly excited about it. I agree with this feedback."
            }
        },
        "id": "H2o7U8dPbl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XbcprEi57p",
        "replyto": "XbcprEi57p",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission228/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482460,
        "cdate": 1696707482460,
        "tmdate": 1701465392086,
        "mdate": 1701465392086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a novel distillation framework where student and teacher models are co-trained to enable superior transfer between them.  The framework is novel and the paper is well written. The rebuttal has acknowledged some of the limitations around the experimental setting (instantiations of the student and teacher models). The performance gains are small but significant. I think the community would benefit from this novel framework."
            }
        },
        "id": "hQYFQz7fBR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XX73vFMemG",
        "replyto": "XX73vFMemG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission185/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481334,
        "cdate": 1696707481334,
        "tmdate": 1701465390516,
        "mdate": 1701465390516,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper explores the impact of retrieval on pretraining autoregressive LLMs. It reproduces the RETRO model and compares it with GPT, and proposes a new variant of it called RETRO++. The paper presents several useful experimental analyses and discussions such as performance gains and impact of retrieval database quality. The community can benefit from this work in furthering research on retrieval-augmented generation, which is becoming increasing important given the advent of LLMs. \n\nFor the authors — please include the additional experiments and insights you shared in the author responses, they would make the paper much stronger."
            }
        },
        "id": "xZfp871Jre",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XW4t7P2hpN",
        "replyto": "XW4t7P2hpN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2937/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552502,
        "cdate": 1696707552502,
        "tmdate": 1701465481795,
        "mdate": 1701465481795,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Strengths**:\n\n1. Discrete prompt optimization with more human readable prompts is clearly an important problem.\n\n2. \"attack\" prompts analysis to determine whether optimized prompts are true knowledge probes is a good contribution. Discovery of misalignment between constructed prompts and knowledge in current prompting methods is intriguing.\n\n3. The discussion provides a lot of clarifications, and extra results.\n\n**Suggestions**: There is a wealth of clarifications and new information in your rebuttal. Request you to incorporate the same in your draft."
            }
        },
        "id": "mW8sPOMuOP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XT1hoHqs12",
        "replyto": "XT1hoHqs12",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1411/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512460,
        "cdate": 1696707512460,
        "tmdate": 1701465430054,
        "mdate": 1701465430054,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary: This paper introduces a new dataset called SCITAB, which is designed to address the limitations of existing benchmarks for scientific fact-checking. The dataset consists of 1225 challenging scientific claims that require compositional reasoning with scientific tables. These claims are derived from actual scientific statements, and the evidence is presented in the form of tables, which mirror real-world fact-checking scenarios. The authors have established benchmarks on SCITAB using state-of-the-art models and have identified unique challenges, such as ambiguous expressions and irrelevant claims. \n\nStrengths: The main contribution of this paper is the creation of the SCITAB dataset. All the reviewers unanimously agree that this is a solid contribution and provides a new resource for the research community to further explore and improve scientific fact-checking. The paper is well-written and easy to follow.\n\nWeaknesses: I don't think there are any major weaknesses -- some of the weaknesses have been addressed during the discussion phase. The authors should take the experimental results during the rebuttals into account when preparing for the final version of the paper."
            }
        },
        "id": "CUSvG4WPer",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XQm8tlPKgY",
        "replyto": "XQm8tlPKgY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2316/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539295,
        "cdate": 1696707539295,
        "tmdate": 1701465461095,
        "mdate": 1701465461095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces CoherentED, an entity disambiguation system that incorporates topic embedding of context sentences extracted by VAE and an external category memory to retrieve relevant categories for undecided mentions. The approach design is reasonable and effective though the key components of CoherentED are borrowed from existing studies. Experimental results demonstrate promising improvements on 6 benchmark datasets. One main concern from most reviewers is the experiment analysis: more discussions about the performance on long documents, supervised performance on AIDA, latency, etc. are needed."
            }
        },
        "id": "IBk94TpAwK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XNnFTKCacy",
        "replyto": "XNnFTKCacy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission746/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495051,
        "cdate": 1696707495051,
        "tmdate": 1701465409133,
        "mdate": 1701465409133,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main conclusions of the reviews and the post-rebuttal discussions:\n- 2/ 3 reviewers consider the paper sound (scores 2, 5, 3)\n- 3/ 3 reviewers find the paper exciting (scores 4, 3, 3)\n\nFrom reading the rebuttal and seeing the scores above, I find that the reviewers consider strong points for soundness the following:\n- tackle an important problem: (1) how to predict an LLM's performance without actually running the experiment, and (2) how to quantitatively select the best subset of comprehensive benchmark for quick evaluation.\n- the authors present the idea of “small-bench,” which could reduce computational barriers for models with limited training computation \n- solution is effective even for challenging scenarios.\n- comprehensive experimental results\n- easy to read"
            }
        },
        "id": "hssjkOXAmv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XMpzcC9L5z",
        "replyto": "XMpzcC9L5z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2516/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543741,
        "cdate": 1696707543741,
        "tmdate": 1701465467374,
        "mdate": 1701465467374,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a new approach to simultaneous speech translation, where real-time translations are produced by a system as audio is incrementally consumed. Reviewers were impressed with the proposed method, both in terms of soundness and novelty, especially after the authors addressed some of the concerns during the rebuttal period. The reviewers also commented positively on the paper's writing and the thoroughness of the experiments. The result is a good piece of work that could have wide interest."
            }
        },
        "id": "O33gUWnBsT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XLXCWNNWvL",
        "replyto": "XLXCWNNWvL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2522/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543907,
        "cdate": 1696707543907,
        "tmdate": 1701465467734,
        "mdate": 1701465467734,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The mojority decision of the reviewers does a fair job of weighting the pros and cons of the work as presented. The updated results and cleanup of the material should be reflected in further edits of the paper inorder to clarify the discussion and highlight the contributions."
            }
        },
        "id": "Iblz7XPqjm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XJRNw74kXK",
        "replyto": "XJRNw74kXK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3973/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580419,
        "cdate": 1696707580419,
        "tmdate": 1701465515762,
        "mdate": 1701465515762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses the task of multimodal dialog act classification in an online setting – raw audio and ASR text. \n\nReviewers appreciated the alignment of assumptions towards real-world applications. They also found the experimentation to be comprehensive and sound.\n\nFrom a technical perspective, the excitement was average as the components proposed in the paper are fairly standard and known.\n\nIn addition, some of the choices and assumptions in the architecture were found to be confusing – which warrants improvements in the writing."
            }
        },
        "id": "BV2iRnW6ux",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XILoK6g4va",
        "replyto": "XILoK6g4va",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4291/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586815,
        "cdate": 1696707586815,
        "tmdate": 1701465526770,
        "mdate": 1701465526770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Using Reinforcement Learning (RL), this work proposes a method for training generative style-transfer models that is particularly useful in settings with limited parallel data. Additionally, this work demonstrates out-of-domain generalization and shows improvement over large language models (LLMs) such as GPT-3, despite being significantly smaller in size. One main concern raised by reviewers is the comparatively complicated method required to train the model, especially when compared to the simplicity of prompting or zero-shot with LLMs. However, I would argue that this isn't really a fair comparison. While LLMs offer the convenience of prompting, they are also computationally expensive. In many cases, fine-tuned, smaller models should outperform zero-shot prompting, as is the case here.\n\nAnother point raised by reviewers is whether the evaluation adequately captures general improvement. While STEER has a superior aggregate score, it's important to note that it excels significantly in terms of style strength at the expense of meaning similarity. Therefore, the utility of STEER may depend on whether preserving the original meaning is a requirement or not."
            }
        },
        "id": "9d94T84f6V",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XIHl40UylS",
        "replyto": "XIHl40UylS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5147/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608065,
        "cdate": 1696707608065,
        "tmdate": 1701465551190,
        "mdate": 1701465551190,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a context-aware framework for argument structure extraction (ASE). The method, ECASE, consists of a sequence attention module, a distance-weighted similarity loss, and a discourse marker masking technique. The paper evaluates ECASE on five benchmark datasets and shows that it outperforms context-aware baselines on most of them.\n\nThe reviewers have mixed opinions about the paper. Reviewer 1 is very positive and praises the paper for its clear motivation, well-designed method, and thorough experiments ablation study. Reviewer 2 is ambivalent and gives two reasons to accept the paper (clear presentation and superior performance) and two reasons to reject the paper (unclear ablation results and marginal techniques). The reviewer was not convinced by the authors' response. Reviewer 3 is borderline and gives only one reason to accept the paper (exploiting contextual information) and several reasons to reject the paper (outdated model design, lack of comparison methods, inconsistency with a previous paper, and missing ablation experiment). This reviewer didn't change their opinion after reading the authors' response to these concerns. \n\nBased on these reviews and other comments, I think the paper has some merits in proposing an efficient context-aware ASE model that achieves state-of-the-art results on some datasets, but it also has some weaknesses in its comparison with older work in terms of model design, ablation analysis, and presentation quality. The authors should address the reviewers’ comments and questions in their revision and clarify their contributions and limitations."
            }
        },
        "id": "sPRsPtXVia",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XHftyT3k4j",
        "replyto": "XHftyT3k4j",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1608/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707519693,
        "cdate": 1696707519693,
        "tmdate": 1701465436021,
        "mdate": 1701465436021,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces the FORD framework, a novel approach to investigate the inter-consistency and collaboration capabilities of Large Language Models (LLMs) by facilitating debates among them. Through various scenarios like fair debate, mismatched debate, and a roundtable setup, the research assesses whether LLMs can effectively collaborate and converge in their thinking to improve answer quality, especially in commonsense QA datasets.\n\nThe strengths of the paper include: \n1) This paper presents early research that systematically evaluates inter-consistency among LLMs \n2) Several ideas are very innovative such as INCON measure. \n3) The paper conducted a comprehensive experiment across 7 datasets. \n4) the paper is well written and easy to read\n\nThe main concerns from reviewers are: \n1) some parts of the paper require clarification \n2) Most of the results rely on closed-source models, which affect the reproducibility. \n3)  Some conclusions are inconsistent. \n4) experimental comparison is inadequate."
            }
        },
        "id": "7srHr1Ypgh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XEwQ1fDbDN",
        "replyto": "XEwQ1fDbDN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2405/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541579,
        "cdate": 1696707541579,
        "tmdate": 1701465464366,
        "mdate": 1701465464366,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a large dataset of NLI annotated with cultural awareness. The resulting CALI dataset would enable more research into how cultural differences affect the task of NLI. This work also opens future research directions with other languages and cultural differences. While, as pointed out by one reviewer, the dataset is limited to just one other culture of India, it is nevertheless challenging and important to build this cross-cultural dataset, and I encourage the authors (and our NLP research community) to broaden the set of cultures considered in future work.\n\nThere was some concern that this research collected human annotations but does not present IRB approval. Please describe how the research ethics were considered and adhered when collecting the annotations and go through IRB if such process exists in your research institution."
            }
        },
        "id": "uTwhACmZKb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XEBHsJpFY9",
        "replyto": "XEBHsJpFY9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission796/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496135,
        "cdate": 1696707496135,
        "tmdate": 1701465410689,
        "mdate": 1701465410689,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel pipeline to automatically create unanswerable questions given QA examples. The pipeline includes four key components: question perturbation, paraphrase detection, answerability prediction, and filter of unanswerable questions. All reviewers appreciated the well-structured content, innovative methods, and comprehensive experiments."
            }
        },
        "id": "644DFD6IOg",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "XB0u7RTXrV",
        "replyto": "XB0u7RTXrV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission611/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492044,
        "cdate": 1696707492044,
        "tmdate": 1701465405226,
        "mdate": 1701465405226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a novel autoregressive text-to-text retrieval model designed to learn the smart reply task from a dataset using a bootstrapping method for generating data.  Extensive experiments demonstrate superior performance of the proposed method over baselines in terms of relevance and diversity.  The reviewers requested some clarifications in their reviews, which were addressed by the authors in their rebuttal.  Another concern raised was the absence of a subjective (human) evaluation, which the authors addressed to some extent by pointing to evidence of correlation between ROUGE and click-through rate.  However, the addition of a subjective human judgement evaluation of relevance and diversity might have been useful."
            }
        },
        "id": "BG1vXQSBG2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "X6HDI4cqWF",
        "replyto": "X6HDI4cqWF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2697/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547829,
        "cdate": 1696707547829,
        "tmdate": 1701465473803,
        "mdate": 1701465473803,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new task (derived from wikidata) for LLMs knowledge editing, which measures the dependency of knowledge -- whether the editing of learned facts respects internal logical constraints. The eval has two metrics (measuring lexical variation and the disruption of irrelevant facts), and has 2 phases:\n1. The establish phase prompts a model to extract its knowledge about the facts and implications within a knowledge set. \n2. The update phase edits some facts and tracks the change of the facts and accordingly implications in the updated model.\n\nThe paper adopt a semi-supervised online learning process:  a) initial fine-tuning of LLMs  b) \"retrieval\" of relational facts from LLMs  c) \"repairing\" or validating generation errors, against a number of rules  d) re-use corrected facts to further fine-tune the LLM\nExperiments with  baselines (continuous finetuning, MEND, MEMIT) and 2 LLMs (BART, GPT-XL) shows that  the baselines do not establish logical dependencies when editing knowledge, and future research is needed to improve this.\n\nStrength:\n1. The proposed task and the dataset described are novel, and would likely be impactful for future work in knowledge editing\n2. The experiment and analysis is comprehensive.\n\nWeakness:\n1. The reviewers also like to know more details about the quality of the ratings.\n2. The reviewers have some concerns over the importance of knowledge editing in general, and the downside of iterative finetuning. I guess these are not specific to this paper but the research area in general. \n3. The result is restricted to only one dataset"
            }
        },
        "id": "BGk1V4F6d1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "X6DrwxlMD9",
        "replyto": "X6DrwxlMD9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3760/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707573984,
        "cdate": 1696707573984,
        "tmdate": 1701465509029,
        "mdate": 1701465509029,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Based on the reviews, the paper proposed a better method for code-switching in spoken language understanding (SLU). The main contributions include the selection of critical words for code-switching and the consideration of contextual information. Reviewers appreciated the outperformance of the proposed method compared to existing methods and the effectiveness shown in the ablation studies. However, there were concerns about the lack of novelty in one of the proposals and the dependency on the quality of dictionaries, which may limit the applicability to low-resourced languages. \n\nOn the positive side, the reviewers mentioned that the paper is easy to understand and reproduce, and the results are promising. They also found the proposed techniques to be well-explained and the experimentation and analysis to be solid. However, they suggested conducting more qualitative analysis to provide concrete examples of the improvements. They highlighted the potential generalizability of the proposed techniques to other tasks.\n\nTo summarize, the work is considered to be of strong soundness and it provides sufficient support for its claims and arguments. There is generally strong excitement about the paper, as it contributes to the understanding of code-switching in SLU and offers potential improvements. However, there are concerns about the lack of novelty in one aspect of the work and the limited applicability to low-resourced languages. The reviewers suggest including more qualitative analysis and exploring the generalizability of the proposed techniques."
            }
        },
        "id": "rIhLL9Dl5K",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "X597Q58y1U",
        "replyto": "X597Q58y1U",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1915/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529963,
        "cdate": 1696707529963,
        "tmdate": 1701465446963,
        "mdate": 1701465446963,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores how to use linguistic knowledge to enable resource-efficient adaptation via hyper-networks on 5 English dialects. The most interesting part of this paper is the usage of expert knowledge instead of labeled data. The paper sheds light on an interesting and under-studied phenomena: dialects. In general the reviewers agree that the proposed method is strong, as well as the experimental setup and the used baselines. Additionally ablation studies and a in-depth analysis was also performed. \n\nThe limitations of this paper are: that the method is only tested on RoBERTa-base (g9kY);  that it is tested on synthetic data and that this dataset is not well describe and might be insufficient to measure the linguistical diversity (i59w) ; and a lack of proper comparison with other dialectic adaptation methods (wWbU). \n\nOverall the reviewers have strong scores for this paper. However, I would also take in consideration the weaknesses pointed out."
            }
        },
        "id": "ACKV3L5wXd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "X570XzeYSW",
        "replyto": "X570XzeYSW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4561/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593001,
        "cdate": 1696707593001,
        "tmdate": 1701465534745,
        "mdate": 1701465534745,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the effect of two types of data manipulation on the generalization ability of Bert models. Reviewers gave different recommendations for this paper. R1 found this paper insightful to study the interaction between these two augmentations. R2 and R3 found this paper rather narrow due to concerns about the training size, unclear comparison, and limited tasks. The AC read the full paper and the discussions and stood with R1. The motivation and the scope of this paper have been clearly described, and the authors conducted most of their study under a fixed training size. Increasing the training size leads to even more improvements. Hence, there AC deemed this paper made good empirical discovery and should be accepted to Findings."
            }
        },
        "id": "jIGuEDWdsm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "X2R4yhtenj",
        "replyto": "X2R4yhtenj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission419/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487186,
        "cdate": 1696707487186,
        "tmdate": 1701465398328,
        "mdate": 1701465398328,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper delves into the false negative issues within conversational response selection tasks. To enhance negative sampling, the authors put forth a variational method accounting for diverse characteristics, along with the TRIGGER framework to dynamically refresh negatives based on the model's evolving capacity. This approach is tested on various retrieval models and validated using two renowned benchmarks: the Ubuntu and Douban datasets.\n\nSoundness scores stand at (4, 3, 4), reflecting reviewers' consensus on the paper's solidity. The presented claims are well supported by the studies and experimental data.\n\nOn the other hand, excitement scores read (3, 2, 3). A primary critique highlights the proposed solution as overly complex, with limited generalizability and practical application. The performance improvement is marginal. However, there is still a notable highlight is section 2, which presents an insightful human-centric pilot study on false negatives."
            }
        },
        "id": "Bv43zYMZBa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Wz1jEwvpGO",
        "replyto": "Wz1jEwvpGO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2854/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550839,
        "cdate": 1696707550839,
        "tmdate": 1701465478785,
        "mdate": 1701465478785,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Pros:\n- It is valuable to assess the implicit assumptions in prompt perturbation about the correct or human way to handle a misleading or irrelevant prompt. They find the \"human\" way to handle things is not to execute misleading instructions, but to make reasonable adjustments,  and to ignore irrelevant information. The authors argue that papers that test the consistency of prompting make this assumption often, but it has never been tested in humans.\n- This paper is a direct response to a specific work on NLI, and mimics their setup, which justifies the focus on a single task.\n\nCons:\n- Results are limited to only NLI. Not only does this make the title overly broad, but as 3rDv pointed out, the particular nature of the NLI task could easily shape the findings because it's not clear how to handle certain perturbations without direct instructions.\n- Real world annotation is often performed with complex instructions, so it is hard to see whether these results would generalize in more complex settings."
            }
        },
        "id": "mU2E32BlPp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Wyod73NboS",
        "replyto": "Wyod73NboS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4030/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581585,
        "cdate": 1696707581585,
        "tmdate": 1701465517420,
        "mdate": 1701465517420,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a Sequence-to-Structure approach for document-level targeted sentiment analysis (TSA), addressing the challenge of multi-level entity extraction and sentiment prediction in review documents. They employ DeBERTa to extract entity-sentiment pairs, construct a graphical model capturing semantic relations between entities, and identify hierarchical affiliations among entities. Experimental results demonstrate the approach's effectiveness, outperforming existing methods on multiple benchmark datasets.\n\nReasons to Accept:\n- Addresses the underexplored area of document-level targeted sentiment analysis.\n- Introduces a technically valuable multi-grain graphical model using GCNs to capture entity dependencies.\n- Demonstrates substantial performance improvements across multiple datasets, showing real-world impact.\n-Proposes a Sequence-to-Structure approach for modeling hierarchical structures among opinion targets.\n- Utilizes a multi-grain graphical model to capture long-distance dependencies among entities.\n- Evaluation of the approach on six datasets with detailed analysis.\n- Effectively addresses document-level TSA, considering the interplay of sentences and document structure.\n- Well-written and comprehensible text.\n\nReasons to Reject:\n- Limited representation of longer documents in experiments, with unclear performance on typical document-level TSA scenarios.\n- Lack of groundbreaking innovation in the core techniques used.\n- Uncertainty about performance on noisy or massive documents and concerns about limited generalizability.\n- Absence of user studies to assess practical usefulness.\n- Potential difficulties in generalization to diverse document types and domains.\n- Issues with dataset selection and potential challenges in data accessibility.\n- Baseline models lacking diversity and potential issues with the experimental design."
            }
        },
        "id": "8Nz8ZKof3h",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Wy4adj2FUJ",
        "replyto": "Wy4adj2FUJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1495/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515730,
        "cdate": 1696707515730,
        "tmdate": 1701465432485,
        "mdate": 1701465432485,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new architecture that provides direct access to passage rep and question history for conversational QA, through a gated memory layer. This provides inference-time space savings that become increasingly significant as the conversation grows.\n\nThe new approach achieves significantly worse performance than recent architectures that use contextualized embeddings. However, those also require much more memory to run. Compared to similar base architectures, the new approach performs well although it should be noted that the closest comparison from Reddy et.al 2018 achieves essentially the same performance, in terms of accuracy.\n\nMultiple reviewers raised questions about the significant gap in performance to more recent, Transformer-based architectures and whether the accuracy / space trade-off is reasonable, especially for the 0-ctx case. It is clear that the current focus of the paper is not just increasing the accuracy metrics but the work would benefit from a deeper discussion of these tradeoffs and / or discussion of how they could be overcome."
            }
        },
        "id": "a1AdWH9LaL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WxxYSpsv97",
        "replyto": "WxxYSpsv97",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5896/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707621365,
        "cdate": 1696707621365,
        "tmdate": 1701465569111,
        "mdate": 1701465569111,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes techniques for improving the efficiency of prompt tuning in a federated learning setting. The core of the method is around selecting a subset of most impactful layers to update, and efficiently communicating the gradients. Reviewers found the proposed methods solid and novel, the experiments to be mostly convincing, and the problem and approach to be of high interest to the community. Most of the concerns were around the scope of experiments, which were mostly addressed by the new results provided during the rebuttal and will be added to the paper. I will only add that the evaluation can be cleaner if some confidence intervals were reported, and also that I was surprised not to see a comparison to LoRA for a parameter efficient training method."
            }
        },
        "id": "8fhSHDyRmT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WuuxbObghx",
        "replyto": "WuuxbObghx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2407/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541595,
        "cdate": 1696707541595,
        "tmdate": 1701465464392,
        "mdate": 1701465464392,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper focusses on the description and evaluation of TheoremQA, a novel dataset designed to assess the capabilities of Large Language Models (LLMs) in applying theorems to solve complex scientific problems. TheoremQA is unique in its approach, as it requires the application of appropriate theorems to answer questions, making it a challenging benchmark for AI models. The dataset comprises 800 high-quality questions that are based on over 350 theorems spanning diverse fields such as Mathematics, Physics, Electrical Engineering & Computer Science, and Finance. The papers highlight extensive experiments conducted on 16 different LLMs using various prompting strategies. Among the models evaluated, GPT-4 outperformed others, while open-source models lagged in performance. The research provides valuable insights into the strengths and limitations of current AI models in scientific reasoning. Overall, TheoremQA serves as a significant benchmark that promises to drive further research in the domain.\n\nThe paper's introduction of TheoremQA, comprehensive evaluations, clear presentation, and valuable insights make it a significant contribution to the AI research community. In particular: a)  The paper introduces TheoremQA, a novel dataset designed for theorem-driven question-answering. It's the first dataset of its kind, covering a diverse range of theorems from fields like Mathematics, Physics, Electrical Engineering & Computer Science, and Finance. This dataset is poised to be a valuable resource for the research community. b) The authors have conducted an extensive evaluation of 16 state-of-the-art Large Language Models (LLMs) using TheoremQA. Their analysis is thorough, revealing both the strengths and limitations of current models in applying theorems and understanding multimodal inputs. c) The research employs innovative prompting strategies, such as Chain-of-Thoughts (CoT) and Program-of-Thoughts (PoT), to guide the LLMs in their evaluations.\n\nHowever, one of the primary criticisms is the imbalanced distribution of questions within the dataset. Out of the 800 questions presented, a significant majority are from the Mathematics domain. This uneven distribution could limit the dataset's applicability and versatility across diverse fields, potentially skewing results or insights derived from it."
            }
        },
        "id": "NjqBXA290D",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Wom397PB55",
        "replyto": "Wom397PB55",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1768/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707526772,
        "cdate": 1696707526772,
        "tmdate": 1701465440829,
        "mdate": 1701465440829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes HARL, a method for performing the automatic coding task (i.e. predicting diagnosis codes based on clinical text). HARL uses attention on diagnosis label taxonomy to learn useful features for rare/imbalance diagnosis codes, and empirically showed favorable performance. All reviewers agreed on the value of the task itself, and the performance is promising, there were several strong concerns such as: 1) HARL's performance on predicting rare diagnosis codes (which is the core motivation of this paper) are comparable to baselines; 2) HARL makes an exaggerated claim of novelty regarding the use of diagnosis code hierarchy, which has been explored multiple times before in the healthcare domain 3) HARL fails to accurately position its core claim between medical text classification and general imbalanced classification algorithm."
            }
        },
        "id": "yvMIRNkDSY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WmpyDkTHvI",
        "replyto": "WmpyDkTHvI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1397/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512177,
        "cdate": 1696707512177,
        "tmdate": 1701465429638,
        "mdate": 1701465429638,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a scalable dialogue state error correction approach, and when paired with DST models (to which the DSC is agnostic), the system achieves state-of-the-art performance on the MultiWOZ datasets. The empirical studies presented in the paper is considered as rigorous, with room for improvement by expanding experiments towards non-MultiWOZ DST datasets. Also the practical limitations of employing a DSC component should be discussed more thoroughly in the paper."
            }
        },
        "id": "CNxaeLoIM3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WkpTWlXGHC",
        "replyto": "WkpTWlXGHC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5231/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609408,
        "cdate": 1696707609408,
        "tmdate": 1701465553518,
        "mdate": 1701465553518,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are in agreement that this a strong paper that poses interesting questions and appropriately investigates those questions experimentally. They feel that the community will benefit from the paper's proposed methods for investigating a model's performance disparities across languages, and from its experimental results showing that models exhibit translating-like behavior in multilingual settings."
            }
        },
        "id": "1PsjuUvMPG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WiKLXsWzBy",
        "replyto": "WiKLXsWzBy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1901/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529680,
        "cdate": 1696707529680,
        "tmdate": 1701465446408,
        "mdate": 1701465446408,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This is a strong paper according to most reviewers. They value the effectiveness and novelty of the method, the proper referencing, clear explanations and motivations.  Several of the issues brought up by reviewers were properly addressed by the authors, for example, they conducted experiments on the datasets proposed in Gosh et al.’s work. All in all, the authors did a good job in addressing the reviewer’s concerns and building trust in being able to improve the final version of this paper, which resulted in some scores being raised. There are four reviewers for this paper as one of the three initial reviewers explained that the topic was outside their field of expertise. I am basing my meta-review on the three other reviewers. Both soundness and excitement scores are close to 4 on average."
            }
        },
        "id": "cn07Lg5zMV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Weszm4zCzP",
        "replyto": "Weszm4zCzP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2768/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548951,
        "cdate": 1696707548951,
        "tmdate": 1701465475610,
        "mdate": 1701465475610,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a clever adaptation of recent advances in LLMs - task decomposition and step-by-step reasoning to the task of NER. Various strategies are used to incorporate syntax prediction as an intermediate step and break down the eventual decision making problem into stage-wise decisions and their impact is carefully studied. All the reviewers like this paper. There was an extensive discussion on this paper and more analyses were provided by the authors in support of their results. These new results look convincing to me."
            }
        },
        "id": "qiaWImEYeV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WVs1qhIUms",
        "replyto": "WVs1qhIUms",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2811/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549919,
        "cdate": 1696707549919,
        "tmdate": 1701465477270,
        "mdate": 1701465477270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a chain-of-thought style prompt editing method that iteratively enhance initial prompts with techniques of beam search and bandit algorithms. It demonstrates its efficacy on several classification benchmark datasets. Experiments are well-executed and comprehensive baselines are well compared. However, the proposed method requires minibatches of a training set and large amounts of queries, which seems to be inference-expensive for practical setups."
            }
        },
        "id": "giUbtdnGDJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WRYhaSrThy",
        "replyto": "WRYhaSrThy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission211/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481909,
        "cdate": 1696707481909,
        "tmdate": 1701465391338,
        "mdate": 1701465391338,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates the co-occurrence bias in large language models, i.e. the tendency to assign higher probabilities to words that have higher co-occurrence statistics instead of the correct answer. The paper shows that this bias persists in models of different sizes and cannot be effectively removed using fine-tuning, but can be partially remedied with knowledge editing. While the methodology for studying and mitigating co-occurrence bias is novel, the insights that the paper provides are limited. Some of the claims the paper makes are not substantiated clearly enough, although during the discussion the authors provided some clarifications and additional results that will help to strengthen the paper."
            }
        },
        "id": "zpmZssqspE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WQamRhhbsf",
        "replyto": "WQamRhhbsf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476158,
        "cdate": 1696707476158,
        "tmdate": 1701465383574,
        "mdate": 1701465383574,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper aims to improve few-shot prompting of large language language models for translation. The authors introduce a novel selection method based on a regression model using multiple feature to find better examples for the prompt. Experimental results on different language pairs leveraging different language models show the effectiveness of this approach.\n\nThe reviewers highlight the clear and understandable presentation, the extensive set of experiments as well as the comprehensive analysis and ablations. This paper could be further improved by comparing against efficient fine-tuning methods and some minor writing and presentation adjustments."
            }
        },
        "id": "QiJ7FkMzzU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WQR3xpEJRJ",
        "replyto": "WQR3xpEJRJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2570/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545005,
        "cdate": 1696707545005,
        "tmdate": 1701465469437,
        "mdate": 1701465469437,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new method, Active Retrieval, which prompts an LLM to trigger iterative retrieval while generations. Their experimental results show effectiveness on Multi-hop QA, long-form QA, and aspect-based summarization datasets. While I agree with some of the limitations shared by the reviewers (e.g., limited novelty given Toolformer, applicability to other LLMs, evaluations on focused tasks), overall the idea is exciting and the experiments are well conducted. Therefore, I recommend for acceptance."
            }
        },
        "id": "NL1WhGCVpT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WLZX3et7VT",
        "replyto": "WLZX3et7VT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4469/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707590143,
        "cdate": 1696707590143,
        "tmdate": 1701465532357,
        "mdate": 1701465532357,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The methodology is fairly clear and appears to offer interesting results for the intended taks. The linguistic or cognitive significance are however not explicitly highlighted."
            }
        },
        "id": "dVGn6jSkJV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WLV8cm80DB",
        "replyto": "WLV8cm80DB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission892/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498592,
        "cdate": 1696707498592,
        "tmdate": 1701465413915,
        "mdate": 1701465413915,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors introduce a novel attention module that leverages polar coordinates instead of cartesian coordinates to capture the layout of the images in the documents. While the reviewers raised some concerns about the experimental setup, most of them seem to have been addressed in the rebuttal."
            }
        },
        "id": "lUfpuXkYnj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WLIFsPSq3t",
        "replyto": "WLIFsPSq3t",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5027/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605271,
        "cdate": 1696707605271,
        "tmdate": 1701465547796,
        "mdate": 1701465547796,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors provide a well-written paper that addresses an important issue in NLP, the cultural-awareness of commonsense knowledge models. The proposed GD-COMET, a culturally-aware generative COMET model, appears a valuable contribution to the field, and the (for a short paper) comprehensive evaluation of its benefits is a strength of the paper. There are a few areas where the paper should be further improved, such as providing a more detailed analysis of the effect of pretraining on CANDLE, and comparing the baseline models' performance in the intrinsic evaluation. Additionally, the paper could benefit from evaluating on more datasets if possible. The authors have provided detailed and helpful responses to the reviewers' comments which would allow them to address most concerns raised for the final version. Overall, this paper appears a valuable contribution to the discourse in the field."
            }
        },
        "id": "yff6bRoXFo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WEHwc4hSQR",
        "replyto": "WEHwc4hSQR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4269/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586404,
        "cdate": 1696707586404,
        "tmdate": 1701465526081,
        "mdate": 1701465526081,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "A new degeneration penalizing objective to improve knowledge internalization (or grounding). Some reviewers expressed their uncertainty about the results, however minor, and they think the paper can be benefitted by incorporating them in the final version."
            }
        },
        "id": "AP705XQJoR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WCxfj3PsWb",
        "replyto": "WCxfj3PsWb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1227/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507854,
        "cdate": 1696707507854,
        "tmdate": 1701465424347,
        "mdate": 1701465424347,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work introduces a novel technique for NER augmented with the visual modality, and introduces a novel learning algorithm module named Entity Spans Position Visual Regions (ESPVR), designed to address common challenges encountered in existing solutions. The authors claim that the ESPVR module enhances the efficacy of multimodal named entity recognition by identifying the most relevant visual regions corresponding to entities within the text. The reviewers generally agree on the novelty of the work and soundness of the presented method. There has been minor concern on missing experimental setup details, persuasiveness of some results, and sufficient diversity in experimental datasets."
            }
        },
        "id": "pnfIO901TS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WC9yjSosSA",
        "replyto": "WC9yjSosSA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1697/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707524107,
        "cdate": 1696707524107,
        "tmdate": 1701465438737,
        "mdate": 1701465438737,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a new metric called PFLAT that enhances prompt selection by integrating it with existing metrics.\nPros:\n It is more robust to LLMs’ parametric perturbations and provides significant improvements for large model sizes. \nThe paper also provides a comprehensive understanding of existing methods and their relationship with PFLAT. \nThe experiments demonstrate the effectiveness of the method for prompt selection\n\nCons:\nThe paper showed experiments for classification tasks only, no results for more challenging tasks such as generation tasks.\nIt is not clear what leads to higher prompt-selection scores,  lack of interpretability of the scores."
            }
        },
        "id": "GJOQe2kz2o",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WC1jbtEwRS",
        "replyto": "WC1jbtEwRS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2124/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535075,
        "cdate": 1696707535075,
        "tmdate": 1701465454936,
        "mdate": 1701465454936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates the use of knowledge graphs (KG) for improving lay summarization in the biomedical domain. Three strategies are proposed to integrate KG information into summarization models and experiments are conducted on the eLife dataset augmented with KG. Overall, reviewers are positive about this work. They found the paper well-written and acknowledge the novelty of applying KG integration to lay summarization. The authors  addressed all of the questions in the rebuttal, though there are remaining concerns about the size of the manual evaluation (only 5 articles)."
            }
        },
        "id": "guqGzsDHeA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "WAhhZcaA3R",
        "replyto": "WAhhZcaA3R",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5740/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617703,
        "cdate": 1696707617703,
        "tmdate": 1701465566126,
        "mdate": 1701465566126,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agreed that the paper tackles an under-resourced problem by means of thorough experiments and detailed dataset and error analyses. Criticism was centered around the generalizability of results, the small size of the dataset and a lack of novelty, which is made up for by the usefulness of the approach and the new Byzantine setting."
            }
        },
        "id": "razQwtZD5O",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "W76aMA1x9l",
        "replyto": "W76aMA1x9l",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1547/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517096,
        "cdate": 1696707517096,
        "tmdate": 1701465433993,
        "mdate": 1701465433993,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents K-DIAL which introduces extensions to the FFN in Transformer neural architecture, and apply a reinforcement learning for factual consistency method to align the responses with ground-truth knowledge. The presentation clarity in the paper leaves room for further improvement. While the authors did clarify  some of the important details missing from the original paper during rebuttal, these should be incorporated back into the manuscript."
            }
        },
        "id": "7bEqhFlnAv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "W6ijeWfHFU",
        "replyto": "W6ijeWfHFU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2971/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553198,
        "cdate": 1696707553198,
        "tmdate": 1701465482861,
        "mdate": 1701465482861,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper received mixed reviews with respect to soundness and excitement. \n\nWhile the MTDD corpus has been praised as an interesting resource for the community, the reviewers have raised concerns about the evaluation protocol and the validity of some claims."
            }
        },
        "id": "i1AcCXMfNW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "W4Vk1ufh7l",
        "replyto": "W4Vk1ufh7l",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission259/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483176,
        "cdate": 1696707483176,
        "tmdate": 1701465393041,
        "mdate": 1701465393041,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes to reduce the amount of data needed to achieve good language modeling perplexity and downstream task performance, especially that on semantic tasks, by removing sentences consisting entirely of high frequency tokens.\n\nReasons To Accept:\n- The authors propose a simple method to filter a large corpora down by on average around 4x by removing sentences containing frequent tokens.\n- Reducing the required training data, and thus computation, of language model pretraining is an important area of research.\n- Authors demonstrated the efficacy of their approach on a comprehensive list of tasks and probes.\n- The authors intend to publish the datasets and code.\n- Idea and writing was easy to follow.\n- This is an interesting idea that attempts to deal with power-law issues that may imply we as a field are brute-forcing in a non-intelligent way.\n\nReasons To Reject:\n- The explored design space of filtering algorithms was limited.\n- Need to evaluate the obtained results with full datasets used for obtaining 10M-freq. Not clear whether the baseline comparison is correct (whether were sampled same data)\n- The suggested approach is quite straight-forward and there is no specification in what setups it could be beneficial"
            }
        },
        "id": "6RSFaO1Bno",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "W4GlqAnXqv",
        "replyto": "W4GlqAnXqv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3482/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563708,
        "cdate": 1696707563708,
        "tmdate": 1701465498948,
        "mdate": 1701465498948,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The work considers discovery of new intents from limited labeled known intent data using a graph theoretic framework to capture structural similarities and perform inference.\n\n**Pros**: Most reviewers agree the approach is novel, the paper is well-written, and the experiments convincingly argue the utility of the method. Versatile benchmarks are considered with competitive results and ablation study is convincing. \n\n**Cons**: Reviewers concerns are generally addressed during the rebuttal period: authors provide results which ease concerns on the statistical significance of the results, potential real-world scenarios wherein the method may fail, and sensitivity to hyper parameters. Concerns from one reviewer do remain about the novelty of the work, representing the method as a combination of known approaches. \n\nGiven this disagreement on excitement, I looked into the work myself (considering the authors rebuttal and verifying in the paper's details). The backbone model, techniques, and use of GNN across *ACL venues is common. The graph-theoretic techniques are arguably known, as pointed out by reviewer pSNe, but the application of such techniques to self-training for new intent discovery is new. At the very least, the application specific results (which are comprehensive according to most reviewers) provide some new insight to the community."
            }
        },
        "id": "DD8v1Rf7vm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "W2ka7qsx1j",
        "replyto": "W2ka7qsx1j",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3645/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566923,
        "cdate": 1696707566923,
        "tmdate": 1701465505227,
        "mdate": 1701465505227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work presents develops an uncertainty-aware and parameter-efficient self-training framework called UPET, which involves using uncertainty estimates to select reliable samples from unlabeled data and introduces an easy-hard contrastive tuning method. The experiments are conducted against a series of baselines and provide statistically significant results that demonstrate gains in F1 scores on a number of natural language understanding test sets. In addition to the positive comments, the reviewers also present some concerns: Reviewer 1 notes that the authors focus solely on pretrained language models that do not use transformer-based decoders,  Reviewer 2 notes that the methodological innovations behind the proposed methods may not add significant new insights or techniques for NLP practitioners and researchers, and Reviewer 3 suggests that the paper could benefit from more discussion and testing in high-resource scenarios, and a more detailed analysis of the choice of parameters that are updated during the efficient training method."
            }
        },
        "id": "RMd9JYFbKg",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "W1w2eovejY",
        "replyto": "W1w2eovejY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1037/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503495,
        "cdate": 1696707503495,
        "tmdate": 1701465418636,
        "mdate": 1701465418636,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are in general agreement that this paper could be accepted in EMNLP-2023. The reviewers also raised some concerns about this submission. The authors have provided a rebuttal that appeared to alleviate some concerns. A suggestion was made to accept the paper as Main Conference. The reviewers' concerns are also suggested to be addressed in the final version."
            }
        },
        "id": "hZRJj4FuF2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "W0WeKrnfbX",
        "replyto": "W0WeKrnfbX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3777/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707574906,
        "cdate": 1696707574906,
        "tmdate": 1701465509526,
        "mdate": 1701465509526,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposed methods to decompose CoT with dialog format for smaller models and a step-wise PPO to improve the results further.\n\n**Pros**: The paper is very well written and proposes a very well-motivated method to solve the problem timely with open-source smaller models. (all reviewers)\n\n**Cons**: Some of the figures could be improved (**orRw**)\n\nOverall, the paper presentation is clear, with strong motivations. The proposed method is intuitive, with solid logic behind the design choices. The experiment results are strong compared to baselines using smaller models."
            }
        },
        "id": "SXuMJGVfHZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VyjNXY2wgi",
        "replyto": "VyjNXY2wgi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3572/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565474,
        "cdate": 1696707565474,
        "tmdate": 1701465502024,
        "mdate": 1701465502024,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new method which is TR-Rules, a rule-based model for extrapolating temporal knowledge graphs. This innovative approach effectively addresses temporal redundancy through a straightforward algorithm. However, the reviewers also state the technical contribution is incremental, more experimental studies are expected to support the claims."
            }
        },
        "id": "K0J3b9BYfS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VyIe1iVHZ4",
        "replyto": "VyIe1iVHZ4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5204/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608981,
        "cdate": 1696707608981,
        "tmdate": 1701465552935,
        "mdate": 1701465552935,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper conducts a thorough empirical evaluation of the transferability of visually grounded PCFGs. The experiments are very solid, covering several datasets with detailed analysis. The introduced idea is also intersting. The major drawback is that as an emperical study, the findings in this paper are plain, which makes the contributions of this paper incremental."
            }
        },
        "id": "37k9jiY12l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VqGX02f2lS",
        "replyto": "VqGX02f2lS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2165/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535945,
        "cdate": 1696707535945,
        "tmdate": 1701465456364,
        "mdate": 1701465456364,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are in broad consensus that this paper makes a substantial contribution to our understanding of the capabilities of RNNs. They agree that the insight is valuable, the investigation is thorough, and the paper is well written. While Reviewer uUpK points to some simplifying assumptions that may be considered a bit strong, but the authors openly address these limitations in the paper. The extensive discussion in the appendix might make this work better suited as a journal paper, but if the authors prefer to keep that discussion in the appendix and publish in a conference, I see no good reason to stop them.\n\nI hope the authors incorporate the presentation and literature review suggestions in their paper revisions."
            }
        },
        "id": "1MgrPfpUBF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Vp8WwRMWfv",
        "replyto": "Vp8WwRMWfv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4568/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593182,
        "cdate": 1696707593182,
        "tmdate": 1701465535189,
        "mdate": 1701465535189,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes a method for the analysis of style-shifting on social media. The problem is interesting and relevant, the evaluation is robust, and the results are promising. The reviewers point out some issues for which the authors have provided a reasonable explanation, and they are eager to provide more context or fix them. Two main issues are related to reproducibility, for which authors will provide the code and data upon acceptance. The second one is related to the proposed method and the fact that it relies on hand-crafted features; I see this as an opportunity for further work.\n\nPro\n* Interesting problem applied to a current topic and of interest to the community\n* Robust evaluations\n* Authors eager to address issues and release code and data\n* Authors estimate the addition would be on time\n\nCons\n* None"
            }
        },
        "id": "K8jY7RoMMU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VnMfQuDSgG",
        "replyto": "VnMfQuDSgG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3022/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554499,
        "cdate": 1696707554499,
        "tmdate": 1701465484686,
        "mdate": 1701465484686,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The results are sound and convincing but their contribution to linguistics and/or cognition are not immediately perceivable."
            }
        },
        "id": "IIcz6SGejr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VmoWVc04KY",
        "replyto": "VmoWVc04KY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4263/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586320,
        "cdate": 1696707586320,
        "tmdate": 1701465525889,
        "mdate": 1701465525889,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces MCLF, a two-stage, multi-grained contrastive learning framework aimed at improving the robustness of SLU systems. In its initial stage, the framework employs multi-grained contrastive learning alignment at both the sequence and token levels as a pre-training objective. Various data augmentation (DA) strategies are utilized to produce additional transcripts with a specific word error rate (WER). In the finetun-ing phase, a joint training objective is applied, and multiple classification losses are proposed. MCLF showcases its efficacy across four public benchmarks.\n\n**Soundness Scores**: (3, 4, 4)\nAll reviewers agree that the experiments are methodologically sound. The original manuscript includes comprehensive experiments on four public benchmarks as well as an ablation study. Although a detailed error analysis was absent in the initial submission, the authors have subsequently addressed this in their rebuttal. Overall, the primary claims of this work are well supported.\n\n**Excitement Scores**: (3, 4, 3)\nAll reviewers agree that the novelty and contributions as incremental, primarily because the techniques detailed have already been extensively employed in SLU tasks. In their rebuttal, the authors seem to acknowledge that their proposed method's performance might be comparable to, or only slightly better than, random chance data augmentation."
            }
        },
        "id": "hxnoiawPo4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VjSxQNhdKs",
        "replyto": "VjSxQNhdKs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1341/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510712,
        "cdate": 1696707510712,
        "tmdate": 1701465427840,
        "mdate": 1701465427840,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a persona-grounded dialog generation pipeline. The paper lacks extendability and generalizability beyond what can be hypothesized (author responses are less convincing). Hence, the contribution of the paper is a bit limited.|meta review"
            }
        },
        "id": "Ua11LFHxy6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VhL4lZXY1U",
        "replyto": "VhL4lZXY1U",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1657/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707522327,
        "cdate": 1696707522327,
        "tmdate": 1701465437613,
        "mdate": 1701465437613,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The work augments kNN-MT with source context enhancement, including a distance calibration module. Such a method has the benefit of being easily integrated into existing kN-MT setups, and the authors test this across various settings. The reviewers express some concerns, such as more competitive baselines (especially in non-artificial low-resource settings), and the authors present interesting results in the rebuttal. I'd recommend the authors also include some sense of the variation in BLEU, to better understand if improvements like +0.6 are significant."
            }
        },
        "id": "OQcnx4kbCk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VgLBPLvHuK",
        "replyto": "VgLBPLvHuK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3006/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554055,
        "cdate": 1696707554055,
        "tmdate": 1701465484083,
        "mdate": 1701465484083,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses an important yet previously overlooked issue of visual-linguistic ambiguity in multimodal understanding tasks. The authors have conducted thorough research, from task design, dataset construction, baseline method development, to empirical validation. This lays a solid foundation for subsequent research in the area. While there are some areas in the manuscript that could be improved, we look forward to an engaging camera-ready version that incorporates feedback from the reviews and discussion."
            }
        },
        "id": "6TKpuuT8eg",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VecgMidd4I",
        "replyto": "VecgMidd4I",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3233/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558710,
        "cdate": 1696707558710,
        "tmdate": 1701465491244,
        "mdate": 1701465491244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an auxiliary task for span-level prediction for NER to improve efficiency/robustness. They provide a thorough evaluation on multiple benchmarks, showing the effectiveness of their approach. Although in their original submission, their baselines were extremely low (17 vs 77SOTA); this has been resolved in the rebuttal. One of the reviewers still worries about the authors using more hyperparameter tuning for the proposed method compared to the baseline (private discussion); but from section 4.2 it seems like they were tuned the same way (and the rebuttal only adds the number of epochs). Therefore, the results seems sound (with a much better baseline, trends remain). In conclusion: a simple and motivated approach is introduces, which improves performance/robustness, this seems a valuable contribution.\n\nRemaining weaknesses as reported by the reviewers are: no real low-resource setting is tested, no separate evaluation on nested entities, no ablation of the sampling strategy that was also introduced (besides the auxiliary task), tuning of 2 hyperparameters on test data. Furthermore, I think a (qualitative) analysis of which cases have improved could be expected from a long paper."
            }
        },
        "id": "txFaJ3varU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VeBoHwiA7g",
        "replyto": "VeBoHwiA7g",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission478/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488754,
        "cdate": 1696707488754,
        "tmdate": 1701465400587,
        "mdate": 1701465400587,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces the task of background summarization from event timelines and proposes an experimental framework that includes a benchmark dataset and a dedicated evaluation metric. Overall, reviewers are all positive about this work after the authors's responses and discussion. They agree on the the contribution and novelty of the paper (interesting new task, introduced human-annotated dataset). Reviewers raised a concern about the potential bias/weakness of the introduced evaluation metric that was discussed in the rebuttal."
            }
        },
        "id": "b3yU1pQewH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VdvdRfwTtk",
        "replyto": "VdvdRfwTtk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4578/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593452,
        "cdate": 1696707593452,
        "tmdate": 1701465535574,
        "mdate": 1701465535574,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work presents a multi-task NLP benchmark and analysis platform for evaluating Swedish language models. \n\nThe three reviewers agree on the strong/excellent quality of the work with respect to soundness and excitement. The reviewers have performed a thorough work and the authors provided clear responses during rebuttal. The only relevant concern has to do with some remaining doubts on inter-annotator agreement. This information should be clearly stated in the paper, together with all other minor comments raising during the discussion period."
            }
        },
        "id": "2QEyIhreAb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VacjehPkIU",
        "replyto": "VacjehPkIU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1083/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504608,
        "cdate": 1696707504608,
        "tmdate": 1701465419954,
        "mdate": 1701465419954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new zero-shot benchmark, built upon the SCROLLS benchmark and called ZeroSCROLLS. 6 tasks are from the SCROLLS dataset (and adapted) and 4 are completely new. The objective of this new dataset is to evaluate both the zeroshot capability and the long-text understanding capabilities of SOTA LLMs.\nAnalysis and results are provided for 8 LLMs (3 open source models and 5 closed models) and 2 baselines for most of the tasks.\nData and code will be made available (anonymized link in the submission).\nThe motivation, difference with the SCROLLS benchmark and the need for such a benchmark could be better presented."
            }
        },
        "id": "TkVG51JUA3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VWFKRxsgt3",
        "replyto": "VWFKRxsgt3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission406/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486724,
        "cdate": 1696707486724,
        "tmdate": 1701465397767,
        "mdate": 1701465397767,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel approach to enhance the reasoning capabilities of large language models using a Reasoning via Planning (RAP) framework, incorporating an external world model and principled planning. The study exhibits a high degree of soundness and provides comprehensive, data-driven support for its claims. Some remaining concerns are over the inherent limitations of LLMs, the design of rewards, as well as a few clarifications needed and a more enriched limitation section needed. The authors’ rebuttal addresses the concerns effectively with additional studies and detailed explanations. Should the paper incorporate all the suggestions, it overall presents an exciting stride in language model research, providing a framework applicable across varying tasks with substantial performance improvements."
            }
        },
        "id": "RGGYGFFtvc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VTWWvYtF1R",
        "replyto": "VTWWvYtF1R",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission988/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707501472,
        "cdate": 1696707501472,
        "tmdate": 1701465416999,
        "mdate": 1701465416999,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents an interesting approach to improve cross-domain parsing, with a clever way to use LLMs, producing small gains over reasonable baselines. The reviews found the use of LLMs to produce target domain text for self-training to be an appealing idea, and suggested several ways in which the paper could be strengthened, including the inclusion of results over multiple runs with random seeds, and clarification of details such as whether the size of available target texts affects performance, whether different data amounts are used in vanilla vs LLM-enhanced self-training, etc. The responses provided by the authors seem satisfactory."
            }
        },
        "id": "2Zg9usXzp2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VSBBOEUcmD",
        "replyto": "VSBBOEUcmD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5475/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613705,
        "cdate": 1696707613705,
        "tmdate": 1701465560557,
        "mdate": 1701465560557,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a learning difficulty metric based on margin-aware list-wise ranking loss and curriculum learning for summarization. The incorporation of a dynamic margin to modify the original loss from the BRIO is a significant step. While the paper showcases promising results on the CNN/DM dataset with respect to both ROUGE metrics and BertScore, the validity of these findings could benefit from a broader array of tests, including other datasets and different pre-trained checkpoints. In addition, one of the reviewers is not convinced by the main conclusion, claiming optimal results in less than 20% of the instances, and believes this statement lacks sufficient rigor. \n\nQuality: \n- Limited to a single dataset (CNN/DM) for testing, raising questions about the method's generalizability.\n- More validation is needed to substantiate the claim regarding the optimal amount of data for training to achieve the highest performance.\n\nClarity:\n- While the paper has been acknowledged for its clear presentation and readability, there is a consensus among reviewers regarding the insufficient detail in explaining the core concepts, notably the difficulty metric and its utilization in curriculum learning. \n\nOriginality:\n- The introduction of a dynamic margin in the loss function is somewhat novel for summarization. \n- However, the curriculum learning appears more tailored for contrastive learning models, as indicated by one reviewer, requiring further justification of its applicability for abstractive text summarization.\n\nSignificance: \n- The technique proposed holds promise as evidenced by the experimental results, with potential implications for improving summarization performance while reducing the amount of data necessary for training. \n- Yet, the work's significance is somewhat constrained by the limited experimentation and the lack of validation across various datasets.\n\nThe paper explores an interesting research topic and integrates recent developments from other areas into text summarization effectively. However, there are significant issues with the paper: it needs clearer explanations of its methods and broader testing on various data sets to strengthen its arguments (one reviewer is concerned with over-claim). While the paper shows promising results, it requires more experiments to fully realize its potential. I am leaning towards accepting it with some reservations, but the overall soundness is not fully satisfactory (even the reviewer gave 4 for soundness indicated in the reasons for rejections many points about the claims are not backed up by enough experiments)."
            }
        },
        "id": "JoQGYRphnc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VQQeyiAqtv",
        "replyto": "VQQeyiAqtv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1443/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707513987,
        "cdate": 1696707513987,
        "tmdate": 1701465431215,
        "mdate": 1701465431215,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes UROMAN to adapt multilingual pretrained language models (PLMs) through transliterating UTF-8 characters to Latin scripts. All authors agree that the task of adapting PLMs to unseen scripts is important and well-motivated. The strength of UROMAN is its generalizability to any new language with unseen scripts. The authors show that UROMAN adapts PLMs to new languages with unseen scripts as good as or even better than language-specific transliteration, and the choice of languages in the experimental setup is diverse enough (with runs over 6 different seeds).\n\nThe paper lacks some baseline results, such as the finetuning of the entire model for language adaptation. Note that in this current climate, finetuning the entire small models like mBERT is an extremely reasonable practice; adapters are particularly useful for models with billions of parameters, so the authors' argument against not finetuning the whole model as baseline comparison due to computational cost doesn't really hold in my opinion. There is also work that shows that finetuning the whole model is better than language adapters at mBERT model sizes [1, 2].  While it's mentioned in the rebuttal, the authors should also include the results for out-of-the-box mBERT results as well in the paper and the training details such as number of tokens for adaptation per language. \n\n[1]: Ebrahimi, A., & Kann, K. (2021). How to Adapt Your Pretrained Multilingual Model to 1600 Languages. In ACL 2021.\n[2]: Yong, Zheng-Xin, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M. Saiful Bari et al. (2023) \"Bloom+1: Adding language support to bloom for zero-shot prompting.\" In ACL 2023."
            }
        },
        "id": "qm2TBtCGXB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VN298kRz91",
        "replyto": "VN298kRz91",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission941/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499904,
        "cdate": 1696707499904,
        "tmdate": 1701465415562,
        "mdate": 1701465415562,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary:\nThis paper addresses two challenges in Chinese Named Entity Recognition (CNER): catastrophic forgetting and non-entity type semantic transfer issues. To tackle these problems, it introduces a phased approach with corresponding strategies. This includes the introduction of an aggregated feature distillation loss to balance stability and plasticity, effectively mitigating catastrophic forgetting. Additionally, it proposes a confidence-based pseudo-labeling plan to explicitly extract old entity types within non-entity types, reducing label noise's impact and handling semantic transfer. The significance of component collaboration in solving CNER is validated through ablation experiments.\n\nReason To Accept:\n1.The motivation behind this work is clear: it addresses the semantic shift problem in Continual Named Entity Recognition (CNER), a challenge overlooked by previous methods.\n2.The paper's strengths lie in its innovative approach to tackling catastrophic forgetting in CNER.\n3.The paper is well-written, and its experimental results validate its effectiveness. It offers a comprehensive comparison of the proposed method with other baselines across various settings and datasets. Extensive experiments conducted on ten CNER settings involving three datasets showcase the approach's effectiveness and significant improvements over previous state-of-the-art methods. The proposed approach and experimental results have the potential to inspire further research and enhance performance in CNER and related tasks.\n\nReason To Reject:\n1.This paper adopts similar attention regularization and uncertainty-aware distillation such as [1][2] to alleviate the stability-plasticity dilemma and semantic shift proposed by [3]. ([1] Pelosin F, Jha S, Torsello A, et al. Towards exemplar-free continual learning in vision transformers: an account of attention, functional and weight regularization[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 3820-3829. [2] Kurmi, Vinod K., et al. \"Do not forget to attend to uncertainty while mitigating catastrophic forgetting.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2021. [3] Zheng, Junhao, et al. \"Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.)\n2.The experiments are inadequate. 1) The paper only conducts experiments on BERT, leaving the performance on larger models unexplored. 2) The proposed approach may not be universally applicable to all NLP tasks and datasets, with its effectiveness potentially contingent on specific data characteristics. 3) The paper lacks comparisons of the proposed approach with other state-of-the-art methods outside the CNER domain, limiting its generalizability to other NLP tasks.\n3.Certain details and analyses are lacking or unclear. 1) The performance under different hyper-parameter settings (e.g., \\lambda) is unclear. 2) A comprehensive analysis of the proposed approach's limitations, including scenarios where it might perform poorly or be less effective, is missing. 3) There's no detailed discussion regarding the computational complexity of the proposed approach, which could be a concern for large-scale datasets or real-time applications. The authors should provide a balanced and nuanced discussion of both the strengths and weaknesses of their approach."
            }
        },
        "id": "A8ME6cuFKb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VLrtaSXOWP",
        "replyto": "VLrtaSXOWP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission680/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493650,
        "cdate": 1696707493650,
        "tmdate": 1701465407506,
        "mdate": 1701465407506,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces DSI++, an approach using continual learning for Differentiable Search Indexes. The authors define a setting in which old and new queries are not available when updating a DSI with a new set of data, and they demonstrate that their proposed approach is able to mitigate the catastrophic forgetting happening on the old data when doing this operation. The reviewers all agree that the paper is well written and it discusses a topic worth of investigation to improve the capability of updating DSIs over time, which is a practical problem in real-world scenarios. The reviewers agree that the methodology is sound and that the provided experiments show its contribution."
            }
        },
        "id": "EMM8uelwJS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VKHWtusV6H",
        "replyto": "VKHWtusV6H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4253/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586139,
        "cdate": 1696707586139,
        "tmdate": 1701465525606,
        "mdate": 1701465525606,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a linguistical analysis of how NLP paper abstracts vary by nationality of the first author. The topic addressed, linguistic bias, is an important one, and overall, the reviewers are positive. However, the reviewers also raise a number of concerns related to presentation that undermine the impact of the authors' work, such as referring to \"native speakers\" rather than \"native English speakers\". Some of the assumptions made in the text (i.e. that Indian authors are not native English speakers) seem easily avoidable by discussing between-country differences rather than categorizing authors from specific countries as native or non-native English speakers. Bias towards differences between, for example, Indian English and American English are just as pertinent to the broader discussion as bias against non-native English speakers.\n\nIn summary, the reviewers seem to agree that the topic is important and the work is technically sound; one reviewer even recommends the paper for an award. Yet all reviewers also agree that some of the paper's language needs revision."
            }
        },
        "id": "X6xFi5yVmt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VJenYElbmY",
        "replyto": "VJenYElbmY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3298/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559983,
        "cdate": 1696707559983,
        "tmdate": 1701465493237,
        "mdate": 1701465493237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a study on editing the issues in editing common-sense knowledge in transformers. \n\nThe study is well-motivated. While there has been work in understanding and applying edits to factual knowledge, the issues involved in editing common-sense knowledge are largely unknown. The paper presents a high-quality and well-designed study by suitably adapting an existing method. The paper is well written. The main significance of the work is in the findings of the study, rather than in the modeling aspect. The other significant result from the paper are the datasets that have been created as part of the study. \nThe problem of editing complex inter-linked knowledge such as common-sense is an important one and understanding aspects of this problem is crucial. \n\nIn terms of weakness, the writing can be improved to more clearly present the key adaptations that have been made, so that the contributions relative to existing methods are clearer. This however is a minor issue that can be fixed easily."
            }
        },
        "id": "pmA2snl9R1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VIDDZO2f0A",
        "replyto": "VIDDZO2f0A",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4534/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592302,
        "cdate": 1696707592302,
        "tmdate": 1701465533970,
        "mdate": 1701465533970,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary**: The reviewers have reached a consensus that the paper presents a novel and effective approach to an important problem of CTG. The reviewers also have mentioned that the paper is well-written and easy to understand. Reviewer yX2p provided very limited views and didn't respond to the rebuttal and any of authors and AC's ask, so AC is weighing more on the other reviews. One reviewer asked about more details of human evaluations, and the authors provided comprehensive response to them. Also, some reviewers asked more baseline comparisons, and the authors provided multiple different comparison studies in the rebuttal and showed that the proposed method was more effective. Lastly, a reviewer asked about actual generation examples, and the authors provided them accordingly.\n\n**Reviewers' recommendations**: Except for the reviewer yX2p, the other reviewer rated the 'soundness' of the paper as 4:'strong', therefore there is a consensus that the paper is very soundly written. In terms of excitement, it's in between two 4:'strong' and two 3:'ambivalent'. However, by weighing more on other reviews than yX2p's, it's more on the exciting side."
            }
        },
        "id": "fBvpqdzF5U",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VGb2RhMFAI",
        "replyto": "VGb2RhMFAI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission968/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500465,
        "cdate": 1696707500465,
        "tmdate": 1701465416372,
        "mdate": 1701465416372,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an extension of the Word Embedding Association Test (WEAT) for Instruction-Following Language Models (IFLMs) like Alpaca and Flan-T5, referred to as P-AT. The study aims to detect biases across dimensions such as gender, race, and age in these models. In addition to the bias score, the paper introduces an \"Entropy\" metric to evaluate the quality of language modeling. The authors apply their bias assessment to various state-of-the-art models and discuss the relationship between bias and model sizes and the impact of prompt templates on bias. The reviewers' comments can be summarized as follows:\nReasons to Accept:\nValuable Tool: Reviewer 1 highlights the paper's contribution as a valuable tool for assessing bias within models that comprehend prompts. They appreciate the clear presentation, well-defined experiments, and the introduction of the Entropy metric.\nSolid Support: Reviewer 3 finds the paper's identification of a gap in bias measurement for instruction-tuned models and the proposal of a novel metric well-motivated. They also acknowledge the solid evaluation, even though some presentation improvements are suggested.\nReasons to Reject:\nLack of Significance Testing: Reviewer 1 points out a key weakness in P-AT compared to previous work like SEAT/WEAT, which is the absence of direct measurement of statistical significance in observed bias. While entropy provides some evidence that the model is not ignoring prompts, it does not substitute for a significance test like the p-value.\nChoice of WEAT: Reviewer 2 questions the choice of using WEAT as the basis for P-AT, suggesting that more recent and comprehensive bias detection datasets might be more appropriate. They also express concerns about the suitability of WEAT for probing bias in IFLMs.\nLack of Bias Definition: Reviewer 3 finds the paper lacking in providing concrete definitions for \"bias\" and \"harm.\" They suggest linking the score to social issues and consider the brittleness of prompts in bias detection.\n\nOverall Assessment:\n\nThe reviewers generally appreciate the paper's contribution to bias detection in IFLMs and the solid experimental support. However, concerns regarding the lack of significance testing, choice of WEAT, and the need for clearer definitions of bias and harm have been raised. Addressing these concerns could significantly strengthen the paper.\n\nBased on the reviews, the paper demonstrates strong soundness and provides valuable contributions to the field, although it receives mixed excitement scores. The concerns raised by the reviewers, particularly regarding significance testing, choice of metrics, and clarity in definitions, should be addressed in a revised version to enhance the paper's overall quality."
            }
        },
        "id": "BtEstoG4AZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VCyOXC8RfQ",
        "replyto": "VCyOXC8RfQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3405/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561983,
        "cdate": 1696707561983,
        "tmdate": 1701465496342,
        "mdate": 1701465496342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose a framework called KEEP, designed for predicting answers in open-ended commonsense reasoning tasks. KEEP extracts reasoning paths from an external knowledge base via concept extraction, entity linking, and knowledge graph expansion. The method incorporates the implicit knowledge in the PLM to identify relevant reasoning paths (pruning irrelevant ones). The relevant reasoning paths are used either in a zero-shot setting or as training instances to finetune the PLM.\n\nThe paper introduces a novel approach to address open-ended commonsense reasoning without restricting the answer scope. The paper reports competitive performance on two commonsense reasoning benchmarks (CSQA and QASC) when compared to both masked and generative language models.\n\nNonetheless, the empirical validation on the CSQA benchmark presents a major limitation. The proposed approach extracts reasoning paths from ConceptNet to solve CSQA, although the CSQA benchmark was created based on the ConceptNet knowledge base itself (Note that the CSQA leaderboard no longer accepts submissions that make use of ConceptNet). The reviewers have also suggested improvements to strengthen the paper's contribution, including clarifying the problem definition and conducting comparisons with stronger baselines."
            }
        },
        "id": "lOjZwolK4h",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "VC2vPPetCU",
        "replyto": "VC2vPPetCU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3832/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707577019,
        "cdate": 1696707577019,
        "tmdate": 1701465511197,
        "mdate": 1701465511197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper investigates how syntactic cues affect the transcription task of homophones in speech models. It adapts various context-mixing methods from the text domain to study this phenomenon. The work primarily focuses on homophony in the French language, utilizing both encoder-only and encoder-decoder models for analysis. \nOverall, the reviewers find the topic interesting and the paper well-written with clear technical aspects. The reviewers do not have major concerns but they concern about limited focus on a single phenomenon of homophones and that too using a single language (French)."
            }
        },
        "id": "YCAta8cnQl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "V9xsOja2oC",
        "replyto": "V9xsOja2oC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3809/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707575965,
        "cdate": 1696707575965,
        "tmdate": 1701465510265,
        "mdate": 1701465510265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers agree that the work presented in the paper is well-motivated and paper is generally well-written. Empirically, the proposed method achieves marginal improvements. Questions around missing experiments and result analysis were largely addressed during rebuttal."
            }
        },
        "id": "Puc64QeqCs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "V76kMIJI37",
        "replyto": "V76kMIJI37",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1359/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511042,
        "cdate": 1696707511042,
        "tmdate": 1701465428337,
        "mdate": 1701465428337,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new dataset for open-domain question answering where the questions contain counterfactual/hypothetical situations. The model must retrieve passages based on these counterfactual situations to correctly answer the question. The reviewers found that the proposed dataset targets an important ability for ODQA systems and demonstrates that existing state-of-the-art systems perform poorly on the task. They also found that the paper presents sufficient detail and provides interesting insights about the dataset and models."
            }
        },
        "id": "JIDyQt61Cs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "V49Jx2Lj04",
        "replyto": "V49Jx2Lj04",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission560/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490847,
        "cdate": 1696707490847,
        "tmdate": 1701465403500,
        "mdate": 1701465403500,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper address the problem of speaking style conversion and proposed a approach DISSC. \n\nPros:\n- comprehensive results for comparing with baseline methods\n- Showing evaluation criteria for speaking style conversion.\nCons:\n- Large scale speaker experiment would be more helpful\n- There is some content loss with de-duplication."
            }
        },
        "id": "p8cGxvJDLs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "V3O0NNaPNW",
        "replyto": "V3O0NNaPNW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1089/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504775,
        "cdate": 1696707504775,
        "tmdate": 1701465420149,
        "mdate": 1701465420149,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The main conclusions of the reviews and the post-rebuttal discussions:\n- 3/ 3 reviewers consider the paper sound (scores 3, 4, 3)\n- 3/ 3 reviewers find the paper exciting (scores 4, 4, 4)\n\nFront reading the rebuttal and seeing the scores above, I find that the reviewers consider strong points for soundness the following:\n- the paper provides a comprehensive survey on LLM alignment over changing information that does not require training from scratch\nOne concern is no quantitative comparisons of different approaches through experiments but only empirical analysis and summarization of existing work and it is addressed in the limitations section of the paper.\nConcerns from R3 about inclusion of more citations is clarified in the rebuttal.\n\nAll reviewers find the paper very exciting!"
            }
        },
        "id": "CqH9khoJ5B",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UyLaqZ6PHA",
        "replyto": "UyLaqZ6PHA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission957/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500202,
        "cdate": 1696707500202,
        "tmdate": 1701465416016,
        "mdate": 1701465416016,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a new method for attribute value extraction that addresses the limitations of existing methods by leveraging pre-training and query expansion. The experiments on two benchmark datasets show it achieves state-of-the-art performance on both datasets and outperforms existing methods on rare and unseen attributes.\n\nThis paper makes some marginal but valuable contributions that are explained through a long rebut process. However, such contributions are still just incremental to existing literature and they still leave the paper lacking in novelty."
            }
        },
        "id": "t16XLBvm2J",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UxdVVhWVq2",
        "replyto": "UxdVVhWVq2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2231/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537495,
        "cdate": 1696707537495,
        "tmdate": 1701465458429,
        "mdate": 1701465458429,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This is an interesting work that addresses for the first time ASR error correction in code-switched data (Chinese-English). The paper proposes data augmentation strategies for training ASR error correction models. The ASR error correction datasets are first created using pairs of outputs from an ASR model (XLSR-53) and ground truth transcripts, and automatically assigning error types. Then the paper proposes an approach to data augmentation via a set of text transformations.\n\nThe paper should include more detail on the dataset construction and the data augmentation approach. As the authors use a single ASR model to construct the datasets and to analyze the resulting errors, it is unclear how the choice of the model would affect to constructed dataset and the results on the error correction task. The paper can be strengthened by experiments on a different ASR model.\nThe related work section should  include research on the more general error correction task and error correction on monolingual ASR (including specific examples would help the reader). How can insights from these tasks benefit the ASR error correction? Can you use existing data augmentation techniques from the general error correction task for ASR error correction?"
            }
        },
        "id": "uWKvGH7UtA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Uuqv7iSNif",
        "replyto": "Uuqv7iSNif",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1391/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512015,
        "cdate": 1696707512015,
        "tmdate": 1701465429565,
        "mdate": 1701465429565,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents ADACUR to improve upon previously proposed method of annCUR  by improving the set of anchor items over multiple rounds of retrieval. The motivation of this paper is mostly acknowledged by the reviewers, however, there are major concerns on the novelty, comparison or discussion with recent literature, and the presentation (many parts are unclear and there are numerous grammars). Significant improvement over the quality of this paper is required."
            }
        },
        "id": "e9Lg6UDAat",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Upk6WrdJYM",
        "replyto": "Upk6WrdJYM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4607/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594172,
        "cdate": 1696707594172,
        "tmdate": 1701465536509,
        "mdate": 1701465536509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper suggests a multi-step prompt-based approach for handling open-domain questions, where the questions may contain false-presuppositions or be ambiguous. Specifically, the authors propose to first prompt an LLM for extracting the presuppositions in the question, then prompt it again to decide whether those presuppositions are correct and how to address them in the answer, and finally, given this feedback, generate an answer. The experimental results are sound and the problem is timely and important. Given the above, this paper can be a good fit for the conference."
            }
        },
        "id": "R02KBs1tfF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UmKaHvjkiu",
        "replyto": "UmKaHvjkiu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3633/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566627,
        "cdate": 1696707566627,
        "tmdate": 1701465504861,
        "mdate": 1701465504861,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "We thank the authors for their submission and engagement in the rebuttal. \n\nThe paper presents a two-step fine-tuning method for better zero-shot cross-lingual transfer. \nThe first step optimizes an isotropy-aware objective. The second step consists of code-switched data representation alignment based on cosine similarity. \n\nReasons to Accept: \n- The introduced method improves cross-lingual transfer on three tasks and for two models. \n- Analysis of the internal representation pre- and post-isotropy enhancement supports the method's effectiveness. \n\nReasons to Reject: \n- The boost in performance for some languages is minor and would benefit confidence interval reporting."
            }
        },
        "id": "IpOtbKB1kA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UlgNWOzMz2",
        "replyto": "UlgNWOzMz2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5438/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612951,
        "cdate": 1696707612951,
        "tmdate": 1701465559425,
        "mdate": 1701465559425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "There is clear consensus amongst the reviewers that this is a strong paper both in terms of Soundness and Excitement. The weaknesses and questions raised by the reviewers were sufficiently answered by the authors, as is clear in the review scores and discussion post-rebuttal. The remaining weaknesses are mainly suggestions that would make the work even better, rather than fill in some big gap in the current work. Overall the paper is well written, proposed methodology novel and transferable, backed by sufficient experimentation and targets an important problem in the field of NLP. It has _strong soundness_ and _strong excitement_.\n\nThe following is a summary of the strengths, weaknesses and scores across the four reviews:\n\n**Strengths:**\n\n- Well written and well structured (**cDFT**, **Jciq**, **5nce**)\n- Claims made in the paper are backed by sufficient experimental results (**cDFT**, **pPXn**, **Jciq**, **5nce**)\n- Paper works towards an important problem of generalization/memorization/hallucinations, which are very pertinent to the current state of NLP (**cDFT**, **pPXn**)\n- Provides actionable suggestions on how one can improve models by modifying their data (**cDFT**, **pPXn**, **5nce**)\n- Suggested methodology of memorization mapping is novel and brings a new perspective to knowledge within a network (**pPXn**, **cDFT**, **Jciq**)\n\n**Weaknesses:**\n\n- Application to low resource settings is not obvious (**pPXn**)\n- Reviewers agreed that the following are *not major weaknesses* after rebuttal:\n\t- Experimental design limits the findings to the specific models and settings (**cDFT**, **pPXn**, **5nce**)\n\t\t- Rebuttal: Authors agree, but also note (and reviewers allude to as well) that the experimentation targets a few languages and datasets, and goes into depth in each of these instead of covering a breadth of models. However, the depth of experiments already uncovers a new perspective sufficiently.\n\t- Breadth of the experimentation is limited in lieu of depth (**pPXn**, **5nce**)\n\t\t- Rebuttal: The depth of experiments already reveals a lot of insights, and the breadth was inherently limited by computational cost\n\n\n**Scores in decreasing order of confidence:**\n\n|      | Soundness | Excitement | Reproducibility | Confidence | Additional Notes             |\n|------|-----------|------------|-----------------|------------|------------------------------|\n| 5nce | 4         | 4          | 3               | 4          |                              |\n| Jciq | 4         | 4          | 4               | 4          | Did not acknowledge rebuttal |\n| cDFT | 4         | 4          | 3               | 3          |                              |\n| pPXn | 4         | 3          | 3               | 3          |                              |"
            }
        },
        "id": "VpmxQ07MD8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UlewKJFkUV",
        "replyto": "UlewKJFkUV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1636/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521401,
        "cdate": 1696707521401,
        "tmdate": 1701465436903,
        "mdate": 1701465436903,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers gave scores 4,3,2 (soundness) and 3,3,3 (excitement).\n\nThe following strengths and weaknesses were prominent:\n\nStrengths:\n\n- well-written (R1)\n- link to well-established idea (Raven's matrices) from psychology (R3)\n- comprehensive experiments (R1)\n- interesting qualitative analysis (R1, R3)\n- includes two languages (R3)\n\nWeaknesses\n\n- lack of quantitative evaluation (R1, R3) and small test set (R2)\n- lack of baselines, narrowly focusing on ChatGPT (R2)\n- perceived as incremental (R2)\n- unclear whether this kind of reasoning is accessible to humans (R3)\n- main text is not self-contained (R3)\n\nOn reconciling the disparate scores:\n\nR3 (Reviewer djfD) gave the lowest soundness score. One of the key weakness they mentioned, such as lack of quantitative evaluation, were also noted by the other reviewers, including R1 (6RnV), who nonetheless gave a high (4) soundness score. The other key weakness mentioned by R3, and not so much by the others is doubt about whether this kind of reasoning is actually accessible to humans. On the other hand, the link to Raven's matrices, a well-established idea in psychology was mentioned positively.\n\nTaken together, an overall lack of quantitative evaluation appears to be the main soundness weakness noted by all three reviewers, including those giving high (4), medium (3), and low (2) soundness scores."
            }
        },
        "id": "gtvjk9U0Sk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UjOPUHPoTM",
        "replyto": "UjOPUHPoTM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3404/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561957,
        "cdate": 1696707561957,
        "tmdate": 1701465496270,
        "mdate": 1701465496270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper deals with improving sentence embedding learning. The paper demonstrates that the key issue with knowledge distillation is high variance in the teacher model due to the nature of contrastive loss.  The paper  solves this problem by introducing additional regularization into contrastive training and knowledge distillation. It shows experimentally that the proposed techniques improve BERT performance in a standard sentence similarity benchmark. In rebuttal response the authors show that this method also works with RoBERTa, while experiments with XLNet show that this model does not perform well on this task and cannot be improved within the proposed paradigm.\nThe reviewers are generally agree in their view on the paper. All raised questions of novelty and meaningful comparison, especially comparison to other regularization techniques.\nI would add to concerns raised by the reviewers that the paper does not fit very well into the \"Semantics\" category, because it does not discuss anything specific to sentence semantics. I would put it under \"Machine learning\" category, since it discusses a general topic of knowledge distillation with contrastive loss. Thus, I believe the paper might be stronger if the proposed regularization techniques were tested on a broader range of knowledge distillation tasks beyond sentence similarity (and maybe even beyond the NLP realm)."
            }
        },
        "id": "MjBvgAQUyH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UixzK8evk5",
        "replyto": "UixzK8evk5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1012/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502501,
        "cdate": 1696707502501,
        "tmdate": 1701465417757,
        "mdate": 1701465417757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers were enthusiastic about the potential of a new NER resource for scholarly information extraction and appreciated the thoughtful approach to designing fine-grained entity classes for machine learning. Several reviewers questioned the schema of NER tags chosen by the authors, particularly raising questions of distinctions between classes such as ML model and ML architecture or Dataset and Datasource, which may also impact annotator agreement. Reviewers also questioned the efficacy of having a small, human-annotated corpus versus other works that have larger, crowdsourced approaches. Reviewers offered many suggestions for improving the writing by correcting terminology, typographical errors, and improving the clarity of ideas. Ultimately, the limitations of the dataset flagged by the reviewers may limit the impact and adoption of this resource by the community."
            }
        },
        "id": "9yyadxI90R",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UhuizFH1Hx",
        "replyto": "UhuizFH1Hx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2031/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532622,
        "cdate": 1696707532622,
        "tmdate": 1701465451136,
        "mdate": 1701465451136,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper analyses the influential factors that made humans prefer one generated output over another. As human preference is readily used in aligning LLMs nowadays, all reviewers thought the paper is timely and insightful - as this is one of the first studies that attempts to provide an explanation of human preference - and the use of BTL as a method for analyses is creative. Minor revisions that can be improve the paper include: (1) a better framing (as the current framing of human preference judgements is more general than the actual analysis [ZwsG]); (2) a discussion on the impact of demographics on preference (which is important as it influences the robustness of the results [u9Ct, ZwsG]); and (3) a discussion on the limitation of the current approach which requires the factors to be pre-specified (i.e. it is unable to 'discover' new factors that may not be apparent)."
            }
        },
        "id": "53i9GcTL6G",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ue9i6qgiCw",
        "replyto": "Ue9i6qgiCw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission124/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479799,
        "cdate": 1696707479799,
        "tmdate": 1701465388115,
        "mdate": 1701465388115,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates how multi-document summarization (MDS) models behave when the set of input documents is provided by an IR model. It formalizes this task as open-domain MDS, bootstraps it using existing datasets and summarization/retrieval models, and conducts extensive experiments. Reviewers agree that the paper makes meaningful contributions (recasting MDS in a more \"realistic scenario\", thorough experiments and analysis, opening potentially enabling further work in MDS) and I believe some of their concerns were addressed in the rebuttal period."
            }
        },
        "id": "YhazmXMUkH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ud2UQ9ZCep",
        "replyto": "Ud2UQ9ZCep",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4620/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594580,
        "cdate": 1696707594580,
        "tmdate": 1701465536917,
        "mdate": 1701465536917,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper outlines a systematic analsys aimed at studying automatic gender bias evaluation metrics in the context of image captioning tasks. The authors released a dataset to measure this issue and also introduce a novel evaluation to reduce metric bias while mantaining correlations with human judjements.\n\nI agree with the positive comments of Reviewer 3 and I believe the authors strongly replied to the critiques of the reviewers, especially Reviewer 1. I strongly encourage the authors to refine the paper, taking into account the feedback provided by all the reviewers, especially Reviewer 1."
            }
        },
        "id": "CwVlpbbppb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UaZe4SwQF2",
        "replyto": "UaZe4SwQF2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4762/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598004,
        "cdate": 1696707598004,
        "tmdate": 1701465540975,
        "mdate": 1701465540975,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers overall agreed on the novelty of the approach and its good performance on multiple datasets. However, they also raised concerns about the limited applicability of the method to complex claims, the lack of discussions on integrating the approach into large language models, the efficiency of the pipeline, the scarcity of proof annotations on the test set, and the experimental fairness in the comparison with baselines.  The authors have provided a fair amount of discussion to the questions raised by the reviewers."
            }
        },
        "id": "2vDN40zEaa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UXSqUOMwbE",
        "replyto": "UXSqUOMwbE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2257/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538029,
        "cdate": 1696707538029,
        "tmdate": 1701465459241,
        "mdate": 1701465459241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates the question of building a unified QA model trained on a collection of QA datasets, primarily investigating if it is better to train it using full fine-tuning or prompt-tuning. \n\nResults on T5-base and T5-large show that prompt-tuning with proper initialization for the additional param can outperform full fine-tuning. \n\nThe paper is set to explore a big question, but the execution is limited to a narrow setup that it is not clear if it generalizes. Specifically, \n- Previous work showed that prompt-tuning is not flexible enough to change the model behavior significantly. The positive results with prompt-tuning might just indicate that the full-finetuning baseline wasn't trained properly\n- Results in Table 5 need to be contextualized in previous results from the literature and in existing out-of-the-box methods\n- Giving the known limits of prompt-tuning, other methods like lora are more promising to explore.\n- Experiments are limited to T5-base and T5-large, not including auto-regressive models like opt, pythia and llama."
            }
        },
        "id": "UMdeeA00lj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UVoA0rALMC",
        "replyto": "UVoA0rALMC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5242/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609584,
        "cdate": 1696707609584,
        "tmdate": 1701465553850,
        "mdate": 1701465553850,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper delves into the concept of \"surface form competition\" (SFC) and its correlation with the classification accuracy of language models, especially in multiple-choice QA tasks. The research introduces a metric, PMA, to determine the upper boundary of SFC, thereby providing insights into its potential influence. Through a series of experiments on varied QA datasets, the study indicates that increased probability mass on answer choices, which quantifies SFC, doesn't always elevate model accuracy.\n\nThe main strengths of the paper are: \n1) The paper is well-written and easy to follow, formal definition of the task is given \n2) The proposed approach is interesting and provides practical insights on how to use vanilla LMs and instruction-tuned models in multiple-choice settings. \n3) The experiment is well design and comprehensive\n\nThe main concern from reviewers is  The results are less conclusive, some observed phenomena contradict the insights proposed in the paper and may not be applicable to other tasks."
            }
        },
        "id": "ZRhAXcpsWh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UTMwcLMhso",
        "replyto": "UTMwcLMhso",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4505/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707591828,
        "cdate": 1696707591828,
        "tmdate": 1701465533283,
        "mdate": 1701465533283,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a method to automatically create semantic parsing datasets by prompting large language models (LLMs) and using the generated data to train smaller task-specific models. The authors demonstrate the effectiveness of this approach in the context of tasks that map text to symbolic outputs such as code or logical forms. They generate questions via prompting the 175B Codex model and then prompt to generate responses to the generated questions. The authors show that the generated data is effective by finetuning T5-3B on it in addition to the original data, which leads to a performance comparable to Codex on most tasks. This approach is particularly effective in settings where only a few (~10) human-labeled samples are allowed. The authors perform detailed ablations to show which components of the prompts are essential for data quality, including analysis of generated data.\n\nPros:\nSimple and intuitive method to create an artificial dataset. The method is highly reusable across domains.\nTested over 4 diverse semantic parsing tasks, therefore the conclusions are more convincing. \nThe setting is relevant and the method is effective - it is simple to execute and leads to large improvements in low data regime.\nExperiments are comprehensive and convincing and performance comparable to strong baselines.\nThe generated data can be useful to others.  The relevance of this paper to the state of the field is strong. Using LLMs as data generators for these situations are an obvious way to reduce the need for human effort.\n\nCons:\nLacking some citations.\nDetails on the generation of the natural language questions are vague. \nThe method is not novel - but still the study is interesting so I don’t have any major weaknesses to point out.\nThe paper is lacking novelty . The main contribution is the exploration of the capabilities of an off-the-shelf LLM for data generation. \nThe greatest performance is gained from the inclusion of domain-specific knowledge and few-shot demonstrations to the prompt. \nMissing is a deeper look into how SymGen might be scaled to new domains."
            }
        },
        "id": "C1pePAAs9m",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UQpbq4v8Xi",
        "replyto": "UQpbq4v8Xi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2382/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541037,
        "cdate": 1696707541037,
        "tmdate": 1701465463457,
        "mdate": 1701465463457,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main contribution of this paper is to annotate the Callhome corpus with speaker and hearer beliefs and their common ground. The dataset is used for the tasks of event extraction, belief classification  and common ground update classification, and provides baseline performances for each of these tasks. \n\nThe reviewers appreciate the dataset as timely to allow for evaluation of a pragmatic task, but note that more comparison to related tasks in dialog systems could be discussed. The reviewers also note the small size of the dataset as the main drawback of this work (4 dialogs, 561 utterances); they also suggest modelling the task using a model that fits the task better, such as a rational speech act model. \n\nOverall, the task is timely and important, but the dataset is small and the modelling not fully convincing, so this seems like a piece of ongoing work that might achieve higher impact if extended."
            }
        },
        "id": "Ri0UPBISBj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UNvLur0th4",
        "replyto": "UNvLur0th4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2190/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536555,
        "cdate": 1696707536555,
        "tmdate": 1701465457234,
        "mdate": 1701465457234,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The study introduced a dataset aimed at potential human trafficking operations, comprising 87,595 text ads and 5,244 vendor labels. This addresses a significant social issue, and reviewers agree that the dataset will be a valuable asset to the research community.\nVarious important points were brought up by the reviewers that should be considered to enhance the paper. For instance, Reviewer buUC highlighted i) the time frame of the dataset, ii) multilingual models - providing some justification through experimental results would be more beneficial than mere speculation.\n\nReviewer iA59 pointed out ethical concerns, which the authors addressed in the rebuttal. Such discussion should be elaborated upon in the paper for better clarity."
            }
        },
        "id": "W5cKVBN3hp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UNFR2Y6Xx0",
        "replyto": "UNFR2Y6Xx0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3558/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565225,
        "cdate": 1696707565225,
        "tmdate": 1701465501669,
        "mdate": 1701465501669,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "*Summary of the work:* The work explores a mixture (ensemble) of \"experts\" to create more generalizable models. While gaining generalizability, the work also aims for a more interpretable aggregation of experts' results. \n\n*Summary of reviewer comments:* \nBased on my reading, most of the comments by the reviewers involve the expansion of the experiments. For example, azTT suggested comparison with multi-tasking-based baselines (which are not particularly interpretable). Others ask for more analysis on the contribution of the experts (G4bF) and analysis using more models (MmuJ). Implicit to these comments (including those by azTT) is the nature (pros/cons) of the suggested \"interpretability\" (which leads to \"selective QA\" which may be attainable with other forms of ensembling). After all, the interpretability here does not allow one to find out \"why\" a certain \"expert\" was chosen over the other ones. \n\nIn summary, the fact that the reviewers are asking for more experiments is not a killer issue. However, it is an indication that they expected a more rigorous set of analyses. \n\n\n*Overall recommendation:*\nOverall, it is a worthwhile study in a promising direction, though based on the concerns raised in the reviews, it is not clear a \"accept.\" This is evident in that all the reviewers have indicated an \"ambivalent\" level of excitement for this work. As such, I will recommend this work for the \"findings\" track."
            }
        },
        "id": "8JvkHkDsz7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UMywlqrW3n",
        "replyto": "UMywlqrW3n",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2088/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534176,
        "cdate": 1696707534176,
        "tmdate": 1701465453591,
        "mdate": 1701465453591,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper studies the LLMs’ linguistic knowledge in serving as an expert linguistic annotator. In particular, the paper analyzes the capabilities of the GPT-3, ChatGPT, and GPT-4 models in generating \nthe Abstract Meaning Representation (AMR) of sentences. \n\nPros: \n* All reviewers have found that the paper is very well-written and easy to follow.\n* The paper presents a thorough analysis of the quality of generated AMRs from  GPT-3, ChatGPT, and GPT-4, assessing the strengths and weaknesses of current LLMs. \n* The finding that “these models out-of-the-box can accurately identify some core aspects of semantic structure, but there remain key limitations in their ability to support fully accurate semantic analyses or parses” will be valuable to the practitioner interested in semantic parsing and analysis of texts.\n\nCons:\n* The reviewers have rightly raised concerns that the analysis has been done with a fairly small dataset (10 sentences per dataset), and this could lead to bias in the results. There is a lack of large-scale quantitative analysis in the paper. \n* * I find the authors’ rebuttal to be convincing that we should allow space for smaller-scale yet comprehensive fine-grained evaluations at ACL venues. As a short paper, the analysis presented in the paper accounts for a solid contribution and will be valuable to the community. \n* Other concerns, such as simplicity of prompts etc, were addressed adequately in the authors response."
            }
        },
        "id": "276kNnkevE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UIIi9hBNW8",
        "replyto": "UIIi9hBNW8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4663/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595496,
        "cdate": 1696707595496,
        "tmdate": 1701465538042,
        "mdate": 1701465538042,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents Zero-Shot Data Maps based on a single bi-encoder model with diverse label descriptions to assess the confidence and variability of each data point. The authors illustrate that Training Dynamics maps can be approximated using  Zero-Shot Data Maps while offering significantly faster computation. \n\n\nReviewers are divided on this submission. I think the review by Reviewer FBi8 has a low quality and needs to be discarded. While the remaining two reviews have acceptable qualities, the two reviews with positive opinions are more extensive and the mentioned strengths outweigh the weaknesses raised by the negative review. I have read the reviews and the follow-up discussions and I think it is reasonable to accept this paper into the Findings track."
            }
        },
        "id": "0RzC6F4qof",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UGd9eSwsvn",
        "replyto": "UGd9eSwsvn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1835/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528474,
        "cdate": 1696707528474,
        "tmdate": 1701465444031,
        "mdate": 1701465444031,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper identifies an important problem in iterative NAR model -- the anisotropic problem where the high similarity of different tokens can mislead the model to generate repetitive tokens at different steps. This finding is inspired by recent work on AR models (Su et al. 2022). The authors revisit the solution from Su et al (2022) -- contrastive learning, and find that it fails to learn the token representation well during training due to the natural difference between NAR and AR models. In response to this problem, the authors propose Look Neighbors to supplement contrastive learning during the training of CMLM. Experiments on widely-used NAR translation benchmarks demonstrate the effectiveness of the proposed approach.\n\nSome necessary information are missing in the submitted version, which are supplemented in the rebuttal. Please include the key information in the revised version:\n1. Supplement necessary information about the claims associated with \"failure to learn representations\" and \"anisotropic problem/loss\" to make the motivation and design principles more clear.\n2. Clarify the motivation of Look Neighbors method, and add an ablation study to prove that the performance improvement is indeed from the method.\n3. Experiments on more NAT models are appreciated to demonstrate the universality of the conclusions and the proposed approach."
            }
        },
        "id": "LZHpSyzX89",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UEzKGW4U39",
        "replyto": "UEzKGW4U39",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2690/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547627,
        "cdate": 1696707547627,
        "tmdate": 1701465473538,
        "mdate": 1701465473538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates how the number of parameters, the amount of training data, and the maximum sequence length affect the performance of transformer-based document-level machine translation models. It also explores why the performance of these models decreases when the maximum sequence length exceeds a certain threshold.\nThis paper studies the maximum length of text sequences that can be translated effectively by document-level machine translation (MT) models. This hyperparameter, or tuning knob, is important for training high-performance document-level MT systems in practice, but it has not been well-studied yet. The authors run many experiments that are sufficient to support all claims. However, all experiments are on a single language pair. Nevertheless, the author promised to include Chinese-English results in the camera ready version."
            }
        },
        "id": "CSauNSa170",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UEx5dZqXvr",
        "replyto": "UEx5dZqXvr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5261/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610022,
        "cdate": 1696707610022,
        "tmdate": 1701465554514,
        "mdate": 1701465554514,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper analyzes different types (intrinsic and extrinsic) of gender bias amplification correlating to different stages (pre-training and fine-tuning) of training vision and language models. The paper also proposes a method for addressing bias amplification through an additional pre-training step. Reviewers suggest including a limitation in the work that it focuses on a single dimension of social bias (gender), and doesn't consider how different dimensions of identity may intersect with one another in nuanced ways (e.g., race and gender). Other suggestions that I encourage the authors to make include clarifying some of the notation and expanding some of the discussion on related work."
            }
        },
        "id": "nfwVyHcUgE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "UECSdvL8U7",
        "replyto": "UECSdvL8U7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3154/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557100,
        "cdate": 1696707557100,
        "tmdate": 1701465488592,
        "mdate": 1701465488592,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a multitasking framework that combines dialogue discourse parsing and addressee recognition to address the issue of data sparsity in dialogue discourse parsing. It introduces a reinforcement learning agent to identify the most beneficial training examples from addressee recognition, and a task-aware structure transformer to optimize the performance for both tasks. The method has shown superior performance over state-of-the-art baselines on the Molweni and STAC datasets.\n\nPros:\nThe multitasking approach doesn't need extra annotated labels so it alleviates data sparsity in dialogue discourse parsing.\nThe incorporation of a reinforcement learning agent enhances dialogue discourse parsing.\nState-of-the-art performance on two datasets.\nThe techniques proposed can be potentially applied to other NLP tasks, benefiting various applications like chatbots and personal assistants.\nThe paper is described as being well-written, organized, and straightforward.\n\nCons:\nThe significance of the improvements for downstream tasks, especially those without data sparsity challenges, is not well-demonstrated."
            }
        },
        "id": "5CoKLll7lj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "U8PL5FzvrV",
        "replyto": "U8PL5FzvrV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1461/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707514970,
        "cdate": 1696707514970,
        "tmdate": 1701465431669,
        "mdate": 1701465431669,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a method to improve the consistency of pretrained LMs predictions by leveraging structured semantic information extracted from a dictionary. The augmented model is able to outperform baseline ones on the GLUE and BECEL datasets. Though the method is not particularly innovative, there is consensus about the methodological soundness of the paper, which makes it a valuable contribution. A possible limitation is the fact that the method is only applied to RoBERTa, lacking a larger experimentation with other LMs. However, this does not negatively affect the general strength of the paper."
            }
        },
        "id": "OQBbPQMPP3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "U7mWHBoTfb",
        "replyto": "U7mWHBoTfb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4658/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595423,
        "cdate": 1696707595423,
        "tmdate": 1701465537954,
        "mdate": 1701465537954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this paper,  the authors present a framework named DALE for generating synthetic data augmentations specifically designed for low-resource legal NLP text tasks settings. The model has designed an encoder-decoder language model pre-trained on the large-scale unlabeled legal corpus. The main contributions of this paper are proposing denoising objectives based on the selective masking technique, which masks co-occurring and highly correlated text spans instead of random words. The experiments show the state-of-the-art performance over strong baselines on 6 legal NLP tasks and 4 low-resource settings. However, the modeling process, experiments' settings and results are confusing the reviewers such as the fine-tuning step and multiple experiments, variance, and statistical significance."
            }
        },
        "id": "1acpDlQ7JG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "U78nBY8hRi",
        "replyto": "U78nBY8hRi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission804/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496347,
        "cdate": 1696707496347,
        "tmdate": 1701465411065,
        "mdate": 1701465411065,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Thanks reviewers so much your efforts in providing comprehensive reviews and comments to improve the paper.\n\nThanks authors for providing actionable rebuttals and facilitating discussions.\n\nSummary: The authors conducted research on Federated Distillation (FD) in large-scale pre-trained language models, both homogeneous and heterogeneous. They introduced Federated Interactive Distillation (FedID) to address confirmation bias issues in FD. FedID is a Federated Learning variant for Pre-trained Language Models (PLMs) and was tested extensively on GLUE tasks in various model settings. Their approach includes a new training algorithm that uses a public proxy dataset to collect client knowledge and adjusts the gradient of the small labeled dataset to combat confirmation bias. Experimental results demonstrated the effectiveness and robustness of their proposed method, which outperformed several baseline approaches in different GLUE tasks.\n\nAverage Soundness: (3+4+5)/3 = 4\nAverage Excitement: (3+4+4)/3 = 3.7\nReproducibility: (3+4+4)/3 = 3.7\n\nSummary of Pros:\n+ Solve the homogeneous and heterogeneous in federated distillation through some labeled data of the server\n+ Comprehensive experiments providing interesting insights around public dataset distribution between clients and communication aspects\n+ Relevant for EMNLP.\n+ Useful privacy-preserving training method with good performance\n+ Reasonable and effective algorithm design\n\nSummary of Cons:\n+ Lack theories\n+ Lack of reproducible content\n+ Contribution is incremental\n+ Lack of discussion on resilience\n+ Need more comparisons such as case-level analysis, alternative designs, real-world use case analysis\n\nThe authors have clarified and promised the changes to tackle the cons. The changes are feasible and can be timely ready for the final version."
            }
        },
        "id": "tj5brf2T6f",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "U6SEUS76IE",
        "replyto": "U6SEUS76IE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2481/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543058,
        "cdate": 1696707543058,
        "tmdate": 1701465466427,
        "mdate": 1701465466427,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Originality**\n\nThe presented paper is a very well written literature review of automatic pronunciation assessment. As such it is not original in and of itself.\n\n**Significance**\n\nThe paper is a nice introduction to automatic pronunciation assessment for someone new to the field with some descriptive figures and tables thoroughly covering past approaches and describing current and future challenges. This is a useful resource though not something that has much immediate impact on the field.\n\n**Clarity**\n\nThis paper is extremely thoughtful and well written.  \n\n**Pros:**\n   - A very valuable introduction for those trying to enter the field of automated pronunciation assessment\n   - Details current and past approaches as well as future challenges\n   - The paper is incredibly well written.\n\n**Cons:**\n   - There are no new methods, data, etc., in the paper\n   - Limited excitement"
            }
        },
        "id": "VpD9DhJEHO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "U1rj4p5aKa",
        "replyto": "U1rj4p5aKa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4085/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582830,
        "cdate": 1696707582830,
        "tmdate": 1701465519733,
        "mdate": 1701465519733,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary: This paper presents a large-scale reinforcement learning framework that learns from human feedback. \n\nPros:\n- Large scale RLHF framework that extends to 70 billion parameters\n- Open source implementations \n- Effective demonstration of the usefulness of the proposed framework in experiments\n\nCons:\n- Absence of discussions on ethics and limitations. I would suggest that the authors kindly add it to the manuscript."
            }
        },
        "id": "zY7hvkT76g",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TxEV8D0z0r",
        "replyto": "TxEV8D0z0r",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2125/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535095,
        "cdate": 1696707535095,
        "tmdate": 1701465454958,
        "mdate": 1701465454958,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers agree that the work is moderately sound and moderately exciting. Reviewers like the efficiency and strong experimental results of the novel way to use attention during generation. While some flaws are highlighted there is consensus that the overall method and evaluation is sound. I recommend acceptance to the main conference or findings."
            }
        },
        "id": "fhxDUiusBB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TvTwz12BZN",
        "replyto": "TvTwz12BZN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4373/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588469,
        "cdate": 1696707588469,
        "tmdate": 1701465529701,
        "mdate": 1701465529701,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The focus of this paper is to investigate whether LLMs are capable of understanding negation. The authors presents a dataset containing utterances with various forms of negation. It is found that negation is still a challenge for LLMs.\n\nThe reviewers found that this paper was an excellent linguistic study on the topic of negation and its handling by LLMs and that the provided resource would be very useful for future research. It also fits the EMNLP theme track particularly well. The main criticism identified by the reviewers lies in the somewhat small part of the corpus that was evaluated by human assessors.\n\nFor future work, it would also be interesting to see versions of this benchmark in different languages, especially languages that use different negation patterns (e.g. negative concord)."
            }
        },
        "id": "IOyiQCMYFH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TubO0kgAeL",
        "replyto": "TubO0kgAeL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3665/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567303,
        "cdate": 1696707567303,
        "tmdate": 1701465506015,
        "mdate": 1701465506015,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a multi-task machine translation model called MT2 and uses in-context learning (ICL) paradigm for model training. It has a clear structure, extensive experiments and analyses.\nAuthors demonstrate the effectiveness of large language models (LLMs) for various machine translation tasks via in-context learning with convincing results. Some minor aspects could be improved for the camera-ready: references (see reviewers comments), use more diverse evaluation metrics of translation quality (not only BLEU), inclusion of detailed results for the second training strategy, minor improvements in grammar and style."
            }
        },
        "id": "j4dzciZHRC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TtQfZwf5s5",
        "replyto": "TtQfZwf5s5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3407/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562030,
        "cdate": 1696707562030,
        "tmdate": 1701465496446,
        "mdate": 1701465496446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a substantial contribution in the form of a corrected and enhanced version of the CoNLL-03 corpus, which helps improving reliability and accuracy of Named Entity Recognition (NER) evaluation. The main contribution of this work, apart from the valuable new dataset, is the interestingly novel methodology employed for the automatic correction and cleaning of the corpus. Although it’s transferability to other languages still has to be proven experimentally, it appears promising.  \n\n**Pros:** \n\n- The paper is well written and clear \n\n- The methodology is convincing and sound \n\n- Produced a new corrected version of the well-known CoNLL-03 dataset, a very useful resource for the community  \n\n- gives detailed thorough statistics of the dataset, to assess its quality \n\n- creative / innovative applications of known techniques to the task of annotation correction and cleaning \n\n- the methodology seems portable to other languages\n\n\nThe work has no important weakness. It is interesting, mature, and the dataset it presents is likely to have a long-term impact on the field."
            }
        },
        "id": "XsqHVtYamd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TqIDmoIzLT",
        "replyto": "TqIDmoIzLT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1583/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518469,
        "cdate": 1696707518469,
        "tmdate": 1701465435313,
        "mdate": 1701465435313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a pretraining model PUNR for news recommendation. The proposed PUNR is pretrained by two objectives of BERT-like masked behavior modeling and GPT-like autoregressive behavior generation. \n\n\nPros:\nThe proposed paper is backed by thorough experiments, demonstrating its effectiveness. The straightforward and understandable architecture of the proposed method simplifies its comprehension. Furthermore, the pretraining methods are considered reasonable and well-reasoned, aligning with established practices in language modeling.\n\nCons:\nDespite these strengths, there are some concerns raised by the reviewers. The novelty of this paper is relatively limited.  Furthermore, the gradual improvement observed in the experimental results has been identified as a drawback. \nConcerns about data overlap between pretraining and finetuning data are significant, with suggestions that this could lead to data contamination and impact the generalization of the model. There are also recommendations to compare the proposed method with recent news recommendation baselines, address equation inconsistencies, evaluate the necessity of an additional decoder, rectify minor writing flaws, and consider using a more diverse set of datasets in experiments to enhance the paper's overall robustness."
            }
        },
        "id": "2YJFtDODSL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Tpd5RuSzpq",
        "replyto": "Tpd5RuSzpq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission67/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478307,
        "cdate": 1696707478307,
        "tmdate": 1701465386066,
        "mdate": 1701465386066,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors proposed an approach for domain adaptation of the Large Language models.\nThe 2 out of 3 reviewers selected highly positive score (one selected \"borderline\")"
            }
        },
        "id": "BYtE0gTPqD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ToMdTqVIb5",
        "replyto": "ToMdTqVIb5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4491/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707591130,
        "cdate": 1696707591130,
        "tmdate": 1701465533042,
        "mdate": 1701465533042,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In general, all reviewers agreed that this is sound research.  There was mixed enthusiasm for including the work at EMNLP, since the primary objective of this research was on domain-specific task needs rather than on developing innovative NLP techniques.\n\n**Summary of Reviewer Feedback and Discussion:**\n- **Reviewer x51Z** thought that the work was sound but the research was straightforward and uncomplicated with only trivial performance improvements.  They thought the study may be more suited to non-CS venues, since the work didn't include NLP-oriented technical challenges.  In their rebuttal, the authors clarified that the technical challenges lied in extending multimodal models to the task, which differs greatly from existing multimodal task formulations (e.g., VQA).\n- **Reviewer LmF8** liked that the paper introduced a new task, and thought that the authors' solution for this task was nicely motivated, reasonably structured, and convincingly evaluated.  They wished that more information was provided regarding subjects in the case study and the distribution of evaluation scores given by human subjects, and the authors provided this information in the rebuttal and promised to include it in the revised manuscript.\n- **Reviewer J6oG** liked the results provided in the evaluation, but they felt that the task and dataset were trivial.  They felt that the contributions of the proposed method would be more convincingly demonstrated if it were evaluated on multiple datasets or a more complicated task.  They were also concerned that the paper's minimal technical contributions would make the paper uninteresting to NLP researchers, and wished that the paper clarified the real-world impact and potential benefits of the proposed solution.  In their rebuttal, the authors explained that the technial challenges lied in extending multimodal models for this task, included additional study information, and promised to include these details in the revised manuscript.  When responding to the rebuttal, Reviewer J6oG reiterated that the proposed method demonstrated performance improvements over the baseline, but that these results did not convincingly demonstrate task complexity or relevance to the NLP community.  In a follow-up response, the authors noted that their focus was not on innovating NLP technology, but rather on designing and improving NLP techniques for application to specific target issues in the domain of psychological assessment."
            }
        },
        "id": "OsEIn6c1wR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ToGkF2nCNG",
        "replyto": "ToGkF2nCNG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1072/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504401,
        "cdate": 1696707504401,
        "tmdate": 1701465419610,
        "mdate": 1701465419610,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a novel approach to topic modeling using decoder-only Transformer-Based Language Models (TLMs). The proposed approach is clear and easy to follow. All reviewers agree on the novelty of the paper's approach which executes the idea well and evaluates it thoroughly, including analysis of quantitative evaluation and extending to LLaMA. Experiments using widely used language models such as GPT2 and Llama are impressive and remarkably comprehensive. From the experimental results, we can find that leveraging pre-trained TLMs could highly benefit the area of topic models."
            }
        },
        "id": "zsts0XMK5c",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Tnx0922coo",
        "replyto": "Tnx0922coo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1303/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509988,
        "cdate": 1696707509988,
        "tmdate": 1701465426928,
        "mdate": 1701465426928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an approach to conversational semantic parsing over large knowledge graphs. The paper has unfortunately received rather superficial reviewing which could not be improved during the discussion period. In what follows, I summarize my own view (AC), departing from reviewers' evaluations.\n\nThe paper addresses an important task of conversational QA. This is a very relevant contribution -- and the findings are evaluated on a very recent (and already established) benchmark. The paper follows a principled approach, by introducing dynamic graph modeling (\"dynamic context graph\"). \n\nThe paper presents a thorough and detailed evaluation. It shows a detailed comparison against SOTA and a very insightful analysis of the system's performance, going beyond simple numbers. The results suggest an improvement for both simple and complex questions.\n\nMinor suggestion for the authors: Figure 1 is very instructive and does a good job at making your task clear -- however, the queries are impossible to understand and rather confusing. Would it be possible to either incorporate node names into the figure directly or provide some explanation on them a bit earlier than line 067?"
            }
        },
        "id": "B3eWHcVWsb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TnpFFjHCcw",
        "replyto": "TnpFFjHCcw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2976/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553279,
        "cdate": 1696707553279,
        "tmdate": 1701465483008,
        "mdate": 1701465483008,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary (adapted from Reviewer 7BGY): This paper proposes an approach to build a multilingual colexification graph from the Parallel Bible Corpus (PBC). The resulting English-concept-centered colexification graph ColexNet is extended into the multilingual ColexNet+, which contains concept-realizing n-grams as intermediate nodes between abstract concepts. ColexNet is shown to robustly identify common colexification patterns to the gold standard CLICS. ColexNet+ is used to train multilingual embeddings which outperform multilingual baselines on 3 diverse downstream tasks of roundtrip translation, verse retrieval and verse classification.\n\nOverall, the reviews were positive, with the “Reasons to Reject” mostly focusing on improvements in clarity in the paper and explanation of the results. Reviewer 7BGY lays out concerns about whether the colexification graph and embeddings will be of use to other tasks, and whether the Bible data is sufficient for evaluation.\n\nThese concerns are valid, but as the authors’ response makes clear regarding these comments and others, there is a tradeoff between trying to evaluate in as many languages as possible in terms of limitations of both data and models available (i.e. pretrained Transformers do not cover the number of languages needed). The authors have had to make compromises, but the paper seems sound within the scope it attempts to address.\n\nThe response period was extremely productive, and the authors provided clear responses to the reviews, all of which the reviewers acknowledged.\n\nAs came up in the discussion with Reviewer 7BGY, this paper might benefit from further discussion of its limitations with regards to the use of a Bible corpus as the primary dataset. While this is mentioned in the limitations, it receives little attention and it may be worthwhile to discuss it more explicitly and earlier in the paper."
            }
        },
        "id": "eUQyJNc5Rc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Tn5hALAaA4",
        "replyto": "Tn5hALAaA4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1153/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506080,
        "cdate": 1696707506080,
        "tmdate": 1701465421860,
        "mdate": 1701465421860,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper is sound and addresses problems at the core of this track."
            }
        },
        "id": "D6GkyKxg20",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TkJkSkmhUy",
        "replyto": "TkJkSkmhUy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1357/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510984,
        "cdate": 1696707510984,
        "tmdate": 1701465428265,
        "mdate": 1701465428265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a probing study to explore the ability of LLMs to understand various types of generalized quantifiers. Reviewers and I agree about the methodological soundness of the work, in which state-of-the-art models, such as T5, FlanT5, DeBERTa, ChatGPT, and LLaMA, are tested. Moreover, the paper addresses a relatively little-studied topic in nowadays NLP and represents an interesting example of using formal semantics to explore the inferential competences of LLM."
            }
        },
        "id": "hCPstVhfK8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Tk4tvmdKVP",
        "replyto": "Tk4tvmdKVP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission564/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490881,
        "cdate": 1696707490881,
        "tmdate": 1701465403565,
        "mdate": 1701465403565,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a new framework for knowledge graph to text generation (KG-to-text) that combines graph-structures modules with pretrained language models (PLMs). A planner is also proposed to fuse the input knowledge graph to the PLMs. Although concerns are raised about ablation study for contribution of different components and novelty. Overall, the paper is well written with strong empirical results. Incorporating graph-aware encoders with PMLs is an exciting direction and it can be beneficial to a broader community."
            }
        },
        "id": "oDCsHy7Ljb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TioAqBt8lz",
        "replyto": "TioAqBt8lz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission911/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499195,
        "cdate": 1696707499195,
        "tmdate": 1701465414565,
        "mdate": 1701465414565,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a case based reasoning approach to reading comprehension question answering. They use BERT embeddings to identify similar questions in the training data to a given test example, and use these to help predict the final answer. The reviewers found the approach presented to be novel for these sorts of QA tasks, and that the paper was well-written. The results were also strong, and during rebuttal the authors provided additional results with DeBERTa that strengthened their contribution.\n\nUnfortunately, the authors missed the following paper, which seems to be a generalization of their work:\n\nRoshni G. Iyer, Thuy Vu, Alessandro Moschitti, and Yizhou Sun. Question-Answer Sentence Graph for Joint Modeling Answer Selection. The 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2023)"
            }
        },
        "id": "nuR0gUHHhz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Tha4jW8er9",
        "replyto": "Tha4jW8er9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4946/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602115,
        "cdate": 1696707602115,
        "tmdate": 1701465545445,
        "mdate": 1701465545445,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces the SOUL (Sentiment and Opinion Understanding of Language) task, which includes review comprehension and justification generation. It provides a dataset with 15,028 annotated statements and evaluates various models, including SLMs like Roberta, T5, and Flan-T5, as well as LLMs like Flan-T5XXL and ChatGPT, in zero-shot settings. The results highlight the challenge of justifying sentiment labels, with GPT-4 performing similarly to human evaluation in assessing generated justifications.\n\nReasons to accept:\n\nNovel Task: Introduces the SOUL task that addresses shortcomings in sentiment analysis, focusing on detecting indirect sentiment expressions like irony.\nEnhances AI model reliability by requiring justification for sentiment decisions.\nEffective Evaluation: Uses GPT prompting for model justification evaluation, demonstrating its validity compared to human evaluation.\nThe paper is well-written and organised.\n\nReasons to reject:\n\nNovel Method: The paper lacks a novel method to address the SOUL task. -Limited Model Evaluation: The experiments primarily focus on small language models (Roberta and Flan-T5), while state-of-the-art Machine Reading Comprehension (MRC) methods are not evaluated. -Missing Error Analysis: The paper lacks error analysis to identify gaps between existing models and human performance. -Confusing Results: The results for the Justification Generation task are inconsistent.\nAmbiguity in Annotation Criteria and lack of Inter Annotator Agreement for data annotation. -The paper reports an \"overall accuracy\" metric without explaining how it's measured, and relies on non-human metrics like BLEU, ROUGE, and BERTScore for text generation evaluation, which may not be suitable for this task."
            }
        },
        "id": "sEPzlZ8tYx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TemPqRDMJ8",
        "replyto": "TemPqRDMJ8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5771/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618265,
        "cdate": 1696707618265,
        "tmdate": 1701465566848,
        "mdate": 1701465566848,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This position paper discusses issues surrounding regulation of LLMs, including risks, ethical considerations, and scientific integrity. This topic is important and timely and deserves further discussion in our community. Concerns about the paper mainly revolve around the applicability of the paper, due to the hypothetical nature of this paper and the lack of concrete recommendations."
            }
        },
        "id": "scI9B2yOVI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TdrI4F7wS8",
        "replyto": "TdrI4F7wS8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3886/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578462,
        "cdate": 1696707578462,
        "tmdate": 1701465512848,
        "mdate": 1701465512848,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a pretraining framework for learning representations of nodes in text-attributed graphs, applicable to GNNs and other models of structured data. The authors demonstrate that their proposed approach is competitive with or marginally better than previous approaches, using node classification in scientific graphs as the test case. \n\nThe primary flaw identified by reviewers was the decision by the authors to evaluate on only one of the standard benchmarks for the task. This weakened the empirical arguments in the paper. Since the initial submission, the authors have in their rebuttal provided another set of results, significantly strengthening the paper.\n\nAs the framework is quite general and appears to yield performance improvements (albeit only to a small degree), the method presented here may find wider applicability."
            }
        },
        "id": "SNZmnF3jbz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Td9LjgO91J",
        "replyto": "Td9LjgO91J",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2992/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553646,
        "cdate": 1696707553646,
        "tmdate": 1701465483574,
        "mdate": 1701465483574,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Most reviewers concur that the manuscript proposes a simple but effective zero-shot method that helps privacy preservation via leveraging the strong capabilities of LLMs. The introduced DP-prompt has demonstrated utility and can be applied without any fine-tuning. To improve the quality of this draft, however, consider the following suggestions from our reviewers.\n\n1. It would be beneficial to test stronger baselines that exploit various sizes or types of LLMs. This would offer a clearer understanding of sensitivity-related concerns and pinpoint the sources of performance improvement. It is highly encouraged to include at least one additional state-of-the-art open LLM.\n\n2. The current draft does not sufficiently clarify if the DP-prompt ensures the preservation of the original sentential meaning. Incorporating additional experiments, such as embedding-level similarity inspection or human evaluations, would provide valuable insights for both modelers and practitioners.\n\n3. The authors are encouraged to contemplate the philosophical question: the feasibility of utilizing LLMs, which might themselves have privacy concerns, to address privacy issues. Addressing such inherent paradox in the conclusion section would be desirable."
            }
        },
        "id": "RC45bv18kP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TZW4nzgtQ8",
        "replyto": "TZW4nzgtQ8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission578/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491145,
        "cdate": 1696707491145,
        "tmdate": 1701465403943,
        "mdate": 1701465403943,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This meta-review assesses the quality of the paper based on three reviews provided by independent reviewers. The paper introduces the M4 benchmark for evaluating language models in the medical domain. It covers a wide range of tasks and domains, including sentence-level and document-level classification, summarization, and more. Expert annotations enhance the quality of the dataset, and it offers insights into the performance of various language models. Below are the reasons to accept and reject, as well as the average Soundness and Excitement scores.\n\nReasons to Accept:\n1. **Broad Task Coverage:** The M4 dataset encompasses various tasks and domains, making it relevant and applicable to multiple medical fields.\n2. **Expert Annotations:** The inclusion of expert annotations enhances the credibility and utility of the dataset, addressing a common challenge in healthcare datasets.\n3. **Comprehensive Model Evaluation:** The paper evaluates 10 language models, providing valuable insights into their strengths and limitations for medical applications.\n4. **Real-world Relevance:** The evaluation sheds light on practical applications and potential pitfalls of using large language models in healthcare, benefiting clinical usage.\n5. **Reproducibility Commitment:** The authors' commitment to releasing data and evaluation codes promotes reproducibility and encourages further research in the field.\n\n\n**Reasons to Reject:**\n1. **Limited Evaluation Details:** The abstract lacks clarity on the depth of the evaluation of the 10 language models, especially regarding performance across different tasks and domains.\n2. **Potential Data Bias:** The curated dataset may still have biases that are not explicitly addressed, which is a concern in healthcare applications.\n3. **Dataset Scope:** Given the vastness and diversity of the medical field, some aspects may not be covered adequately.\n\nThe average soundness and excitement scores suggest that the paper has a strong level of soundness in its methodology and arguments and provides exciting insights into the research direction. However, there are concerns about the level of detail in the evaluation and potential data biases, which should be addressed to strengthen the paper's overall quality."
            }
        },
        "id": "KIxRtx43Vb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TW831RjYQO",
        "replyto": "TW831RjYQO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4172/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584639,
        "cdate": 1696707584639,
        "tmdate": 1701465522967,
        "mdate": 1701465522967,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a deterministic autoencoder for NLP, as an alternative to VAE. The key idea is to introduce a contrastive learning loss to approximate entropy regularization. The paper also presents experiments on a variety of model architectures and downstream tasks. \n\nOverall, reviewers have divided view on soundness while show excitement in this paper's approach. \nReviewers like the novel technical method, and the strong empirical performance on various architectures. However, reviewers also point out missing discussion with vast past literature on VAE for text, use of contrastive loss in NLP, analysis of mode/posterior collapse issues in VAE, isotropic Gaussian prior versus other priors (e.g. mixture-of-Gaussian). Reviewers also have concerns about the paper's reference to \"large language models\" since BERT, GPT-2 are no longer considered large nowadays."
            }
        },
        "id": "ANTozFMSVJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TW2cBze4ZB",
        "replyto": "TW2cBze4ZB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3663/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567267,
        "cdate": 1696707567267,
        "tmdate": 1701465505865,
        "mdate": 1701465505865,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. The authors conduct experiments on medical datasets and discuss the impact of weight alignment in simple models.\n\nReviewers appreciate the interesting task addressed in the paper and commend the comprehensive experiments and clear writing style. They find the experiments illustrative and the task relevant, especially in the medical domain. However, concerns regarding the novelty of the work, privacy implications, and the necessity of the linear classifier are raised.\n\nIn light of the reviewers' comments, the paper's contributions are evaluated. The paper's strengths lie in its attempt to bridge the gap between LLMs and interpretable features in the medical domain. However, the lack of clear evidence demonstrating the superiority of the proposed approach over traditional methods like TF-IDF and the absence of explanations for certain model decisions weaken the paper's overall impact.\n\nDespite concerns raised by reviewers, the paper receives an average score above the acceptance board-line, indicating its potential value. Therefore, considering the overall positive sentiment and the potential significance of the paper's contributions, the decision should lean toward acceptance. However, the authors are encouraged to address the reviewers' suggestions, particularly by providing more robust evidence of the proposed method's superiority and further clarifying the interpretability aspect."
            }
        },
        "id": "IkasKBWTsH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TSdWY9GaHA",
        "replyto": "TSdWY9GaHA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission208/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481822,
        "cdate": 1696707481822,
        "tmdate": 1701465391110,
        "mdate": 1701465391110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper deals with shortcuts / spurious correlations that LLMs might rely on to make predictions. Authors propose a framework where one LLM (the \"editor\") makes minimal edits to text to add or remove shortcut triggers, guided by the confidence of a second \"target\" LLM. All reviewers highlighted that the paper addresses an important issue of LLMs making predictions via shortcuts as opposed to relying on a full ‘understanding’ of the context. Results reveal that even strong LLMs such as GPT-4 and ChatGPT are sensitive to such shortcut triggers.\n\nThere were some concerns about the quality of ShotcutQA. In particular, whether shortcuts could add ambiguity. Additionally, the paper could benefit from analysis that underscores the contribution of each type of editing, results based on domains (fiction vs non-fiction), etc.. Authors should consider updating papers to add such analyses and discussions."
            }
        },
        "id": "JrXqEaOWdq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TKzERU0kq1",
        "replyto": "TKzERU0kq1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1514/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516172,
        "cdate": 1696707516172,
        "tmdate": 1701465433109,
        "mdate": 1701465433109,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors present a richly annotated dataset (in English and Chinese) for character alignment based on Harry Potter, aiming to provide a benchmark and also to boost research in this field. The reviewers generally appreciate the importance and usefulness of the dataset as well as clarity of presentation but also raise concerns regarding the amount of effort that would be needed to train a character-aligned system (especially one that is not aligned with Harry Potter), the small size of the dataset, and some concerns regarding annotation. The authors have provided responses to address most concerns and therefore I recommend accepting this paper in Findings."
            }
        },
        "id": "dVmTZiNbZW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TKo2JXw7vL",
        "replyto": "TKo2JXw7vL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2605/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545666,
        "cdate": 1696707545666,
        "tmdate": 1701465470504,
        "mdate": 1701465470504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agree on the fact that this paper provides a valuable contribution to investigate the diachronic phenomenon of lexical polysemy, by providing a clearly defined theoretical and formal approach to gain a better understanding of language structure and use.\nThe main concern is related to the relevance of this formal approach for the nlp community. The methodology presentation is dense and would necessitate further details, in particular concerning the interpretation of results."
            }
        },
        "id": "NYkTAuZz11",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "TKGgLVYRqJ",
        "replyto": "TKGgLVYRqJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4227/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585730,
        "cdate": 1696707585730,
        "tmdate": 1701465524669,
        "mdate": 1701465524669,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a method (quick back-translation) that can mostly be seen as an efficiency improvement for unsupervised MT, re-using the encoder of a unsupervised MT training setup to perform non-autoregressive decoding instead of using a separate, autoregressive model.\n\nstrengths:\n- on average, reviewers consider the novelty one of the strengths of the paper\n- writing and extensive experimentation are convincing.\n\nweaknesses:\n- improvements in quality are minimal. The main benefit is training efficiency for unsupervised MT, which has lower significance than demonstrating quality improvements, or applicability beyond unsupervised MT."
            }
        },
        "id": "kAEtCAoqkc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "THr9aJ3z9k",
        "replyto": "THr9aJ3z9k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4744/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597652,
        "cdate": 1696707597652,
        "tmdate": 1701465540566,
        "mdate": 1701465540566,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new method that incorporates syntactic dependency information into a Transformer model without using GNNs for Aspect-Based Sentiment Classification (ABSC) task. Concretely, the proposed method inserts a new token [g] to represent syntactic information. Moreover, the authors introduce VDC (Variable distance control) that specifies the attention mask used by [g], and further extend it from heuristic to learnable. They also introduce DAA (dependency-aware aggregation) to represent dependency labels.    \n\nAs reviewers mentioned, the authors present a new and interesting method for ABSC. Unlike the baselines, this method does not use GNNs therefore mitigates suboptimal interaction and aggregation for the task. \n\nI appreciated that the authors were highly engaged in the discussion period and provided a large amount of additional results and clarification. Addressing reviewers' feedback, the authors conducted ablation experiments to show the impact of DAA. Although DAA improves the results only by a small margin, when combined with Auto-VDC the model results in the best performance. Additionally, the authors provided another set of comparisons with the BERT-based baselines. Reflecting on the authors' comment, their method is only competitive in this setting, however, methodological novelty and achieving high results with Roberta model is an important contribution. Finally, I believe incorporating additional content provided during the discussion period, will improve the paper."
            }
        },
        "id": "q4hqKelznW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "T9wuVnNa5v",
        "replyto": "T9wuVnNa5v",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2852/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550774,
        "cdate": 1696707550774,
        "tmdate": 1701465478728,
        "mdate": 1701465478728,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper focuses on scientific paper summarization and investigates the grounding of generated summaries in the context of the citances (i.e. citation contexts), aiming at producing more informative summaries to assist readers in understanding a citation without the need to refer to the cited paper. Reviewers see many merits in the paper (adresses an interesting and timely task, new dataset, extensive experiments), and some of their initial concerns were addressed by the comprehensive authors's responses. After the discussion period, I feel that there are no real major issues with this work but reviewers have remaining concerns mainly about the evaluation methodology (e.g. GPT generated ground truth, limited scope of manual evaluation)."
            }
        },
        "id": "QVOiG5uIvk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "T9jJsFUGtI",
        "replyto": "T9jJsFUGtI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4057/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582157,
        "cdate": 1696707582157,
        "tmdate": 1701465518626,
        "mdate": 1701465518626,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a novel method for data argmentation for speech translation.  By providing different segmentations of the document level triples of the training data, the data size are multiplied.  Intensive experiments are provided showing the effectiveness of the proposed method.  The paper is clearly written and the experiments are well organized and the results show the improvement is significant.   Three reviewers gave positive reviews, while the reviewer n8SA had some concerns on the comparison.  The authors gave detailed response which I think answered the concerns well."
            }
        },
        "id": "1AM9xBGpwm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "T8ABT8q3FS",
        "replyto": "T8ABT8q3FS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission519/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489747,
        "cdate": 1696707489747,
        "tmdate": 1701465401980,
        "mdate": 1701465401980,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Note: This paper involves human-subjects recruited from Amazon Mechanical Turk. Any kind of research that involves paid human-subjects requires an approved IRB. The human-tasks involved in the study required annotation of datasets including potentially harmful content. This further warrants that the proposed submission can only be accepted with an approved IRB. Without an IRB the paper should not be accepted. The current ethic statement or the authors' responses in the rebuttal do not address this issue. \n\n\nMost reviewers find the soundness of this work to be good, providing sufficient support for its major arguments. All, are however, ambivalent in terms of excitement. Several key areas  of improvement highlighted by the reviewers can make the submission stronger as summarized below, to which the authors have sufficiently responded to in their revision plan. \n\n1.\tImprove overall clarity of presentation/ texts \n2.\tClarify annotator guideline/ background/ survey details\n3.\tDefine stereotype and provide more examples\n4.\tDiscuss limitations of subjective interpretation around stereotypes (e.g., what one ethnic group might consider offensive is not to others, in other words, there is limitations around the generalizability of stereotypes the authors are discussing; how is this resolved in the annotation process?\n\nOverall the authors address these concerns raised by the reviewers, however there is no discussion of an IRB for this submission, which is required for such studies that involve paid human subjects."
            }
        },
        "id": "U1x6pS9iUa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "T6GJ2Y0dn7",
        "replyto": "T6GJ2Y0dn7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4563/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593047,
        "cdate": 1696707593047,
        "tmdate": 1701465534846,
        "mdate": 1701465534846,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper focuses on the analysis and mitigation of dataset biases in Visual Question Answering (VQA) tasks with long answer choices. All reviewers appreciated the detailed analysis of bias in this task. But the reviewers also raised concerns on the inference process of the proposed method."
            }
        },
        "id": "6gHZjlwu1d",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "T3n9nbeIKc",
        "replyto": "T3n9nbeIKc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4198/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585247,
        "cdate": 1696707585247,
        "tmdate": 1701465523944,
        "mdate": 1701465523944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper tackles the common approach to evaluating of Knowledge-Base Embeddings (KBE) on link prediction task. It shows, comparing several KBE models that performance in this task does not correlate with performance in the downstream task (POS tagging in this case).\nAll three reviewers agreed that the paper is sound and interesting. The main objection was that the evaluation is performed using only one downstream task, namely POS tagging. During the rebuttal period the authors conducted additional experiments and showed that they support the main claim of the paper. I think these results make the paper stronger, and should be reported in the final version.\nOther objections concerned main assumptions and positioning of the paper. I found the authors' responses convincing, and they probably should be reproduced on the extra page."
            }
        },
        "id": "JqreB01OwU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "T3kZcQ2ivs",
        "replyto": "T3kZcQ2ivs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission766/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495445,
        "cdate": 1696707495445,
        "tmdate": 1701465409717,
        "mdate": 1701465409717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The decision based on the reviews suggests acceptance, but with revisions needed. The reviewers have recognized the paper's systematic research approach, clear argumentation, and its contribution towards the understanding of automated fact-checking. They find the topic relevant and timely, and appreciate the detailed annotation guidelines and analysis procedure which lend credibility to the arguments in the paper. \nHowever, they also identify certain limitations such as the paper's complexity and excessive use of specialized terminology which makes it difficult to understand. There was also criticism about the missing information on the papers examined. Therefore, while the paper has potential, it requires some revisions before publication."
            }
        },
        "id": "T5yMoBwQVv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SzH7d4617q",
        "replyto": "SzH7d4617q",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission155/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480540,
        "cdate": 1696707480540,
        "tmdate": 1701465388927,
        "mdate": 1701465388927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors of this paper evaluate generative LLMs on three NLP tasks, namely text summarization, text simplification, and grammatical error correction, using both automatic metrics and human evaluations. They also experiment with using GPT-4 as if it were a \"human evaluator\".\n\nThe reviewers highlighted the following reasons to accept:\n- a systematic evaluation of multiple LLMs on three tasks, leading to clear and important takeaways: automatic evaluation metrics are insufficient for evaluating LLM outputs; GPT-4 is a good evaluator; ground truth references are bad\n- interesting negative result that GPT4 is not very good as an evaluator for grammar error correction\n\nThe following reasons to reject were mentioned:\n- A more detailed analysis of inter-annotator agreement would have been desirable. The authors noted that some of the requested information is available in an appendix, and that other parts of the requested discussion are either not possible or not meaningful with the available data.\n- Lacking details of the experimental settings leading to poor reproducibility. The authors gave complementary information in the rebuttal that addressed most issues. One reviewer decided to keep a low reproducibility score due to the confusing and changing OpenAI model naming conventions, which are however totally beyond the control of this paper's authors.\n- Limited novelty, in the sense that similar conclusions about human vs. automatic evaluations have been drawn before. The authors responded that there is no prior work on text simplification or grammatical error correction evaluation in connection with LLMs.\n- Some limitations are not presented sufficiently prominently. The authors promised to make the requested changes in the camera-ready version."
            }
        },
        "id": "RyzRVhaNe8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SyEwsV52Dk",
        "replyto": "SyEwsV52Dk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1504/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515937,
        "cdate": 1696707515937,
        "tmdate": 1701465432784,
        "mdate": 1701465432784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces the novel and socially impactful task of tracking event locations in personal narratives. The compelling application to Holocaust survivor testimonies underscores the moral importance of computationally analyzing such historical documents. The key technical contribution is a rigorous exploration of structured prediction architectures for extracting location sequences, including sequential conditional random fields (CRFs) and hierarchical transformers. Experiments demonstrate that modelling the global narrative context significantly improves prediction accuracy compared to greedy localized approaches.\n\nThe proposed task taps into a broader interest in narrative understanding and temporal commonsense reasoning. The testimony dataset appears remarkably well-suited, containing a large collection of topically-related personal accounts centered around a small set of canonical locations. However, transparency is needed around the subjective data filtering process to enable reproducibility without public access.\n\nReviewers praise the novelty, motivation, and dataset potential, but note a few areas for improvement. For example, Reviewer 2 believes the CRF focus should be downplayed given the superior performance of transformers. Reviewers 1 and 3 raise important concerns about aspects of the data creation process, such as the subjectivity of filtering location labels. Reviewer 3 offers multiple insightful suggestions like evaluating segmentation explicitly and analyzing bidirectionality. Overall the reviews indicate an intriguing task and technical approach while recommending improvements to evaluation, transparency, and framing.\n\n\nPersonally, I note the following: The CRF models underperform simple transformers, this suggests to me an overemphasis on sequential dependencies in framing and analysis. Additional segmentation metrics are recommended to better evaluate location trajectory extraction. Also, I am unconvinced about the results reported for ChatGPT. I have tried to replicate the task with ChatGPT and Claude and both appeared to have performed excellently on event location extraction. I concur that I have used different data for my experiments (mostly pseudo-text and not a real-life dataset like the one used in the paper). Also, I have used a simplified prompt than the example presented in the appendix of the paper. However, I do not think both factors should greatly impact the outcome. I think this calls for a more in-depth analysis on the part of the authors, especially to justify their approaches against out-of-the-box finetuning of LLMs for the same task. Also, an in-depth analysis of model failures could provide useful insights into remaining challenges. Lastly, details on hyperparameter tuning and model specifications would further support reproducibility."
            }
        },
        "id": "uefvopOV81",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SxrA1okPXY",
        "replyto": "SxrA1okPXY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1614/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707520220,
        "cdate": 1696707520220,
        "tmdate": 1701465436187,
        "mdate": 1701465436187,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new model compression procedure that uses, at inference time, information prepared with a larger model. The proposed method also incorporates further components such as soft prompts and PPO, and its main application scenario lies in low-resource settings. The reviewers highlight the good empirical performance of the proposed method at high compression rates, and the useful ablations. They note that the work is novel in its way of combining existing methods. One of the central criticisms reviewers raise is the complexity of the training/compression procedure, which requires several steps to train soft prompts, PPO, and construction of the database. While this may motivate future work (on simplifying said procedure) it could also limit practicality. Reviewers also mention that the proposed procedure has been evaluated on only NLU/classification tasks, which might be a limiting factor. The information provided via author response on the remaining points of criticism, such as comparison to other baselines, and faithfulness of the database are useful additional datapoints that should be incorporated in the next iteration of the paper."
            }
        },
        "id": "Q6agok1x35",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SwphsE7hYO",
        "replyto": "SwphsE7hYO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1649/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521854,
        "cdate": 1696707521854,
        "tmdate": 1701465437336,
        "mdate": 1701465437336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper improves on existing work on unlikelihood training for detoxification by proposing a new objective that directly contrasts the gold standard rephrasing with the identity input-to-output mapping to focus directly on the style of the text. \n\nReviewers generally found this work sound, interesting and clearly presented and the improvements over the baseline compelling.\n\nReviewers brought up some valuable criticism based on the empiricism: lack of multiple models, human evaluation and analysis of model failures. The author rebuttal addresses many of these concerns, including an analysis of the complexity of their approach among some new empirical results.\n\nOverall, while it’s not convincing to me that detoxification is merely style transfer, the proposed objective seems quite effective at improving the baselines presented. We hope the authors include the new results and suggested changes in the next iteration of the paper."
            }
        },
        "id": "CgTCNlAwJM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SvmlxXMLYr",
        "replyto": "SvmlxXMLYr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2015/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532245,
        "cdate": 1696707532245,
        "tmdate": 1701465450625,
        "mdate": 1701465450625,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the issue of robustness in GEC – they study how the contextual changes that are not relevant to the error being corrected affect model performance, and show that state-of-the-art models exhibit variation in proposed corrections when the context is varied in ways that should not affect the correction. The paper presents a new dataset, which is a sample of 2 English GEC benchmarks, combined with data from the TEM-8 dataset. They annotated the new dataset with contextual perturbations. The paper also proposes a method of fine-tuning GEC models to avoid variation and to encourage consistency.\n\nThe paper presents solid work, and addresses an interesting topic that should be relevant to GEC researchers and those working on model robustness in NLP in general. \n\n Ethical concerns identified during the regular  reviewing and pointed out in the ethical reviews should be addressed."
            }
        },
        "id": "EeubAOJOBS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Srxf1V2jPa",
        "replyto": "Srxf1V2jPa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1766/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707526709,
        "cdate": 1696707526709,
        "tmdate": 1701465440745,
        "mdate": 1701465440745,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviews for this paper contain rather divided opinions, some arguing the importance of this contribution, others that it is insufficient.   \nReviewers X7cv and  s1qS praise the paper for addressing a significant problem, offering a well-annotated dataset, and presenting a clear and accessible write-up. They find no reasons to reject the paper. I tend to agree with the importance of the proposed resource. \nReviewer KqAR had concerns about discussion of certain choices as well as ethics concerns, however most of their questions were answered through the rebuttal and turned to a positive outlook. \nOverall, authors provided a thorough and convincing  responses in the rebuttal period to all of the reviewers. \nReviewer YUyd provided important clarification that while the dataset is novel and challenging, the task of detecting propaganda itself is existing, and finds the contribution mediocre.  Reviewer o5WW is concerned with the small size of the dataset and explanations behind some experimental choices. \nOverall, I would recommend the authors to include the reviewers suggestions, answers to their questions, clarifications into the text of the paper."
            }
        },
        "id": "lZJslrADwB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SoypWgmvqP",
        "replyto": "SoypWgmvqP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2023/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532408,
        "cdate": 1696707532408,
        "tmdate": 1701465450907,
        "mdate": 1701465450907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper delves into the task of link prediction within incomplete KGs using an innovative technique that utilizes in-context learning (ICL) through a mechanism called Knowledge Prompt. They show that their method, KICGPT, demonstrates strong predictive abilities for missing entities in KGs. The authors claim that their approach is the first in combining Large Language Models (LLMs) with triple-based KGC methods for alleviating the long-tail problem in KGC. \n\nThe main concerns raised by the reviewers were:\n1. Lack of clear motivation for the long tail problem - this was adequately addressed in the rebuttal as acknowledged by the reviewer\n2. Use of a proprietary LLM (ChatGPT) - in response the authors have now repeated the experiments using Llama2 which is great\n\nA few other minor issues were also adequately addressed by the authors. \n\nOverall, I think the paper has some interesting ideas and a thorough evaluation. The community would benefit from these ideas and build upon them.\n\nI request the authors to fit all the additional results provided during the rebuttal, in the main body of appendix of the paper, as appropriate."
            }
        },
        "id": "nBpxNVIa1r",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SnFmGmKTn1",
        "replyto": "SnFmGmKTn1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission405/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486707,
        "cdate": 1696707486707,
        "tmdate": 1701465397741,
        "mdate": 1701465397741,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "A dialog inpainting pipeline to convert dialogs into question-answering. Sound fidelity evaluation."
            }
        },
        "id": "LqeVBuK8a5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Sm3RzRKCel",
        "replyto": "Sm3RzRKCel",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3032/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554762,
        "cdate": 1696707554762,
        "tmdate": 1701465485058,
        "mdate": 1701465485058,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Proposes an approach for article generation that is enhanced with two key components: a NER module and an entity-aware component. Experiments on 3 public datasets show significant gain on perplexity and recall@k. \n\nPLUS: \n\n- extensive comparison with LMs on closely related tasks. \n- ENGINE can potentially contribute valuable training data for machine generated text detectors. \n\nMINUS:  \n\n- human study does not correlate with the main use case. \n- precision of NEs obtained from articles? \n- Missing details about loss function, learning rate and training strategies for ENGINE. \n- incomplete ablation study."
            }
        },
        "id": "lTNyol9ddF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SlL3dr0Xa9",
        "replyto": "SlL3dr0Xa9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission469/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488512,
        "cdate": 1696707488512,
        "tmdate": 1701465400064,
        "mdate": 1701465400064,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper under review introduces and evaluates language models with respect to human-like sentence processing, specifically focusing on cue-based memory retrieval and cognitive plausibility. The models are assessed in terms of their sensitivity to syntactic dependencies and their alignment with human sentence processing data, particularly in sentences involving agreement attraction. While the paper has notable strengths, such as clear theoretical motivation and interesting evaluations, there are concerns regarding the choice of baselines, generalizability, and the paper's organization.\n\nPros from the reviews:\n\n- A reviewer highlights the paper's clear theoretical motivation to address limitations in prior work, making it valuable for researchers in both cognitive science and neural network modeling.\n\n- A reviewer appreciates the incorporation of both expectation- and cue-based serial processing, which is a significant contribution, and suggests that the results support the authors' claims.\n\n- A reviewer acknowledges the well-controlled experimental design and positive results, indicating that the proposed models can replicate human sentence processing behavior.\n\nCons from the reviews:\n\n- Two reviewers express concerns about the absence of comparisons with baseline models, such as multi-head attention Transformers, which would help contextualize the contributions of the proposed models.\n\n- One reviewer suggests that the paper's focus on subject-verb agreement may limit the generalizability of the model, and encourages exploring other dependencies associated with cue-based retrieval.\n\n- Another reviewer criticizes the paper's writing, pointing out typos and the need for a more focused reorganization of the structure to improve readability.\n\nGiven the strong paper, good reviews and comprehensive rebuttals, the soundness of the paper is quite clear. Yet, the authors often explain that they will include additional results, explanation, data etc. in the final version of the paper. For an excellent contribution, these additional resources would be essential. Given the overall favourable reviews, I would like to see the paper at EMNLP."
            }
        },
        "id": "OnyPRlJn0W",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SkWgL49qwI",
        "replyto": "SkWgL49qwI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5052/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605899,
        "cdate": 1696707605899,
        "tmdate": 1701465548583,
        "mdate": 1701465548583,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper received a mixed reviews from 3 reviewers. Reviewers Zmbu and 4roi recommend acceptance given the strong perfromance of the method and the potential of elimate annotation cost. While all reviewers recommend rejection as the paper is overall not well-written and is hard to understand, e.g., complicated figures and excessive use of symbols. Reviewer MGd2 also mentioned the work unnecessarily claimed as first work for unsupervised TSG. AC values the contributions of targeting reducing the use of annotations in the TSG task, however, AC does not agree with the authors that this work should be described as \"truly unsupervised\" due to its use of annotated assets (Visual Genome and Kinetics). There are work that use purely unannotated data for the task, e.g., \"Zero-shot Video Moment Retrieval With Off-the-Shelf Models\"."
            }
        },
        "id": "pKpfpTfdGS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SihQ9bBLWa",
        "replyto": "SihQ9bBLWa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission8/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476642,
        "cdate": 1696707476642,
        "tmdate": 1701465383808,
        "mdate": 1701465383808,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a novel method for predicting task transferability in the context of natural language understanding tasks. The approach involves learning an affinity scoring function based on soft prompts, allowing efficient selection of source tasks for transfer learning. Extensive experiments demonstrate the method's effectiveness compared to baselines, although some additional comparisons and clarifications are needed. The paper also highlights the practicality of using soft prompts over manually-crafted prompts and emphasizes its applicability to various NLU tasks.\n\nThe authors commit to including a comparison of soft prompts and manually-written prompts, as well as more OOD experiments in the revised version. While they promise to look into LLMs like ChatGPT, I don't think that's necessary given that direct access to the weights is not given, making the approach difficult to assess from a research perspective."
            }
        },
        "id": "zEUv9Z1g8F",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ShQoWnMu1b",
        "replyto": "ShQoWnMu1b",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission623/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492354,
        "cdate": 1696707492354,
        "tmdate": 1701465405641,
        "mdate": 1701465405641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose to split questions that require multiple steps into finer grained questions, answering them and then combining this to generate the final answer. This is done by training a model over QDMR parsings. As QDMR annotation does not have intermediate answers, the authors use hard EM to train the model to answer the questions.\n\nReviewers overall like this approach and find the methodology convincing.\n\nOn the other hand, the reviewers think that the experimental setup in the paper is limiting as the authors does not evaluate on an eval set for which there is no training data. \nI agree with this -- this is indeed limiting as QDMR parsings are only available for a limited amount of dataset, and hard expensive to produce. In general the setup would be that there is no training set for the target domain so it is important to test the generalization of the proposed method."
            }
        },
        "id": "NmjJmMWEVz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SfI8GT3xdb",
        "replyto": "SfI8GT3xdb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5134/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707607734,
        "cdate": 1696707607734,
        "tmdate": 1701465550915,
        "mdate": 1701465550915,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers agree that this work has made some interesting contributions to the field of IE by introducing the unified approach that casts various IE problems into a span graph prediction problem. The authors used a non-auto-regressive approach, which to me is the right way to go. There are useful suggestions for strengthening the work with additional experiments, which I suggest the authors incorporate into the final version."
            }
        },
        "id": "sWNwm99hJV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SdpSaw26XT",
        "replyto": "SdpSaw26XT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1776/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527005,
        "cdate": 1696707527005,
        "tmdate": 1701465441131,
        "mdate": 1701465441131,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes a new approach for multi-task efficient fine-tuning. Proposed methods creates a prompt bank using a set of \"meta-training\" tasks and then combines individual items in the prompt bank at evaluation to achieve superior transfer. Reviewers agree on the clarity of writing and presentation and find the proposed method novel. There are some concerns about comparison to relevant work and using of larger models, authors provide comparison to other relevant work and commit to adding larger models by the camera-ready."
            }
        },
        "id": "OsduKrQspo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SViJgzox1z",
        "replyto": "SViJgzox1z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2515/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543669,
        "cdate": 1696707543669,
        "tmdate": 1701465467361,
        "mdate": 1701465467361,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper explores how LLMs can be used to support teachers in using growth mindset supportive language (GMSL). Teachers annotate classroom transcripts and provide expert reframings. GPT-4 is used to reframe student-directed language to exhibit a growth mindset. Then, student and teachers are surveyed on (1) teacher’s perceived growth mindset, (2) teacher’s promotion of challenge-taking behavior, (3) shame felt by students in the teacher’s class, and (4) respect. It is found that GPT-4 responses are often scored higher than expert responses.\n\nThis paper displays a nice application of LLMs with positive social impact. The issues mentioned by reviewer jpn8 regarding the GMSL guide seem to have largely been addressed. Reviewer swnb's point regarding testing this approach in a classroom is an important one, but the paper can stand alone prior to such testing (with the expectation of future publication of time-consuming follow-up work). This also is related to reviewer QY3p's point about actual learning outcomes."
            }
        },
        "id": "sF3kNmXtQj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SUAeMJKg6b",
        "replyto": "SUAeMJKg6b",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4154/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584294,
        "cdate": 1696707584294,
        "tmdate": 1701465522260,
        "mdate": 1701465522260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores the ability of large language models (LLMs) to handle character-level scrambled text. The authors created a benchmark and conducted experiments to test this. They found that GPT-4 excels at understanding scrambled text, even when the letters of words are scrambled. Other LLMs also perform well, but GPT-4 stands out for its remarkable performance in processing inputs with unnatural errors. The authors evaluated the performance of the GPT-4 method for two tasks: scrambled sentence recovery and scrambled question answering. They also propose two new datasets.  This work is interesting and well conducted.\n\nPros:\nThis paper addresses an interesting research question and proposes a benchmark for the corresponding task. \nThe paper is well-grounded in current literature, citing recent approaches and advances in the field. \nThe methodology seems simple but well thought-out. The  investigation could contribute to the probing of the abilities of LLMs. \nThe authors extended  the evaluation models and datasets during the rebuttal.\nMost of the cons covered in the rebuttal.\nTypos, proofreading is highly recommended."
            }
        },
        "id": "kSqzWG82Wh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "STHKApXVMH",
        "replyto": "STHKApXVMH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5800/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618611,
        "cdate": 1696707618611,
        "tmdate": 1701465567396,
        "mdate": 1701465567396,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The work presents a rule-based and a neural-based tool for gender-neutral rewriting for Portuguese and provide the first Portuguese dataset explicitly containing gender-neutral language, including a manually annotated gold standard.\n\nThe reviews show ambivalent scores with respect to soundness and excitement, which mostly fall on the upper bound, as it is recognized to be an interesting contribution and topic. Yet, the concerns raised are sensible, and have to do with a fundamental part of the work: the strategy of using a rule-base system to create the data to train the neural system, something which may explain why the rule-based system performs better, even though the authors do not sufficiently discuss this issue. The remaining issues should also be clarified in the final version if accepted (the NER problems, data description, Portuguese examples translated or glossed to facilitate comprehension of gendered terms, etc.)\n\nI should say that a gold standard or 500 sentences for a task like this does not seem too small for a low-resource context. Furthermore, the fact that the paper does not make novel technical (e.g. algorithmic or modelling) contributions should not preclude its publication, as it is  not the work's objective."
            }
        },
        "id": "7aci4NLa03",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ST0ejo0mnc",
        "replyto": "ST0ejo0mnc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2938/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552531,
        "cdate": 1696707552531,
        "tmdate": 1701465481843,
        "mdate": 1701465481843,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "As a reviewer indicates the authors propose a new model named EARA, an entity-aligned attention-based and retrieval-augmented Pre-trained Language Models. EARA firstly aligns the same type of fine-grained information in each sentence pair with an entity alignment matrix. Then, EARA regularizes the attention mechanism with an entity alignment matrix with an auxiliary loss. Finally, a retrieval module is added which retrieves similar instances to improve the model’s generalization. The experiments reflect that EARA can achieve state-of-the-art performance on three datasets.\n\n\nThe main reasons to accept the paper are the following ones:\n\n\tThis is an appropriate methods paper. It would provide a new SotA in Medical STS.\n\n\tThere is an innovative methodological contribution here with an adequate evaluation showing excellent results. \n\n\tThis paper shows that this method makes PLMs pay more attention to important entities by aligning the same type entities in sentence pairs.\n\n \tEARA uses a retriever module and feeds retrievals into transformers, which improves the model’s generalization and eases the overfitting problem for the biomedical STS datasets.\n\n\tThe authors visualize their results well for their evaluation metrics and perform a useful sensitivity analysis. Results show that their model can achieve promising performance both on in-domain and out-of-domain 102 datasets.\n\n \tTwo innovative points show good transferability on three BERT-based backbones. ● EARA achieve state-of-the-art performance on both in-domain and out-of-domain datasets.\n\tThe proposed method is effective, model-agnostic, and could be applied to different fields where entity-alignment is important.\n\n   \t The authors proved the relevant code to reproduce the results\n\n\nThe main reasons to reject the paper are the following ones:\n\n\tEnthusiasm is dampened somewhat by the lack of clarity in presentation. The writing and presentation needs some polish (see other notes), and the discussion needs some depth -- loose ends remain untied. \n\n\tThe experimental results lack a significance test; hence, the demonstration of experimental validity is insufficient.\n\n\tThere are plenty of evaluations, but they don't seem to all follow a progression of thought. Another example: the Limitations section doesn't discuss limitations\n\n\tFAISS as an unsupervised retrieval technique has already been quite mature. In this paper, the authors applied FAISS to the task of biomedical semantic text similarity and conducted retrieval of similar instances. Although this application is not particularly innovative.\n\n   \t The method of regularizing the attention mechanism is not novel, and the author did not explain in depth why introducing the entity alignment matrix brings significant effects to this method, especially for Bio data.\n\n\tEntity-Aligned Multi-Head Attention（EA）did not bring significant improvement, as evidenced by the results of the ablation experiments. Compared to the supervision signals brought by retrieving additional knowledge, the effect of EA itself is not significant. (solved by introducing significant test)\n\n   \t The optimized backbone models used in the study are all methods before 2020, This makes the credibility of the EARA insufficient. (have more backbones now)\n\n\nIn sum, although the paper can be methodologically interesting, reviewers agree that the way it is written hampers the understanding of the paper. More work is needed for it to become a good paper."
            }
        },
        "id": "C81hXIIGYR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SS44Mrv21o",
        "replyto": "SS44Mrv21o",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1381/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511682,
        "cdate": 1696707511682,
        "tmdate": 1701465429133,
        "mdate": 1701465429133,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a novel neuro-symbolic approach that models WSD as a word substitution task to improve sentiment analysis performance. It identifies the ambiguous words and substitutes them using candidates from WordNet. \nThe approach is considered reasonable by the reviewers. It offers interpretability advantages and improves the sentiment analysis performance. However, the reviewers raised concerns regarding the reliance on static resources like WordNet, limited baselines, the time-consuming nature of this approach, and the experimental results. The authors addressed some of these concerns in their response by providing further clarification, however, the reviewers are still quite ambivalent about the excitement of this work."
            }
        },
        "id": "JRmdZjL8vr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SQodZvCM5g",
        "replyto": "SQodZvCM5g",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission913/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499280,
        "cdate": 1696707499280,
        "tmdate": 1701465414644,
        "mdate": 1701465414644,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This short paper investigates the contribution of context in unsupervised methods for building sentence representations, by comparing content-based methods (e.g. topic modeling, word vector averaging, LSTM autoencoders) with their context-based version, which was obtained by using the same vectors as the input for a Skip Thought-like training regime. The evaluation, conducted on a dialog act tagging task on the Switchboard corpus via a linear probing classifier, reveal that the advantage of the latter training regime is not significant, while there is a clear benefit in the simple increase of the dimensionality of the sentence vector.\n\nAll the reviewers agree that this is a clear and well-conducted study. On the weak side, I tend to agree with Reviewer 2, who pointed out that the contribution of this paper is incremental, and it would be beneficial to add some extra experiments with more recent sentence representations method."
            }
        },
        "id": "X5xj8mXHL5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SPtskxPEiV",
        "replyto": "SPtskxPEiV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4344/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587890,
        "cdate": 1696707587890,
        "tmdate": 1701465528516,
        "mdate": 1701465528516,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a multi-task learning framework for continual training on procedural data, combining the methods of contrastive learning and mask-step modeling. \n\nThe paper contains a thorough set of experiments and the final system achieves state-of-the-art results. \nThere is also sufficient analysis and ablation experiments.\n\nAs a downside, the framework seems to be a combination of two existing methods, which limits its novel contribution. \nRelevance compared to zero-shot LLMs is left somewhat unclear.\nSeveral reviewers point out that the paper is difficult to read and understand, containing dense text, many acronyms, and very few examples."
            }
        },
        "id": "r9JuBJEKmy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SP8zIwanHD",
        "replyto": "SP8zIwanHD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2159/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535869,
        "cdate": 1696707535869,
        "tmdate": 1701465456205,
        "mdate": 1701465456205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The proposed approach focuses on addressing hallucination issues in multilingual summarization, particularly in cross-lingual transfer learning with low-resource languages. It introduces a model-based metric called mFact to measure the faithfulness of document-summary pairs, helping to reduce hallucinations. This metric is created through a meticulous process involving selecting pseudo-faithful and hallucinated pairs, translating them into the target language, and fine-tuning a multilingual BERT model. By leveraging mFact, the approach introduces a weighted loss method to improve summarization quality and reduce hallucinations. \n\nPro:\n- Introduction of the novel mFact metric, which is quite novel for studying hallucination in cross-lingual settings\n- Extensive experiments validate its effectiveness in enhancing ROUGE scores across multiple languages\n\nCons:\n- There are some concerns regarding the complexity of the mFact metric creation, its generalizability, and the robustness of the hallucination mitigation approach\n- Need more insights into the practical application of the mFact metric, including discussions on associated costs and potential challenges, would be valuable. \n- Mixed results observed in Hindi and Turkish with no further clarification\n\nGiven these points, I would recommend for acceptance of the findings, but not against it being accepted to the main conference if SAC finds it suitable."
            }
        },
        "id": "C3XHKU3YHa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SNB6BwY2zy",
        "replyto": "SNB6BwY2zy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3617/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566299,
        "cdate": 1696707566299,
        "tmdate": 1701465503891,
        "mdate": 1701465503891,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This short paper offers a series of experiments with open-source LLM for document re-ranking. Some surprising results are obtained, such as instruction-tuning degrades the retrieval effectiveness. While the phenomenon is observed, the paper falls short in providing plausible explanations. There is thus limited insight about the observations.\n\nThe paper could be published for the surprising results obtained."
            }
        },
        "id": "ZJsu0s6B5W",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SJYTfbI59J",
        "replyto": "SJYTfbI59J",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1524/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516330,
        "cdate": 1696707516330,
        "tmdate": 1701465433397,
        "mdate": 1701465433397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The short paper proposes a set of 13 language-specific (English, Spanish, Swahili) probes for morphological generalization that are based on conjugation classes.\n\nAll reviewers agree that the work presents a nice, well-written linguistically-motivated study and strongly suggest it for acceptance. I do not see any clear objections and/or potential risks and would be happy support the paper and see it accepted."
            }
        },
        "id": "gJ5mOWo8mX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SJ0Da0j8n7",
        "replyto": "SJ0Da0j8n7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4648/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595204,
        "cdate": 1696707595204,
        "tmdate": 1701465537608,
        "mdate": 1701465537608,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "As reviewers indicate, this paper introduces a new method for AMR parsing called CHAP. In contrast to recent work which has focused on using pretrained transformer decoders to output linearized AMR graphs, CHAP attempts to add graph structure to the decoder architecture, while still maintaining compatibility with pretrained transformer decoders (namely BART). The new decoder architecture has three major modifications: (1) a new hierarchical attention mechanism based on Transformer Grammars (Sartran et al. 2022) is used in some decoder layers; (2) the transformer decoder generates a linearized tree representing a depth-first traversal of the AMR (\"base layer\"); (3) a \"coreference layer\" then uses an attention-based pointer mechanism to connect nodes in the tree that refer to the same thing, forming a graph with reentrancies. A benefit of using pointers is that it eliminates arbitrarily-named variables from the output.\n\nThe method itself seems to be an innovative improvement over prior work.The main experimental results pointed out by reviewers are the following: \n\n    They test multiple variants of TG-style hierarchical attention and find that the original one works best.\n    They test multiple ways of incorporating hierarchical attention into pretrained decoder layers and settle on an approach that uses it in parallel with the standard attention mechanism.\n    They show that CHAP performs better than several baselines on the AMR 2.0 and AMR 3.0 datasets, except for a concurrent model called LeakDistill, which uses additional alignment data. The authors explored multiple alternatives for CHA, justifying the architecture they eventually settled on.\n    They show that CHAP has better performance on 2 of 3 out-of-distribution benchmarks.\n    An ablation study showing that only the combination of all aspects of CHAP results in the best performance on AMR 3.0.\n\nAdditionally, the  paper is well written and clear, well-organized, and well-motivated.\n\nAccording to reviewers there are not many reasons to reject the paper.  \n\n    A reviewer indicates that in the experimental section, a lot of the SMATCH scores are very close, and  std devs or significance testing is recommended. The authors addressed this in the rebuttal. \n    Somewhat narrow applicability. No other applications beyond AMR parsing are discussed.\n    There are some unclarities an issues pointed out by Reviewer 3. Tthey can be ironed out within an iteration to a camera-ready version.\n\nAll in all, it seems that the authors could address the shortcomings in the final version of the paper."
            }
        },
        "id": "CsLU7POBBh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SI2CXa5eok",
        "replyto": "SI2CXa5eok",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4994/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707604027,
        "cdate": 1696707604027,
        "tmdate": 1701465546697,
        "mdate": 1701465546697,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a theoretical contribution on the highly relevant and timely topic of natural language understanding, in the light of recent technological developments. A few pointers to the literature have been suggested to better position the work in the context of EMNLP. Some definitions and theoretical principles may be further expanded to improve the readability and self-containment of the paper. \nWhile different from the \"usual\" contribution, this short paper has the potential to spark interesting and important discussions."
            }
        },
        "id": "FuoBB2k1Ti",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SHkMYY26KP",
        "replyto": "SHkMYY26KP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3576/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565573,
        "cdate": 1696707565573,
        "tmdate": 1701465502228,
        "mdate": 1701465502228,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This papers tackles the problem of out-of-distribution detection. The reviewers appreciated the combination of theoretical foundations and empirical evaluations that outperform the proposed baselines on the given datasets. The authors were very responsive to the questions and feedback from the reviewers and the additional insights should certainly improve even more this paper."
            }
        },
        "id": "wIbrb2FApS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SFTvQQA4KJ",
        "replyto": "SFTvQQA4KJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission81/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478629,
        "cdate": 1696707478629,
        "tmdate": 1701465386571,
        "mdate": 1701465386571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a Unified Summarization Benchmark (USB) encompassing 8 interconnected summarization tasks based on Wikipedia content, with a focus on factuality and correctness. The authors compare fine-tuned models and large language models (LLMs) on these tasks, revealing that fine-tuned models outperform LLMs, particularly in tasks tied to factuality. However, concerns regarding the quality of the human-annotated dataset and the need for more extensive human evaluation are raised, emphasizing the importance of assessing the dataset's reliability. Additionally, the paper lacks an in-depth analysis of experimental outcomes, especially the notable performance gaps observed in tasks other than generation. Some issues were resolved during the response period, specifically on human annotation."
            }
        },
        "id": "030DU2qeI7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SEFD0G4kf0",
        "replyto": "SEFD0G4kf0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission212/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481968,
        "cdate": 1696707481968,
        "tmdate": 1701465391306,
        "mdate": 1701465391306,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work studies mixup for few-shot text classification and proposes a 2-stage adaptation using two different subsets of the data. Reviewers in general agree about the experimental rigor of this work and found it novel and well-written. There are some concerns about the limited improvements observed. Though a SOTA result with big improvement is not a requirement for acceptance, it reduces the excitement felt by the reviewers."
            }
        },
        "id": "bRjKQHlzWY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "SAM1HFH6iB",
        "replyto": "SAM1HFH6iB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2484/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543150,
        "cdate": 1696707543150,
        "tmdate": 1701465466601,
        "mdate": 1701465466601,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a simple method to generate a single coherent and detailed image caption through the use of LLM to summarize outputs from multiple generated short image captions. The authors have done a nice job of rebuttal.\n\nIn general, 3 reviewers are positive about the paper, and one reviewer is slightly negative about the paper. Reviewers found that (1) the proposed method is simple and exciting; (2) experiments are comprehensive with detailed human evaluation study; (3) the paper is well written. One reviewer shared the concern that the standard evaluation metrics like BLEU/CIDEr scores are not reported and there is a lack of comparison with most SoTA image captioning models. The AC thinks that this is not a major concern, since the proposed method aims to provide a detailed comprehensive caption, rather than a short caption that aims to chase SoTA CIDEr score. Overall, given the reviews, the AC is positive about the paper."
            }
        },
        "id": "hr5bFQ2NoU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "S81zso7Imh",
        "replyto": "S81zso7Imh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission596/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491625,
        "cdate": 1696707491625,
        "tmdate": 1701465404664,
        "mdate": 1701465404664,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a system for resolving and following compound (multi-task) instructions in a robot instruction following task. The approach also maps the extracted tasks and arguments to known robot skills and the surrounding environment. The paper proposes a formal annotation scheme to represent subtasks and referenced objects. Performance is quantified using label prediction accuracy (rather than downstream instruction following performance), as well as inference time as the end goal is to run inference on-device. Details on the annotation process, dataset statistics, and more analysis of model errors should be added to the next revision of the paper."
            }
        },
        "id": "eeAu3NOwGM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "S5eTDhfjHM",
        "replyto": "S5eTDhfjHM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3110/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556316,
        "cdate": 1696707556316,
        "tmdate": 1701465487403,
        "mdate": 1701465487403,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary: This paper presents a new method called Hierarchy-aware Joint Supervised Contrastive Learning (HJCL), which aims to tackle the challenge of representation learning in Hierarchical Multi-Label Text Classification (HMTC). HJCL uses multi-head attention to extract label-aware embeddings from input features and employs different strategies to create contrastive pairs based on the label hierarchy, allowing for both instance-wise and label-wise contrastive learning. Extensive experiments conducted on four HMTC datasets demonstrate the effectiveness of the proposed approach.\n\nStrength: This paper presents a promising method to improve the accuracy of HMTC models. The paper is well-written and easy to follow.  Extensive experiments on four multi-path HMTC datasets are conducted.  All reviewers agree that this is a solid contribution but the excitement is relatively limited."
            }
        },
        "id": "DWzAwf6Rq6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "S0eqbM16k2",
        "replyto": "S0eqbM16k2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1526/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516417,
        "cdate": 1696707516417,
        "tmdate": 1701465433482,
        "mdate": 1701465433482,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a method for evaluating text to image generation models. They propose a metric Structured Semantic Alignment (SSA) SSA to address the gap in  existing evaluation methods on the assessment of fine-grained semantic concepts, such as object attributes, context details, and semantic relationships. SSA projects text/image features into a shared embedding space by text parsing and scene graph generation respectively, and is trained using contrastive learning.\n\nThe reviewers agree with the importance of the problem and are mostly excited about the work. However the reviewers do voice some key concerns:\n\n&nbsp;  (1) the lack of comprehensive comparisons with existing methods, and lack of human evaluations.\n\n&nbsp;  (2) lack of detailed analysis of the experimental results e.g. on longer prompts, complexity etc.\n\n&nbsp; (3) comparisons using different compositional generation approaches.\n\nNevertheless, the reviewers find the idea interesting and well motivated. The authors' responses also indicate that they are committed to addressing the above issues. It is important for the authors to address these valid and important issues to substantiate the main claims and arguments made in the paper and strengthen their paper."
            }
        },
        "id": "GNQVEhwPu8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RzWrY4KYg8",
        "replyto": "RzWrY4KYg8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission46/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477707,
        "cdate": 1696707477707,
        "tmdate": 1701465385189,
        "mdate": 1701465385189,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "All the reviewers agree on the fact that the paper provides a valuable contribution to the research on Community Question Answering (CQA) forums. In this paper, an intent-based and annotation-free method for duplicate question detection in CQA forums is proposed, that detects duplicate questions based on the intents they have, and does not require human annotation for preparing training data. Extensive experiments on several domains from two real-world datasets have been carried out to demonstrate the effectiveness of the proposed method. However, some concerns have been raised by the reviewers concerning the complexity of applying such method to a new CQA forum, and the effectiveness of the intent extraction method in particular for implicit and undefined intents. The writing of the paper should also be improved."
            }
        },
        "id": "mdf6npvsVb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Rz5eVgy8Sd",
        "replyto": "Rz5eVgy8Sd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2368/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540642,
        "cdate": 1696707540642,
        "tmdate": 1701465462989,
        "mdate": 1701465462989,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper addresses the multi-modality Spoken Language Understanding (SLU) problem. The authors aim to enhance the performance of a small-sized, fast-inference-speed student model (which is non-autoregressive) by distilling knowledge from a larger, slower-inference-speed teacher model (which is autoregressive). A novel \"Targeted Knowledge Distillation Framework\" (TKDF) is introduced. This framework incorporates an evaluator and employs a curriculum learning approach to choose appropriate targets for the student model. Experiments reveal that the distilled student model, though smaller in size, matches the performance of the larger model while being 4.5 times faster in inference.\n\n**Soundness Scores**: (3, 3, 2). \nThe paper's soundness is good. It includes results from two benchmarks. In the rebuttal phase, the authors introduced another benchmark result. TKDF exhibits a performance advantage over standard knowledge distillation, even though their inference speeds are comparable. However, to bolster the paper's claims, the following enhancements are suggested: \n1. A comprehensive analysis of the model across all test benchmarks.\n2. Inclusion of detailed experimental outcomes in the appendix.\n\n**Excitement Scores**: (3, 3, 3). \nFrom an excitement standpoint, the paper doesn't particularly stand out. The rationale for employing TKDF to tackle the SLU issue isn't convincingly articulated in the manuscript. As a result, the proposed TKDF appears to be a modest incremental improvement over existing methods."
            }
        },
        "id": "VVLp3f90kK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RxvMKDgZH6",
        "replyto": "RxvMKDgZH6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission838/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497316,
        "cdate": 1696707497316,
        "tmdate": 1701465412368,
        "mdate": 1701465412368,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates hallucination detection from LLMs, introducing a formal version of the task and creating strong baselines. This will encourage future work both through methods and through the provided dataset. The only noteworthy negatives identified in the review process were some assumptions of what is required from the underlying LLMs. However, these assumptions do not seem impossible for most models."
            }
        },
        "id": "K5UnW6hvsI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RwzFNbJ3Ez",
        "replyto": "RwzFNbJ3Ez",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission349/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485438,
        "cdate": 1696707485438,
        "tmdate": 1701465396060,
        "mdate": 1701465396060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This article primarily enhances few-shot named entity recognition tasks by utilizing label information and span filtering to address two key challenges: the issue of over-detected false spans and the problem of inaccurate and unstable prototypes. The proposed method demonstrates strong performance in the realm of few-shot named entity recognition tasks, as substantiated by a comprehensive array of experiments. After serious discussion and consideration, we appreciate this solid work but think that the innovation appears somewhat limited. Furthermore, the explanation of filtering false spans is unclear, making it difficult to determine the positive impact of label information on the model and whether it filters out useful knowledge.\n\nOverall, we appreciate both the efforts of reviewers and authors. This work is ready to publish, and the only concern is the novelty. We hope the authors can continue to improve the paper according to the comments and your response."
            }
        },
        "id": "wXhiNk4bF5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Rvz7LvHcdX",
        "replyto": "Rvz7LvHcdX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1228/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507895,
        "cdate": 1696707507895,
        "tmdate": 1701465424422,
        "mdate": 1701465424422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a new intersectional fairness definition that combines both absolute and relative performance across sensitive groups. The authors demonstrate the necessity of the proposed definition. The paper is well-written and has a clear structure. This technically sound paper could provide a better evaluation and understanding of intersectional fairness.\n\nHowever, this paper lacks a description of how to choose $\\alpha$. And there should be better evidence to show how exactly the \nIF improves model performance. Including an NLP task is also recommended."
            }
        },
        "id": "WFqCUQZLnF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RubWYFBZbG",
        "replyto": "RubWYFBZbG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2067/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533564,
        "cdate": 1696707533564,
        "tmdate": 1701465452442,
        "mdate": 1701465452442,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores the task of using LLMs to evaluate text quality. In particular, they study how components of LLM evaluation and G-Eval contribute to correlations with human raters. They have some interesting findings, e.g., that asking ChatGPT to explain it's rating improves correlations with human raters.\nOne main concern about this paper is the use of only ChatGPT. While the authors state why they made this decision in their rebuttal, the framing (especially early in the paper, e.g., the abstract) leads one to believe that their findings are more generalizable. The authors present some new results in the rebuttal, but there is missing information making these results seem rushed and incomplete (e.g., lack of a p-value when stating that something is statistically significant). I am concerned about the amount of content in the paper as well, which was mentioned by multiple reviewers. The reader shouldn't have to flip between the paper and the abstract to find details that are crucial to understanding the paper, and the fact that the authors include a note in the limitations section stating \"We cannot include all the experiment results in the main content, and we need to make our language as concise as possible due to the page limit\" is telling. The authors have made a lot of promises about what they can add to the final page for a camera-ready, but they have also included a huge amount of new results in their rebuttals, and space seems like it will continue to be an issue."
            }
        },
        "id": "xAcC34v2tf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RsK483IRuO",
        "replyto": "RsK483IRuO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4841/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599238,
        "cdate": 1696707599238,
        "tmdate": 1701465543084,
        "mdate": 1701465543084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The work introduced Instruction Fine-Tuning (IFT), a paradigm that enhances the zero-shot capabilities of Large Language Models (LLMs) by fine-tuning them on natural language instructions. It proposed two new requirements for metrics used to evaluate IFT models: Comparability Across Task (CAT) and Task and Format Agnostism (TFA). Showed that using LLMs as scoring agents is a good way to meet these requirements, and compares them with existing metrics such as BLEU, ROUGE, and GLUE. The work also demonstrated how synthetic data can be used to improve formatting skills, and how IFT models can reduce the need for expert data in industrial settings.\n\nPros: \n1: Introduced new evaluation metrics that are more suitable for measuring the performance of IFT models across different tasks and formats, which can help researchers and practitioners compare and benchmark their models more effectively.\n2: Provided practical insights and recommendations for fine-tuning IFT models on different tasks, and shows how synthetic data and IFT models can reduce the dependency on expert data in industrial settings, which can lower the cost and complexity of deploying IFT models in production."
            }
        },
        "id": "r2QwY5p1Pq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ror9xJhbdc",
        "replyto": "Ror9xJhbdc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3163/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557327,
        "cdate": 1696707557327,
        "tmdate": 1701465488965,
        "mdate": 1701465488965,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates the use of Graph Neural Networks (GNNs) in text classification and conducts a thorough analysis of different graph representation and text representation methods. The study compares the performance of GNNs with mainstream text classification methods and addresses questions related to their efficiency and effectiveness for text classification tasks. The paper's strengths lie in its comprehensive experimental design, extensive analysis, and clear presentation. However, there are some concerns about the motivation, the original contributions, and the lack of explanation regarding the choice of pre-trained models.\n\nOne of the paper's significant strengths is its comprehensive analysis of GNNs for text classification, covering various GNN architectures and message-passing strategies. This thorough examination provides valuable insights into the applicability of GNNs in this domain. Additionally, the study's systematic comparison with other text classification methods enhances its overall quality.\n\nHowever, the paper faces some challenges. First, the motivation for the research and its original contributions should be clarified further. The paper appears to build upon existing works without explicitly stating its unique contributions to the field. Second, the choice of using the 'bert-uncased-base' model in experiments without exploring potentially superior pre-trained models needs justification. Third, while the study demonstrates the efficiency and effectiveness of GNNs for text classification, there is a need for more extensive performance evaluations on different text datasets to assess potential resource utilization issues. Lastly, the paper should provide a clearer rationale for why structural information captured by GNNs is beneficial for specific text classification tasks, such as sentiment analysis."
            }
        },
        "id": "KtAfJ6ZXFX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ro3x3mCAkD",
        "replyto": "Ro3x3mCAkD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4298/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586985,
        "cdate": 1696707586985,
        "tmdate": 1701465527097,
        "mdate": 1701465527097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper contributes a dataset containing fragments of Python programs annotated with natural language preconditions and postconditions.\n\nThis type of dataset is novel and fills a gap among other existing datasets.\n\nThe actual potential applications of this type of dataset are left unclear in the paper. \nThe dataset itself is also somewhat small, which restricts its usefulness, and lacks evaluation of annotation quality."
            }
        },
        "id": "QANQPn0Qla",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RndkyLWLHc",
        "replyto": "RndkyLWLHc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission134/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480065,
        "cdate": 1696707480065,
        "tmdate": 1701465388363,
        "mdate": 1701465388363,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers agree that the proposed method is simple and effective for improving NMT robustness in a cascade De|Fr-En speech-to-text translation scenario. The main concern that is not addressed by additional experiments promised in the rebuttals is the limited focus on only 2 high resource language pairs. While adding low resource language pairs would indeed strengthen the paper since it is likely to highlight the strengths of the proposed method, the reviewer agrees that this is beyond the scope of a short paper. However, the long paper format might have been a better fit."
            }
        },
        "id": "Zgk4uWJPsk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Rn1k3Na4Cn",
        "replyto": "Rn1k3Na4Cn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2832/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550391,
        "cdate": 1696707550391,
        "tmdate": 1701465478030,
        "mdate": 1701465478030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a way to quantify an intrinsic bias in the pre-trained model and shows the observation of a human-like bias in the pre-trained model and downstream as well. The authors addressed several questions and concerns from reviewers and showed potential evidence to strengthen the paper upon final acceptance."
            }
        },
        "id": "1mPbe6sPLC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RlPI6mERbr",
        "replyto": "RlPI6mERbr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission894/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498616,
        "cdate": 1696707498616,
        "tmdate": 1701465413928,
        "mdate": 1701465413928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores text classification in GPT-3, and introduces a method (CARP) to help models use superficial clues for better classification performance. The paper reports new SOTA performances in 4 of 5 chosen text-classification benchmarks.\n\nThe reviewers agree that the paper is clear and well-written, generally find the methods and experiments solid, and appreciate the demonstrated performance improvement.\n\nThe reviewers also raise concerns about the strength of the motivations and impact of the paper, pointing out that baseline model performance is already high on these tasks, and the improvements are fairly incremental. The reviewers would also prefer to see stronger support for the claims made about the reasons for poor model performance in these domains, (alongside stronger motivation and justification for the claim that this performance needs improvement in the first place)."
            }
        },
        "id": "GKmPr32XA1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RkqyZj5QNN",
        "replyto": "RkqyZj5QNN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission16/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476946,
        "cdate": 1696707476946,
        "tmdate": 1701465384182,
        "mdate": 1701465384182,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers found the paper to be strong/good across all metrics. All reviewers highlighted the novelty of the work. Reviewers were mixed on the quality of the writing. Two reviewers noted the paper lacks comparisons with other methods."
            }
        },
        "id": "bZCJtxJuoq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RgA1tcrxan",
        "replyto": "RgA1tcrxan",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5355/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611490,
        "cdate": 1696707611490,
        "tmdate": 1701465557178,
        "mdate": 1701465557178,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This submission studies a very real and practical problem of multimodal few shot entity recognition and handles the imbalance issue at entity-level with a newly proposed method. The soundness of the proposed method is validated by the original experiment results and the additional results provided during the rebuttal."
            }
        },
        "id": "74Y5pck264",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RenTc1sUb7",
        "replyto": "RenTc1sUb7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1175/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506634,
        "cdate": 1696707506634,
        "tmdate": 1701465422684,
        "mdate": 1701465422684,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper looks at the task of learning semi-structured sequences. They frame this as a task of learning  a key-value data structure.  They propose key-centric and record centric representations. They propose a temporal modeler (TVM) for the time-series, and KA (key aggregator) for key-centric representation, and further propose interleaved training to learn both representations. They evaluate and show good performance on 3 datasets.\n\nThe reviewers agree that the approach is exciting, the paper is well motivated, and ablations are persuasive. The reviewers also raise some clarifications on the evaluation and training. In particular,\n\n&nbsp; (1) the evaluation with the flattened encoders is still competitive and hence questions whether the premise of the key-centric and record-centric representations is even necessary. The authors have addressed this in their response.\n\n&nbsp; (2) there were also some clarifications requested regarding the interleaved training and the joint training.\n\nThe authors have responded to these questions. Perhaps there is still scope to clarify the writing, especially with regard to other possible baselines suggested by reviewers and how those might compare to the baselines in the paper. Incorporating the feedback from the reviewers should help improve the quality of the paper."
            }
        },
        "id": "x0OTIpOed3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ReGzwoL3Sl",
        "replyto": "ReGzwoL3Sl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1506/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515993,
        "cdate": 1696707515993,
        "tmdate": 1701465432810,
        "mdate": 1701465432810,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper tackles the problem of detecting multiple mental health disorders from social media posts, which is experimented with Reddit posts. The proposed method leverages two streams of learning to enable this detection.\n\nThe contribution is novel, achieving remarkable improvements over competitive baseline models on a challenging and important task.\n\nI appreciate the authors for the effort in preparing a solid rebuttal and I would likewise ask them to incorporate the new results reported in the rebuttals in a revised version of the paper."
            }
        },
        "id": "m7MYe8X65C",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RcvJnskt0n",
        "replyto": "RcvJnskt0n",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1376/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511466,
        "cdate": 1696707511466,
        "tmdate": 1701465428929,
        "mdate": 1701465428929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Almost all the reviewers have agreed that this is an interesting paper and has shown some good observations.\n\nHowever, concerns have been raised, though not that critical, on the lacking details and experimental designs.\n\nDuring the rebuttal period, the authors have partly addressed these concerns and only left some long-term concerns."
            }
        },
        "id": "QBSI2IP3oC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RbE83Pmtfk",
        "replyto": "RbE83Pmtfk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1249/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508498,
        "cdate": 1696707508498,
        "tmdate": 1701465425145,
        "mdate": 1701465425145,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores how robust language models are to missing input token characters. Their results are surprising: they find that language models retain high performance when pre-trained with extreme character loss. The reviewers agree that this is a novel research question, and, on the whole, that the methodology is sound. There is a concern about the reliability of the results given that only BERT is studied, but BERT remains a very commonly used model (particularly in computational humanities and social science), so I think this concern is outweighed by the computational costs of additional experiments, as acknowledged by the reviewer who raised this concern. I do agree that it is important to revise the text to clarify the scope of the findings (English)."
            }
        },
        "id": "0ZRAX4Duzl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ra6gfR3XuI",
        "replyto": "Ra6gfR3XuI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4290/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586745,
        "cdate": 1696707586745,
        "tmdate": 1701465526697,
        "mdate": 1701465526697,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates the energy cost of BERT during the fine-tuning process across various datasets and contextualizes this information in relation to pre-training costs. The reviewers praise the paper for the insightful empirical results and data points it provides on fine-tuning energy use. The reviewers highlight the potential value of these data points as a reference for the community, particularly in the context of hyperparameter optimization and assessing the overall cost associated with fine-tuning BERT models.\n\nHowever, reviewers have also raised some concerns regarding the paper's limitations, particularly its focus on specific hardware and models. This limitation could restrict the broader applicability of the study's findings as a community reference. While this is an important shortcoming, it might be partially addressed in the next iteration of the paper, e.g., by discussing the observed differences between the two employed GPU variants. For instance, a discussion might provide some guidance to the reader to what extent the empirical results and which aspects of the present study will remain relevant in the future. The next iteration of this paper should also clarify/address the concerns raised by the reviewers regarding the distinction of multi-GPU (for pre-training) and single-GPU (for fine-tuning) results, and the impact of this distinction on the present study."
            }
        },
        "id": "riJ5kd6O4V",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RYvNvCU109",
        "replyto": "RYvNvCU109",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4677/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595669,
        "cdate": 1696707595669,
        "tmdate": 1701465538221,
        "mdate": 1701465538221,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores improving unsupervised Chinese word segmentation by fine-tuning BERT to this task with pseudo-labels generated by an unsupervised segment model. The authors report that their approach yields improvements over prior state of the art on seven out of eight CWS datasets, while requiring significantly less training time. \n\nThe reviewers agree that the approach is simple, sound, competitive with prior work while requiring less time to train. The reviewers' initial concerns about how the work is distinct from or comparable to prior work seem to have been resolved with the author response. The most critical reviewer assigned strong soundness and excitement scores, which suggests that their criticisms were meant to be taken as suggestions for improvement rather than as reasons to reject."
            }
        },
        "id": "QG6vI5Tl0s",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RXIYmRUWGD",
        "replyto": "RXIYmRUWGD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3422/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562342,
        "cdate": 1696707562342,
        "tmdate": 1701465496866,
        "mdate": 1701465496866,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a quantization technique independent of training data. Reviewers see merit in the proposal, especially because it is the first data-independent quantization technique. Reviewers have some outstanding questions (primarily about outliers)."
            }
        },
        "id": "R1tz43Iwop",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RWJYEeaW1d",
        "replyto": "RWJYEeaW1d",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1353/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510903,
        "cdate": 1696707510903,
        "tmdate": 1701465428107,
        "mdate": 1701465428107,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a domain-agnostic self-refinement approach with a novel ranking metric to enhance the performance of open-source Large Language Models (LLMs) that evades external interferences. The paper also performs extensive evaluations \n\nPros:\n- The paper is solving an important and relevant problem\n- The ranking metrics are novels and comprehensive\n- Promising results on benchmark datasets across multiple tasks.\n- The authors have provided code\n\nCons:\n- Unclear description of experiment results \n- Bias in using GPT-4 as a evaluator\n- Empirical comparison to self-refine. The authors have compared to self-refine in introduction and related work but comparison to self-refine in experiments is missing"
            }
        },
        "id": "I7rfs1tD6T",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RWH1WazQqE",
        "replyto": "RWH1WazQqE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4895/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707600490,
        "cdate": 1696707600490,
        "tmdate": 1701465544177,
        "mdate": 1701465544177,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers found the paper well-written, easy to follow, and addressing an important problem. The reasons to reject are few and seem easy to address before camera ready. For example, publishing the source code could help readers grapple with the proposed method's complexity. During the author response period, the authors confirmed that they will publish the source code together with the paper."
            }
        },
        "id": "xKNqlOTKhe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RVQccn8rcr",
        "replyto": "RVQccn8rcr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1832/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528424,
        "cdate": 1696707528424,
        "tmdate": 1701465443829,
        "mdate": 1701465443829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces APrompt as a parameter-efficient fine-tuning method for large PLMs. Rather than modifying input layers, APrompt integrates query, key, and value prompts into the attention layer of the transformer. These prompts are learned alongside the original input prompts during model tuning. APrompt's effectiveness is demonstrated through comprehensive experiments on various SuperGLUE benchmark tasks, outperforming standard approaches across different model scales. Additionally, the study highlights that existing prompt tuning can be seen as a special case of attention prompt tuning, emphasizing the method's practicality and efficiency.\n\nAll reviewers engaged in extensive discussions with the authors and all agree the contribution of this work is strong. All reviewers agree that the results are reproducible and the excitement of this work is more than average."
            }
        },
        "id": "1swxS8D9rM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RSuN6p3wXR",
        "replyto": "RSuN6p3wXR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission6/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476538,
        "cdate": 1696707476538,
        "tmdate": 1701465383608,
        "mdate": 1701465383608,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors introduce a new resource for Chinese metaphor relation extraction with almost 10K annotated sentences, consisting of the addition of a new annotation layer to the metaphor dataset by Zhang et al. (2018). They annotate spans of text to identify the source and the target of the metaphor, and they add more fine-grained types for both the source spans (object, action, attribute etc.) and the relations.\nFinally, they propose a span-based end-to-end model to extract span pairs from sentences and detect if they are linked by a metaphor relation. Using the newly-annotated dataset, the system is compared with the relation extraction model by Eberts and Ulges (2020) on the span/relation extraction task and with MelBERT/MrBERT on token-level metaphor detection, showing improvements over the baselines in both scenarios.\n\nAmong the positives of this paper, the introduction of a new dataset for Chinese metaphor relation extraction and a fine-grained characterization of metaphors in the annotation scheme. On the downside, there is need of some more clarification about the annotation procedure (see discussion with R2) and about the experimental settings and the baseline choice (see discussion with R3)."
            }
        },
        "id": "4V2ZDMeeUN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RO460OVpev",
        "replyto": "RO460OVpev",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2312/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539254,
        "cdate": 1696707539254,
        "tmdate": 1701465460939,
        "mdate": 1701465460939,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper investigates how Vision and Language Models (VLMs) grasp spatial relationships. It undertakes a comprehensive examination of VLMs, evaluating various models. An analysis of the LAION dataset is presented, along with the introduction of several new corpora, namely COCO-prep from COCO, GQA-prep from GQA, and RealCLEVR. Despite the meticulous experimentations and introduction of new data to understand rare spatial relations, the results indicate that most VLMs are not performing exceptionally better than random thinking in spatial reasoning. Moreover, the study supplies insights but sometimes advances our understanding of why the models fail in this domain.\n\n**Reason To Accept**\n\n- Contributions: The paper's contributions are notable, especially with testing a significant number of VLMs (18 in total).\n- Data Introduced: The authors have integrated new data to evaluate rare spatial relations, which is commendable.\n- Clarity: The findings are clearly presented, detailed experimental descriptions support their conclusions, and the writing is clear.\n- Relevance: The issue of spatial understanding in models is crucial, and this paper's results shed light on existing shortcomings.\n\n**Reason To Reject**\n- Ambiguous Impact: Even after extensive exploration and analysis, the paper must significantly advance the understanding of why VLMs fail at spatial reasoning.\n - Literature Review: The paper needs to adequately differentiate its contributions from previous significant works in this area. Key works like Zhang et al. (NAACL2022), Mirzaee et al. (NAACL2021), and Singh et al. (ArXiv 2022) are insufficiently acknowledged.\n- Missing Details: The paper must explain essential details about human judgments on specific data points, such as annotator information, recruitment, and calculation methods.\n- Limited Insightful Discussion: The paper could benefit from a more in-depth discussion and analysis of the results, especially regarding the implications of the models' abilities concerning spatial prepositions.\n\n**Overall**\nThe paper contributes valuable insights into the domain of spatial relationships in VLMs. However, the reviewers have highlighted several concerns that limit its potential impact. To be considered for a top-tier conference, it would be essential to address the gaps in a literature review, provide more in-depth discussions on the results, and clarify ambiguities around."
            }
        },
        "id": "fXXIQ38YM8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RN5KLywTll",
        "replyto": "RN5KLywTll",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4104/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583222,
        "cdate": 1696707583222,
        "tmdate": 1701465520438,
        "mdate": 1701465520438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents the Iterative Bias-Aware Dataset Refinement (IBADR) framework, a new dataset-refinement method for debiasing NLU models. The authors introduce an iterative process involving a shallow model and a sample generator. The shallow model identifies biases in samples while the generator produces pseudo samples with fewer biased features in each iteration. Experimental results show that IBADR not only outperforms existing dataset refinement methods but also surpasses performance from model-centric approaches. However, potential weaknesses in this study include substantial data augmentation requirements and lack of comprehensive analysis surrounding the implemented sample generator.\n\nThe key novelty of this approach is its independence from predefined biased features or manual data analysis, which contributes to overcoming crucial limitations in contemporary dataset refinement approaches. Comprising a shallow model and a sample generator, the IBADR framework effectively identifies bias degrees of samples and generates pseudo samples with fewer biased features iteratively.\n\nSome reviewers pointed out that the paper falls short in providing an exhaustive analysis on the role and impact of the sample generator. They also pointed out the lack of significance testing, computational costs and the heavy reliance on augmented samples raising questions over scalability. The authors have offered adequate analysis in author response, which has been acknowledged by the reviewers to be satisfactory."
            }
        },
        "id": "BIny9rPqHO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RMDZNIjTt7",
        "replyto": "RMDZNIjTt7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3079/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555805,
        "cdate": 1696707555805,
        "tmdate": 1701465486538,
        "mdate": 1701465486538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents several strategies for automatically generating a dataset for training a human preference model. Reviewers found the methods well-motivated and effective. Reviewers also praised the thorough experiments covering multiple tasks, although the paper could benefit from further comparisons/analysis of the data generation strategies (reviewer s7CG) or discussion of alternative training methods (Dan7). Reviewer s7CG wanted to ensure the benefits of the strategies hold while controlling for dataset size and D9n7 suggested a few further experiments/numbers to report; the authors provided new results in their rebuttal which addressed the concerns. Although the intrinsic evaluation of the preference model itself is convincing, the only downstream evaluation on using it to improve LLM generation uses rejection sampling rather than RLHF. While this is a slight gap in the paper, I do agree with the authors’ point that RLHF is complex and potentially high-variance. A missing but relevant reference is Constitutional AI (https://arxiv.org/abs/2212.08073), which I encourage the authors to discuss in the paper. Overall I agree with reviewers that the method is solid and well-justified empirically."
            }
        },
        "id": "ggdFRgpHqx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RLmpJ4xol2",
        "replyto": "RLmpJ4xol2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5368/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611725,
        "cdate": 1696707611725,
        "tmdate": 1701465557551,
        "mdate": 1701465557551,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes using kNN-MT for low-resource languages. Most prominently, it combines representations from multiple different languages into the same datastore --- traditional kNN-MT uses huge datastores built for one language. The work contains extensive experimentation and reviewers are in consensus for acceptance."
            }
        },
        "id": "fIKT4OuANe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RKqtOoMC1M",
        "replyto": "RKqtOoMC1M",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1640/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521527,
        "cdate": 1696707521527,
        "tmdate": 1701465436984,
        "mdate": 1701465436984,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "All the reviewers agree that this method paper is well-written, works on a practical problem, and presents a convincing improvement for unseen domain adaptation given the extensive experimental setup (across domains, tasks, and languages). One common weakness of the paper are the motivation for the usage of hypernetworks, which manifests in different ways such as \"Why not consider disentanglement\" and \"Why hypernetworks help generalization?\"  In my opinion, I think that the authors have done a good job in the rebuttals in motivating the use of hypernetworks, explaining the advantages of their method, and showing the results comparing their proposed method and the disentanglement method mentioned by reviewer cySR. The performance gap between their proposed method and the existing methods such as PADA and MoE is quite impressive.\n\nWhile the reviewers raise other issues on the simplicity of the approach and the lack of other types of tasks and datasets, in my opinion, this paper has already demonstrated a promising approach to tackle a non-trivial task that they set out to explore and carried out extensive experiments including ablation study to verify the strength of their proposed approach. While it is undesirable that many findings are in the Appendix, such as the ablation and interpretability experiments, this paper is information-heavy and yet the authors have done a good job in articulating the settings and their important results. However, certain arguments in the paper could have been strengthened as suggested by reviewer 4JUK. The motivation for the proposed approach should also be detailed further in the revised version of the paper."
            }
        },
        "id": "wEHqwDugSu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RJq3hJlK6w",
        "replyto": "RJq3hJlK6w",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission456/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488255,
        "cdate": 1696707488255,
        "tmdate": 1701465399642,
        "mdate": 1701465399642,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agree that the paper is very interesting in that it uses diachronic semantic change models to investigate synchronic variation (in addition to presenting a new dataset).\nQualitative analysis is persuasive and the work is extremely well grounded in sociolinguistics. It is even more impressive for a short paper.\n\nThe questions posed by the reviewers got persuasive answers from the authors in the rebuttal phase. The authors should make sure to include the main points of these answers in the camera-ready version of the paper."
            }
        },
        "id": "ANEvSF3xCZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RJRCWXGtds",
        "replyto": "RJRCWXGtds",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission730/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494597,
        "cdate": 1696707494597,
        "tmdate": 1701465408511,
        "mdate": 1701465408511,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a 2-stage framework for harmful meme detection, leveraging the reasoning capabilities of Large Language Models (LLMs). Reviewers recognized this as an early exploration into using text-based LLMs for a multimodal task, praising the proposed two-stage training strategy involving knowledge distillation from LLMs and fine-tuning for specific tasks. However, reviewers expressed some concerns, which the authors addressed during the rebuttal stage and committed to resolving in the final version."
            }
        },
        "id": "h0efaVvo9F",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RGmQOhSGp0",
        "replyto": "RGmQOhSGp0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission500/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489322,
        "cdate": 1696707489322,
        "tmdate": 1701465401331,
        "mdate": 1701465401331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a method for conversational query production, where search queries are generated from interlocutor utterances during a dialogue. The method builds on previous works by introducing RAG and knowledge distillation techniques.\n\n**Pros**: After rebuttal, reviewers find the paper is well-written with most claims sufficiently defended by experiments. Most reviewers agree the paper is complete, with clear motivations and coverage of multiple languages.\n\n**Cons**: While most concerns on soundness are addressed during rebuttal, the reviewers discussion reveals some disagreement on perceived novelty of the work. It is noted the work will be useful to the community, but in terms of methodology, the work is perceived as mostly incremental.\n\nGiven the disagreement, I took a look into these details myself. There is some lack in methodological novelty, and perhaps, more of the novelty comes from the new evaluation setup (domain adaptation) which highlights the need for the method. Still, this aspect of the paper appears somewhat underdeveloped (limited domains, no domain adaptation specific generation baselines, etc.) with most of the papers' focus on the methods."
            }
        },
        "id": "zcsoWFdU7n",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RE4oyAdAvM",
        "replyto": "RE4oyAdAvM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2435/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542151,
        "cdate": 1696707542151,
        "tmdate": 1701465464980,
        "mdate": 1701465464980,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a collaborative framework for causality explanation generation where LLMs assume five roles: input processing, knowledge retrieval, and iterative answer refinement. The primary strength of the paper is the novelty of the framework, including bi-directional reasoning mechanism in the cause analyst and effect analyst. Indeed, such a collaborative framework can hold applications beyond causality in the future.\n\nSeveral concerns related to empirical evaluation were addressed during the rebuttal phase. Please include the new empirical results in the camera ready. Additional ablations would further strengthen the paper, e.g., if cause and effect analysts do not communicate, what will be the impact. \n\nIn summary, this is an interesting and novel framework, with good experiments and analysis (incl. those conducted during the rebuttal)."
            }
        },
        "id": "HZTh604d1L",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "RAtrnAtAsM",
        "replyto": "RAtrnAtAsM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission534/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490140,
        "cdate": 1696707490140,
        "tmdate": 1701465402504,
        "mdate": 1701465402504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose a new method for synthesizing inductive loop invariants in program verification, as well as an accompanying dataset for future advancements in this direction. Reviewers agree the approach offers new speed ups and analysis will be useful to practitioners facing this problem. Listed limitations/concerns from the reviewers mainly concentrate around discussion of how this work's contributions differ/advance prior work, and qualitative analysis of iRank."
            }
        },
        "id": "8z2eLx9MkI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "R7f5euZ9RA",
        "replyto": "R7f5euZ9RA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5266/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610109,
        "cdate": 1696707610109,
        "tmdate": 1701465554691,
        "mdate": 1701465554691,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper aims to enhance out-of-distribution (OOD) generalization in Visual Question Answering (VQA) by emphasizing causal reasoning through a novel construct, the Cognitive pathways VQA (CopVQA). This model promotes multimodal predictions and primarily works in two layers of cognitive pathways. The paper has garnered attention for its state-of-the-art (SOTA) performance on the PathVQA dataset and its innovative approach to introducing cognitive pathways into VQA. The novel two-layer reasoning pathway provides a fresh perspective on causal VQA research, especially among tons of LLM-related papers. This  work also provides a comprehensive qualitative analysis supporting the effectiveness of the two-layer reasoning pathway. Reviewers unanimously rated this work positively, but authors are encouraged to further improve the paper based on the suggestions given in the reviews."
            }
        },
        "id": "OLRHVeXXsr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "R7Op9CHdPz",
        "replyto": "R7Op9CHdPz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2574/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545088,
        "cdate": 1696707545088,
        "tmdate": 1701465469555,
        "mdate": 1701465469555,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a mechanism for iterative reading then reasoning on large language models over structured data sources. A number of data source formats are considered, including SQL, tables and KG data. The method exploits the table / data structure as an interface  or type hint for information extraction and uses this information to return information from a linearized version of the data. \n\nThere is reasonable consistency with reviews, stating that the methodology is sound and has reasonable excitement. The method is dependent on the underlying LM. However, the methods in the paper describe an approach that should be agnostic to the underlying LM provider. It would be advantageous to also provide further experiments with open source LLMs as part of the final version. The disadvantage of only relying on OpenAI's implementation is (as demonstrated by the author's rebuttal), the final answer accuracy can be heavily dependent on the version of the model. \n\nThe reviewers identified several reasons to reject and spotted some useful areas that were missing in the paper, including the representation of the task as generating an intermediate query language. I would further suggest linking this to work in multi-step QA: for example the BREAK task: https://arxiv.org/abs/2001.11770 - regarding the point made by the reviewer about open source toolkits such as langchain, i believe that a module released by the authors of by this paper would be of significant value for the community and i would encourage the authors to release a their code."
            }
        },
        "id": "NBZsRzv72O",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "R635gF7lXD",
        "replyto": "R635gF7lXD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission970/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500557,
        "cdate": 1696707500557,
        "tmdate": 1701465416430,
        "mdate": 1701465416430,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper makes an important contribution by rigorously evaluating the performance of language models on a complex legal reasoning classification task. The key technical innovation is a high-quality dataset annotated by legal experts categorizing passages of US Supreme Court opinions as either formal/textualist or grand/purposivist interpretive reasoning. This granular labelling required extensive domain knowledge and deliberation, even for human annotators.\n\nThe core experiments compare several model architectures, including BERT, DistilBERT, GPT-3, and FLAN-T5. The striking finding is that most models struggle to match human performance levels absent fine-tuning on this in-domain labelled data. Even a strong BERT model fine-tuned on the annotations reaches only ~80% accuracy, indicating inherent challenges in this reasoning style classification task.\n\nIn general, reviewers praise the novelty of the dataset and approach, but identify areas for improvement:\n\nAdditional SOTA models could be tested to comprehensively benchmark performance. The authors sufficiently address this by including results for models like LLaMA.\nConnections could be drawn to the literal vs. purposive interpretation distinction used in some legal frameworks. Of course, I note that the authors already indicate they will incorporate such parallels to situate the conceptual framing.\nLastly, further analysis into changes in court membership vs. judicial philosophy over time was suggested. \n\nI agree with the reviewers that the key contribution here is the introduction of the dataset. However, the task appears to me to be quite trivial - a classification task with 3 labels! Tasks like these are already well solved even in the legal AI field. Moreover, I find it surprising that a finetuned version of a BERT variant, i.e., LEGAL-BERT, even though confers some domain specificity will be able to match the performance of recent LLMs. I am not convinced and I think the authors need to do more exploratory studies with proper prompting techniques - be it in-context or few/zero-shot learning employing newer/larger models."
            }
        },
        "id": "qM1ySMgLC8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "R5NzXYY7S2",
        "replyto": "R5NzXYY7S2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2074/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533759,
        "cdate": 1696707533759,
        "tmdate": 1701465452871,
        "mdate": 1701465452871,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces Model-tuning via Prompts (MVP) as a novel approach to enhance adversarial robustness in NLP models. MVP involves fine-tuning a model on prompts and ensemble results during inference, demonstrating superior robustness without accuracy loss on various datasets. Reviewers found the idea is interesting and the experiments are strong. They also raise concerns about limited evaluations on simple tasks, the need for a comparison with LPFT, and the scalability of MVP. Overall, this paper is of high quality and I therefore recommend acceptance."
            }
        },
        "id": "xgT37f7GHh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "R4yb4m7Nus",
        "replyto": "R4yb4m7Nus",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5093/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606893,
        "cdate": 1696707606893,
        "tmdate": 1701465549841,
        "mdate": 1701465549841,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Quality:\n\nThe paper presents high-quality results, which is noted by the reviewers as follows:\n\n-\t“The proposed model outperforms existing state-of-the-art models by quite a margin setting a new standard for state-of-the-art models.”\n\n-\t“The experiments and the quality analysis of the results are thorough.”\n\n-\t“Experimental work presented in the paper is technically sound and detailed - model performance is compared to multiple strong baselines, several ablation studies give a good understanding of the impact of individual model components, impact of different modalities, impact of gender, and the effectiveness of the proposed Speech-Gesture Graph Encoder. In addition to this quantitative analysis, the qualitative analysis is performed on four representative cases involving different types of aphasia. All the claims made in this work are strongly supported by empirical evidences.”\n\nClarity:\n\nThe paper is clear and easy to understand: “This is a well-written paper that presents a sound work with some interesting and useful contributions to the NLP community. The proposed model is explained in sufficient details and the corresponding code is promised to be shared with the community.”\n\nOriginality:\n\nThe work is highly original in that it uses a GNN model with multimodal information from speech and gesture data to detect different types of aphasia and provides evidence on the importance of acoustic features.\n\nSignificance:\n\nFinally, the work is significant in that it presents a multimodal study on aphasia detection, making a contribution towards a holistic approach towards detecting the condition.\n\nThe rebuttal provided by the authors in response to the questions of the reviewers is substantial."
            }
        },
        "id": "P8iUBSGNVN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "R4VfYDluYi",
        "replyto": "R4VfYDluYi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission729/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494518,
        "cdate": 1696707494518,
        "tmdate": 1701465408478,
        "mdate": 1701465408478,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The proposed method (STINMatch) is a semi-supervised learning framework that uses semantic information and topological association (via a News-Enterprise Knowledge Graph aka NEKG) to evaluate the propagation of news risk and enterprise risk.\n\nThis approach is really interesting although some concerns are mainly related:\n1) The task definition is not properly defined; specifically more relevant definitions may support the readers in understanding the task\n2) Some basic information about the data source is not disclosed.\n3) Key challenges need to be more properly defined\n4) More clarifications about the framework need to be provided (also improving Figure 1)."
            }
        },
        "id": "HLspMvhPhB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "R4N3RNBNzJ",
        "replyto": "R4N3RNBNzJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1052/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503862,
        "cdate": 1696707503862,
        "tmdate": 1701465418941,
        "mdate": 1701465418941,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This submission proposes to exploit WordNet information to perform graded lexical entailment, showing that a WordNet-based approach reaches high performance on the HyperLex dataset. \n\nWhile the reviewers appear to diverge quite a bit on their soundness/excitement scores, unfortunately one reviewer failed to perform their duties in acknowledging the author rebuttal, while the authors had raised fair points regarding their review.\n\nThe more positive leaning reviewers mention mainly textual aspects, e.g. regarding the somewhat bold title of the paper."
            }
        },
        "id": "ZTDHnhaBY2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "R2PwXB08i4",
        "replyto": "R2PwXB08i4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1528/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516507,
        "cdate": 1696707516507,
        "tmdate": 1701465433564,
        "mdate": 1701465433564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work provides notable improvements in performance by exploring an interesting question around “knowledge corpus error”. Reviewers expressed concerns about the impression that the method is augmenting “external knowledge” (presumably from your use of “passages outside the knowledge corpus”). We strongly recommend this wording is adjusted and clarified to bring the intended claims in line with the reviewers’ perceived claims. Additional analysis into why and how this works, as suggested by reviewer bwTs would also greatly strengthen this work. However, we do believe overall this set of experiments yield interesting and noteworthy results for a short paper."
            }
        },
        "id": "hsRwWNGJ1b",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "R0XABYPVKI",
        "replyto": "R0XABYPVKI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5351/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611438,
        "cdate": 1696707611438,
        "tmdate": 1701465557061,
        "mdate": 1701465557061,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper compares different sampling strategies for MBR decoding, including a number of well-known approaches such as top-k and nucleus sampling, and the more obscure epsilon sampling, originally introduced by Hewitt et al., but dismissed as having a \"key failure\" in high entropy distributions. The main contribution of the paper is showing that in the constrained setting of MT, epsilon sampling is an effective sampling strategy, and in combination with MBR decoding, performs better than other strategies tested in human evaluation.\n\nReviewers agree that empirical insight by the paper are valuable, and potentially have practical impacts (currently limited by the ineffiency of MBR). There is more disagreement on the more subjective excitement score. On the one extreme, reviewer YyrA finds that repurposing epsilon sampling for MT is an incremental contribution. On the other extreme, reviewer FrLm gives the paper credit for \"proposing the epsilon sampling approach\", and b3eK for \"bring[ing] up the MBR decoding to the community\".\n\nFactually, the view of YyrA is more precise, since epsilon sampling is not introduced in this paper, nor can the paper be accepted on the basis of bringing MBR to the community, since it follows a number of other papers on the issue. Still, given the previous obscurity of epsilon sampling, which was even dismissed in the paper introducing it, the empirical findings in this paper are significant and I expect it to have some impact on the field."
            }
        },
        "id": "hVdURBg16L",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Qyw4k4ohgr",
        "replyto": "Qyw4k4ohgr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission819/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496779,
        "cdate": 1696707496779,
        "tmdate": 1701465411635,
        "mdate": 1701465411635,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Reject"
            },
            "comment": {
                "value": "The paper presents a human evaluation of GPT3.5 translating paragraphs/documents of literature across 18 language pairs. The outputs are compared against Google Translate which only works at sentence level. Analysis with MQMs shows interesting findings. Most of the work (350h) was done by professional translators. Annotated data is released. \n\nPros: This is an interesting analysis that complements previous archival papers and the released annotated data could be useful for future work. Some of the results are good to know. The paper is clearly written and organized. \n\nCons:  I see a limited scientific contribution with respect to the standards of EMNLP.  1) The type of analysis is not novel and the experimental set up is not scientifically sound:  results are not reproducible, no control of major variables like training data of compared systems, which makes this not an “apple with apple” comparison). The compared systems are black boxes that have been trained independently on data that is not disclosed. \n2) The work does not make efforts to improve the status quo, such are improving the design of scientific experiments with third-party LLMs.  As suggested by one reviewer, a better experiment would have been that of comparing document vs. single sentence with one fine-tuned publicly available LLM. \n\nIn conclusion, I don’t find   this contribution scientifically very solid and thus not appropriate for a venue like EMNLP. Still, it contains many good-to-know results, that could be valuable for a workshop on translation quality analysis, although the paper focuses on just a single case study."
            }
        },
        "id": "i7sDEVivBR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QwejPcX96r",
        "replyto": "QwejPcX96r",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission1030/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503243,
        "cdate": 1696707503243,
        "tmdate": 1701465418246,
        "mdate": 1701465418246,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper examines how linguistic variations in prompts affect the performance of large language models (LLMs) on various classification tasks. The paper tests different LLMs, including instruction-tuned LLMs, on datasets from GLUE and SuperGLUE. The paper manipulates the prompts by changing their grammatical properties (such as mood, tense, and modality) and lexical properties (such as synonyms). The paper finds that the prompts are unstable and sensitive to these variations, and that the performance varies significantly across datasets and LLMs. The paper provides some empirical suggestions for future prompt-related research. The paper elaborates upon the prompt variations aptly, is well written and definitely has merits for downstream use-cases, the scope limited to classification based tasks only for a long paper."
            }
        },
        "id": "ToirguX90r",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Qv2CTIcCPJ",
        "replyto": "Qv2CTIcCPJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission202/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481736,
        "cdate": 1696707481736,
        "tmdate": 1701465390939,
        "mdate": 1701465390939,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This manuscript examines how toxicity detection methods impact marginalized communities using outlier detection. The reviewers are generally positive towards the manuscript, for instance, they note that 1) the manuscript is well written and organized, 2) the method presented is well tested, and 3) the experiments are useful towards grounding the point. Moreover, reviewers highlight that 4) the background research of the paper as particularly well done.\n\nIn terms of weaknesses, reviewers ask that a) the authors describe the dataset in more detail, b) the method is not novel, c) experimental details are missing, d) limited claims (due to single model tested).\n\nIn response to a) and c) the authors provide further details in their author response, which should be included in the manuscript. Responding to d) the authors show similar results for two additional models - which should similarly be included and discussed. In response to b) the authors highlight that while neither method nor data are new, they apply them together to identify a novel method for identifying harms to demographic groups, i.e. the use case and findings are novel.\n\nThe ethics reviewers ask that the authors address the issue of dual use of their method to identify outliers with malicious goals (i.e., to cause harm).\n\nI would ask that the authors include a consideration on “norm” in their limitations, particularly what norm means and what the impact of assigning communities to be outside of the norm has as implications.\n\nSome reference issues/suggestions.\nOn normalcy, Lepawsky, 2019 (https://discardstudies.com/2019/09/23/no-insides-on-the-outsides/) and Haraway (Situated Knowledges) of which speak to normalcy and positionality w/ Lepawsky specifically talking about content moderation and toxicity.\nThylstrup and Talat (Dirt and Toxicity) speak to norms in data (and subjectivity) and harms in the context of content moderation.\nKearns et al. 2018 (Fariness Gerrymandering) is useful reference for identifying subgroups and the issues that arise computationally.\nRather than Crawford 2017, the better reference there would be Butler’s Excitable Speech, which deals directly with the issue of harms arising from recognition (or to be identified) - particularly the introduction and chapter 1 would be useful for the authors."
            }
        },
        "id": "ZhgbcNbLwN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Qu0OZXL29t",
        "replyto": "Qu0OZXL29t",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5132/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707607684,
        "cdate": 1696707607684,
        "tmdate": 1701465550863,
        "mdate": 1701465550863,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper examines 3 debiasing techniques for neural networks. They find that the techniques indeed result in models that rely less on the biases aimed to be removed, but may cause them to instead start relying on other unknown and unidentified biases, rather than helping it robustly solve the task.\n\nThe reviewers are generally in agreement that the question as well of the results are interesting. Both BvPV and FLhX point out that the synthetic nature of the used data takes away a bit from the strength of the results. I do not find that the authors succesfully rebut this concern (and neither does BTLE). Nevertheless, I don't think this should stand in the way of acceptance. There is always a trade-off between using realistic data and synthetic/toy data in terms of the conclusions that can be drawn, and I think the paper presents a valuable contribution as is."
            }
        },
        "id": "amdDiWxrWA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QtyJZe9Sfz",
        "replyto": "QtyJZe9Sfz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5044/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605787,
        "cdate": 1696707605787,
        "tmdate": 1701465548390,
        "mdate": 1701465548390,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new method to synergize retrieval and generation iteratively by using a response from a previous iteration at a retrieval step to refine the output. Evaluation on 4 multi-hope QA datasets, 1 fact verification and 1 commonsense task show consistent improvements.\n\nReviewers agreed that the method is simple yet effective (vbix, yjLF, ij2Q), and acknowledged that experiments are comprehensive, along with a new, more reliable metric proposed (vbix, yjLF, ij2Q) and insightful analysis (yjLF).\n\nHowever, two reviewers pointed out limited novelty (yjLF, ij2Q). Also, reviewers mentioned how the method solves the issues with previous method mentioned in the paper—failing to process all retrieved knowledge and needing to use retrieval multiple times—is unclear (vbix, yjLF). Authors are encouraged to revise the paper to address this concern in the next version of the paper."
            }
        },
        "id": "kxfwuFRs3r",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QtOybganmT",
        "replyto": "QtOybganmT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1433/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707513440,
        "cdate": 1696707513440,
        "tmdate": 1701465430919,
        "mdate": 1701465430919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the problem of generating descriptions for perceptual concepts beyond instance representations. It evaluates communicative success by performing zero-shot classification with generated class descriptions. The proposed evaluation setup allows for fine-grained analysis of class description generation and interpretation separately. This is a very well-motivated and compelling setting for evaluating language grounding and communication, and it opens up the possibility of studying a variety of new questions. One point which reviewers did not make but I (as AC) am curious about is about human evaluation of generated class descriptions (i.e., replacing IPT with a human \"listener\"). Given the confusion regarding the paper's contribution (i.e., in contrast to contributing a method for the existing task of zero-shot image classification), I would suggest clarifying this point e.g. in the introduction. I also agree that additional experiments and analysis (e.g. on additional datasets) would further strengthen the contribution."
            }
        },
        "id": "WHX2PSxmbt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QoiOmXy3A7",
        "replyto": "QoiOmXy3A7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1456/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707514694,
        "cdate": 1696707514694,
        "tmdate": 1701465431459,
        "mdate": 1701465431459,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes a method for compressing Transformer model parameters by leveraging matrix decomposition. Basic idea is to dynamically tracks the importance of parameters and adjust the rank budget to derive a model in fine tuning.\n\nStrengths\n* The proposed method is simple yet well-designed by capturing the changes in the importance associated with singular groups.\n* Experiments on GLUE show better performance under higher compression rates with detail analysis on the impact of importance score.\n\nWeaknesses\n* Experiments are limited to BERT and it has no experiment on other variations. It is not clear whether the proposed approach is also effective for the text generation tasks, e.g., MT.\n* Novelty might be limited given the prior work on FWSVD/TFWSVD, i.e., a static variant."
            }
        },
        "id": "qiqtnDNIq5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QnXfnQ3MFe",
        "replyto": "QnXfnQ3MFe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4368/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588371,
        "cdate": 1696707588371,
        "tmdate": 1701465529467,
        "mdate": 1701465529467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers all found this paper sound, with extensive evaluation and a solid experimental setup, and most found it exciting. They noted that the insights from the paper will be beneficial to the community and that the paper is well-grounded in the existing literature on the topic while building upon it. The main reasons to reject had to do with style, formatting, and discussions that could be expanded upon, rather than things like the experimental setup or conclusions being made. One drawback brought up related to using LLMs to evaluate topics in cases where domain expertise is needed, is valid, but is already addressed somewhat by the limitations section. The authors also commit to expanding on this. I do not see this as a major methodological flaw (and I believe the reviewer agrees, given their scores) but rather something to be discussed more in the paper, if accepted."
            }
        },
        "id": "s5eXJqmvnq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QlY0TSxVIl",
        "replyto": "QlY0TSxVIl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1499/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515810,
        "cdate": 1696707515810,
        "tmdate": 1701465432560,
        "mdate": 1701465432560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper looks at the challenging problem of converting speech so that it appears to be produced by a speaker with a different accent from the source. The paper looks specifically at pitch and rhythm modelling. The reviewers agreed that the good performance of the proposed approach to a relevant task was a strength. But they also all highlighted that parts of the paper were difficult to follow, especially with regard to missing technical details. Although the authors provided follow-up explanations in the rebuttal period, this did not alter the reviewers' opinion regarding the clarity. Nevertheless, despite ambivalent excitement, the reviewers seem to agree that the work is generally sound."
            }
        },
        "id": "lElWJKefyf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QkCYv3TlGk",
        "replyto": "QkCYv3TlGk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2281/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538569,
        "cdate": 1696707538569,
        "tmdate": 1701465459889,
        "mdate": 1701465459889,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates the challenges of compositional generalization in Data-to-Text models, particularly in relation to predicate structures. The authors propose a new methodology that begins with sentence planning, whereby input predicates are clustered (each cluster mapping to a subsequent sentence), followed by text generation. The approach is evaluated using predicate structure-controlled instances from the WebNLG benchmark. The experimental results indicate that decomposing the predicate structure into smaller units improves the faithfulness of the output and enhances the model's domain adaptability.\n\nMain Contributions:\n\nThe authors present a novel evaluation methodology centered on predicate configurations, providing a robust benchmark for future research.\nThe paper introduces multiple predictors for predicate decomposition, including numerical-based, neural network-driven, and those enhanced by REINFORCE. A random predictor also serves as a baseline.\nThe authors propose an innovative framework (predicate decomposition) which demonstrates improved performance compared to T5.\nThe paper provides a comprehensive analysis of the experiment results, highlighting the nuances and broader implications of the compositional generalization methods.\nThe authors create a benchmark for assessing compositional generalization, few-shot learning, and domain adaptation, which can be useful in this field.\nReasons for Acceptance:\n\nThe paper is well-written and clearly explains the development and implications of their cluster-based compositional generalization method.\nThe proposed method requires smaller training sets and does not require additional labeled or unlabeled data.\nTheir approach performs well when evaluated against T5 baselines, and the method of evaluation and discussion of results is clear and relevant.\nThe introduction of an evaluation method and a novel approach to address compositional generalization fills a gap in the current research.\nReasons for Rejection:\n\nThe study is limited to a single model size, leaving questions about the method's efficacy in bridging compositional generalization gaps in larger models.\nThe proposed method seems prone to repetition issues, particularly in WebNLG content that spans an average of 1 to 2 sentences, and effective solutions to this concern aren't elaborated upon.\nThe CG-Random performs quite well in many scenarios, raising questions about the necessity and effectiveness of the complex predicate composition process."
            }
        },
        "id": "rFsXLfFPmQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QdhjuI19nv",
        "replyto": "QdhjuI19nv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2193/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536615,
        "cdate": 1696707536615,
        "tmdate": 1701465457333,
        "mdate": 1701465457333,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper delves into the mechanism of in-context learning (ICL) within LLMs and proposes a novel approach that involves compressing training samples into a task vector and using it to modulate a transformer for handling queries. Extensive experiments using various LLMs support this concept.\n\nThe reviewers collectively acknowledge the empirical insights gained from the study and agree on the potential value of this work for researchers seeking to understand ICL. Given the average scores being above the acceptance borderline and considering the valuable insights provided by the paper, the decision should lean towards acceptance. However, the reviewers highlight the need for further theoretical analyses, deeper insights, missing references and some presentation issues. It's recommended to encourage the authors to address some of these concerns in a revision as promised in the discussion, to strengthen the overall impact and quality of the paper."
            }
        },
        "id": "jXETnZE3Fn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QYvFUlF19n",
        "replyto": "QYvFUlF19n",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5764/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618195,
        "cdate": 1696707618195,
        "tmdate": 1701465566652,
        "mdate": 1701465566652,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers agreed that the core contribution of this paper - the new dataset and novel tasks presented regarding the characteristics of empowering language - was strong and compelling. The CSS analyses of this data presented in the paper were characterized by reviewers as \"deep\" and \"comprehensive,\" providing \"valuable insights for future research.\"\n\nReviewers presented some issues for requiring clarification and noted areas of possible future work (such as addressing other aspects of social context beyond gender, multi-turn dialogue, comparisons with TalkDown, etc); authors responded thoroughly to reviewer concerns and noted ongoing future work that addresses these considerations, ultimately resulting in reviewer consensus that the work is sound and merits some excitement.\n\nOverall reviewers found that this work makes a meaningful contribution worthy of dissemination."
            }
        },
        "id": "FoNiMJpzgq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QYSPlIZ6bV",
        "replyto": "QYSPlIZ6bV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5211/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707609179,
        "cdate": 1696707609179,
        "tmdate": 1701465553158,
        "mdate": 1701465553158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work introduces a new framework, UniMMQA for multimodal QA (text, table, image), which unifies multimodal input into a text-to-text format by using table linearization and diversified image captioning techniques. On multimodalQA datasets, the proposed method outperforms prior approaches. While the method is solid and the experimental results are strong, I agree with the reviewer smpP on limited novelty---according to the author's response, the main technical novelties are combining TopP and TopK sampling at the decoding time and exploring different ways of linearizing table inputs, which sounds incremental to me as well."
            }
        },
        "id": "gngAqWjbgs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QVuVwt1QLh",
        "replyto": "QVuVwt1QLh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5039/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605708,
        "cdate": 1696707605708,
        "tmdate": 1701465548263,
        "mdate": 1701465548263,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose a new Chinese dataset for stance detection, debate summarization, and stance-specific debate summarization for argumentative dialogues. The dataset comes with a set of baseline methods to address the task; They find that summarization performance is relatively low, while stance detection is found to be a relatively easy task.\nThe reviewers agree that the paper is unique in that it proposes the first Chinese dataset for these tasks, and the modelling experiments provide a good set of baselines on the tasks. One reviewer also highlights the quality of the related work discussion, while another misses a more detailed description of the dataset collection process (this aspect can be addressed easily enough in the final version of the paper, judging from the author rebuttal). The evaluation contains both automatic metrics and human judgments."
            }
        },
        "id": "0p3OtXX5FS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QVnlBmGrWS",
        "replyto": "QVnlBmGrWS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission709/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494201,
        "cdate": 1696707494201,
        "tmdate": 1701465408054,
        "mdate": 1701465408054,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper examines MoE models from an empirical and theoretical perspective, aimed at understanding the value of trainable vs fixed routers and providing some theoretical results for relatively simple mixture of gaussian settings.\n\nPros\n\nIt provides theoretical analysis and empirical evidence on the advantages of learning to route in Mixture-of-Experts (MoE) models\n\n It establishes the advantages of these models beyond mere scalability, which can contribute to the theoretical foundation of the field.\n\nThrough experiments on synthetic and real data, the paper gives evidence that MoE routers can learn to route inputs intelligently based on latent structure in the data and that learning to route yields a significant boost over fixed routing schemes.\n\nCons\n\nMost of the experiments are on synthetic data. It would be more impactful to show its practical usefulness in real-world complex datasets \n\nThe authors have done empirical analysis using quite small models (T5-base). In the current dynamics of this space, in order to make this paper really relevant it would be good to understand whether their findings hold true for large enough models.  While for the theoretical part i can understand simplifying the model architecture, but for the empirical analysis its important to show its generalization to larger LLMs. \n\nPresentation of the paper and writing style needs to be improved so that the motivation and the exact problem setup and discussion of the results is clear to readers, especially those who may not be very familiar with the topic"
            }
        },
        "id": "gRRdQnhLn0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QV79qiKAjD",
        "replyto": "QV79qiKAjD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1124/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505476,
        "cdate": 1696707505476,
        "tmdate": 1701465421167,
        "mdate": 1701465421167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The submission proposes data augmentation techniques to improve performance on lexical simplification in an unsupervised way. The reviewers converge towards the view that the current work is exciting; while the proposal is relatively incremental, it is a simple and effective way to improve on previous results."
            }
        },
        "id": "WVB1bjUosW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QPP8wNMBBk",
        "replyto": "QPP8wNMBBk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1535/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707516823,
        "cdate": 1696707516823,
        "tmdate": 1701465433723,
        "mdate": 1701465433723,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a new dataset for multilingual, multifaceted summarisation evaluation, consisting of 96,000 summaries with human ratings that relate to six quality dimensions (comprehensibility, repetition, grammar, attribution, main ideas, conciseness). The resource covers six languages and nine \"systems\" (one of which is a human-created summary).\n\nThe reviewers were unanimous in that the proposed dataset will be a very valuable contribution to the community, thanks to its size, multilingual coverage and rich annotation. The paper also appears to be clearly written and easy to follow. The reviewers only identified minor issues with this paper, some of which could be addressed in the rebuttal."
            }
        },
        "id": "mGfPxVntXZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QMCjppVJbB",
        "replyto": "QMCjppVJbB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3755/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707573981,
        "cdate": 1696707573981,
        "tmdate": 1701465508818,
        "mdate": 1701465508818,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes Query2doc to generate a potential relevant passage with LLMs and uses it to expand the query. The experiments show the effectiveness of the approach.\nThe experiments confirm the usefulness of LLMs for improving ad hoc IR in this way. It can inspire other researchers to work in a similar direction in the future."
            }
        },
        "id": "vu4blnJLFn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QH4EMvwF8I",
        "replyto": "QH4EMvwF8I",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1284/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509355,
        "cdate": 1696707509355,
        "tmdate": 1701465426147,
        "mdate": 1701465426147,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents multilingual LongT5 (mLongT5), a multilingual version of LongT5 which accepts longer context length than other Transformer-based models and reports experimental results in summarization and QA tasks. Overall, the reviewers found that the paper presents interesting observations and promising experimental results with mLongT5. However, a few points are raised by the reviewers such as Russian summarization results on the MLSUM dataset are worse than multilingual BERT without detailed analysis and using only ROUGE scores for summarization. Based on the reviews, the AC strongly recommends the authors to 1) add analysis on settings where the mLongT5 falls short when compared to mT5 or multilingual BERT, and 2) add at least one model-based evaluation metric since ROUGE scores are measuring only one aspect of generated summaries."
            }
        },
        "id": "j2PSaVSoFC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QH19wfJrX1",
        "replyto": "QH19wfJrX1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission209/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481865,
        "cdate": 1696707481865,
        "tmdate": 1701465391115,
        "mdate": 1701465391115,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The submission deals with language model pre-training. \n\nThe reviewers have praised the paper's approach to the task in hand as computationally efficient but they have also identified a number of shortcomings described in detail in the reviews."
            }
        },
        "id": "AdX6TqJcO4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QG4BWnsX6m",
        "replyto": "QG4BWnsX6m",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5627/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616115,
        "cdate": 1696707616115,
        "tmdate": 1701465564062,
        "mdate": 1701465564062,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The manuscript proposes a novel idea of jointly encoding audio and visual signals. Not treating audio and video separately using joint encoding and CSL loss authors shown that the proposed approach improves on SoTA in a benchmark dataset. While mainly one benchmark available (excluding recently published dataset), this raises the question of generalizability of the approach. Some errors in technical presentation made the understanding of details harder which is very important when limited dataset available. Reviewers and authors have engaged in discussion to clarify these concerns. The final manuscript should have these discussion and corrections incorporated in the text.\n\n\nA version of this manuscript appeared online before anonymity period, but the EMNLP policies indicates that authors should notify PCs about existence of such non-anonymized versions. Unfortunately, this policy is not followed and resulted in a partially non-anonymized review."
            }
        },
        "id": "SgACzZs1a0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QEf1MyZGZu",
        "replyto": "QEf1MyZGZu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4968/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602996,
        "cdate": 1696707602996,
        "tmdate": 1701465545993,
        "mdate": 1701465545993,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes KG-GPT,  a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions. The sentence segmentation step partitions a sentence into discrete sub-sentences, each aligned with a single triple.  The graph retrieval step retrieves a potential pool of relations within the sub-sentences. In the final step, inference obtains graphs used to derive a logical conclusion, such as validating a given claim or answering a given question. Meanwhile, this paper evaluates KG-GPT using KG-based fact verification and KGQA benchmarks, which fills the gap in complex reasoning on knowledge graphs for LLMs."
            }
        },
        "id": "ME66i0RbkB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QAZ2QV8SqN",
        "replyto": "QAZ2QV8SqN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5059/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606139,
        "cdate": 1696707606139,
        "tmdate": 1701465548800,
        "mdate": 1701465548800,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper focuses on improving the cross-lingual reasoning of multilingual language models (here XLM-R). \n\nIt introduces a learnable query layer (as part of the self-attention layers) trained specifically on code-mixed data to help the model to reason on context in one language to answer a question in another language. \nThe method leads to better cross-lingual performance compared to baselines. \n\nReasons to Accept: \n- New empirical evidence that adding adapter-like parameters as the query in the self-attention and explicitly trained in the cross-lingual setting helps the cross-lingual reasoning abilities of the MLMs.  (R1, R2, R3)\n\nReasons to Reject: \n- Lack of mention, discussion, and comparison with related methods such as adapters (Pfeiffer et al. 2020)\n- Running the experiments exclusively on machine-translated may lead to noisy results (R3). \n- Cross-lingual reasoning defined in the paper is a synthetic task that requires further discussion and comparison with existing cross-lingual tasks (R1 and R3)"
            }
        },
        "id": "WSpw675iFE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QAT5suGpNL",
        "replyto": "QAT5suGpNL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3966/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580187,
        "cdate": 1696707580187,
        "tmdate": 1701465515577,
        "mdate": 1701465515577,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper titled \"Continual Instruction Tuning: A Benchmark Study\" presents a benchmark for the continual instruction tuning (CIT) problem. The benchmark consists of two long task streams constructed from the SuperNI dataset, and the authors propose an evaluation protocol to assess performance on old, upcoming, and unseen tasks. The experiments and analysis on several well-studied continual learning (CL) methods reveal an unexpected effectiveness of the simple fine-tuning method over other CL methods.\nOverall, the reviewers think that the authors design a useful evaluation protocol to evaluate on both the test set of the pretrained and finetuned stages as well as the unseen test set. However, the paper lacks detailed analysis on task characteristics such as the instruction style, negative and positive ratio, and it only evaluates on a small model, and it is challenging to confidently assert that the current observations are applicable for larger models."
            }
        },
        "id": "SKpTZjOoaF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "QA1jlb1VG7",
        "replyto": "QA1jlb1VG7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission955/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500198,
        "cdate": 1696707500198,
        "tmdate": 1701465415983,
        "mdate": 1701465415983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree that the method proposed in the paper is interesting and novel. They find the paper well written and easy to follow. Two of the reviewers seem to be concerned by the lack of ablation tests/clear indication of what information has an impact on the results. The authors' answer indicates that they can provide at least a preliminary analysis of this, should the paper be accepted. The reviewers ask a few other questions related to methodology/technical details, which are successfully answered by the authors. \n\nThe scores of one of the reviewers seem to be rather low given the wording of their review. The questions and objections of the reviewer are addressed in the authors' rebuttal. The reviewer seems to be reasonably satisfied with the answer, so I assume the reviewer forgot to increase the scores. \n\nOverall, the reviews (and partially the scores) indicate that this is a sound paper which contains some novel ideas. The discussion during the rebuttal period will enable the authors to address some of the problems identified in the original version."
            }
        },
        "id": "sZ3YdYPfZl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Q9BLbN1p6h",
        "replyto": "Q9BLbN1p6h",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission798/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496179,
        "cdate": 1696707496179,
        "tmdate": 1701465410756,
        "mdate": 1701465410756,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses the impact of splitting strategies on reproducibility. It provides theoretical motivations for using blocked 3x2 cross-validation and presents experiments showing that this indeed provides a higher signal-to-noise ratio. The paper is presented nicely with theoretical motivations that are backed up by results.\n\nThere were concerns about the appropriateness of the signal-to-noise ratio as a measure for reproducibility and that the evaluation is optimised to show a specific result. The rebuttal provides extensive explanations of why signal-to-noise is suitable and convincingly explains why the experimental setup is different from the regular case where we want to test whether something is it case. \nIt would be good if a future version provides these explanations (in main text of appendix, depending on space) so that readers understand it at first hand."
            }
        },
        "id": "AFwaBk3pZM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Q93hLxLKLB",
        "replyto": "Q93hLxLKLB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5005/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707604557,
        "cdate": 1696707604557,
        "tmdate": 1701465547154,
        "mdate": 1701465547154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposed an interesting idea, an effective proposed method, supporting ablation tests and representation demonstrations, and insightful hyper-parameter analysis. However, this paper has limitations in poor writing quality, unclear transitions in equations, a lack of detail in methodology, and potential concerns about the competitive advantage of the proposed method in terms of F1 score improvement."
            }
        },
        "id": "H2Prx075jD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Q5nM3rpiVm",
        "replyto": "Q5nM3rpiVm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission878/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498250,
        "cdate": 1696707498250,
        "tmdate": 1701465413415,
        "mdate": 1701465413415,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this paper the authors propose an approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). The proposed method outperforms several baselines in terms of diversity and quality, even those using domain-specific data enhancement techniques. Also, it brings better downstream performance on specific domains like maths, brainstorming and rewriting.\n\nPros:\n\nIt is a well-motivated, highly timely and relevant research topic\n\nComprehensive investigation on the quality of generated instruction data\n\nThe approach seems to work well at  increasing the diversity of an instruction set, keeping the overlap between instructions relatively low. \n\nThe authors also performed a graph search over the space of domains which is an interesting and novel idea, and seems to perform reasonably well.\n\nCons:\n\nNeeds more comprehensive test domains, which are more “domain-intensive” like biology or medical (than maths, brainstorming and rewriting)\n\nNeeds more fair comparison with other baselines.\n\nThe original work will also benefit from incorporating the additional discussion points/experiments conducted during rebuttal."
            }
        },
        "id": "ESGNuvnvaa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Q4u18Ui7YS",
        "replyto": "Q4u18Ui7YS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission976/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500779,
        "cdate": 1696707500779,
        "tmdate": 1701465416617,
        "mdate": 1701465416617,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary of Reviews and Post-Rebuttal Discussion:**\n\nReviewer RMs4 appreciated the combination of theoretical and empirical aspects of the paper and acknowledged the deeper understanding it offers on the capabilities of linear Transformers. However, they raised concerns regarding the clarity of certain results, whether results were from single-run experiments or hyperparameter searches, and pointed out specific issues related to citations, task descriptions, and the overall presentation. The authors addressed these concerns in their rebuttal, providing explanations for their design choices, acknowledging and promising to fix some issues, and clarifying certain aspects of their experimental design.\n\nReviewer 5khN recognizes the clarity and well-written nature of the paper, and commends it for its interesting results. The main criticism stems from uncertainty about how the findings might advance the field, which the authors addressed in their rebuttal by pointing out the importance and relevance of studying neural networks using formal languages.\n\nReviewer Dg9f praised the paper for its theoretical analysis of normalized linear Transformers' expressive power and their empirical evaluations on formal language recognition tasks. The paper effectively provides insights into the properties and limitations of linear Transformers, a computationally efficient alternative to standard Transformers. Criticisms include the paper's limited focus on certain formal language tasks, not comparing linear Transformers to models like RNNs with external memory, and not thoroughly exploring the computational complexity of certain extensions. In response, the authors argued that their chosen languages are based on prior research and are meant to assess specific computational capabilities. They defended the absence of comparisons by focusing on tasks where traditional linear Transformers falter and clarified that their main contribution isn't the architecture but the insights provided, with computational complexities discussed in earlier research.\n\nReviewer Vzz8 highlighted that while the theoretical aspects are incremental, the paper makes novel contributions in the experimental section. The experiments indicate that the extensions of linear Transformers allow the model to recognize specific non-star-free languages. The authors defended their contributions in the rebuttal, emphasizing the importance of their results.\n\nReviewer AXki criticized the submission for its lack of novelty, noting an over-reliance on proofs from previous works. The authors defended their work, emphasizing the technical contributions and novel findings in the experimental section, particularly about the capabilities of linear Transformers to recognize non-star-free languages. Despite the authors' rebuttal, the reviewer remained skeptical about the paper's novelty, citing unfamiliarity with the extensively cited prior works.\n\nIt should be noted that overall confidence of reviewers was very low. Reviewers 5khN and AXki reported confidence 1 and the rest reported confidence 2. Still, reviewers mainly align in their judgment of soundness and excitement of the paper.\n\n**Soundness:**\nThe paper appears to be mostly sound in its claims and arguments. The reviewers' concerns were mainly about clarity and presentation rather than the validity of the results. The authors have proposed solutions to address the clarity issues.\n\n**Excitement:**\nThere is a mixed reception to the paper's excitement level. Some reviewers have reservations about the paper's potential to advance the field. However, the authors' rebuttal was strong, indicating the importance of studying neural architectures with formal languages."
            }
        },
        "id": "SscjqXj2ro",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Q2Wu2Cfp2x",
        "replyto": "Q2Wu2Cfp2x",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3713/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707568228,
        "cdate": 1696707568228,
        "tmdate": 1701465507374,
        "mdate": 1701465507374,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper studies the task of multi-label intent prediction and proposes SPCL - Supervised Prototypical Contrastive Learning, which is a mashup of PCL and SCL. \n\nReviewers appreciated certain aspects of the paper which includes its presentation, description of the baseline approaches which the paper uses to build upon, and decent levels of analysis. \n\nAmongst concerns, primary points that were raised included:\n- lack of relevant comparisons/discussions. For this point, the authors argue the recent works are not comparable due to slot-filling task. While that's true, a discussion of latest research is warranted so that reviewers can better evaluate the positioning of the paper. \n- lack of excitement. The paper largely fuses known ideas in the CV and NLP community. As such, reviewers were not highly excited about the proposed innovations.\n- blurry figure. While a minor issue, the text in figures are indeed blurry which can (and should) be easily fixed using pdfs or other vector-based images.\n\nThe above issues are addressable and should be fixed in the revision as promised by the authors."
            }
        },
        "id": "ZzSWe5egQD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Q2IInBu2kz",
        "replyto": "Q2IInBu2kz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission9/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476641,
        "cdate": 1696707476641,
        "tmdate": 1701465383843,
        "mdate": 1701465383843,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose InterFair, a novel human-in-the-loop pipeline approach to model debiasing, where free-text feedback from humans is used to update importance scores for model rationales. The free-text feedback is parsed by a text-davinci-003 model and transformed into numeric scores, which are then used to update the bias importance of each individual token for a frozen model on the BiosBias classification task. Following the claim that it is challenging to fairly eliminate biases algorithmically, the authors evaluate their scenario at inference-time for two experiments, where users interact with test samples and inputs, showing that both scenarios further mitigate bias from explanations.\n\nThe reviewers agree that the paper is clearly motivated (beK8, KjGw, mchw), well written and inuitively structured (mchw). Reviewers note that the proposed methodology is novel (beK8) and has a lot of potential (mchw) — and that the presented experimental results support the authors’ approach (beK8, Kj4w).\n\nThe main criticisms presented in the reviews are lack of details regarding the human study (beK8) and more clarity with respect to the gap to previous work (beK8, Kj4w) as well as the experimental setup (mchw). Furthermore, the choice of a LSTM-based model as part of the pipeline and not a Transformer-based variant limits the scope of the work (Kj4w). Lastly, a question was raised with respect to the choice of natural language feedback instead of using plain token editing (beK8).\n\nThe authors responded to the questions raised in the reviews, committing to add clarifying details. The authors also presented additional results with a BERT-based model, motivated their choice of free-text input through a user study and clarified the gap with respect to previous work.\n\nUpon reading the paper, reviews, as well as the discussion, it is my opinion that the paper is for the most part well written and easy to follow. Furthermore, it tackles a relevant problem in a novel manner. When reading the paper, my main concerns come from a significant reliance on work of He et al, where the reader is assumed to be familiar with the setup — something that can be improved in the manuscript. Another concern was the limited scope of experiments, which is addressed by the authors in the discussion period by adding experimental results on a Transformer-based model."
            }
        },
        "id": "KMhDk2UdZa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PzINxIyV9o",
        "replyto": "PzINxIyV9o",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5507/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614323,
        "cdate": 1696707614323,
        "tmdate": 1701465561279,
        "mdate": 1701465561279,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Very useful paper on the customization of very-long prompts to improve in-context learning. The paper is principled, well-written and impactful."
            }
        },
        "id": "LX6u23x4YE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PyJ78pUMEE",
        "replyto": "PyJ78pUMEE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2607/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545734,
        "cdate": 1696707545734,
        "tmdate": 1701465470663,
        "mdate": 1701465470663,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper tackles two tasks: predicting the 143 different categories of the ManIfesto Research on Political Representation project (MARPOR) and RILE values, Standard Right–Left Scale that uses pre-defined right-wing and left-wing categories from the MARPOR project and a deterministic formula to get a RILE score between -1 and 1. They highlight that there needs to be more methods that deal with long texts and work robustly across domains and languages. \n\nReviewers generally found the experimental set-up to be very sound. \n\nReviewers had a few questions about the motivations and impact of the substantive task (the predicting MARPOR categories and RILE scores). However, the authors wrote a pretty convincing rebuttal that the tasks are “relevant for the analysis of short-term policy and public-opinion shifts between election cycles or the study of the influence of extraneous factors, such as economics, climate change, or large-scale international events, on the political landscape” and “pre-theoretical assessments of party positions are typically specific to individual countries, but party comparison across countries is a relevant field of investigation that ideally requires a country-agnostic operationalisation of such scales.” These do seem to be well-established tasks that social scientists would benefit from having improved predictive accuracy. \n\nOverall, it seems this is a sound and exciting paper that spans *both* the NLP and social science communities. I think both communities could potentially benefit from this paper."
            }
        },
        "id": "zS3WWwhGrB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PyAzL6Z802",
        "replyto": "PyAzL6Z802",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3975/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580554,
        "cdate": 1696707580554,
        "tmdate": 1701465515840,
        "mdate": 1701465515840,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies an interesting topic about whether GPT can be a good data analyst. The paper is well-motivated and easy to follow, the experiments are solid, and it has the potential real impact to significantly reduce the workload for data analytics. However, reviewers also point out the paper could add more in-depth analysis, the data and label generated are subjective, the evaluation results have some issues."
            }
        },
        "id": "RfzJ8Oxp9R",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PxEhoPiBB0",
        "replyto": "PxEhoPiBB0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2597/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545523,
        "cdate": 1696707545523,
        "tmdate": 1701465470297,
        "mdate": 1701465470297,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "All the reviewers have agreed that it is interesting to see the integration of diffusion models into generative retrieval and the improvement gained.\n\nHowever, common concerns have also been raised that the motivation is somehow weak and some discussion on the relations and differences between the proposed method and its competitors are missing.\n\nDuring the rebuttal period, the authors have only partly addressed these concerns, leaving room for further improvement."
            }
        },
        "id": "NpLrXsJFGo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Pw9vYSPJKk",
        "replyto": "Pw9vYSPJKk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3158/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557192,
        "cdate": 1696707557192,
        "tmdate": 1701465488713,
        "mdate": 1701465488713,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new ART task (rule based future-inference deduction task) to explore the deductive reasoning in multi-modal domain. Moreover, this paper construct a large-scale dataset to promote multi-modal deductive reasoning research, which consists of 23, 895 examples where dense annotations including the rule set, reasoning processes, and auxiliary commonsense knowledge are provided. Based on it, a strong baseline model ARTNet is also developed with three modules: 1) knowledge-guided target perception with encoder-decoder transformers, 2) rule-based graph reasoning network, and 3) reasoning path review to decide the correct future event. Experimental results demonstrate the effectiveness of the proposed model.\nAll reviewers rate soundness and excitement highly. In summary, the paper formulates the ART task as an early exploration of deductive reasoning in the multi-modal domain. This work provides a large and rich annotated dataset which could benefit the research related to this topic. Also, authors develop a well-designed baseline towards better understanding the task. The proposed task is interesting and has the potential to promote rule-based reasoning in the multimodal domain. The paper is well-written, offering a comprehensive task definition and clear formulation. This paper has great potential for accept to main conference."
            }
        },
        "id": "e9QBGcQv5G",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Pu5tJykUeT",
        "replyto": "Pu5tJykUeT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2772/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549043,
        "cdate": 1696707549043,
        "tmdate": 1701465475775,
        "mdate": 1701465475775,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The main conclusions of the reviews and the post-rebuttal discussions:\n- 3/ 3 reviewers consider the paper sound (scores 3, 3, 3)\n- 3/ 3 reviewers find the paper exciting (scores 4, 4, 3)\n\nFrom reading the rebuttal and seeing the scores above, I find that the reviewers consider strong points for soundness the following:\n- Novel dataset for evaluation of epistemic capabilities of language models using inspection of knowledge integration using several reasoning tasks. \n- Good experimental analysis of the performance T5 and Flan-T5 models on the dataset and final recommendations \n- Knowledge consolidation in LLMs is important. The ability to combine information seen in different documents during training is crucial for many applications. This is the first study that investigates LMs’ capability to perform this information combination effectively."
            }
        },
        "id": "tLiWJWgvoI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PomhVDrvco",
        "replyto": "PomhVDrvco",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1856/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528867,
        "cdate": 1696707528867,
        "tmdate": 1701465444918,
        "mdate": 1701465444918,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper examines label variation/human disagreement in case outcome classification.\n\nReviewers generally agree that the paper presents a valuable contribution, especially as it provides important insight into human subjectivity and limitations of current models. They appreciate the thoughtfulness of the fine-grained dataset and the taxonomy of disagreement sources. One reviewer also notes the methodology as one that opens up opportunities for additional investigation."
            }
        },
        "id": "1gcQgIzrT0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PoMCId4iez",
        "replyto": "PoMCId4iez",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5493/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614066,
        "cdate": 1696707614066,
        "tmdate": 1701465561017,
        "mdate": 1701465561017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper examines the use of LLMs for the task of Bilingual Lexicon Induction (BLI), and in particular examine and evaluate different prompting strategies. They find that LLMs have strong capability to generate bilingual lexica. \n\nSoundness: All reviewers agree that the soundness of the paper is very high (giving scores of 4/4/5), and that comprehensive experiments are performed to support the conclusions in the paper.  \n\nExcitement: Reviewers generally score high excitement for the paper (4/4/4) and list the readability and thoroughness as main reasons. Some challenges to excitement are raised in that (1) it is not surprising that LLMs can generate BLIs well, and (2) it is somewhat unclear why one would need an LLM-generated BLI instead of simply using the LLM (which generated the BLI) to perform a downstream task itself. The authors reply with an example in machine translation and point to 2 arXiv papers that investigate this issue."
            }
        },
        "id": "aP3gS3w2hZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PnAmH1silV",
        "replyto": "PnAmH1silV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1644/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521576,
        "cdate": 1696707521576,
        "tmdate": 1701465437096,
        "mdate": 1701465437096,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose transformer-based text embedding (TTE) depth, and illustrate the use of this statistical depth on NLP tasks including in-context learning prompt creation and difference measurement between human-written and machine-generated text. The performance of TTE depth shows small improvement over baselines.\n\nWhile the reviewers were happy with the methdology, the impression from the evaluation results were mixed. The response was able to allviate only some of the concerns."
            }
        },
        "id": "M3uDG9bqJs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PffUQuD8sn",
        "replyto": "PffUQuD8sn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1997/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531737,
        "cdate": 1696707531737,
        "tmdate": 1701465450073,
        "mdate": 1701465450073,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a meta-model to estimate the accuracy and confidence of other language models with in-context learning in the absence of labeled test data. This approach has the potential to considerably save time and resources, and the experiments are solid and well-motivated. Multiple reviewers expressed concerns about the applicability of this method, but this issue seems to have been answered in the rebuttals."
            }
        },
        "id": "BayDT3xyVD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Pb1DhkTVLZ",
        "replyto": "Pb1DhkTVLZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4153/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584277,
        "cdate": 1696707584277,
        "tmdate": 1701465522134,
        "mdate": 1701465522134,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This study proposed clickbait detection dataset in Romanian, which is first of its kind. The dataset is comprised with 7,913 news samples, which are manually annotated with clickbait and non-clickbait labels.   \n\nReviewers provided insightful feedback. For instance, Reviewer DCj2 pointed out an imbalanced distribution in the dataset (e.g., a majority of non-clickbait samples). This should be clarified to improve the quality of the paper. This reviewer also expressed concerns about the baseline experiments. Since this study aims to serve as a baseline for future work, it is important to present additional results using different models. The selection of these models could be based on those that are widely used and demonstrate superior performance in related studies. \n\nReviewer X7XQ suggested comparing the dataset with other popular datasets. It will help readers understand the utility and significance of the presented dataset."
            }
        },
        "id": "RwOJZz131y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PaVP2Sc6pJ",
        "replyto": "PaVP2Sc6pJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission82/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478668,
        "cdate": 1696707478668,
        "tmdate": 1701465386686,
        "mdate": 1701465386686,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Offsite-Tuning (OFT) transfers transformer blocks between LLMs and downstream emulators to address this issue of privacy when instruction tuning LLMs with private data. The authors conducted an empirical study of LLMs, focusing on their representation and functional similarity. They discovered that as the model size grows, a unique modular structure emerges within LLM layers. They proposed a training-free strategy, CRaSh (Clustering, Removing, and Sharing), to derive improved emulators from LLMs that significantly enhances  the performance of OFT with billions of parameters.\n\n\nPros:\n- This paper analyzes LLMs from the perspective of representation and functional similarity, shedding light on how they work as model size increases.\n- CRaSh is a novel and effective strategy for improving the performance of OFT with billions of parameters.\n- They demonstrate empirically that CRaSh improves performance across multiple datasets.\n\nCons:\n- The paper will improve by clearly motivating how OFT ensures privacy. Adding the answers provided by the authors during rebuttal will improve the paper."
            }
        },
        "id": "TBp8HD7EEV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PXkS70nuNp",
        "replyto": "PXkS70nuNp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5490/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614001,
        "cdate": 1696707614001,
        "tmdate": 1701465560823,
        "mdate": 1701465560823,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper studies the task of Emotion Recognition in Conversations (ERC) in the code-mixed setting. In particular, it studies the commonsense-aided ERC task.\n\nLargely, the paper (a) Proposes a dataset towards this task (b) Does a study of translating code-mixed sentences to English to perform commonsense inference, and (c) Provides comprehensive experiments.\n\nReviewers appreciated the presentation of the paper and its technical soundness. The questions raised during the discussion were aptly addressed by the authors. Furthermore, reviewers liked the fact that ERC is being studied in code-mixed setting and appreciated the promise to release the prepared dataset."
            }
        },
        "id": "wtVJYbuY1U",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PWWg9q3S0C",
        "replyto": "PWWg9q3S0C",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4137/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583865,
        "cdate": 1696707583865,
        "tmdate": 1701465521467,
        "mdate": 1701465521467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents SAFARI as a framework to leverage the LLM's planning capability to incorporate multiple knowledge sources for personalized knowledge-grounded dialogue response generation. The paper also introduces a new dataset KBP for experiments, and experiments shows the SAFARI is effective at producing persona-consistent and knowledge enhanced dialogue responses. \n\nWhile the reviewers generally agreed the paper is well-written and easy to follow, the knowledge within the KBP dataset is limited to persona-related information, making it less clear whether the SAFARI framework can generalize towards general knowledge-grounded dialogues."
            }
        },
        "id": "et20rXr7Mz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PTko0qsiA4",
        "replyto": "PTko0qsiA4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2905/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551930,
        "cdate": 1696707551930,
        "tmdate": 1701465480788,
        "mdate": 1701465480788,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary: This paper explores “added toxicity” in machine translation (MT) (i.e. where the output contains toxicity not present in the source) by evaluating the 3.3B NLLB MT model. The authors evaluate/analyse the automatic translation of the HolisticBias dataset (consisting of non-toxic lists constructed from templates) from English into 164 languages. Added toxicity varies by target language (worse for low-resource). They use the Alti+ method to analyse input tokens’ contribution to the generation of toxic words - many cases are due to mistranslation and they recommend additional data curation to reduce this. They combine the source-input analysis with one using using Gini impurity (looking at model robustness) to define a rough toxicity explanation method that flags 22% of toxic translations. They also perform human evaluation of the toxicity detection method they present.\n\nOpinion: The paper is well written and the analysis well motivated. Although the paper looks at only one model, the analysis is quite extensive. The paper would benefit from a few additional details concerning certain experimental choices to aid clarity and reproducibility, as well as the information required by the ethics review (recruitment/pay of annotators and how they were prepared for toxic content). There are also a few references pointed out by reviewer 2 that would strengthen the paper. However, the paper is in my opinion reasonably solid and a good contribution to the analysis of added toxicity, with sufficient discussion around the problems faced when designing analysis methods."
            }
        },
        "id": "vFV5egvodk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PT6lSdWEgw",
        "replyto": "PT6lSdWEgw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission154/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480479,
        "cdate": 1696707480479,
        "tmdate": 1701465388889,
        "mdate": 1701465388889,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors evaluate GPT-4 (and on request of two reviewers also other models) on the task of judging the \"profoundness\" of mundane or pseudo-profound statements. While they find a correlation between human scores and model scores, the models tend to give higher profoundness ratings to  statements than humans. The paper compares different prompting techniques, which all exhibit the reported problem.  \nTwo of the reviewers appreciate the importance of the issue that is being addressed; one of the reviewers is worried about a lack of novelty, but this worry is not substantiated by the reviewer -- instead, the task of judging profoundness / detecting lack of substance is a rather underresearched problem."
            }
        },
        "id": "6u9bgxUQVz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PT63nNpyKg",
        "replyto": "PT63nNpyKg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3827/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707576856,
        "cdate": 1696707576856,
        "tmdate": 1701465510944,
        "mdate": 1701465510944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces approaches for combining the training of conversational recommender systems (CRS) and large language models (LLM) for pre-sales dialogue. They look at how CRS can assist LLM and vice versa. They look at four subtasks; intent understanding, preference elicitation, product recommendation, and response generation. They use the U-NEED dataset which has five product categories; beauty, phones, fashion, shoes, and electronics. The reviewers agree that this is a well motivated and interesting problem that has real world applications. The reviewers disagree on the clarity of the paper and have provided helpful suggestions for improvements. The authors provide detailed results and analysis that provide insights that future work may build off of. The reviewers range in excitement. The novelty is limited in comparison to recent works pointed out by the reviewers. Small improvements are seen in the results. Reviewer n8cG points out the potential confound in the number of model parameters that may contribute to the improvements. However, the work provides insights that will help inform future work in this area. The overclaim of novelty should be corrected in the next revision. Similarly, the plagiarized portion of the appendix must be rewritten and/or properly attributed in the next revision."
            }
        },
        "id": "bxgsmku4Ud",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PSlrVYPTAX",
        "replyto": "PSlrVYPTAX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission106/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479349,
        "cdate": 1696707479349,
        "tmdate": 1701465387508,
        "mdate": 1701465387508,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers appreciated the incorporation of multimodal information for recommendation and the efficiency of the method in terms of performance. However, the authors, are highly encouraged to include their clarifications during the rebuttal in the final version of the paper and: \n\n- clarify the writing as suggested by R3: avoid any potential overclaim, revise the related work section and situate this work a bit better,\n\n- add experiments (e.g., those suggested by R3 and R4),\n\n- better explain the motivation for parameter efficiency."
            }
        },
        "id": "kYesSOCGzo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PPwRa7Wmg1",
        "replyto": "PPwRa7Wmg1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4135/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583822,
        "cdate": 1696707583822,
        "tmdate": 1701465521414,
        "mdate": 1701465521414,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Overall this paper makes a solid short research contribution, as also pointed out by the reviewers. The authors have provided sensible answers to the reviewers' questions, and these additional explanations should be incorporated in the paper to improve its readability.\n\nMoreover, the point of motivating the Eigenvalue decomposition is also an important one."
            }
        },
        "id": "8DbOyzvnon",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PNpRxOhVut",
        "replyto": "PNpRxOhVut",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5335/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611205,
        "cdate": 1696707611205,
        "tmdate": 1701465556513,
        "mdate": 1701465556513,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper creates a new benchmark dataset for assessing factual accuracy in summarization. They show that the task is challenging for LLMs. \n\nBoth reviewers hL41 and jN9Y agree on the soundness of the paper and the authors have addresses or promised to address the comments. In the rebuttal the authors seem to have addressed the concerns of eGPn.\n\nThis resource seems valuable to the community and could be quite impactful in judging new LLMs."
            }
        },
        "id": "cXLQkqZcTO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PHtXqUNGUA",
        "replyto": "PHtXqUNGUA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission844/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497523,
        "cdate": 1696707497523,
        "tmdate": 1701465412658,
        "mdate": 1701465412658,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a novel framework for generating visual commonsense content, resulting in more diverse and detailed outputs. The experimentation is meticulously conducted, and the ablation analysis is well-explained. The proposed framework brings substantial improvements to both BLIP and a vision-language enhanced BART across various metrics, including both automated (Tables 1-3) and human evaluation (Table 7).\n\nThe reviewers find the paper well-written and clear, and they appreciate the innovative and robust approach. Following the rebuttal phase, all reviewers have voted in favor of accepting the paper. I recommend accepting the paper."
            }
        },
        "id": "Asn5NU7Gaz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PHh1s8dNlY",
        "replyto": "PHh1s8dNlY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2779/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549228,
        "cdate": 1696707549228,
        "tmdate": 1701465476078,
        "mdate": 1701465476078,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a pipeline to enhance text-guided image editing by improving the quality of paired data through recent advancements in segmentation, chain-of-thoughts prompting, and VQA. While the proposed approach is intriguing and leverages important developments in computer vision and NLP, there are notable weaknesses in clarity and presentation. Specifically, the paper lacks detailed explanations regarding the use of supervision signals in fine-tuning and exhibits confusion regarding the treatment of fine-tuned data in experiments. Presentation issues, such as missing labels in figures, hinder the understanding of model inputs and outputs. Furthermore, the paper would benefit from a comparative analysis with other relevant datasets or methods, such as MetaCLUE. Despite these shortcomings, the use of TIFA scores and human evaluation adds credibility to the proposed dataset's impact, making it a valuable contribution to text-guided image editing with significant potential after addressing the mentioned issues."
            }
        },
        "id": "W5WZ3Ebjov",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PCyB5LUF4z",
        "replyto": "PCyB5LUF4z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4105/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583225,
        "cdate": 1696707583225,
        "tmdate": 1701465520557,
        "mdate": 1701465520557,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers found the paper interesting, exciting, and sound. The authors addressed the issues that were brought up in the discussion phase. The only unaddressed criticism is related to the potentially narrow scope."
            }
        },
        "id": "rLCj6rIM9j",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PCNsizlhRU",
        "replyto": "PCNsizlhRU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission599/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491718,
        "cdate": 1696707491718,
        "tmdate": 1701465404744,
        "mdate": 1701465404744,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a benchmark for topical text classification called TTC23, which is curated from a diverse set of existing datasets for the task. The authors use this benchmark to explore the zero-shot capabilities of LLMs for the task, and also show that fine-tuning on diverse TTC data can further enhance performance on new unseen categories.\n\nReviewers agree that the proposed benchmark is comprehensive, and that the set of experiments presented is extensive, involving various pre-trained models. That will benefit \"understanding the capabilities and limitations of LLMs in zero-shot TTC scenarios, providing guidance for practitioners interested in this field.\"\n\nOne of the concerns raised by the reviewers involves the novelty of the results, since similar conclusions have been obtained previously in more general settings. The authors argue that their contribution lays in providing in-depth insights in a narrower set of downstream tasks, which improves zero-shot performance in TTC in unseen ones. Another concern is regarding not comparing against ICL models, which the authors argue that, while relevant, does not align with their focus on allowing end users to only rely on zero-shot interactions with good performance. Authors are encouraged to include their valid justifications for not comparing against GPT models, as well as their clarifications for the effects of data used during pre-training and testing."
            }
        },
        "id": "kC0XPfk7la",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PBwotNgvp3",
        "replyto": "PBwotNgvp3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1817/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527933,
        "cdate": 1696707527933,
        "tmdate": 1701465443051,
        "mdate": 1701465443051,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors focus on explicitation in translation, which explains the missing context by considering cultural differences between source and target audiences.\nThey first create a dataset of naturally occurring explitations, WIKIEXPL, that they collect from Wikipedia and annotate with human translators.\nInspired by WIKIEXPL, they propose a technique to generate explicitation automatically.\nThey show the effectiveness of the proposed automatic explicitation method, both intrinsic evaluation by human evaluators and extrinsic evaluation by multilingual question answering tasks.\n\nMany reviewers agree that WIKIEXPL is a valuable resource for research on translation and that there is much to learn from how they create it.\nOne reviewer is not impressed with the proposed automatic explicitation method because it simply extracts some structured fields from Wikipedia.\n\nAlthough none of the reviewers pointed this out, the meta-reviewer thinks the proposed method has something in common with the following work, which generates a description for unknown phrases to bridge the knowledge cap.\n\n[Ishiwatari+, NAACL-2019] Learning to Describe Unknown Phrases in Local and Global Contexts"
            }
        },
        "id": "t12CzjPfUu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PBvSGqYCSa",
        "replyto": "PBvSGqYCSa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4656/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595354,
        "cdate": 1696707595354,
        "tmdate": 1701465537845,
        "mdate": 1701465537845,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reasons to accept\n(1) The paper addresses the important issue of syntactically controlled paraphrase generation, specifically focusing on selecting appropriate templates.\n(2) The proposed method is intuitive and can be applied to any Syntactically Controlled Paraphrase Generation (SPG) model to enhance its performance.\n(3) The paper highlights and addresses the potential problem of low diversity in generated paraphrases when using a quality-based retriever approach to select templates.\n(4) Thorough evaluation includes a wide range of baselines, automated metrics, and human evaluations, providing comprehensive insights into the proposed approach's effectiveness, including its role in data augmentation for downstream tasks.\n\nReasons to Reject:\n(1) The paper's proposed method appears to be less effective when used with the SISCP SPG model compared to the performance gains observed for AESOP.\n(2) Some parts of the paper are not clearly written, making it challenging for readers to fully understand certain aspects, such as the choice of templates and their role.\n(3) The task definition could be clearer, and the choice of exemplar-as-template as a baseline may not align with the typical setup for controlled paraphrase generation tasks.\n(4) The lack of certain strong baselines from previous related work and the use of two RoBERTa-based encoders make it challenging to determine whether performance gains are due to the proposed method itself or other factors.\n(5) The manual evaluation scores do not include a separate evaluation of syntactic controllability, which is a central aspect of the task.\n\nTwo of three reviewers agree that the paper is strong and exciting, however one reviewer gave ambivalent marks for the paper's strength and excitement. The authors also carefully responded to each reviewer's comments and questions by providing additional information, detailed explanations, or new experiments. I have read through the responses from the authors and consider their responses to be reasonable and valid."
            }
        },
        "id": "IjPY853Swl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "PAByut8fMZ",
        "replyto": "PAByut8fMZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3327/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560557,
        "cdate": 1696707560557,
        "tmdate": 1701465494080,
        "mdate": 1701465494080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Tokenization methods in SoTA MT consider subwords with the same orthography as equivalent, which does not make much sense with totally unrelated languages. The paper proposes using semantic equivalence instead of orthographic equivalence. They achieve systematic improvements in translation quality in all major automatic metrics (some of which were added during the discussion period) both in small (IWSLT14) and large (WMT) data setup.\n\nThe paper received 4 reviews, 3 of which are very positive about the paper, both in terms of soundness and excitement, one review was very negative. Two of the reviewers raised important issues concerning the evaluation metrics, which were later clarified during the discussion period and the only negative-leaning reviewer increased their score."
            }
        },
        "id": "Fp1qeueXGG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "P9V2jcotAF",
        "replyto": "P9V2jcotAF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2039/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532809,
        "cdate": 1696707532809,
        "tmdate": 1701465451457,
        "mdate": 1701465451457,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper uses modeling techniques to further understanding of human language and cognition. It is an excellent example of its kind."
            }
        },
        "id": "gNNlW844re",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "P5hYS77k10",
        "replyto": "P5hYS77k10",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4831/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599160,
        "cdate": 1696707599160,
        "tmdate": 1701465542887,
        "mdate": 1701465542887,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The research delves into the examination of \"persona biases\" in dialogue systems, highlighting the potential harmful behaviors that arise when models adopt specific personas. These biases are categorized into two primary types: harmful expression and harmful agreement, further divided into five categories: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic Agreement. To facilitate this study, the authors introduce UniversalPersona, a new dataset that provides a taxonomy of various persona traits. Using this dataset, the paper evaluates the biases present in four distinct dialogue models, including ChatGPT and Blender. The results reveal significant persona biases across these models, with ChatGPT showing a notably high level of bias. The findings underscore the importance of reevaluating the integration of persona traits in dialogue systems to ensure their safe and responsible use.\n\nThe paper is commended for its clarity, well-structured presentation, and significant contribution to the field of persona-based biases in language models. The introduction of the UNIVERSALPERSONA dataset is seen as a valuable addition that will aid further research in this domain. The paper's comprehensive taxonomy of persona variables is highlighted as a potential foundation for future studies on the topic. The research differentiates itself from existing work, providing a clear motivation for its relevance and importance. A notable finding is that ChatGPT exhibits more social bias compared to other models, especially significant given ChatGPT's proprietary nature and its leading performance in various tasks.\n\nOne primary concern is the absence of case studies for the different Large Language Models (LLMs), which could have provided more depth and understanding to the research. Another significant point of contention is the clarity and precision of the metric categories. A clearer distinction between \"harmful expression\" and \"harmful agreement\" is desirable, with examples to illustrate the difference. There are concerns about the comparability of different metrics, especially when they are determined by separate classifiers. The potential biases of these classifiers could skew the results, making cross-metric comparisons potentially misleading."
            }
        },
        "id": "MttxGY7F2o",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "P2jDML1Ub6",
        "replyto": "P2jDML1Ub6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2599/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545589,
        "cdate": 1696707545589,
        "tmdate": 1701465470413,
        "mdate": 1701465470413,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree that the paper provides sufficient support for its major claims/arguments, but were ambivalent about the merits and readiness. The main strengths of the paper are clear motivation, model setup relevant to the application, experimental performance, choice of comparators. The excitement is diminished mainly because of lack of clarity in methodology that left the readers wonder what were the exact contributions of the paper."
            }
        },
        "id": "saeCnIghMe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "P04rLpllH7",
        "replyto": "P04rLpllH7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3144/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556951,
        "cdate": 1696707556951,
        "tmdate": 1701465488371,
        "mdate": 1701465488371,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Reject"
            },
            "comment": {
                "value": "This paper introduces a method to generate pre-training data customized for downstream tasks by prompting LLMs. Experimental results on two QA tasks demonstrate the effectiveness of the proposed method. \n\nWhile the reviewers appreciated importance of the problem space, novelty of the proposed method, and comprehensive experiments, they raised the concerns about the clarity of experimental design and result analysis."
            }
        },
        "id": "KwZGHKvnM8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OxoP1qFotz",
        "replyto": "OxoP1qFotz",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission4092/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582993,
        "cdate": 1696707582993,
        "tmdate": 1701465520021,
        "mdate": 1701465520021,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a novel dataset to evaluate ability of sentence embedding models (such as SentenceBERT) to distinguish semantic differences expressed using syntactical means: e.g. A dog chase a cat -> A cat chase a dog. \nThe paper uses a semi-automatic process to expand an existing evaluation dataset and then benchmarks existing sentence models, showing their limitations. \nAll three reviewers agree in their general assessment of the paper. Authors made significant efforts to further clarify their task during rebuttal period. Parts of their response should be included into the final version, especially the table with human assessment."
            }
        },
        "id": "WRBU6NwoGr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ox0OoyLass",
        "replyto": "Ox0OoyLass",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2853/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550798,
        "cdate": 1696707550798,
        "tmdate": 1701465478831,
        "mdate": 1701465478831,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces an NER augmentation method, which involves finding similar texts in the dataset, generating various subtree swaps, and filtering candidates based on language model scores and Jaccard similarity. Experiments demonstrate its effectiveness, particularly in low-resource settings and for languages like Dutch. The method remains robust even with imperfect dependency parses, outperforming existing baselines on noisy social media datasets. Notwithstanding the inquiries or apprehensions raised by the reviewers, the authors adeptly resolved them in their rebuttal."
            }
        },
        "id": "TwFc3OiPBD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OwxjgsX68V",
        "replyto": "OwxjgsX68V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission252/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482937,
        "cdate": 1696707482937,
        "tmdate": 1701465392802,
        "mdate": 1701465392802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "he paper presents CROW, a benchmark for evaluating commonsense reasoning in real-world NLP tasks. All reviewers viewed the framework for constructing a commonsense reasoning dataset grounded in a real world NLP context as fairly innovative. However, the reviewers also raised some concerns about the soundness (details) of the construction procedure of the dataset.  Most of these concerns were at least partially addressed by the authors during the rebuttal phase. I recommend the authors to add more analysis of stableness."
            }
        },
        "id": "U8uSNfEnwT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OwWIl6gb1z",
        "replyto": "OwWIl6gb1z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3523/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564487,
        "cdate": 1696707564487,
        "tmdate": 1701465500076,
        "mdate": 1701465500076,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This is an interesting paper that did extensive experimentation on using LLMs for zero-shot video understanding tasks, showing an interesting result beating supervised baselines. They demonstrated the utility of language generation systems in domains where a strong data signal could be limited such as video-based data that heavily relies on human annotation for quality control.\n\nPros:\n- Clear description of their framework, pipeline and task formulation. Would be essential for reproducing such a setup on other tasks\n- Their method of multi source text data extraction and comprehension via LLMs is sound\n- Experiments are sufficiently detailed and paper is well written with extensive literature review\n\nCons: \n- Representing non-text modalities via text has been covered by other papers, reducing novelty of the idea\n- Video understanding is an umbrella term comprising several complex understanding tasks, handling missing data, and varying noise levels/durations of data. Tasks covered in this paper do not necessarily reflect takeaways towards complex video understanding problems."
            }
        },
        "id": "TJ8utdlcwB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ov6OZ2TFKI",
        "replyto": "Ov6OZ2TFKI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission232/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482518,
        "cdate": 1696707482518,
        "tmdate": 1701465392206,
        "mdate": 1701465392206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper provides a deep dive into the mechanics of in-context learning (ICL) for classification tasks by analyzing how label words function as anchors.\n\nIn general, the paper is well-structured and written. Proposed anchor re-weighting and anchor-only context compression that improved the effectiveness and inference speed over vanilla ICL. All reviewer agreed the paper could have a wider impact and encourages relevant research on interpretable LLM.\n\nSome concerns were also pointed out by several reviewers that require additional experiments and clarification."
            }
        },
        "id": "TdigxV2gbG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OkQD6RMUK5",
        "replyto": "OkQD6RMUK5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1611/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707519792,
        "cdate": 1696707519792,
        "tmdate": 1701465436050,
        "mdate": 1701465436050,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents \"NEWTON\", a benchmark designed to evaluate fine-grained physical reasoning abilities in Large Language Models (LLMs). NEWTON comprises 160K templated question/answers that focus on properties such as texture and brittleness, as well as deeper relative reasoning scenarios. The benchmark is built using a curated collection of 700 objects with associated physical attributes. A unique feature of NEWTON is its custom pipeline that allows users to synthesize personalized template questions for custom dataset/object/attribute splits. The paper also evaluates several LLMs on the NEWTON QA benchmark, providing insights into their strengths and weaknesses in physical reasoning. The benchmark aims to fill the gap in evaluating LLMs' ability to reason about physical properties and scenarios.\n\nNEWTON is a timely and valuable contribution to the field, addressing the increasing need for benchmarks that evaluate LLMs' physical reasoning abilities, especially as robotics and vision communities increasingly use LLMs. The resource is extensive, covering a wide range of objects and physical attributes, making it significantly larger than existing benchmarks. The paper is well-structured, with clear motivations and comprehensive experiments. Ablation studies further strengthen the paper's arguments, and the fine-tuning of models on the dataset is a commendable effort. Overall, the paper provides a systematic approach to studying physical reasoning, making it a significant addition to the community.\n\nWhile NEWTON is a valuable contribution, it has some limitations. The benchmark might lack the ability to reason based on richer object descriptions, such as the condition of objects, which can affect physical reasoning. Questions can be ambiguous due to the general nature of objects, as objects made of different materials can have varied physical properties. The authors, during the rebuttal, provided the reviewers with effective answers and new relevant experiments."
            }
        },
        "id": "awrj6zAb50",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OhZLO1yunf",
        "replyto": "OhZLO1yunf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4286/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586668,
        "cdate": 1696707586668,
        "tmdate": 1701465526597,
        "mdate": 1701465526597,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a neat yet effective promoting method for scientific name understanding in VLMs, which is a good example of a short paper. The reviewing process is generally towards positive and the rebuttal helps to answer some of the reviewers' concern. Please help include the additional results in any final versions."
            }
        },
        "id": "nPQs0ieP4b",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OgK0kMz5Va",
        "replyto": "OgK0kMz5Va",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission336/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485051,
        "cdate": 1696707485051,
        "tmdate": 1701465395388,
        "mdate": 1701465395388,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers thought that this paper was interesting and covered an important topic. They thought that the taxonomy of proposed strategies was well-motivated, and could be useful to build on in future work; some also found the annotation and LLM-enabled analyses valuable as well. Taken together, my sense is that the paper makes an interesting contribution to the area of NLP-enabled counterspeech, but could be substantially improved. I’ve highlighted some concerns from the reviews, and I encourage the authors to attend to these, as well as to the rest of the reviewers’ comments, as they revise their paper.\n* The notion of “convincingness” is key to the paper, and reviewers had doubts about whether the authors operationalized it in a sound way. I’d further add that “convincingness” means entirely different things in the context of CMV and in the instructions to annotators. This merits clarification: it is find for the authors to examine multiple notions of convincingness, but they should be more explicit that their operationalization is narrow, and perhaps tailored to specific purposes that they could elaborate on (e.g., certain forms of moderation).\n* The classifier performance is low. We realize this is because the task is hard, but the low performance, barring any further explanation, seems to undermine much of the analyses in section 6. For instance, if classification errors, as the authors say, often come from “mere mentions of alternative qualities/groups” that do not have a countering nature, then this seems like a systematic overestimation. Here, the authors could either improve their classifier — perhaps by increasing their data size, as reviewers suggest, or more critically discuss the results of their analysis."
            }
        },
        "id": "0ZA4VFRWyZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OfBAABKH5X",
        "replyto": "OfBAABKH5X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3603/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565985,
        "cdate": 1696707565985,
        "tmdate": 1701465503180,
        "mdate": 1701465503180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper performs an empirical analysis of the calibration of LLMs, after pretraining, instruction fine-tuning (full and LoRA), and RLHF, varying model size and training steps. Reviewers mostly agreed this was an important and informative analysis, though some of the findings (namely that larger models appear to be better calibrated after pretraining) are not new and replicate prior work. However, this work does provide analysis on additional datasets and training settings (after fine-tuning and RLHF). One reviewer questioned the soundness of evaluation of calibration using ECE, but seemed less concerned after the authors provided additional experiments. Overall this paper provides a comprehensive understanding of model calibration that goes beyond what was been studied previously, using existing accepted strategies for evaluating LLMs."
            }
        },
        "id": "Q2koAMFtly",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Of2xc2GVid",
        "replyto": "Of2xc2GVid",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission943/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499983,
        "cdate": 1696707499983,
        "tmdate": 1701465415717,
        "mdate": 1701465415717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary:\nThe reviewers agree that the paper's positives including a systematic and novel study of active learning (AL) for natural language generation (NLG). Overall, the paper is said to lay the foundation for future NLG AL research and is a pioneering contribution to the field. It categorizes AL strategies into representativeness and informativeness and applies them to four primary tasks: paraphrase generation, style transfer (formality), summarization, and question generation. The study utilizes the Flan-T5 pre-trained model, avoiding the need for additional training data. The research provides valuable insights into the performance of existing AL strategies, highlighting their inconsistencies and the need for new methods. It motivates further research in NLG AL by offering a comparative analysis of strategies and addressing the complexities of human language. \n\nThe reviewers share the following limitations of the paper: (1) hyper-parameter tuning is not feasible in the context of a large active learning (AL) study like the one presented. This limitation may impact the applicability of the findings to real-world scenarios. (2) The study's findings are based on a single base model and specific hyperparameters and could restrict the generalizability of the results, as AL strategy performance may vary with different models or hyperparameter settings. (3) The authors recognize inherent gaps between AL experiments conducted in the paper and the ultimate goal of achieving label efficiency with human involvement in a real-world context. Factors such as varying dataset qualities and temporal shifts in data distributions, typical of real-world data, are not addressed in the experiments. \n\nReasons to Accept:\n(1) The paper provides a systematic study of active learning (AL) for natural language generation (NLG), addressing a novel and relevant research area.\n(2) It explores a diverse set of NLG tasks and selection strategies, shedding light on their performance and behaviors.\n(3) The paper highlights the inconsistencies in the performance of existing AL strategies, providing valuable insights into the challenges and opportunities in NLG-related AL.\n(4) The findings of this study can motivate further research and innovation in AL for NLG, serving as a reference point for future studies.\n\nReasons to Reject:\n(1) The study primarily relies on a single base model and specific hyperparameters, potentially limiting the generalizability of the results to other models or settings.\n(2) The experiments do not fully account for real-world dataset challenges, such as variations in dataset quality and temporal drifts in data distributions."
            }
        },
        "id": "b71GmAeDye",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OcaifDZKkA",
        "replyto": "OcaifDZKkA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1577/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518165,
        "cdate": 1696707518165,
        "tmdate": 1701465435136,
        "mdate": 1701465435136,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper addresses a task of cost reduction when creating application using LLM APIs (for example, from OpenAI). This is indeed a bottleneck for many applications: while LLMs achieve impressive results, encoding user-generated information as context can get costly. In this sense the study provides an important solution to today's problems (see reviewers 1 and 2). The solution itself is based on existing techniques (cf. reviewer 3), however, the application is novel and impactful. Altogether, this study might be more appropriate for the industrial/applications track.\n\nThe experiments are convincing, showing a good compression rate with virtually no performance loss.\n\nHowever, there are several shortcomings:\n\n1) a more detailed cost analysis (see reviewer 1). this is important as the cost is the main focus here. this issue has been addressed thoroughly in the rebuttal -- and has to be incorporated into the next version\n2) presentation. a lot of information (e.g. hyperparameters etc) is presented in the supplementary material and should be upgraded to the appendix; the paper needs proofreading. this is a minor issue that can be fixed for the next version\n3) the experiments are run on a single domain. it is not clear how well the proposed solution generalize to other domains/settings (after all, the main motivation behind this study is to reduce cost for user domains, so the paper should focus on diverse use cases). This is a major issue raised by all the reviewers."
            }
        },
        "id": "zCYf68xddO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OZOrQQBDou",
        "replyto": "OZOrQQBDou",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5457/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613325,
        "cdate": 1696707613325,
        "tmdate": 1701465559835,
        "mdate": 1701465559835,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes a computational model to detect what the authors describe as conspiracy theories in news articles. The authors hypothesize that “conspiracy theories” will contain what they describe as “uncorrelated events” or an “unusual distribution of relations between events.”  Based on this hypothesis, they propose a model which attempts to use both the events in a news article (e.g. an infected event or hospitalization event) and the relationships between such events (e.g. infected before hospitalization) to detect conspiracy theories in news articles. The authors evaluate their model on an existing dataset LOCO (Miani et al., 2021), which appears to use credible news sources and conspiracy news sources to label individual news documents. The authors also emphasize that their work is different from prior studies, which focus on detecting conspiracies in shorter social media posts. \n\nOverall, the reviewers offered a mixed assessment of this work. \n\nReviewer nzA4 offered a shorter review emphasizing the importance of the task, the quality of the exposition and the soundness of the experiments. This reviewer also praised the authors for testing their model with source-based splitting (i.e., the model had to predict conspiracies in unseen news sources), which seems important for checking that the model can actually learn to detect substantive markers of conspiracy theories instead of just stylistic features of conspiracy news sources. This reviewer gave the paper a soundness score of 4 and an excitement score of 4.\n\nReviewer 9T2j also believed the experiments were sound (awarding a score of 3). But they were less excited about thew work than reviewer nzA4, awarding an excitement score of 3. In their review, 9T2j asked for additional evidence that models designed for shorter texts would perform less well on longer news articles. In response, the authors added an additional baseline in the rebuttal period. They showed that this method had F1 scores that were nearly 20 points lower than models designed for longer text. Reviewer 9T2j also asked about if it was necessary to include hard labels in the model, and the authors discussed their role in their rebuttal. This discussion appears less relevant because the authors compared these approaches empirically in Table 4. Reviewer 9T2j did not respond to the rebuttal. \n\nReviewer dygx offered lower scores, and asked questions about the nature of conspiracy theories in their review. They hypothesized about the kinds of reasoning that might lead people to believe conspiracy theories, and the extent to which LLMs might already have representations of such reasoning. These comments from dygx were just a hypothesis without grounding in experiments or prior work, and should not be given too much weight. But the idea of a “conspiracy theory” could have been much more sharply defined in the submission. The authors do seem to show that extracting events and event relations can detect a conspiracy news source in the LOCO dataset. But the work could have been richer and more convincing if the authors used their model to explore the kinds of event relations which may mark conspiracy theories, and the ways in which these patterns can inform our understanding of what constitutes a conspiracy theory. For instance, the authors might provide an exploratory analysis of the kinds of spurious event relations that are common to conspiracy theories.\n\nReviewer dygx also asked an interesting question surrounding soundness. They hypothesized that there might be high-level stylistic differences between conspiracy and non-conspiracy sources which might undermine the findings from the experiments in this paper. If all conspiracy and non-conspiracy sources share stylistic similarities (e.g., if each source is drawn hierarchically from a parent conspiracy or non-conspiracy distribution) then source-based splitting might not be sufficient to check that models are not just learning broad stylistic differences between kinds of news sources. However, in their rebuttal, the authors noted that including event information improved F1 scores in the random split setting (i.e., naively splitting the dataset without considering source). In this setting, the authors argue, event and baseline models each had access to the same stylistic information. But including event relations lead to a 4.22 point increase in F1 score over a vanilla longformer model (80.46 => 84.68). Reviewer dygx did not respond to this argument.\n\nFinally, because of these concerns about stylistic information, the reviewer dygx also noted that they are “not convinced that this model would perform so well with completely new topics.” The authors did attempt to address this comment from dygx in their rebuttal. But their experiments on “topic splitting” were hard to assess because they were lacking detail. The authors do not appear to describe “topic features” in their paper so it is hard to understand this new experiment."
            }
        },
        "id": "4d5pSc3A2k",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OYWeQdQiIn",
        "replyto": "OYWeQdQiIn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1191/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507025,
        "cdate": 1696707507025,
        "tmdate": 1701465423225,
        "mdate": 1701465423225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work utilizes large language models (LLMs) to construct a SalesBot and ShopperBot to simulate both sides in product purchasing. One aim of the work is to introduce a new paradigm for conversational recommender systems (CRS) which, in addition to providing product recommendations, includes customer education.\n\nThis work makes several contributions, including:\n - Presentation of a novel idea of exploring the role of educational value in CRS\n - Demonstrates utilization of LLMs to build CRS\n - Conducts a study that compares SalesBot against professional salespeople\n\nThe result are however largely in line with expectations for LLMs, i.e, while it provides fluent chat responses, the quality of it's recommendations and informativeness are significantly below those of a human salesperson. Additionally, there is a lack of comparisons with other relevant chatbot/CRS systems and analysis of the ablation studies to better inform conversational agent design.|meta review"
            }
        },
        "id": "lXpeWwhCsl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OXQFcwKrTM",
        "replyto": "OXQFcwKrTM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2197/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536709,
        "cdate": 1696707536709,
        "tmdate": 1701465457572,
        "mdate": 1701465457572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the data scarcity problem of long-turn dialogues in pretraining large-scale open-domain dialogue models. The authors introduce a novel and effective \"Retrieve, Reorganize, and Rescale\" framework that can automatically construct billion-scale long-turn dialogues by reorganizing existing short-turn dialogues. The proposed method has been evaluated on Chinese dialogue datasets, effectively demonstrating its efficacy. The authors have committed to releasing their model, toolkit, and data for public use.\n\nThe soundness scores for this paper are as follows: (4, 4, 4, 4, 3). Initially, reviewers expressed concerns that evaluating the method using only a single language might not validate the generalizability of the approach. However, in the rebuttal, the authors presented additional experiments on an English benchmark, which addressed these concerns, leading all reviewers to agree on the robustness of the work.\n\nThe excitement scores for this paper are: (3, 3, 4, 4, 4). The majority of reviewers show enthusiasm for this work, primarily because the proposed data augmentation method can be adapted to various languages without requiring manual intervention. This indicates the significant potential and broad applicability of the work."
            }
        },
        "id": "h0kFC9KKhX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OVt2dIwxR1",
        "replyto": "OVt2dIwxR1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3512/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564243,
        "cdate": 1696707564243,
        "tmdate": 1701465499669,
        "mdate": 1701465499669,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a novel Dynamic Open-book Prompt approach for conversational recommendation, where the open book stores users' experiences in historical data, and the prompt is dynamically constructed to memorize the user's current utterance and selectively retrieve relevant contexts from the open book. Extensive experimental results on the ReDial dataset demonstrate the significant improvements achieved by the proposed model over the state-of-the-art methods.\n\nAll reviewers have pointed out that the evaluation of the generation performance is questionable by using only metrics Dist-n. Although the results of BLEU are added in the rebuttal, its validity should be justified given the multiple acceptable responses that can vary in syntax/lexicon, and some contributions also seem to be overclaimed. Reviewers further suggest using more recent and more powerful PLMs rather than DialoGPT in the experiments. Besides, reviewer MJfa mentioned that the paper lacks clarity in many parts, which need to be fixed.|meta review"
            }
        },
        "id": "FWdxWXz2bJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OVmOQs85Xb",
        "replyto": "OVmOQs85Xb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1387/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511919,
        "cdate": 1696707511919,
        "tmdate": 1701465429376,
        "mdate": 1701465429376,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper present multi-turn spoken conversation transcript cleaning process. They label switchborad corpus with proposed annotation shceme.\n\nPros:\n- paper presents a dataset and baseline experiments\n- labelling schema is well designed\n\nCons:\n- it is unclear if dataset is better for downstream tasks"
            }
        },
        "id": "dc4SLIc9Lb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OVLnZliSHs",
        "replyto": "OVLnZliSHs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2069/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533595,
        "cdate": 1696707533595,
        "tmdate": 1701465452625,
        "mdate": 1701465452625,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "*Summary*: It is fairly well-known that the suboptimal tokenization of text across different languages leads to large variance in the number of tokens required to convey the same content in different languages. In this work, the authors systematically analyze the cost and utility of OpenAI's API on 22 typologically diverse languages. The authors find that text in many languages tend to be heavily fragmented, and consequently incur higher costs and derive lower model utility from the OpenAI APIs. This is an analysis-driven paper critiquing the API pricing policy of charging by tokens that puts over-fragmented languages at a clear disadvantage and suggests ways to move forward.\n\n*Evaluations*: R1 and R2 were very favourable in their review of this work (5/4 soundness scores, 4/4 excitement scores). R3 diverges in their review and has listed many concerns including clarifications needed about the main results in both Figs 2 and 3, details about the BLOOM-Z tokenizer, details about the datasets (FLORES corpus, etc.), the more careful use of ``script\" whenever invoked, and a few other minor points. The authors have addressed most of these concerns."
            }
        },
        "id": "yKj7XP8va8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OUmxBN45Gl",
        "replyto": "OUmxBN45Gl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1374/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511455,
        "cdate": 1696707511455,
        "tmdate": 1701465428877,
        "mdate": 1701465428877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work explores the effect of knowledge memorized in pretraining by language models vs information that is supplied to them in-context, finding that LMs have a strong preference towards memorized information. The authors also explore interesting questions related to the term frequency of these facts in the pretraining data. All reviewers appreciated this line of investigation, and the rigorous analyses in this work."
            }
        },
        "id": "pkmEQNnrrZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OUiW2DzpzT",
        "replyto": "OUiW2DzpzT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4193/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585166,
        "cdate": 1696707585166,
        "tmdate": 1701465523751,
        "mdate": 1701465523751,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Addresses the issue of the need for prompt engineering LLMs and seeks to generate better instructions/prompts using LLMs for tasks automatically to improve downstream performance. Through prompting, an LLM (text-davinci-003) generates diverse instructions, followed by a FLAN-T5 reranking model to select the best ones. There were concerns about novelty, the significance of the improvement and doubts about the ranking performance. \n\nAn unusual KL-divergence loss is used to align ROUGE-L scores of outputs with the classifier likelihood which has some similarity with BRIO (Liu et al, 2022), but this choice is not compared with the simpler approach of a vanilla classifier. Furthermore, the LLM reranker baseline may significantly improve simply by using GPT-4 instead of text-davinci-003.\n\nReliance entirely on automatic metrics for evaluating LLM outputs, such as ROUGE-L, while good to have, do not give a full picture that only human evaluation would complete. Including a human evaluation would significantly improve the confidence in the evaluation."
            }
        },
        "id": "l28YFmZbKM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ORHg3RKho0",
        "replyto": "ORHg3RKho0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4587/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593713,
        "cdate": 1696707593713,
        "tmdate": 1701465535865,
        "mdate": 1701465535865,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a modification for continuous diffusion models by introducing a soft absorbing state, which enables removing the clamping operation in models like diffusion-LM and DiffuSeq. This modification enabled the utilization for DPM-solver++ for speeding up continuous diffusion models applied in text. Experiments are provided for speed comparison to show significant speed-ups compared to other models based on continuous diffusion. The resulting speed-up in roughly equivalent to the speed of discrete diffusion models. The paper requires writing and presentation improvements to clarify some of the points raised by the reviewers."
            }
        },
        "id": "81sYik3MwY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OLcDbSRjbx",
        "replyto": "OLcDbSRjbx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5563/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615158,
        "cdate": 1696707615158,
        "tmdate": 1701465562460,
        "mdate": 1701465562460,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel large-scale benchmark dataset for multilingual machine-generated text (MGT) detection, named MULTITuDE. The dataset is comprised of 74,081 texts in 11 languages and includes both human-written and machine-generated texts. To explore the performance of current state-of-the-art detection methods, the authors conduct a comprehensive multilingual benchmark. They also evaluate the cross-language generalization capabilities of fine-tuned models in a multilingual setting.\n\nMain Contributions:\n\nThe authors introduce MULTITuDE, a new benchmarking dataset for multilingual machine-generated text detection, which includes 7,992 human-written texts and 66,089 machine-generated texts.\n\nThe paper provides a comprehensive multilingual benchmark for state-of-the-art detection methods, offering a detailed study of these techniques across 11 languages and employing 11 LLMs.\n\nThe authors evaluate the cross-language generalization capabilities of fine-tuned models in a multilingual setting, offering valuable insights into the performance and adaptability of these models.\n\nReasons for Acceptance:\n\nThe introduction of the MULTITuDE dataset provides a valuable resource for the research community, fostering further study in multilingual machine-generated text detection.\n\nThe comprehensive benchmarking and evaluation of state-of-the-art detection methods enhance understanding of these techniques and their performance across different languages.\n\nThe authors' exploration of cross-language generalization provides useful insights into the performance of fine-tuned models in multilingual settings.\n\nReasons for Rejection:\n\nThe constructed resource is limited to one domain (news texts), which may limit its applicability to other domains.\n\nThere is a limited amount of training data available for only three languages, and the amount of human-written text is also limited.\n\nThe experimental findings, while extensive, are quite straightforward and expected.\n\nThe quality of the experiments may be questionable due to the lack of hyperparameter optimization, which could affect the reliability of the results and the validity of the claims made in the paper."
            }
        },
        "id": "IaVbxnUtzj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OK5yv6Fhl9",
        "replyto": "OK5yv6Fhl9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1298/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509872,
        "cdate": 1696707509872,
        "tmdate": 1701465426695,
        "mdate": 1701465426695,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a thorough investigation into quantization methods for Large Language Models (LLMs), focusing on 8-bits or less. Reviewers commended the comprehensive comparison against both baseline and block-based methods across multiple task settings, emphasizing the innovative introduction of non-linear quantization strategies. Particularly notable are the impressive results achieved with nearly lossless 6-bit and 4-bit quantized LLMs using Block Floating Point (BFP), surpassing existing methods in arithmetic and memory density. The study's detailed insights, combined with its well-structured presentation, make it a significant contribution to both numerical LLM research and potential applications in hardware accelerators.\n\nPros:\n\nextensive experiments and well written\n\nCons:\n\nCould use more explanation on some ablations"
            }
        },
        "id": "uVmewfZUJl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OGdl9d3BEC",
        "replyto": "OGdl9d3BEC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2422/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541841,
        "cdate": 1696707541841,
        "tmdate": 1701465464553,
        "mdate": 1701465464553,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Originality:** \n\nThis work is the logical next step for integrating LLMs with ASR models. Many similar prior works have explored this space, albeit only very recently. This work differs from others, for instance (Ma et al. 2023), primarily in the scale of the model  (Llama) and data (Gigaspeech) used for fine-tuning the error-correction model, as well as in the type of LLM (decoder only). The more novel aspect of this work is the fact that it gives LLMs more direct access to acoustic representations, through the Whisper encoder, rather than just through n-best lists or ASR outputs, through cross-attention.\n \n**Significance:**\n\nThis work, as well as the topic of leveraging LLMs for ASR error correction is very significant.\n\n**Clarity:**\n\nThe paper is well-written though there are a number of minor grammatical errors that could be fixed. As pointed out by some of the reviewers, some corrections to tables need to be made.\n\n**Pros:**\n   - A new method addressing an incredibly important problem\n   - Thorough experimentation\n   - Promising results\n\n**Cons:**\n   - This method should be evaluated in other settings.\n   - Some minor concerns about reproducibility.\n\n**General Comments**\nLarge pre-trained models such as Whisper and Llama are trained on large amounts of web-scraped corpora. Without knowledge of the training data, It is difficult to ascertain to what extent data leaking (training data in the test sets) is a problem. Evaluation sets must be carefully selected to ensure conclusions hold across data sets, and to limit the potential for data leakage. In this study most results are presented on web-scraped corpora. the results from ATIS are very encouraging, but it would be useful to see results on more datasets.\n\nThe authors seem to have addressed many of the concerns about using Whisper Tiny as well reproducibility."
            }
        },
        "id": "N8efhzdWz6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OETPPc15XG",
        "replyto": "OETPPc15XG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4032/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581608,
        "cdate": 1696707581608,
        "tmdate": 1701465517471,
        "mdate": 1701465517471,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates manga understanding using visual and textual features (completing the missing text when only the visual cues are available). The research is original and the methodology is clear.\n\nPros:\n* The paper shows how to use the complementary textual and visual information present in the comics\n* The methodology that includes visual features and visual prompt using co-attention for sentence completion\n\nCons:\n* Lack of statistical evidence that supports the claims in a real scenario\n* Lack of ablation studies"
            }
        },
        "id": "9UBIsQJz60",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ODeHH5FBwx",
        "replyto": "ODeHH5FBwx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3883/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578430,
        "cdate": 1696707578430,
        "tmdate": 1701465512705,
        "mdate": 1701465512705,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a method based on instruction tuning of LLMs for localized sequence transduction tasks where source and target sequences have substantial overlap. Experiments demonstrate that model performance was improved in several tasks, such as grammatical error correction, paraphrasing, formal style transformation, and simplification.\n\nReviewers agree that the method is simple and effective, and that it could possibly be adopted as a new baseline for the NLP community. It was also suggested that such an approach could encourage researchers to think about alternative concise output representations which are better suited to the task at hand.\n\nSome aspects to improve include a comparison of computing costs, such as how much the memory and time are reduced during inference (which the authors' rebuttal agree with and commit to including). In addition, a couple missing references were pointed out."
            }
        },
        "id": "TTu8s5yWjK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "OC4OLQGtIR",
        "replyto": "OC4OLQGtIR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5432/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612873,
        "cdate": 1696707612873,
        "tmdate": 1701465559317,
        "mdate": 1701465559317,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Authors propose a new tokenization method for language modeling alternative to other sub-word tokenization or byte/char-level methods. Reviewers all acknowledged the merits of the proposed approach but also highlighted the need for a more controlled setup when it comes to the granularity of the learning signal and how it compares with the other methods, since this could be a confounder needs studying and further explanations."
            }
        },
        "id": "CnRAN6JycV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "O9zrG7NB3X",
        "replyto": "O9zrG7NB3X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4421/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589312,
        "cdate": 1696707589312,
        "tmdate": 1701465531203,
        "mdate": 1701465531203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a prompt-based approach to IE where users extract tables of text from documents by providing instructions.  A Llama-7B model is fine-tuned on a dataset of instructions, input documents and output tables generated by ChatGPT.  The approach that is taken seems to be similar to Alpaca, however the model is specifically targeting IE based on user instructions.\n\nThe paper is well written, and the experiments seem convincing.  Reviewers raised some concerns, including the motivation for the new task and regarding the chosen evaluation metrics."
            }
        },
        "id": "83JPCpLql6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "O7eKiJpePJ",
        "replyto": "O7eKiJpePJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5175/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608631,
        "cdate": 1696707608631,
        "tmdate": 1701465551940,
        "mdate": 1701465551940,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "To address the limitations of existing evaluation protocols, which might overemphasize matching with ground-truth items and overlook the interactive nature of CRSs, this paper introduces iEvaLM, which leverages LLM-based user simulators.\n\nThis paper has gone through a thorough discussion among reviewers and many concerns about it have been well addressed. Nonetheless, the paper can still be improved by addressing the following issues:\n1. The proposed iEvaLM might be biased towards the LLM used during evaluation.\n2. Add a discussion of the potential risks of using iEvaLM, as reviewers have concerns about the objectivity and fairness of the proposed method. It would be nice if the authors could add some use hints or guides for readers or practitioners.\n\nThe authors are also encouraged to include all results listed in the rebuttal in their paper if accepted."
            }
        },
        "id": "McOLeKEUlm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "O4kDO3yS9B",
        "replyto": "O4kDO3yS9B",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2578/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545142,
        "cdate": 1696707545142,
        "tmdate": 1701465469609,
        "mdate": 1701465469609,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors \n\nHowever, the proposed analysis shows some drawbacks and limitations:\n1) The novelty of the work seems limited. It is not clear why detecting toxicity in gaming platforms is unique compared to other settings for toxic content detection;\n2) The chat speaker segmentation embeddings should be better described, as well as discussing how it is integrated into the architecture.\n3) The proposed approach slightly improve the performance w.r.t. the other approaches.\n4) Performance varies substantially across toxicity classes and is much worse for certain classes, eg, F1 below 30"
            }
        },
        "id": "mPfgKyuiwv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "O4gELC78Bq",
        "replyto": "O4gELC78Bq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3609/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566175,
        "cdate": 1696707566175,
        "tmdate": 1701465503573,
        "mdate": 1701465503573,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a novel multilingual dataset for sign language processing, leveraging Bible Translations to enhance diversity. The paper also presents baseline results to bilingual pairs and explores the application of multilingual approaches in sign language processing. In general the paper seems like a good contribution, but there might be some issues involving the data licensing that needs to be addressed."
            }
        },
        "id": "Rvsmxzg5Lo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "O36QcmUEDM",
        "replyto": "O36QcmUEDM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2780/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549277,
        "cdate": 1696707549277,
        "tmdate": 1701465476095,
        "mdate": 1701465476095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper under discussion attempts to detect LLM synthetic texts. We hypothesize that LLM synthetic texts, compared to human-generated content, may lack emotion present. The authors hypothesize that this difference can be leveraged to create a more effective synthetic text classifier. \n\nPros:\n1. All three reviewers acknowledge the novelty of the paper's idea – using the emotional difference between human and synthetic content to distinguish between them.\n2. Clear writing and presentation. The proposed method is clearly described. \n3. The datasets composed by the authors are a valuable contribution to the field and are likely to aid future research.\n\nCons:\n1. Incomplete Analysis: Reviewer 1 and 3 highlights a main drawback in that there is insufficient analysis backing the central claim of the paper, particularly about the role of emotion in synthetic text detection.\n2. Validation Concerns: Reviewer 2 brings up concerns about the lack of statistical validation. While the authors respond that the results in Table 2 are an average of five runs, it could be more robust if they reported variance or a significance test."
            }
        },
        "id": "TptNMiMEuZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "O1IEUXd4SI",
        "replyto": "O1IEUXd4SI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1140/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505878,
        "cdate": 1696707505878,
        "tmdate": 1701465421634,
        "mdate": 1701465421634,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces Variator, a parameter-efficient acceleration method for large language models (LLMs) that aims to reduce computational costs and enhance efficiency. Variator utilizes compression plugins to compress multiple hidden vectors into a single representation, shortening the sequence length. Experiments conducted on seven datasets show a 53% reduction in computational costs with only a 0.9% increase in parameters and less than a 2% performance drop.\n\nStrengths:\n* The idea is novel and interesting, with limited additional overhead.\n* The method is technically sound and demonstrates effectiveness across various datasets.\n* The paper is well-written and provides sufficient details to reproduce the results.\n\nWeaknesses:\n* The method has similarities to existing approaches, such as convolutional layers with stride > 1 and token merging, and requires comparisons with these methods.\n* The experimental evaluation needs further clarification and elaboration, including comparisons with other compression models and exploring the method's applicability to a wider range of LLMs.\n* The proposed method may not be suitable for large language models requiring pre-training, as it may violate the acceleration objective.\nThe method's suitability for generation models needs to be addressed.\n\nOverall, the paper has merits and potential and the weaknesses and concerns raised by the reviewers are partially addressed."
            }
        },
        "id": "wEyayzAlLP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NxOeOxe6qs",
        "replyto": "NxOeOxe6qs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2556/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544748,
        "cdate": 1696707544748,
        "tmdate": 1701465468951,
        "mdate": 1701465468951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper studies the task of factual error correction - recovery of a true claim from a misinformed claim by revising the sentence to be factually accurate. The paper contributes a method for generating synthetic false claims which aid in training a factual error corrections system.  \n\nThe paper claims to aid training data sparsity by introducing factual error injection as a task to go generate the synthetic data. This approach seems well motivated. But I would also suggest that the authors compare their work to Cao et al 2020 https://aclanthology.org/2020.emnlp-main.506.pdf in the literature survey who perform artificial corruptions of summaries to train an error correction system. Much like this paper. There is value in the author's paper, especially relying on an LLM rather than a set of text transformations. \n\nThere is reasonable consistency among the reviews for this paper, scoring 3.3 for soundness and 3 for excitement. I think the experiments provided show the the value of the methods proposed by the authors. There could be some scope for error detection and correction from LLMs in a larger context.  The authors rebutted the reviews well. However there reviewers are still concerns about novelty, and quality of generated instances that could be further addressed.\n\nRegarding novelty, the use of the LM as a corruptor is interesting, but other methods for data corruption injection have been identified (e.g. the contrastive method of CLIFF, or the distantly supervised method of cao et al., 2020).  Comparing LLM-generated corruptions might be of interest to the community so I am not sure about the strength of this negative\n\nRegarding quality, I would also encourage the authors to manually verify the generated instances to better understand the fluency and specificity requirements suggested by T+V2021."
            }
        },
        "id": "20tCLwJmwc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Nx9D21g1lW",
        "replyto": "Nx9D21g1lW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1246/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508416,
        "cdate": 1696707508416,
        "tmdate": 1701465425034,
        "mdate": 1701465425034,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a new methodology for covariance matrix estimation in portfolio optimization, which incorporates semantic similarity derived from textual descriptions or knowledge graphs. The innovation of this paper lies in using semantic information to estimate the shrink target matrix. Overall, this could be an interesting application paper."
            }
        },
        "id": "zPX760NSjI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NwJVbDxfTd",
        "replyto": "NwJVbDxfTd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2869/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551132,
        "cdate": 1696707551132,
        "tmdate": 1701465479409,
        "mdate": 1701465479409,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Paper Summary:\nThis paper proposes using large language models (LLMs) as collaborators in a human-in-the-loop framework for thematic analysis of qualitative data. A human coder provides exemplars to teach the LLM to generate codes, followed by joint codebook creation. Experiments show high agreement between human coder and LLM, but lower agreement with a separate human coder.\n\nI will now provide a snapshot of the reviews. Four reviewers were engaged and unfortunately, I'm unable to remove one of the reviewers. I'm leaving this aspect for the SAC to evaluate and decide which one to expunge.  \nSummary of Reviews:\n\nReviewer 1 found the collaborative LLM approach novel. But only one LLM and two datasets make claims preliminary. Lower coder 3 agreement is concerning and prompts bias questions.\n\nReviewer 2 thought the problem motivation and presentation were strong, and called the application domain very novel. However, more comprehensive experiments are needed. Unsure of true technical novelty vs. prompt engineering.\n\nReviewer 3 liked the higher agreement than the gold standard, and the discussion reducing hallucination risks. But felt that some concepts were not clearly explained. The reviewer thinks that metrics like accuracy can be more evaluated in the paper and that missed code analysis is lacking.\n\nR4 noted framework enables teaching LLMs new codes, and the human-LLM agreement was good. However, the topic may be better suited for other venues. More coders are needed in the evaluation. The reviewer also believes that the dataset currently lacks details.\n\nIn general, I find incorporating LLMs into the qualitative analysis workflow quite interesting and relevant given the application area. I think it's an eye-opener towards using LLMs to beat down annotation costs. Moreover, all four reviewers praise the novelty. However, concerns exist around evaluation methodology and broader impact. The agreement between LLM and untrained human coders is notably lower, raising questions about bias and generalization. More importantly, the majority of the reviewers recommend authors' using more datasets, and coders, and probe the transparency around disagreements. I note that the authors' rebuttal to the reviewers has shed more light on this aspect. The authors provided a more detailed analysis of the discrepancies between the LLM-trained coder and the untrained coder 3. They identify sources like code ambiguity and granularity differences that account for many of the disagreements. By calculating agreement at the theme level, they showed improved results to address reviewers' concerns. Particularly, I note that the newly adopted theme-level calculation justifies the lower coder 3 agreement as initially reported and alleviates bias concerns. The additional accuracy metrics are useful as well. Personally, I agree with the reviewers that the potential is apparent but current validation is limited and not fully answered by the reviewers and I'm hoping that the authors can improve these areas of the paper in the final draft after extending their experiments."
            }
        },
        "id": "BACbqAocsd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NuMemgzPYT",
        "replyto": "NuMemgzPYT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1081/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504593,
        "cdate": 1696707504593,
        "tmdate": 1701465419894,
        "mdate": 1701465419894,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces \"ReCEval,\" a novel metric designed to evaluate Large Language Model (LLM) reasoning chains based on their accuracy and informativeness. Using Semantic Role Labeling, reasoning chain units are defined as \"subject-verb-object\" triplets. The metric evaluates the correctness of these units through entailment relationships and uses the Pointwise Value of Information (PVI) to assess their informativeness. The paper emphasizes the importance of ensuring reasoning chains are both accurate and contain non-redundant, valuable information. Experiments were conducted on two datasets, Entailment Bank and GSM-8K. Compared to other metrics, including ROSCOE, ReCEval demonstrated superior performance, highlighting its potential in evaluating the quality of reasoning chains for QA tasks.\n\nThe paper presents a well-structured and novel approach to evaluating reasoning chains, outperforming previous state-of-the-art methods like ROSCOE. The method effectively leverages existing resources in a unique manner. The paper's clarity and comprehensive experimental design, including ablation studies, enhance its credibility. The evaluation metric addressed is of significant importance, and the solutions proposed by the authors are deemed feasible. The multi-dimensional approach of ReCEval, considering both entailment and LM-based pointwise V-information, is well-motivated and comprehensive. The extensive experiments, including various settings and hyperparameters, further validate the metric's potential to improve downstream reasoning chain generation.\n\nSome weaknesses, discussed during the rebuttal, are listed below.\nThe paper's claim about the inadequacy of existing reference-free metrics is not surprising, and the inclusion of a large language model baseline is suggested for a more comprehensive evaluation. The dynamic nature of the proposed metric, relying on an external neural network, raises concerns about its stability and reproducibility. Questions arise about the neural network's ability to detect its own errors and the feasibility of concatenating all premises for evaluating intra-step correctness. The paper's experimental design is limited to only two datasets, questioning its generalizability. The method of selecting the best performance metric among the three sub-metrics is unclear, potentially limiting ReCEval's applicability."
            }
        },
        "id": "h3cOzKCRwQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NtHfJrjkiv",
        "replyto": "NtHfJrjkiv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4243/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585970,
        "cdate": 1696707585970,
        "tmdate": 1701465525342,
        "mdate": 1701465525342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces DocGen, a new paradigm of generating synthetic documents from queries to augment the data for ranking models. An extended version DocGen-RL is further proposed to enhance the relevance between generated synthetic documents and queries using reinforcement learning. All reviewers find this paper strong and exciting with minor concerns on including more experimental analysis."
            }
        },
        "id": "wBB4MF003l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NrmYYAO7N4",
        "replyto": "NrmYYAO7N4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2105/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534541,
        "cdate": 1696707534541,
        "tmdate": 1701465454122,
        "mdate": 1701465454122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The three papers propose methods for extractive summarization of customer-agent dialogs using large language models (LLMs) to generate pseudo-labels for dialog data. The paper's main contribution is the novel use of pseudo-labels for semi-supervised learning, which enables knowledge transfer from the LLMs to specialized models. They demonstrate the effectiveness of their methods on different datasets, achieving good performance with minimal labeled data. However, several concerns about the efficiency of the methods, and uncertainty about guaranteeing extractive summarization in some cases need to be addressed. Overall, these papers propose innovative techniques for dialogue summarization, but certain limitations and clarity issues require further revision."
            }
        },
        "id": "u49nIkiVGS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NrCLVmq0KD",
        "replyto": "NrCLVmq0KD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3865/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578147,
        "cdate": 1696707578147,
        "tmdate": 1701465512306,
        "mdate": 1701465512306,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper provides an in-depth analysis of the CorefUD multi-lingual coreference corpora used in the CRAC2022 shared task, in different linguistic levels (mention, entity and document). Then they did the error analysis on the best-performing system of the shared task to find out the challenges faced by the system. Last they proposed using UD-related features (e.g. UPOS, UD relation) to mitigate the errors. The evaluation on 10% of training data suggests using such features in the baseline system can achieve a marginal improvement of 0.9%. The analysis on the CorefUD corpus and error analysis provides useful insight into multi-lingual coreference resolution. The evaluation, however, is less convincing given that only one system is evaluated on a non-standard test set, which makes it hard to compare with other systems in the future. It is also unclear the reason behind the decision to use CoNLL 2012 F1 instead of the same evaluation metric (CorefUD scorer) used in the CRAC 2022 shared task, This again makes the evaluation non-comparable with the shared task systems even use exactly the same portion of the training set for the test.  The switch between CorefUD 1.1 and CorefUD 1.0 is also a bit confusing, it would be more helpful if the paper stuck to one dataset or at least explained their decision more clearly. There are a few other points that need to be clearly mentioned as well to improve the reproducibility and avoid confusion, e.g. according to the author's response in the discussion the error analysis is done on the dev set. On the UD resources side, (UPOS, UD relations etc.), the author claims during the discussion that those resources that come with the CorefUD are all predicted, but according to the shared task overview paper ( Žabokrtský et al. 2022) in Section 2.1 it mentioned many corpora has UD columns manually annotated (e.g. Prague Dependency Treebank (Czech)) and at the end of Section 2 the organiser mentioned \"With some exceptions, if the original resources contained manual annotation of morpho-syntax, it has been kept also in CorefUD. \" so it does contain manually annotated UD columns which I would request the author to investigate which corpora they used are with manual UD annotations and make it clear that the results they achieved are not benefit from the gold annotation, e.g. there is no distinguish improvements gained from the datasets with gold annotations."
            }
        },
        "id": "5Z3Gl8NILh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NphKIYvm9D",
        "replyto": "NphKIYvm9D",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4635/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594867,
        "cdate": 1696707594867,
        "tmdate": 1701465537280,
        "mdate": 1701465537280,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates the task of generating live sports updates from tweets and proposes a series of T5-based models that controls the number of updates to be generated to mitigate redundancy. Overall, reviewers are rather positive about this submission and agree that the proposed task / models are interesting and that the content is a good fit for a short paper. On the negative side, reviewers point out that the experiments could be strengthened (more baselines, oracle / upper bound performance evaluation metrics)."
            }
        },
        "id": "bTmxL6HSTP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NomitcTG87",
        "replyto": "NomitcTG87",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1380/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511676,
        "cdate": 1696707511676,
        "tmdate": 1701465429110,
        "mdate": 1701465429110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the stability of self-influence scores, which determine memorizability and are task-agnostic, to training and model hyperparameters. Not only does self-influence have intrinsic properties that makes it desirable, the paper also aims to determine its capability towards cleaning noisy NLP datasets by removing outliers that have high self-influence.\n\nThe reviewers like the novelty of the idea and found it interesting, the application strongly motivated. They also commended that the AutoCL method is great.\n\nThere were concerns about the empirical evidence, raised by the most critical reviewers. I believe the authors’ rebuttal adequately addressed the concerns. There were some other questions about synthetic / natural noise which also seem addressed. The most critical reviewer also was convinced by the author rebuttal, which clarified some of the misunderstanding about the empirical evidence. Overall, this idea seems to be fairly unique and novel and exciting, as reflected in the reviews."
            }
        },
        "id": "ANzFGCxMeZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NnVIFpsMAy",
        "replyto": "NnVIFpsMAy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission181/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481233,
        "cdate": 1696707481233,
        "tmdate": 1701465390086,
        "mdate": 1701465390086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new metric, FactSpotter, to evaluate factual faithfulness by how the generated text covers facts in the knowledge base (KB). FactSpotter uses a classifier based on a pretrained LM and trained with pairs or triples and texts with both positive and negative examples. By using ELECTRA, FactSpotter outperformed conventional metrics in correlation to human evaluation results on the 2017 and 2020 WebNLG graph-to-text (G2T) challenges. Due to the necessity of the knowledge base, there is a consideration that FactSpotter may be restricted to specific tasks. However, the authors' response to Reviewer wiwF's question refers to the extension of FactSpotter by converting tables into knowledge bases. Since this approach partially solves the problem, we can judge that the paper should be accepted as a findings paper."
            }
        },
        "id": "Hp3bdPJUwO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NlWH0Kvptf",
        "replyto": "NlWH0Kvptf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission771/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495546,
        "cdate": 1696707495546,
        "tmdate": 1701465409832,
        "mdate": 1701465409832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper provides a qualitative study of how experts in systematic reviews perceive the potential uses and drawbacks of using LLMs to conduct systematic literature reviews in medical research. All reviewers agree that this is a well-written and well-motivated paper tackling an important issue in a robust way. The reviewers agree that the insights derived from the qualitative data are novel and insightful and likely to be impactful for future medical researchers and practitioners. I concur with the reviewers, particularly given the prevalence of medical domain experts (and domain experts from law, education, policy, etc) using LLMs to support research, it is critical to understand how such practitioners perceive the capabilities and limitations of LLMs for such tasks. \n\nReviewers wanted to see a comparative analysis of the performance of different LLMs, and raised concerns about the number of samples, both of which the authors addressed in their rebuttal. The authors should add additional context about the qualitative data collection and analysis methods used to further justify the approach taken.\n\nReviewers raised concerns about the prompting approach taken, which the authors satisfactorily responded to by justifying their approach - that experts in systematic reviews who are turning to LLMs to support their work may not necessarily have expertise in prompt engineering, and thus a qualitative approach that maintains fidelity to the practitioners’ typical work practices is a reasonable approach to take (Salvagno et al., 2023), rather than one that may adopt state of the art practices in prompt engineering, which medical practitioners may not themselves adopt. To address this, I recommend that the authors better contextualize that the qualitative approach taken here is designed to have ecological validity with respect to medical practitioners’ use of LLMs for systematic reviews and the tradeoffs of such use within the social context of medical research practice, rather than making claims about the technical capability of LLMs divorced from such social context (where, for instance, using SotA prompting techniques and conducting comparative analyses of various model capabilities would be more appropriate). \n\nReviewers raised concerns about the sample size, however, for qualitative interview studies such as this, such sample sizes are well within the bounds of acceptable sample sizes. Prior research suggests that sample size is not a sufficient grounds on which to evaluate rigor of qualitative research and should be contextualized with respect to other approaches to establishing validity of the results (Caine, 2014).\n\n\nhttps://ccforum.biomedcentral.com/articles/10.1186/s13054-023-04380-2"
            }
        },
        "id": "f954h9pK2k",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Nk2vfZa4lX",
        "replyto": "Nk2vfZa4lX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3229/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558619,
        "cdate": 1696707558619,
        "tmdate": 1701465490983,
        "mdate": 1701465490983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary:**\nThe paper focuses on analyzing and improving end-to-end (E2E) Speech-to-Text (S2T) models by probing acoustic and linguistic information. The authors introduce a linguistic probing benchmark, Speech-Senteval, and propose the PromptST model, which utilizes prompt learning to address modality gap issues. The analysis of linguistic tasks highlights differences between S2T and Text-to-Text (T2T) models. The proposed model improves S2T performance and linguistic evaluation benchmark on CoVoST-v2 datasets.\n \n**Pros:**\n \n- The paper introduces a novel linguistic probing benchmark, Speech-Senteval, to analyse the speech models (Reviewer 1, Reviewer 3).\n \n- The analysis of linguistic tasks between S2T and T2T models contributes to understanding the behaviors of acoustic encoders (Reviewer 1, Reviewer 2).\n \n- The PromptST model's introduction provides an innovative solution for mitigating the modality gap in S2T models (Reviewer 1, Reviewer 2, Reviewer 3, Reviewer 4).\n \n- The experimental validation of the proposed approach on CoVoST-v2 datasets demonstrates consistent performance improvement over baselines (Reviewer 3, Reviewer 4).\n \n- The paper's methodology of analyzing information processing within models and probing the balance between linguistic and acoustic information is insightful (Reviewer 4).\n \n**Cons:**\n \n- The literature review related to methods addressing the modality gap or fusion techniques is deemed rather brief by some reviewers, potentially affecting the paper's novelty and thoroughness (Reviewer 1, Reviewer 2).\n \n- The benefits of using the proposed efficient approach on the encoder side, while fully fine-tuning the decoder, lack clarity and comparison against alternative strategies (Reviewer 2).\n \n- The analysis primarily focuses on linguistic performance, potentially overlooking the broader implications of the proposed model's improvements across various fields (Reviewer 4).\n \n- The absence of an evaluation on the MUST-C dataset limits the persuasiveness and generalization of the results (Reviewer 3).\n \n- The mechanism by which prompt representations enhance linguistic information learning on the upper encoder layers needs further analysis and clarification (Reviewer 3).\n \nReviewer 1: aEu5,\nReviewer 2: gne1,\nReviewer 3: A846,\nReviewer 4: LoxQ"
            }
        },
        "id": "x46SJo1ln2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Nijnhwu1Uz",
        "replyto": "Nijnhwu1Uz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5704/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617168,
        "cdate": 1696707617168,
        "tmdate": 1701465565365,
        "mdate": 1701465565365,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a comparative analysis of various rendering algorithms utilized in pixel-based language models. The reviewers acknowledged the level of detail in the study and commended the efficiency of the methods employed. They also emphasized the need for further research in this area, considering the growing interest in multimodal language models."
            }
        },
        "id": "Pz3JczJqMp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NiEYKbNnQO",
        "replyto": "NiEYKbNnQO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3343/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560870,
        "cdate": 1696707560870,
        "tmdate": 1701465494646,
        "mdate": 1701465494646,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The overall clarity and significance of the paper are strong as indicated by all reviewers. The proposed method of combining adaptors and prompt tuning is novel, and experimental results are significant."
            }
        },
        "id": "TDkZnAr6le",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ni57pgQVqq",
        "replyto": "Ni57pgQVqq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2109/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534683,
        "cdate": 1696707534683,
        "tmdate": 1701465454291,
        "mdate": 1701465454291,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel unsupervised keyphrase extraction method called SAMRank, which uses the self-attention map of PLMs to determine the importance of phrases. The idea of computing scores based on self-attention map is simple but still novel in this task. More datasets are encouraged."
            }
        },
        "id": "fmQiA7sQzP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NfN3ZDCcsO",
        "replyto": "NfN3ZDCcsO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2873/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551240,
        "cdate": 1696707551240,
        "tmdate": 1701465479603,
        "mdate": 1701465479603,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an end-to-end solution to document image translation.  The encoder jointly takes text and the layout information as input.  The decoder has three step to deal with ordering, sentence boundary, and translation respectively.  Experiments show significant improvement over baselines.   The paper also present a new dataset for this research.  Reviewers raised concerns and the authors responsed in details.  I think the main contribution of the paper is clear: the end-to-end architecture / the three step decoder and the dataset.  The results are positive."
            }
        },
        "id": "SY1CKHgm1E",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NeOsOzNMiS",
        "replyto": "NeOsOzNMiS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2349/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540127,
        "cdate": 1696707540127,
        "tmdate": 1701465462216,
        "mdate": 1701465462216,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a novel BERT-type pre-training model that learns to transform output hidden states with knowledge about hypernymy, hyponymy, and antonymy (from WordNet) injected. They trained the model from scratch on English Wikipedia and BookCorpusOpen, comparing to BERT. The proposed method is interesting, evaluated on prompt completion and monotonicity NLI (MoNLI). The amount of improvements while did not impress some of the reviewers, have be in reasonable or expected range of improvement as similar research that also attempts modifications for BERT-type of encoder models. At the same time, this work has demonstrated novelty and technical contributions that *ACL/EMNLP conferences are looking for, offers a good reference point for future work on pre-training models with external semantic knowledge, and should be encouraged for publication."
            }
        },
        "id": "2jBwchyKw7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Nc6U1Z0DDt",
        "replyto": "Nc6U1Z0DDt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3562/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565295,
        "cdate": 1696707565295,
        "tmdate": 1701465501589,
        "mdate": 1701465501589,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Plug-and-play retrieval + in-context learning-based dialog generation for knowledge grounding. While the method is intuitive, the paper lacks a proper demonstration of how each component of the model contributes to a high performance."
            }
        },
        "id": "WovTOv1sgK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NbkVQsbaqJ",
        "replyto": "NbkVQsbaqJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3601/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565961,
        "cdate": 1696707565961,
        "tmdate": 1701465503104,
        "mdate": 1701465503104,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper examines the inductive reasoning ability of language models in dialogue-based tasks by analyzing the semantic information gap between the context and the inference required. The authors propose a contrastive learning approach to improve model performance. They conduct thorough experiments and evaluations, including human evaluation, to support their claims.\n\nThe reviewers generally find the topic interesting and well-motivated. They appreciate the clarity and thoroughness of the experiments and analysis. The proposed method is considered simple and easy to implement.\n\nHowever, there are some concerns raised by the reviewers. One reviewer points out that the average performance improvements on the test set are minor and suggests analyzing why NLI metrics don't improve. Another reviewer questions the consistency of the results across different metrics and asks for further analysis. There are also remarks about inconsistent conclusions, weak baselines, and the need for more recent models and few-shot learning strategies.\n\nIn terms of the contributions, the human-annotated dialogue dataset for commonsense inference task difficulty classification is considered valuable. The framing of the research question is also seen as interesting. The contrastive learning approach shows consistent improvements.\n\nOverall, while there are some weaknesses and areas for improvement, the paper provides sufficient support for its major claims. The reviewers have mixed levels of excitement, with some suggesting further revisions. But none of them explicitly object to accepting the paper if co-reviewers support it.\n\nIn summary, the paper addresses an important research topic, presents a valuable dataset, and proposes a promising method to improve language model performance in dialogue-based tasks. Some revisions and clarifications are recommended to address the concerns raised by the reviewers."
            }
        },
        "id": "7DO9Muyxr5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Na4DonsjLx",
        "replyto": "Na4DonsjLx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission659/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493145,
        "cdate": 1696707493145,
        "tmdate": 1701465406753,
        "mdate": 1701465406753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**quality, clarity, originality**\n\nThe paper and its  handling of the topic of model editing is high quality. The writing was very clear and provided clarity in a number of aspects of dealing with ways to edit language models and impacts on performance. The reviewers appreciated the depth of the work as well as how thorough the authors were in handling the topic. The questions that touched on encoder only architectures were answered and context provide to other literature.\n\nThe originality of the work in creating a uniform field to explore the language editing approaches expands on the novelty and provides insight for readers and field. Benchmarks will prove useful for others to work to emulate or build upon. \n\n**significance**\nThe topic itself is very important to the NLP community especially as more and more model reuse is accelerating. The work is significant as it provides insight into the model editing area and also provide data benchmarks that can be reused by others. This serves as a great resource for the field. \n\n**notes for authors**\n\nPlease do make sure that the salient points such as providing an overview on encoder models as well as descriptions/discussioins of failures on the editing are documented in the paper itself."
            }
        },
        "id": "rynUyoZI6b",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NZZB3UGcd8",
        "replyto": "NZZB3UGcd8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1409/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512436,
        "cdate": 1696707512436,
        "tmdate": 1701465429969,
        "mdate": 1701465429969,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper is built on top of the recent progressive relational graph neural network (PRGNN) methods for KG link prediction that has been proven to be quite effective. Specifically, this paper identifies two weaknesses of existing PRGNN methods and propose novel strategies to mitigate those. Experiments on multiple standard datasets with small KGs (WN18RR/FB15k-237/NELL-995) demonstrated the effectiveness of the proposed method, often setting a new state of the art on the datasets.\n\nStrengths:\n- The paper is well motivated and well executed. PRGNN is indeed a strong performing family of methods, and further solid improvement is meaningful. \n- The writing and presentation generally make it easy to follow the logical flow and understand the main points\n- The experiment results are impressive, considering that the performance on these datasets has largely saturated. \n\nWeaknesses:\n- As several reviewers pointed out, the paper was lacking an efficiency analysis, which is important considering that the proposed method adds new components. The authors are encouraged to incorporate that into the revised version.\n- Evaluation on larger KGs such as YAGO and those in the Open Graph Benchmark would significantly strengthen the paper.\n- The related work discussion is a bit insufficient for this specific topic (though the most related work seems to have been covered). The authors are encouraged to strengthen this part the revised version."
            }
        },
        "id": "ikYILN7HoW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NZK22y40DS",
        "replyto": "NZK22y40DS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2357/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540299,
        "cdate": 1696707540299,
        "tmdate": 1701465462533,
        "mdate": 1701465462533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a KBQA dataset with the main focus of numerical reasoning (and also multi-hop). Authors showed limits of existing resources and propose a novel resource to address this issue."
            }
        },
        "id": "ds2xsUxIks",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NYstQhld8J",
        "replyto": "NYstQhld8J",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3198/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558008,
        "cdate": 1696707558008,
        "tmdate": 1701465489963,
        "mdate": 1701465489963,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper \"Comparing Biases and the Impact of Multilingual Training across Multiple Languages\" presents work on detecting bias similarities and differences across several language and based on monolingual and multi-lingual training data. \n\nArguments against the paper raised by the reviewers are related to details in the explanation of several aspects of the paper. \nIn general, the reviewers are very much in favour of the paper and see the presented work as an important topic and that the paper addresses this paper in a very appropriate way."
            }
        },
        "id": "8InQbKMOWz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NYlL3oACU2",
        "replyto": "NYlL3oACU2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1941/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530437,
        "cdate": 1696707530437,
        "tmdate": 1701465447933,
        "mdate": 1701465447933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers collectively see merit in this comprehensive survey, both in terms of the content and style of the paper. The main issues seem to be related to the 8-page length restriction. Because of this restriction, more detailed descriptions, analyses, and examples are lacking, which may be added if the paper is revised as a journal article.\n\nPaper Topic And Main Contributions:\n\nThis paper provides a comprehensive survey of narrative understanding tasks, encompassing reading comprehension, summarization, and question answering. It explores various aspects of narrative understanding, including taxonomy, datasets, evaluation measures, and potential applications. Additionally, the authors propose a novel framework leveraging Large Language Models (LLMs) to tackle narrative understanding tasks, emphasizing the estimation of latent author thoughts using task descriptions and prompts.\n\nReasons To Accept:\n\n* Comprehensive Survey, Useful Resource, and Insights for Future Directions\n    * The paper provides an in-depth and comprehensive survey of narrative understanding tasks, making it a valuable resource for both new and experienced researchers in the field.\n    * The paper serves as a useful resource for researchers by providing an index of narrative understanding datasets, facilitating easy access to essential research materials in the domain.\n    * The paper offers valuable insights into the future directions of narrative understanding tasks, including the proposal of a Bayesian estimation approach for understanding the author's thoughts. This forward-looking perspective adds significant value to the paper.\n* Clarity and Accessibility\n    * The write-up is clear and easy to follow, enhancing its accessibility to a wide range of readers.\n\nReasons To Reject:\n* Maybe Better Suited as a Journal Article: \n    * Because the paper attempts to cover a broad range of narrative understanding topics, this exceeds the depth that can be provided in an 8-page conference paper. This may suggest the need for a narrower focus or a submission as a comprehensive journal paper.\n* Issues with the Proposed Framework: \n    * The paper introduces a framework for narrative understanding but does not provide any evaluations or practical implementations of this framework, which leaves its effectiveness unproven.\n    * The authors could have strengthened the paper by providing a more fine-grained categorization of tasks and datasets, considering factors such as dataset size, annotation procedure, domain, period, language, etc., which would have added depth to the survey.\n* Need for More Analyses and Examples: \n    * The paper lacks quantitative and qualitative studies that would allow for objective comparisons among the works discussed. This absence of empirical evaluation or analysis reduces the paper's contribution.\n    * While the paper discusses the Bayesian estimation of the author's thoughts, it lacks a concrete task-specific example, making it challenging for readers to understand how this concept can be applied in practice."
            }
        },
        "id": "R768eKwg4J",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NXXrvcilq8",
        "replyto": "NXXrvcilq8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4527/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592195,
        "cdate": 1696707592195,
        "tmdate": 1701465533657,
        "mdate": 1701465533657,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes an approach to produce personalized recommendation through multi-turn dialogue. User interests are represented hierarchically through a tree during dialogue, which is pruned and used to make recommendations (in a policy learning phase).\n\n**Pros**: Reviewers agree the paper introduces new concepts (hierarchical interest tree and associated policy learning) that provide practical benefit to the to the conversational recommendation problem. Some reviewers note the solution has \"interesting\" new ideas, is more \"user friendly\", and has extensive evaluation (4 benchmarks).\n\n**Cons**: Most reviewers raised technical concerns including: missing ablations, the use of simulated data, statistical significance of results, consistency of improvements compared to baselines as well as some concerns about motivation/hypotheses. Still, during the rebuttal authors appear to address many of these concerns. All reviewers acknowledge these responses post-rebuttal, and no review indicates serious technical concerns in the scoring. At the same time, most reviewers are ambivalent - there is no champion among the reviewers.\n\nOverall a fairly middle ground paper: reviewers technical concerns are not serious enough to warrant low scores, and excitement among reviewers is not high enough to elicit a champion."
            }
        },
        "id": "Y4wtV3A1Gj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NW09xt3kvH",
        "replyto": "NW09xt3kvH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2963/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553017,
        "cdate": 1696707553017,
        "tmdate": 1701465482532,
        "mdate": 1701465482532,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper evaluates ChatGPT for the tasks of out-of-domain intent discovery and generalised intent discovery. They show that while ChatGPT performs well in zero-shot settings. However, it still does not outperform finetuned models. \n\nThe reviewers were positive about this work and appreciated that this is the first work to evaluate ChatGPT's capabilities in out-of-domain (OOD) intent discovery and generalized intent discovery (GID). The following concerns were raised by the reviewers:\n\n1. Lack of variations in prompts which may affect the performance of ChatGPT - the authors address this concern partially in the rebuttal and try experiment with more variations in the prompts. \n2. Only one dataset was used - the authors have addressed this by providing additional results on other datasets\n3. No thorough analysis of the results and no investigation on why ChatGPTs' performance is poor - the authors mention this is beyond the scope of the paper but I do not fully agree.\n4. Models other than ChatGPT have not been tried - Given that CHatGPT was state of the art at the time this work was undertaken and considering the costs of experiments with multiple LLMs I think it okay to keep the scope limited to GPT.\n\nReviewer x2rM had raised some other concerns but those have been clarified. \n\nOverall, I find this paper to be interesting and worth publishing. I would request the authors to address the following in the final version:\n\n1. Add all additional results shared during the rebuttal phase to the main body or appendix of the final paper, as appropriate.\n2. Honor their commitment made during the response period of making the code, data and evaluation scripts publicly available.\n3. Address some of the concerns related to investigating the reason for poor performance."
            }
        },
        "id": "wCQ9F9xcQr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NT4ehxCifo",
        "replyto": "NT4ehxCifo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1675/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707523059,
        "cdate": 1696707523059,
        "tmdate": 1701465438086,
        "mdate": 1701465438086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the role of speaker information in the task of dialog act classification. It proposes to use speaker-information using graph neural networks to improve the utterance representations. Experiments on both dyadic and multi-participant conversations demonstrate the utility of the proposed model. \n\nOverall, the paper, while well-written, suffers some concerns with respect to novelty. In particular, discussions amongst reviewers and authors revealed the existence of many related approaches that share the same motivation – using/encoding speaker information using some form of graph network. Given minor differences, strong justification for novelty was missing. \n\nAnother valid suggestion revolves around space optimization where space could be used to perform a better comparison to graph-based baselines and also non-GAT-based speaker-encoding baselines."
            }
        },
        "id": "065lEJiMiA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NPkkvrv2Vp",
        "replyto": "NPkkvrv2Vp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1173/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506620,
        "cdate": 1696707506620,
        "tmdate": 1701465422635,
        "mdate": 1701465422635,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes and investigates the hypothesis that the likelihood assigned by a language model to a task-specifying prompt is a useful predictor of the performance of that model on the task that the prompt specifies, such that among intuitively similar prompts, one can/should pick the highest-likelihood prompt under the model to maximize performance in expectation. The reviewers were overall positive, despite some concerns about the scope of experiments (no instruction tuned / RLHF’d models) as well as uncertainty about how far the hypothesis can hold true (as perplexity alone certainly isn’t the only factor.) Overall, it seems that the claims of the authors are interesting enough and backed up enough by their experiments for a good empirical takeaway.\n\nThe reviewers collectively had concerns as to the seemingly causal claims, or at least rather strong correlational claims, that lower prompt perplexity implies better few-shot performance, whereas certainly this is only true within a certain (even if just intuitive) bound of task-relevance or task-descriptive quality; one can likely always find prompts with increasingly high probability under the model with increasingly little to do of the task of interest, which would not work. This is an important nuance that I encourage the authors to engage more seriously with, starting in the abstract. Another related nuance that deserves mention is that this work deals exclusively with the perplexity of the prompt under the model of interest, which is related to the particularities of how the model has learned (or failed to learn) and is distinct, e.g., from the “true” probability/perplexity of the prompt under the training distribution (which is mentioned in the motivation).\n\nThanks for doing Bonferroni correction, authors!"
            }
        },
        "id": "YpGCmfNUZC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NPJznfA7ZC",
        "replyto": "NPJznfA7ZC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4500/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707591577,
        "cdate": 1696707591577,
        "tmdate": 1701465533202,
        "mdate": 1701465533202,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a Chinese corpus on cognitive distortions including testing some techniques for capturing cognitive distortions and some multi-lingual analysis for detecting depression and PTSD. \n\nOverall strengths include a new dataset that is likely to be useful in the NLP for mental health space and an approach to create such data that could be useful in other languages while weaknesses include a lack of evaluation on the quality of human experts and a lack of integrating cognitive distortions, the main feature of the Chinese dataset, into the multilingual analyses."
            }
        },
        "id": "bziu9HfHo0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NO5dc8Ljvj",
        "replyto": "NO5dc8Ljvj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission189/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481440,
        "cdate": 1696707481440,
        "tmdate": 1701465390593,
        "mdate": 1701465390593,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The main contribution of this paper is formal definition of the \"distributional hypothesis\" which allows the authors to set up a synthetic dataset on which to measure the impact of this phenomena on language models. They conclude that this definition of semantic equivalence explains partially the models' sample efficiency (but does not fully explain their generalization capability).\nAs one could expect, a set-up which aims to measure this on real data is less conclusive which might be explained due to a myriad of cofounding variables.\n\nThis type of contribution is compelling due to its originality, and this was mentioned by the reviewers. The main drawbacks is the limited conclusions that can be drawn from such a synthetic set-up, something that is expected from a first work in this direction."
            }
        },
        "id": "Yvf1cEy2vd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NMMnxhQm01",
        "replyto": "NMMnxhQm01",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1265/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508839,
        "cdate": 1696707508839,
        "tmdate": 1701465425598,
        "mdate": 1701465425598,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces three data input formulation approaches aimed at enhancing translation performance during the fine-tuning process.\n\n**Pros**:\n- The proposed techniques are straightforward and consistently demonstrate effectiveness for both high-resource and low-resource languages.\n- The authors conducted exhaustive experiments to understand why and when the proposed data reformulation techniques work.\n\n**Cons**: The only drawback of this work is that when fine-tuning large foundation models, it still falls short of the performance achieved by dedicated multilingual translation models such as NLLB. Additionally, the proposed technique is not compatible for models like NLLB. Nevertheless, I believe this work retains its significance given the prevalent practice of fine-tuning language models with instructional input. The authors have adeptly addressed other queries and concerns raised by the reviewers."
            }
        },
        "id": "TyCztgXkDC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NMMRH80gha",
        "replyto": "NMMRH80gha",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2114/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534832,
        "cdate": 1696707534832,
        "tmdate": 1701465454516,
        "mdate": 1701465454516,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Reject"
            },
            "comment": {
                "value": "The paper proposes a more paremeter-efficient version of the Transformer model by factorizing the feedforward layers into multiple heads with a merging/gating mechanism inspired by the multihead attention in the self- and cross-attention layers. The results show better translation quality with the same number of parameters when evaluated on WMT14 en-de, WMT16 en-ro datasets (which are rather outdated) and several language pairs of WMT17 dataset.\n\nThe reviews view the paper as technically sound (all scores were 3, during the discussion period, one reviewer incresed their score to 4), but moderately exciting (all scores are 3)."
            }
        },
        "id": "QhxhK1yOmz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NHeAUKlTO8",
        "replyto": "NHeAUKlTO8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2584/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545305,
        "cdate": 1696707545305,
        "tmdate": 1701465469948,
        "mdate": 1701465469948,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses the topic of data augmentation for Grammatical Error Correction (GEC). They introduce two measurements of data quality – Affinity (based on KL-divergence) and Diversity (measured in terms of entropy). They propose a MixEdit loss function that optimizes the model using more appropriate pseudo data based on the two metrics.\n\nExperiments on English and Chinese benchmarks show that high Affinity seems to improve model performance, but there is no consistent trend for the Diversity metric performance-wise. They conclude that a data augmentation strategy should be characterized by high Affinity and appropriate Diversity.   \n\nThe paper could be substantially improved by strengthening  the argument regarding the utility of the proposed metrics. Specifically, it  is not clear  how to quantify “appropriate diversity” for a dataset. The paper should make a better effort to explore this issue and to make a more convincing argument regarding the utility of the metric. The individual reviews have specific suggestions about this. The paper would also be strengthened significantly if experiments on other languages were added."
            }
        },
        "id": "BoIOqok5fo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NBH3x0u5oQ",
        "replyto": "NBH3x0u5oQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2561/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544826,
        "cdate": 1696707544826,
        "tmdate": 1701465469115,
        "mdate": 1701465469115,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "A Point-wise Mutual Information-based technique to measure and improve the faithfulness of generated response with the grounding document and dialogue history. The paper is well written. Please add addressed feedback in the final version."
            }
        },
        "id": "uvyp5BG4S6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "NAmRjAIMkz",
        "replyto": "NAmRjAIMkz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5729/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707617475,
        "cdate": 1696707617475,
        "tmdate": 1701465565879,
        "mdate": 1701465565879,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a scholarly corpus containing 73K peer reviewed publications from various computational linguistics conferences and workshops from the past seven decades. It massively extends upon existing corpora, whose contents may now be considered obsolete and includes topic labels for a subset of publications that may benefit downstream research on topic classification. Reviewers were pleased with the design of the corpus, the data collection and curation methodology (two 3s, two 4s for Soundness), and the inclusion of case studies demonstrating its utility and impact (e.g. the corpus may be used to observe waning and resurgence trends among particular topics). There were some concerns over the format used (Parquet) and the lack of labels for every item in the corpus, but these were addressed to reviewer satisfaction during the author rebuttal period. Lastly, though corpus creation is not particularly exciting research, the Excitement scores were still relatively high (two 3s, two 4s). One reviewer also suggested this submission would be better as a short paper, but I believe the authors’ defense to be sufficient. I would, however,  advise authors to consider extending their bullet point list of contributions to also include those mentioned in the rebuttal.\n\nIn summary, only minor revisions, addressing reviewers’ comments and questions, need to be made to ensure this paper is camera ready."
            }
        },
        "id": "bEHxpDmAuH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "N924k3YM8V",
        "replyto": "N924k3YM8V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2901/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551902,
        "cdate": 1696707551902,
        "tmdate": 1701465480634,
        "mdate": 1701465480634,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces an artificially developed French corpus to better understand the circumstances under which transformer models capture gender information. Its findings are especially impactful, suggesting that gender biases can persist even when training corpora appear fully balanced. This contradicts previous research which often attributes gender biases to imbalanced training data. While the paper was acknowledged as sound (two 4s, one 3), there are potential issues in regards to the extensibility of this work to other languages and the lack of engagement with existing research on gender bias. The authors addressed these concerns in their rebuttals, however, and acknowledged that missing discussions would be included in the final version of the paper.\n\nIn light of this, only minor revisions, addressing the aforementioned concerns, need to be made to ensure this paper is camera ready.\n\n*As an additional note for the authors, I highly recommend adding to the title and at minimum, the abstract that the corpus was made for French. I would also like to reiterate Reviewer WdXg's recommendation to explicitly state how you believe \"experiments on artificial data will ultimately lead to 'understanding the biases in the models used today'.\""
            }
        },
        "id": "aGWtB4oOte",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "N8nQjYuyhO",
        "replyto": "N8nQjYuyhO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4552/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592839,
        "cdate": 1696707592839,
        "tmdate": 1701465534458,
        "mdate": 1701465534458,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper work presents a Chinese-centric machine translation evaluation dataset paired with 11 other languages. It covers 6 domains and is filtered by hand to ensure quality. This is a valuable resource since most other evaluation datasets are either not curated or not Chinese-centric, but it remains unclear if the proposed resource provides any practical advantages over other evaluation datasets like Flores or Tatoeba.\n\nDuring the author response period, the authors showcased the effectiveness of their benchmark on actual machine translation models, and they also promised to provide a comparison between their dataset and Flores, which hopefully addresses some criticisms made by the reviewers."
            }
        },
        "id": "qoTW9jG3fl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "N8TTwaIBId",
        "replyto": "N8TTwaIBId",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3096/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556032,
        "cdate": 1696707556032,
        "tmdate": 1701465486819,
        "mdate": 1701465486819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "***Quality***: Reviewers largely agreed that this work investigates a promising research topic and makes research contributions. Several reviews noted that the experimental evaluation did not sufficiently ablate performance improvements to differentiate improvements from data augmentation versus re-ranking. The authors performed additional experiments during the rebuttal period to address these issues. The authors provided sufficient materials to allow reproducibility of their findings.\n\n***Clarity***: Reviewers found the ideas to be clear, although suggested some details and discussion be added to improve clarity.\n\n***Originality***: The core idea of this paper (data augmentation using LLMs) is an area of active research, and is not novel. The application to contextual NER is an original application of this research approach.\n\n***Significance***: Improvements from the proposed method on the NER task investigated were small, but the task area is mature and small improvements are still meaningful. This paper is of interest to the EMNLP community and provides a useful contribution. Overall, the paper did not generate strong excitement from the reviewers, so the impact to the community may be small.\n\n***Pros***:\n* Promising research direction for contextual NER\n* Clear ideas and a novel research contribution\n* Reproducible experiments supporting the claim \n\n***Cons***:\n* Additional experiments necessary to more clearly support the method\n* Extending to other datasets with different NER tags may be difficult"
            }
        },
        "id": "o9fjLWPzog",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "N7R2emgl67",
        "replyto": "N7R2emgl67",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5458/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613366,
        "cdate": 1696707613366,
        "tmdate": 1701465559961,
        "mdate": 1701465559961,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduce a partially machine-generated new dataset, named ROME, to evaluate the reasoning capability of VLMs to correctly interpret counter-intuitive content. All reviewers agreed that evaluating VLMs under counter-intuitive content is interesting. However, the reviewers also pointed out that the scale of the dataset is relatively small and raised some concerns on the annotation process."
            }
        },
        "id": "LDVLpIYaI3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "N6sXsHuWDE",
        "replyto": "N6sXsHuWDE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2966/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553056,
        "cdate": 1696707553056,
        "tmdate": 1701465482640,
        "mdate": 1701465482640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper under review addresses a key issue in the scientific community - improving the quality of peer reviews. It focuses on the aspect of substantiation, which is defined as the degree to which claims in a review are supported by evidence. The authors frame the issue as a claim-evidence extraction task and introduce a new dataset (SubstanReview) for this purpose. This dataset, although small and domain-specific (550 reviews from NLP conferences), is the first of its kind, and the reviewers acknowledge its potential value to the NLP community. The creation of this dataset and the development of a system to automatically analyze the level of substantiation in reviews are the main contributions of the paper. The reviewers generally agree that these are significant and valuable contributions. They also commend the clarity of the paper and its potential applicability beyond the task-specific domain.\n\nHowever, the reviewers also raise some concerns. Firstly, the reviewer's main concern was on dataset's limitedness being applied to NLP models. Secondly, the evaluation of the system is seen as limited due to the domain-specific nature of the data collection. Questions were also raised about the generalizability of the findings and the potential bias in the baseline sentiment classifier, which is based on tweets. Thirdly, some of the claims made in the paper were seen as lacking sufficient support. For instance, the substantiation score is not discussed extensively in the paper, and the relationship between the number of claims with evidence and the level of substantiation is assumed without systematic proof.\n\nOverall, while the paper makes a significant contribution to the field and could potentially impact the way peer reviews are conducted, there are several important issues that need to be addressed to enhance the robustness and generalizability of the findings. The reviewers' scores reflect these mixed feelings, with some expressing strong support and others being more ambivalent or even critical. Regardless of the concerns discussed by the reviewers, this paper has substantial merits that can affect the broader communities of NLP."
            }
        },
        "id": "m0lNjQcoSe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "N6f1iHjWvB",
        "replyto": "N6f1iHjWvB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission537/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490154,
        "cdate": 1696707490154,
        "tmdate": 1701465402575,
        "mdate": 1701465402575,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper looks at the diversity of outputs generated by LLMs. They note that generation tends to lack diversity when it comes to text generated from underrepresented groups. They develop a new metric to measure diversity and then present a prompting technique to improve on this metric. Overall, the reviewers found this work to be sound and exciting. This is a very important application area. Two of the reviewers mentioned a cost analysis that would be helpful to be included. They reviewers also mentioned the need for some open source code/models to be able to reproduce the study. They also mention important limitations to discuss in the paper including the type of diversity, cost, and access to diverse knowledge. Even with these limitations, this paper makes a solid contribution with thorough automatic and human evaluation."
            }
        },
        "id": "pOKzWZo3nH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "N58BZj5JB7",
        "replyto": "N58BZj5JB7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4051/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581973,
        "cdate": 1696707581973,
        "tmdate": 1701465518277,
        "mdate": 1701465518277,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers agreed that the paper addresses an important new topic by creating a new dataset of perturbed student essays and highlighting the vulnerability of current AIGC detectors. They found the paper mainly to be sound and well-executed, although there were suggestions that including a human evaluation would be helpful and more baselines from previous work could be included. Other criticism concerned improvements in the presentation of the content that could easily be fixed in a final version."
            }
        },
        "id": "UJoR8Tk3LB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "N4VUOeVOfS",
        "replyto": "N4VUOeVOfS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4844/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599302,
        "cdate": 1696707599302,
        "tmdate": 1701465543146,
        "mdate": 1701465543146,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This manuscript proposes a method called Hierarchical Hint Assistant for optimizing the performance of LLMs on web navigation tasks. The effect of the proposed method is verified by experiments on the Webshop dataset.\n\nInitially, reviewers were concerned about the representativeness of the LLMs tested. The authors addressed this issue by providing results on ChatGPT. Ablation results were also added in the rebuttal. The paper can be further improved by adding details of how SUMMARIZER works, details about the baseline models, and details of the experimental setup."
            }
        },
        "id": "p4OpZmTm3s",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "N3a2vVk8vu",
        "replyto": "N3a2vVk8vu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5328/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610943,
        "cdate": 1696707610943,
        "tmdate": 1701465556226,
        "mdate": 1701465556226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates how we can use LLMs to clean noisy Debatepedia for query-focused summarization using rule-based filtering and query rewriting. Reviewers agree that the experiments have demonstrated the effectiveness of the proposed approach, and this could have potential impacts if we can use LLM to fix annotation errors for broader tasks. The author response has clarified the reason why it reports both in-domain and out-of-domain evaluation and additional GPT-4 based and human evaluation to show the improvements and query quality. The paper is narrow in scope which limits its excitement. In addition to the new results, we also strongly encourage authors to incorporate the feedback to improve the paper clarity."
            }
        },
        "id": "2qUfDEJ6Cb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MzDakXdBbM",
        "replyto": "MzDakXdBbM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1154/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506125,
        "cdate": 1696707506125,
        "tmdate": 1701465421907,
        "mdate": 1701465421907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main concern raised by the reviewer are that the code and data are not publicly available. The authors did make the code and data privately available for review but the reviewers felt that is not enough (and I agree). Apart from this the paper needs a few changes:\n\n1. Need to report results on public datasets [the authors do promise that they will report results on 2 public datasets but the results on only 1 were provided during the response period)\n2. More details on the manner in which the training dataset was curated.\n3. Comparison with more recent related work (such as [5] as suggested by the authors)\n4. The presentation could be better.\n\nI request the authors to address the above concerns in a subsequent revision."
            }
        },
        "id": "pMunMksvv5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MyTyc69kKK",
        "replyto": "MyTyc69kKK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5887/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707620870,
        "cdate": 1696707620870,
        "tmdate": 1701465568939,
        "mdate": 1701465568939,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agreed on the importance of analyzing and understanding the structural similarity between arguments to assess their quality. They found the approach novel and sound. The reviewers also appreciated the author rebuttal which clarified the issues highlighted in the reviews."
            }
        },
        "id": "c6sj1YaWrm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "My6Rgv7xXV",
        "replyto": "My6Rgv7xXV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3568/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565379,
        "cdate": 1696707565379,
        "tmdate": 1701465501916,
        "mdate": 1701465501916,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a framework for evaluating the human-like behaviors of foundation models, which are large-scale machine learning models that can be adapted to various tasks. The framework, called RealBehavior, measures the reproducibility, consistency, and generalizability of the models’ behaviors using psychological theories and tools."
            }
        },
        "id": "KzEjq7snss",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MxhTQC9AYV",
        "replyto": "MxhTQC9AYV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1892/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529523,
        "cdate": 1696707529523,
        "tmdate": 1701465446002,
        "mdate": 1701465446002,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Overall, the reviewers are positive about the paper, highlighting its novelty, innovative approach, and strong performance in intent classification. However, Reviewer 2 raises some concerns about the intent role labeling, the use of the RoBERTa-base model, and the treatment of positive and negative pairs in the contrastive learning. Reviewer 3 also mentions the lack of investigation on intent classification and contrastive learning and suggests comparing with other zero-shot intent-classification models. Reviewer 4 appreciates the novelty of the method but suggests comparing it with similar published work and the state-of-the-art.\n\nIn terms of pros, the paper is commended for proposing a unique pre-training approach for text encoders, leveraging contrastive learning and intent pseudo-labels. The method is considered inspiring and has the potential for other NLP tasks. The motivation behind the methodology is clearly stated, and the experiments show significant improvement in performance over the state-of-the-art pre-trained sentence encoder. Additionally, the paper is well-written, has a clear structure, and provides sufficient support for its claims.\n\nIn terms of cons, the concerns raised by Reviewer 2 regarding intent role labeling, the RoBERTa-base model dependency, and the treatment of positive and negative pairs in contrastive learning should be addressed. Reviewer 3 suggests conducting further investigation and comparing with other zero-shot intent-classification models. Reviewer 4 suggests comparing the proposed method with similar published work and the state-of-the-art."
            }
        },
        "id": "40HJrUllYM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Mtgbc9XFPU",
        "replyto": "Mtgbc9XFPU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission481/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488852,
        "cdate": 1696707488852,
        "tmdate": 1701465400747,
        "mdate": 1701465400747,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers found the problem interesting and the subject of the paper novel. The methodology was seen as fairly comprehensive and sound overall, and the paper’s writing is clear and easy to follow. There were some concerns about the evaluation setup, but these appear to be addressed by the authors in their rebuttals, so I believe the soundness should not be negatively affected by most of these evaluation issues. One reviewer still felt that the scope of the paper was a bit limited as the paper focused on a single task. None of the reviewers were particularly excited about the paper (all felt ambivalent)."
            }
        },
        "id": "8bTCPOrXg9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Mte6BK69zv",
        "replyto": "Mte6BK69zv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1548/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517110,
        "cdate": 1696707517110,
        "tmdate": 1701465434043,
        "mdate": 1701465434043,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method for rewriting junior counselor's responses using a system of templates to offer better responses in terms of reflective listening, a key skill for mental health counselors.  The templates themselves are too restrictive, and so paraphrasing is used to broaden their matching power.\n\nOne of the reviewers cited several concerns with the lack of comparison with a broader set of relevant methods, and multiple reviewers pointed out the lack of adequate comparison to LLM's.  The paper now argues for why template-based approaches are superior to any LLM based approach on the grounds that they provide more controllability and preservation of the original content.  Controlled generation is an active area of research for LLM's, but an LLM, particularly one that has been fine-tuned, offers quite a high degree of controllability for a given task, and so this claim should be properly evaluated.  Moreover, saying that templates provide a better ability to preserve original content is also a claim that needs evaluation.  The authors note in one of their rebuttals that they have, in fact, tried to use LLM's and that they were superior to their own method, but less good at preserve original text, but this was without sufficient attempts to improve this capability.\n\nThis paper explores an important area, is (generally) clearly written and offers good results in its proposed direction.  However, the reviewers here have pointed out legitimate concerns with this work and its place alongside similar work in the area of response rewriting and, indeed, in the broader area of NLP."
            }
        },
        "id": "NZVZfa3YsE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Mq5cyRMGlD",
        "replyto": "Mq5cyRMGlD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission617/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492209,
        "cdate": 1696707492209,
        "tmdate": 1701465405454,
        "mdate": 1701465405454,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes  to flexibly call the retriever for making better use of both internal and external knowledge. Specifically,  a memory bank for positive/negative samples is maintained. and the new method detects LLMs’ self-knowledge by either training a classifier (SKR_cls) or using context learning based on the memory bank (SKR_knn) or directly asking the model whether it knows the answer (SKR_prompt). \n\nOn five different types of question answering datasets the KNN approach achieves improvements over other baselines.\n\nStrength\n1. A novel framework for an important problem -- considering when to make retrieval for retrieval-augemented LLMs .\n2. The knn approach achieves mostly consistent improvements over retrieval augmented and non-retrieval augmented baselines\n\nWeakness: more comprehensive experiment\n1. Not experimented with knowledge-intensive tasks such as KILT. \n2. Not compared to baselines in IR-augmented  (Self-Ask and DSP) and reasoning (Recite-and-answer). \n3. only one type of retriever is experimented with.\n4. Would be helpful to compare to related work that leverages the confidence of retrieval model (SearChain)\n\nWeakness: limitations of the model\n1. It relies on access to the training dataset, which may not be readily available.\n2. The proposed method would retrieve the knowledge with the whole query rather than the part the LLM does not know."
            }
        },
        "id": "FK9jnzKThf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MoEfm3iPMy",
        "replyto": "MoEfm3iPMy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5155/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608188,
        "cdate": 1696707608188,
        "tmdate": 1701465551455,
        "mdate": 1701465551455,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Strengths**:\n\n1. The idea of pretraining using both text + graph info is interesting. Also, using graph info to handle text-richness imbalance is a good idea. It considers higher-order and heterogeneous structures.\n\n2. The experiments were conducted extensively. New expts added as part of rebuttal process are relevant additions. Authors were also able to show that their method provides stat sig better results compared to methods already described in the paper and other recent methods like OAG-BERT.\n\n\n**Weaknesses**:\n\n\n1. Accuracy gains are not significantly large, considering the extra computations involved. But they are statistically significant.\n\n2. Novelty is a little bit on lower side. As a reviewer indicates, earlier studies have tried combining language models and graph structures, and have handled imbalanced text in heterogeneous graph settings also.\n\n3. Comparison with GraphFormers is missing.\n\n**Suggestions**:\n\n\n1. Please include a summary of several results presented in rebuttal, into the main paper.\n\n2. Also, discussion on difference with Heterformer is nice. Please include it in related work.\n\n3. Many clarifications during rebuttal are important for the reader. E.g., Clarification on the performance in Table 1, Table 2, and Table 3. Clarification on the performance between THLM and BERT. Please include them briefly in your main paper."
            }
        },
        "id": "XizzbcxTX5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MnPnE4xV0H",
        "replyto": "MnPnE4xV0H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1334/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510555,
        "cdate": 1696707510555,
        "tmdate": 1701465427652,
        "mdate": 1701465427652,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**quality, clarity, originality**\n\nThe paper is well written, as also stated by the reviewers. It is comprehensive and high quality. The authors set out the problem well and work to provide different views on the synthetic data generation and classification problems. The inclusion of a wide array of NLP classification tasks works in the favour of this paper. There were a number of reviewers who raised the challenge about the \"subjective\" vs \"less subjective\" datasets (the latter having less examples in the paper). The authors have addressed this by adding more experiments with more datasets that would be classified (by their measure) as \"less subjective\". This is welcome as it allowed better evaluation and understanding of the overall contribution of this paper. The final result might be something that many researchers had an intuition about, but this work provides an empirical and scientific study that confirms it and will form the basis (as I see) of how others also build on evaluating such approaches to data generation for their use cases. \n\n** significance**\n\nAs stated earlier, the work gives very strong evidence about zero shot vs few shot performance for nlp classification data generation. It is a significant result that will be helpful in this LLM era. There was a concern about the LLMs covered for the experiments, but these have also been expanded and this adds to the pros of this paper. \n\n**Notes to authors**\n\nWill the generated data be made available for further evaluation by other groups?"
            }
        },
        "id": "UZV2ZOkefV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MmBjKmHIND",
        "replyto": "MmBjKmHIND",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission401/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486607,
        "cdate": 1696707486607,
        "tmdate": 1701465397620,
        "mdate": 1701465397620,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The work introduces a dataset for enhancing the causal reasoning of Large Language Models. \n\nAmong the major concerns expressed during the reviewing period, the lack of human evaluation of the contents produced by language models stands out (even though it is becoming a current practice supported by some investigations) and the circular evaluation of the model generating the task and being evaluated on it. The authors provided additional materials that tackle this issues and that should be included in the final version of the paper if accepted - together with all other comments."
            }
        },
        "id": "zVIlPO96W2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Mm5GXKvpXm",
        "replyto": "Mm5GXKvpXm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3553/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565082,
        "cdate": 1696707565082,
        "tmdate": 1701465501299,
        "mdate": 1701465501299,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper has received positive feedbacks from all reviewers. All reviewers agree the dataset is a unique and important contribution to robot-human interaction and collaboration studies, while there have been concerns over dataset generalization and a lack of experimental comparison with other VQA methods, the authors provided satisfactory responses leading reviewers to increase their scores."
            }
        },
        "id": "KCUJ60u5Jk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MkD0VGShAq",
        "replyto": "MkD0VGShAq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission679/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493641,
        "cdate": 1696707493641,
        "tmdate": 1701465407335,
        "mdate": 1701465407335,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work explores various ways to generate Counterfactually Augmented Data creation, such as through ChatGPT, FLAN-T5 and PolyJuice. Authors show that aside from manual CADs, ChatGPT is also an effective tool for such data augmentation. Author has primarily used Finetuned RoBERTa classifier and the task of harmful language detection to showcase the efficacy. Reviewers agree that the discussion and comparison around manual vs automated data augmentation is an interesting contribution. Most of the points such as manual evaluation of the generated data, concentrating only on RoBERTa has been addressed by the authors during rebuttal period. It is interesting work and the authors should incorporate the suggestions made by the reviewers to the best of their capability."
            }
        },
        "id": "Qv0X82mquA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MhbiD5FVPF",
        "replyto": "MhbiD5FVPF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2892/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551747,
        "cdate": 1696707551747,
        "tmdate": 1701465480370,
        "mdate": 1701465480370,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper systematically presents an interesting observation: when training with different dimensionality of a pooler, performance of using directly with the embedding out of transformer (without pooler) will change correspondingly. Thus performance loss could be decomposed into two part, and author proposed a way to select a low dimensionality with better performance. It's an interesting observation but the significance is not prominent. \n\nPros:\n\n1. Interesting observation.\n\n2. Present it well and easy to follow\n\nCons:\n\n1. After running the proposed 2-stage \"multiple-train + plugin replacement\" algorithm, the only benefit is for pooler to be from D to d, which is less than 1% or even less computation in the whole model. And the performance is not improved a lot either. This leads to the question why we want to study the problem.\n\n2. Results are not consistent on all models. Authors agreed that for RoBERTa-base, the effect is not very prominent, but on other models it matters more.\n\n3. The paper only focused on the \"loss decomposition\" part, but didn't explain why the performance could actually be \"improved\". For example, the Figure 3a, using a d=32 pooler to co-train and use encoder output give about 5% accuracy improvement, which is much larger than most improvement in the experiments. This effect is not clearly explained."
            }
        },
        "id": "yyo8iJC4TZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MhU0zxuZ5K",
        "replyto": "MhU0zxuZ5K",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2922/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552197,
        "cdate": 1696707552197,
        "tmdate": 1701465481153,
        "mdate": 1701465481153,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper is a solid work and very interesting. By making NTK, the work studies the Feature Extraction Mechanisms of existing models, such as self-attention, cnn, etc."
            }
        },
        "id": "91KSABqYtQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MhEJqeCzgE",
        "replyto": "MhEJqeCzgE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1458/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707514828,
        "cdate": 1696707514828,
        "tmdate": 1701465431587,
        "mdate": 1701465431587,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All the reviewers have agreed that this paper has tackles a pain in dual-cross-encoders.\n\nHowever, some concerns have also been raised such as some important details are lacking and generally during the rebuttal period, the authors have provided these details and addressed these concerns.\n\nI recommend the authors incorporate these details to the paper."
            }
        },
        "id": "wdyJwgawY1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Mefvmgkb9G",
        "replyto": "Mefvmgkb9G",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1242/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508287,
        "cdate": 1696707508287,
        "tmdate": 1701465424885,
        "mdate": 1701465424885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a  new query and gallery banks method to mitigate hubness in cross-modal retrieval. The research is original and the methodology clear. The results and experiments are well-supported.\n\nPros:\n* The theoretical analysis of the Hubness in a Cross-Modal Retrieval scenario\n* Extensive experimentation for different Cross-Modal retrieval datasets.\n\nCons:\n* Minor improvements when compared to previous research."
            }
        },
        "id": "33sONpXUr0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Md1YdfqAed",
        "replyto": "Md1YdfqAed",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1133/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505665,
        "cdate": 1696707505665,
        "tmdate": 1701465421404,
        "mdate": 1701465421404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers appreciate the soundness of the work although they disagree about excitement, I think the question really revolves around the impact of modeling emotionally correlation in dialogue. I think this speaks to rPyr point about motivation. \n\nIt is somewhat concerning that the details of the human evaluations are not stated in the appendix nor do there seem to be tests of statistical significance. Given the large superiority I do not see this is a fatal flaw, but something that the authors must include."
            }
        },
        "id": "7W2C7eA3kR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MbKRJUowYX",
        "replyto": "MbKRJUowYX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission990/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707501502,
        "cdate": 1696707501502,
        "tmdate": 1701465417096,
        "mdate": 1701465417096,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes to save space in transformers by using a single matrix for all query, key, value projections for all layers. They compare their approach to various baselines, showing memory savings. Reviewers found the approach simple and intuitive, and liked that it can be combined with other methods. They also generally found the results strong, but  complained about missing details in the reporting, and about the quality of the baselines. The rebuttal helped alleviate some of these concerns, though the baseline concern still remains."
            }
        },
        "id": "BoiEnSiWUS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MZwFbA3DSF",
        "replyto": "MZwFbA3DSF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5422/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612714,
        "cdate": 1696707612714,
        "tmdate": 1701465559017,
        "mdate": 1701465559017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The short paper deals with analysis of concepts collected in a large knowledge base. The method leverages LLMs to find common properties  for a given two concepts, which is opposed to a common practice of distilling concepts from texts. The paper presents a rare approach to concept representation, which is supported by solid experiments. \nAll three reviewers are generally agree in their comments and scores. However, some crucial points are left out from the main article body and presented either in appendixes or in rebuttal responses. The authors should think carefully how to use an extra page in the final version."
            }
        },
        "id": "Vp9LpeKqgs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MYdmanqfvm",
        "replyto": "MYdmanqfvm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1070/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504311,
        "cdate": 1696707504311,
        "tmdate": 1701465419432,
        "mdate": 1701465419432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors replace political entities in a given article/document and check their impact on summarization. There exists a significant political bias in summarization models. \n\nReviewers agree the task is interesting and the results also support the claim. This provides good directions for future research on the exploration of biases in summarization models (Reviewers utt1, f6t5). \n\nHowever, results are restricted to some specific entities and carry geographical bias. There might be issues with the generalizability of the findings (Reviewer f6t5, 7A5T)."
            }
        },
        "id": "KCg9vuWuBG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MXMA6vQtSZ",
        "replyto": "MXMA6vQtSZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4070/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582442,
        "cdate": 1696707582442,
        "tmdate": 1701465519180,
        "mdate": 1701465519180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers agree that the paper contributes to an important research area, which is to add nuances of dialect usage by Arabic speakers in the existing work of Arabic Dialect Identification. While the idea of representing dialectness as a continuous variable isn't novel, the authors operationalize it and perform extensive annotations on the news articles and users' comments, where each sentence is annotated three times. Furthermore, one important highlight of the paper is to demonstrate the usefulness of the dataset for the sociolinguistic field, where the authors show that we can use models trained on their dataset to characterize the Arabic speaking stylistic choices (and even better than using vectors of scores from existing Dialect Identification models). During the post-rebuttal discussion, the authors also clarify the dimensional differences between dialectness and formality, and therefore this work contributes to the existing resources on formality and the binary representation of dialectness for Arabic dialect identification."
            }
        },
        "id": "ihuHeVv8cc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MWisc5Amup",
        "replyto": "MWisc5Amup",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5419/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612642,
        "cdate": 1696707612642,
        "tmdate": 1701465558966,
        "mdate": 1701465558966,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an unsupervised stylistic headline generation model.\n\nThe method is easy to implement and can achieve good performance.\nThe paper is well-organized and easy to follow.\nThe experiments are reasonably thorough, including both automatic and human evaluation.\n\nSeveral reviewers comment that the novelty is limited and similar methods have been proposed before.\nInclusion of statistical significance would strengthen the results.\nAlso, lack of comparison to LLMs is hindering its relevance among current research directions."
            }
        },
        "id": "Jr2RC6ilns",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MWhwZjFCcq",
        "replyto": "MWhwZjFCcq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3361/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561219,
        "cdate": 1696707561219,
        "tmdate": 1701465495077,
        "mdate": 1701465495077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new model for referring expression grounding in 3D point clouds. Their model uses attention to model spatial relationships between objects, and relate them to the description.\n\nMost reviewers agree that there are multiple noteworthy and exciting model contributions that result in compelling performance.\n\nMost reviewers agree that the experiments used to evaluate the model are comprehensive and sufficient, and the work is overall sound."
            }
        },
        "id": "JqxKix54v2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MSQrAoa7iy",
        "replyto": "MSQrAoa7iy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4958/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707602277,
        "cdate": 1696707602277,
        "tmdate": 1701465545722,
        "mdate": 1701465545722,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper uses response generation and builds an intent detection using gold responses as retrieval as pre-training tasks for building an intent detection model. While it's a great paper, results on more datasets would strengthen the argument."
            }
        },
        "id": "EE8XNnRzz6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MRehcsVc4y",
        "replyto": "MRehcsVc4y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4921/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601624,
        "cdate": 1696707601624,
        "tmdate": 1701465544833,
        "mdate": 1701465544833,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper tackles to improve QA performance by generating synthetic question-answer pairs and training on them. This method is demonstrated to improve two low-resource QA datasets as well as two high-resource QA datasets.\n\nReviewers pointed out that experiments support the effectiveness of the method well (xYud, N3fX, KiMC), baselines are comprehensive (N3fX) and analysis is insightful (xYud, N3fX).\n\nReviewers raised concerns on the sensitivity of the method (N3fX) and the presentation (KiMC). Concerns on lack of ablations were also raised (xYud) which are addressed in the author responses.\n\nDuring the reviewer discussion period, reviewers raised additional concerns on limited novelty, given that the general pipeline of answer identification -> question generation -> filtering for data augmentation in QA has been done in prior work [1, 2, 3]. However, the paper still made a small-but-non-trivial contribution on improving answer identification which goes beyond simply using linguistic rules and encourages the diversity of the answers, which is well-supported by a set of experiments and additional ablations provided during the rebuttal period.\n\n[1] https://arxiv.org/pdf/2004.11546.pdf\n[2] https://arxiv.org/pdf/2010.12643.pdf\n[3] https://arxiv.org/pdf/2102.07033.pdf"
            }
        },
        "id": "Tlk0hy4Pl4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MQsvD6YOan",
        "replyto": "MQsvD6YOan",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1839/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528503,
        "cdate": 1696707528503,
        "tmdate": 1701465444113,
        "mdate": 1701465444113,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a safety detection framework, InstructSafety, comprising 7 common subtasks in safety detection, as well as a pre-processed dataset for instruction tuning and a fine-tuned model (Safety-Flan-T5). Reviewers agree that these contributions are useful and relevant, and that the paper is well-written and interesting. All three reviewers agree that the approach is generally sound, while two reviewers are highly excited about this work (score of 4).\n\nThe main criticisms of the work relate to the evaluation of the fine-tuned model. Reviewer UwgW initially had questions about the effect of adversarial examples and the data set generation method, although these issues appear to be resolved by the author response. Reviewer XQAP asks about extending the evaluation to more than one dataset per subtask, comparing Flan-T5 with other models, and about the variance within experimental results. The latter two issues are clearly and quantitatively addressed in the author rebuttal. The authors are encouraged to address the first point (i.e., to explain why certain datasets were included and not others, or to consider additional experiments) in their revision. Reviewer 8L5c also highlights the need to carefully define the scope of “safety detection” and to explain why the present definition excludes model robustness against adversarial attacks, etc. The authors are also encouraged to explain how additional tasks can be added to the framework, and to report the diversity of the data included in the instruction-tuning dataset. \n\nPros\n- Unified framework comprising 7 tasks for safety detection\n- Release of pre-processed dataset containing 39 human-annotated sub-datasets for instruction tuning\n- Safety-Flan-T5 shows state of the art performance \n\nCons\n- Evaluation is somewhat limited in that it includes only one dataset per subtask\n- Scope of the “safety detection” problem needs clarification\n- Further discussion needed around how to incorporate new or other tasks (e.g. adversarial attacks)"
            }
        },
        "id": "CIMRZndWsO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MNTCi0i3cU",
        "replyto": "MNTCi0i3cU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3522/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564436,
        "cdate": 1696707564436,
        "tmdate": 1701465500056,
        "mdate": 1701465500056,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Thank you to the authors for their work and the reviewers for their reviews. This work focuses on automatically generating perspectives about abstract entities in polarized social media text, focusing on #BlackLivesMatter and #BlueLivesMatter tweets.\n\nPros:\n- All reviewers rate the paper high for soundness\n- As noted by GbZ4 \"The task is challenging and interesting - identifying differing stances on the same topics - and shows up frequently in analyses of social data and social phenomena.\" This methodology has the potential to be broadly useful for social text analysis\n- GbZ4 and Yvsx agree that the graph-based methodology with several prediction tasks is innovative\n\nCons:\n- Several reviewers raise concerns about how the authors do not allow for neutral or ambiguous stances. As the #BlackLivesMatter movement has been highly polarized and the authors remove any users who tweet about the events <5 times, I don't see this as a fundamental flaw in this particular case study. However, the authors could add more clarity on how methods might need to be changed to use this methodology in other settings or if there are settings where they are not appropriate to use\n\nqRYV questions the reproducibility of the work because the authors a paid API (GPT-3) for one small part of their analysis. I don't agree that using a black-box API makes the work inappropriate for academic publication, and I don't believe the authors should be penalized for this in the absence of a higher-level ACL policy precluding comparisons with APIs.\n\nI think the task and methodology in this work will be of interest to NLP and computational social science researchers."
            }
        },
        "id": "rp606mOL7m",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MMrqu8SD6y",
        "replyto": "MMrqu8SD6y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission409/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486855,
        "cdate": 1696707486855,
        "tmdate": 1701465397936,
        "mdate": 1701465397936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new framework for unsupervised clustering of documents, which tries to satisfy the desiderata of goal driven clustering with free-form textual explanations of clusters. Their approach involves prompting a language model to generate candidate explanations, assigning documents to those explanations, and aligning possible matches using an ILP. Using both automatic and human evaluations, the authors find they are able to generate clusters and explanations that conform to the user-driven goals.\n\nReviewers were intrigued by this task, and found the paper to be clearly written and easy to follow. They also thought this was creative work with practical value in applications.\n\nThe reasons to reject given by the reviewers were relatively minor, including pointing out the high computational cost of this method, suggesting additional baselines, and requesting sensitivity analyses. The most serious concern seems to be that the authors to some extent assume that the generated explanations are correct, without adequately verifying this, but the reviewer who pointed this out was still quite favorable about the paper.\n\nAlthough the reviewers were evenly split on soundness (between Good and Strong), 3 of the 4 reviewers rated their excitement as Strong, suggesting that this would be a reasonable choice for a main conference paper."
            }
        },
        "id": "XIMQCdOxI1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MLzoMwlxTh",
        "replyto": "MLzoMwlxTh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4273/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586454,
        "cdate": 1696707586454,
        "tmdate": 1701465526231,
        "mdate": 1701465526231,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are in agreement that this is an excellent submission. It is the first study of factual consistency across languages, and provides valuable analyses that are deep and comprehensive."
            }
        },
        "id": "sh0qB1eMuz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MLKLYoXypN",
        "replyto": "MLKLYoXypN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3661/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567195,
        "cdate": 1696707567195,
        "tmdate": 1701465505802,
        "mdate": 1701465505802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work is concerned with the challenge of discovering new intent categories from user utterances, and introduces Cluster Semantic Enhanced Prompt Learning (CsePL) as a means to discover new intents. The proposed approach employs a two-level contrastive learning framework, incorporating label semantic alignment to acquire meaningful representations of intent clusters. These representations are subsequently employed as soft prompt initializations for distinguishing new intents, thereby reducing the influence of pre-existing intent categories. Experimental evaluations demonstrate that the proposed method outperforms baseline approaches. While the novelty of the paper may require further clarification and conciseness, the primary components of the proposed approach appear to integrate elements from previous works."
            }
        },
        "id": "jIWn9lrFIm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MKjGklW9TP",
        "replyto": "MKjGklW9TP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2677/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547367,
        "cdate": 1696707547367,
        "tmdate": 1701465473131,
        "mdate": 1701465473131,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work hypothesized that pre-finetuning NLI models with intermediate tasks related to discourse and pragmatics (namely coherence modeling, sarcasm detection, and discourse relation classification) would improve model performance on cases involving presupposition.\n\nThe paper provided empirical support for the authors' hypothesis, thus empirically establishing the connection between presupposition and these tasks. The reviewrs agreed that this was a useful result that could benefit future work. Overall, evaluation is solid with extensive ablation studies.\n\nThe rebuttal sufficiently addressed all of Reviewer joCE's concerns, particularly regarding the label randomization experiment and the use of the Vanilla setup as an informative baseline.\n\nIt's clear from Reviewer PE3F's response to the rebuttal that his/her concerns remained, specifically regarding the contribution of this paper in light of related work. I think the authors provided a reasonable response to PE3F's response and am personally satisified with their response. I view PE3F's concerns more as suggestions for improvements than serious flaws, and hope that the authors can take them into account when revising their paper."
            }
        },
        "id": "gaH1le9M2l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MFimS05rLW",
        "replyto": "MFimS05rLW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5347/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611375,
        "cdate": 1696707611375,
        "tmdate": 1701465556933,
        "mdate": 1701465556933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Strengths**:\n\n1. SALAM helps improve LLM performance by providing a systematic framework to help the LLM learn from previous mistakes. It collects mistakes in the training phase. During inference, it uses this mistake collections to provide guidelines for LLMs to help them avoid similar mistakes. The idea is novel and intriguing.\n\n2. Authors present a principled framework with all mathematical details.\n\n3. comprehensive analysis (on BBH and BBQ datasets with 3 models) and ablation studies.\n\n4. Code is shared. Feedback dataset will also be released.\n\n**Weaknesses**:\n\n\n1. For multi-step reasoning, the feedback could be indirect and not targeted. I think more work needs to be done to structure out which part of the reasoning is making mistakes and should be influenced by feedback.\n\n2. \"Computational overhead of imitation learning\" -- more objectivity (trying out different study assistant sizes and then comparing their relative impact) would be nice to have.\n\n**Suggestions**:\n\n1. Include some learnings from these in main paper: (1) Comparison with baselines like Self-Refine or Self-Correct. (2) expts with larger LLMs like GPT4. (3) Supervised finetuning baseline Llama results. (4) SALAM with 100% Training Set on BBH. (5) GPT-4 as the study assistant\n\n2. Please update the writeup for 3.4 Imitation Learning for Study Assistant ."
            }
        },
        "id": "447jagaH4d",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "MEByW1upLk",
        "replyto": "MEByW1upLk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2789/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549448,
        "cdate": 1696707549448,
        "tmdate": 1701465476462,
        "mdate": 1701465476462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers agreed that the paper investigates a well-motivated and interesting research question, using comprehensive evaluations. Reviewer Vvi2 raised a soundness concern related to unnormalized use of the CLIP score. However, I agree with the author response that the unnormalized score provides a measure of label-image relevance which is independent of the contrasting images/text chosen, as also used by previous work. I also agree that the paper does make a novel contribution, as previous work on this Dollar Street benchmark does not look at disparities related to income values, and while I agree with reviewer Vvi2 that this result might not be too surprising given prior work, this is still valuable to confirm."
            }
        },
        "id": "TAHodsdefh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "M9NdVElcbs",
        "replyto": "M9NdVElcbs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2129/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535172,
        "cdate": 1696707535172,
        "tmdate": 1701465455120,
        "mdate": 1701465455120,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper delineates an approach that employs conceptors for the purpose of debiasing Large Language Models (LLMs). The paper introduces a novel architecture capable of diminishing bias while simultaneously preserving high accuracy. The architecture is characterized as cost-effective and potentially integrable into various architectures, both of which are highly desirable attributes for debiasing methods.\n\nAll reviewers unanimously concurred on the novelty of the proposed methods, as well as the rigor and the favorable outcomes of the experiments.\n\nI urge the authors to refine the paper, taking into careful consideration the feedback provided by all the reviewers, with a particular emphasis on addressing the insights from Reviewer 3."
            }
        },
        "id": "CrUtjTEsZr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "M6BJfQ9oup",
        "replyto": "M6BJfQ9oup",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1903/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529725,
        "cdate": 1696707529725,
        "tmdate": 1701465446533,
        "mdate": 1701465446533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work applies reinforcement learning (RL) to address the multi-modality and misalignment problems in non-autoregressive multi-intent SLU. It introduces a new method called Modifying the Reference via Reinforcement Learning (MRRL).\n\nThe paper identifies two interesting challenges in non-autoregressive slot filling and intent detection, demonstrates good performance and proposes an interesting reward. However, the reviewers considered that additional analysis would be helpful in determining how the issues are being addressed, and the stability of the proposed RL approach."
            }
        },
        "id": "tugbDmoSkn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "M5knJ7ovgz",
        "replyto": "M5knJ7ovgz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission338/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485072,
        "cdate": 1696707485072,
        "tmdate": 1701465395654,
        "mdate": 1701465395654,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a comprehensive new evaluation suite for Abstract Meaning Representation (AMR) parsing, and shows that the AMR parsing remains far from solved.\n\nPros:\n- Thorough evaluation suite covering a range of phenomena, with a practical taxonomy informed by real-world applications.\n- Very clearly written, well-structured paper.\n- Thorough experiments written up with clear analyses, rich detail.\n- Introduces not just a data set, but also standard tooling to compare AMR parsers.\n- Data and tools will be publicly available. Clear reproducibility.\n\nCons:\n- Review copy isn't super clear in some parts when it comes to some comparisons with previous work, but if accepted, authors will address this, as per the review thread; as can be seen there, they already have a good understanding of what needs to be added.\n\nThis is an exciting contribution to the field; many thanks to the authors."
            }
        },
        "id": "A4xIUnnx6S",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "M51c00VxiJ",
        "replyto": "M51c00VxiJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4132/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583715,
        "cdate": 1696707583715,
        "tmdate": 1701465521269,
        "mdate": 1701465521269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the impact of auxiliary tasks (ASR, MT) on speech translation in the multi-task learning (MTL) setup. The authors presents in-depth analysis on the interaction between tasks and identifies issues causing degradation. The authors propose an improved MTL approach with a number of techniques (looking-back mechanism based on CTC output, local-to-global strategy to insert audio-like noises, dynamic adjustment based on task impact).\n\nThe paper is well positioned, and all reviewers enjoy the comprehensive and systematic analyses for deeper understanding of how each auxiliary task benefits or harms the primary task. The proposed improved MTL approach are also evaluated extensively and show favorable performance over many baselines. Authors have addressed most of the comments regarding experimental setups and positioning of the paper compared to the literature. I recommend the authors to incorporate the feedback from the reviewers into the manuscript."
            }
        },
        "id": "bNr1FSawaW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "M3uTqtEgNo",
        "replyto": "M3uTqtEgNo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3433/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562661,
        "cdate": 1696707562661,
        "tmdate": 1701465497348,
        "mdate": 1701465497348,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary Evaluation:\n\nThe paper introduces DemoNSF, a multi-task demonstration-based generative framework for the noisy slot-filling task in practical dialogue scenarios. It proposes three novel noisy auxiliary tasks in the pre-training stage to capture semantic structural information of input perturbations at different granularities. Experimental results demonstrate that DemoNSF outperforms baseline methods and achieves strong generalization.\n\nPros:\n- Addresses an important issue of input perturbations in slot filling tasks in dialogue systems.\n- Proposes a novel multi-task demonstration-based generative framework called DemoNSF that improves performance and generalization.\n- Clear and well-written paper with thorough experiments conducted.\n- First comprehensive investigation of the effects of diverse input perturbations on generative frameworks in slot filling tasks.\n\nCons:\n- The novelty seems to be limited, and the improvements could be expected.\n- While it uses T5 and BART as baseline models, GPT-style models are not considered.\n- Some important experimental results such as ablation studies and ChatGPT experiments are placed in the appendix instead of the main paper.\n\nAfter considering the updated scores of the reviewers for soundness and excitement, this paper presents a valuable contribution to the field of NLP and dialogue systems. The proposed framework, DemoNSF, shows improved performance and generalization over existing models in addressing input perturbations in slot filling tasks. However, the authors could consider incorporating GPT-style models in their comparison and moving some experimental results from the appendix to the main paper to strengthen the work further."
            }
        },
        "id": "iTVyo7sEpH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "M1Nogs3zR5",
        "replyto": "M1Nogs3zR5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission450/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488067,
        "cdate": 1696707488067,
        "tmdate": 1701465399331,
        "mdate": 1701465399331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents SHARCS, a method to accelerate Transformer inference using a mechanism that adaptively adjusts the width of Transformer layers based on the perceived difficulty of the input samples.\n\nThere was a general concern raised by multiple reviewers that the approach was compared against only a subset of dynamic inference techniques (early-existing approaches) which was considered strange as SHARCS is not an early-exiting approach. The authors provided a general note in the rebuttal that they are focusing on just adaptive approaches (not static methods like pruning). However, the paper would have been better positioned if the initial comparison was against other (more recent) adaptive approaches. In the rebuttal, the authors provide additional results against one token reduction technique (Transkimer).\n\nAnother concern raised by the reviewers was a general explanation of why this approach achieves a good trade-off. In the rebuttal, the authors explain that the improvement is due to the introduction of routing based on sample hardness. Additional experiment settings (or ablation studies beyond 4.4) on this main contribution would make the key novelty more clear."
            }
        },
        "id": "TS2f5FFkh3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "M1GRz46Ahz",
        "replyto": "M1GRz46Ahz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3729/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707569959,
        "cdate": 1696707569959,
        "tmdate": 1701465507877,
        "mdate": 1701465507877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores a causal model for cross-domain few-shot relation extraction. The reviewers have several concerns for the paper regarding the lack of clarify and motivation for some model details. However, the rebuttal provides further information and seems to address the major concerns. The authors are encouraged to update the paper according to the discussion with the reviewers."
            }
        },
        "id": "9JqPe7Qf1t",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Lp4CMWnSyb",
        "replyto": "Lp4CMWnSyb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission29/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477290,
        "cdate": 1696707477290,
        "tmdate": 1701465384699,
        "mdate": 1701465384699,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers all recognized the importance of this task for practitioners and are excited about how intuitive the presented methods are, and I agree that this is an important and interesting task. This could be even better supported, however, as one reviewer raised a concern about the motivation of the paper and specifically the toy example given in Section 1. I also found this hard to follow; perhaps the authors can select a more intuitive and/or compelling example, which would address this concern. Generally though, the paper is very well motivated and this is recognized by all the reviewers.\n\nMultiple reviewers expressed confusion over the system explanation and visualization. This part of the paper should be clarified and additional context should be given about the sources of the various components. I found this part confusing as well. \n\nReviewers gave ambiguous soundness scores, but I believe these concerns were mostly addressed in the rebuttal (e.g., the variation tests in response to Reviewer PUtu).  I agree these results should have been included in the original paper, but they do address this particular concern. The authors also provided an analysis of additional datasets, on top of the many datasets already included in the submitted paper."
            }
        },
        "id": "SCDvoXqUlE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LkV7Xx06yq",
        "replyto": "LkV7Xx06yq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2304/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539091,
        "cdate": 1696707539091,
        "tmdate": 1701465460674,
        "mdate": 1701465460674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new benchmark for evaluating anomaly detection methods in natural language processing, called AD-NLP. This benchmark includes 4 existing datasets and adds 3 new datasets. All the data are organized following a classification of anomality (syntactic, semantic, pragmatic, and stylistic). Moreover different methods are evaluated on this benchmark. The benchmark will be made available.\n\nIt would be great to better present the results in the paper instead of in appendix. Also more explanation and information are need at some point (choice of hyperparameters for the NN methods and representations methods)."
            }
        },
        "id": "h8OiXNivEB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Lk1KaQcjaM",
        "replyto": "Lk1KaQcjaM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission703/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494104,
        "cdate": 1696707494104,
        "tmdate": 1701465407893,
        "mdate": 1701465407893,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary: This paper investigates how to estimate causal effects using text data while addressing the issue of selection bias. It proposes a method that disentangles covariates into treatment-related, outcome-related, and confounder categories, ensuring that covariates only impact their relevant parts. Different from most existing causal inference methods which only adjust for confounders, this work disentangles different factors based on a VAE framework. This paper also introduces a new dataset consisting of 115,880 transcripts. The study includes experiments with two treatment factors in various contexts, and the proposed model outperforms other benchmarks. \n\nThis paper studied a fundamental and challenging research problem. The paper is generally well-written and easy to follow. The proposed method is evaluated on different benchmarks with promising results. Most of the reviewers agree that this is a solid contribution but the excitement is relatively limited."
            }
        },
        "id": "o7WDrrt3W3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LepuyCeWcw",
        "replyto": "LepuyCeWcw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3230/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558630,
        "cdate": 1696707558630,
        "tmdate": 1701465490988,
        "mdate": 1701465490988,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces evidence-based label smoothing, a computationally efficient method that prevents assigning high relevance to false negatives by leveraging the similarity of candidate documents within the ranking context of a query to the annotated ground truth to compute soft relevance labels. Extensive experimentation on two large-scale ad hoc text retrieval datasets demonstrates that the method improves the ranking effectiveness of dense retrieval models. Overall a nice paper. Well well-written and well-supported by reproducible experimentation."
            }
        },
        "id": "wnjz0hqbA0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LawAC9vh8q",
        "replyto": "LawAC9vh8q",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5303/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610629,
        "cdate": 1696707610629,
        "tmdate": 1701465555588,
        "mdate": 1701465555588,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers are cautiously positive in their assesment of the paper (3-4).  There are objections to the task being suboptimally selected relative to other work in cross-lingual stance detection and to the set-up being unnecessarily complex.  Despite this, reviewers agree that the experimental set-up is well-documented and the results are positive."
            }
        },
        "id": "RWmcNEOCuF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LZq3crn3Bv",
        "replyto": "LZq3crn3Bv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3446/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562864,
        "cdate": 1696707562864,
        "tmdate": 1701465497767,
        "mdate": 1701465497767,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This is a comprehensive empirical study that leads to the observation that LLMs are generally not as competitive for IE tasks than fine-tuned small LMs. The paper then proposes a hybrid strategy that uses LLMs to rerank for \"hard\" examples that a small LM finds challenging. Empirical results demonstrate the effectiveness of this strategy.\n\nStrengths:\n- Solid and comprehensive empirical study on an important problem\n- Clear and professional writing\n- An intuitively plausible hybrid strategy with demonstrated effectiveness\n\nWeaknesses:\n- As the reviewers pointed out, the paper falls short in investigating and explaining why LLMs are underperforming, especially given the known sensitivity to prompts. The authors have responded to this criticism with new analysis and experiments. The authors are strongly encouraged to add these to the revised version.\n- As Zhang et al. (2023), which is already cited in this paper, have shown, LLMs' RE performance can be substantially improved by reformulating RE as a QA problem. One possibility demonstrated through that work is, perhaps it's not that LLMs are inherently bad at IE, but that we haven't found the best way to use them for IE yet. LLMs are also becoming better over time. A blanket statement, like \"LLMs are not good few-shot information extractors in general\", may need more qualification, like \"the **current** LLMs **under vanilla prompting settings** are not good few-shot information extractors in general\". This doesn't significantly undermine the contribution of this paper, and I think it's still a worthy paper to be presented at the conference. But it's important to send the most accurate message to the community to inform future research.\n\nKai Zhang, Bernal Jimenez Gutierrez, Yu Su. Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors, Findings of ACL 2023."
            }
        },
        "id": "8SL5q33GNs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LUDljw5VVD",
        "replyto": "LUDljw5VVD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission656/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493099,
        "cdate": 1696707493099,
        "tmdate": 1701465406696,
        "mdate": 1701465406696,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a new multilingual dataset of task-oriented dialogues with structured contextual information (user contacts, lists).\n\n**Pros**: All reviewers agree the dataset will be of significant value to the community, with most reviewers emphasizing the dataset's focus on phenomena which is absent from most current benchmarks; e.g., disfluencies, code-switching, and revisions. Experiments are considered \"commendable\" and \"show real world insights into the task\". The dataset is considered to be \"clearly informed by [real-world] experience\" and one which \"enhances the realism... which is often lacking in existing datasets.\"\n\n**Cons**: One reviewer raises concerns about the caliber of the model used for baselines."
            }
        },
        "id": "DicgYWTNqq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LRRThBBiov",
        "replyto": "LRRThBBiov",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3787/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707575164,
        "cdate": 1696707575164,
        "tmdate": 1701465509698,
        "mdate": 1701465509698,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper is about a new parallel method IPGPF designed for Document-level Event Extraction (DEE). They introduces a pseudo-trigger-aware pruned complete graph (PT-PCG) representation for events, which reduces the computational complexity of event extraction. They also use of a pre-filling strategy to improve the efficiency and accuracy of event extraction by generating candidate events before filling in the event arguments. Experimental results on ChFinAnn and DuEE-fin show that IPGPF reaches new SOTA performance. The main concern about this paper is the limited number and language of used evaluation datasets."
            }
        },
        "id": "6zPNGM6fR7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LQqlapYGeR",
        "replyto": "LQqlapYGeR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2801/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549694,
        "cdate": 1696707549694,
        "tmdate": 1701465476834,
        "mdate": 1701465476834,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents an exploration of proactive dialogue systems using prompted LLMs, with a systematic evaluation of their capabilities. While the paper makes interesting contributions, there is a lack of reproducibility of the results since they rely on very specific prompting techniques."
            }
        },
        "id": "EA9IhiRupG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LPtO1evrGa",
        "replyto": "LPtO1evrGa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2571/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545072,
        "cdate": 1696707545072,
        "tmdate": 1701465469442,
        "mdate": 1701465469442,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses an interesting problem, one of the common objections is the limited scope and the small size of the provided resource. However, considering that it is a short paper, the scope and the contribution of the paper are adequate."
            }
        },
        "id": "E86JMDxJKq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LNKGWaRtlE",
        "replyto": "LNKGWaRtlE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4779/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598346,
        "cdate": 1696707598346,
        "tmdate": 1701465541503,
        "mdate": 1701465541503,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary of the paper: The paper proposes a novel framework for characterizing and evaluating caricature in LLM simulations. In the process the paper proposes two metrics. They use their framework, in scenarios such as interviews, to identify demographics and topics that are more susceptible to caricature.\n\nPros\n- The proposed framework is novel\n- The problem being solved is timely given the adaptation of LLMs and is very relevant\n- The paper offers interesting and useful insights that might be helpful to practitioners.\n- The paper is sound\n\nCons\n- Empirical support: While there is sufficient evidence in the experiments performed in the paper, there is some shortcomings:   a) “the experiments were conducted on single LLM GPT4” b) human annotated evaluation c) false +ve error analysis. The authors have satisfactorily (somewhat) answered these questions during the rebuttal period. \n- The paper will also improve from fixing typos and figure description"
            }
        },
        "id": "GGmOY19mDV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LGX5hFWPK2",
        "replyto": "LGX5hFWPK2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission217/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482086,
        "cdate": 1696707482086,
        "tmdate": 1701465391491,
        "mdate": 1701465391491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduced a new large-scale benchmark for link prediction derived from Wikidata5M. This dataset focuses on fact prediction for unseen entities given a variety of contextual information. \n\nReviewers appreciated the importance of the problem space, and detailed evaluation presented in this paper, but they concerned about the motivation of semi-inductive link prediction and dataset construction."
            }
        },
        "id": "3QCQftBTyS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LDAgFeA55o",
        "replyto": "LDAgFeA55o",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3964/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580215,
        "cdate": 1696707580215,
        "tmdate": 1701465515429,
        "mdate": 1701465515429,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces SummIt, which iteratively refines its outputs using LLM self-evaluation and feedback. The paper is well-written with thorough experiments, the approach performs competitively on automatic metrics and also receives positive feedback during human evaluation. While the approach shows promise, reviewers highlighted some concerns about the model's adaptability beyond CNN and XSum, its comparison to the fine-tuning approach, and mixed results on the faithfulness evaluation."
            }
        },
        "id": "wE5xbY7c8S",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "LCEbV5nsb8",
        "replyto": "LCEbV5nsb8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission442/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487881,
        "cdate": 1696707487881,
        "tmdate": 1701465399097,
        "mdate": 1701465399097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a novel approach to address the problem of automatic evaluation for conversational information retrieval (CIR) tasks. The authors propose a method called HumCoE that leverages active testing to reduce the amount of human labor required for evaluation. The selection of representative data for human annotation is done using a surrogate model, which generates pseudo labels that differ from the CIR model's prediction. The paper presents empirical results, demonstrating that HumCoE can produce evaluations comparable to full human evaluation with much less human labor. The authors apply this method across multiple CIR tasks, showing its versatility and effectiveness.\n\nThe paper addresses a significant challenge in CIR research, particularly the labor-intensive aspect of human evaluation. The proposed method brings together active learning and human-machine collaboration, making it an innovative solution in the field. The paper presents a comprehensive empirical analysis across multiple datasets and tasks, demonstrating the effectiveness of the approach compared to baseline methods. The application of active testing in CIR is a novel contribution, and the approach could potentially inspire future research in human-machine collaboration for system evaluation.\n\nWhile the paper's contributions are notable, there are several areas where it could be improved. One of the main concerns raised by the reviewers relates to the clarity and depth of explanation in the methods section, which seemed to be hard to understand. The use of the surrogate model could be better explained, and it is suggested that a few running examples could help clarify this. There are also questions about the sensitivity of the final HumCoE scores to the choice of the surrogate model. It would be beneficial to include more discussion or experiments showing how using different models for the surrogate model affects the scores. It has also been highlighted the need for a clearer explanation of the evaluation metrics used (or cite relevant works), namely the stability and consistency ones, which will increase the readability. It would be helpful if the authors could provide references to other works that use these metrics, or explain them in more depth. The choice of datasets and models used in the study has also been questioned. The reviewers have asked for more clarity on why certain datasets were chosen, and why only Bert and Bart were used instead of other models. They have also suggested that the authors could provide more detail on scenarios where HumCoE may not work as well, perhaps through additional error analysis.\n\nDespite these areas for improvement, the paper presents an innovative approach to a significant problem in CIR research. The authors are encouraged to address the concerns raised by the reviewers to further strengthen the paper. In particular, the authors should provide more clarification on their methodology, the choice of datasets and models, and the evaluation metrics used. Further experimentation with different surrogate models and a deeper analysis of scenarios where HumCoE may not work well could also enhance the paper. AC acknowledged that the authors tried to address most of these issues during rebuttal."
            }
        },
        "id": "6z6PozKDAR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "L8W6RyMRmL",
        "replyto": "L8W6RyMRmL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3159/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557217,
        "cdate": 1696707557217,
        "tmdate": 1701465488755,
        "mdate": 1701465488755,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This submission introduces a training dataset for Galician Semantic Role Labeling, and a pre-processing method called verb indexing, to improve the accuracy of parsing complex sentences. The proposed method has been also evaluated on Spanish to measure its impact. \nAfter author rebuttal, the reviewers discussed it, and they agreed that a new resource for Galician is valid and worthy for the community. However, they also point out the need to improve the evaluation comparison with recent multilingual SoTA SRL works that provide better results (not included in the submission). Moreover, the paper would also benefit from a better explanation of the novelty of their proposed verbal indexing method."
            }
        },
        "id": "JPxMRebeea",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "L8Cxea5krb",
        "replyto": "L8Cxea5krb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4485/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707590782,
        "cdate": 1696707590782,
        "tmdate": 1701465532756,
        "mdate": 1701465532756,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose O-LoRA, a new method for continual learning with large language models. For each new task, a new set of LoRA parameters is learned, but, different than the straightforward IncLoRA baseline, an orthogonality regularizer encourages the LoRA parameters to add new information compared to prior sets of parameters. Experiments vs. baselines demonstrate promising performance improvements. In addition, the proposed method is parameter efficient, and doesn't require task replay, which are important practical considerations, e.g., for privacy reasons.\n\nReviewers were mixed in their assessments, even after significant discussion.\n\nA summary of the more negative leaning reviews/associated discussion:\n\n===\nReviewer jxjZ maintains some concerns, including: 1) are the baselines LoRA or not? --- the authors say that \"except for prompt-based methods such as L2P and LFPT5, all others are implemented based on LoRA\" but the reviewer reasonably points out that the most plausible read of the results \"suggests that all model parameters are fine-tuned for EWC and Replay.\" (which might cause more forgetting than just using LoRA); 2) How will the promised SAM-based results compare?; and 3) Were hyperparameters like LoRA r were optimized for the baseline?\n~\nReviewer GrPc shares jxjZ's concerns about hyperparameter configuration optimization, but also, mentions yet another missing method from ICLR 2023 from Razdaibiedina et al. 2023 (which the authors promise to compare to, but experiments are not finished).\n\n===\n\nAmong these concerns, I find the points about the baselines most compelling: it's not clear if they are LoRA versions (the authors didn't reply to the reply from jxjZ) and, as both reviewers mention, it's not clear how hyperparameters of baselines were optimized, nor is it clear how the author's method will compare to several missing baselines which the reviewers expected.\n\nOverall: the authors could likely address many of the above concerns in revision for a potential camera ready, but I am hesitant to give a higher rating without seeing 1) the additional clarifications requested by jxjZ; 2) more details of how the baselines were hyperparameter tuned; and 3) how the missing baselines perform."
            }
        },
        "id": "dYYlQekTqd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "L7ZBpZZ8Va",
        "replyto": "L7ZBpZZ8Va",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2477/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543043,
        "cdate": 1696707543043,
        "tmdate": 1701465466316,
        "mdate": 1701465466316,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses program translation and proposes an intermediate language (called distilled code) that prunes language-specific details but preserves the semantics of a program. (It's noticed that _distilling_ here has a different meaning from knowledge distilling).\n\nReviewers generally find the idea interesting - having an intermediate language hasn't been work really well for translating natural language but may be a good direction to go for translating programming language. Overall, the approach is well designed and achieves high performance."
            }
        },
        "id": "APrWD6kjHJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "L7YoWxQq5t",
        "replyto": "L7YoWxQq5t",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2669/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547179,
        "cdate": 1696707547179,
        "tmdate": 1701465472800,
        "mdate": 1701465472800,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers were enthusiastic about the paper and agreed that the authors presented a strong motivation and proposed methodology was novel and robust for the task. The comparators used for the experiments are justified and sufficient and the paper was well-written. The additional results during rebuttal helped clarify some of the concerns raised by the reviewers. While the new results will need to be included before the paper is published, there is strong interest in this line of work and the excitement on the proposed approaches is high."
            }
        },
        "id": "wxuyrM3xLU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "L7IW2foTq4",
        "replyto": "L7IW2foTq4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2009/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532026,
        "cdate": 1696707532026,
        "tmdate": 1701465450419,
        "mdate": 1701465450419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers provided scores 4,4,3 for soundness and 3,3,3 for excitement.\n\nThe following strengths and weaknesses were prominent in the reviews:\n\nStrengths:\n- addresses interesting cognitive device relevant to NLP (R2)\n- interesting dataset that should be easy to use (R1, R3)\n- analysis of model behavior (R1, R3)\n- evaluate a range of LLMs (R3)\n\nWeaknesses:\n\n- only considers zero-shot settings (R1)\n- questionable realism of examples (R1)\n- it was questioned whether the task might be inappropriate for LLMs (R2, R3)\n- dataset is small (R3) and implemention details are underspecified (R3)"
            }
        },
        "id": "9HtPANhYGe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "L4yVLb6cLu",
        "replyto": "L4yVLb6cLu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2311/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539222,
        "cdate": 1696707539222,
        "tmdate": 1701465460895,
        "mdate": 1701465460895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper is about disaster urgency detection. It proposes a data augmentation method inspired by super resolution in image processing. The reviewers highlight the effectiveness of the proposed method (including for unseen disaster events). They also note that the application of super resolution to NLP classification tasks is new and interesting. The reviewers criticize some aspects of novelty, e.g., how the proposed method distinguishes from domain transfer techniques, which parts of the proposed approach are novel in NLP and what the specific challenges of this application were. Two reviewers note that the presentation and formalism requires revision. Finally, this work could benefit from further comparisons to better contextualize the reported results, e.g., by comparing against LLMs or other augmentation methods."
            }
        },
        "id": "9JnZ3dbV9u",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "L0u9Dkito7",
        "replyto": "L0u9Dkito7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3279/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559650,
        "cdate": 1696707559650,
        "tmdate": 1701465492679,
        "mdate": 1701465492679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes to use the learning-to-rank method for out-of-distribution detection with only in-distribution data. The method being proposed is mainly used to lower the desired confidence ranking by decreasing the probabilities associated with incorrect labels. The paper is well-written and well-motivated. However, as all the reviewers pointed out, the paper is restricted to a single task of text classification without the generalization to any other NLP tasks. This limitation decreases the impact of this paper."
            }
        },
        "id": "F5OQB73U6C",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "L0SEfyrLsW",
        "replyto": "L0SEfyrLsW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1116/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505273,
        "cdate": 1696707505273,
        "tmdate": 1701465420929,
        "mdate": 1701465420929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a multi-session chat dataset where each session is separated by a time interval. During this time interval, speaker-related or world-related events may change. The authors also propose a model to handle such dialogues. The reviewers appreciate the novelty of this dataset as well as its thorough presentation and evaluation but also raise some concerns, including confusion in some formulas, lack of novelty of the proposed model (not the dataset), as well as some issues with the evaluation. The authors address the main concerns and therefore I recommend accepting this paper."
            }
        },
        "id": "3KjG4XhvdR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KxGI7hLxAo",
        "replyto": "KxGI7hLxAo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3013/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554234,
        "cdate": 1696707554234,
        "tmdate": 1701465484352,
        "mdate": 1701465484352,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper explores methods for improving the faithfulness of medical summarization. It offers a solution to a tangible issue in summarization systems. The paper is commendable for its clear writing, inclusion of related work, and comprehensive experimental analysis. However, reviewers also pointed out concerns, including an omission of prior work on contrastive learning for summarization, similarities between the error taxonomy in Table 1 and Pagnoni et al. 2021, insufficient testing of FaMeSumm's benefits compared to other CL methods, and several unanswered questions that may need further experimentation."
            }
        },
        "id": "MN8J71rm8e",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KkR8wahYQN",
        "replyto": "KkR8wahYQN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3769/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707574361,
        "cdate": 1696707574361,
        "tmdate": 1701465509149,
        "mdate": 1701465509149,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers agree that the paper is well-written and presents a simple yet effective approach to a common problem in NLP, obtaining structured output from large language models. It formalizes the outputs of various structured prediction tasks as formal grammars. The proposed approach, combining a grammar completion engine for validity and an LLM for plausibility, is intuitive, simple, and demonstrated to be useful. It also shows strong experimental results demonstrated particularly in few-shot learning scenarios. The study provides informative insights into constrained decoding to improve outcomes. Results include both positive and negative findings, making them instructive. \n\nThe demerits of the paper which are suggested improvements include the need for more comprehensive baselines for various structured prediction tasks, a more detailed explanation of the application of the Grammatical Framework (GF), and consideration of factors like latency, task suitability, and compatibility with cloud-based large language models (LLMs). There is a suggestion to explore whether a small amount of fine-tuning could further enhance the model's performance on downstream tasks, surpassing previous supervised methods\n\n\nReasons to Accept:\n(1) The paper is well-written and presents a simple yet effective approach for formalizing the outputs of structured prediction tasks using formal grammars.\n(2) It demonstrates the effectiveness of the proposed approach in few-shot learning settings, which is a valuable contribution.\n(3) The paper provides informative insights into constrained decoding to improve outcomes and presents results with both positive and negative outcomes, which are instructive for the research community.\n\nReasons to Reject:\n(1) While there are no strong reasons to reject the paper, there are areas that could be improved, including providing more comprehensive baselines for structured prediction tasks, explaining the application of the Grammatical Framework (GF) in more detail, and addressing the limitations related to factors like latency, task suitability, and compatibility with large language models (LLMs).\n(2) The size of the contribution is considered modest, and the paper could potentially extend its scope to cover more tasks or compare its approach to fine-tuning for a broader understanding of its effectiveness.\n(3) The proposed method's efficiency in handling complex grammars should be quantitatively analyzed, providing insights into issues like the number of generation steps or parsing failures required to generate correct outputs.\n\nThe authors seem to have written a clear rebuttal as well with one reviewer providing further comments."
            }
        },
        "id": "Fm2uWUjndi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KkHY1WGDII",
        "replyto": "KkHY1WGDII",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4162/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584440,
        "cdate": 1696707584440,
        "tmdate": 1701465522549,
        "mdate": 1701465522549,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a new method for word-embedding-based bilingual lexicon induction (BLI) using generative adversarial networks (GAN). The way GANs are used is taken from previous work. The main innovation is using a graph convolutional network to inform word embeddings about the topological structure of the embeddings, i.e., about its neighbors in the space (although one review disputed the terminology, claiming it is not really GCN).\n\nOverall, the reviewers mainly criticized somewhat weak experimental results and somewhat limited novelty. During the discussion period, the reviewers raised their scores from rather negative to rather positive."
            }
        },
        "id": "5fgIijFfRT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Kjs0mpGJwb",
        "replyto": "Kjs0mpGJwb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1824/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528190,
        "cdate": 1696707528190,
        "tmdate": 1701465443364,
        "mdate": 1701465443364,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This short position paper stresses the need of revisiting our standard practice of evaluating and benchmarking our progress using publicly available test sets, due to data contamination issues in large pretrained models. The paper argues for a need for community effort in identifying and flagging data contamination in available benchmarks. \n\nPros: \n* This is a very timely paper discussing data contamination as a critical issue in NLP evaluations. The paper makes several suggestions and identifies important directions for further research in this area.\n* The authors proposed a community wide effort in detecting data contamination, building a contamination registry, and promoting transparent evaluation protocols.\n\nCons: \n* The reviewers have pointed out that the paper lacks any empirical results or a clear standardized methodology to identify and measure data contamination. \n\nWhile I agree with the reviewers that the paper would improve (and be more impactful) with empirical evidence or a concrete plan for the contamination registry, I agree with the authors’ rebuttal that we are in a dire need of a wide discussion in our community on this critical issue. As a short position paper, it will raise awareness and engage practitioners to work towards that direction.  \n\nOther concerns were adequately addressed in the rebuttals.  If accepted, please include discussion addressing all the raised concerns to improve the paper."
            }
        },
        "id": "IhcKbp0se7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KivNpBsfAS",
        "replyto": "KivNpBsfAS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3019/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554419,
        "cdate": 1696707554419,
        "tmdate": 1701465484508,
        "mdate": 1701465484508,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers unanimously agree that the paper is both sound and exciting. The SWORME framework it introduces can improve large language models' understanding of polysemy and sense extension processes, while being easy to understand and clearly explained. The approach is well based in linguistic and cognitive science, making it even more interesting."
            }
        },
        "id": "URt2kw7EjD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KgcuY2KIkf",
        "replyto": "KgcuY2KIkf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2017/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532296,
        "cdate": 1696707532296,
        "tmdate": 1701465450768,
        "mdate": 1701465450768,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper focuses on evaluation metrics in natural language generation, proposing a new framework for evaluating these metrics inspired by measurement theory. The authors have devised a new framework, influenced by measurement theory, to evaluate these metrics. The ideas are effectively communicated, and the paper is well-written. Additionally, the authors have presented a case study on text summarization to demonstrate the scoring of different metrics on various dimensions and their robustness and validity implications. The paper's only suggested improvements are to expand the related work section and add the missing references mentioned by the reviewers, which can be easily addressed with an additional page. All in all, I regard the paper as of high quality and a significant contribution to its field."
            }
        },
        "id": "sgbJx1fNs8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KfJffhdWO1",
        "replyto": "KfJffhdWO1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2531/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544092,
        "cdate": 1696707544092,
        "tmdate": 1701465468089,
        "mdate": 1701465468089,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a concentrated contribution in terms of creating a procedure to improve pacing in story generation. All reviews (after rebuttals) agree that the work is sound in terms of experiments that support its claims and a potentially novel methodology that can be built on."
            }
        },
        "id": "N93euisfNX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KUSzNKRI2g",
        "replyto": "KUSzNKRI2g",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5156/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608225,
        "cdate": 1696707608225,
        "tmdate": 1701465551461,
        "mdate": 1701465551461,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agreed that the proposed method to take advantage of the “chain of thought” structure for Argument Mining is novel and sound. The reviewers highlighted some drawbacks in their reviews (i.e., the experiments and the results lack of key details like on the training process, and deserve a deeper investigation like for the issue of considering full argumentation graphs). The reviewers appreciated the author rebuttal which addressed most of the mentioned issues."
            }
        },
        "id": "RWkuQxcrfv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KTFxOnrbvu",
        "replyto": "KTFxOnrbvu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4374/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588527,
        "cdate": 1696707588527,
        "tmdate": 1701465529701,
        "mdate": 1701465529701,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies how knowledge is fused in models, performing an empirical study and comparison of the existing approaches. Furthermore, experiments with randomly injected knowledge demonstrate that these models aren't behaving well as expected and that the misaligned information may result in improvements. Highlighting limitations in this approach for modelling that the community can rally around and address.\n\nThe reviews for the paper are generally strong and consistent. Weaknesses identified by the reviewers identify that the paper only contains empirical findings rather than analytical findings and regarding missing information. The missing information is adequately added in the rebuttal and response from the authors and I find the empirical contributions to be sufficient."
            }
        },
        "id": "DnL3qaWzMo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KSjnVt9awC",
        "replyto": "KSjnVt9awC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5767/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618238,
        "cdate": 1696707618238,
        "tmdate": 1701465566674,
        "mdate": 1701465566674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces HuatuoGPT, a large language model trained for Chinese medical consultation tasks. The model leverages distilled data from ChatGPT and real-world medical data, along with a mixed-feedback reinforcement learning strategy (RLMF). The paper shows that the model performs well in both single-turn and multi-turn medical dialogue as well as medical QA tasks, often outperforming existing open-source large language models (LLMs). The reviewers generally agree that the paper is technically sound and reports state-of-the-art results. However, they express concerns about the paper's incremental contributions and lack of significant experimental validation. All reviewers highlighted that the work appears to be an engineering-focused extension of existing techniques rather than a novel scientific contribution. There are concerns about the limited size and representativeness of the test sets used for validation, which raises questions about the generalizability of the results. Considering the paper's technical soundness and its ability to contribute to the field in a meaningful, if incremental, manner, the paper is thus recommended to be accepted to the \"Findings\" track. To further enhance the paper's value, the authors may wish to consider addressing concerns raised by the reviewers."
            }
        },
        "id": "UzR6ZR9SiJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KRQADH68fG",
        "replyto": "KRQADH68fG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1480/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696708162425,
        "cdate": 1696708162425,
        "tmdate": 1701465432109,
        "mdate": 1701465432109,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a novel data-augmentation method to reduce spurious correlations when training on NLU datasets. Proposed method uses another pre-trained Masked-LM to generate perturbations input features and therefore reduces the cost compared to previous work (Z-aug). Reviewers agree on the soundness of the results and find them interesting."
            }
        },
        "id": "n9X9gSoPB4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KOxEqQzvOZ",
        "replyto": "KOxEqQzvOZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3662/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567231,
        "cdate": 1696707567231,
        "tmdate": 1701465505828,
        "mdate": 1701465505828,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors present a method to enable more consistent persona based dialogue generation for LLMs by incorporating situated, habitual knowledge in the form of schemas.\n\nMost reviewers (at least the ones who have acknowledged the rebuttal) agree that this is a exciting approach to enable more complex personas in dialogue with LLMs, though there are some questions (partially answered) about human evaluation protocols and inclusion of gold responses, etc. especially as GPT 3.5 is used making the results somewhat irreproducable. Overall, I believe the pros of this approach outweighs the cons and merits presentation for further study."
            }
        },
        "id": "EIbD846sOY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KNFG5KLXD3",
        "replyto": "KNFG5KLXD3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2142/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535448,
        "cdate": 1696707535448,
        "tmdate": 1701465455557,
        "mdate": 1701465455557,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "ABSA is such a well-explored problem. The authors focus on eliminating the \"noisy interference\" issues in ABSA introduced by usual methods (dependency tree, attention) by proposing a scope-assisted multi-view graph contrastive learning framework. The paper is comprehensive, structured and easy to follow. The reviewers agree that the authors achieved state-of-the-art results on multiple datasets. In terms of the novelty of the task, as I mentioned, it is a well-explored one; however, their methods supposedly overcome the interference problem. The responses made by the authors to the reviewer's questions seem justified. The paper might find an audience."
            }
        },
        "id": "pQzxvZeG6U",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KIysY1fMCJ",
        "replyto": "KIysY1fMCJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission472/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488599,
        "cdate": 1696707488599,
        "tmdate": 1701465400220,
        "mdate": 1701465400220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper studies the robustness of LMs to choice of named entities (e.g. person name) and their effect showing that there is high variance in downstream performance when NEs are substituted. This is an interesting finding motivating the community to build stronger ways to validate models or build more robust models. The paper presents the findings only, but does not present any suggestion on how this could be improved. I think this is a reasonable contribution for a short paper. The paper presents other analyses regarding the frequency and token length.  There is a reasonable consensus among reviews that the the methodology is sound, but the excitement scores are mixed. I agree that the work has potential, but without mitigation, it is hard to take any action about this finding other than to check for it in other models.\n\nReviews iCLV raises some concerns that we do want models to be sensitive to NEs. Premise that name changes shouldn’t alter the answer is not necessarily true. I think this is an interesting discussion point that the authors should further address where name sensitivity is recommended and where models should be expected to be insensitive to this."
            }
        },
        "id": "iqLeyv8v9M",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KHfQKygNSc",
        "replyto": "KHfQKygNSc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4343/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587820,
        "cdate": 1696707587820,
        "tmdate": 1701465528501,
        "mdate": 1701465528501,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the interpretability of the contrastive learning-based sentence encoder. More specifically, the authors use the integrated gradients method to analyze the SimCSE contrastive learning model and investigate how the encoders internally weigh input words to encode a sentence. The experiments reveal that contrastive learning induces the encoders to weight input words according to information-theoretic quantities, which are somewhat similar to previous sentence embedding methods.\n\nI did not find in the reviews any strong reason to reject the paper except that its most important contribution is theoretical rather than empirical. To me, this is not an actual limitation and I think this paper can make a good contribution to the EMNLP audiance."
            }
        },
        "id": "o6EGLrNGAZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KFieG8rclT",
        "replyto": "KFieG8rclT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission698/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494020,
        "cdate": 1696707494020,
        "tmdate": 1701465407784,
        "mdate": 1701465407784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper focusses on legally enforceable hate speech detection.\nThe most important contributions is the release of a dataset annotated by legal experts.\nThe paper also presents a useful benchmark for comparison of future systems, which includes various models, model setups (zero-shot, tuning) and evaluation setups. \n\nThe criticism that the paper \"lacks a broader analysis of other aspects related to hate speech and its impact.\" is somewhat irrelevant to the main contribution it is making."
            }
        },
        "id": "OoSReEnOgS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KEH6Cqjdw2",
        "replyto": "KEH6Cqjdw2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1174/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506610,
        "cdate": 1696707506610,
        "tmdate": 1701465422674,
        "mdate": 1701465422674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper primarily introduces a pre-trained BERT model tailored for implicit hate speech detection. The paper introduces ToxiGen dataset, which is built with ChatGPT, to generate positive samples. The paper also introduces a contrastive learning appraoch called ConPrompt. This contrastive learning approach similar to simCSE.\n\nOne of the key claims in the paper is the cross-data set performance. This is a nice feature as exposure to identity terms in one dataset can cause the model to be overly confident on that data, and not transfer well to unseen task settings. However, while the model does exhibit performance increases in this transfer setting. It is not exactly clear whether this is an issue with the model or the dataset. Examining table 1, Large benefits are provided when trained on IHC, but these are much less pronounced with SBIC-H and DH. There was no discernable difference in the in-dataset setting. \n\nThe paper received mixed but positive reviews from the reviewers. The reviewers have concerns about the use of machine-generated data. Additionally, ethical concerns regarding privacy and potential misuse were raised by the ethics meta reviewer. The use of machine-generated data, while innovative, necessitates a thorough discussion on its implications, especially in a sensitive domain like hate speech detection."
            }
        },
        "id": "M8vsVjXn0A",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KE9MKZOOca",
        "replyto": "KE9MKZOOca",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3018/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554343,
        "cdate": 1696707554343,
        "tmdate": 1701465484452,
        "mdate": 1701465484452,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper provides solid experiments over a variety of settings, making a convincing case quantitatively for the claim of improvement over the baseline that does not include training with explicit syntactic knowledge. An issue brought up in each of the three reviews is proper placement of the proposed approach in the context of existing work on adding syntactic knowledge of language models. Although direct quantitative comparisons may not be straightforward, the authors can still present a qualitative case for the approach, contrasting it more directly with existing work in greater detail, since the reviewers identified this as a weakness of the paper. A second issue is whether the approach would be effective with other kinds of models, especially those in more current use, but this is beyond the scope of the current paper. Overall, the paper presents interesting results that add to the understanding of a kind of language model that may not be the most current, but was widely used in recent years and is still relevant."
            }
        },
        "id": "faoXTC0PrY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KE5QunlXcr",
        "replyto": "KE5QunlXcr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission55/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477979,
        "cdate": 1696707477979,
        "tmdate": 1701465385529,
        "mdate": 1701465385529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The proposed technique to evaluate factuality in a zero-shot manner leveraging probability changes of a moderately sized model (LLama-7B). The reviewers all agree that the proposed method is extensively evaluated and would be useful for the community. The authors should clearly distinguish their approach from prior work (HaRiM) in future versions."
            }
        },
        "id": "brPEl3B7Te",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "KCe98ynJl3",
        "replyto": "KCe98ynJl3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1369/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511299,
        "cdate": 1696707511299,
        "tmdate": 1701465428714,
        "mdate": 1701465428714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses source task selection for transfer learning. In particular, the authors construct a large-scale benchmark for pairwise task transfers for 22 NLP tasks, and introduce a method that predicts the transferability from a source task to a target task. The experimental results show the effectiveness in single-task and multi-task transfer settings.\n\nPros / Strengths:\n- Research questions and proposed method are sound and well-designed\n- Paper is well written\n- Experiments are comprehensive and show the effectiveness of the proposed approach\n- Observations on the commutativity and transitivity are insightful and potentially valuable to the community\n\nCon / Weaknesses:\n- Scalability: The authors acknowledged in their response that a new target task needs to share \"sufficiently similar traits\" with the tasks in TaskWeb for their method to work. While this is also briefly mentioned in the limitation section, the authors should elaborate more on which steps are necessary (and how computationally intense those steps might be) if a target task is not sufficiently similar to the tasks in TaskWeb\n- Simple baselines missings\n\nAction items for improved version of paper:\n- Add simple heuristic baselines\n- Clearly define limitations (see above)\n- Clearly define \"zero-shot\" setup (having access to labeled examples of a target task is typically not refered to as zero-shot)\n- Add statistical significance tests"
            }
        },
        "id": "SSwGmo3WBA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "K8ixbJPkMQ",
        "replyto": "K8ixbJPkMQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1977/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531250,
        "cdate": 1696707531250,
        "tmdate": 1701465448957,
        "mdate": 1701465448957,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The question paper rotates around the automatic prompt generation theme for NLP tasks. The authors delve deep into the mechanics of prompt tuning, emphasizing human readability and the efficiency of the prompts. Their primary contributions include introducing the FluentPrompt method, analyzing factors making prompts effective, and an unsupervised approach to generating prompts without relying on labeled data. While there is consensus on the paper's merits, some reviewers have raised concerns about certain study factors.\n\n**Reason To Accept**\n- Originality: The paper offers the NLP community fresh insights through several points, which detail the attributes of a good prompt. The characteristics emphasized—topic relevance and calibration for label bias help understand why prompt tuning is practical.\n- Methodological Contributions: The proposed FluentPrompt technique is an innovative solution incorporating fluency constraints to generate effective prompts. Along with an unsupervised approach, this method consistently outperformed baselines across experiments.\n- Analysis: Reviewers praised the exploration of factors contributing to prompt efficacy. The emphasis on topic relevance and calibration aids researchers in understanding the nuances of effective, prompt design.\n- Potential for Follow-up Research: The analyses presented, especially concerning prompts' attributes, are anticipated to inspire subsequent studies in the domain.\n**Reason To Reject**\n- Limitations: Despite its potential, the FluentPrompt method is not perfect. The so-called 'human-readable' prompts sometimes need to be clarified, raising concerns about their true interpretability.\nScope of Analysis: Several reviewers felt the analysis need to be narrower in focus. The effectiveness of prompts is primarily studied for classification problems, which might only generalize to some NLP tasks.\n- Comparison with Baselines: The paper mainly contrasts FluentPrompt with only a few baselines, such as Empty Prompt and AutoPrompt. Some reviewers wanted a broader comparison, including methods like continuous soft prompt tuning and other discrete prompt techniques.\n- Exps Limitations: GPT-2 for experiments raises concerns about the findings' applicability across different models. The studys generalizability remains in question, as no other models were considered.\n$FFinal Verdict:\n\nWhile the paper introduces some innovative ideas and methods in the domain of prompt generation and tuning, certain reservations exist about its breadth and depth of analysis. Addressing these concerns might enhance its credibility and value to the NLP community.\n\n\n\n\n**Overall Summary**\nWhile the paper introduces some innovative ideas and methods in the domain of prompt generation and tuning, certain reservations exist about its breadth and depth of analysis. Addressing these concerns might enhance its credibility and value to the NLP community."
            }
        },
        "id": "KPMeZkgq1J",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "K7p2SnqFoN",
        "replyto": "K7p2SnqFoN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3106/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556226,
        "cdate": 1696707556226,
        "tmdate": 1701465487179,
        "mdate": 1701465487179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a debiasing framework without prior knowledge of the bias. This is a challenging but realistic setting. In particular, they proposed to use one-against-all (i.e., multiple binary classifiers) to construct a bias-only model that better captures the bias in the dataset. \n\nPros: \n- The paper focuses on a critical, challenging, and realistic problem. Specifically, several prior work in bias mitigation assumes type of biases is known but this paper does not make such an assumption. \n- The proposed approach using a set of binary classifiers to capture biases is reasonably novel.\n- Experiments are sufficient to support the claim. \n\nCons: \n- It is unclear whether the approach is practical given the tradeoff between in-distribution and out-distribution performance.\n- Experiments on the BERT-based model are a bit out-of-date. Although it's understandable that the authors want to compare with prior work, it would be interesting to show performance on more recent LLMs."
            }
        },
        "id": "UOTz29AchR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "K6KcA4ODql",
        "replyto": "K6KcA4ODql",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3302/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560097,
        "cdate": 1696707560097,
        "tmdate": 1701465493307,
        "mdate": 1701465493307,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes a two stage method for tabular reasoning and calculations. The two-stage involves using smaller language models to reduce the cost of running larger language models for this task. The reviewers highlighted the comprehensive list of experiments and ablations and novelty of exploring CoT for the task. Reviewers have suggested some revision to improve the presentation of the work which should be considered by the authors and community would benefit from more clear presentation. Also since one of the main contribution of the work is the computational cost it would be good to discuss this along the dimensions of accuracy. It is good to see the limitation section in the manuscript, where authors discussed the limitations including reliance on CoT training data which might not be available in more general settings."
            }
        },
        "id": "6drrCbvlDZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "K5o8oDa0Z0",
        "replyto": "K5o8oDa0Z0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5364/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611579,
        "cdate": 1696707611579,
        "tmdate": 1701465557356,
        "mdate": 1701465557356,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper is about query focused summarization (QFS). It adapts GENIE, a diffusion LM, to query focused summarization. The reviewers highlight that the proposed solution is effective and that it addresses shortcomings of existing autoregressive models via diffusion process that comes with GENIE. Most of the critique of the reviewers centers around the incremental nature of adapting GENIE to QFS, and whether that constitutes a significant contribution. While this is a central point of critique, most reviewers don't rate this as major issue regarding soundness. The next iteration of this paper should, more clearly, elaborate what the novel aspects of this adaptation are and why this is relevant for QFS. Moreover, the reviewers critisize limited analyses, regarding the quality of generated summaries and regarding the reasons for performance improvements. The authors provide additional insights in their response which would benefit the next iteration of this paper. Finally, this paper has been submitted to the efficiency track, though it seems unrelated to that topic. If the authors believe efficiency plays a major role in this submission, they should highlight this more clearly."
            }
        },
        "id": "cHASC0maHc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "K5DBkivtyO",
        "replyto": "K5DBkivtyO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2272/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538329,
        "cdate": 1696707538329,
        "tmdate": 1701465459664,
        "mdate": 1701465459664,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers felt that the work was both sound and exciting. In particular, there was general agreement that the proposed model, and thorough experiments, show a convincing improvement over strong results from previous work. Reviewers had a few points about baselines and clarity of motivation, which the author response addressed satisfactorily."
            }
        },
        "id": "xK7BTdzq5D",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "K35sqjeg5J",
        "replyto": "K35sqjeg5J",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission981/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500978,
        "cdate": 1696707500978,
        "tmdate": 1701465416734,
        "mdate": 1701465416734,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies whether grounding (defined as providing the model with data in modalities that are richer than just text) causes qualitative differences in behaviour of language models as measured by differences in attention patterns and comparisons of embeddings of the models. Reviewers state that the additional definitions of grounding introduced here (“weak” vs. “strong”) are not defined clearly enough, the quantitative analysis should be strengthened and better metrics and visualisations should be used to fully back up the claims made in the paper about whether models that have weak vs. strong vs. no grounding do differ in their behaviours. This paper has useful findings but would be good to have a deeper evaluation analysis to more thoroughly answer the questions here."
            }
        },
        "id": "TV7mQL2P9R",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "K2CrJIcFqg",
        "replyto": "K2CrJIcFqg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1747/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707526199,
        "cdate": 1696707526199,
        "tmdate": 1701465440146,
        "mdate": 1701465440146,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper systematically studies the impact of a large number of diverse MLM properties on task performance and social biases, applying a factor analysis/regression framework. Its main contribution is the large-scale analysis (many factors, many model) enabled by the adopted analysis methodology.\n\nOverall, this is an innovative paper, which suggests a promising methodology for the ever important task of understanding the impact of MLM design decisions on their performance and biases.\n\nThe main weakness is the poor fit of the regression model for the GLUE benchmark, which calls into question the validity of all following analyses on GLUE -- to the extent that I would suggest to remove the GLUE results from the paper. At the very leas, the paper needs to add clear warning messaging around the reliability of the GLUE results. I also suggest to check whether specific GLUE task(s) are responsible for the low R2 value. \n\nI concur with all other concerns raised by the reviewers (and discussed subsequently) and take these into account."
            }
        },
        "id": "btAlRhnVGd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "K1ih2El1IO",
        "replyto": "K1ih2El1IO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4323/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587472,
        "cdate": 1696707587472,
        "tmdate": 1701465527929,
        "mdate": 1701465527929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a method for collecting Twitter data related to emotions and their causes, resulting in the creation of a large-scale dataset. This dataset, sourced from tweets, is designed to identify the cause of a given emotion by prompting state-of-the-art Language Models (LLMs) with specific questions. The dataset encompasses over 700,000 tweets and is categorized into 48 distinct emotion classes. These classes aim to capture nuanced differences between emotions that may fall under a broader category. However, the paper does not clearly outline the motivation behind the creation of this dataset or how it compares to existing datasets in terms of advantages.\n\nThe paper's strengths lie in its ambitious data collection approach, with a significant emphasis on human annotation. The creation of a dataset with 700,000 tweets and 48 emotion categories is commendable, as it offers a more detailed perspective on emotions compared to existing datasets. This refined approach to categorizing emotions could be beneficial for in-depth emotion-cause analysis and might aid in the application of pretrained models in this area.\n\nThe method used for emotion cause extraction seems to be based on a specific pattern, which might not capture the full spectrum of emotions in real-world scenarios. The paper's approach lacks novelty, especially in data modeling, and there's an over-reliance on existing models. The authors, during the rebuttal phase, convincely answered some concers of the reviewers."
            }
        },
        "id": "jEgCefil8E",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JyvycLG00G",
        "replyto": "JyvycLG00G",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5599/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615570,
        "cdate": 1696707615570,
        "tmdate": 1701465563291,
        "mdate": 1701465563291,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents methods for improving the inference speed for parameter shared pre-trained language models, presenting both theoretical and empirical evidence. The reviewers appreciate the importance of the problem, the novelty, elegance, practicality and theoretical foundations, the richness of the results, and their quality. In contrast, there were initially claims about missing experiments and comparisons, which resulted in unjustified claims, and clarity issues. Post-rebuttal, many of these claims seem to be resolved (as evident in the internal discussion amongst the reviewers), but the clarity issues are still very much concerning to some of them. The scaling concerns also still remain, though I have discounted them due to the ACL policy (https://www.aclweb.org/portal/content/efficient-nlp-policy-document) which discourages unjustified requests for larger experiments."
            }
        },
        "id": "Y3X72DvYpo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JwbEwhL3VP",
        "replyto": "JwbEwhL3VP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2928/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552345,
        "cdate": 1696707552345,
        "tmdate": 1701465481391,
        "mdate": 1701465481391,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper adds argumentation related annotations to an existing dataset on US political debates. No additions include argument component and relation annotations as well as 6 common logical fallacies.  It also presents MultiFusion BERT, a model to identify text snippets with a logical fallacy, achieving .74 F1.\n\nReviewers appreciated the strong motivation and the resource the authors construct, as well as a novel model. The paper would benefit from a more detailed description of the model with justifications for the design choices. Also, either expanding to wider domains or providing more detailed discussions within the political science domain would be helpful."
            }
        },
        "id": "jIjc1hLi8k",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Js80TDwMfY",
        "replyto": "Js80TDwMfY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3103/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556193,
        "cdate": 1696707556193,
        "tmdate": 1701465487206,
        "mdate": 1701465487206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduced a new reading comprehension dataset in Chinese named Penguin. The proposed dataset is large, 200k examples. Each one is <Paragraph, Question, Answer, Response>.  The paragraph/question/answer are all from existing datasets, where the Answer is an extractive span from the paragraph. The Response is a more detailed, non-extractive answer to the question, and it is what the dataset user needs to output. \nThe Paragraph, Question, Answer are translated from English datasets, followed by an automatic method to generate potential Responses, then the output is filtered automatically then by human annotators. The test set is human-authored. \nThe manual effort to annotate the dataset is relatively big given the dataset size. \nThe paper presented reasonable baselines (not super strong baselines). They also did human eval to compare Responses vs. Answers, and found Responses to be a lot more helpful to the user. \n\nSome of the reviewers expressed concerns about the naturalness of the dataset, which is a legitimate concern. \nRegarding evaluation, I would also be interested in finding out human performance and how far it is from the best model performance (e.g., an additional row in table 5), followed by an error analysis for the examples that humans get wrong."
            }
        },
        "id": "OuwFZUjERv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JrlSX4nHTv",
        "replyto": "JrlSX4nHTv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2624/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546165,
        "cdate": 1696707546165,
        "tmdate": 1701465471143,
        "mdate": 1701465471143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates the capabilities and limitations of using GPT-k to edit prompts for text-to-image generation. It focuses on understanding the capability of the GPT-k models for text-to-image generation rather than trying to improve it. Overall this is a solid paper. Reviewers found this paper well written and easy to follow. 2 reviewers found the experiments well-defined and executed with concrete examples and good details. One reviewer commented on the lacking of qualitative evaluation and verification of the edit clusters. The authors were able to provide the manual verification results and the cluster error breakdown in the response. This is an important aspect for supporting the finding presented in this paper. The authors should consider add these details in the later version."
            }
        },
        "id": "KY071IUUN6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JotVdrvFtJ",
        "replyto": "JotVdrvFtJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission164/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480778,
        "cdate": 1696707480778,
        "tmdate": 1701465389318,
        "mdate": 1701465389318,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes to solve entity linking from a new perspective of structure prediction. To to this, the authors design a fixed vocabulary strategy to reduce the model prediction space, and address the training/inference mismatch issue by 2-stage finetuning. After serious discussion and consideration, the reviewers' main concerns are about the datasets (GERBIL is a platform easily scalable to new datasets), baselines (comparison with LLMs), as well as the above two contributions. The authors actively provide more evidence which we think has mostly solved the concerns. \n\nOverall, we appreciate both the efforts of reviewers and authors. This work is ready to publish. We hope the authors can continue to improve the paper according to the comments and your response."
            }
        },
        "id": "GVfLd11fBj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Jo9P7hrDdy",
        "replyto": "Jo9P7hrDdy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4416/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589206,
        "cdate": 1696707589206,
        "tmdate": 1701465530925,
        "mdate": 1701465530925,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores the impact of different architectural choices in designing models that can be trained with data containing subjective variance between annotators, looking in particular at the task of argument quality. The authors argue that such models can be organized along a continuum from models in which labels are aggregated ('annotator agnostic') to models in which every annotator is explicitly modeled ('annotator specific').\n\nThe shared perspective of the reviewers wrt this paper include:\n* the paper is well-written and the approach is sound and explored with reasonable baselines\n* the contribution of a continuum of annotator agnostic and annotator specific models and the findings on which models yield best results are interesting and valuable\n* the novelty of the proposed approach is not clear \n\nAs the approach is accepted to be quite sound and there are no scientific or technical issues raised in the reviews, I tend to base my score strongly on this. This is encouraged also by the interesting results provided and the potential application to other tasks. The issues of novelty and comparative discussions wrt existing approaches seems one that can be resolved through elaboration in a camera-ready (as done so in the author response)."
            }
        },
        "id": "80B2GOztLx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JnJsaXfVte",
        "replyto": "JnJsaXfVte",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3743/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707571189,
        "cdate": 1696707571189,
        "tmdate": 1701465508422,
        "mdate": 1701465508422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new method combine the strengths or large models with explicit planning. A contrastively trained model scores reasoning paths to prevent spurious correlations. Reviewers assessed the work as \"high quality\", with \"pretty strong empirical results\" and \"novel and meaningful\". They also raised concerns about presentation and clarity, as well as missing comparison with SOTA baselines, which were addressed in the rebuttal."
            }
        },
        "id": "1PXLA4hvpy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Jk6LA0NGOU",
        "replyto": "Jk6LA0NGOU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission27/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477182,
        "cdate": 1696707477182,
        "tmdate": 1701465384615,
        "mdate": 1701465384615,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Overall, this paper makes interesting contributions to evaluating LMs as agents by framing the evaluation paradigm as gameplay. \n\nThe paper addresses an important topic, takes an innovative (though debatable) approach, arrives at intriguing conclusions, and ultimately could inspire future work as the reviewers collectively describe. With that said, I do think there are important questions around design and decision-making with the games, largely echoing the comments of Reviewer LsBC in their thorough review. More broadly, as the paper partially describes, I am left dissatisfied with the current understanding of the validity/reliability of the benchmark (i.e. the core matters for assessing any measurement approach), the underlying theoretical understanding of the construct being evaluated (I think the paper does an alright job, deferring partially to prior work, but we really need a very good understanding of what we are trying to measure before we can say we are doing it well), and ultimately on how much this buys us over other evaluations. To me, the last point is the biggest question: its clear the evaluation regime here is very distinctive/different, but if it doesn't new/different insights, what's the point? I would like to see the authors engage with this as the core matter if the paper is accepted."
            }
        },
        "id": "bfx0RHtarM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JiUTJJrkL4",
        "replyto": "JiUTJJrkL4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3023/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554546,
        "cdate": 1696707554546,
        "tmdate": 1701465484723,
        "mdate": 1701465484723,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Reject"
            },
            "comment": {
                "value": "In this work, the authors present Caro, a context aware out of domain intent detector that models multi turn dialog contexts, using a multiview approach to remove information not related to intent detection. The authors report very strong results, accompanied by comprehensive experiments, on a pair of variants on the STAR dataset.\n\nThe paper is reasonably well-written, but would be improved if the authors incorporated the notes from the discussion into the manuscript, since some aspects of methodology could be made clearer. The authors did a good job of addressing these questions in their rebuttal. \n\nThe core contributions of this paper is centered around a key insight: the use of the multiview approach to improve performance for this task, which is significant and worth improving upon, and reporting a clear and comprehensive set of experiments on the STAR dataset, which is appropriate for the task.\n\nThe main weakness is that the paper provides results on variants from a single dataset, leaving open the question of whether these improvements are generalizable across datasets. The author's comments on this topic are well-taken, but it is nonetheless a limitation: datasets that would be useful for this aren't available today, but that also provides an opportunity for contributions in this space. \n\nAs such, I think my recommendation below largely reflects the position of the reviewers."
            }
        },
        "id": "MqWP1mAikH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JhzzvJnL9t",
        "replyto": "JhzzvJnL9t",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission62/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478156,
        "cdate": 1696707478156,
        "tmdate": 1701465385805,
        "mdate": 1701465385805,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This article proposes a tree tokenization method that provides a substantial speed up of autoregressive semantic parsing strategies. The method provides a way to do faster autoregressive semantic parsing with comparable accuracy, with comparable speed and higher accuracy compared to non-autoregressive methods.\n\nReviewers agree that some substantial information is contained in the appendix, but argue that this can be resolved within the extra page for a final revision. One reviewer was a bit more negative than the others, based on a claim that has been verified to be false after inspecting the paper. Since no response from said reviewer was received, this has been taken into account in the recommendation."
            }
        },
        "id": "rZPigcDJdU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JhC3lwWDhZ",
        "replyto": "JhC3lwWDhZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission831/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497110,
        "cdate": 1696707497110,
        "tmdate": 1701465412065,
        "mdate": 1701465412065,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the zero-shot dialogue state training problem. The authors introduce a novel in-context learning approach, ParsingDST, which utilizes large language models. This method translates DST into JSON via semantic parsing, acting as an intermediary state. The tests indicate that the proposed approach sets a new state-of-the-art for zero-shot DST settings.\n\nThe soundness scores for this paper stand at (4, 3, 2). In the initial submission, a prevalent concern among reviewers was the outdated evaluation benchmarks. However, the authors addressed this in their rebuttal by presenting results from the most recent benchmark. Consequently, there's a consensus among reviewers on the soundness of the experiments. The score of 2 for soundness primarily reflects the paper's methodological section, which could benefit from clearer exposition. We advise the authors to enhance the clarity of the technical details.\n\nRegarding excitement, the scores are uniformly (3, 3, 3), indicating unanimous agreement among reviewers on this metric."
            }
        },
        "id": "Xsa0Ay4DR5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Jc0sVyM0JP",
        "replyto": "Jc0sVyM0JP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission523/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489848,
        "cdate": 1696707489848,
        "tmdate": 1701465402243,
        "mdate": 1701465402243,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method for obtaining neutral articles from a multi-document summarization system by introducing a polarity minimization loss into the overall loss.\n\nLike at least one of the reviewers, I found the intuition for the specific loss function neither well-described nor well-motivated, nor did I find the authors' rebuttal helpful in further elucidating this part.  This is the crux of the work, so this was rather important.\n\nNevertheless, this paper still presents a worthwhile problem with a novel loss function and good—if sometimes hard-to-follow—analysis."
            }
        },
        "id": "xAMGZ4ZS94",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JaP8ZnOxmi",
        "replyto": "JaP8ZnOxmi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3706/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567965,
        "cdate": 1696707567965,
        "tmdate": 1701465507163,
        "mdate": 1701465507163,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper focus on how to better assist humans to identify translation differences, proposes a new approach, and performs detailed analyses and comparisons between different approaches. \n\nAll reviewers agree that this paper is sound and exciting. The paper is well-written and easy to follow. \n\nBut in the final version, the authors should include discussions on the scale of the data used in the application-grounded evaluations, as pointed out by reviewer  uUif."
            }
        },
        "id": "daWNQfjNNR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JWMIm1EyaE",
        "replyto": "JWMIm1EyaE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4261/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586297,
        "cdate": 1696707586297,
        "tmdate": 1701465525917,
        "mdate": 1701465525917,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers generally agree that the paper provides a comprehensive evaluation of the causal reasoning abilities of GPT models, offering valuable insights for future improvements. However, they also point out that the paper lacks novelty, with some findings already discussed in previous studies. They suggest that the authors could have used more recent baselines for the Causal Explanation Generation task and discussed the order of few-shot demonstrations. The reviewers also note inconsistencies in the paper's claims and results, and suggest clarifications on certain terms and results. The authors' rebuttals address most of the concerns raised, providing detailed explanations and promising to include additional analyses in the revised version."
            }
        },
        "id": "WBB41Q4c4b",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JW3UKn4bmG",
        "replyto": "JW3UKn4bmG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission505/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489418,
        "cdate": 1696707489418,
        "tmdate": 1701465401483,
        "mdate": 1701465401483,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper shows that finetuning with LoRA-based SFT is a good choice for improving the translation performance of LLaMa 7B. It also shows that such a way of finetuning may harms the few-shot translation ability and can be refined by introducing few-shot samples into finetuning. The experiments were conducted on datasets of 10 language pairs.\n\nFortunately, we have 5 reviewers for this submission. The reviewers agree that this paper investigates interesting issues, and some of the results could be of interest to the community, especially given that tuning LLMs for downstream tasks plays an important role in LLM applications. However, some of the reviewers have concerns about the novelty and experiments. Also, the base model selection is another concern.\n\nMy major concern is that the claims here should be examined carefully. For example, the author states that finetuning is harmful to few-shot translation. This might not be a solid conclusion because the author did not finetune the LLM in a few-shot manner. As usual, to achieve good translation abilities for LLMs, we need to use SFT to involve few-shot and zero-shot promotes, rather than zero-shot promotes. Therefore, the finding here is due to the way the author chooses, but not the conclusion we can draw in a general setup. A related problem is that the author uses a relatively large number of samples to tune the model. As is pointed out in related work, more tuning data is not always helpful, and the resulting model tends to overfit this data in many cases.\n\nAnother concern is that the use of LLaMa 7B narrows the scope of this work. I basically agree with Reviewer G2rv in that LLaMa is not the best choice for translation. I suspect that the conclusion may change if we use multi-lingual LLMs and/or increase the model size. In this sense, the results are highly dependent on the LLM used in this work."
            }
        },
        "id": "NWvHFErAMP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JRHhpw77q3",
        "replyto": "JRHhpw77q3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5491/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614066,
        "cdate": 1696707614066,
        "tmdate": 1701465560845,
        "mdate": 1701465560845,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper provides an empirical study of the impact of multiple demonstrations for in-context learning with large language models. Reviewers all agree that the empirical findings are interesting and would be of benefit to the community to understand, especially since they contrast with conventional wisdom that more demonstrations are better. The primary concern from R2 and R3 are with the experimental setup: they perform their analysis with only one model, so the results might not generalize to other LLMs. R2 points out that existing bias in the used datasets may have impacted their results towards confirming their initial hypotheses. R3 notes that certain findings, namely that positive demonstrations are key for in context learning, might have limited utility since they require knowledge of whether a demonstration is positive a priori, which is not available at inference (though the main purpose of the paper, as the authors state, is to gain insight into what makes ICL work). The authors responded by providing further experiments on GPT-3.5-turbo which they claim to be in line with the findings in their paper, as well as some additional dataset analysis. The paper explores and gives insight into a timely research question, and would likely spur on good discussions at the conference."
            }
        },
        "id": "3C3oGeYxvU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JPUx2nVgWa",
        "replyto": "JPUx2nVgWa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission645/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492835,
        "cdate": 1696707492835,
        "tmdate": 1701465406276,
        "mdate": 1701465406276,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agreed that this was an interesting study and the focus on non-English settings will have an impact on many people in the field working with non-English data. Even in a short paper, the authors make several important contributions, including a new multilingual dataset and evaluation of several major LLMs over the data with some interesting implications about the use of models in multilingual settings. There were no addressable reasons to reject pointed out other than lack of depth of some discussions; I do not see any major issues with the soundness of the paper that were pointed out. Several of the reviewers also found the paper to be an exciting direction. There were a few minor presentation and formatting suggestions for the authors to consider, as well."
            }
        },
        "id": "gMIykoXBmo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JNd6XPdaXj",
        "replyto": "JNd6XPdaXj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3607/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566074,
        "cdate": 1696707566074,
        "tmdate": 1701465503473,
        "mdate": 1701465503473,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper extends the Word Mover's Distance (WMD), which measures sentence similarity, by incorporating structural information based on the self-attention matrix of encoder language models (e.g., BERT). Thus, the main contribution is a solid extension of an existing technique. Generally, this paper is well-written and easy to follow, although a Reviewer highlighted that some figures are hard to interpret. Despite the experiments being fairly extensive and based on significant sample sizes (as shown in the rebuttal), the results are mixed. In fact, the Authors report gains in paragraph identification but no change in performance in semantic textual similarity. Thus, the new method seems beneficial only in a subset of sentence similarity tasks. Based on the rebuttal, the Authors convincingly demonstrated that these findings hold true independent of the choice of backbone language model (although it remains uncertain how the method behaves at larger model scales). Potential limitations that remain to be fully addressed are 1) the fact that different encoders may rely on different vocabularies (as a result of different tokenizers), as the proposed solution (i.e., mapping) remains error-prone; 2) a thorough comparison with baselines for sentence similarity based on methods different from WMD, such as BERTScore, which already take into account structural information. The rebuttal has added some of these results to the initial submission; however, they remain incomplete and therefore no definitive conclusions can be drawn from them. Ultimately, I strongly recommend accepting this paper for Findings."
            }
        },
        "id": "7Eqp42utXQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JMbJeMTFos",
        "replyto": "JMbJeMTFos",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission902/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498843,
        "cdate": 1696707498843,
        "tmdate": 1701465414256,
        "mdate": 1701465414256,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes and analyzes pruning techniques to efficiently calculate span-level representations. Reviewers have found the proposed approach solid, and the experiments are thorough, whereas the novelty of this paper is somewhat limited."
            }
        },
        "id": "XuQgyN5k4N",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JMSkoIYFSn",
        "replyto": "JMSkoIYFSn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4936/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601925,
        "cdate": 1696707601925,
        "tmdate": 1701465545180,
        "mdate": 1701465545180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a potentially valuable resource, focusing on a unique domain of dialogue that differs from traditional task-oriented or chit-chat dialogues. The experiments are well-designed, executed, and thoroughly evaluated and analyzed. The paper is also well-written., Concerns raised by reviewers are adequately addressed in the rebuttal and author responses."
            }
        },
        "id": "lKOCg9ajUH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JKmsjKJ0Q8",
        "replyto": "JKmsjKJ0Q8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4684/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595914,
        "cdate": 1696707595914,
        "tmdate": 1701465538493,
        "mdate": 1701465538493,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the task of sequential Model editing (Huang+, 2023) -- fixing erroneous knowledge in LLMs continuously. The speed of existing editors requires continuous modification of LM params and has an increasing cost during sequential editing. \nThis work presents a retrieval-augmented editing framework. it borrows the memory-based editing approach (Mitchell+ 2022) and applies it to the continuous setting. It \n1.  trains a fact encoder and a sentence encoder with self-supervised contrastive losses. \n2.  construct a fact-patch memory \n3. uses a query module then retrieves from this memory to identify and apply edits.\n\nExperiments on FEVER and ZsRE datasets shows the framework can enhance existing model editors by providing relevant facts during editing. The approach scales to thousands of edits.\n\nStrength:\n1. The paper addresses an important problem of sequential model editing (although the paper has given little context of why this task is important in production settings).  \n2. The proposed retrieval approach is novel and the results demonstrate clear improvements in efficiency and scalability over prior SME methods\n3. The proposed framework can be flexibly combined with other editors.\n\nWeakness:\n1. More details of the editing speed and extra memory should be prominently presented in the paper.\n2. I think it would be helpful to discuss the relationship (and pros/cons) comparing SME to retrieval augmented models (e.g. REALM).\n3. more ablation study is needed to explain where the gains come from (e.g., the retrieval vs the fact representations)."
            }
        },
        "id": "2KCjgD0zDK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JIrP8CIvx6",
        "replyto": "JIrP8CIvx6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2048/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533018,
        "cdate": 1696707533018,
        "tmdate": 1701465451749,
        "mdate": 1701465451749,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper evaluates the text-to-SQL parsing ability of various large language models (LLMs), both open-source and closed-source. The paper compares different LLMs, such as Dolly, LLaMA, Vicuna, Guanaco, Bard, and GPT-3.5, on several text-to-SQL benchmarks, using different prompting strategies. The paper measures the execution accuracy (EX) and test suite accuracy (TS) of the generated SQL queries. The paper finds that open-source models are still far behind closed-source models in text-to-SQL parsing, and that there is no single prompting strategy that works well for all models. The paper also observes that few-shot learning with random examples has limited impact on the performance. The paper provides a comprehensive analysis of the current state-of-the-art and the challenges of text-to-SQL parsing with LLMs."
            }
        },
        "id": "wqN0shTA8b",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JI5lhPHVbK",
        "replyto": "JI5lhPHVbK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1648/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521657,
        "cdate": 1696707521657,
        "tmdate": 1701465437295,
        "mdate": 1701465437295,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces AncSetFit, a method for improving few-shot text classification in resource-constrained scenarios. It incorporates semantic label information and focuses on scenarios with minimal training data and small model sizes, aiming for efficiency. The authors conducted experiments on various datasets, comparing AncSetFit to SetFit and ADAPET. Reviewers appreciated the simplicity and efficiency of the proposed method but raised concerns about limited comparisons and the performance relative to ADAPET. While the paper's contributions are smaller due to it being a short-paper, the authors have taken steps to address the reviewers' concerns and enhance the comprehensiveness of their work.\n\n**Key Concerns of Reviewers:**\n\nReviewers expressed concerns about the paper's limited experimental scope, focusing mainly on comparisons with SetFit and not including comparisons with other methods.\nThere were concerns about the performance comparison with ADAPET, which outperformed AncSetFit in some instances, possibly due to differences in model size and computational resources.\nThe authors responded by proposing additional experiments and comparisons, including results with ADAPET using a smaller backbone model and introducing results with the PERFECT method. \n\nThe authors are encouraged to extend the abstract with 1-2 sentences that briefly describe the proposed method in more detail.\n\n**Note:** The review and scores of reviewer \"@Dzxn\" need to be ignored due to the low-quality review."
            }
        },
        "id": "HpZgNl5YGY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JHd4FSJSC5",
        "replyto": "JHd4FSJSC5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1586/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518742,
        "cdate": 1696707518742,
        "tmdate": 1701465435478,
        "mdate": 1701465435478,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers have found paper to be :\n\"Well-motivated paper shedding light on the biomedical applications for minor languages.\"\nDemonstrates impressive performance improvement (up to 10+ points) in cross-lingual zero-shot and few-shot scenarios.\nProviding a valuable insight that knowledge (which is usually multilingual in the KB) is essential for bridging multilingual abilities.\"\n\nFurthermore, the authors have been very proactive with sharing additional analysis requested by the reviewers. We suggest the authors to include the same in the revised draft, e.g.\n- Performance of the model when there are fewer tokens at each granularity level\n- Effectiveness of each granularity separately,"
            }
        },
        "id": "Fyjjf7XkeK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "JC7uPaMwpW",
        "replyto": "JC7uPaMwpW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1616/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707520226,
        "cdate": 1696707520226,
        "tmdate": 1701465436229,
        "mdate": 1701465436229,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors introduce UDAPDR, a method for adapting neural retriever models to new domains efficiently. UDAPDR leverages both expensive language models (LLMs) like GPT-3 and less expensive ones like Flan-T5 XXL, along with multiple passage rerankers, to improve retrieval accuracy in zero-shot settings while maintaining competitive latency. The contributions of the paper include a novel domain adaptation approach, efficient query generation, and empirical evaluation on datasets like LoTTE and BEIR.\n\n\nPros:\nThe strengths of this paper lie in its innovative approach to domain adaptation, particularly the focus on the target domain without using source domain data.   Reviewers acknowledge that UDAPDR improves out-of-distribution generation of neural retrievers without adding latencies at test time.\n This adaptation improvement is consistent across multiple datasets, demonstrating the method's reliability. The approach is also commended for its computational efficiency compared to previous adaptation methods that use synthetic queries. The paper's contributions, including the pipeline's consideration of inference latency and the comprehensive evaluation and ablation experiments, are highlighted.\n\n\nCons:\nThe computational cost of Flan-T5 XXL is also considered non-negligible. Additionally, questions are raised regarding the novelty of the proposed approach, as certain aspects, such as prompts, have been seen in prior works. The paper is compared to a prior method called \"promptagator,\" and it's suggested that the paper should better demonstrate the effectiveness and cost-efficiency of UDAPDR compared to this method. There is also a suggestion to explore the performance of training a first-stage retrieval model using the generated synthetic set. Lastly, the long-term viability of relying on large language models like GPT-3 for the method is questioned, given the potential for hardware advancements and improved efficiency through other techniques like parameter-efficient finetuning and distillation.\nA concern raised by the reviewers  is the lack of strong support for the claim that \"only 1000s of synthetic queries\". \n\nIn summary, the paper presents a promising approach to unsupervised domain adaptation in neural information retrieval models, but concerns have been raised regarding the novelty of the approach, the support for certain claims, and its long-term sustainability with current hardware trends. Addressing these concerns and providing a clearer comparison with existing methods could strengthen the paper's contribution to the field."
            }
        },
        "id": "xoDkvOFoyK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "J9vgDEDjAw",
        "replyto": "J9vgDEDjAw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission297/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484094,
        "cdate": 1696707484094,
        "tmdate": 1701465394149,
        "mdate": 1701465394149,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a method for decoupling topic-agnostic and topic-aware representations for stance detection. This is done via two seperate embedding layers, trained via contrastive learning. The authors further introduce a new dataset for pre-training the topic-aware representations. This method leads to favourable empirical results. However, the method proposed is incremental and there are issues with the presentation. Some reviewers also ask for additional ablation results, which have been reported in the author response."
            }
        },
        "id": "GolW3mZlGV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "J9Vx7eTuWb",
        "replyto": "J9Vx7eTuWb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4248/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707586064,
        "cdate": 1696707586064,
        "tmdate": 1701465525439,
        "mdate": 1701465525439,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper in question investigates the impact of tokenization methods on the predictability of word reading times in large language models (LLMs). It compares the use of subword tokenization to morphological segmentation and assesses their effectiveness in psycholinguistic research. They provide evidence that predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. While the paper addresses an important question, there are notable concerns regarding methodology, theoretical implications, and the overall quality of the results.\n\nPros from the reviews:\n\n- Relevant Research Question: The paper addresses a pertinent question about the influence of tokenization methods on the predictability of reading times in LLMs, which is of interest to both cognitive scientists and the NLP community.\n\n- Clear Methodology: One reviewer praises the clarity of the paper's methodology and its presentation of results, which contributes to the ease of replication and future research in this area.\n\n- Important Implications: The study's findings have implications for understanding the cognitive processes underlying LLMs' language understanding and their utility in psycholinguistic research.\n\nCons from the reviews:\n\n- Methodology and Results Concerns: A reviewer raises concerns about the judgment criteria used in the paper, suggesting that more nuanced criteria could be employed to evaluate the impact of tokenization methods. Another reviewer expresses doubts about the overall predictive power of the surprisal models and calls for more rigorous exploration of theoretical implications. Both reviewers question the cognitive plausibility of LLMs in predicting reading times.\n\n- Lack of Theoretical Discussion: A reviewer criticizes the paper for failing to address the theoretical implications of its findings. The reviewer questions the use of LLMs if they differ significantly from human cognitive processing and suggests that the paper should delve into these theoretical questions.\n\n- Limited Scope: While a reviewer appreciates the clarity and presentation of the paper, they point out that the study focuses on English and suggests extending the research to typologically different languages with richer morphology. This limitation reduces the generalizability of the findings.\n\nI have read the rebuttals and followed the changes after each acknowledgement. The rebuttals helped to clarify many of the reasons to reject the paper. Overall, the conference could benefit from such a short paper contribution, but this short paper contribution could also benefit from some additional content that has been raised in the rebuttal discussions. Initially, various point have been unclear and although the authors could clarify them, the paper could benefit from some additional content. Overall, all reviewers would like to see the paper as a contribution to the field and I can agree with that."
            }
        },
        "id": "DhpZXsyoak",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "J8iaZda5aG",
        "replyto": "J8iaZda5aG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4340/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587774,
        "cdate": 1696707587774,
        "tmdate": 1701465528342,
        "mdate": 1701465528342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the effects of the similarity between pretraining data and finetuning data on downstream model performance. They find that the downstream performance may not correlate with the data similarities. The findings are interesting and insightful to the research community and experiments are thorough and extensive. Reviewers raised some concerns regarding the phrasing and discussions in the paper. Authors addressed these concerns very well. Thus I recommend accepting this paper to main conference."
            }
        },
        "id": "ZdTlPuOr0o",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "J6uWPjukdR",
        "replyto": "J6uWPjukdR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4316/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587324,
        "cdate": 1696707587324,
        "tmdate": 1701465527753,
        "mdate": 1701465527753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Strengths of the paper include the systems contribution with well-motivated individual components. The paper is also written well and clear and the reviewers thought the ideas themselves are useful in guiding the field and discovering novel problems. A potential drawback of the paper is that it was conducted with a proprietary LLM rather than an open-source one which would make it difficult to reproduce results."
            }
        },
        "id": "crTHbfH7nP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "J6pq6AcmbE",
        "replyto": "J6pq6AcmbE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission280/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483624,
        "cdate": 1696707483624,
        "tmdate": 1701465393569,
        "mdate": 1701465393569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In this work, the authors introduce SteerLM -- a new technique for model alignment that allows for specific attributes to be controlled by the end-users during inference (unlike existing alignment techniques such as RLHF). Experiments show that using SteerLM yields responses that are preferred in both human and automatic evaluations, compared to RLHF, and the training cost is also lower than RLHF.\n\nThe reviewers have all rated this work as good or higher on soundness. With moving to a larger model (43B), reviewers asked for more clarity on how to disentangle the positive effect of more pretraining data vs. the proposed attributed-conditioned finetuning. The authors responded with more results on models, that were released post the EMNLP submission, with licenses permitting use by commercial labs. The reviewers also pointed to very recent and relevant references that the authors have promised to include in their revised version. A concern about reproducibility was also addressed with a promise to release the SteerLM recipe on publicly available models."
            }
        },
        "id": "g8qm4LFEVO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "J5FFUHZjNx",
        "replyto": "J5FFUHZjNx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1165/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506405,
        "cdate": 1696707506405,
        "tmdate": 1701465422328,
        "mdate": 1701465422328,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The manuscript addresses critical challenges in the quantization of language models (LMs), focusing on two primary concerns: access to original training data for Quantization Aware Training (QAT) and the loss of generality due to sharper loss landscapes in quantized models. It introduces an adversarial training setup and incorporates sharpness-aware terms in the min-max loss function for QAT. Below is a synthesis of reviewers' perspectives on the paper's strengths and weaknesses.\n\nStrengths:\n\n1. Novelty and Relevance: The paper addresses important problems in LM quantization and introduces new paradigms like adversarial training and sharpness-aware minimization to the domain. These have been prevalent in computer vision but are largely underexplored for LMs.\n2. Theoretical Analysis: The paper provides a robust theoretical framework, particularly through its introduction of a sharpness-aware-minimization algorithm in an adversarial min-max optimization setting. This is backed by convergence rate analysis.\n3. Empirical Results: The paper demonstrates the efficacy of its method on established models like BERT and OPT, highlighting how the method outperforms the baseline, particularly in terms of model generalization.\n4. Transfer of Techniques: The manuscript successfully adapts techniques like adversarial training from computer vision to the domain of NLP, which is commendable.\n\nWeaknesses:\n1. Inconclusive Evidence: Certain empirical results, such as the benefits of adversarial training on perplexity numbers for OPT-350M, are not definitively proven.\n2. Complexity of the Method: The method involves multiple stages, including floating-point training, post-training quantization (PTQ), and QAT fine-tuning, making it challenging to analyze and extend.\n3. Unclear Motivation for Certain Choices: Some reviewers pointed out that the motivation behind zero-shot quantization for generative tasks, and the specific choice of adversarial setup, are not convincingly argued."
            }
        },
        "id": "eIn3XcdJFc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "J2l0R8N3ks",
        "replyto": "J2l0R8N3ks",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3552/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565070,
        "cdate": 1696707565070,
        "tmdate": 1701465501250,
        "mdate": 1701465501250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In their work “Deciphering Stereotypes in Pre-Trained Language Models”, the authors present an in-depth study on stereotypical biases in language models by combining shaply value probing and textual analysis. They find that only a small subset of attention heads encode the stereotypes. Subsequently, they demonstrate that pruning exactly those heads can reduce the biases present.\n\nThe reviewers appreciate this work both in terms of soundness and excitement (after clarifications provided by the authors, all reviewers raised their scores). I suggest the authors to incorporate the reviewers suggestions and revise their manuscript w.r.t. to the clarifications provided, especially related to the potential limitations of the data set created."
            }
        },
        "id": "kE1wfU75Ts",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IwI7Wpkzm7",
        "replyto": "IwI7Wpkzm7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4588/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707593716,
        "cdate": 1696707593716,
        "tmdate": 1701465535931,
        "mdate": 1701465535931,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new model called IdealGPT to utilize LLMs to construct a multiple inference steps for zero-shot VQA tasks. The proposed method outperforms other zero-shot methods.\n\nThe reviewers were all in agreement that the method is sound with good experimental studies, strong results, and interesting ideas. Most of the concerns regarding soundness were minor, regarding more discussion of complexity, ablation studies, experimental details, and also asking for more comparisons and datasets regarding different LLM reasoning abilities, risk of hallucination, and choice of prompts. The reviewers adequately addressed these concerns during the discussion period. Overall, the reviewers were mixed in their excitement regarding the paper, with 2 ambivalent and 1 finding it strong. The reviewers who were ambivalent largely found that the proposed method mostly inherits known limitations of language models, which might limit its use due to existing issues wrt reasoning abilities, risk of hallucination, and sensitivity to prompts."
            }
        },
        "id": "exDxXqgFy1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IvwcvJHLpc",
        "replyto": "IvwcvJHLpc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2344/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540031,
        "cdate": 1696707540031,
        "tmdate": 1701465462030,
        "mdate": 1701465462030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper provides a survey on mental health chatbots, paying particular attention to \"bridging the gap\" between medical and computer science perspectives.\n\n**Pros**: Reviewers all agree on the importance of the problem. Most reviewers highlight the paper as well-written with clear takeaways, bringing light to an understudied area of health-oriented conversational agents (i.e., for mental health). Reviewers find the survey to be \"thorough\" and \"diligent\". \n\n**Cons**: Regarding the last \"pro\", two reviewers agree the survey is comprehensive, but express concerns about potential selection bias (w.r.t the papers chosen) due to an apparently limited number of papers from important venues. Authors provide a detailed rebuttal with exact counts from mentioned venues, but some reviewers still express minor concerns about this. One reviewer would have liked to see even more venues, while another reviewer was still struggling to make sense of the numbers (based on their existing experience). In any case, these concerns may be more minor as evidenced by the scores."
            }
        },
        "id": "DXKAztQ3nY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Itnbse9MMW",
        "replyto": "Itnbse9MMW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4020/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581374,
        "cdate": 1696707581374,
        "tmdate": 1701465517243,
        "mdate": 1701465517243,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a graph convolution approach for the Bilingual Lexicon Induction task. Overall, the reviewers found that the paper presents interesting observations and promising experimental results. Strong experimental results over comparable and the latest method in the field of bilingual lexical induction with the graph isomorphism-inspired method such as IsoVec (Marchiso et al. 2022). However, multiple reviewers pointed out concerns about the clarity of the submission (e.g., math notation [1, 2], lack of definitions of some phrases). The AC would also strongly encourage adding technical details and motivation for selecting components (e.g., why using Graph Convolutional Network but not other graph NN models) but make the paper clear and concise. The AC strongly recommends addressing detailed comments made by Reviewer AjeL to improve the next version of the paper for future readers.\n\n\n[1] In addition to what reviewers recommended for increasing the clarity on math notations, the AC would also suggest using different styles for loss $L$ vs. layer $L$ e.g., by using $\\mathcal{L}$. These small changes pile up and save a lot of mental effort for future readers. \n\n[2] E.g., the definition of $L_{ISO}$ is not clear from the main text, and adding the explanations mentioned in the rebuttal https://openreview.net/forum?id=IsDxBXUEd8&noteId=FV6vFXbdKa would definitely benefit future readers to easily understand the paper."
            }
        },
        "id": "KLWBAgfHFx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IsDxBXUEd8",
        "replyto": "IsDxBXUEd8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission390/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486347,
        "cdate": 1696707486347,
        "tmdate": 1701465397180,
        "mdate": 1701465397180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new theory-driven benchmark, SOCKET, that contains 58 NLP tasks testing social knowledge, grouped into five categories: humor & sarcasm, offensiveness, sentiment & emotion, and trustworthiness.\nLLMs are evaluated on this benchmark, providing interesting findings.\n\nWhile not all the more recent LLMS have been evaluated, the benchmark is the focus of the paper and the findings are already interesting.\nWhile very interesting, the sudy of the social capability transfer could be improve, maybe in a future work."
            }
        },
        "id": "fI4ANktjF7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IqEy2fbpt5",
        "replyto": "IqEy2fbpt5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4772/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598259,
        "cdate": 1696707598259,
        "tmdate": 1701465541297,
        "mdate": 1701465541297,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes Inter-RAT, a method based on causal analysis to enhance the explainability of the neural networks. By eliminating spurious correlations using the causal intervention, the proposed method outperformed previous baselines on multiple datasets. Reviewers all agreed on the potential utility of the proposed method in the NLP community. However, a strong concern remained regarding the improper understanding of structural causal model (SCM), which the paper relied on developing the proposed method, and hence the potentially unreliable validity of the discovered causal relationship."
            }
        },
        "id": "EPXBVGv1Ax",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IpzCUvade7",
        "replyto": "IpzCUvade7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2859/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550961,
        "cdate": 1696707550961,
        "tmdate": 1701465479120,
        "mdate": 1701465479120,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Originality:**\n\nThis paper describes a novel method for domain / user personalization via language model adaptation using a domain encoder trained with a contrastive loss.\n\n**Signifciance:**\n\nPersonalizing language models to domains/users, especially in order to accurately recognize proper nouns and other domain specific vocabulary is very important. This work describes a method that achieves impressive performance improvement by conditioning on cached n-gram statistics from detected close domains.\n\n**Clarity:** \n\nThe consensus appears to be that the paper is somewhat difficult to follow. In general the explanation of methodology and motivation could be improved, including definitions of some basic yet crucial terms which are missing. Furthermore, Section 4.2 difficult to understand given the diverse datasets and task construction described, and much of Section 3.3, which one reviewer found difficult to follow, might have benefited from description in algorithmic, or mathematical terms rather than a purely textual description.\n\n**Pros**\n   - Addresses a relevant and interesting problem\n   - Novel model\n   - Very extensive experiments\n   - Thoroughly described training and decoding parameters and experimental setup\n   - Impressive results on multiple domains\n\n**Cons**\n   - A very complicated experimental setup which may be difficult to reproduce, in spite of the fact that it is well described. \n   - For instance the ASAP task is well defined, but to reproduce results would require recreating the dataset setup from scratch as it is not released as a part of the paper,  nor is the code-base to reproduce results\n   - The paper, as currently written, could significantly improve the narration, motivation and explanation of the data and experimental setup"
            }
        },
        "id": "XP3Ie7ovC7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ipo264MKyt",
        "replyto": "Ipo264MKyt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4138/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583927,
        "cdate": 1696707583927,
        "tmdate": 1701465521604,
        "mdate": 1701465521604,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores the effectiveness of MOE models in vision-language pretraining and VL tasks. MOE has been shown success in both vision and language models and thus it is natural and valuable to also explore its application in VL models.\n\nPros:\n1. This paper presents a comprehensive study of MOE, and strong experimental results are sufficient to demonstrate the effectiveness of MOE. These experimental results and observations are potentially helpful and inspiring for the field.\n\nCons:\n1. As pointed out by reviewers, the novelty and technical contribution of this work is somewhat limited. Many designs are based on existing works and thus it is less exciting in that sense.\n2. Besides, some reviewers also have some concerns about the experimental setting and the model is not large enough compared to state-of-the-art language/vision models (with billions of parameters), despite the word \"scaling\" in the title."
            }
        },
        "id": "2mYquQYh1l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IpJ5rAFLv7",
        "replyto": "IpJ5rAFLv7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1195/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507067,
        "cdate": 1696707507067,
        "tmdate": 1701465423343,
        "mdate": 1701465423343,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "There is a consensus among the reviewers that the paper presents a novel and well-organized approach to tackling implicit sentiments in ABSA. The proposed ELCoM model leverages both document-level and sentence-level coherence through contrastive learning and hypergraphs, and it also incorporates cross-category enhancements to address sentiment conflicts.\nReviewers have commended the paper for its clear presentation and extensive experiments, which demonstrate that ELCoM achieves state-of-the-art performance. However, there are also some concerns and suggestions for improvement. Reviewer 1, for instance, raises questions about the choice of the XLNET representation model over BERT or RoBERTa, and suggests a fairer comparison with these models. Reviewer 2 raises the issue of why ELCoM performs better on implicit sentence accuracy (IAC) than explicit sentence accuracy (EAC) on some test datasets and calls for more explanation. Reviewer 3 suggests that the paper should justify its research purpose by presenting the number and percentage of aspects with implicit sentiment in each dataset and also recommends evaluating ELCoM on the MAMS dataset.\nIn summary, the paper is well-received for its innovative approach to ABSA but also needs to address some concerns and incorporate additional experimental analysis to strengthen its claims."
            }
        },
        "id": "WjDC8hGDJz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "InhYJzIuBi",
        "replyto": "InhYJzIuBi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3320/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560448,
        "cdate": 1696707560448,
        "tmdate": 1701465493868,
        "mdate": 1701465493868,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper highlights the importance of context in stylistic text rewriting, proposing the integration of textual context into rewriting and evaluation stages. It introduces the CtxSimFit evaluation metric, showing that human preference aligns with contextual rewrites and that this metric better correlates with human evaluation. The research explores various rewriting tasks and contexts, emphasizing the significance of contextual modeling in stylistic rewriting.\n\nReasons to accept:\n- The paper introduces the concept of contextual style transfer, which is better aligned with human preference compared to previous non-contextual approaches, bringing a novel perspective to the field.\n- Thorough investigation of non-contextual evaluation metrics and their lack of correlation with human annotations, along with a proposal to integrate context for improvement.\n-Introduction of a new metric, CtxSimFit, which considers both semantic similarity and cohesiveness and demonstrates superior results, offering a valuable addition to the evaluation community.\n-The paper is well-organized and well-written, featuring clear tables and figures that enhance readability.\n-Sound experimentation with three tasks, extensive experiments, and insightful analysis.\n-Clear presentation of research scope and limitations, aiding readers in understanding the paper's coverage and contributions.\n\nReasons to reject:\n-Lack of empirical validation regarding the sensitivity of CtxSimFit metric to the relative weight hyperparameter alpha, and a need for theoretical exploration of the metric's underlying meaning.\n-Insufficient evidence to support the claim that existing automatic text revision metrics are not correlated with human preference beyond the specific evaluation of formality, toxicity, and sentiment.\n-Potential issues with the use of large language models (LLMs) and in-context learning without a clear explanation of their selection and their applicability to smaller models.\n-Unclear definitions of formal linguistic concepts such as \"Coherence\" and \"Cohesiveness,\".\n-Lack of clarification and insight regarding the NSP head and the choice of alpha=0.5 in the formula."
            }
        },
        "id": "QgmhLipiyn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "In4L79U5n7",
        "replyto": "In4L79U5n7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2150/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535630,
        "cdate": 1696707535630,
        "tmdate": 1701465455827,
        "mdate": 1701465455827,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates methods for guiding reward models to better align with human preference, and develops an axiomatic framework for generating a rich variety of performance signals to uphold them. Reviewers found this paper addressed an important research problem of building reward models that better align with human preferences. All reviewers recognized the strong empirical results with the proposed method that a model trained with 220M parameters agrees with gold human-annotated preference labels more often than GPT-4. Two reviewers recognized the comprehensive experiments conducted by the authors which helped add credibility to the study. Reviewers suggested adding additional comparison with strong baseline models. The authors responded it with additional experiment results against the LAION Open-Assistant model and HuggingFace's Stack-Llama model. The authors should consider adding these results to the revised version of the paper."
            }
        },
        "id": "i3O9xoR7wQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IlgpELdUeK",
        "replyto": "IlgpELdUeK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2820/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550092,
        "cdate": 1696707550092,
        "tmdate": 1701465477629,
        "mdate": 1701465477629,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposed a GAN-like approach to generate adversarial samples. These augmented samples are used during training to enhance robustness of the target model in front of various attacking techniques. The reviewers find this work overall sound but with some issues on details, clarify, and ablations, as summarized below.\n\n### Pros\n1. \"Simple but powerful performances regarding the experiments done by authors.\"\n2. \"The efficiency of the proposed method was significantly improved over the comparison methods, allowing for time and cost reduction over prior approaches in generating large datasets.\"\n3. \"Test on three different tasks to reveal the novelty of works.\"\n\n\n### Cons \n1. There are many clarity issues such as missing details, missing ablations and writing problems raised by Reviewer m7F9 and Reviewer CHih, as reflected by relatively low reproducibility scores as well.\n\n2. Ablation analysis is not comprehensive enough to justify authors' choices.\n\nOverall, I think this paper sound but not exciting enough."
            }
        },
        "id": "hHvUoqPAzE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IksoHnq4rC",
        "replyto": "IksoHnq4rC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3318/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560457,
        "cdate": 1696707560457,
        "tmdate": 1701465493754,
        "mdate": 1701465493754,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This study proposes a dataset consisting of approximately 12K claim-response pairs, which are linked to debunked articles. Each claim in the dataset is accompanied by an emotional response, aimed at countering misinformation.\n\nAll reviewers appreciated the contribution of the dataset. However, there are some concerns. Reviewers Emnp, QBDx, and NTyP mentioned the lack of details regarding the annotators, which the authors addressed in the rebuttal. Please include this information in the paper while making revisions. Additionally, an important point raised by Reviewer LBT1 regarding LLM hallucinations should be addressed in the paper."
            }
        },
        "id": "1dp829egx8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ihgea6IIWo",
        "replyto": "Ihgea6IIWo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3561/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565251,
        "cdate": 1696707565251,
        "tmdate": 1701465501587,
        "mdate": 1701465501587,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a seq2seq approach to coreference resolution. Unlike Bohnet et al (2023) which use a transition-based approach to processing a sentence at a time. The proposed approach directly predicts the documents with intext coreference annotations. The evaluation of a number of corpora showed results matching or exceeding the previous state-of-the-art results. The paper also reports results on the smaller LMs which is useful for researchers who do not have access to the resources needed for the large models. The main issue of the paper, however, is they made some strong claims to oversell the paper, e.g. claims on Bohnet et al (2023) are task-specific system etc. Apart from this, I would also expect the author to make the code freely available, and address other issues raised by the reviewers. A discussion on post-processing is also needed, given the LMs might not always generate texts following the input text. How you align the input and output can be a piece of useful information, e.g. Bohnet et al used the 3-word immediately after the mention to find the position of the mention."
            }
        },
        "id": "QHVj0UOikc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IhHB1l1mwp",
        "replyto": "IhHB1l1mwp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4382/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588623,
        "cdate": 1696707588623,
        "tmdate": 1701465529950,
        "mdate": 1701465529950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposed a graph transformer-based approach (Query2Triple) for knowledge graph logical query answering, building upon the pretrained knowledge graph embeddings.\n\nReasons to accept:\n- The task addressed in the paper is relevant for EMNLP audience and it is supported by robust experiments and results.\n- The paper proposes an interesting approach whose results are very competitive with SOTA.\n- Analysis and discussion are robust and insightful for future research.\n\nReasons to reject:\n- some concepts (baselines, contributions, and other minors, see reviews) can be described better in the text."
            }
        },
        "id": "2d1u2nGlWt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IgPf3oLp6B",
        "replyto": "IgPf3oLp6B",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1769/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707526813,
        "cdate": 1696707526813,
        "tmdate": 1701465440866,
        "mdate": 1701465440866,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Authors propose an inference-time solution to control gender inflections in speech translation. Internal language model is replaced by a gender-specific external language model to choose gender (when generating translation) according to speaker's preference. Main result is a significant improvement in gender accuracy  (for reasonable computational cost). Paper is clear and well written, it is a reasonable contribution for a short paper.\nHowever, there were some discussions about the overall translation quality when gender control is activated (small degradation). The paper should also discuss more on how to select the gender-specific external language model. Overall my recommendation is Accept to Main Conference or Findings"
            }
        },
        "id": "FbOKhwcLAH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ie040B4nFm",
        "replyto": "Ie040B4nFm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4034/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581688,
        "cdate": 1696707581688,
        "tmdate": 1701465517595,
        "mdate": 1701465517595,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses an important and under-studied NLP problem. The main contributions of this work are a substantially novel large dataset of story analogies created by crowdsourcing, a valuable evaluation framework to assess the identification and generation of story-level analogies, a method for enhancing analogy generation which makes use of the novel dataset and fine-tuning with specific models like FlanT5.  The dataset is used experimentally to benchmark the performance of several LLMs; results highlight the task is still challenging both for sentence embedding models and LLMs.  Overall, the work has the potential to help advance research in story-level analogical reasoning. However, the presentation could be improved (see reviewer 2 comment about the imbalance in sections). Although some reviewers consider as limitations the lack of coverage of some domains, my view is that the current version of the work still sufficiently supports the authors’ claims. It is recommended that authors’ incorporate all suggestions by reviewers, as also accounted for in rebuttals. Especially, “any details important for understanding the key aspects of the work should be in the paper rather than in appendices, as per ACL reviewing policies.  \n\n**Pros.**\n\n- The described approach to story-level analogies is novel and addresses a gap in existing NLP/AI research; \n\n- The paper is generally well written, and the argumentation is solid, but there is some imbalance in details among sections;  \n\n- It states clearly its claims and goals and provides details about the prompts and the dataset creation process; \n\n- The dataset is extremely valuable and can stimulate future research on the task of complex analogical reasoning; \n\n- The annotation process is solid and carefully thought-through; \n\n- The paper presents extensive empirical results in evaluation of current models' capabilities and shows how tested models can benefit from the newly produced dataset; \n\n- The work has potential for future research.   \n\n**Cons:**\n\n- Important implementation details are given only in Appendix; \n\n- A proprietary tool is used for analogy generation. While this AC tends to agree with authors that this does not weaken the claims nor the results, it hampers full replicability; \n\n- The dataset might be too small for an effective fine-tuning of LLMs; \n\n- Missing justification/explanation of the use of α as the metric for evaluation and cornerstone for analysis;  \n\n\n**Other revisions needed:**\n\n- Fix font issues in figures and tables;  \n\n- Better explain Figures and tables;"
            }
        },
        "id": "NQHfveQEMw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IdXpzsTWRs",
        "replyto": "IdXpzsTWRs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2847/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550658,
        "cdate": 1696707550658,
        "tmdate": 1701465478501,
        "mdate": 1701465478501,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a simple yet effective idea of problem-side refinement as a means to facilitate LLMs’ reasoning ability by refining the problems to be more solvable. The strategy seems effective when used in combination with Chain-of-thought prompting. While this is primarily a prompt engineering contribution, I believe it can be useful and impactful like the CoT idea. The author responses to reviewers’ concerns seemed reasonable and convincing to me.\n\nThere are concerns about the novelty of specific sub-ideas within this idea, and about missing discussion on performance differences between different variants of the proposed idea. The latter particularly can be useful to enhance the paper."
            }
        },
        "id": "5mCPpuNIxi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IdSrFSqhHl",
        "replyto": "IdSrFSqhHl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2800/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549636,
        "cdate": 1696707549636,
        "tmdate": 1701465476779,
        "mdate": 1701465476779,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "A framework for interpretation of rumors and cospiracy theory has been proposed. Its main principles are focused on the attention on response and to analyze summarization module.\n\nThis paper clearly identifies two key drawbacks of existing rumor detectors, lack of robustness against critical responses and lack of interpretability.  Furthermore, the task has been well defined and the experiment results are complete, also providing an interesting ablation study\n\nSome concerns are related to the evaluation of non-graph approaches might also be considered as baselines for a fair comparison, also providing more information about human evaluation in the appendix"
            }
        },
        "id": "UTqqAkcSUP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IadylMsom5",
        "replyto": "IadylMsom5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission983/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707501147,
        "cdate": 1696707501147,
        "tmdate": 1701465416769,
        "mdate": 1701465416769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a randomized pruning approach that is adapted from iterative magnitude pruning (IMP), where multiple masks are generated using weight magnitude as part of random selection and then independently evaluated for one epoch. The mask with the best validation score after that epoch is then selected.\n\nGenerally, reviewers liked the idea and agree that it improves performance when compared against a commonly used approach (IMP) across many evaluated settings. The method is also quite clear and the addition of pseudo-code for the approach makes it easy to implement.\n\nOne point raised by reviewers was the training overhead introduced by this mask selection process. In the rebuttal, the authors provided evidence (via code and floating-point op estimates) to demonstrate that these masks can be generated relatively efficiently (significantly less than one forward/backward pass of the evaluated model. However, the authors do not provide an estimate of the actual training cost of the approach (not just the mask generation). As the paper says that each mask is trained for one epoch (i.e., many forward and backward passes), it seems that this may lead to significant overhead depending on the number of masks per layer, number of layers, and number of pruning stages. A detailed analytical model of this cost and/or training time comparison of this approach compared to IMP would make it more clear how important of an issue this is."
            }
        },
        "id": "tTJw9FGhjo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IaBBd8Fod8",
        "replyto": "IaBBd8Fod8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4679/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595765,
        "cdate": 1696707595765,
        "tmdate": 1701465538314,
        "mdate": 1701465538314,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a method Crystal that introspects knowledge statements related to a given commonsense question and uses this knowledge to provide an answer.  Crystal considers two steps: (a) introspective reasoning and (b) knowledge introspection. The idea of knowledge introspection is similar to Rainier's framework.  Crystal extends this idea to a unified introspective reasoning model. \n\n\nPros: \n- The paper is well-written and thorough with every detail. \n- The idea of optimising the knowledge generator and reasoner in an interleaved fashion is interesting. \n- The qualitative and quantitative analysis is solid. \n\n\nCons: \n- The main concern is that the baseline is weak. Comparison with only DirectQA does not give the full picture since it doesn’t use any knowledge. An ideal comparison should be:\n L_QA = -logp(a*|q,k) where k is the silver knowledge generated by GPT-3 (Table 3). This will show how the overall model improved knowledge quality and reasoning capability.  In Lines 288-289, it is unclear which version of the GPT-3 model is used to generate silver knowledge. Is it the same knowledge as Rainier's paper?  It will make the results in Table 2 more reliable. \n\nFinally, The AreaChair appreciate the authors' response to clarify the doubts of the reviewers."
            }
        },
        "id": "DSHhOl9MX5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IZzZnp7IUs",
        "replyto": "IZzZnp7IUs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission224/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482291,
        "cdate": 1696707482291,
        "tmdate": 1701465391803,
        "mdate": 1701465391803,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper \"Hallucination Detection Using Attention Biases Derived from Human Gaze Data\" proposes a novel approach to hallucination detection by leveraging gaze patterns in the form of human cognitive and behavioral information. The main contributions of the paper include the creation and sharing of the first eye-tracking corpus for hallucination detection. \n\nHowever, there are also several weaknesses as the reviewers raised. Firstly, the experimental results were obtained using a dataset constructed by the authors, and there is no validation on other datasets to verify the reliability of the proposed method. Secondly, the explanations for the baselines in the appendix are overly simplistic, and the authors merely list their experimental results and the compared baselines without conducting further analysis on the experimental outcomes. Thirdly, the paper does not provide a comprehensive analysis of the differences between the proposed dataset and the other published datasets, and it is unclear what specific characteristics set the dataset apart from the others.\n\nA final note is that there are quite a few hallucination related studies released before the submission deadline of EMNLP 2023. But unfortunately, the authors seem to be unaware of these work,  and thus the novelty and reliability of this work can be questionable."
            }
        },
        "id": "3yOJJxgNLV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IZjyMygbw4",
        "replyto": "IZjyMygbw4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5277/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610240,
        "cdate": 1696707610240,
        "tmdate": 1701465554937,
        "mdate": 1701465554937,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This submission studies the noisy label issue in dense retrieval, which is neglected in the existing work. However, the technical novelty of the proposed method is limited since similar technique has been proposed in similar IR tasks. While the rebuttal has highlighted some differences between the proposed method and existing methods, these points are minor, making this work a more incremental one."
            }
        },
        "id": "e5dJxCcXHk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IXuCeFnnxU",
        "replyto": "IXuCeFnnxU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2845/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550618,
        "cdate": 1696707550618,
        "tmdate": 1701465478467,
        "mdate": 1701465478467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposed diffusion models for textless speech-to-speech translation. Proposes integrating continuous and discrete diffusion models.\n\nPros:\n- Performance is surpassing vanilla diffusion models\n- introduces diffusion generative models for textless s2st task\n\nCons:\n- Comparison with other stronger baseline models is not done\n- introduction is too long"
            }
        },
        "id": "mf24Ct4lFv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IUKw6SyCxv",
        "replyto": "IUKw6SyCxv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2952/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552759,
        "cdate": 1696707552759,
        "tmdate": 1701465482167,
        "mdate": 1701465482167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This short paper introduces new parallel resources for Portuguese Sign Language (which is an extremely low resource language) and Portuguese (Portugal): These include a considerable corpus as well as an expert-annotated gold-standard test set which could provide an evaluation benchmark for other work on LGP.\n\nReasons To Accept:\n- This paper presents a large body of data including a small gold-standard\n- The methodology is mostly described well \n- The methodology itself is robust and suitable for the resources available and novelty of the task\n- Addresses a rarely seen but important task\n\nReasons To Reject:\n- Having a gold collection annotated in parallel on a plain (not pre-annotated) text and calculating inter-annotator agreement would make it more reliable.\n- The metrics used may not fully capture the quality of translation, especially for sign language. Additional metrics, such as fluency, adequacy, and subjective human evaluations, already started and presented in Appendix 1, could provide a more comprehensive assessment.\n- There's very, very little information on how the fully automatic rule-based system works. While it's true that this is a short paper, the automatic rule extraction is the most important part of the paper in my view, as it enables all the rest, so not properly explaining how it works is a major weakness.\n\n\nIn my humble opinion, this work is very important to develop tools for sign language. The golden dataset is very precious and this gift to the community is really significant."
            }
        },
        "id": "pZ7TTgTP39",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IT2bT8UigY",
        "replyto": "IT2bT8UigY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3991/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580805,
        "cdate": 1696707580805,
        "tmdate": 1701465516320,
        "mdate": 1701465516320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This article describes some interesting work on the quality of morpho-syntactic analysis of learner sentences in Korean, a new task that raises many interesting questions. The reviewers all emphasised the quality and level of detail of the analyses. Many suggestions were made by the reviewers to improve the quality of the presentation. Given the number of modifications requested, I think a second round of reviews is necessary"
            }
        },
        "id": "2N5J1Xvqul",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IRUGqnZQwt",
        "replyto": "IRUGqnZQwt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission787/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495921,
        "cdate": 1696707495921,
        "tmdate": 1701465410372,
        "mdate": 1701465410372,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper combines the \"Learning neural network subspaces\" approach and the \"Prefix tuning\" to improve generalization of language models in few-shot learning setting. Experiments focus on K-shot (K=50,100,200,500) adaptation of Bert-base on GLUE benchmark. Reviewers find the existing evaluation sound, however recommend adding (a) experiments with full GLUE dataset (b) applying it with another PEFT method.  I recommend authors to consider adding these experiments as these experiments would help reader to understand limitations and potential of the main hypothesis of the paper better. These experiments should require << 6k experiments and worth the time."
            }
        },
        "id": "PkFVCzXxOT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IPPURnxK2S",
        "replyto": "IPPURnxK2S",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4063/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582338,
        "cdate": 1696707582338,
        "tmdate": 1701465518870,
        "mdate": 1701465518870,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper's problem setting is realistic, the proposed method is interesting and clear, and the experiments are solid and address all relevant questions."
            }
        },
        "id": "w2fLs49caW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ILxXKWHkIB",
        "replyto": "ILxXKWHkIB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2808/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549859,
        "cdate": 1696707549859,
        "tmdate": 1701465477154,
        "mdate": 1701465477154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper provides a benchmark measuring how well ML language models can reduce formal mathematical proofs.\n\nPros:\n- Understanding mathematical reasoning capabilities of ML language models is a relevant task, even before the advent of ChatGPT, but even more important now that users commonly expect language models to assist with maths-related topics.\n- The datasets, algorithms, and code are openly available and will be useful to the community.\n- Well-written and well motivated.\n- Proposes an original data-generation methodology combining human experts with a formal environment; can inspire future work.\n- Extensive empirical evaluation on state-of-the-art models.\n\nCons:\n- Somewhat small data set, perhaps restricting the use case to fine-tuning only. \n- Manual annotation effort is not super clearly described, but additional details were provided in the review threads and these could be included into the paper.\n- Some concerns around clarity and fairness of experiments involving GPT-4, which were addressed in the review threads and clarifications could be incorporated into the paper."
            }
        },
        "id": "9fQrk4khQr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ILQnct9H4H",
        "replyto": "ILQnct9H4H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1637/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521496,
        "cdate": 1696707521496,
        "tmdate": 1701465437055,
        "mdate": 1701465437055,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies how physicians might or might not trust the outputs of machine translators to provide these to their patients. A human study was carried out to see how two different tools, quality estimation and back translation, compare in helping physician decide if they will use the MT outputs. \n\nAs one of the reviewers has pointed out, this work makes an important contribution in showing how tools such as QE can be used in real applications in more technical fields where there might be severe consequences for any translation errors. All reviewers agreed that this paper has strong soundness, and some reviewers found it strongly exciting.  Still, some suggestions have been made on how to further improve the writing of the paper. \n\nSome of the limitations of this work include the fact that only 6 discharge instructions with 28 sentences were used. Also, the fact that only the English to Chinese translation pair was considered in the study. In a way, the authors need to consider these to these two factors before making any strong claims on the generality of their study. Also, there seems to be a missing opportunity to study the usage of QE and BT together. Adding this condition would have made the study even stronger.\n \nOverall, this paper is good, and I am recommending acceptance to the Main conference."
            }
        },
        "id": "jgfpjcvcvd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IKz1dWj0I5",
        "replyto": "IKz1dWj0I5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4240/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585921,
        "cdate": 1696707585921,
        "tmdate": 1701465525129,
        "mdate": 1701465525129,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers all thought that this paper covered an important topic from an interesting perspective, in a technically sound way. They also raised various questions and concerns throughout their reviews that would be worth addressing in a revised version of the paper. \n\nThe main suggestion I want to highlight, that was shared across all reviews, was the choice of machine moderators. Reviewers noted some obvious omissions (e.g., Perspective), and suggested some natural explanations for why they might disagree (divergence in training data), that the authors do not appear to have mentioned. The sense I got from reviews was that this could be addressed via writing edits:\n* why were moderators like Perspective not highlighted? (the authors could choose to include additional results from their responses, or they could explicitly provide a reason, e.g., black-box nature of that particular algorithm)\n* what are features of the machine moderators, that might contribute to noise? here the authors could go beyond simply listing moderators in a paragraph of text; a table could help to explicitly contrast algorithm/training data differences."
            }
        },
        "id": "C43HD7s3Hy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IIfdKVyeVh",
        "replyto": "IIfdKVyeVh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission204/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481766,
        "cdate": 1696707481766,
        "tmdate": 1701465391073,
        "mdate": 1701465391073,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper explores methods for generating text summaries with controlled readability. It is well-written and well-organized. The effectiveness of the proposed RL and LA approach is demonstrated through comprehensive automatic and human evaluations. The tradeoff between readability and specificity in this summarization task also holds promise. While promising, reviewers have revised some moderate concerns about the motivation for controlling readability levels, the novelty of the proposed method, and issues with the experimental setup."
            }
        },
        "id": "xUzUYhLIH3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IFNbElsnCi",
        "replyto": "IFNbElsnCi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1287/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509447,
        "cdate": 1696707509447,
        "tmdate": 1701465426305,
        "mdate": 1701465426305,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers found merits in the submission. Especially, it extends the study scope of Aggretriever from monolingual to multilingual scenarios. Although the performance improvement is not so great, I think the technical contribution may be inspiring to the EMNLP community."
            }
        },
        "id": "BnsgX11EZb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "IEH9YsR5Ty",
        "replyto": "IEH9YsR5Ty",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2186/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536521,
        "cdate": 1696707536521,
        "tmdate": 1701465457167,
        "mdate": 1701465457167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a diffusion model for natural language to code generation problems. Code generation is a growing area of research and this paper adds the first diffusion model to improve the diversity of the generated code. Reviewers find that the presentation is clear, the method to be novel and interesting. Reviewers express concern on lack of evidence for why diffusion models work better, for which authors responded with fine-grained analysis of diversity of generated programs. Authors also added the requested baselines such as StartCoder and CodeT5. Authors should add these results along with other clarifications in the revision."
            }
        },
        "id": "dse8TG94WN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "I9DVeu8XKa",
        "replyto": "I9DVeu8XKa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4394/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588875,
        "cdate": 1696707588875,
        "tmdate": 1701465530421,
        "mdate": 1701465530421,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Based on the provided reviews, the paper has several strengths. It presents a novel framework called CESAR that addresses the challenge of handling complex instructions in dialog applications. The motivation and details of the approach are described in great detail, and the paper provides abundant experiments that validate the authors' method. The enhanced InstructDial benchmark, InstructDial++, is considered a meaningful contribution. The results show that CESAR-FLAN performs better than existing approaches on both atomic and compositional tasks.\n\nHowever, there are also some concerns raised by the reviewers. One reviewer questions the lack of comparison and evaluation between CESAR and GPT-3.5-turbo, and suggests providing qualitative examples to analyze the performance gap. Another reviewer suggests providing more transparency in the generation of compositional tasks and addressing the ambiguity in task combination within CESAR. It is also suggested to differentiate the paper from FLAN-T5 and provide more thorough evaluation and comparisons with existing methods or baselines. Furthermore, one reviewer challenges the assumption that open-source models are less capable of handling compositional tasks and raises concerns about the dataset used for evaluations.\n\nOverall, the paper is well-written and well-structured, and provides sufficient support for its main claims. It deepens the understanding of instruction-based multitasking in dialog applications and lowers the barriers to this research direction. However, there are some areas that need improvement, such as providing more detailed comparisons, addressing concerns about task generation and combining, and clarifying the assumptions and dataset used for evaluation."
            }
        },
        "id": "HsLklKoL6q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "I8VTNsq5eB",
        "replyto": "I8VTNsq5eB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2472/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542957,
        "cdate": 1696707542957,
        "tmdate": 1701465466261,
        "mdate": 1701465466261,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a dataset for research in legal NLP on assessing the \"vulnerability type\" (a technical legal term) of cases before the European Court of Human Rights.\n\nPros:\n- Important task which proves to be challenging for today's NLP systems in preliminary experiments (sufficient for the scope of a short paper).\n- Well-presented and well-written. It's obvious that a lot of careful thought has gone into this work, both in setting it up as well as in running the project, and then finally in the write-up. The paper was easy to follow and used a solid strategy for delegating details to the appendix, which can be consulted where details are sought, but that's certainly not necessary to understand the paper.\n- Extensive data annotation efforts to ensure broad coverage of the data set, which are also thoroughly described.\n- Focus on both NLP accuracy as well as explainability, which is critical in this domain.\n- Compliments to the authors on well-considered Ethics and Limitations sections, critical in applications like this.\n\nCons:\n- Limited generalizability as this is a fairly niche task (no less important, but still).\n- Some implementation details (like hyperparameters) were missing in the review copy, but this will be addressed for the camera-ready, as per the review threads."
            }
        },
        "id": "nDffg2ytmv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "I5hTganf3z",
        "replyto": "I5hTganf3z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5478/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613777,
        "cdate": 1696707613777,
        "tmdate": 1701465560552,
        "mdate": 1701465560552,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Three reviewers provided feedback for this work and were in consensus. They found the newly proposed dataset to be interesting and very useful to the community. The task is challenging and need for such datasets is quite high. The reviewers expect that this would be an impactful dataset and would motivate work in the direction of multimodal models with counterfactual reasoning capabilities. They also found the benchmarking to be thorough and well done. The one major concern was positioning the dataset in light of recent works at CVPR 2022. The authors have responded well and have agreed to add these extensive details into the paper. In light of these reviews and discussion, I recommend acceptance."
            }
        },
        "id": "gqzC9dm5rW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "I5NWLjXbQl",
        "replyto": "I5NWLjXbQl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4993/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707603945,
        "cdate": 1696707603945,
        "tmdate": 1701465546695,
        "mdate": 1701465546695,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new parse-execute-refine framework that interleaves logical form generation with KB execution to let a LM take the execution context into account when generating an answer.\n\nThis method yields significant gains over comparable baselines on overall and most partial metrics, including significant gains across the board over the relevant LLM baseline. These are all evaluated on a single dataset.\n\nWhile the results are promising, the paper could benefit from a more thorough comparison to related work that focused on similar tasks and datasets. Alignment and intermediate execution are not novel (they even preceed neural semantic parsing) and, while the approach here is novel, the paper would benefit from a deeper discussion of how it relates to other work."
            }
        },
        "id": "uYH9o7cFMG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "I5BnQIgQIM",
        "replyto": "I5BnQIgQIM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3340/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560817,
        "cdate": 1696707560817,
        "tmdate": 1701465494632,
        "mdate": 1701465494632,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a great theoretical solution and theories and proofs are well explained, however, in practice the results are comparable or superior only with traditional encoding methods like tf-idf, other results where more advanced encoding methods like BERT are used other approaches mostly outperform the proposed methods. \n\nAlthough the proposed method is used in 4 different cross-domains it is still done on a small set of benchmarked datasets, hence, it makes the effectiveness of the method questionable. Sentiment is very subjective and such a theoretical method should be tested on a big dataset instead."
            }
        },
        "id": "uZgieGb9wZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "I4BFSevtRv",
        "replyto": "I4BFSevtRv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2065/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533486,
        "cdate": 1696707533486,
        "tmdate": 1701465452397,
        "mdate": 1701465452397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a method to more effectively do controlled generation by using a reward model as a heuristic during a search-based decoding process. The method is a natural extension of multiple existing approaches and the reviews all agree for the most part that this work is both exciting and has sound methodology."
            }
        },
        "id": "QBHo0l6lsN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "I13VHLJjLO",
        "replyto": "I13VHLJjLO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4200/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585275,
        "cdate": 1696707585275,
        "tmdate": 1701465523954,
        "mdate": 1701465523954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree that the authors present a new method for FVP and show SOTA performance on three benchmark datasets. The paper is generally well-written with comprehensive analysis. The authors did a good job in making a detailed response to the reviewer's queries and clarifying their doubts. The new study on non-text baselines w.r.t. the reviewer comments forms an important artifact to the paper and should be included in the subsequent versions. The same goes for the relevant references from non-NLP journals (financial) to provide more context in the literature review section for this type of interdisciplinary research. In light of these arguments, the paper has merit to feature in the conference."
            }
        },
        "id": "mdqPPlBo8M",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HzecOxOGAS",
        "replyto": "HzecOxOGAS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2535/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544192,
        "cdate": 1696707544192,
        "tmdate": 1701465468239,
        "mdate": 1701465468239,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers overall agreed that the proposed method is simple and effective and that the experiments are sound, with some concerns regarding the amount of improvement over the baselines. The authors have tried to address most reviewers' questions, by providing a fair amount of discussion and justification."
            }
        },
        "id": "c62CFmeDn1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HvYxdKPqYt",
        "replyto": "HvYxdKPqYt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5036/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605647,
        "cdate": 1696707605647,
        "tmdate": 1701465548143,
        "mdate": 1701465548143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a few-shot relation classification dataset called CORE, focusing on company relations and business entities. The paper shows that the dataset is challenging for robust domain adaption, but the scope of the company scenario is a bit narrow."
            }
        },
        "id": "EKAyUgPLk2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HtQvhCRTxo",
        "replyto": "HtQvhCRTxo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3359/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561145,
        "cdate": 1696707561145,
        "tmdate": 1701465495013,
        "mdate": 1701465495013,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies an interesting topic of hallucination in video captioning models. All reviewers positively rated the work and agreed that contributions of the paper, namely two datasets and a novel metric, are significant and exciting to the field. In addition, all reviewers also commented high on the quality of the paper."
            }
        },
        "id": "cBRHireOHD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HtNQXg979A",
        "replyto": "HtNQXg979A",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1214/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507515,
        "cdate": 1696707507515,
        "tmdate": 1701465424404,
        "mdate": 1701465424404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper describes a novel dataset for resolving question ambiguity in open-domain QA and proposes an innovative approach: instead of directly generating disambiguated questions (DQs) for every possible interpretation, Clarification Questions (CQs) are generated to be asked to the user before generating the correct answer.  The dataset builds upon the existing AMBIGNQ and adds appropriate clarification questions for each prompt. CQs are generated automatically, then evaluated and revised by human annotators. Human evaluation of the preference between the two settings (i.e. DQ vs CQ) supports the argument that CQ is preferable. Experiments with BART, however, do not report better performance, which indicates the tasks are challenging. The reviewers initially had different positions but ended up in coming closer on soundness after the author rebuttals.   \n\n**Pros.** \n\nThe paper presents a novel approach to a still challenging task in QA; \n\nIt contributes a novel dataset, useful for future developments in dealing with AQ in QA; \n\nIt establishes a benchmark performance on three tasks: ambiguity detection, clarification generation and clarification-based question answering, for easier and better comparison in future works;  \n\nIt demonstrates the need for better models/ methos to handle ambiguity in QA; \n\nIt is fairly well written, it states clearly its objectives, and authors seem to have covered the related literature well.  \n\n**Cons:**\n\nThe paper lacks an explanation for the use of different models in some experiments (see reviewer SP89 comments). Authors, however, provided satisfactory details in their rebuttal.   \n\nexperiments with ground-truth CQs are missing, which would better complement and explain the result in Figure 5.  As, authors already run such experiments, they can easily be added to the paper; \n\nThe benchmark experiments do not include SOTA models, which makes the claim that the targeted tasks are still a challenge less convincing. For their rebuttal authors performed experiments with InstructGPT and ChatGPT that confirm the claim. This should be included in the paper. \n\n \n\n\nIt is recommended that the authors integrate the promised additional materials into the final paper."
            }
        },
        "id": "SpGX6Ko5za",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HsvZUde6wT",
        "replyto": "HsvZUde6wT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission179/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481152,
        "cdate": 1696707481152,
        "tmdate": 1701465389941,
        "mdate": 1701465389941,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes and addresses a novel problem - the \"length bias\" problem - that of document level machine translation models struggling to translate documents which are much smaller or larger than those in the training data. \nUsing a number of strategies during training (sampling and length-normalised attention) and decoding (sliding decoding) they show strong results on several open datasets and mitigate the length bias problem in the document-level translation task.\nOne reviewer noted that it would have been better to test their model on datasets which display the \"length bias\" problem instead of standard datasets and the authors provided a table with some relevant results. The authors also provided a comparison with other recent document level MT methods and their methods perform better. \nThere were four reviews and lengthy rebuttals with new results and clarifications which led to one reviewer increasing their score."
            }
        },
        "id": "BWVykwfroh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HsGirsKN5l",
        "replyto": "HsGirsKN5l",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5306/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610707,
        "cdate": 1696707610707,
        "tmdate": 1701465555757,
        "mdate": 1701465555757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers appreciated the well-written presentation of a new pretrained model and NER dataset for the field of economics. Several reviewers found issues with the level of detail provided about how the dataset was created, as well as the specific protocols for annotation which may impact the reproducibility and soundness of the approach. Annotations were shared with reviewers during the evaluation period, but the pretrained model was not made available. Reviewers also suggested a deeper description of the motivations of targeting the economics domain, domain-specific considerations that express the unique challenges in economics, and additional or revised analysis. Reviewers also suggest a discussion of related work, both for comparing their approach to other domain-specific pretrained LMs, existing, widely-used NER models, and the core tasks in the economics domain. This paper is unlikely to have an impact in NLP communities due to a lack of novel methods or analysis, but may be a valuable contribution for research in economics."
            }
        },
        "id": "7BWxnbaxK7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Hkj3WyR1JB",
        "replyto": "Hkj3WyR1JB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1826/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528227,
        "cdate": 1696707528227,
        "tmdate": 1701465443394,
        "mdate": 1701465443394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work addresses the problem of assessing the robustness of Natural Language Understanding (NLU) models to speech recognition errors. The method proposed repurposes NLU training data and uses Text-to-Speech (TTS) models to generate synthetic utterances. These synthetic utterances are then processed by Automatic Speech Recognition (ASR) models to simulate speech recognition errors. By comparing the NLU model's performance on the original text and the TTS-generated text, the paper quantifies the impact of speech recognition errors on NLU.\n\nBeyond automating combined ASR-NLU testing, one of the key contributions that the paper provides is in proposing a number of additional metrics that can be derived by this method and explaining the insights that they provide versus existing metrics. That said, more than one reviewer would prefer that WER/F1 rate be provided in the main text to make the argument for these new metrics more compelling."
            }
        },
        "id": "r2ppyef8fb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HkXbOUaL4W",
        "replyto": "HkXbOUaL4W",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2984/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553457,
        "cdate": 1696707553457,
        "tmdate": 1701465483304,
        "mdate": 1701465483304,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes to represent English text with only consonants, in order to increase model efficiency, based on the assumption that consonants are more discriminative than vowels. The proposed idea is simple yet effective. However, Reviewers pointed out several problems in the experimental setup: 1) the paper currently lacks a comparison with alternative strategies to reduce the embedding matrix size, such as dimensionality reduction and quantisation (among others); 2) the neural architectures chosen for the language modelling experiment is obsolete and should include subword-based Transformers. (The Authors provided some first results in the rebuttal to this effect but these should be expanded to be significant). Moreover, the approach suffers from a major limitation, namely relying on retrieving back vowels to make generated text human-readable, which is error-prone and partly reduces efficiency gains."
            }
        },
        "id": "fBm42jOGQ0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HjBDSop3ME",
        "replyto": "HjBDSop3ME",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4369/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588393,
        "cdate": 1696707588393,
        "tmdate": 1701465529486,
        "mdate": 1701465529486,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers are in disagreement about the soundness and exciting qualities of this work, giving scores between ambivalent and strong. \n\nQuestions about soundness centered on the goals of the study. Reviewers critiqued the practical applicability of the method (e.g., when gold data is not available), but the authors clarified that their goal was not to build a universal tool for model improvement but a higher level goal of problem illumination. I believe the authors adequately responded to these concerns, which were the strongest of the soundness concerns. \n\nThe simplicity of their method is a pro, and and the various insights and tests seeking harmful passages are useful in themselves and can inspire future research. These insights contribute to some reviewers finding the work strongly exciting.\n\nThe reviewers suggested clarifying the organization and writing in some sections of the paper, and these change would greatly improve a final version of the paper. I agree with this request and believe the authors can make the paper clearer (as one example: by labeling axes on plots and providing more intuitive explanations in figure captions)."
            }
        },
        "id": "UQ2hHKIUi5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HickNiCqk9",
        "replyto": "HickNiCqk9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3434/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562665,
        "cdate": 1696707562665,
        "tmdate": 1701465497407,
        "mdate": 1701465497407,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In this study, the authors proposed multilingual headline summarization datasets. These datasets encompass 196 language pairs across 14 languages. The authors have made these datasets publicly available for the community. Reviewers mentioned that the dataset would be a valuable resource for the community and that the paper is well-written and easy to follow.\n\nReviewer jf8L expressed concerns about the need for a more detailed analysis of the experiments. These concerns should be addressed to give readers a comprehensive understanding of the experiments. Additional information can be included in the appendix."
            }
        },
        "id": "805g0ExJKe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HhoG04UD3E",
        "replyto": "HhoG04UD3E",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission395/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486460,
        "cdate": 1696707486460,
        "tmdate": 1701465397450,
        "mdate": 1701465397450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper probes to what extent LMs encode conceptual spaces in the sense of Peter Gardenfors’ theory, using a new dataset on the taste domain. The works is interesting and focuses on the problem of how competence of LMs can be related to interpretable semantic dimensions, such as the ones in the Conceptual Space theory. On other hand, one reviewer raises important methodological issues I agree with. These concern the experiments and especially their interpretation, and  MUST be carefully addressed in the revised version of the paper."
            }
        },
        "id": "XJce2RPgKF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HewtRLig9V",
        "replyto": "HewtRLig9V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1062/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504206,
        "cdate": 1696707504206,
        "tmdate": 1701465419247,
        "mdate": 1701465419247,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new method to incorporate explanations from human annotators into active learning. The idea is a very reasonable extension to existing active learning approaches, and dual-model solution also seems clean and effective. There was some initial concern about the generalizability of the approach since the experiment only focused on NLI, which the authors justified by the lack of high-quality explanation annotation. During the discussion period, the authors conducted additional experiments on another dataset, which although showed smaller improvements compared to NLI, validates the soundness of the method. It would be exciting to see how the proposed approach complement alternative active learning strategies."
            }
        },
        "id": "ibLSEXEaN4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Hbqsmv4jqY",
        "replyto": "Hbqsmv4jqY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3635/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566680,
        "cdate": 1696707566680,
        "tmdate": 1701465504917,
        "mdate": 1701465504917,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new evaluation benchmark for of the ability for language models to represent real-world physical concepts, including color, shape, material, mass, temperature, hardness, etc. The paper also performs thorough evaluation of existing text-only and vision-and-language models, including (in the rebuttal) updated results with more recent models that became available after the submission deadline. The paper finds that both scaling and knowledge distillation from vision-language to text-only models improve performance for visual concepts (but not embodied concepts), and that models that use visual features perform better than text-only models. The more recent models also perform better than those available at submission time."
            }
        },
        "id": "yjwWldg62O",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HaSS8a3Oe7",
        "replyto": "HaSS8a3Oe7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission671/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493363,
        "cdate": 1696707493363,
        "tmdate": 1701465407093,
        "mdate": 1701465407093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique problems that pose challenges for human solvers but are easily verifiable. The paper compares the performance of four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. The paper finds that the new-generation LLMs excel in solving stumpers and surpass human performance. However, humans exhibit superior skills in verifying solutions to the same problems. The paper enhances our understanding of LLMs’ cognitive abilities and provides insights for enhancing their problem-solving potential across various domains.\n\nPros:\nThe paper addresses an interesting and novel research question: how well can LLMs solve stumpers compared to humans?\nThe paper uses a large and diverse set of stumpers from various sources and domains, covering logic, math, language, trivia, and creativity.\nThe paper employs rigorous experimental methods and statistical analyses to compare the performance of LLMs and humans on both solving and verifying stumpers.\nThe paper discusses the implications of the findings for the design and evaluation of LLMs, as well as the ethical and social issues raised by their problem-solving abilities.\n\nCons:\nThe paper does not provide a clear definition or formalization of what constitutes a stumper, or how to measure its difficulty or uniqueness."
            }
        },
        "id": "vRYZQIVTF0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HYxJoAWLgT",
        "replyto": "HYxJoAWLgT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission668/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707493310,
        "cdate": 1696707493310,
        "tmdate": 1701465407017,
        "mdate": 1701465407017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes an approach for learning to select prompt tuning layers. The authors formulate this problem as a bi-level optimization problem, and proposes an approach for learning gating functions which modulate the layer selection procedure. Experiments show that the proposed approach is able to outperform various baselines across standard datasets. During the discussion period, the authors have performed extensive additional experiments to address many of the reviewers' concerns. This paper is a solid contribution to the parameter-efficient learning literature.\n\n[As a side note, some reviewers raised concerns that this paper may be outdated or irrelevant given the the ICL + LLM paradigm with closed source models. I do not share this viewpoint, and have not given much weight to these concerns in the decision process.]"
            }
        },
        "id": "yy7m4GraFW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HUzbEPMd6v",
        "replyto": "HUzbEPMd6v",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2451/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542472,
        "cdate": 1696707542472,
        "tmdate": 1701465465540,
        "mdate": 1701465465540,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a few-shot conversational emotion recognition approach using PLMs. The idea is based on a partial refinement design that is training-efficient, avoids backpropagation in refinement, and enhances transferring efficiency. The proposed system achieves high recognition quality, comparable to the existing TSPT method. Additionally, the paper presents Cross-Task Prompt Tuning (CTPT) which is a derivative-free optimization technique for few-shot conversational emotion recognition. CTPT leverages cross-task knowledge, improving learning performance by incorporating external knowledge from other source tasks.  \n\n\nThe majority of the reviewers engaged in post-rebuttal discussions and all agreed that the soundness of this work is mostly sufficient with an acceptable level of excitement and generally reproducible results."
            }
        },
        "id": "a2MT2xkwcr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HS5BWSqK5I",
        "replyto": "HS5BWSqK5I",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission233/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482549,
        "cdate": 1696707482549,
        "tmdate": 1701465392258,
        "mdate": 1701465392258,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a method for incorporating time-sensitive information into pretrained language models. The authors extract time-stamps, encode them as a graph and propose the Time Relation Classification (TRC) pretraining objective. The paper shows promising results on temporal QA and, interestingly, offers better modeling of complex dependencies. This is an important advancement since temporal reasoning is indeed problematic for SOTA LMs.\n\nOne reason the paper got initially moderate scores with the reviewers was due to problematic evaluation: all the three reviewers raise concerns wrt baseline. This, however, seems more like a presentation issue that has been resolved with the authors' thorough rebuttal (cf. discussion). I would strongly suggest, however, that the authors incorporate the relevant discussion into the next version to improve clarity.\n\nAnother issue, raised by all the reviewers in some aspect, is the approach for extracting temporal information from text (pipeline). As it seems, the authors only extract explicit timestamps and then merge them rather straightforwardly. They do not assign temporal information to sentences with no explicit timestamps and do not model non-trivial cases of temporal expressions (e.g., \"later\") -- for example, \"X submitted a paper to EMNLP-2023. They resubmitted it again to ACL\" would be problematic as it seems? However, even with a simplistic approach to extracting temporal expressions, the model achieves interesting results."
            }
        },
        "id": "yI0KhOStlj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HR90GXVHUn",
        "replyto": "HR90GXVHUn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5481/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613922,
        "cdate": 1696707613922,
        "tmdate": 1701465560802,
        "mdate": 1701465560802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes the task of expository text generation and its evaluation dataset. Furthermore, the authors also propose the method named \"Imitate, Retrieve, Paraphrase\" (IRL) for generating more factually accurate and stylistically appropriate expository documents. This three-step approach has advantages for both interpretability and factuality in generating text. The experimental results on the newly created dataset by the authors show that IRP produces more factual and stylistically appropriate expository documents than competing models. The authors answered all questions by the reviewers, and all reviewers agreed with the acceptance of this paper. Thus, we should follow their agreement."
            }
        },
        "id": "RdCCmFQmBW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HQfzPDZJAL",
        "replyto": "HQfzPDZJAL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2256/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538008,
        "cdate": 1696707538008,
        "tmdate": 1701465459221,
        "mdate": 1701465459221,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors introduce an efficient approximation to the computation of the Word mover's distance, a distance metric used to compare distributions which has found applications in NLP through its usage for document classification.\nReviewers highlighted the efficiency brought by the use of approximate nearest neighbour search while only resulting in minor performance drop when using the original formulation. The method seems readily applicable and could open up venues to the usage of Wasserstein distances to larger datasets. At the same time, concerns were raised on the limitations of using this to only one variation of that distance, as well as the small datasets used to perform experiments. However, this work fulfils the call for a short paper which is to propose one neat idea."
            }
        },
        "id": "ImSJPwnf0s",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HNfwD7QOaq",
        "replyto": "HNfwD7QOaq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1196/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507074,
        "cdate": 1696707507074,
        "tmdate": 1701465423353,
        "mdate": 1701465423353,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In general, all reviewers agreed that this research is both sound and exciting.\n\n**Summary of Reviewer Feedback and Discussion:**\n- **Reviewer XsLy** appreciated the neatly-curated dataset and in-depth analysis thereof, as well as the paper's fair performance assessment.  They were curious about a few finer details in the paper, which the authors provided in the rebuttal and promised to include in the revised manuscript.  The reviewer then thanked them for providing these details.\n- **Reviewer Ymez** liked the detailed explanation of the dataset and the authors' careful investigation of the underlying task.  They also appreciated the thorough evaluation, and were curious about a few details pertaining to annotation quality.  The authors provided these details in the rebuttal, and promised to include them in the updated manuscript.\n- **Reviewer r6d1** liked the carefully-curated dataset and thorough analysis thereof.  They also thought that the authors' methods were well-designed, and that the authors had extensively evaluated their work.  They felt that if the data is made publicly available, it may benefit numerous research subfields.  In the rebuttal, the authors thanked them for their appreciation of the study and acknowledgement of these potential benefits."
            }
        },
        "id": "cl1dxp6I8A",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HMVNu8oKAK",
        "replyto": "HMVNu8oKAK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1342/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510748,
        "cdate": 1696707510748,
        "tmdate": 1701465427933,
        "mdate": 1701465427933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Summary:**\nIn this paper, the authors introduce SYMPOMIFY, an extension of the VAERS dataset that includes annotations of symptom information. These annotations were generated using Large Language Model (LLM) knowledge harvesting, particularly with the assistance of ChatGPT. The dataset comprises over 800,000 reports, along with reasoning-based explanations and background knowledge. Additionally, the authors evaluate the quality and utility of the new dataset by conducting symptom recognition tasks across various learning paradigms, providing insights for future comparisons and benchmarking.\n\n**Strengths:**\nThe reviewers unanimously agree on the following strengths:\n1. The article introduces the SYMPTOMIFY dataset which incorporates explanations and background information, offering valuable context and detailed data for symptom recognition, thereby improving the overall quality and utility of the dataset.\n2. The newly introduced dataset in this work is notably substantial, with over 800,000 entries, providing an additional and valuable resource for the research community.\n3. The experiments conducted in the paper are both interesting and up-to-date, including efforts to address rare symptoms, address zero-shot scenarios, and incorporate the Falcon-7B-Instruct model.\n4. The method outlined by the authors expands the existing field of the research on symptom recognition and holds the potential to advance medical decision-making.\n5. The paper offers valuable insights and evaluation results for various baselines and learning paradigms, serving as a guide for future research and comparisons.\n6. The authors' approach harnesses the capabilities of Large Language Models (LLMs) to make AI an accessible tool, rather than a human replacement.\n\n**Weaknesses:**\nAll the reviewers share the same concerns regarding this work. \nFirst and foremost, the annotations in the VAERS dataset are carried out by trained professionals. While it's a commendable idea to assess the quality of these annotations, it should ideally be conducted by experts rather than crowdsourced workers. Consequently, the results of the verification in this work may lack reliability. Given that the primary contribution of the paper is the dataset itself, the accuracy of the annotations becomes a critical concern.\nAdditionally, the evaluation of the proposed method is somewhat limited, and it would be advantageous to have a more extensive evaluation and conduct ablation studies.\n\n**Author-Reviewer discussion and acknowledgment:**\nThe authors have provided clarifications in response to the concerns raised by the reviewers and have outlined the planned improvements to be made during the rebuttal response and discussion phase. All reviewers have responded and acknowledged the authors' arguments.\n\n**Conclusion:**\nThe paper is well-motivated, and the background is well-written. The work is also appropriately contextualized within the relevant areas of research. However, reviewers suggest that the authors correct the identified typos. Furthermore, reviewers recommend that the authors improve the paper by addressing the questions and points raised during the discussion phase."
            }
        },
        "id": "PL7KdifUfq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HKMvR1UaWH",
        "replyto": "HKMvR1UaWH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4502/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707591694,
        "cdate": 1696707591694,
        "tmdate": 1701465533219,
        "mdate": 1701465533219,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents two methods to improve the efficiency of fine-tuning, one relates to randomly dropping some of the input tokens, and the second is based on bucketing.  The reviewers appreciated the quality of the results, and the ablations. Some of the reviewers initially had some concerns around the analysis and the baselines, as well as the scope of the experiments. The rebuttal helped alleviate many of these concerns."
            }
        },
        "id": "A89MMBTSVS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HJTbcidL5a",
        "replyto": "HJTbcidL5a",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5075/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606378,
        "cdate": 1696707606378,
        "tmdate": 1701465549237,
        "mdate": 1701465549237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a model based on BART to generate contextualized representations of non-compositional expressions (specifically, English potentially idiomatic expressions; PIEs), showing good results on PIE processing tasks. Reviewers mostly agreed that the results were sound, though the paper could benefit from more ablations/analysis demonstrating the need for certain modeling decisions. This paper represents a focused work providing a new state-of-the-art for PIE identification, which will be of interest to those in the *CL community focused on modeling idiomatic/MWE expressions, but may be less exciting to the community more broadly."
            }
        },
        "id": "t0F95Ayfmm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HIPPG2SH3u",
        "replyto": "HIPPG2SH3u",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2147/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535571,
        "cdate": 1696707535571,
        "tmdate": 1701465455716,
        "mdate": 1701465455716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper focuses on continual event extraction which sequentially learn different event types to extract. This paper incorporates \"pseudo labels\" into each fresh training dataset. These labels are generated by employing a model previously trained on earlier datasets to automatically annotate the previously acquired event categories in the new data. This approach helps prevent the model from forgetting previously learned information. Another technique involves constraining the learned features and predictions to remain consistent with past models. Additionally, they make a valuable contribution by identifying characteristic feature vectors for infrequently occurring event types. This is accomplished by computing the average and standard deviation of feature vectors from observed training examples with those specific event types and subsequently employing these prototypical feature vectors for further learning. From my perspective, this is a solid work on continual event extraction. It builds upon some proven techniques from the field of continual learning and validate them within the context of event extraction. As the reviewer mentioned, there is room for improvement in the writing, particularly in the elaboration of technical details."
            }
        },
        "id": "zTWRe8edrq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HIBDxkl5n4",
        "replyto": "HIBDxkl5n4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2310/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539192,
        "cdate": 1696707539192,
        "tmdate": 1701465460897,
        "mdate": 1701465460897,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Paper on combination of MT outputs generated by LLMs. Different strategies to generate multiple hypotheses with LLMs, as well as methods to produce the final translation are explored. Contributions of the paper are: ensembling multiple hypotheses significantly improves translation quality;   instruction tuning affects diversity/temperature relationship; hypothesis ensembling reduces hallucinations. Experiments are provided for 8 directions of 4LPs. Paper is well written and well-organized paper. \n\n\nReasons to accept: \nPaper investigates MT with LLMs in a scientifically solid way: they use both closed and open source models; the former to see performance impact on high end models, the latter to allow for reproducibility of results and more in depth investigations. \nExperimental results are informative and performed with up to date evaluation metrics. \n\nReasons to reject: \nLimited language coverage (4 language pairs)\nLimited novelty in the approaches, the authors apply (and properly cite) known methods for their experiments.    \nSome methods are just tried and do not work consistently across models and metrics making this work sound very empirical. We do not know if these results will still hold for another version of the same models. \nThe impact of instruction tuning is measured on one single model and LP. \n\nDetailed comments\nFigure 1: the type of plot is misleading as it suggests a continuum between languages which does not exist. \n269: “significantly increases  the cost”. Cost of ChatGPT should not be part of the equation. If you are using external LLMs you should assume they are for free. \n303 Understanding the prompt requires reading the appendix."
            }
        },
        "id": "IuGM7jJ7qC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HGLvAAKNKx",
        "replyto": "HGLvAAKNKx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3027/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554646,
        "cdate": 1696707554646,
        "tmdate": 1701465484902,
        "mdate": 1701465484902,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In general, reviewers found that the paper sufficiently supported its major claims and arguments, with only minor points requiring extra support or details.  Their enthusiasm ranged from ambivalent to strong, with Reviewer YJZA in particular advocating for the paper and noting that the work presents an innovative path forward in the application of NLP to real-world health monitoring.\n\n**Summary of Reviewer Feedback and Discussion:**\n- **Reviewer Y3PY** liked that the work is broadly impactful and describes a substantial effort to collect data and develop a comprehensive technical solution.  However, they were uncertain about some aspects of the paper, such as whether hyperparameters were tuned, and they felt that the proposed method itself was only novel in its use of speech and text as modalities.  In their rebuttal, the authors provided clarifying information regarding their hyperparameter tuning process, as well as an additional experiment to enhance clarity.  They also more fully elaborated on FedTherapist's novelty compared to the prior approach mentioned by Reviewer Y3PY.  They promised to incorporate these revisions in the updated manuscript.  Reviewer Y3PY thanked the authors for the detailed rebuttal, and noted that while their concerns regarding hyperparameter tuning were addressed, they were still unconvinced of the approach's novelty.\n- **Reviewer YJZA** felt that the work was innovative and addressed important privacy concerns associated with the use of personal data for health monitoring.  They also appreciated the empirical evaluation of the proposed system.  However, they noted that the evaluation was conducted on a relatively small sample size and that the work could benefit from more extensive comparison with other systems for mental health monitoring.  They also wished that the paper had included a detailed analysis of potential privacy risks and how the proposed approach mitigates them, and they were curious whether the software or pretrained models would be released publicly.  In their rebuttal, the authors noted that some privacy analysis is provided in the appendix but that in their revised manuscript they will summarize this in the main body of the paper.  They also acknowledged the limited sample size of their study, explaining that it was due to budget limitations and the sensitive nature of their work, but noted that they did ensure that their participant pool was distributed across a broader demographic group than seen in prior work.  They included some additional results comparing their approach to existing methods, and they noted that while they plan to release the application used for data collection and the source code for data preprocessing and model training, in keeping with IRB protocol they cannot share their data or pretrained models to protect participants' privacy.\n- **Reviewer PGBC** liked that the work tackled an important and well-motivated problem, and that the presented results are strong and support the idea of using text data for mental health monitoring.  However, they felt that many important details were omitted from the main body of the paper or relegated to the appendix.  They also asked some questions pertaining to specific methodological details.  In their rebuttal, the authors noted specific elements from the appendix that they plan to move to the main body of the paper, given additional space following acceptance.  They also ran additional experiments based on questions asked by Reviewer PGBC, and presented these new results to further support the work.  They provided clarifying responses to the reviewer's other questions."
            }
        },
        "id": "IVkUAI7QOE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "HFbtrmefx7",
        "replyto": "HFbtrmefx7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2648/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546647,
        "cdate": 1696707546647,
        "tmdate": 1701465472091,
        "mdate": 1701465472091,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors introduce the Contrastive Reading Model (Cream) designed to enhance text-only LLMs for visually-situated language comprehension. In particular, Cream aligns visual features from both image patches and OCR texts through a contrastive objective. The aligned features are subsequently encoded as soft prompts, enabling LLMs to interpret visually-situated language found in documents, charts, and infographics.\n\nThe unanimous consensus among reviewers is that Cream offers a novel approach to integrating visually situated language into text-only LLMs. Although there were concerns regarding the rigor of the experimental setup and ablations, I believe the authors adequately addressed these in their rebuttal."
            }
        },
        "id": "BD079ir4f4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "H5vtCpKisA",
        "replyto": "H5vtCpKisA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3644/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566892,
        "cdate": 1696707566892,
        "tmdate": 1701465505174,
        "mdate": 1701465505174,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper provides an interesting analysis of the effects of context quality (ie the proportion of relevant passages in the top N documents)  on extractive open-domain QA. They found that FiD's resulting performance is strongly affected by the quality of the context during training and can overfit the training data distributions, resulting in degraded performance when test context quality differs. Based on the observations, they introduce a temperature parameter to the loss, to change the sharpness. \n\nOverall, the research questions and experiments are intriguing, but the main takeaway isn't really surprising to me. To summarize, the core finding (e.g., Section 3.1; *For a given evaluation context quality, models trained with similar context quality showed the highest performance*) is not surprising and can be explained by the tendency of a model performing poorly when there's distribution shift between training and test.  This point is also noted by the reviewer t9yE. On the other hand, I think some findings on attention distributions are somewhat underexplored in prior work and provide unique contributions, and may inspire future work to further investigate some mixed results in this work (e.g., Table 2). \n\nAnother concern I have is that the analysis is limited to a single model architecture (FiD) on an extractive question-answering task, and given the experimental designs, this analysis may not be easily adapted to other architecture or tasks."
            }
        },
        "id": "8JElt3DuCH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "H0SoE2ch5l",
        "replyto": "H0SoE2ch5l",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2287/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538685,
        "cdate": 1696707538685,
        "tmdate": 1701465460138,
        "mdate": 1701465460138,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the problem of adding language pairs to multilingual MT models. It proposes\na method that detectes parameters in the base model which are both unneeded for the original\nlanguages, and suitable for fine-tuning in the incremental languages. Experiments show that the\nmethod achieves bleu improvements on the incremental languages, whilst mostly preserving\nperformance on the original languages.\n\nThe reviewers are in broad agreement about the soundness of the paper, with two 3s and two 4s. I cannot\nsee any substantive issues raised by the reviewers which would justify the 3 score. \nhe issue that I would raise is that\nthis paper's claims and conclusions depend heavily on bleu scores. I would expect to see additional\nresults on a trainable neural metric (eg comet) to validate the results.\n\nThere is some disagreement about the excitement, with reviewer 5Yj3 giving the paper a 2. However their\nfirst 2 reasons to reject seem to be based on misunderstandings, and the third reason (the choice\nof base model) is addressed in the rebuttal. Both eCJC and Tywa rate the excitement at 4 and liked\nthe interesting method, and the thorough experiments and analysis.\n\nOverall, an interesting paper, with strong experiments, except for their over-reliance on bleu scores."
            }
        },
        "id": "QS0T3l24CG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Gzuzpl4Jje",
        "replyto": "Gzuzpl4Jje",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5611/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615833,
        "cdate": 1696707615833,
        "tmdate": 1701465563666,
        "mdate": 1701465563666,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This is a strong paper on improving sentence representations. The authors start with \"standard\" contrastive learning and present two improvements. The paper presents an intuitive rationale and solid empirical results.\n\nIdeally the paper would have been more polished at submission time. I don't see any reason to believe that the authors will have issues fixing the wording suggestions raised by the reviewers (and thank you to the reviewers for the thorough reviews!).\n\nThe paper would benefit from an analysis of why the method works. Solid empirical results are certainly a strength of the paper, but I recommend the authors explore when the method works better than the \"standard\" contrastive learning. This analysis will be limited by the choice of task (only sentence similarity), but I think it would be interesting. Does their method perhaps work better when syntactic structure is different? Is it better at subject replacements?"
            }
        },
        "id": "te5BKpVD5Z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Grj9GJUcuZ",
        "replyto": "Grj9GJUcuZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission113/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479577,
        "cdate": 1696707479577,
        "tmdate": 1701465387764,
        "mdate": 1701465387764,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses the problem of effectively forgetting some information in large language models\n\nThe authors propose a simple but effective method to teach the model unlearn without any retrain steps.\nSeveral experiments are conducted to test the model on both classification and generation tasks under two base models.\nThe motivation and method parts are clear and the experiments are reasonable.\nMachine unlearning is a an interesting and underexplored area. The proposed method is promising when removing mislabeled data is needed.\n\nExperiments should have been performed on larger LLMs such as Llama.\nThe contribution of individual components could be evaluated better.\nSeveral important questions are not considered - whether the model could recall previously forgotten content, and whether the method makes forgotten data identifiable."
            }
        },
        "id": "sZsxCPegqN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GprvtTwOxy",
        "replyto": "GprvtTwOxy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4604/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594044,
        "cdate": 1696707594044,
        "tmdate": 1701465536288,
        "mdate": 1701465536288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose a new learned evaluation metric for sentence simplification, SLE, which focuses on simplicity and outperforms almost all existing metrics in terms of correlation with human judgments."
            }
        },
        "id": "VcKv5E0AgD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Gp8EmdJLUj",
        "replyto": "Gp8EmdJLUj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3070/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555590,
        "cdate": 1696707555590,
        "tmdate": 1701465486157,
        "mdate": 1701465486157,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose a re-ranker model for text-to-SQL generations. The model is build on top of CodeBERT with extra GNN on top.\n\nReviewers have different views for this paper:\nReviewers 12YA and nSMo recognize the positive results presented in the paper, but state that such the concept of a re-ranker is well explored and that the baseline parsers on top of which the re-ranker is applied are not the best text-to-SQL models out there.\nI tend to agree that it is not clear how this work is different than past work that presents a re-ranker -- the concept is not new, and was specifically proposed for semantic parsing in the past. I also believe the following reference is missing -- \"Reranking for Neural Semantic Parsing\". I would expect stronger baseline parsers and comparison to past re-rankers.\n\nOn the other hand, reviewer jWP3 recognized the positive results and the extensive experiments, and sees no issues with the paper."
            }
        },
        "id": "GPAjlzOHiD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GnEGvlOcwr",
        "replyto": "GnEGvlOcwr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4602/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594033,
        "cdate": 1696707594033,
        "tmdate": 1701465536220,
        "mdate": 1701465536220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a fine-grained entity typing system using pseudo-labels to enhance recall for less frequent types. Post-hoc constraints are introduced for these labels to ensure prediction of at least one actual type from the cluster and avoid prediction of mutually-exclusive types. These exclusive types, termed conceptual neighbors, are learned using a contrastive NLI model. Enhancements in the paper's presentation are warranted. The experimental results should explicitly demonstrate the system's superior performance, particularly emphasizing its effectiveness with respect to long-tail types."
            }
        },
        "id": "TGJVKNGgiR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GgriuyaTZU",
        "replyto": "GgriuyaTZU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1077/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504519,
        "cdate": 1696707504519,
        "tmdate": 1701465419784,
        "mdate": 1701465419784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers agree that this paper is sound and moderately exciting, adding to existing work on understanding of political bias in LLMs. Reviewers raised some concerns about clarity and particular methodological choices; authors provided an extensive set of rebuttals including updated empirical results with newer multilingual models and satisfied many reviewer questions and concerns. \n\nReviewers noted several issues that could be clarified in any final version of the paper, including: a) that binary left/right political stance is a simplification and potential limitation with long-form text, b) that the corpus includes non-news articles, c) some further explanation on the heuristics used for data cleanliness, and so on. We ask the authors to integrate these changes and clarifications following reviewer feedback."
            }
        },
        "id": "toCxammXgz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GeFFYOCkvS",
        "replyto": "GeFFYOCkvS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1938/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530387,
        "cdate": 1696707530387,
        "tmdate": 1701465447739,
        "mdate": 1701465447739,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a new dataset for English NER that focuses on English varieties different from US and UK English. The authors then test various existing NER tools and models to evaluate their robustness to non-standard varieties.\n\nThe reviewers found that this paper addresses an important gap in current NLP that focuses on the varieties of English with the most (native) speakers. The error analysis was also found to be instructive. In contrast, the reviewers mentioned that some confounding factors (temporal drift etc.) were insufficiently taken into account, and that the paper lacked details about the annotation process, the statistics of the resulting dataset, and potential ethical issues in connection with the annotation. The authors addressed all these points in their rebuttals and promised to add them to the paper. As a result, all three reviewers were able to raise their scores."
            }
        },
        "id": "BJdxoibVhK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GbApUL7sDL",
        "replyto": "GbApUL7sDL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4668/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595565,
        "cdate": 1696707595565,
        "tmdate": 1701465538166,
        "mdate": 1701465538166,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes an interesting retrieval-augmented approach utilizing LLMs for the task of legal judgement prediction.\nThe proposed method combines the strength of both LLMs and domain models, and provides a promising direction for the collaboration of LLMs and domain-model.\n\nThe specific doubts of reviewer ZR8e have been addressed in the author response."
            }
        },
        "id": "EC8lURYTtG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GWOCiRkjCF",
        "replyto": "GWOCiRkjCF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2910/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552031,
        "cdate": 1696707552031,
        "tmdate": 1701465480946,
        "mdate": 1701465480946,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents experiments in generating more or less interesting 5-sentences stories by manipulating the generator's beam search size dynamically and introducing a specific generation step for 'twist sentence' candidates and some lexicon-based reranking, which outperform GPT models in the automatic and human evaluation. Reviewers liked the task idea and some of the proposals in the paper, but criticized the very tailord nature of the task (exactly 5-sentence stories, in which for example a twist in sentence 2 seems most important), the lack of experiments using more effective prompts for the GPT comparisons, and the relatively weak results in Table 1 - as the rebuttal points out, the main positive result here hinges only on the arousal score, which is highly dependent on individual lexical items based on Mohammad 2018, and may have some connection with the lexicon-based reranking. Having read all of the rebuttals I am not completely convinced by the authors' argument about limitation to 5-sentence stories based on LLMs' difficulties in creating longer coherent stories - especially when heuristics are involved, a tailored approach will likely fare better when the outputs are more templatic in nature, and an attempt at a few-shot approach with examples of what the authors want from the model is also missing here. The latter point is especially important given the claim that LMs don't understand what humans find interesting - they cannot know what the authors are targeting exactly, unless we give them examples. At the same time, I agree that the experiments form an interesting first step in exploring this task and the nature of 'twists', so although reviewers were consistently on the fence with excitement scores of 3, if there is space, I would rather see this paper published than rejected. If it is not accepted to the main conference, I think it would make a good candidate for a Findings paper."
            }
        },
        "id": "fPD8RMDrRl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GTqt0X2Swn",
        "replyto": "GTqt0X2Swn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5106/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707607070,
        "cdate": 1696707607070,
        "tmdate": 1701465550118,
        "mdate": 1701465550118,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers saw both strengths and weaknesses in the submitted version of the paper. The authors have provided a rebuttal that appeared to alleviate some concerns. After some discussion, the overall verdict was leaning toward an acceptance of this submission. However, there are still some issues to be addressed as raised by Reviewer mx4P. So, a suggestion was made to accept the paper as Findings. The reviewers' concerns should be considered in the final version."
            }
        },
        "id": "di4DqAQMSZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GSnAO2qUHy",
        "replyto": "GSnAO2qUHy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3808/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707575896,
        "cdate": 1696707575896,
        "tmdate": 1701465510272,
        "mdate": 1701465510272,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "A simple and effective approach to do data generation. Written well and easy to understand. It's technically sound and show good performance. However, for originality, it's not a very novel idea that would surprise people.\n\nPros:\n\n1. A simple and effective method to do data generation.\n\n2. Great performance on some benchmark datasets.\n\nCons:\n\n1. It's not a particularly interesting method. None reviewer showed the excitement high enough to support the paper. It reads like reviewers all feel it's reasonable to get this performance and there is no analysis showing why pervious(or simple heuristic) methods would fail and why the problem is extremely difficult."
            }
        },
        "id": "PC66XTOq7B",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GSNoZKqHgO",
        "replyto": "GSNoZKqHgO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3556/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565173,
        "cdate": 1696707565173,
        "tmdate": 1701465501420,
        "mdate": 1701465501420,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper explores the task of identifying Early Maladaptive Schemas (EMS) in mental health texts obtained from community question-answering forums.  While the work makes valuable contributions, particularly in dataset curation and initial analysis, it also suffers from a series of shortcomings that affect its overall impact. All reviewers acknowledge the significance of applying NLP in the mental health domain. he dataset generated is of noteworthy value, especially since it focuses on Early Maladaptive Schemas, a topic not previously covered in available resources. Multiple reviewers noted that the dataset presentation could be more precise, with more context and examples for easier comprehension. Given the sensitive nature of mental health data, reviewers have expressed concerns over the lack of discussion regarding dataset de-identification. The reviewers found the paper’s contributions to be incremental. They recommend further experiments for a more thorough evaluation. Given the strengths and weaknesses pointed out by the reviewers, the paper is recommended accepted into the Findings. The authors are encouraged to consider the detailed feedback from the reviewers to enhance the quality and impact of their work."
            }
        },
        "id": "m7sObY2crm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GQ1rtVVIy2",
        "replyto": "GQ1rtVVIy2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2380/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540947,
        "cdate": 1696707540947,
        "tmdate": 1701465463360,
        "mdate": 1701465463360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposed the VL models with augmentation ability, trying to use <image, text> paired data as an additional database to augment the model's knowledge. The experiments are comprehensive with multiple experimental setups. The method is new to the reviewer while there is a discussion regarding the novelty of the method. Also, the reviewer also pointed out that the effectiveness of this method is not fully demonstrated with the empirical results."
            }
        },
        "id": "fEkbATKjEN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GOBxWdRpfz",
        "replyto": "GOBxWdRpfz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3574/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565480,
        "cdate": 1696707565480,
        "tmdate": 1701465502098,
        "mdate": 1701465502098,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Pros:\n1. Paper is well written and easy to follow. Work is sound and the experimental evaluation rigorous. \n2. Problem is interesting and critical in the area of text-based generative AI -- a new score for evaluating factual precision  of outputs from LLMs automatically.\n3. Simple metric. Insights from evaluation of FACTSCORE for 12 large language models are interesting.\n4. Contribution to open source is a strong positive.\n\nCons:\n1. Work is very empirical, sometimes leading to lack of concrete definitions, e.g., \"facts\". But this is rightly pointed out in Limitations section. And EMNLP is an empirical venue.\n2. Generalization is a problem since the work deals with 1 domain and 1 knowledge base. \n3. Incompleness of the knowledge base could hurt the evaluation. But the work stands on its own assuming that incompleteness of knowledge base is a separate problem to solve.\n\nSuggestions:\nFor the last 2 points in cons section: Experiments done as part of rebuttal are not very rigorous and done on small scale. While authors are encouraged to include them as part of appendix in the revised submission, this weakness should be explicitly called out in the Limitations section."
            }
        },
        "id": "VVUADr9mDH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GLA4ablO3M",
        "replyto": "GLA4ablO3M",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission144/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480280,
        "cdate": 1696707480280,
        "tmdate": 1701465388701,
        "mdate": 1701465388701,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes to restrain the attention of a tranformer LM based on syntax. They show that this improves performance across multiple SLU datasets. The approach is easily used in many LMs (as long as they use attention), and the paper is well written. The reviewers mentioned multiple competing approaches that aim to incorporate syntax into LMs; which are not compared to (both descriptive and performance). Furthermore, ablation studies and qualitative analysis are not included; limiting the conclusions we can draw from the current paper (it would probably fit better as a long paper including the missing parts).\n\nPS: I think SLU is a very misleading name; as it is not speech, and it is not fully understanding either. I  would suggest to refer to this as intent detection or intent classification"
            }
        },
        "id": "tIPUpMSalT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GFgPmhLVhC",
        "replyto": "GFgPmhLVhC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission998/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707501806,
        "cdate": 1696707501806,
        "tmdate": 1701465417327,
        "mdate": 1701465417327,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a novel evaluation method for large language models (LLMs), focusing on their reasoning capabilities. It introduces a debate-like interaction where the LLM must defend the truth against a user presenting a wrong solution. The study reveals that LLMs struggle to defend correct solutions, indicating reasoning deficiencies not captured by conventional benchmarks. The reviewers appreciate the innovative approach and the paper's implications for AI safety. However, they also raise concerns about the evaluation method's robustness and the lack of clarity in some areas, such as the treatment of uncertain answers in commonsense reasoning tasks. The authors' rebuttals address these concerns, promising further clarification and examples in the revised version. Overall, the paper is seen as a valuable contribution to the field, but it could benefit from further refinement."
            }
        },
        "id": "APYpz5WGlt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GEZW6VqQNg",
        "replyto": "GEZW6VqQNg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission875/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498141,
        "cdate": 1696707498141,
        "tmdate": 1701465413284,
        "mdate": 1701465413284,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper has details about comprehensive experiments, including investigations into model size and the order/number of safe demonstrations. Reviewers note that this first large-scale evaluation of in-context learning for dialogue safety, adding significant value to the field."
            }
        },
        "id": "w5UYNolgKr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "GDPMVALXqv",
        "replyto": "GDPMVALXqv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3898/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578710,
        "cdate": 1696707578710,
        "tmdate": 1701465513313,
        "mdate": 1701465513313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a dataset designed to showcase the use of a calculator tool in mathematical reasoning chains. By leveraging four existing datasets, namely GSM8K, Ape-210K, AQuA-RAT, and MathQA, the authors have created over 300K examples. These datasets were parsed into a unified HTML-style format, termed Calc-X, where calculations are represented using specific tags. When T5 models are fine-tuned using this format, there's a noticeable improvement in the accuracy of results, except for the AQuA-RAT dataset. The authors have made the dataset and the fine-tuned model available on Huggingface, accompanied by comprehensive documentation. The paper emphasizes the potential of a unified tool-use format in enhancing the accuracy of language models in arithmetic problems.\n\nThe paper's contribution of a dataset with a unified format for arithmetic tool use is invaluable. The results indicate a clear enhancement in the mathematical reasoning abilities of language models. The authors have made both the dataset and the fine-tuned model accessible to the public, which can further stimulate advancements in this domain. The documentation detailing the data creation process and the released resources is commendably clear. The datasets and format proposed in this work could be instrumental for future research on tool-use in large language models. The study's outcomes serve as robust evidence supporting the advantages of tool use for arithmetic reasoning in large language models.\n\nThe paper's proposed data format conversion strategy might have limited scope, especially for datasets that involve complex and less structured arithmetic operations. This limitation is evident from the challenges encountered with the AQuA-RAT dataset. While the paper emphasizes the benefits of using calculators in large language models, similar findings have been previously reported, raising questions about the novelty of this study.  A reviewer suggests that the paper might be better suited for a resources and evaluation conference rather than an NLP conference."
            }
        },
        "id": "APzF9feW50",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "G7IbRKrAOE",
        "replyto": "G7IbRKrAOE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4189/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585069,
        "cdate": 1696707585069,
        "tmdate": 1701465523573,
        "mdate": 1701465523573,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes the Hearing Enhanced Audio Response (HEAR) framework that helps Video-grounded Dialogue (VGD) systems to better perceive and understand audio. The framework consists of two main components: Sensible Audio Listening (SAL), and Reconstructive Listening Enhancement (RLE). SAL predicts whether the question about the video clip is related to the audio track or not. For questions related to the audio track, RLE forces the model to reconstruct masked frames of the audio given the context frames.\n\nContributions:\n- The HEAR framework is model-agnostic and can be applied to most VGD systems. \n- The SAL component explicitly directs the model to focus on audio features, this achieves better performance compared to feeding both audio and video features to the transformer blocks. Reducing VGD \"deafness\".\n- Experiments show that it improves the performance of VGD systems on audio-related questions.\n\nThe reviewers generally saw the contributions as being incremental with respect to existing work. That said, the authors provided additional comparisons as part of the rebuttal process which demonstrate the effectiveness of multi-modal reconstruction vs single-modal reconstruction, and importantly, the effectiveness of their approach vs a prior multi-modal reconstruction approach."
            }
        },
        "id": "4LLL6ARWUq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "G6gj7Dydc5",
        "replyto": "G6gj7Dydc5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission785/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495888,
        "cdate": 1696707495888,
        "tmdate": 1701465410223,
        "mdate": 1701465410223,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In order to assess the accuracy, factualness, and coherence of summaries, this paper defines three energy functions embedded into a supervised summarization model. To enhance the corresponding characteristics of the system-generated summaries, the energy functions are included in both the training and inference processes. The generated sequences are reranked according to these scores during the inference stage. \n\nPros:\n1. The proposed integration of consistency-based energy functions into summarization models is definitely a novel approach; \n2. The paper is well-written;\n3. The evaluation results are clearly described.\n\nCons:\nAccuracy performance which is comparable to SOTA baseline systems with multiple metrics, but not always outperforming. \n\nThe authors provided very detailed answers to all reviewers. Some issues that were raised by reviewers are left for future work."
            }
        },
        "id": "fFD1isd54W",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "G6E3uzABf1",
        "replyto": "G6E3uzABf1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission348/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485392,
        "cdate": 1696707485392,
        "tmdate": 1701465395990,
        "mdate": 1701465395990,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper investigates the application of Chain-of-Thought (CoT) to Large Language Models (LLMs) in Natural Language Understanding (NLU). The proposed CoF-CoT approach enables LLMs to acquire and leverage essential concepts for solving tasks at different granularities. The authors also integrate structured knowledge via Abstract Meaning Representation (AMR) to capture the nuances and diverse structures of utterances. The paper evaluates the method on two multi-domain NLU datasets and shows improvement in LLMs' capability in multi-grained NLU tasks.\n\nThe reviewers generally find the topic and main contributions of the paper interesting. The proposed method is considered reasonable and incremental. However, there are concerns about the experimental analysis, lack of strong baselines, and comparisons with other methods. Reviewers also suggest exploring the advantages of different LLMs and providing more comprehensive comparisons with related works. Additionally, there are questions about implementing the proposed method in zero-shot and few-shot scenarios and its applicability when intermediate ground truths are not available.\n\nOverall, the reviewers acknowledge the merits of the paper, such as reporting state-of-the-art results and presenting a novel idea. However, there are also key weaknesses, and the paper would benefit from another round of revision."
            }
        },
        "id": "jVVebYsxfl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "G3IjhUERrD",
        "replyto": "G3IjhUERrD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2364/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540500,
        "cdate": 1696707540500,
        "tmdate": 1701465462814,
        "mdate": 1701465462814,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a study of animacy processing in LLMs, analogizing it to human processing as evidenced by psycholinguistic studies. The paper analyzes animacy processing in openly available datasets, such as BLIMP. However, the main contribution is adapting several Dutch psycholinguistic studies to evaluation by English LMs.\n\nThe reviewers unanimously gave very positive feedback, which I summarise in the following lines:\n\n** Reasons To Accept: **\n\n- The experiments are described clearly and contain relevant and interesting discussions.\n- The paper further navigates its interdisciplinary setting very well by describing the psycholinguistic investigations and describing in enough detail its adaptation to LM evaluation.\n\n** Reasons To Reject: **\n\n- The background section is missing previous work on animacy detection.\n- Section 5 as mentioned by the third reviewer, should be refined to have a very competitive paper."
            }
        },
        "id": "4ftCLrZmIG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "G13P9iWzKc",
        "replyto": "G13P9iWzKc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4124/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583591,
        "cdate": 1696707583591,
        "tmdate": 1701465521062,
        "mdate": 1701465521062,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a study on unsupervised relation extraction that involves better data augmentation. The authors also highlighted the inadequacy of NCE and argued that some alternative loss function may be more suitable. The reviewers largely acknowledge that the work is interesting and presents good results. Some major reservations include that the work seems to leverage external tools which may compromise the unsupervised setup, and that the two major techniques involved (loss, data augmentation) do not seem to naturally align with (or motivate) one another well."
            }
        },
        "id": "Iz65keCEUj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "G12y1Pz3vJ",
        "replyto": "G12y1Pz3vJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3744/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707571189,
        "cdate": 1696707571189,
        "tmdate": 1701465508426,
        "mdate": 1701465508426,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a survey on the current state of NLP. The authors characterize the current state of knowledge and the research that has already been done, and point out where further efforts are needed. The paper covers many research areas related to large language models, and as a result, there is a lack of depth. However, it addresses a central concern of the NLP community of how to position NLP in view of the challenging breakthrough of LLMs."
            }
        },
        "id": "KM4aTdN5Pm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "G0ZGGpSj7i",
        "replyto": "G0ZGGpSj7i",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission452/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488137,
        "cdate": 1696707488137,
        "tmdate": 1701465399458,
        "mdate": 1701465399458,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies the task of paraphrase identification and generation. Unlike traditional binary settings, the authors consider 26 fine-grained paraphrase types (most of which proposed by Kovatchev, Marti, Salamo 2018) -- this is a promising direction of research on paraphrase, and addresses the issue that the definition of paraphrase is often not precise enough. Authors conducted some interesting analyses on four paraphrase datasets, with Llama and several other LLMs. \n\nAuthor's responses also raised some confidence -- two of the reviewers are happy with the author responses. We hope the authors will make improvements based on the review comments for their camera-ready, if the paper gets accepted. We also want to ask the authors to discuss more thoroughly the related work and limitations."
            }
        },
        "id": "NGdkY9qpI6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Fqv0rgvkol",
        "replyto": "Fqv0rgvkol",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3653/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567064,
        "cdate": 1696707567064,
        "tmdate": 1701465505539,
        "mdate": 1701465505539,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Thanks reviewers so much your efforts in providing comprehensive reviews and comments to improve the paper.\n\nThanks authors for providing actionable rebuttals and facilitating discussions.\n\nSummary: This paper presents UPTON, a novel method to protect authors' privacy when their writings are published. UPTON uses synonym substitution to modify text, assigns different labels, and thwarts authorship attribution tools. It offers a framework for safeguarding authors, especially activists and whistle-blowers, without needing access to attribution models. Experiments show UPTON reduces attribution model accuracy to about 35%, making it an effective privacy solution.\n\nAverage Soundness: (2+3+5)/3 = 3.3\nAverage Excitement: (2+3+5)/3 = 3.3\nReproducibility: (3+4+5)/3 = 4\n\nSummary of Pros:\n+ The authors conducted comprehensive experiments in three different scenarios to validate their approach, which yielded superior results compared to alternative methods like sample-wide label assigning and non-target label assigning. \n+ The paper is well-written and tackles an important problem, making it both interesting and relevant. \n+ The discussion section is robust, addressing potential questions thoroughly. \n\nSummary of Cons:\n+  It questions how word-level synonym replacement can affect a distinct model (F_A) when the attack model is independent and inaccessible. It suggests that F_A should be a static model without parameter tuning, indicating uncertainty about the model's behavior.\n+ The main contribution as the label-assigning process for poisoned data could be limited.\n+ Other metrics could be used\n+ Further clarify the motivation to replace original content with machine-generated text, raising the question of whether users desire anonymity or recognition.\n+ Further clarify the assumptions about the attacker model, such as it being a transformers-based model\n\nThe authors have provided a comprehensive rebuttal and proposed the changes to tackle the cons. The changes seem be feasible and can be timely ready for the final version."
            }
        },
        "id": "7tPSzqPvSb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Fm0Brp3cTS",
        "replyto": "Fm0Brp3cTS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission403/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486639,
        "cdate": 1696707486639,
        "tmdate": 1701465397702,
        "mdate": 1701465397702,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Quality, clarity, originality and significance\n\nTLDR: The paper proposes a suite of measures -- sufficiency, completeness, stability, and plausibility, to evaluate instance attribution explanations, which would benefit future work on instance attribution methods. An analysis using those is performed on four different classification tasks and three instance attribution methods. The reviewers appreciate the importance of the work and the thoughtful design of measures and experiments. The downside of the work is regarding the plausibility measure, which the reviewers have found both not needed and, most importantly, not correctly designed. Besides, the authors have addressed most reviewers' comments and provided additional results to support their claims further. \n\nPros:\n1. Clarirty/Quality - the design of each metric in IAEval is well motivated and documented (sPvk, wyYh)\n2. Significance - instance attribution is an important problem, the work can facilitate future work by providing means to measure progress in the space (VKZW)\n3. Quality - the experiment and analysis part is thoughtfully designed, the evaluations are done on four different datasets, improving the generality of observations (sPvk), and the results are consistent across metrics (wyYh)\n\nCons:\n1. Quality  - VKZW raised a concern about plausibility evaluation that was also acknowledged by the other reviewers. The authors should reconsider including plausibility in their suite of measures as 1) the reviewers are debating whether it is always an important property of an explanation and 2) **most importantly, the current way of measuring plausibility does not reflect that an instance is plausible to humans**. In case of missing human annotations, this property should be abandoned. See also the discussion on faithfulness vs. plausiblity in Jacovi et al. where plausibility is defined as \"how convincing the interpretation is to humans\" where automated measures do not directly measure that.\n2. Clarirty -  the authors have addressed most of the concerns of the reviewers and are kindly requested to include all clarifications and additional results in the final version and to tone down their claims that the reviewers found to be overstated.\n\nJakovi et al. Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?"
            }
        },
        "id": "4s65RIe9ks",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Fj07R03qkz",
        "replyto": "Fj07R03qkz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission710/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494289,
        "cdate": 1696707494289,
        "tmdate": 1701465408065,
        "mdate": 1701465408065,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Authors proposed an approach focusing on learning a generator that can directly generate new sentences by mixing labels and syntactic templates from different samples. The soundness and excitement are both 2 strong (4) and 1 good (3). Authors have responded and answered the questions raised by reviewers. This paper trains to generate samples for dataset augmentation and this method is shown to be effective. It can be helpful to the community and could be applied to other tasks."
            }
        },
        "id": "d1Sl0nmqOX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FghDWBBsIm",
        "replyto": "FghDWBBsIm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission736/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494743,
        "cdate": 1696707494743,
        "tmdate": 1701465408813,
        "mdate": 1701465408813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces an unsupervised technique for generating training data for Referring Expression Comprehension (ReC). The proposed method initiates by constructing a core scene graph, subsequently filters out ambiguous queries, and lastly rewrites the query using GPT-3.5 to enhance the quality of the pseudo labels.\n\nWhen tested on standard benchmarks, this data generation method has shown efficacy across various academic ReC datasets. However, questions about its generalizability have emerged. Notably, the most significant improvement is evident on RefCOCO, which contains spatial terms, making scene graphs apt for capturing principal relations between objects. This might not necessarily translate to datasets outside this specific domain. Additionally, there are reservations about the template-based filtering step's adaptability to diverse distributions and the robustness of its underlying heuristics."
            }
        },
        "id": "raDSMNloMS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FgEM735i5M",
        "replyto": "FgEM735i5M",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission240/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482731,
        "cdate": 1696707482731,
        "tmdate": 1701465392532,
        "mdate": 1701465392532,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a self-training strategy to iteratively improve the augmented training data. Compared with baseline models, the proposed method can not only improve the performance, but also reduce the need for labeled data. The reviewers generally hold a positive view of this paper. The author has also addressed some of the concerns raised by the reviewers. However, further careful revision and supplementation may still be required. The rebuttal has addressed reviewers' concern on the motivation and novelty concerns, and the authors agree to revise this paper accordingly."
            }
        },
        "id": "bZ1wkXZC4d",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Faxkz2V56o",
        "replyto": "Faxkz2V56o",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1254/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508580,
        "cdate": 1696707508580,
        "tmdate": 1701465425234,
        "mdate": 1701465425234,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors formalize the task of direct translation post-editing with LLMs and explore the use of GPT-4 to automatically post-edit NMT outputs across several language pairs. The authors formalize the post-editing task in a generative setting with LLM, without intermediate error detection steps. They demonstrate that they can improve translations generated with Microsoft Translator and the WMT22 submissions via post-editing across several language pairs. This is the first work investigating a LLM as a post-editing system for MT. The author did a fantastic job with evaluation including a human evaluation.\nThe main criticism of this work is the reliance on an external model via an API, where it's unclear how the underlying model changes, making it hard to reproduce the results. It would have been nice to also include experiments with open source LLMs and even try to improve their post-editing quality via e.g. fine-tuning on post-edited data."
            }
        },
        "id": "Fk7iBOrawa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FXObwPWgUc",
        "replyto": "FXObwPWgUc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4077/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582623,
        "cdate": 1696707582623,
        "tmdate": 1701465519540,
        "mdate": 1701465519540,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Pros:\n* The paper introduces a new task to study whether language models are capable of generating text with the similar level of complexity observed in the respective prompts. \n* All reviewers agree that studying the text complexity in prompted models is an interesting and novel idea, and will be of interest to the community.\n* The authors present  a through analysis for the proposed task with a comprehensive set of complexity metrics.\n\nCons: \n* The authors needs to improve their motivation and the limitations of the study elaborating on 1) the assumption that \"humans will maintain complexity between a text prompt and its continuation\" (given that the complexity metrics tell a different story), 2) the fact that a long text could have varying level of complexities (in contrast this work focuses on prompt continuation only), and 3) whether or not a more fine grained text analysis would be more informative. \n* All reviewers pointed out that the analysis in the paper will improve with larger set of models. Currently, only GPT-2 results are analysed. \n* The paper could also improve with better presentation and visualisation of their results. The authors have responded to these requests and have agreed on improving these aspects."
            }
        },
        "id": "ftGpXA5B7f",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FTiXh63BVO",
        "replyto": "FTiXh63BVO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1470/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515234,
        "cdate": 1696707515234,
        "tmdate": 1701465431867,
        "mdate": 1701465431867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The author propose a hypeparameter optimization + gradient descent modification method which is inspired by minimizing a generalization bound. All reviewers agree that the synthesis of PAC-bayes generalization bounds with PLMs is interesting, and while there are some shared concerns about baselines and evaluations (to other regularized fine tuning methods, prompting etc), this is not a major issue."
            }
        },
        "id": "Rcv4bSPRwl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FS1a4CDZsP",
        "replyto": "FS1a4CDZsP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4839/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599207,
        "cdate": 1696707599207,
        "tmdate": 1701465542947,
        "mdate": 1701465542947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces the Cue-CoT method, which uses linguistic cues to improve LLM's responses by adding an intermediate reasoning step. The method searches for cues in dialogue to create more personalized and engaging responses. The authors created a new benchmark using 6 datasets in both Chinese and English, focusing on three linguistic cues: personality, emotion, and psychology. They tested their method with 5 LLMs in both zero-shot and one-shot scenarios. Results show that Cue-CoT performs better than standard prompting methods in terms of helpfulness and acceptability.\n\nReviewers gave soundness scores of (3, 3, 3). All reviewers found the paper well-written, and post-rebuttal, agreed that the evaluations were thorough, considering both automatic metrics and human assessment.\n\nExcitement scores were (4, 3, 3). The overall excitement is moderate due to some overstatements in the results, unclear writing, and an incremental rather than transformative methodology."
            }
        },
        "id": "is7zlikbnj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FRRlmKxuf2",
        "replyto": "FRRlmKxuf2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2823/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550153,
        "cdate": 1696707550153,
        "tmdate": 1701465477664,
        "mdate": 1701465477664,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a new pre-training strategy using contrastive learning for Aspect Sentiment Triplet Extraction. The experimental setup is sound and thorough. The proposed method is novel and provides good performance gains on the task. The authors should clarify a bit better the efficacy of individual components such as TCE and improve the related work section as well as the motivation behind their approach."
            }
        },
        "id": "uSSiWzLuhU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FMwflM9yVJ",
        "replyto": "FMwflM9yVJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5841/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707619456,
        "cdate": 1696707619456,
        "tmdate": 1701465568161,
        "mdate": 1701465568161,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors consider a continual learning setting without labels: models are trained in an unsupervised fashion given a sequence of unlabelled task inputs.\nEvaluation happens via the introduction of labels: i.e., at any time in this training sequence, the model may be fine-tuned for any of the previous, current, or future tasks.\nThe proposed hypernetwork method outperforms alternatives on two datasets.\n\nReviewers largely agreed that this work is sound: experiments are thorough, baselines reasonable, etc. There was some discussion about continual fine-tuning vs. continual pretraining which was resolved. Existing limitations include that only 2 datasets and RoBERTa are considered but in response the authors address this by adding GPT2 and new experiments. Only classification is considered.\n\nI personally wonder how much this proposed setting truly differs in practice from the general pre-training setting other than that the unlabelled text is provided on-line in task-sized chunks --- this setting also seems to be an extension of the somewhat common \"unsupervised domain adaptation\" setting where a pretrained model undergoes additional unsupervised training on a new unlabelled corpus (but here, there are a sequence of new domains, instead of just one).\n\nOverall, it seems like the authors have addressed several concrete shortcomings (by adding new experiments on different models/datasets). The proposed method works better than a reasonable set of baselines. But, the importance of this setting beyond existing pretraning/unsupervised domain adaptation setups (both of which are closely related) is a bit less clear, which may be causing some reviewers to be less excited about this particular setup."
            }
        },
        "id": "YvG4jQj5Ls",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FMWVtVct0V",
        "replyto": "FMWVtVct0V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission237/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482635,
        "cdate": 1696707482635,
        "tmdate": 1701465392423,
        "mdate": 1701465392423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper focuses on knowledge-augmented VQA, which requires common sense knowledge and reasoning among VQA problems. The paper aims to improve the performance of VQAs by providing a variety of relevant knowledge in the form of language guidance; performance gains have been observed when language guidance is introduced for CLIP and BLIP-2-based baselines.\n\nPros:\nIntroducing a variety of relevant knowledge in the form of language guidance is a convincing approach.\nPerformance improvements have been observed.\n\nCons: \nAs two reviewers questioned, reviewer PMF9's comments include a misunderstanding, but there are some questions about whether the baseline should only be CLIP and BLIP-2.\nAs reviewer WdmU points out, there are several baselines and experimental settings, and the range of effects this study demonstrated is limited. \nAlthough Rebuttal shows experimental results for different data sets, it is not a justification for the baseline setting.\nCLIP is not good at focusing on the finer points of an image and can only understand the broad features represented in the image. A more detailed discussion is needed regarding what type of instruction is effective. Also, authors may discover cases where the proposal would work better by using a different VQA model (e.g., T5-based) that can capture different image features."
            }
        },
        "id": "6b6kWxLZ4g",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FLSQjYmzIp",
        "replyto": "FLSQjYmzIp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4817/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599018,
        "cdate": 1696707599018,
        "tmdate": 1701465542559,
        "mdate": 1701465542559,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper is written well and clear to read. The introduced problem and method are novel, and the experiments are thorough. The related work could be updated with some references suggested by the reviewers. The paper would also be stronger if it considered more than one environment for evaluation."
            }
        },
        "id": "tFTBuVh2CN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FKNtgr0qQy",
        "replyto": "FKNtgr0qQy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4558/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592976,
        "cdate": 1696707592976,
        "tmdate": 1701465534655,
        "mdate": 1701465534655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method for improving unsupervised speech segmentation by leveraging the large pre-trained self-supervised model XLS-R and off-the-shelf unsupervised word segmentation models to produce labels for self-training. The paper studies three existing approaches, namely DP-PARSE, VG-HUBERT and DPDP, and finds that pseudo labels obtained with DP-PARSE can be successfully used to fine-tune the XLS-R model for unsupervised word segmentation, resulting in an improvement of 130%. The authors report discrepancies between different off-the-shelf word segmentation methods and leave the analysis for future work.\n\nThe reviewers liked that this simple idea improves results by a large margin and that the paper evaluates the method in multilingual settings. The reviewers asked for several ablations experiments and clarification of some results. The authors ran the requested ablation experiments and tried to clarify the results, noting that some observations will probably need a more thorough analysis in future papers. After reading the paper and reviews, I base my decision on the review by the reviewer j6sr, who works on unsupervised word segmentation and is excited about this paper."
            }
        },
        "id": "yM6hSnDy1i",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FGBWDf7Z19",
        "replyto": "FGBWDf7Z19",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission111/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479507,
        "cdate": 1696707479507,
        "tmdate": 1701465387755,
        "mdate": 1701465387755,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presented a work with a few nice contributions as mentioned by the reviewers and summarized by the authors during discussions, which I would prefer not to repeat. I do have a few additional concerns to mention. \n\n1. The authors added a large amount of new results, unfortunately, I did not see all those corresponding results for other related methods, such as turbo results of AutoCoT. Such results are very important given the similarity of these works. In addition, the following trend usually holds: the stronger the LLMs' capability, the less the necessity of labeled data.\n\n2. As LLMs are dominating the field, it is mostly acceptable to challenge a traditional method without being compared with an LLMs-based method. But here I would like to mention the opposite way. Given that hundreds of labeled data are needed, I would be curious what result a supervised method will achieve by using these few hundreds of labeled data for training. \n\nIn addition, I have some optional advice. The authors might want to change the name Automate-CoT to something else because it is misleading with Auto-CoT. Given that your method needs hundreds of training data, maybe weakly-supervised-CoT is more appropriate."
            }
        },
        "id": "jptAOy7sDR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FGBEoz9WzI",
        "replyto": "FGBEoz9WzI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1913/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529888,
        "cdate": 1696707529888,
        "tmdate": 1701465446871,
        "mdate": 1701465446871,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a new task, defeasible moral reasoning, to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. \n\nThe authors introduced the δ-RULES-OF-THUMB (δ-ROT) dataset of 1.2M entries of combined context and rationales for 115K defeasible moral actions. δ-ROT is created through an iterative approach, which includes self-distillation, targeted filtering using a critic model trained with human judgment and (3) self-imitation learning.\n\n\nPros:\n\n- The δ-ROT dataset is an important contribution with over 1.2M entries.  It also contains rationales that *explain the moral judgment of an action* that is useful for the community. \n- The method to generate the data is not novel but effective and can be used for other tasks. \n- The qualitative analysis of contextualised categories per moral variance is interesting.\n\nCons: \n- Strong baselines such as finetuned open-sourced models are missing. \n\nIn the result section, there are too many **bold** words. Try to use italics instead. \n\nFinally, The AreaChair appreciate the authors' response to clarify the doubts of the reviewers."
            }
        },
        "id": "Nmk6vhpMbj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FAimEpR9Fh",
        "replyto": "FAimEpR9Fh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4322/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587435,
        "cdate": 1696707587435,
        "tmdate": 1701465527890,
        "mdate": 1701465527890,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes to combine SSM and TNN to reach a better tradeoff between task performance and speed performance. Reviewers found this paper interesting, novel, and well-organized, and may benefit the research of other types of language models. Several concerns were raised initially regarding the limited results, low reproducibility, and insufficient analysis. The authors addressed them during the rebuttal periods, which are acknowledged by all reviewers. Therefore, the AC deemed this paper should be accepted for the main conference."
            }
        },
        "id": "q5Y6rfC2Hi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "FAiFBfFTGZ",
        "replyto": "FAiFBfFTGZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3074/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555672,
        "cdate": 1696707555672,
        "tmdate": 1701465486326,
        "mdate": 1701465486326,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "- The paper uses many abbreviations and it is hard to read the manuscript.\n- An excessive number of experiment is done but the results are marginally better\n- The model is complex and that makes reproducibility a challenging task\n- The method is tested only on one dataset"
            }
        },
        "id": "A6yrlEaJY5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "F4qNZtkk3V",
        "replyto": "F4qNZtkk3V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2723/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548179,
        "cdate": 1696707548179,
        "tmdate": 1701465474424,
        "mdate": 1701465474424,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper provides a well-presented study on how factual associations flow through the internal structure of a transformer. It addresses an important topic and includes relevant findings. It is a targeted study with a (narrow) focus on subject-query, that can be seen as a nice first step into diving further into this topic."
            }
        },
        "id": "0fWTVSF8fh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "F1G7y94K02",
        "replyto": "F1G7y94K02",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission60/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478106,
        "cdate": 1696707478106,
        "tmdate": 1701465385682,
        "mdate": 1701465385682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper examines the extent to which knowledge of social stereotypes is encoded in LLMs, by prompting for evaluations of various social groups along salient dimensions (e.g., warmth and competence). The dimensions, as well as the prompts, are grounded in social science theory, namely Fiske’s Stereotype Content Model (SCM).  Reviewers appreciated the fact that the study was interdisciplinary and based in social science findings, and are generally in agreement that the methodology is sound (ratings of 3-4). There were some concerns about how dependent the results are on the specific prompts used, but this is addressed quite conclusively by the authors in their rebuttal with further discussion as well as an ablation study. Additional discussion on cases where the LLM results diverge from expected results (i.e. published human studies), as well as how stereotypical biases can be more subtly expressed in open-ended text responses, would strengthen the revised version. \n\nWhile two reviewers are “ambivalent” with respect to excitement, one reviewer is “strongly” excited by the work. \n\nAC Note: It was not raised in the reviews, but I would caution against attributing these stereotypes to the LLMs directly. The prompts specifically state “I am not interested in your personal beliefs, but in how you think they are viewed by others.” Therefore, in statements like “We propose a framework StereoMap that measures *large language models' perceptions of social groups* using dimensions of Warmth and Competence” I would prefer to see something like “measures *large language models’ perceptions of how social groups are viewed in society*”.  As an example of this distinction, I do not personally believe that women are irrational or can’t do math, but I have knowledge that these exist as stereotypes, which is what the question asks about. \n\nPros:\n- Interesting study examining what knowledge of social stereotypes is encoded in LLMs\n- Grounded in Fiske’s Stereotype Content Model from social psychology\n- Well-written paper\n\nCons:\n- Potential disconnect between model’s stated stereotype “beliefs” when asked directly, versus more subtle expressions of stereotypical bias that can occur in open-ended tasks\n- Possible dependence of results on specific prompts"
            }
        },
        "id": "XznIrihyZb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ExpskenHdP",
        "replyto": "ExpskenHdP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4136/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583841,
        "cdate": 1696707583841,
        "tmdate": 1701465521468,
        "mdate": 1701465521468,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The proposed methodology capitalizes on the numerical data embedded within LJP texts, demonstrating a notable improvement in performance.\nThe authors innovatively apply a momentum contrast-based supervised contrastive learning approach, addressing the issue of large class numbers and the subsequent difficulty in identifying sufficient negative examples within mini-batches.\nThe introduced framework exhibits versatility, being adaptable to current encoder-based methodologies.\n\nThe proposed framework is built around a specific setting (3-task Chinese LJP), and there is no evidence that a similar methodology would be useful in other related LJP tasks from other jurisdictions (Chalkidis et al., 2019; Niklaus et al., 2021; and others)."
            }
        },
        "id": "Ab6DFf2rij",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Exh156fVSS",
        "replyto": "Exh156fVSS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3374/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561377,
        "cdate": 1696707561377,
        "tmdate": 1701465495420,
        "mdate": 1701465495420,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper brings merits regarding \"select-prompt-filter knowledge distillation\" and enhance the summarization for low-resource domains, which is valuable. The paper is expected to add more tech details as mentioned in the rebuttal.|The paper presents an approach for generating weakly supervised annotated datasets for conversation summarization using the outputs from a large language model ; showing strong summarization improvements for low-resource settings. The reviewers agree the paper is sound but have concerns about the incremental nature of the approach and the limited applicability."
            }
        },
        "id": "eQfV97RZZ7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EwNVh5fuRF",
        "replyto": "EwNVh5fuRF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2824/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550172,
        "cdate": 1696707550172,
        "tmdate": 1701465477701,
        "mdate": 1701465477701,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a cumulative averaging of model snapshots from different runts. This aims to avoid the need for extensive hyper-parameter search for zero-shot tasks. Overall the idea is innovative and might open the doors for future research. However, the reviewers also found a number of issues: lack of proper testing on standard methods like ``English-to-X cross-lingual transfer, disregarding standard baselines like \"translate-train\"''(Rev c3Ph); an unlcear writing. In general, having a stronger comparison with baselines could help to improve the paper. That said, the authors  clarified some not clear aspects of the paper."
            }
        },
        "id": "1QhWNAtcrC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EvVWHQ5l6X",
        "replyto": "EvVWHQ5l6X",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5506/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614292,
        "cdate": 1696707614292,
        "tmdate": 1701465561237,
        "mdate": 1701465561237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Two  (APKa, KsvZ) reviewers feel strong excitement about this paper and find the soundness of the study to be quite sufficient in supporting its major claims. Reviewer bZMt urged authors to better clarify the paper’s seeming disconnect between the motivation and the operationalized tasks. Overall, there are some few issues that can be better addressed in the final version of the manuscript as suggested by the authors 1. Clarifying theoretical motivation in intro; tying back findings to theoretical motivation / terminology/ definitions (e.g., agonism), 2. Addressing low annotator agreement, 3. Making the motivation more explicit in connection to the tasks. \n\nThe authors address the reviewer questions/ concerns quite sufficiently in their rebuttal."
            }
        },
        "id": "pDIRqdtxMI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EuMmDTVFjL",
        "replyto": "EuMmDTVFjL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4530/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592232,
        "cdate": 1696707592232,
        "tmdate": 1701465533859,
        "mdate": 1701465533859,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In words of one of the reviews:\n\"The paper investigates the application of the Learning form Label Proportion (LLP) to natural language. In particular, the authors present a new training objective which is a weighted combination of a modified version of total variation distance and self-supervised contrastive loss. The paper presents some theoretical justification and an extensive parameter analysis. The effectiveness of the method is demonstrated by experiments on five well-known datasets and four models.\"\n\nA clear strength of this paper is the out-of-the-trodden path setting which opens up interesting new research questions, as well as - arguably (see below) practical applications. The contributions of this paper are both theoretical as well as empirical.\n\nThe main criticisms are about the practicality of the setting - having proportion of levels but not fine-grained ones. While this seems like a reasonable situation, it would be arguably more compelling if a real use-case could be mentioned (if not used). There are although concerns about the clarity of the writing - the authors are encouraged to take those comments to heart."
            }
        },
        "id": "mRcqI1IthW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EtNebdSBpe",
        "replyto": "EtNebdSBpe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1267/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508916,
        "cdate": 1696707508916,
        "tmdate": 1701465425636,
        "mdate": 1701465425636,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work evaluates human abilities to identified original & translated texts.\n\nAll three reviewers agree that the soundness of the proposal is either good or strong, while there is more variability with respect to excitement (varying from 2-4).  The three reviewers have raised interesting points of strength and weakness during the review period, to which the authors have provided clear answers during the rebuttal period. Although there is some disagreement between the reviewers with respect to the issues of criticism, they are compatible and enriching. \n\nShould the manuscript be accepted, I recommend the authors to incorporate the suggestions made by reviewers, including all the details possible on the methodology (e.g. annotators, etc.), a detailed motivation and context/scope of the work, the missing references and typos. The suggestion to carry out additional experiments is interesting for future research but, in my opinion, should not preclude publication."
            }
        },
        "id": "pe9OqDdULr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EtC8wfjSw4",
        "replyto": "EtC8wfjSw4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission277/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483539,
        "cdate": 1696707483539,
        "tmdate": 1701465393427,
        "mdate": 1701465393427,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper defined a new Grounded Compositional Concept Learning (GCCL) task, modified two existing datasets into CompFlickr and CompCOCO for this task, and proposed a method based on retrieval and meta-learning for this task,\n\nAll reviewers were in consensus that the paper is sound with sufficient experiments backing up their main contribution of combining meta-learning techniques with retrieval methods.\n\nOn the flip side, reviewers were also concerned regarding the practical application of the proposed approach, since the main experiments are conducted on synthetically modified versions of existing image-text datasets. There were also concerns that experiments were tested primarily on relatively old methods, and as a result there were no reviewers who felt particularly excited about the new practical applications of the methods proposed in the paper. I would suggest the authors try to tailor their problem setting so that the more recent (generative) vision language models are compatible, which should lead to a more accurate reflection of the difficulty of the problem and can pave the way for more impactful methods designed for the problem. It would also help to have one key experiment on a truly real-world task (not modified from other datasets) in order to showcase how realistic the task actually is."
            }
        },
        "id": "Z3Kw5p56wq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EpJ7qqR0ad",
        "replyto": "EpJ7qqR0ad",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4194/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585197,
        "cdate": 1696707585197,
        "tmdate": 1701465523832,
        "mdate": 1701465523832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a new metric that's robust to perturbations and that can be used for multilingual image captioning. Alongside the metric, a data set is introduced that is used to verify the robustness of this metric.\n\nPros:\n- Tackling an important problem; many other evaluation metrics are highly susceptible to perturbations.\n- Multilingual approach, much better than just focusing on one language.\n- Introduces both a new metric and a new data set that can both be used by the community.\n- Maintaining a strong correlation with human judgment while improving robustness to perturbations.\n\nCons:\n- The comparison to other metrics -- especially as it relates to human judgment correlation -- in the current review copy could be clarified further, e.g. with a table as shown at the bottom of https://openreview.net/forum?id=EpBNf4Arod&noteId=SIuJnYRUqN (it would make sense for this to be in the main body, not relegated to an appendix).\n- There's not a lot of detail around how the data set was constructed."
            }
        },
        "id": "JIZBU8hv4U",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EpBNf4Arod",
        "replyto": "EpBNf4Arod",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1088/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707504721,
        "cdate": 1696707504721,
        "tmdate": 1701465420158,
        "mdate": 1701465420158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Three reviewers provide feedback for this paper and their reviews were with consensus. The reviewers found the dataset interesting and useful. They found the extensive evaluations to be well done. They appreciated the human evaluation. The details provided in the paper, particularly about the annotation process was appreciated. There were very few concerns pointed out by the reviewers, and these were fairly minor. Given this consensus in soundness and excitement, I recommend accepting the paper."
            }
        },
        "id": "5WNG89IamV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EnUgSeghBl",
        "replyto": "EnUgSeghBl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3333/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560667,
        "cdate": 1696707560667,
        "tmdate": 1701465494264,
        "mdate": 1701465494264,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper delves into the problem of fine-grained category discovery and introduces denoised neighborhood aggregation, a self-supervised framework aimed at encoding semantic structures into the embedding space. The paper provides both theoretical analysis and empirical experiments to demonstrate the effectiveness of the proposed model.\n\nThe consensus among all reviewers is that this paper exhibits strong soundness and excitement. However, there are several concerns that need further attention from the authors, including:\n\n1. Definition Clarification: Some term definitions are found to be confusing and would benefit from clearer explanations.\n2. Boundary Settings: For example, how to solve the situation without any neighbors. The authors can give more explanations.\n3. Ablation Study: A broader range of ablation studies on multiple datasets should be conducted to make the experiments more convincing.\n4. Hyper-parameter Settings: The paper lacks sufficient details regarding the choice of hyper-parameter settings, which should be provided for transparency.\n\nIn conclusion, this paper offers strong quality and novelty, with the potential to contribute significantly to the research community. To further improve its quality, I recommend that the authors diligently address the concerns raised by the reviewers during their final revision."
            }
        },
        "id": "H1s1JfDATw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EkftL7NgtW",
        "replyto": "EkftL7NgtW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3622/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566383,
        "cdate": 1696707566383,
        "tmdate": 1701465504110,
        "mdate": 1701465504110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces ProAttack, a method for clean-label textual backdoor attacks on language models that directly uses prompts as triggers. Reviewers acknowledge the paper's importance in LLM security and its effectiveness in various settings. They also appreciate the paper's attempt to highlight the vulnerabilities of LLMs to prompt-based attacks, which could have significant real-world implications. However, concerns were raised about experimental evaluation, including the absence of evaluations against prompt-based baselines and common defense methods. The authors addressed these concerns in their rebuttal and provided extensive experimental results. Based on unanimous feedback from the reviewers regarding evaluation, it is highly recommended to incorporate the new results into the main content rather than the appendix."
            }
        },
        "id": "zVccjmeBpZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ek87791lcO",
        "replyto": "Ek87791lcO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission992/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707501690,
        "cdate": 1696707501690,
        "tmdate": 1701465417229,
        "mdate": 1701465417229,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary:\nThe paper presents a unified framework for addressing three scientific literature understanding tasks: multi-label classification, link prediction, and search. To overcome the limitations of backbone parameter sharing, the authors employ task-specific Transformer blocks within the unified model. They introduce two types of Mixture-of-Experts in the Transformer architecture to route inputs to different Multi-Head Attention (MHA) and Feed-Forward Network (FFN) sub-layers based on the task at hand. Experimental results confirm the method's effectiveness.\n\nReason To Accept:\n1.The paper is well-structured and presents its ideas in a clear and comprehensible manner.\n2.The paper introduces an innovative approach by unifying various scientific literature understanding tasks within a contrastive learning framework. It devises four model variants to generate task-aware representations, facilitating shared knowledge acquisition across diverse tasks while mitigating interference from task-specific skills.\n3.The authors address the issue of unwanted backbone parameters-sharing, preventing task interference, and propose a reasonable approach involving instruction tuning to further enhance the performance within the proposed unified framework.\n4.The paper conducts extensive experiments across a wide range of benchmarks. The results demonstrate that the proposed SciMult method surpasses state-of-the-art scientific language models in both in-domain and cross-domain settings.\n\nReason To Reject:\n1.According to Table 4 and Table A4, the proposed method underperforms or performs on par with SPECTER 2.0 on SciDocs (an in-domain dataset) when the evaluation setting aligns with SPECTER 2.0 (Linear and Reranking). This suggests that the performance improvement may be subject to the contrastive learning setting.\n2.The assumption of the method is somewhat strong, that each label's name and definition must be both available.\n3.The idea of multi-task learning and contrastive learning are not new, and there are no specific new insights for the task itself from the task nature perspective, which leads to a marginal contribution for the research field."
            }
        },
        "id": "lHxyLju6TW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Eib6OOeVJI",
        "replyto": "Eib6OOeVJI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2390/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707541225,
        "cdate": 1696707541225,
        "tmdate": 1701465463634,
        "mdate": 1701465463634,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a lightweight approach for retrieving prompts from a collection of tasks for an unseen target task. \n\nPros:\n\nThe approach is novel and intuitive\n\nComprehensive evaluation which demonstrates superior performance in comparison to traditional methods in multiple large LLMs and a wide range of tasks\n\nAdditional ablations, analysis of how and why UPRISE works \n\nCons\n\nNeed more intuition about the retriever to back up the claim that the prompts are indeed “interpretable”\n\nMore analysis on where and which types of tasks UPRISE does not perform well. \n\nThe original work will also benefit from incorporating the additional discussion points/experiments conducted during rebuttal."
            }
        },
        "id": "5AXCgzZ9iU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EhPYwBBFYb",
        "replyto": "EhPYwBBFYb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission462/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488375,
        "cdate": 1696707488375,
        "tmdate": 1701465399798,
        "mdate": 1701465399798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Based on the three reviews, the majority of reviewers are positive about the paper's contributions and presentation. Reviewers appreciate the clear motivation and well-organized structure of the paper. The experimental results are considered solid, and the proposed method shows improvement over baselines. The use of a fine-grained reward function to focus on keywords is seen as a valuable contribution.\n\nHowever, there are also some concerns raised by the reviewers. One reviewer questions the necessity of the proposed method, as the improvement over baselines is not substantial and the fluency of the generated responses is affected. Another reviewer points out the unclear explanation of the reward model and questions the uniqueness of the next word sampling approach. The confusion around the baselines and the comparison in Table 2 is also mentioned.\n\nOverall, the paper is well-written and the proposed method is straightforward and valuable. The experimental results and analysis support the claims. However, there are some areas that need improvement, such as further explanation of the reward model and clearer comparison with baselines. The necessity and effectiveness of the proposed method should be addressed more thoroughly."
            }
        },
        "id": "T00SaGYvOK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EY9k2x5qWB",
        "replyto": "EY9k2x5qWB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1258/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508727,
        "cdate": 1696707508727,
        "tmdate": 1701465425393,
        "mdate": 1701465425393,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work presents a benchmark using K-12 exams for LLMs in Indonesian languages. It demonstrated that LLMs encountered difficulties in answering questions related to local languages and cultures. The reviewers agree that the dataset, i.e. IndoMMLU, is a valuable resource for evaluating the reasoning abilities in Indonesian languages and real-world knowledge about Indonesia. The comprehensive analysis showed interesting insights about different LLMs' abilities. Another good aspect is that this research is on promoting AI democratization, i.e. drawing attention to the LLMs' limitation on low-resource languages."
            }
        },
        "id": "BUmSjpfQcO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EVfHUvhRra",
        "replyto": "EVfHUvhRra",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3978/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580502,
        "cdate": 1696707580502,
        "tmdate": 1701465515931,
        "mdate": 1701465515931,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper probes sentence embeddings from two pretrained language models, RoBERTa and ELECTRA, to better understand how well these models capture and generalize properties of spray/load verb alternations in English. It uses a sentence pattern-matching task (Blackbird Language Matrices) to do so, and shares the dataset and findings from resulting experiments, which suggest the models are adequate at the task but commit a consistent type of error. All in all, reviewers praised the novelty of the task and the leveraging of linguistic insights to investigate model “understanding”, as well as the detailed qualitative error analysis. There was some concern, however, that experimental and dataset construction details were missing and unclear (e.g. how were the embeddings acquired from RoBERTa and ELECTRA, how were sentences extracted in SPIKE, Figures 2, 3, 9, 10 could use clarifying details, etc.). One reviewer also requested a more detailed exposition of the maintenance and extensibility of this dataset to other phenomena. Lastly, there was some disagreement over the inclusion of baseline comparisons for this task, e.g. random chance, bag of word2vec vectors, human performance. The authors defended their choice to exclude these comparisons, and while I find their rebuttal satisfactory, it would perhaps be beneficial to include the points raised in the rebuttal in the revised paper.\n\nIn summary, quite a few minor revisions are needed to address all reviewers’ comments and concerns, which would benefit the Soundness (two 3s, one 4) and clarity of the paper. Barring one reviewer, the Excitement scores were quite high (two 4s, one 2); I would advise the authors to consider the concerns of that reviewer though, i.e. the maintenance and extensibility of the dataset. Given how novel and specific the task is, understanding how it contributes to the wider field of language model analysis and interpretability would raise the impact and interest of this paper."
            }
        },
        "id": "k5R91UY40p",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EV5dNDiC7I",
        "replyto": "EV5dNDiC7I",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3507/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564155,
        "cdate": 1696707564155,
        "tmdate": 1701465499559,
        "mdate": 1701465499559,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces the novel technique of Adaptive-Consistency for large language models (LLMs), addressing the limitation of traditional Self-Consistency by dynamically adjusting the number of samples. This adjustment is based on the agreement level in the generated samples, leading to more efficient and targeted sampling. Experimentally, across 17 reasoning and code generation datasets with three distinct LLMs, the technique exhibited an impressive reduction in sample budget (up to 7.9 times) with only a marginal compromise in accuracy. Reviewers noted simple but innovative approach, thorough experiments, and broad application potential. \n\nPros:\n\nInnovative approach that dynamically adjusts sample numbers, addressing existing limitations in Self-Consistency.\n\nComprehensive experimental design and results across various datasets, showcasing efficiency and effectiveness.\n\nClear and well-justified presentation coupled with the potential for significant practical benefits in terms of cost and time.\nCons:\n\nSome lack of clarity on experiments with a dynamic budget and potential complications if the budget is depleted prematurely.\n\nAbsence of details on the computational expense (in original draft) related to the numerical integrations for the Dirichlet/Beta/CRP models, raising questions about real-time efficiency."
            }
        },
        "id": "WpzZjACQYc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ETNa4Wb65J",
        "replyto": "ETNa4Wb65J",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2756/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548720,
        "cdate": 1696707548720,
        "tmdate": 1701465475285,
        "mdate": 1701465475285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work effectively leverages soft prompt retrieval to improve the zero-shot performance of instruction-following models and provides an easy method to improve instruction-following capabilities without fine-tuning. The reviewers raised some concerns about unclear writing and we encourage the authors to improve the presentation."
            }
        },
        "id": "GEC5GL6r3I",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ESyts8YSub",
        "replyto": "ESyts8YSub",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3677/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567536,
        "cdate": 1696707567536,
        "tmdate": 1701465506320,
        "mdate": 1701465506320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers were unanimous that this is a solid paper, and I concur. To add to their notes, I appreciate the distinctive hypothesis that this tests relative to much of the analysis work in NLP.\n\nAs for positives, reviewers note the rigor and likelihood of stimulating discussion and understanding of LMs. For negatives, reviewers wish for more LMs in the evaluation; I’m not overly concerned about this. I think the paper is rigorous and relatively clear, but additional work could be put into presentation of the various compression metrics and their evaluation (some nice figures would help.)\n\nAdding my own concern, I think the destruction of linguistic structure through vocabulary swapping etc. is rather severe, and so the increase in intrinsic dimensionality seems less interesting. I encourage the authors to consider some less severe transforms, like noisy paraphrase.\n\nThe limitations section notes the disconnect between the somewhat lofty discussion of infinite use of finite means in the introduction and the concrete experiments in the paper; I think that walking readers along the connections would help this!"
            }
        },
        "id": "QNOszVvkEz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ESgkAKGUJP",
        "replyto": "ESgkAKGUJP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2814/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550008,
        "cdate": 1696707550008,
        "tmdate": 1701465477444,
        "mdate": 1701465477444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "After several rounds of discussion with the authors, the reviews lean positive on this paper (both reviewers who left 3 \"soundness\" scores indicated they were really 3.5s in in their comments). The reviewers highlight the thoroughness of the the analysis and the usefulness of this work as a resource for others working in comparative reasoning. There are some concerns about comparative reasoning as a task, specifically whether it should come at the expense of other quality aspects and how it relates to other tasks and datasets (that could potentially make a stronger evaluation)."
            }
        },
        "id": "5RjiD6TBeR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ESGY2Ftbfg",
        "replyto": "ESGY2Ftbfg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4089/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582912,
        "cdate": 1696707582912,
        "tmdate": 1701465519847,
        "mdate": 1701465519847,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose a method to create synthetic data for training an MT QE model using a machine translation (MT) model and constrained beam search. The main idea is simple (which is good!), straightforward and leads to quality improvements. The author might want to improve the writing at some points in the paper. The reviewers overall agree that this is solid work and suggested only minor changes."
            }
        },
        "id": "w9i3YOoaEu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EJ4N7PX6dm",
        "replyto": "EJ4N7PX6dm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1806/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527559,
        "cdate": 1696707527559,
        "tmdate": 1701465442351,
        "mdate": 1701465442351,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper \"Geographical Erasure in Language Generation\" presents work on the imbalance of generated data from LLMs, more concrete about the lack of representation of certain languages in generated data. The paper also presents a method to counter this phenomenon. \n\nArguments for rejecting the paper mentioned by the reviewers are related to the novelty of the task and details on basic assumptions.\nArguments for accepting the paper mentioned by the reviewers are related to the aspect studied (geographic erasure) and its potential to extend this to other similar phenomena."
            }
        },
        "id": "cAYTOQ60Qf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EG7gjHZ8cm",
        "replyto": "EG7gjHZ8cm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1562/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707517694,
        "cdate": 1696707517694,
        "tmdate": 1701465434512,
        "mdate": 1701465434512,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In general, reviewers agreed that this paper provided sufficient support for its major claims, although some minor points could benefit from further support or clarification.  They exhibited mixed enthusiasm for seeing the paper at the conference, with excitement ratings ranging from mediocre to strong.  The authors provided additional support for many of the claims that reviewers found to be more weakly justified, and they promised to update the revised manuscript with this additional information.\n\n**Summary of Reviewer Feedback and Discussion:**\n- **Reviewer i9MT** found the proposed task to be interesting and bold, and they appreciated that the authors proposed a dataset to facilitate research towards that task.  They also felt that the experimental results may inspire additional related research.  However, they were unconvinced that multimodal models have demonstrated evidence of emergent abilities, and they felt that the paper's problem definition was unclear.  They also felt that the paper's technical contributions, including comparisons of different potential baselines for the new dataset, were limited.  The authors clarified their problem definition in the rebuttal and promised to formalize it more clearly in the revised manuscript.  They also clarified that their technical contribution is the new dataset, and agreed that there are many more model comparisons that they can perform in the future to study their problem from different angles.  As a point of clarification, the authors noted that they intended to refer to emergent abilities of LLMs, rather than large multimodal models.\n- **Reviewer gqEJ** thought that the proposed task was challenging and that the provided dataset will be useful for investigating it further, but they felt that the paper didn't adequately highlight the nuances involved in associated problem domains.  They also felt that the authors may have missed some relevant literature, and they raised several clarifying questions and a clarifying comment.  In their rebuttal, the authors clarified the variety of tasks in their dataset and noted that they chose one specific technical challenge as the focus of their experiments but that they agree that there are many other nuances to explore as well.  They also responded to Reviewer gqEJ's other clarifying questions, and promised to clarify these points in the revised manuscript.\n- **Reviewer nrzY** thought that the proposed dataset was novel, high-quality, and targeted an interesting scenario.  However, they wished that there had been a baseline trained on this data rather than only baselines using foundation models.  They also requested more details pertaining to both the dataset itself and the subjects who participated in its development.  In their rebuttal, the authors clarified that they hadn't used a baseline trained on their dataset because they were interested in generalizability beyond their dataset alone.  They also responded to Reviewer nrzY's other questions, provided additional details regarding their human subjects, and promised to include more examples and analyses in the revised manuscript."
            }
        },
        "id": "TyhxWfn2DV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EFML9BJcIH",
        "replyto": "EFML9BJcIH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4229/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585796,
        "cdate": 1696707585796,
        "tmdate": 1701465524852,
        "mdate": 1701465524852,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This article aims to study the information stored in text embeddings generated by text encoders, with a particular focus on restoring the original full text from text embeddings. The author describes this task as controlled text generation and adopts a multi-step method to gradually refine the hypothetical text to make it closer to the given target embedding. The proposed method indicates that most short texts (such as 32 tokens) can be almost accurately reconstructed. These findings indicate that private information in the training data may have been leaked from the output text embedding of the language model.In summary, these paper solve privacy issues in vector databases， conduct in-depth research on embedding inversion tasks using embedded geometry and he controlled generation method can recover up to 92% of input text. Meanwhile, this paper's experiments are very comprehensive such as various retrieval-related benchmark datasets are used and covering domain including in-domain and closed-domain reconstruction settings."
            }
        },
        "id": "oDHVWpXFaS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "EDuKP7DqCk",
        "replyto": "EDuKP7DqCk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2068/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533577,
        "cdate": 1696707533577,
        "tmdate": 1701465452648,
        "mdate": 1701465452648,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Pros:\n- Extensive experiments with many models.\n- No reviewers mentioned this, but I think that the discussion of how hyperparameter settings can change scaling laws could be very valuable on a practical level, as it could support hyperparameter selection decisions.\n\nCons:\n- They only consider encoder-decoder models. Because the architectures are limited, there is a missed opportunity to consider inductive biases of radically different architectural approaches.\n- The analysis and discussion is limited. Given that the paper is framed as an exploration of inductive bias, there is little to no discussion of the source or nature of those inductive biases when using different architectures. This makes the title a little misleading; perhaps it should be called “How Does Architecture Influence Scaling” instead.\n- They only consider up to 40B parameter models, which is below the range claimed to show dramatic breakthroughs on capabilities like chain of thought."
            }
        },
        "id": "ELhTNJryGA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "E9dH0BP5VW",
        "replyto": "E9dH0BP5VW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2070/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533654,
        "cdate": 1696707533654,
        "tmdate": 1701465452676,
        "mdate": 1701465452676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reasons to accept, as noted by the reviewers, are mainly in the areas of novelty and originality and include:\n\n- Novelty through the introduction of a new task.\n- Motivation for the application is effectively communicated. \n- The authors evaluated their proposed model using a comprehensive array of baseline models and various metrics to portray its effectiveness (including human evaluation).\n- The method outperforms many strong baselines.\n- The ablation study provides important information on the contribution of different components.\n\nThe areas for improvement are mainly in terms of clarity and the need to add missing details, especially in Section 3:\n- Missing a number of key details in Section 3, which describes the pipeline. In their rebuttal, the authors commit to \"add the example of the input components in the problem setup (Section 4.1) for better understanding and add the implementation details of our method in Experiment section 4.3, including the maximum context length, batch size, etc\"\n-  The coverage of the external knowledge base may not be great enough for all the possible queries.\n- Certain sections may be hard to follow (for instance, the neural prompt section is quite difficult for readers who aren't prompting experts)\n- It's not clear if the data and code will be made available (the authors note that the data is available now and that the code will be made available in the future).\n\nThese can be addressed through revision of the writing and inclusion of further detail, which the authors have committed to provide.\n\nOne specific limitation raised by reviewer AGTQ that the authors do not address in their response is as follows: 'it was mentioned in the first paragraph of the introduction, \"one area where generative LLMs have shown significant potential is in clinical trial protocol design\" without any reference which contradicts the paper's main claim that they are \"first to develop LLMs focusing on trial design\". ' In my view, it is crucial to appropriately address this point in order to convey the novelty of the contribution presented in the current paper.\n\nThe authors should pay careful attention to implementing the suggested improvements to ensure a clearer and more accessible presentation of their work."
            }
        },
        "id": "uBaE9y0hLZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "E5r96sfKO0",
        "replyto": "E5r96sfKO0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission174/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481041,
        "cdate": 1696707481041,
        "tmdate": 1701465389628,
        "mdate": 1701465389628,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates prompting an LLM to perform \"cross-lingual\" chain-of-thought reasoning, in which it first translates the task to English, reasons in English, and then answers in the original language. Experiments show that this yields improvements on several tasks.\n\nThe reviewers appreciated that the approach is intuitive, and effective for the models used in experiments.\n\nHowever, they also had several concerns, including:\n\n* They would have liked to see experiments on more than just two models, and particularly would have liked to see experiments on models whose training was focused on multilinguality (rather than just models main focus is English).\n* They felt the baselines were unnatural/unfair, and the paper omitted the most natural baseline of just prompting the model in the target language.\n* They felt that the paper lacks the level of generalizability or analysis necessary for a long paper."
            }
        },
        "id": "nREYpYLsxA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "E4ebDehO3O",
        "replyto": "E4ebDehO3O",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2997/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553854,
        "cdate": 1696707553854,
        "tmdate": 1701465483819,
        "mdate": 1701465483819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a bootstrapping method to dynamically truncate the hypothesis set in Minimum Bayesian Risk decoding.\nThe reviewers found this paper very exciting, appreciating the problem, the solution, the results, and the writing. There were very minor concerns raised and they were addressed."
            }
        },
        "id": "WSIPnOZP4L",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DzCc4mpH1m",
        "replyto": "DzCc4mpH1m",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2230/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537468,
        "cdate": 1696707537468,
        "tmdate": 1701465458358,
        "mdate": 1701465458358,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Strengths**:\n\n1. Critical problem of detecting whether text is machine generated or human generated.\n\n2. Two novel approaches, DetectLLM-LRR and DetectLLM-NPR. Comprehensive experimental evaluations on three datasets and seven language models. Expts with different decoding schemes such as top-p, top-k, and temperature.\n\n3. The proposed DetectLLM-NPR requires fewer perturbations than previous work while achieving similar performance levels.\n\n\n**Weaknesses**:\n\n1. Anonymous code not shared.\n\n2. It would be interesting to look at certain types of machine-generated text (e.g., text in healthcare or law) that could be more difficult to detect than others.\n\n**Suggestions**:\n\n1. Thanks for putting up results for German. However, since the majority of the evaluation is for English only, limitations should clearly state that the work is for English only.\n\n2. It would be interesting to look at certain types of machine-generated text (e.g., text in healthcare or law) that could be more difficult to detect than others. \n\n3. Please incorporate rebuttal content into the main text."
            }
        },
        "id": "PydkBdRMb7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Dy2mbQIdMz",
        "replyto": "Dy2mbQIdMz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3232/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558704,
        "cdate": 1696707558704,
        "tmdate": 1701465491163,
        "mdate": 1701465491163,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper solves generative retrieval with a new reinforcement learning-based method, which is equipped with different reward models. Experiment results validate the effectiveness of the proposed method. The rebuttal has addressed most concerns of reviewers."
            }
        },
        "id": "2orEhjdY8a",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DxYDP3B31K",
        "replyto": "DxYDP3B31K",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1661/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707522502,
        "cdate": 1696707522502,
        "tmdate": 1701465437847,
        "mdate": 1701465437847,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a Multi-Source Probing (MSP) method to evaluate the dialogue comprehension abilities of open-domain dialogue models. The reviewers mostly agree on the potential significance and originality of the work, as it introduces an innovative method and presents enlightening empirical discoveries. However, there are some concerns and suggestions for improvement.\n\nOne reviewer questions the necessity of employing a multi-source attention mechanism and suggests exploring a variant of probing with decoder tunability. Another reviewer suggests more in-depth probing experiments and analyzing dialogue discourse relations. They also mention the possibility of shortcuts in the probing model due to the growing number of trainable parameters. Additionally, there are requests for clarification on the MSP and Prompt Based methods, the role of decoder models compared to encoder-decoder models, and the use of recurrent models. The implementation details of the verbalizer are also requested.\n\nThe reviewers generally find the paper sound, with sufficient support for its major claims, but some minor points need more support or details. The presentation and writing style are considered clear and easy to understand.\n\nIn terms of excitement, there is ambivalence among the reviewers. While the paper has merits like reporting state-of-the-art results and presenting a nice idea, there are also concerns about incremental work and the need for further revision.\n\nOverall, the paper shows promise but should address the reviewers' concerns and suggestions to improve its clarity, originality, and significance."
            }
        },
        "id": "097YqMVbZb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Dqg9SLOXZu",
        "replyto": "Dqg9SLOXZu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1328/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510398,
        "cdate": 1696707510398,
        "tmdate": 1701465427540,
        "mdate": 1701465427540,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper works on generating the natural language description for knowledge graph triples. The reviewers find the proposed datasets to be useful, practical. The proposed method seems novel and extensively evaluated with human and automatic metrics. The overall paper seems logically structured well. The reviewers share concern about not including the recent language models and not studying the decoder only models. The author responses have mostly addressed the reviewers comments and the authors should include the new results in the draft to make the paper clear. A thorough investigation of when the proposed method generalizes to good accuracy need to be worked out."
            }
        },
        "id": "xU1PtWZ1qX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Dq023aV4Ih",
        "replyto": "Dq023aV4Ih",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4803/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598768,
        "cdate": 1696707598768,
        "tmdate": 1701465542207,
        "mdate": 1701465542207,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Overall this is a solid paper with extensive experiments and convincing results for the proposed framework KnowEE in different knowledge injection settings.\n\nThe novelty of he work was somewhat challenged by one of the reviewers, due to missing reference and comparison with MSDP in the original submission. While the authors provided further experiments with MSDP during author response, the reviewer was not fully convinced of positioning of the work as completely novel instead of incremental. \n\nAnother criticism around the lack of generalization analysis to real-world scenarios is acknowledged by the authors, but will only be addressed in future work."
            }
        },
        "id": "YQI4fcInZw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DpNUrB6SeZ",
        "replyto": "DpNUrB6SeZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1713/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707524822,
        "cdate": 1696707524822,
        "tmdate": 1701465439171,
        "mdate": 1701465439171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a methodology for approaching complex commonsense question answering via decomposition, and component-wise verification. The problem is of high significance to the NLP community, and the story builds on a catchy link to cognitive theory by Kahnemann et al. (system 1/system 2).\n\nThe paper generally presents a sound piece of work, but there are concerns over the novelty of the contribution, in particular, the abstract contains a very sweeping claim (\"we are the first to unravel the cognitive reasoning abilities of LMs\"), while several related works on question decomposition are not at all, or insufficiently, discussed. Similarly, the explanation of the core of the methodology, the generation of subquestions, is not entirely clear, and would benefit from step-by-step discussion of a running example.\n\nOverall, the significance of the topic, and presented ideas warrant publication, yet the paper would significantly benefit from a careful revision, in particular w.r.t. its positioning."
            }
        },
        "id": "BxGulfqAKS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DmrIEHJxN5",
        "replyto": "DmrIEHJxN5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission451/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488114,
        "cdate": 1696707488114,
        "tmdate": 1701465399391,
        "mdate": 1701465399391,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes a preprocessing layer with IIR filters to efficiently handle the inputs. Reviewers appreciate the novelty and the structure of this work, with comprehensive backgrounds and results. Therefore, the AC recommends acceptance of this paper to EMNLP."
            }
        },
        "id": "u2K1OgjQm9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DlQeSfGYfS",
        "replyto": "DlQeSfGYfS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2192/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536593,
        "cdate": 1696707536593,
        "tmdate": 1701465457302,
        "mdate": 1701465457302,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces the task of identifying missed statements that lead to misunderstandings later in a dialogue. This is an interesting task; the proposed solution is novel and might have applications in other tasks. The paper is well-written, and the experiments are comprehensive. The problem statement needs further elaboration to provide convincing motivation for the need for such a system. The practical applications must be clarified.\n\nThe paper will benefit from all the modifications mentioned in the authors' rebuttals. Specifically, the authors need to add:\n\n- Better motivation and problem statement, along with examples \n- Clarifications on the threshold setting\n- Discussions on the computational cost \n- Explaining the differences between validity and uniqueness evaluations"
            }
        },
        "id": "gwatoirWvc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Dl3YgoOh2c",
        "replyto": "Dl3YgoOh2c",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3152/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696708174734,
        "cdate": 1696708174734,
        "tmdate": 1701465488578,
        "mdate": 1701465488578,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses cross-lingual natural language understanding and proposes to minimize representation coding rate reduction between languages in order to learn representations that do not use extra codes to encode language-specific information.\n\nPros / Strengths:\n- The idea is novel and interesting as the approach does not require parallel data or accurate distribution estimates.\n- The paper is well written.\n- The approach is evaluated on three datasets against some baseline models based on mbert.\n\nCons / Weaknesses:\n- The baselines are not convincing (important models such as XML-R are missing), weakening the experimental sections. (Note: the authors have extended their experiments during the response period, including XML-R for at least two of their datasets. Why not all three?)\n- The number of tasks is small considering that there are many more popular multilingual benchmark datasets available.\n\nAction items for improving the paper:\n- The experimental section is a bit weak. To increase the value of the paper for other researchers and its influence on future research, the authors should add results with stronger multilingual models (such as XLM-R (as already partly provided during the rebuttal phase)) as well as results on (at least a few) more datasets."
            }
        },
        "id": "7Ywh9vJe1R",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DjwSceRw7B",
        "replyto": "DjwSceRw7B",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2091/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534238,
        "cdate": 1696707534238,
        "tmdate": 1701465453668,
        "mdate": 1701465453668,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method to generate adversarial examples based on disentanglement and word attribution techniques. Extensive experiments show the effectiveness of the proposed method. Based on the initial reviews, the reviewers found this paper working on an interesting problem, proposing a reasonable method, and conducting comprehensive experiments. However, there are many important details missing regarding, for example, the threat model, the context word substitution process, the motivation and analysis of disentanglement. Also, there is a question raised whether the tested models (i.e., BERT-base and T5-large) are really LLMs. \n\nIn the rebuttal, the authors addressed most of the reviewers' concerns and answered detailed questions. Despite presenting additional results on Llama, the paper would be more interesting if the authors focused specifically on large language models as the NER robustness of pre-trained models has been studied in the literature while we know much less about how the emergent ability of LLMs (due to their enormous size) responds to the adversarial examples. Also, based on the threat model explained, the adversary works on the white-box scenario (with access to NER model's architecture and training data) of which the specific application value is not quite clear. \n\nIf the paper is accepted, please consider fusing the rebuttals into the camera-ready version (especially on the threat model, the analysis of disentanglement, and the results on Llama) and revising the title (as discussed) so it better reflects the actual contributions of the paper."
            }
        },
        "id": "FIyfPyjeuC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Dil6z5sZkD",
        "replyto": "Dil6z5sZkD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4639/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707594997,
        "cdate": 1696707594997,
        "tmdate": 1701465537424,
        "mdate": 1701465537424,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper evaluates the performance of large language models (LLMs) on two legal tasks: statute prediction and judgment prediction. The reviewers appreciate the focus on the Indian legal system, which is not extensively studied, and the comparison of LLM-generated explanations with expert annotations. However, they raise concerns about the lack of novelty in methodology, the small size of the test dataset, and the clarity of the paper. The authors' rebuttal addresses these concerns, providing detailed explanations and additional experiments. The reviewers also suggest improvements in the presentation and request further clarification on certain aspects of the study. Overall, the paper is seen as a valuable contribution to the field, despite some weaknesses."
            }
        },
        "id": "hFLK6BCxg0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DgNnVebNPy",
        "replyto": "DgNnVebNPy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission311/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484423,
        "cdate": 1696707484423,
        "tmdate": 1701465394446,
        "mdate": 1701465394446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper evaluated the capabilities of multilingual LLMs on code-switched (CSW) data based on direct fine-tuning and prompting of LLMs. The evaluation is performed on four downstream tasks (sentiment analysis, machine translation, summarisation and word-level language identification) across four code-switched English corpus (Spanish-English, Malayalam-English, Tamil-English, Hindi-English/Hinglish, and MS Arabic-Egyptian Arabic).\n\nAll the reviewers see the evaluation of LLMs on CSW data has a promising direction especially evaluation in low-resource setting and a well motivated problem. The selection of tasks and models are also adequate to prove the point in general about the inability of LLMs to perform well on CSW data. There a few concerns raised by the reviewers like providing fine-tuning data sizes, non-evaluation of more recent LLMs like ChatGPT, no qualitative analysis of model output, comparison with non-CSW data, many of these concerns have been appropriately answered by the authors."
            }
        },
        "id": "VSzR8slIPD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DgB01RzOqo",
        "replyto": "DgB01RzOqo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2438/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542202,
        "cdate": 1696707542202,
        "tmdate": 1701465465118,
        "mdate": 1701465465118,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a conversational nudging technique that allows the system to proactively plan and suggest items, improving the final outcome of item selection by the user. The paper has done the necessary evaluation and their method is sound."
            }
        },
        "id": "4akEiuCsTj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DVDGNFn1Jm",
        "replyto": "DVDGNFn1Jm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1830/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528374,
        "cdate": 1696707528374,
        "tmdate": 1701465443794,
        "mdate": 1701465443794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper considers the problem of learning subjective tasks where annotators might have different perspectives, and proposes to capture those perspectives with \"annotator embeddings.\" Results show that these embeddings improve performance, and the paper additionally carries out a large number of analyses to understand the learning process.\n\nThe main strength of this paper is the analysis of the approach, which is incredibly detailed and very interesting to read. I suspect that the analysis will be the thing remembered about this paper.\n\nThe main weakness of this paper is the setting - it assumes that the training and test data contain the same annotators. As a result, this almost reads more as a \"personalization\" paper than an \"annotator modeling\" paper. Reading it under that view does not - to me - detract from the paper, and in some ways makes it more interesting. The paper does not make a case for why the \"same annotator\" setting is realistic/interesting.\n\nThe \"unknown annotators\" experiment somewhat addresses this question (in the personalization language, this is the \"cold start problem\"). However, as AC, I do not understand this result. Based on the paper and the discussion with reviewer Ejmf, my understanding is that none of the reviewers at test time are seen at training time, that they're given a random embedding, and yet, even in that situation, there is no large degradation in performance from the Text Only baseline. But all this says is that having random embeddings doesn't hurt over having no embeddings, which is not saying much. The more interesting comparison is to the results from Table 4. As an attempt to reconstruct them (comparing to E_n+E_a for instance), we have:\n\n            E_n+E_a                E_n+E_a\n            (known annots)      (unnkown annots)\n  MDA        75.76                      74.24\n  HUM        53.89                      53.51\n  COM        44.41                      40.28\n  GOE        69.90                      61.96\n  SNT         64.61                      37.90\n\nHere, MDA and HUM show basically no difference, COM shows a reasonably big difference (4%) and GOE and SNT show large differences (8% and 27% respectively). However, in Table 4, MDA and HUM are the ones where E_n+E_a doesn't really improve over Text Only, and GOE and SNT are the ones where they do.\n\nBasically all this seems to be saying is that when this approach provides big gains (in Table 4), it no longer does (in Table 6).\n\nThis is not at all surprising - there's no reason to even expect that this would work.\n\nThis is all to say that I agree with this weakness - though thinking of this as personalization rather than annotator disagreement mitigates this - and I don't think the experiment described really attenuates the concern."
            }
        },
        "id": "LtXhzWDnjf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DTyMi3ReQU",
        "replyto": "DTyMi3ReQU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission129/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479894,
        "cdate": 1696707479894,
        "tmdate": 1701465388172,
        "mdate": 1701465388172,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper comprises an innovative backdoor attack method involving large language models, offering a unique approach to adversarial training labels and delivering strong backdoor attack performance. The work presents a detailed methodology, comprehensive experiments and a defense against backdoor attacks. However, reviewers expressed concern over the model's performance with the AGNews dataset relative to baselines and the lack of a complete defense analysis for all datasets. The paper is generally viewed to be well-written and organized, though some suggest that there is room for clarification and polishing. The authors effectively rebutted raised concerns, resulting in a better understanding of their work."
            }
        },
        "id": "CvFNMtTalE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DTELCDufzE",
        "replyto": "DTELCDufzE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5285/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610383,
        "cdate": 1696707610383,
        "tmdate": 1701465555218,
        "mdate": 1701465555218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses the problem of noisy training data for fine-tuning pre-trained language models and proposes a method that leverages external large language models. In particular, the paper separates the training examples into three parts (easy clean, hard clean, and true noisy) and utilizes different objectives to learn from those different categories.\n\nPros / Strengths:\n- The authors conduct exhaustive experiments on both synthetic and real-world noisy datasets.\n- The paper is well written and the method is well described.\n- Mitigating noise in data is of general interest and any significant contribution is likely to have wide impact.\n\nCons / Weaknesses:\n- The method assumes the existance of high-quality large language models.\n- The relationship of the method (and empirical comparisons) to other areas in NLP with noisy data labeling or other LLM-based methods is missing.\n\nAction items for improving the paper:\n- The comparisons with knowledge distillation as presented during the author rebuttal phase are an interesting baseline and should be added to the paper to contrast the work against this line of work.\n- The related work section should be extended, i.a., with work on label smoothing"
            }
        },
        "id": "luuhhILUtp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DSmHC8bi3j",
        "replyto": "DSmHC8bi3j",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1309/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510093,
        "cdate": 1696707510093,
        "tmdate": 1701465426946,
        "mdate": 1701465426946,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes prompting strategies for multi-hop question answering on HotpotQA, MusiQue, and 2WikiMultihopQA. It generates a set of subquestions that forms a tree (i.e., answers to some questions feed into other questions) and then answers each subquestion by traversing the tree. The reviewers found the results convincing and the paper well-written. On the other hand, two reviewers noted similarities between the proposed approach and other approaches inspired by chain-of-thought prompting. Thus, they had lower excitement scores than soundness scores."
            }
        },
        "id": "OF0Az1kfo6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DRpZjTJKZh",
        "replyto": "DRpZjTJKZh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1315/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510241,
        "cdate": 1696707510241,
        "tmdate": 1701465427199,
        "mdate": 1701465427199,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes multimodal stance classification and provides a new dataset. The dataset collection procedure is well explained. However, it could benefit from more details about the annotators: who were they? How long does it take to label? How much were they paid? I would suggest statistics about the dataset's language content: #tokens, #words, etc. The proposed method is reasonable, and the experimentation is exhaustive. Listed issues have been addressed and defended in rebuttal comments, and there is openness to clarify different aspects of the final version of the manuscript. The dataset might be of interest to the community, and it would be available upon publication.\n\nPros\n* Novel dataset and stance-classification-related task\n* Exhaustive evaluation and promising results\n\nCons\n* Missing minor dataset information about annotators and language content"
            }
        },
        "id": "FST6R5bS5D",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DQVGhBdAPG",
        "replyto": "DQVGhBdAPG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4220/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585579,
        "cdate": 1696707585579,
        "tmdate": 1701465524468,
        "mdate": 1701465524468,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies unsupervised sound source localization from multimodal input (images of objects that make sound, along with the corresponding sound waveforms they produce). \n\nPros (from reviewers):\nThe paper studies an interesting and well-motivated problem\nThe presentation of the experiments in the paper is clear\nThe experimental results demonstrate that the proposed method outperforms previous models\nSome of the model components are technically novel (Sounding Map Refinement)\n\nCons (from reviewers):\nThe work may or may not be a good fit for EMNLP (it does deal with multimodality and sound, but not with language)\nThe motivation and reasoning behind some of the model components (and why they work) is not adequately explained\nThe experiments do not explore computational or memory costs\nThe paper did not fully explore the hyperparameter tuning of its proposed components"
            }
        },
        "id": "UKLju2AHJk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DQ9WeXpgJt",
        "replyto": "DQ9WeXpgJt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3606/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566075,
        "cdate": 1696707566075,
        "tmdate": 1701465503381,
        "mdate": 1701465503381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a method for better calibration in question answering / fact probing through cross examination: where they use a series of paraphrased questions / verification questions to the LM in order to judge whether the model’s answer is correct or not. Results demonstrate consistent improvements over 4 datasets.\n\nThe reviewers acknowledged that the problem tackled in the paper is a critical problem (hr8G), experiments and analysis are insightful  (hr8G, ZiT3) and effectively demonstrate proposed method is effective (eV78, ZiT3), and description of prior work is comprehensive (hr8G, ZiT3).\n\nSome concerns raised by reviewers include: (1) “False” claims used in the experiments are too synthetic/simple (h48G), (2) evaluation in QA is too simple (eV78), and (3) lack of more LM pairs and discussion on the choice of the pair (ZiT3). Lack of experiments with retrieval was also pointed out (ZiT3) but was resolved in author responses. The authors are encouraged to add these additional experiments in the paper and add clarification in the experimental setting in the final version of the paper if accepted."
            }
        },
        "id": "mSsgYD931F",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DPhTTeoyjC",
        "replyto": "DPhTTeoyjC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2117/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534874,
        "cdate": 1696707534874,
        "tmdate": 1701465454682,
        "mdate": 1701465454682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers gave scores of 4,3,3 (soundness) and 4,4,4 (excitement).\n\nStrengths and weaknesses including the following were prominent:\n\nStrengths:\n\n- well written (R1) and thought-provoking (R2, R3)\n\nWeaknesses\n\n- lack of connection to concrete implications for NLP applications using LLMs (R1)\n- questions were raised about the philosophical underpinnings and justification of some claims (R1, R2, R3)"
            }
        },
        "id": "pSXyzlvIaP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DOlbbJhJ1A",
        "replyto": "DOlbbJhJ1A",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2787/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549443,
        "cdate": 1696707549443,
        "tmdate": 1701465476458,
        "mdate": 1701465476458,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers are cautiously positive in their assessment of the paper (3-4).  The proposed method of quickly and cheaply creating labels can be applied to other tasks outside of those tested in this paper.   The agreed upon weakness of the paper is an unclear methodological description, which can be resolved in revisions.  The authors made an effort to address concerns by reviewers."
            }
        },
        "id": "CJFgmbkzPZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "DDvcWpZNgl",
        "replyto": "DDvcWpZNgl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5117/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707607413,
        "cdate": 1696707607413,
        "tmdate": 1701465550403,
        "mdate": 1701465550403,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Proposes a method to generate instruction-tuning data using smaller LLMs in contrast to most earlier work such as Self-Instruct. The main idea is to ensemble multiple LLMs via a greedy consensus algorithm (high inter-model ROUGE-L) to select high-quality samples; the result is similar performance to using much larger closed-LLMs, as measured by ROUGE-L. One issue with closed-LLMs is they often do not have permissive licenses and so a way to generate IT data without them is potentially valuable to the open-source community.\n\nThere were concerns about of using solely ROUGE-L as eval with no human evaluation. In particular, ROUGE-L is also used in the consensus algorithm. There is a risk of over optimizing on that metric."
            }
        },
        "id": "yveHjWGayX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "D9oq45WsKq",
        "replyto": "D9oq45WsKq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3802/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707575740,
        "cdate": 1696707575740,
        "tmdate": 1701465510100,
        "mdate": 1701465510100,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents an innovative and practically relevant approach to model extraction supported by strong experimental evidence. However, it could improve by extending experiments to include larger language models, and a more comprehensive range of tasks would further enhance its quality and applicability. Additionally, considering potential technical depth improvements could strengthen its overall contribution."
            }
        },
        "id": "scgDF7iL4f",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "D97Zfgv4em",
        "replyto": "D97Zfgv4em",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission556/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490749,
        "cdate": 1696707490749,
        "tmdate": 1701465403403,
        "mdate": 1701465403403,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewers unanimously agree the CoT Collection offers a useful resource to practitioners. With the inclusion of additional experiments and clarifications on limitations, requested by the reviewers, this makes for a relevant and robust contribution."
            }
        },
        "id": "R1Cievjix8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "D7omx8QyFP",
        "replyto": "D7omx8QyFP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3909/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578968,
        "cdate": 1696707578968,
        "tmdate": 1701465513785,
        "mdate": 1701465513785,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work contributes a corpus of span interaction annotations for SNLI and FEVER instances. A primary goal of this work is to validate if NLP models use similar reasoning processes to humans. Reviewers appreciated the newly-created corpus, as well as the analyses in the paper."
            }
        },
        "id": "AWdBgB39pr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "D70lPh24o6",
        "replyto": "D70lPh24o6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2674/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547270,
        "cdate": 1696707547270,
        "tmdate": 1701465472959,
        "mdate": 1701465472959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores the linguistic coverage and representation of multilingual language models (MLMs), comparing models from large tech companies and community-driven models. The paper analyzes how different languages, language families, scripts, and dialects are clustered and classified in the embedding space of various MLMs, using visualization and KNN text classification. The paper also evaluates the text generation ability of MLMs for different languages and prompts. The paper finds that community-centered models have better coverage and performance for low-resource languages and dialects, while large-scale models are more focused on high-resource languages. The paper contributes to the understanding of the language representation and separation in multilingual models and provides a nice starting work towards which embedding spaces could be leveraged for visualizing multilingual models. Furthermore, the authors ran additional experiments addressing the concerns/questions from the reviewers about the choice of t-SNE, comparison with other community centered models like BLOOM, other NLU based downstream tasks and will be updating the paper draft with the findings."
            }
        },
        "id": "WFk83Mgg4g",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "D4CoZQY1nt",
        "replyto": "D4CoZQY1nt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4168/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707584542,
        "cdate": 1696707584542,
        "tmdate": 1701465522721,
        "mdate": 1701465522721,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors describe a methodology, TextReact, for including retrieved relevant textual information with respect to a particular chemistry input (e.g., molecular encodings)for predicting chemical reaction tasks (e.g., reaction condition recommendation, one-step retrosynthesis) using machine learned models . Specifically, they use a SMILES-to-text retriever to retrieve relevant text, which is then also encoded (with the chemistry input) to be used as input to a machine learned model -- in this case an encoder-decoder model. Experiments are conducted  using a patent corpus as the auxiliary text source for the reaction condition recommendation and one-step retrosynthesis tasks, showing strong performance over existing baseline methods and a transformer network without text augmentation.\n\n== Quality == The reviewers agreed that the paper was well-written overall, that the methodological approach is well-motivated in considering recent NLP advances, and that the experimental results follow a rigorous procedure and demonstrate strong performance. In combination with the primary results, the the ablation studies further confirm the results, making for a sufficiently convincing case for TextReact as a useful method. The two concerns in this regard were that reviewer EhYp had a question regarding 'retrieved text-only' and reviewer 3bHy pointed out that the experiments aren't universally SotA for both tasks depending on the configuration. However, the authors rebutted this well -- some of these results and discussion which should likely be included in the final version of the paper as they are valid questions.\n\n== Clarity == As previously stated, the paper is well-written overall, especially considering that this isn't a chemistry knowledgeable community (in general). In particular, the figures are effective in giving an overview and providing a scaffolding for the structure of the document. The one comment made is that some qualitative analysis would be useful to given an intuitive perspective regarding characterizing the utility of the text-augmentation in specific settings.\n\n== Originality == As far as the reviewers and I can tell, this is the first use of (text) retrieval-augmented chemistry related predictions. It is a relatively simple augmentation approach shown to work well in practice for two important tasks.\n\n== Significance = First of all, the empirical results are quite promising and convincing -- validating the method and making chemistry contributions in some cases. As RAG-style systems have gained increasing popularity, it isn't unreasonable to believe that this may also be able to be integrated into general LLMs and this work code be a basis for such integration. Overall, this is a good case of applying NLP technology to other potentially impactful use cases. As the code is promised to be released, it is also lowering the barrier for others to continue work in this direction."
            }
        },
        "id": "7CFuabw1Yf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "D4Cb4gAWro",
        "replyto": "D4Cb4gAWro",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2084/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534107,
        "cdate": 1696707534107,
        "tmdate": 1701465453433,
        "mdate": 1701465453433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a novel quality estimation model for GEC that has a higher correlation with the F0.5 score used to evaluate GEC model quality. Then they propose 3 methods of system combination based on edit rescoring and beam search, that use the QE model, and show new SOTA results on two English benchmarks.\n\nThe paper is well-written, the experimental results are solid, with new SOTA on English benchmarks. The paper also offers a thorough analysis of model outputs. It would be interesting to see how this work applied for other languages."
            }
        },
        "id": "NAUGUkybTQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "D1kF1Eq7Mv",
        "replyto": "D1kF1Eq7Mv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5317/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610802,
        "cdate": 1696707610802,
        "tmdate": 1701465555922,
        "mdate": 1701465555922,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All the reviewers have agreed that it is interesting to see such work in code search that tackles with false negatives.\n\nThe reviewers have raised a few questions, and during the rebuttal period, the authors have largely addressed these questions.\n\nI suggest the authors further follow the suggestions of the reviewers to make this paper more promising, e.g., applications to other NLP fields."
            }
        },
        "id": "2XJ5twhblG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "D0gAwtclWk",
        "replyto": "D0gAwtclWk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1213/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507498,
        "cdate": 1696707507498,
        "tmdate": 1701465423798,
        "mdate": 1701465423798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The work presents a benchmark for NLP evaluation in social media, SuperTweetEval. \n\nThe reviewers scores vary considerable from borderline/mediocre to good/strong with respect to both soundness and excitement. One major point of agreement among reviewers has to do with the lack of evaluation of latest/best-performing models to date to provide a wider picture of the challenges & limites presented by the benchmark. The authors add models such as Chat-GPT to the papter as requested in order to provide a wider picture. The authors also provide further justifications during rebuttal for the models selected. This aspect should be reinforced in the camera-ready version of the paper. The authors should clearly describe how the datasets in SuperTweetEval were selected, how are they relevant to social media NLP research, and how they compare to previously existing similar resources.\n\n\nThe points of divergence are: the size of the dataset and the lack of human baselines. The authors address this issues during rebuttal and in my opinion should not preclude publication."
            }
        },
        "id": "GmNEkz5GKF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "D0Mp7ILZME",
        "replyto": "D0Mp7ILZME",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4350/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587968,
        "cdate": 1696707587968,
        "tmdate": 1701465528691,
        "mdate": 1701465528691,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes unsupervised NMT improvement by grounding words in multiple languages to images, so that visual words representations are mixed with text embeddings. The authors show +2.81 BLEU improvement, which is comparable to using a bilingual dictionary.\n\nPros: The paper presents a simple and original method. The scale of the experiment and the data released to the community is valuable (300k images with captions, 5 languages).  Results show consistent improvements in translation quality. \nCons:  The experimentation and analysis of the approach are somehow limited and preliminary. There are many open questions that would need to be addressed, as pointed out by the reviewers.  Also, experimental results depend on the images retrieved by a specific search engine. While this is fine for the case of nouns covered by the paper, it is not clear how this solution scales to other words or terms. Finally, there are ethical concerns about the data set that needs to be addressed before its release."
            }
        },
        "id": "tNiiCMN25G",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CyDf6Q619o",
        "replyto": "CyDf6Q619o",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1773/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707526913,
        "cdate": 1696707526913,
        "tmdate": 1701465441003,
        "mdate": 1701465441003,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers tVhy and 3CNV provide a positive assessment of validity and are quite excited about the paper, whereas Reviewer ED4y gives low scores on both dimensions.\n\nSynthesizing the reviews, the following major strengths or weaknesses were mentioned:\n\nStrengths\n\n- provides a useful overview of pragmatics (R1, R3) for a community that may overlook relevant key properties of language (R2, R3), provinces guidance for relevant challenging tasks (R2)\n\nWeaknesses\n\n- remains vague on how pragmatics could actually come to play in developing LLMs (R1)\nperceived relatively narrow focus in the way linguistic interaction is framed and conceived (R2)\n\nReconciling the disparate scores given by reviewers:\nThe weakness mentioned by R1 (ED4y) – i.e., remaining vague on how pragmatics can come to play in developing LLMs – appears to be largely related to excitement rather than soundness, as it does not undermine the validity of the conclusions drawn by the paper or the arguments made in it. Thus, I believe their low soundness score (1) is not well justified by the review and can be discounted.\nOn the other hand, the three excitement scores (2,4,5), while variable, are supported as subjective assessments by the three reviewers."
            }
        },
        "id": "ZMcgoQyiuW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Cx5vVkpsOY",
        "replyto": "Cx5vVkpsOY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2081/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533950,
        "cdate": 1696707533950,
        "tmdate": 1701465453235,
        "mdate": 1701465453235,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary:** The paper proposes a new paradigm in question answering (specific to questions requiring temporal reasoning) by reformulating the QA task as a program and then utilizing the enhanced code-reasoning and understanding capabilities of LLM to solve for the solution. The question and external knowledge are first parsed into a code query, and then the LLM is prompted to match the parsed question to the external evidence/knowledge in order to answer the question. Empirical evaluation on 3 QA datasets with time-sensitive questions shows that the proposed approach beats SOTA prompting baselines (ReAct and CoT) by significant margins.\n\nOverall, all reviewers acknowledged the clear writing of the paper, and the novelty and simplicity of the proposed solution. The claims in the paper are supported well empirically with a solid evaluation performed over 3 datasets and compared to two strong baselines. Most of the concerns raised in the initial reviews were sufficiently addressed by the author response. \n\n**Recommendations for Improvement:** My suggestion would be to include the additional experiments with other LLMs (text-davinci-003, and possibly more) and elaborating the reason for excluding implicit questions (of TempQuestions and TimeQuestions datasets) in the paper to strengthen it further."
            }
        },
        "id": "i0a6D2hPXL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CuI1xfhxaJ",
        "replyto": "CuI1xfhxaJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1629/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521047,
        "cdate": 1696707521047,
        "tmdate": 1701465436523,
        "mdate": 1701465436523,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers were enthusiastic about the paper and agreed that the paper has significant technical merits that were well-justified and supported well by the experiments in the paper. The rebuttal process helped clarify the technical contributions of the work. While the new results will need to be included before the paper is published, there is strong interest in this line of work and the excitement on the proposed approaches is high."
            }
        },
        "id": "Y6gJpjtSFv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Cu4Jn4Xt22",
        "replyto": "Cu4Jn4Xt22",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5386/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612040,
        "cdate": 1696707612040,
        "tmdate": 1701465558132,
        "mdate": 1701465558132,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposed a novel architecture to enhance the joint process of multi-intent detection and slot filling. Experiments demonstrate that this approach, which is comparatively simpler that related works, achieves state-of-the-art results on both MixATIS and MixSNIPS.\n\nInitial presentation of the work left reviewers unclear as to the significance of the contribution, especially in relation to existing approaches. This was clarified through the rebuttal period. In the final draft, the authors should aim to improve the presentation of the work along the lines indicated by the detailed rebuttal discussion with one of the reviewers.|meta review"
            }
        },
        "id": "E3aTEerUwm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CsCRTvEZg1",
        "replyto": "CsCRTvEZg1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1584/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518486,
        "cdate": 1696707518486,
        "tmdate": 1701465435340,
        "mdate": 1701465435340,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a method for enhancing the recall of the candidate retrieval phase in the entity linking process. It introduces a keyword extractor based on Pre-trained Language Models (PLM) to create a more refined context and utilizes the BM25 retrieval model for obtaining candidate entities."
            }
        },
        "id": "W3xX6bV96D",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Coh1A4iSsl",
        "replyto": "Coh1A4iSsl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2926/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552294,
        "cdate": 1696707552294,
        "tmdate": 1701465481329,
        "mdate": 1701465481329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper describes a new approach to emotion recognition in context by combining a number of techniques. Reviewers were all impressed by the performance of the model, though there were multiple requests for additional comparisons/experiments to understand the performance more (e.g., through an error analysis). The approach represented a meaningful engineering contribution to ERC. However, reviewers were mixed on understanding the motivation for the approach itself and whether the individual pieces (e.g., the graph neural network) were contributing or interacting to solve the ERC task. Some of these issues were caused by the clarity in the writing, though one reviewer was able to follow completely. Overall, found the paper's methods promising but highlighted multiple ways the manuscript could be improved to strengthen the paper's clarity and subsequent impact."
            }
        },
        "id": "3C8tsfMPTd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CoEuk8SNI1",
        "replyto": "CoEuk8SNI1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5598/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615557,
        "cdate": 1696707615557,
        "tmdate": 1701465563205,
        "mdate": 1701465563205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "> Summary\n\nThis paper investigates the classifier system in Mandarin Chinese, studying whether it shows signs of having been optimised for *communicative efficiency*.\nExperimentally, they analyse pairs of nouns; studying how a pair’s frequency, semantic similarity, and co-occurrence probability (or PMI) predicts the amount of overlap in the nouns’ classifiers.\nThey find that, for low frequency nouns, more similar nouns will *more* likely share classifiers.\nSimilarly, they find that for high frequency nouns, more similar nouns will *less* likely share classifiers.\nThey take these results as sign of communicative efficiency.\n\n> Meta review\n\nOverall, the reviewers seem to find the paper’s research question interesting, and appreciate the connection this paper makes between gender and classifier systems. Further, this paper’s experiments seem well motivated. While some important parts of the paper were not clear (e.g., *what is the rationale behind the various used metrics?*, and *what exactly is the hypothesis being tested here, and what would constitute a negative result?*), I believe the author’s thorough responses answered those questions. In case this paper is accepted, I expect the authors will try to clear these sources of confusion in their manuscript as well.\n\nThe paper has a few drawbacks, however. In terms of scope, one of the reviewers suggested that the paper could be significantly strengthened by exploring both \"count\" and \"mass\" classifiers separately, and by comparing how these systems behave. Similarly, another review suggested that expanding these analyses to both adjectives and gender systems could provide a more comprehensive view of their role in communicative efficiency, especially since the authors argue for a similarity in the role of classifier and gender systems in natural languages.\n\nExperimentally, one reviewer suggests that the used similarity metric here will contain information about the words' classifiers. The authors dismiss this concern by saying that only 22%/7% of the studied nouns’ instances occur with a classifier. Personally, I interpret these numbers the other way around, and believe that this is a large percentage of instances which could seriously impact the paper’s results. On the other hand, unless there is a systematic bias in how frequently nouns appear with classifiers (e.g., more frequent nouns appear with classifiers less often), I’d expect results to not change significantly since this bias will be present in most analysed nouns. In any case, training word2vec/fasttext is not particularly expensive, and I agree with reviewer igeR that running an experiment with novel embeddings trained on the corpus with classifiers erased would make these results more reliable.\n\nOn a related note, FastText uses subword information (I am not particularly familiar with how it processes Mandarin Chinese characters though). This means that your semantic similarity metric is likely to also encode wordform similarity as well. While this is not in itself an issue (as similar communicative efficiency arguments might be made for an \"efficient\" natural language to be optimised to distinguish wordform-similar words), it may change how these results should be interpreted. Rerunning experiments with, e.g., word2vec, could be interesting to analyse the extent to which that could have affected results."
            }
        },
        "id": "lSHmyXjK3q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CnLpDkgnCn",
        "replyto": "CnLpDkgnCn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4914/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601484,
        "cdate": 1696707601484,
        "tmdate": 1701465544659,
        "mdate": 1701465544659,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new method that predicts the edit operations required to simplify a text for a specific grade level on an instance-per-instance basis, which improves the quality of the simplified outputs over existing methods.\nThere are concerns that the proposed method has only been verified on one dataset and there is no human evaluation."
            }
        },
        "id": "oyyrW7l2RN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Cn3HNSzh14",
        "replyto": "Cn3HNSzh14",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1633/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521394,
        "cdate": 1696707521394,
        "tmdate": 1701465436806,
        "mdate": 1701465436806,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors conduct probing on pre-trained models for document-level event extraction, covering different neural layers and different elements of structured events. Several fine-grained conclusions are drawn. This is the first attempt to conduct probing on the document level task, and novel probes that require engineering work have been designed. On the negative side, there has been some criticism on the results and the insight for guiding further research."
            }
        },
        "id": "F5gxyw8MYz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CluDBdRhUp",
        "replyto": "CluDBdRhUp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5300/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610563,
        "cdate": 1696707610563,
        "tmdate": 1701465555485,
        "mdate": 1701465555485,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper looks at the problem of transfer learning for offensive language detection. While previous work has looked at language dimensions for finding an optimal source language, this work proposes looking at cultural differences. This paper tackles an important and timely problem with an interesting approach, so I believe this will contribute positively to the community.\n\nThere are a few improvements suggested by the reviewers. For one, clarity of writing could be improved. Other details are in the reviews, so please refer to those when making your revisions."
            }
        },
        "id": "CeIQ3XUMWW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CkvfJdb7mw",
        "replyto": "CkvfJdb7mw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1725/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707525400,
        "cdate": 1696707525400,
        "tmdate": 1701465439578,
        "mdate": 1701465439578,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a novel approach to sign language segmentation by replacing the conventional IO tagging with BIO tagging. The authors also propose a method using optical flow and 3D hand normalization, resulting in improved segmentation quality and robustness across different signed languages and domains.\nThe proposed BIO tagging scheme and 3D hand normalization are discussed as potential improvements for sign language segmentation, but there are concerns about the experimental results and the significance of these contributions. The paper is well-written and easy to follow. While the use of BIO tagging is not considered highly original, the introduction of 3D hand normalization is seen as more novel. However, there are questions about the effectiveness of these contributions. While there is potential for the proposed method to be used in sign language segmentation, concerns are raised about the experimental results, the lack of a clear superiority over the baseline, and the applicability of the method to other tasks.\n\nPros:\n- The paper is well-written and easy to follow.\n- The use of 3D hand normalization is seen as a potentially original and valuable contribution.\n- The paper addresses potential questions and limitations effectively.\n- The proposed model may have applications in data collection for sign language in real-world settings.\n\nCons:\n- Concerns are raised about the experimental results, including the lack of a clear superiority over the baseline and the drop in performance with the introduction of certain features, which the authors have addressed.\n- The applicability of the proposed method to tasks outside of sign language segmentation is questioned but the authors have addressed."
            }
        },
        "id": "Ur4uFP8Lwy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ck3JPqoEeE",
        "replyto": "Ck3JPqoEeE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2720/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548086,
        "cdate": 1696707548086,
        "tmdate": 1701465474362,
        "mdate": 1701465474362,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a reweighting-based active learning strategy for sequence tagging, with a focus on named entity recognition (NER) tasks. While the paper exhibits strengths, there are important aspects that need attention and further clarification. Below I summarize the most relevant aspects to be considered:\nReasons to Accept:\n* \t\tClarity and Readability: The paper is well-written, logically structured, and easily understandable. The presentation of the proposed method is clear, contributing to the overall comprehensibility of the research.\n* \t\tBroad Applicability: The method presented in the paper is straightforward and exhibits potential for broad applicability. Its compatibility with various token-level acquisition functions is a valuable feature, demonstrating versatility.\n* \t\tInnovative Active Learning Strategy: The paper introduces a novel active learning strategy for NER that effectively addresses the challenge of data imbalance. The proposed dynamic smoothed weights for tokens represent an adaptable approach that holds promise for improving sequence tagging tasks.\n* \t\tEmpirical Validation: The paper substantiates its claims through experimental results on multiple datasets, showcasing performance enhancements in the context of NER tasks. This empirical validation enhances the credibility of the proposed approach.\nReasons to Reject:\n* \t\tLimited Hyperparameter Analysis: The absence of a thorough study on hyperparameters, specifically the importance of Beta in the re-weighting method, is a notable limitation. It is essential to explore the sensitivity of the method to hyperparameter variations and provide insights into the optimal choices for different scenarios.\n* \t\tLack of Hyperparameter Optimization: The paper could benefit from experimental results and discussions related to the hyperparameter Beta. Automatic optimization or at least a systematic exploration of this hyperparameter's impact on performance would strengthen the method's practical utility.\n* \t\tStatic Nature of Beta: The authors acknowledge the static nature of the Beta hyperparameter, which may pose limitations when applying the algorithm to new or diverse corpora. Further investigation into the adaptability of Beta and its sensitivity to dataset characteristics, such as the number of classes, is essential.\n* \t\tAdditional Experiment Details: While the paper presents experimental results demonstrating overall performance enhancement, it could be further strengthened by providing more detailed metrics, including the distribution of each NER type and precision, recall, and F1 scores for each type throughout the active learning process. Such details would offer a deeper understanding of the method's effectiveness.\nIn conclusion, while the paper introduces an active learning strategy for NER with several strengths, including clarity, applicability, and empirical validation, there are areas that require attention. These areas were discussed during the rebuttal period and there was convergence on the opinions. Thus, more specifically, addressing hyperparameter sensitivity, exploring automatic hyperparameter optimization, and providing additional experiment details would enhance the paper's overall quality and practicality."
            }
        },
        "id": "I4jqsSEU6u",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CihCvXPiEG",
        "replyto": "CihCvXPiEG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3025/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554586,
        "cdate": 1696707554586,
        "tmdate": 1701465484836,
        "mdate": 1701465484836,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors proposed a method to detect the nationality bias present in a pre-trained Language Model. They fine-tuned LM on a sentiment corpus that is designed based on specific templates. Finally, they showed the bias in LMs while testing on a corpus containing nationality information. \n\nOverall, reviewers agree that the work is sound, and contains interesting findings on bias in language models, though the application scope is limited (Reviewer qjcz and TUSA)."
            }
        },
        "id": "oecfFYH47d",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Cib0JSAVwW",
        "replyto": "Cib0JSAVwW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3868/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578181,
        "cdate": 1696707578181,
        "tmdate": 1701465512389,
        "mdate": 1701465512389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Summary:**\nThe paper introduces a benchmarking approach to assess the impact of predictions generated by knowledge graph embeddings on question answering. The benchmark dataset comprises 3 million triples and 5,000 questions. The experiments aim to evaluate the robustness of state-of-the-art knowledge graph completion (KGC) methods and their influence on knowledge graph question answering (KGQA) techniques.\nThe authors provide insights into the construction of their benchmark, emphasizing the use of embedding methods for knowledge graph completion. The goal is to highlight the relevance of the KGC problem by demonstrating its impact on question answering performance, which is a significant application of knowledge graphs. The contributions of this work include NLP engineering experiments and a data resource derived from existing artifacts such as GrailQA and Freebase.\nThe evaluation results suggest that increasing the incompleteness of the knowledge graph leads to worse performance. While the addition of correct triples tends to improve results, there are cases where the inclusion of predicted triples actually worsens performance.\n\n**Strengths:**\nA notable strength of this paper lies in its approach to bridging a well-established academic task, knowledge graph completion, with a longstanding practical NLP task, namely, the impact of knowledge graph embeddings on question answering. \nThe authors' insight into connecting a task with its potential benefits for other tasks underscores the significance of their proposal within this domain. \nFurthermore, the authors build upon a highly regarded benchmark, GrailQA, which encompasses questions from various domains, including IID, compositional, and zero-shot, lending empirical robustness to their findings, as GrailQA is widely recognized as a high-quality benchmark. Additionally, the authors carefully select a representative set of popular knowledge graph completion (KGC) methods and techniques, including Pangu and DecAF.\n\n**Weaknesses:**\nReviewers have identified the following weaknesses in this work:\n1. The concept of \"incompleteness\" in the knowledge graph completion (KGC) task appears somewhat artificial. The authors create this sense of incompleteness by obtaining relations of answer entities in the dev/test set and then obscuring one of the entities in the relation. Subsequently, they randomly select a proportion (P) of these triplets as the final validation and test sets. This approach might inadvertently lead readers to conclude that a specific KGC model has good performance in KGC but performs poorly in QA, when in reality, the observed KGC model strength on this benchmark may be an artifact of the benchmark construction.\n2. While there is a correlation between KGC and knowledge graph question answering (KGQA), this correlation remains largely unexplored. The paper lacks a thorough analysis that would provide insights into why superior KGC performance does not necessarily translate to better KGQA performance. This analysis could encompass quantitative factors, such as entity types and relations more prone to this phenomenon, as well as qualitative aspects, like questions that remain challenging to answer even if the correct triples are predicted by KGC.\n3. The paper could benefit from the inclusion of more popular and representative techniques, particularly those related to semantic parsing. The absence of a semantic parsing baseline is a concern for several reasons, including the widespread use of semantic parsing in KGQA and its suitability for assessing KGC. Incorporating such approaches would enable the authors to measure the impact of KGC on generated parse success rates.\n4. The selection of DecAF and Pangu as techniques for evaluation should be substantiated further. The authors should provide more justification beyond simply stating that these are two state-of-the-art techniques. Additionally, while the limitation section mentions the absence of an LLM-only baseline, the paper does not incorporate semantic parsing, which is a notable omission.\n5. The overall experimental setting, particularly the choice of incompletion ratios at 20%, 50%, and 80%, might not align with a realistic KG system with such an extremely high incompletion ratio. Further context or justification for these ratios would enhance the paper's credibility.\n\n**Author-Reviewer discussion and acknowledgment:**\nThe authors have not provided any rebuttal comments.\n\n**Conclusion:**\nThis paper is well-written and easy to follow. The contribution is interesting, addressing a task that has been overlooked in the literature. However, reviewers recommend that the authors incorporate more state-of-the-art research and consider additional improvements. It is suggested that the work should include additional data based on the points highlighted within the reviews. Additionally, the authors should address the identified typos in the paper."
            }
        },
        "id": "PwqDBewz3y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CgAfbI4kGS",
        "replyto": "CgAfbI4kGS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3539/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564844,
        "cdate": 1696707564844,
        "tmdate": 1701465500743,
        "mdate": 1701465500743,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Reviewer avEK gave some very detailed critical feedback on the framing of the paper: it only looks at a subset of types of implicit reasoning, and the connection to the implicit reasoning that humans do is a bit tenuous. The authors and this reviewer had a detailed and productive discussion, and I'm confident that the authors will incorporate some of these framing suggestions in the camera ready version of the paper. \n\nConcerns about the generality of the dataset were also raised by Reviewer Gmcn -- \"same\"/\"different\" problems are only one type of REC, and CLEVR does not use real photographs. The author response helped address these, illustrating how to apply the TIE method on Visual Genome, and making a case that findings on the dataset, and the TIE method, are also applicable to other types of REC.\n\nAll together, I'm convinced that this work is sound. In terms of excitement and likely impact, it seems that this dataset, and the proposed method, will probably be a useful component in measuring abilities on one type of implicit reasoning."
            }
        },
        "id": "7neQpDzWxt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CfJiBuysQQ",
        "replyto": "CfJiBuysQQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3334/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560716,
        "cdate": 1696707560716,
        "tmdate": 1701465494298,
        "mdate": 1701465494298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper describes a new approach to multi-criteria Chinese word segmentation (MCCWS) that improves context awareness by incorporating a secondary objective of learning sentence representations. The authors report that their approach achieves state of the art F1 and OOV recall on many of the available CWS benchmark datasets. \n\nThe reviewers agree that the proposed approach is well motivated, simple, and mostly sound, and they note that the set of experiments carried out is comprehensive and thorough. The reviewers were less enthusiastic, however, about the overall contribution of the work. In particular, they note that the reported improvements over the prior s.o.t.a. are minimal, and the proposed approach fails to yield improvements for some of the datasets. One reviewer did raise their soundness score after the rebuttal in response to additional results provided by the authors, but they remained ambivalent about the significance of the work."
            }
        },
        "id": "CrNaWVfyr6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CdcdyN4cvL",
        "replyto": "CdcdyN4cvL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission378/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486027,
        "cdate": 1696707486027,
        "tmdate": 1701465396819,
        "mdate": 1701465396819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree on the main reasons to accept:\n\n* Novel analysis approach that compares across all language pairs at the same time.\n* Large-scale empirical study.\n* Insights into the relationship between morphosyntactic typology and downstream task performance.\n\nHowever, they are mixed on excitement, due perhaps in part to a perceived lack of clarity in the paper's technical details."
            }
        },
        "id": "BBnEKRG1yC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Cc5yhA1PrC",
        "replyto": "Cc5yhA1PrC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1162/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506305,
        "cdate": 1696707506305,
        "tmdate": 1701465422288,
        "mdate": 1701465422288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work studies the \"stability\" of LIME-based word importance scores. It consists in making small perturbations to the input, so as to preserve its meaning, but changing the explanation in terms of the ranking of the most important words. The perturbation technique introduced by the authors is named XAIFooler and is compared to various baseline methods, demonstrating its superiority as an adversarial attack method on LIME, and more broadly, this result highlights the \"instability\" or vulnerability of the LIME explanation method.\n\nA strength of the work is that the authors use various metrics both to quantify the preservation of meaning during perturbation, as well as to quantify the resulting explanation dissimilarity. Thereby the authors make the point that among explanation similarity metrics the Rank-biased Overlap (RBO) is the most suited. Overall the evaluation is thorough, and in addition the paper is well-written.\n\nMore generally, the present work is complementary to a previous work also studying the vulnerability of LIME explanations to small perturbations in the input (Sinha et al. 2021, BlackboxNLP Workshop). Compared to this previous work, the authors additionally: 1) investigate the \"inherent\" instability of LIME (i.e. the impact of the LIME sampling rate on the LIME explanation similarity), 2) provide some analysis and comparisons to justify the choice of a similarity metric for explanations, and finally 3) allow only perturbation of less important words to demonstrate the instability of LIME even when performing only subtle changes in the input.\n\nNevertheless it remains an open question for future work whether the discovered explanation instabilities are specific to the LIME explanation method, or if such instabilities arise from the LIME's explanation setup which could also apply to other explanation methods as well, i.e. having only black-box access to the model and performing only a limited number of queries onto the model (through the sampling rate)."
            }
        },
        "id": "6RDmyu75tx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CblASBV3d4",
        "replyto": "CblASBV3d4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2038/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532794,
        "cdate": 1696707532794,
        "tmdate": 1701465451437,
        "mdate": 1701465451437,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the controlled text reduction task. The reviewers find the paper to be well-motivated, and attempt a combination of reasonable strategies. The paper also seems to benefit from good writing. However, the reviewers find the novelty of the work to be extremely limited with most of the techniques borrowed from the literature. The experiment section needs more work, with thorough human evaluation and clearly identifying the source of improvement. Given the importance of the task, this paper could establish baselines for the text reduction task in the future."
            }
        },
        "id": "68LmV1Rivs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CQgmBmRBMb",
        "replyto": "CQgmBmRBMb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3726/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707569014,
        "cdate": 1696707569014,
        "tmdate": 1701465507812,
        "mdate": 1701465507812,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel framework for extracting quantities from unstructured text using dependency parsing and a unit dictionary. It also presents a new benchmark dataset for evaluating quantity extraction methods. Its soundness and excitement are generally well-supported by reviewers because of the simplicity and insight provided by this work. Two major areas of improvements are brought by one of our reviewers. (1) The definitions of \"concept\" and \"change\" are important for researchers and annotators, which are not articulated very clearly in the manuscript; (2) the use of post-processing to convert output from existing works may lead to unfair comparisons so it's suggested that the authors include a thorough explanation of the post-processing steps."
            }
        },
        "id": "OXWfT8wkHZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CPBEn5mGle",
        "replyto": "CPBEn5mGle",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission64/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478213,
        "cdate": 1696707478213,
        "tmdate": 1701465385904,
        "mdate": 1701465385904,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel method aimed at compressing the memory consumption of Key-Value (KV) cache and reducing the computation time of attention in decoder Transformer models. The approach leverages the use of two sentinel tokens, <CL> and <CR>, which encapsulate spans of tokens to compress. By subsequently fine-tuning the model, it compresses the information between these two tokens into one, enabling memory savings. \n\nIn general, the reviewers were in consensus about the novelty of the and were generally positive on the empirical results compared to prior work.\n\nThere were some methodological concerns related to how the improvement of the approach compared to existing work in practical terms. For instance, while KV Compression was compared against Local Attention in terms of compression ratio and performance metrics (e.g., perplexity), they were not compared in terms of throughput. Instead, throughput is provided only for the proposed method.\n\nAdditionally, one reviewer felt that the datasets and models evaluated in the initial submission were limited. The authors provided additional results that satisfied the reviewer."
            }
        },
        "id": "UvtXyXOHuK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CP1PLnFzbr",
        "replyto": "CP1PLnFzbr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1375/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511470,
        "cdate": 1696707511470,
        "tmdate": 1701465428903,
        "mdate": 1701465428903,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper provides a general formalism for several Information Extraction tasks, ranging from binary relation extraction to MUC4 template extraction, utilizing similarity functions, alignment, and normalization functions.\n\nOverall, the paper is well-written and offers interesting generalizations across various evaluation measures in Information Extraction. It is also the first theoretically motivated framework in this field."
            }
        },
        "id": "ErEOdn0wV2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CO40wNIY5i",
        "replyto": "CO40wNIY5i",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission935/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707499727,
        "cdate": 1696707499727,
        "tmdate": 1701465415354,
        "mdate": 1701465415354,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes an autoregressive method for Non-convergent discourse parsing. The approach and method are interesting, the paper's clarity is notable, and experimentation is on topic and shows promising results. As noted by the reviewers, the main issue is that the method could be evaluated in a more general setting (not just Non-convergent discourse). Therefore, it might have a more significant impact or be described in a larger context than the current version. However, the current version could be published and be a starting point to study the potential and limitations of the proposed approach.\n\nPros\n* Interesting problem and method\n* Promising results\n* It is a self-contained manuscript\n\nCons\n* Topic very specific but applicability to a larger scope\n* Small dataset"
            }
        },
        "id": "SMC1GIsGT2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CMm4w1A4Yd",
        "replyto": "CMm4w1A4Yd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3639/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566771,
        "cdate": 1696707566771,
        "tmdate": 1701465505053,
        "mdate": 1701465505053,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a model that translates unstructured natural language into structured goals/constraints allowing the “intent” or meaning of natural language to be converted into logical representations tied to the underlying world state. Their pipeline shows how such data can be collected and then augmented allowing better training of a model on such data. They demonstrate improved performance of this explicit goal/constraint decoder over a baseline that predicts outputs sequentially with in-context learning as well as a human baseline and show improved performance over both. Most reviewers agree that this is an interesting paper and the dataset/pipeline creation and improved performance of the model are good contributions to the language/RL community. I urge the authors to consider the suggestions of better visualisation/examples in the paper and quality checks of the datasets, but even it's current state the methodology presented here is a valuable contribution to future work in language/RL/game environments."
            }
        },
        "id": "fMzDvEY0uv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CLq5tqZ5SK",
        "replyto": "CLq5tqZ5SK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2139/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535363,
        "cdate": 1696707535363,
        "tmdate": 1701465455447,
        "mdate": 1701465455447,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a method for performing distantly supervised relation extraction based on Huffman tree multi-instance learning and circular cosine similarity, which considers intrinsic associations between sentences using BERT as an encoding, and demonstrates good experimental results. The reviewers agree that the proposed method is novel and the presented results are solid. They are mainly concerned with a need for further analysis and experimentation in order to strengthen the paper. Some further results and details are presented by the authors in their response, including the use of different sentence encoders (SPANBERT and PCNN), a description of the time complexity of their method, and another baseline comparison. The paper appears to be mostly well executed as the results are robust, which the provided new experiments will supplement."
            }
        },
        "id": "fJbR4Kh3N0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CLVOAHdybT",
        "replyto": "CLVOAHdybT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1358/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511022,
        "cdate": 1696707511022,
        "tmdate": 1701465428305,
        "mdate": 1701465428305,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "We thank the authors for their submission. \n\nThis paper introduces a new Disfluency Correction (DC) dataset in English, French, German, and Hindi. It then presents many methods (RNN, Transformers, etc.)\n\nReasons to Accept: \n- An practically valuable dataset for a potentially very useful under-studied but useful task (as pointed out by R1)\n- Large range of baseline scores reported on the dataset (as pointed out by R1)\n- The dataset is further showcasing the importance of DC for Machine Translation\n\nReasons to Reject:\n- A single annotator per language: The paper would benefit from annotator agreement reported at least on a subsample of the data. \n- Better acknowledgment and discussion of related work. \n\nSuggestions: \nEvaluating the performance of rule-based predictions on DISCO would help us understand how challenging the dataset and task is (e.g., removing repetition with simple pattern matching.)\n\nTypos Grammar Style And Presentation Improvements:\n- Most tables do not follow the EMNLP template"
            }
        },
        "id": "fNPCygR4Vu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CK9mApdZFW",
        "replyto": "CK9mApdZFW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4106/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583295,
        "cdate": 1696707583295,
        "tmdate": 1701465520539,
        "mdate": 1701465520539,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a dynamic capacity approach for MoE models to mitigate parameter inefficiency. The reviews expressed concerns about (i) the necessity of applying the approach to tasks beyond MT, (ii) the marginal improvements in MT, (iii) the time requirements and computational efficiency of the approach. The rebuttal has (partly) addressed the concerns. Stronger results on MT and applying the approach n tasks beyond MT certainly strengthen the paper in future."
            }
        },
        "id": "HTWCx0KEOR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CHffPbQXjX",
        "replyto": "CHffPbQXjX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2454/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542552,
        "cdate": 1696707542552,
        "tmdate": 1701465465717,
        "mdate": 1701465465717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "While reviewers pointed out the size, naturalness of the dataset, simplification of the task, and minor ablation study, it seems the work verified itself with the importance of the task, well-designed experiment, original approach, and significant findings. The paper is well written and the evaluation metric is also reasonable to convince readers.\n\nHowever, it still remains an ethical concern and would require additional evidence as suggested by ethics chairs."
            }
        },
        "id": "zXhnputgOz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "CEPkRTOlut",
        "replyto": "CEPkRTOlut",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5641/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616290,
        "cdate": 1696707616290,
        "tmdate": 1701465564465,
        "mdate": 1701465564465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This manuscript examines how deeply NLP research engages with other fields as approximated by citations outside of NLP venues. The manuscript provides in-depth analysis of some of their findings. \n\nIn general, reviewers find many reasons to accept this manuscript and only very few reasons not to, some of which are an excellent basis for further work investigating this. Indeed, the limitation section covers many of the limitations identified by the reviewers. \nThe most compelling reason for rejecting the manuscript is that certain analyses sections are primarily engaged with describing the figures rather than discussing the results themselves. However, this seems very easily addressable by the CR deadline.\n\nMoreover, it's unclear to me why the authors rely on classification, when publication venues (e.g., ACM Transactions on Asian and Low-Resource Language Information Processing or GeoJournal) often explicitly name the field in the name. This information should be available from both citations and the publication venue of different papers. Using a classifier seems like an unnecessary way to introduce error into the analysis."
            }
        },
        "id": "AUbdX7x9o4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "C68cYdgLUs",
        "replyto": "C68cYdgLUs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3725/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707569641,
        "cdate": 1696707569641,
        "tmdate": 1701465507741,
        "mdate": 1701465507741,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper shows the failures of Kendall’s tau in presence of ties when evaluation MT evaluation metrics scores and proposes a remedy. The proposed metric introduces predictions of ties and rewards them. With the increase of quality, ties become more frequent. Also ties in WMT have become more reliable, and frequent with the MQM HE scores (#of errors). Some variants of Tau discard ties loosing important information. The analysis of the calibration mechanism for regression-based metrics is very informative, the calibration method does not generalize across dissimilar data sets. \nReasons to accept: paper  discovers and proposes a fix to relevant problem in MT meta-evaluation. The paper is well written and could have a major impact in the community leading to fairer comparisons across metrics. \nReasons to reject: Limited language pairs are analyzed but they are very diverse. Evaluation limited to MQM HE framework, which is understandable."
            }
        },
        "id": "BsDUzTeM62",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BxY99WBKSV",
        "replyto": "BxY99WBKSV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4728/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597227,
        "cdate": 1696707597227,
        "tmdate": 1701465539865,
        "mdate": 1701465539865,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents SODA, a framework for generating social chit-chat dialogue data using pre-trained LLMs (GPT3.5 in this work). This framework is used to produce a 1.5M dialogue dataset and train a T5-based LLM (COSMO) which is evaluated. Both the code and model will be released as open-source. All reviewers appreciated the need for such a framework and the fact that the authors are willing to open-source both data and code. They also liked the fact that the authors used commonsense triplets for dialogue generation. Most concerns centered around cost and bias, given that GPT3.5 was used for data generation, and one reviewer pointed out lack of details and evaluations. All concerns, however, were properly addressed by the authors and I am happy to accept this paper."
            }
        },
        "id": "mgJeLbtX1O",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BscCXmZopv",
        "replyto": "BscCXmZopv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1396/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512123,
        "cdate": 1696707512123,
        "tmdate": 1701465429603,
        "mdate": 1701465429603,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper approaches the problem of entity typing and proposes a method that leverages entity-type, entity-cluster, and cluster-type information. Reviewers praised the papers clarity, thoroughness of experiments as well as the design and performance of the method. The main concerns for the work were whether the additional complexity of the method was justified by the amount of performance increase tradeoffs and if the methods were sufficiently different from prior approaches."
            }
        },
        "id": "S5TvR7pCAk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BrqDTTga8J",
        "replyto": "BrqDTTga8J",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1843/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528604,
        "cdate": 1696707528604,
        "tmdate": 1701465444241,
        "mdate": 1701465444241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper explores the relation between language complexity when expressing emotions, and the performance of transformer-based models on emotion classificaition. The experimental setul and analysis are sound as highlighted by all reviewers. Reviewers found the paper moderately exciting (i.e. interesting research question grounded on linguistic theory)."
            }
        },
        "id": "Dhlu8T7PY5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BpsWrnfIIn",
        "replyto": "BpsWrnfIIn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3789/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707575253,
        "cdate": 1696707575253,
        "tmdate": 1701465509775,
        "mdate": 1701465509775,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper aims to probe the creativity of LLMs, operationalized in terms of divergent semantic association.\nThe reviewers agree that this is a timely topic, and provided a relatively positive assessment.\n\nThe scores were relatively consistent, in the medium to high range:  3,4,3 for both soundness and excitement. Some of the concerns raised in the original reviews were addressed in the discussion period, leading to increases in the scores. \n\nDistilling the reviewers, the following major strengths and weaknesses were noted.\n\nStrengths:\n\n- The paper was judged as studying an interesting and timely question, in an interesting and intuitive way.\n\nWeaknesses:\n\n- paper specifically considered single nouns (R1)\n- R2 (hFka) originally considered the paper insufficient, but was convinced by additions made during the discussion period.\n- R3 originally found various unaddressed assumptions (validating DAT task for LLMs, concerns about cosine distance, focusing on GLoVe). After the author response, they found some of their concerns to have been addressed.\n\nIn conclusion, this paper was judged to be good or strong in terms of soundness, and scored \"ambivalent\" or even \"strong\" in terms of excitement."
            }
        },
        "id": "P5T8O4imM4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BpibUh0aB3",
        "replyto": "BpibUh0aB3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission506/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489473,
        "cdate": 1696707489473,
        "tmdate": 1701465401531,
        "mdate": 1701465401531,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new method that consists in using Word Sense Disambiguation along with Knowledge Graphs during pretraining to address the translation of polysemous words.\nThe problem is highly relevant for the community and the method precisely address weaknesses of current methods.\nThe paper is clear and well written, the experimentations are well conducted and the results demonstrate the effectiveness of the proposed method, even if some additional analysis would sometimes be beneficial. The authors mentioned in their rebuttal that most of the missing analysis will be added in the camera ready version.\nThe main concern is the usability of the approach for very low and very high resource languages, but this is not a criteria to reject the paper per se."
            }
        },
        "id": "SJw6hgYOFp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Bou2YHsRvG",
        "replyto": "Bou2YHsRvG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1966/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530989,
        "cdate": 1696707530989,
        "tmdate": 1701465448577,
        "mdate": 1701465448577,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper frames the Sequence Labeling task as a conditional generation challenge, utilizing diffusion modeling. In addressing the issue of discreteness, the paper presents a solution involving bit-format inputs using the Bit-Tag Converter and a Bit Diffusion Transformer. It omits a pertinent citation, namely \"Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning.\" A more comprehensive discourse and experimental evaluation comparing the proposed approach with prior systems, including the absent reference and DiffusionNER, is warranted"
            }
        },
        "id": "dsk8j2kfhW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BoKg2pcF0H",
        "replyto": "BoKg2pcF0H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission711/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494293,
        "cdate": 1696707494293,
        "tmdate": 1701465408095,
        "mdate": 1701465408095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a new dataset for multi-event inference on complex long sentences. This is an addition to the line of common-sense inference resources such as COMET and ATOMIC. The key contributions include the resource itself, a demonstration of the utility of the resource on the proposed task.\n\nThe main problem of wanting to address reasoning over complex sentences and the gap with respect to an existing resource like COMET is well motivated. The solution methodology of obtaining a manually curated resource is reasonable. The resulting resource is likely valuable. The paper is well written overall with the caveat of the lack of some details and motivations for the design choices, as identified by the reviewers. The originality of the work lies in stating and tackling a problem that hasn't been addressed yet. The methodology is sound overall and the resulting resource is of high quality. The scope and scale of the resource however are not as big as that of prior resources on similar problems. This is hard task to create resources for and the authors make a reasonable empirical case for the utility of the resource."
            }
        },
        "id": "3XOE3Ey0PC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Beho3ly3qx",
        "replyto": "Beho3ly3qx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1122/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505403,
        "cdate": 1696707505403,
        "tmdate": 1701465421085,
        "mdate": 1701465421085,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a dataset for event extraction from a conversational email threads. The dataset could be a valuable contribution to the research community. The choice of event types is very narrow and in line with a very old literature. The intuition or criterion of the schema used is not well motivated and prompting should be described in a more comprehensible manner."
            }
        },
        "id": "QHaKMqEyBU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BdpoEj33DZ",
        "replyto": "BdpoEj33DZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2373/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540754,
        "cdate": 1696707540754,
        "tmdate": 1701465463206,
        "mdate": 1701465463206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes the use of an alternative evaluation metric for document-level information extraction, instead of relying on the existing method of template filling, which depends on arbitrary decisions about how to distinguish distinct events. Such distinction is challenging for humans and can also pose difficulties for models, as it is often motivated by subjective and unreliable factors, as indicated by an annotation studies. The alternative solution proposed is cast the problem as a question-answering task.\n\nThe paper is well-written, offering insightful observations about a neglected issue that can stimulate discussions. Furthermore, the methodology is thorough, and the approach of framing it as a question-answering problem represents a novel direction. The evaluation is conducted on the MUC-4 and BETTER datasets."
            }
        },
        "id": "2jTFR6GFtz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BcYvkVgkZy",
        "replyto": "BcYvkVgkZy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1033/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503370,
        "cdate": 1696707503370,
        "tmdate": 1701465418491,
        "mdate": 1701465418491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This data paper describes a new gold dataset for biomedical NER and EE focused on scientific literature about animal experiments. The dataset  fills a gap in biomedical NLP.  The corpus is annotated for events and entities, according to a schema specifically designed consulting domain experts. The paper offers a description of the corpus creation process, proposes a new annotation schema,  and reports rich descriptive statistics. The two-stage annotation process followed can be robust enough to guarantee quality, provided the annotation guidelines are made clear. IAA is reported for both entities and events. Various existing NER and EE models are also evaluated with the new dataset, and the results support the efficacy of the dataset for NER and EE.  \nBoth reviewers and authors engaged seriously in reviewing and discussion periods. The authors' responses were accurate and detailed; they acknowledged some of the indicated shortcomings, and provided clarifications and details on many of the points raised. Reviewers generally appreciated the work, but found some serious weaknesses in the original paper.  Particularly in need for attention are the following: \n- an explanation of the differences in inter-annotator  agreement between entities and events is needed. Given the scores, a discussion seem to be beneficial as to whether and how far disagreement is intrinsic to the task or may depend on the annotation guidelines;\n- having just two annotators questions the overall validity. This choice should be well-motivated; \n- the complete annotation guidelines were made available only during the discussion phase, and still seem weak in some parts;\n- the paper is well-written, but the framing of is not fully convincing, for instance “related works” could be more focused on the specific task addressed in the paper;\n- the work would benefit from a comparison with performance on different datasets in the experimental part."
            }
        },
        "id": "8uziZp7wUO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BacLV3QUi8",
        "replyto": "BacLV3QUi8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5866/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707620155,
        "cdate": 1696707620155,
        "tmdate": 1701465568650,
        "mdate": 1701465568650,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "As the AC for the paper, I commend the authors and reviewers for an extremely engaged discussion! Thank you to the authors for raising discussion points about quality reviewing practices, and to the reviewers for being receptive and open to these points. My metareview space is limited but I will summarize to the best of my ability.\n\nIn general, reviewers found this work to sufficiently support its major claims, although additional support was needed for some minor points. Reviewers found the paper to have strong merits (e.g., an innovative direction) but also key weaknesses (e.g., concerns regarding the evaluation) such that it could benefit from another round of revisions.\n\n**Summary of Reviewer Feedback and Discussion:**\n- **Reviewer P9mH** liked the framework provided by the authors, felt that the evaluation metrics and experiments were well-defined and thorough, and thought that the data could be a useful contribution to the community. However, they felt that the paper did not adequately distinguish itself from work relevant to code design for electronic circuits, and that an analysis of time savings from using LLMs could benefit the paper. They felt that the utility of the Micro25 dataset may be limited, requested details regarding the use of LLMs, and noted that reproducibility may be challenging since evaluating the work requires human expertise, in addition to asking clarifying questions. The authors responded that they did not discuss code generation applied to electronic circuits because their work is among the first in this subfield, and that their reliance on human evaluation is a strength. They noted that they are committed to releasing code, benchmarks, library, and experimental results, and that analyzing time savings is beyond the scope of their paper although an interesting direction for future work. They also provided detailed clarifying responses to the other questions.  In their response, Reviewer P9mH thanked the reviewers and clarified that it is important to include relevant papers to the extent possible to help situate the work in the broader research context.\n- **Reviewer bhLT** thought the work was novel and appreciated the strong empirical results and new benchmarks. They noted that the inclusion of case studies validated the paper's claims and provided insights into practical applications, and they felt that the analysis was comprehensive and the work promised broad benefits to the NLP community. However, they felt that potential for end-to-end automation and scalability may be limited, were concerned by the lack of comparison with other potential approaches, and wished the paper had explored why LLMs were superior or necessary for this task. They also raised some clarifying questions. The authors responded that the design of high-level descriptions does not require domain expertise and the devices used in the paper reflect devices throughout the field, and that all coding assistants require human intervention. They explained that they did not compare to other approaches since their task was novel. Reviewer bhLT thanekd their authors and asked several follow-up questions, which the authors answered.\n- **Reviewer 8oFB** liked that the paper presented benchmarks on a novel task, but felt that the high performance benchmarks may suggest that the task itself was trivial or that the data was not representative of the task's true difficulty. They felt that evaluation details were unclear and requested more information about the human expert, and they felt that the pass@k metric was inappropriate and that the merits for the case study were unclear, along with asking some clarifying questions. The authors disagreed that strong performance suggested task triviality, and noted that experimental and evaluation details were provided throughout the paper and appendix. They provided additional details regarding the human expert, and answered the clarifying questions.  Reviewer 8oFb thanked the authors and elaborated on their concerns, and the authors addressed these concerns.\n- **Reviewer zUFP** appreciated the evaluation in a domain unfamiliar to many NLP researchers, and felt that the experimental setups were nicely selected. However, they found the evaluation to be superficial and had difficulty distinguishing the work from work towards automatic code generation. They also asked some clarifying questions. The authors responded that they used the standard evaluation metric for related tasks and that they included an error analysis in the appendix. They clarified the differences between their task and code generation tasks, and responded to the other questions. Reviewer zUFP noted that while the evaluation followed standard practices, they do not agree that it followed best practices, and noted that it would be nice to ultimately see what the task and results tell them about the language model, with NLP-focused insights (e.g., how the results relate to NLP tasks or language model capability)."
            }
        },
        "id": "37VlYXj4ij",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BYxHeGsiay",
        "replyto": "BYxHeGsiay",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission837/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497337,
        "cdate": 1696707497337,
        "tmdate": 1701465412349,
        "mdate": 1701465412349,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "**Summary:**\nThe paper introduces a novel approach for generating synthetic data to train error correction models for automatic speech recognition (ASR) systems. The proposed technique mimics the error distribution of ASR models, offering noticeable performance improvements in training error correction models. This approach demonstrates the ability to transfer error distributions across different ASR models. The paper presents convincing results across multiple languages, highlighting the importance of vocabulary size and token length adjustments for optimal language-specific data synthesis. Notably, the proposed approach outperforms a language model-based post-processing method for error correction.\n \n**Pros:**\n \n- The paper provides a clear description of the proposed technique for generating synthetic data, aligning it with ASR error distributions (Reviewer 1, Reviewer 2).\n\n\n\n- Comprehensive ablation studies across popular languages, ASR models and publicly available well-known datasets demonstrate the effectiveness of the proposed approach (Reviewer 1, Reviewer 2).\n\n\n\n- The approach's ability to transfer error distributions across different ASR models is intriguing and potentially of high interest to the community (Reviewer 1).\n\n\n\n- The study covers multiple languages, showcasing the language-dependent nature of optimal tokenizer vocabulary size and token length (Reviewer 3).\n\n\n\n- The paper offers a clear and coherent discussion of the findings and contributions (Reviewer 3).\n \n \n**Cons:**\n \n- The potential transferability of error distributions across different ASR models is intriguing but is not thoroughly analyzed, which could be seen as a limitation (Reviewer 1).\n\n\n\n- The paper's effectiveness is demonstrated primarily on a subset of languages, raising concerns about the generalizability of results (Reviewer 2).\n\n\n\n- Some details for reproducibility might be lacking, as indicated by one reviewer, potentially affecting the broader applicability of the approach (Reviewer 2).\n\n\n\n- The reasons behind the contextual nature of insertion errors versus deletions and substitutions should be further elaborated (Reviewer 3).\n \nReviewer 1: AQUj,\nReviewer 2: UNKv,\nReviewer 3: z7J7"
            }
        },
        "id": "7dNyCA8Cm3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BYkD1gjbxm",
        "replyto": "BYkD1gjbxm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1763/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707526585,
        "cdate": 1696707526585,
        "tmdate": 1701465440640,
        "mdate": 1701465440640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper suggests partial annotation coupled with self-training for active learning of structured prediction. The paper is clearly-written, and ideas are interesting, addressing issues in real annotations. However, in the term of self-training, the contribution is limited. In addition, the design details should be provided with more explainations."
            }
        },
        "id": "0be4t50kO4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BViIHjzvoY",
        "replyto": "BViIHjzvoY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2261/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538136,
        "cdate": 1696707538136,
        "tmdate": 1701465459469,
        "mdate": 1701465459469,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a new TTA method called Anti-CF to address the problem of performance degradation of pre-trained language models in distribution shift scenarios. The authors applied their methods to extractive question answering.\n\nReviewers agree that the approach is reasonable and that could even be applied to other NLP tasks. Some reviewers highlight the importance of the analysis leading to discover that the imbalance in label distribution in the test set of the question answering task is a major cause of model collapse in TTA method. Some reviewers also mention that the approach if efficient, and that the authors have designed a careful set of experiments to demonstrate that their proposed approach achieves stable performance improvements in various distribution shift scenarios.\n\nThe main concern raised by one reviewer is related to novelty, mainly because the authors could explain better how their proposed method differs and/or builds on existing approaches (e.g. LAME and PEFT techniques). Some of these clarifications were provided during the rebuttal period, showcasing that the authors are aware of these techniques and their relationship with their work. As such, they are strongly encouraged to include these comparisons and, in turn, adapt their novelty claims. Other concerns were related to comparisons with stronger models, which were presented during rebuttal, as well as details on hyperparameter settings (also added during rebuttal). Finally, there was some concern regarding using KL divergence and the limits it brings to the approach, which the authos have acknowledged is a limitation (it is stated in the corresponding section in the paper) and plan on tackling it in future work."
            }
        },
        "id": "ybvxquxy7l",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BSApuhuM87",
        "replyto": "BSApuhuM87",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5583/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615354,
        "cdate": 1696707615354,
        "tmdate": 1701465562831,
        "mdate": 1701465562831,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Based on the reviews, the paper proposes a pipeline framework for Conversational Machine Reading (CMR) that addresses the issue of alignment between the document and user-provided information. The proposed method achieves state-of-the-art results on the CMR benchmark dataset ShARC. \n\nThe reviewers generally appreciate the proposed framework, highlighting its novelty, practicality, and improved performance over baselines. They also commend the clear writing style and thoroughness of the experiments and analysis.\n\nHowever, there are some concerns raised by the reviewers. One reviewer questions the potentially unfair comparison between the proposed model and baselines due to the differences in backbone models. Another reviewer raises questions about the clarity of certain concepts and urges the authors to provide more details and clarity. Furthermore, there are suggestions to include more datasets for evaluation and to consider alternative models or noise in the weakly supervised alignment label. \n\nIn terms of novelty, some reviewers express ambivalence, noting that while the proposed many-to-many entailment reasoning module is novel, other components of the pipeline are incremental. The pros mentioned by the reviewers include the clear writing, improved performance, and insights provided by the ablation studies.\n\nIn summary, the paper presents a promising pipeline framework for CMR, addressing the alignment issue and achieving state-of-the-art results on the ShARC dataset. The reviewers appreciate the clear writing and thorough experiments, but some concerns and suggestions for improvement have been raised."
            }
        },
        "id": "Yhqy5rIWwj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BNcTB8RZfG",
        "replyto": "BNcTB8RZfG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3588/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565728,
        "cdate": 1696707565728,
        "tmdate": 1701465502584,
        "mdate": 1701465502584,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores how to use dataset cartography to improve compositional generalization. In short, the paper uses dataset cartography as a curriculum learning and sampling criterion, and demonstrates its effectiveness in improving compositional generalization. \n\nOverall, I think the paper proposes an exciting direction for the community to look at. On the other hand, there are some drawbacks of the paper: (1)  The scope of this work is narrow, (2) the found hard examples are not necessarily compositional examples. More investigation needs to be done to understand it.\n\nBased on the merits and limitations of the paper, I would recommed acceptance to findings."
            }
        },
        "id": "quvcnPNBAq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BMIjPXooNq",
        "replyto": "BMIjPXooNq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3109/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556303,
        "cdate": 1696707556303,
        "tmdate": 1701465487446,
        "mdate": 1701465487446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The work is about the effects of scaling and instruction tuning on the attention distribution and language modeling performance of large language models (LLMs). The authors use two types of instructions: L1 for general language modeling and L2 for specific tasks. They measure the attention distribution using three metrics: entropy, human resemblance, and trivial attention patterns. They also compare the perplexity scores of different models on various datasets.\n\nPros: \n1: An interesting and surprising finding that challenges the common assumption that instruction tuning improves the self-attention distribution of transformer language models.\n2: Revealed a novel and intriguing phenomenon that the self-attention distribution of large language models is closer to non-native speaker attention distributions than native speakers, which opens up new avenues for research and improvement.\n\nCons:\n1: Uses the term “human attention” loosely and without proper definition or justification, and does not relate it to the model attention or the cognitive processes involved in reading comprehension.\n2: Didn't provide a detailed or meaningful analysis of the difference between L1 and L2 speakers, and only mentions a stronger trivial pattern reliance of L2 speakers, which does not explain the underlying causes or consequences of this phenomenon."
            }
        },
        "id": "bI6rZCShCM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BKchhwhNh3",
        "replyto": "BKchhwhNh3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1590/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518846,
        "cdate": 1696707518846,
        "tmdate": 1701465435586,
        "mdate": 1701465435586,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This research paper explores open information extraction and introduces an approach aimed at addressing the problem of data inefficiency. The study proposes two enhancements for the GEN2OIE predicate extraction model. The first enhancement involves reconfiguring the information extraction task to align with T5's pre-training task known as \"span corruption,\" which significantly enhances the model's data efficiency. The second improvement is the introduction of an anchor method that guides the model in determining the sequence order for generation. In summary, this paper builds upon a previous method and achieves significant advancements within a particular module."
            }
        },
        "id": "bvw8BcRPD5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BGsssE3E4i",
        "replyto": "BGsssE3E4i",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3824/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707576665,
        "cdate": 1696707576665,
        "tmdate": 1701465510862,
        "mdate": 1701465510862,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a generative adversarial training framework that combines three components: gradient-based learning, adversarial example generation, and perturbed token detection. The experiments demonstrate the effectiveness of the proposed method on five benchmark datasets. The reviewers acknowledged the contribution and merit of this work, including the motivation of the research, the proposed method, and empirical evidence from experiments. However, there are also issues with the experiment design, especially the baselines and additional result analysis."
            }
        },
        "id": "9GgRj3dC4w",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BFbdO9GwTZ",
        "replyto": "BFbdO9GwTZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3540/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564868,
        "cdate": 1696707564868,
        "tmdate": 1701465500758,
        "mdate": 1701465500758,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary: This paper examines the problem of determining keyphrase boundaries in scientific documents. To tackle the problem that there only exists limited training data, the authors suggest a solution called multi-task knowledge distillation with embedding constraints. This method trains a student model to replicate the outputs of several teacher models that were trained on similar tasks. The student model also learns to generate comparable language representations as the teacher models. The experimental results demonstrate that this approach achieves superior results on all three datasets.\n\nStrengths: All the reviewers unanimously agree that this is a solid contribution with interesting implications. The paper is well-written and easy to follow. The approach used in this study, which involves a basic cosine adjustment, effectively enhances performance. Additionally, the findings reveal several other potential avenues for research, such as the advantages of auxiliary tasks and their relation to the dataset. The comprehensive experimentation conducted in this study will greatly contribute to understanding the complexity of the task.\n\nWeaknesses: I don't think there are any major weaknesses -- some of the weaknesses have been addressed during the discussion phase. The authors should take the feedback into account when preparing for the final version of the paper. For example, Point 3 raised by Reviewer evsV has not been addressed in full."
            }
        },
        "id": "smOGOSp4w4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BEFiYM5Vtx",
        "replyto": "BEFiYM5Vtx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4087/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582874,
        "cdate": 1696707582874,
        "tmdate": 1701465519775,
        "mdate": 1701465519775,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a clustering and pruning method for black-box prompt search called CLaPS. The methodology integrates evolutionary algorithms with vocabulary pruning techniques to address the extensive search space associated with prompt search. The approach appears to outperform existing black box optimization methods in wall clock time while achieving similar or better prompts.\n\nSome reviewers felt that the multi-stage nature of ClaPS potentially makes the approach a bit convoluted, hindering straightforward understanding and reproduction. Figure 4 does a reasonable job of visually demonstrating the different stages in the algorithm. Similarly, some reviewers found that the method lacked enough details/justifications for each step. During the rebuttal, the authors argue that their main contribution is the use of pruning to reduce the search space, and that a better search algorithm may further improve performance.\n\nMore qualitative comparison on how the prompts derived using ClaPS differ from those of existing methods. During the rebuttal, the authors provide additional comparison and commentary on the differences between the CLaPS output and RLPrompt output. However, this seems like a clear place where the work could be further improved."
            }
        },
        "id": "NEzzUQuwLs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BB1qrcPgRu",
        "replyto": "BB1qrcPgRu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3053/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555207,
        "cdate": 1696707555207,
        "tmdate": 1701465485724,
        "mdate": 1701465485724,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a novel 'set learning' approach to address order bias in Seq2Seq models for IE. Experiments across various IE tasks and datasets demonstrates the effectiveness and applicability of the proposed approach. The paper has the potential to inspire further research in IE and related NLP tasks. However, a more comprehensive comparative analysis with existing order bias mitigation methods is needed to strengthen the paper's claims. Additional experimental evidence is recommended to validate claims related to the proposed Set loss and various other losses. Overall, the paper shows promise but should address these areas in the next version."
            }
        },
        "id": "Izc4gK0uCH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "BAA4209PGJ",
        "replyto": "BAA4209PGJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3681/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567650,
        "cdate": 1696707567650,
        "tmdate": 1701465506529,
        "mdate": 1701465506529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper looks at the task of visual word sense disambiguation, where the goal is to retrieve an image that matches a given ambiguous word within a particular context. Several approaches are explored, including utilising large language models (LLMs) as an external knowledge source. The reviewers agreed that the work was sound, with comprehensive experiments and comparisons to state-of-the-art baseline systems on this task. The reviewers also commented positively on the use of LLMs for this task. The reviewers raised (minor) concerns about novelty and the question of why some of the models perform better than others. But in general, the work was found to be sound and exciting."
            }
        },
        "id": "UgT1asSqyV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "B8mdHlqNfw",
        "replyto": "B8mdHlqNfw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2849/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550707,
        "cdate": 1696707550707,
        "tmdate": 1701465478631,
        "mdate": 1701465478631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a zero-shot learning approach that bridges the gap between cascade and end-to-end speech translation models using a differentiable shrink adapter and WRD loss.\n\nThe reviewers' evaluations are mostly consistent, showing moderate soundness and relatively high excitement scores.\nThe reviewers are concerned about the readability to understand the details, limited comparisons, and the lack of SOTA ASR system, e.g., Conformer-CTC with an external language model.\n\nThe authors answered the reviewers' questions regarding technical details, and the answers sound reasonable to me. \nThe authors also explained that the focus of the paper is to bridge the gap between the cascade and end-to-end systems and not to achieve SOTA performance.\nThis is understandable, but I think that evaluating using Conformer-CTC with a language model would have yielded more reliable results. However, the topic the paper addresses is interesting. The reviewers rated high excitement scores. So, it will be of interest to a large audience. If accepted, I hope that the authors will try to improve the readability as pointed out by the reviewers."
            }
        },
        "id": "SzjFXcR6bj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "B8Hz9HqnFm",
        "replyto": "B8Hz9HqnFm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1701/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707524265,
        "cdate": 1696707524265,
        "tmdate": 1701465438903,
        "mdate": 1701465438903,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes LLMaAA using LLMs to annotate data in the active learning loop by optimizing both annotation and training processes to train task-specific models reliably. Reviewers appreciate that the method is reasonably well motivated and explained, presented with superior performance and comprehensive analysis with ablation study. They also bring up several areas for improvements: (1) The novelty is limited because a large portion of the framework directly borrows from existing techniques; (2) the lack of results on the effect dataset sizes, (3) improving the clarity and adding thorough analysis and discussions on efficiency and privacy. Overall, we strongly encourage authors to incorporate the feedback to improve the paper in the next version."
            }
        },
        "id": "rFCefwIgPW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "B6Gdg7u04y",
        "replyto": "B6Gdg7u04y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4113/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583385,
        "cdate": 1696707583385,
        "tmdate": 1701465520776,
        "mdate": 1701465520776,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In general, all reviewers agreed that this was sound and exciting work.  Only minor issues were raised in the reviews, as described below.\n\n**Summary of Reviewer Feedback and Discussion:**\n- **Reviewer BeTN** liked that the authors thoroughly evaluated multiple agents using both automated metrics and human assessment, and that they also thoroughly analyzed all results.  They also found that the insights resulting from the work were academically interesting.  However, although the authors claimed that their training procedure was novel, Reviewer BeTN was unconvinced of its novelty.  In their rebuttal, the authors clarified that their training procedure was novel in two ways: it introduces a new reward formulation, as well as an automated technique for varying partner personality.  They promised to elaborate on this in the updated manuscript.\n- **Reviewer qS7U** felt that the paper covered an interesting topic with sufficient detail for future replication.  They also thought that the study was solid and backed by economic theory.  However, they felt that the model and task were rather simple, with short and straightforward negotiation dialogues, preventing the model from coming up with more complicated strategies.  They also noted that the experiments were only conducted on a single dataset and task, which made the reported findings less convincing.  In their rebuttal, the authors elaborated on the complexity of their model, and mentioned that these were briefly discussed in the original manuscript.  They could extend these discussions in the revised manuscript.\n- **Reviewer atVW** appreciated the novelty of the work, and felt that the paper thoroughly discussed the implications for designing and evaluating negotiation dialogue systems in the future.  They also liked that the paper clearly explained the limitations associated with the task design and human evaluation.  They felt that some of the results in the appendix should be moved to the main body of the paper.  In their rebuttal, the authors thanked the reviewer and noted that they will move additional details about the human evaluation as well as Figure 6 to the main body of the paper."
            }
        },
        "id": "gwU9pjL4yJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "B6BXB4g8eQ",
        "replyto": "B6BXB4g8eQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2488/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543226,
        "cdate": 1696707543226,
        "tmdate": 1701465466757,
        "mdate": 1701465466757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper has received largely favorable reviews from all the reviewers. Some concerns which were raised in the earlier reviews have been adequately addressed as summarised below\n\n1. High quality datasets [concerns about differences from existing datasets have been adequately addressed]\n2. Evaluation using metrics in addition to GPT 4 [results using SARI and GLUE have been added during the response period]\n3. Usefulness of the MORL method (ablation results have been provided to show that the proposed method MORL indeed helps]\n\nOverall this is a good paper which contributes a dataset, a new method and does a thorough comparison with appropriate ablation studies.\n\nI request the authors to include the new results provided during the response period in to the main body or appendix of the paper."
            }
        },
        "id": "E8rCcz1oEI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "B3rTZovgaA",
        "replyto": "B3rTZovgaA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5633/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616189,
        "cdate": 1696707616189,
        "tmdate": 1701465564238,
        "mdate": 1701465564238,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents the T5-Sentinel method for distinguishing human-generated and machine-generated text and introduces a dataset for this purpose. Reviewers agree that T5-Sentinel demonstrates superior performance in distinguishing human-generated and machine-generated text compared to baseline methods. Additionally, the construction of a dataset with 340k text samples is seen as a valuable contribution to the field. This dataset is expected to benefit future research in the area, making it another positive aspect of the paper. The relevance of the research topic to current trends is acknowledged, and it is noted that the paper is likely to attract a wide audience.\n\nReviewers raise concerns about the lack of reporting statistical significance in Table 1 but the authors addressed this concern during the rebuttal period. They note that certain aspects of the method, such as T5-Hidden containing an additional classifier head, could benefit from a discussion of how different random initializations might affect performance.\n\nThe potential issue of data and model licenses affecting the release of the dataset is raised as the main concern that especially impacts reproducibility. To my understanding,  there are no legal drawbacks to the public release of the constructed dataset and the model weights and the authors mention they will release them after the anonymity period. Another concern is about the paper's self-containment, particularly in the results section. Reviewers note that important results verifying the main claim of the paper are placed in the Appendix, making it less accessible to readers. The paper should ensure that key results are presented in a more prominent manner.\n\nIn summary, the paper has several positive aspects, including superior performance and dataset contribution. However, it needs to address concerns related to statistical significance, self-containment, and replication guarantees. Ensuring the clarity of data release plans and addressing these concerns could strengthen the paper's overall contribution and impact."
            }
        },
        "id": "eoLjzFFdT8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "B3SjWgXHzM",
        "replyto": "B3SjWgXHzM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4454/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589815,
        "cdate": 1696707589815,
        "tmdate": 1701465531998,
        "mdate": 1701465531998,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates negation understanding in LMs. It proposes fine-tuning models on data that includes negation understanding and propose a two-part loss to balance fine-tuning with avoiding catastrophic forgetting. Experiments show that this recipe improves negation understanding for MLMs without hurting performance on other tasks.\n\nThe reviewers agree that the proposed recipe is interesting and experiments support the hypotheses well. There are some concerns regarding generalization to longer sequences and whether the proposed approach would generalize beyond the experiments in this work, both of which the authors acknowledge would be interesting future work."
            }
        },
        "id": "1Nrhyifa2e",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "B3Muf1R1UD",
        "replyto": "B3Muf1R1UD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3072/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555643,
        "cdate": 1696707555643,
        "tmdate": 1701465486202,
        "mdate": 1701465486202,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this paper, the authors create a new word analogy dataset for Bangla. A diverse set of relations are covered spanning many synactic and semantic relation categories. They also translate and filter the original Mikolov dataset. They compare various models (classical word embedding models and sentence embeddings models) on this dataset.\n\nReasons To Accept:\n- The datasets developed by the authors for the Bangala language which is considered one of the low-resource languages although it is spoken by over 300 million native speakers.\n- Creation of a word analogy dataset for Bangla that is diverse\n- Translation of an English word analogy dataset can help cross-lingual studies.\n- Various word representation methods are evaluated and compared, including their fine-grained evaluation on the 12 word analogy relationship types.\n- The paper is well written and easy to follow.\n\nReasons To Reject:\n- Although the paper presents a good trial for developing two datasets for Bangala, they couldn't achieve high results in their experiments. \n- There is limited learning from the paper about word analogy in Bangla beyond reporting accuracies on a few pre-trained embeddings.\n\nI agree with the authors when they write (during the rebuttal): \"The main goal of our paper is to create a benchmark word analogy dataset (the first of its kind for Bangla) to facilitate future Bangla NLP research and evaluation. Although the results reported in this paper do not show a high degree of performance, we want to emphasize that achieving a high performance is NOT the goal of this work.\""
            }
        },
        "id": "v0UOg7LAY3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "B14ohp9mPU",
        "replyto": "B14ohp9mPU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3266/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559422,
        "cdate": 1696707559422,
        "tmdate": 1701465492493,
        "mdate": 1701465492493,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper provides an extensive analysis of various parameter-efficient prompt tuning methods for text retrieval across in-domain, cross-domain, and cross-topic settings and shows that such approaches can mitigate the parameter-inefficiency and weak generalizability issues. Though there is no new approach proposed in this work, the extensive analysis and insights will benefit the research in this line. The authors also contributed a new dataset for academic retrieval. The authors should address the minor concerns raised by reviewers, e.g., the impact of the number of parameters for P-Tuning V2, and clarifications on why only DPR and ColBERT models are chosen."
            }
        },
        "id": "XfhG2ovjK2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "B01gPP5YCh",
        "replyto": "B01gPP5YCh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2733/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548346,
        "cdate": 1696707548346,
        "tmdate": 1701465474624,
        "mdate": 1701465474624,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes and studies a method to utilize sub-network similarity between two languages for predicting the source language in cross-lingual transfer. Overall, the reviewers agree that the sub-network similarity is novel for the selection of languages for cross-lingual transfer. Some concerns regarding the clarity and motivation for the proposed framework (which is depicted in Figure 2) and the proposed approach do not work well in some settings such as when using mT5. In summary, the AC believes that the usage of sub-network similarity in choosing the optimal source language is interesting but also recommends addressing the clarity in details and limitations pointed out by reviewers in the next version of the paper."
            }
        },
        "id": "opE2OFnLft",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AxPGO36LfE",
        "replyto": "AxPGO36LfE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1434/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707513558,
        "cdate": 1696707513558,
        "tmdate": 1701465430933,
        "mdate": 1701465430933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper brings merits of a reconstructor locates the salient information when reconstructing the next utterance, and improves meeting summarization. The paper is expected to added more details and discussions as mentioned in the rebuttal."
            }
        },
        "id": "r9BHJDGf9E",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AtjErbRsg2",
        "replyto": "AtjErbRsg2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4008/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581093,
        "cdate": 1696707581093,
        "tmdate": 1701465516847,
        "mdate": 1701465516847,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work studies the impact of the vocabulary size on a XLM  model, together with a lexical cluster approach and encouraging vocabulary share between clustered languages. This leads to overall performance boosts on a large variety of benchmarks. It is also remarkable that the authors did the experiments on truly low resource languages. The approach was exciting for all reviewers and no major objections to include this paper in to the main conference were made."
            }
        },
        "id": "msLkPrnybt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ariw9I14zZ",
        "replyto": "Ariw9I14zZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2146/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535539,
        "cdate": 1696707535539,
        "tmdate": 1701465455700,
        "mdate": 1701465455700,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents an approach for making good use of the noisily labeled instances in distant supervision. The results show the effectiveness of the proposed approach. However, one major concern is that the approach appear to be presenting a collection of existing techniques, which makes the current work less attractive as an original research article. It is nevertheless worth sharing with the community."
            }
        },
        "id": "pCiG844TKd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ArSMQ3dCUx",
        "replyto": "ArSMQ3dCUx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3162/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557319,
        "cdate": 1696707557319,
        "tmdate": 1701465488925,
        "mdate": 1701465488925,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Overall, the paper addresses an intriguing subject of how characters can be simulated using LMs. The authors devise a novel methodology and playground for making progress on this that allows them to train agents that they then evaluate for their memories/experiences.\n\nThe reviewers jointly all recognize the novelty, thoroughness and insightfulness of many aspects of the work. However, they do also highlight that aspects of rigor (e.g. baselines/ablations, human evals, many important implementation details) could be strengthened. Finally, for this work in particular, I do think there are several ethical concerns with reviewers disagreeing on the extent to which authors address this. If accepted, I would like to see the authors strengthen this aspect based on my own brief reading of the paper. I also encourage the authors to strengthen the implementation details, as I do find the author rebuttals to not adequately address these matters, though the authors do make extensive promises in this vein.\n\nWith all of this said, I believe the paper breaks new ground and some of its clear limitations can be addressed as this line of work grows. If the questions of rigor and ethical concern can be addressed, this paper could play a formative role in imaging new possibilities using language models."
            }
        },
        "id": "XcR4uqMBQb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AptTXihnhH",
        "replyto": "AptTXihnhH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission176/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481099,
        "cdate": 1696707481099,
        "tmdate": 1701465389842,
        "mdate": 1701465389842,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a method for the “interpretation” of text at scale. The authors propose using LLMs to generate propositions which are “inferentially related” to an original text (see AC suggestions at the bottom). They propose that these propositions provide “explicit representations” of the “implicit” interpretations that come with reading textual information. The authors validate generated propositions with a simple crowd experiment, and test the utility of generated propositions for clustering public comments and predicting legislator voting patterns.\n\nReviewers offered a largely positive assessment of the soundness and excitement of this work, especially after discussing with authors during the rebuttal period. All soundness and excitement scores were 3 or higher (following discussion). Reviewer xCQB described the paper as “a valuable method for incorporating implicit knowledge” and suggested that the authors provided a “thorough assessment of the method's effectiveness.“ Reviewer 5xdd noted that it was interesting to see how LLMs could be “effectively applied to problems in social sciences” and reviewer V5xX liked that it supported “automated ways to analyze communication” in a social field.\n\nHowever, reviewers did offer some negative comments about the paper. The AC has summarized negative comments below, in order of what the AC believes to be the most and least important to acceptance/rejection decisions.\n\nReviewer 5xdd noted a lack of comparison with “traditional methods for extracting common sense knowledge.” In the AC’s opinion, this does seem like a limitation of the work. But this limitation should be placed in a broader context. Overall the authors did a good job grounding their submission in recent work from NLP and CSS, so this shortcoming does not seem core to the underlying soundness or excitement of the submission.  \n\n- Note: in the discussion, the authors replied by suggesting that “commonsense knowledge” and “inferentially related” knowledge are not equivalent. They included references to work on LLMs for commonsense reasoning. But some of the nuances between inferentially-related reasoning and commonsense reasoning did not come through on OpenReview.  See the AC’s suggestion about “inferentially related” at the end of this review. \n\nReviewer xCQB wondered if the method required human validation, which might limit the utility of the work. The authors replied that human validation was a “low cost optional step” best applied in “sensitive” use cases. Figure 2 seems to show that (at least for these experiments) human plausibility judgments seem to validate the LLM. Thus it seems like in the future you could use this method with less crowdworker validation (in some settings). So this does not seem like a major shortcoming of the work.\n\nReviewer 5xdd asked for qualitative analysis of the method, especially regarding cases where the method does not work. The authors replied with explanations of the ways in which their paper does incorporate qualitative analysis, and promised to contextualize the paper with examples upon acceptance. They also promised to analyze cases where “the method does not perform as well relative to the baseline.” Reviewer 5xdd replied to say that the authors’ “responses make sense to me mostly” which seems to largely address this issue, provided the authors make the requested changes.\n\nSome of the reviews critiqued the style of the submission. These things might be changed during future revisions, and do not seem central to acceptance decisions. The requested stylistic changes are as follows. (Note: the AC agrees with reviewer 5xdd about the title.)\n- Both reviewer xCQB reviewer V5xX noted that the authors should add more detail about the method in the main body of the paper. The authors agreed that this stylistic change was necessary during discussion. This is a stylistic issue that could be addressed during revision.\n- Reviewer 5xdd pointed out that the title of the paper may not reflect its content. They noted that the paper should have a more concrete and descriptive title that accurately describes the body of the work.\n\nWhile this metareview largely focuses on negative feedback, in total, it seems like the reviewers offered a largely positive assessment of the paper, and that the limitations they brought up appeared to be (1) addressed during the rebuttal period or (2) not central to the soundness or excitement of the work.\n\nSuggestions from AC: It was hard to find the precise definition of “inferentially related” in this paper. The related work section helped explain some of the ideas behind the approach, but a more explicit statement of what “inferentially related” means would help the paper, even if the “formal semantic representation” (L525) of this phrase is itself “relaxed” (L535). Also, the AC agrees with 5xdd that the paper should have a more clear and descriptive title."
            }
        },
        "id": "Is6Yvz27ZQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AoGdaivPEh",
        "replyto": "AoGdaivPEh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4718/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597034,
        "cdate": 1696707597034,
        "tmdate": 1701465539536,
        "mdate": 1701465539536,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a strong baseline dataset comparison, includes an ablation study, has a solid methodology foundation, and provides clear and concise experimental design and results. However, this paper has limitations in a lack of comparison with other state-of-the-art models, which would provide valuable insights and further strengthen the contribution of the proposed framework."
            }
        },
        "id": "ACLHTkwuHs",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AlEeMxkgsi",
        "replyto": "AlEeMxkgsi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4387/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588753,
        "cdate": 1696707588753,
        "tmdate": 1701465530203,
        "mdate": 1701465530203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose a new LLM decoding method that allows to control for multiple attributes. The reviewers raise concerns on comparisons with related works and ablation studies, to which the authors respond by conducting additional experiments, showing that their method outperforms the new baselines on most metrics. On the other hand, the reviewers appreciate the novelty and performance of the proposed method and for all the reasons above I would recommend accepting this paper."
            }
        },
        "id": "fJD4wAdlTn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Akk5ep2gQx",
        "replyto": "Akk5ep2gQx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2460/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542734,
        "cdate": 1696707542734,
        "tmdate": 1701465465898,
        "mdate": 1701465465898,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Scores were mixed: 3,2,4 (soundness) and 3,3,4 (excitement). The authors addressed some concerns, leading to increased scores.\n\nSome of the key strengths and weaknesses identified by the reviewers included:\n\nStrengths:\n\n- The research question and motivation were considered timely and interesting (R1, R2, R3)\n- Reviewers found some of the results interesting (R1) and the paper well-written (R1, R2)\n- paper proposes a new approach for developing concept-aware LLMs by pretraining using concepts (R2)\n- includes promising proof-of-concept results (R2)\n- behavioral experiments well-designed (R2)\n\nWeaknesses\n\n- despite very general motivation, the paper focuses on hyponyms and hypernyms (R1)\n- concerns about how realistic a shift to concept-level LLMs would really be (R1)\n- makes claims about inabilities of LLMs without testing on SOTA LLMs (R2) [a similar concern from R3 was addressed in the author response]\n\nI did not find the soundness-related concerns by the second review (YpEe) to be fully convincing. That review lists three reasons to reject: lack of SOTA LLMs, perceived mismatch with the theme track, and lack of discussion of privacy concerns. Only the first concern appears to be potentially a major soundness concern, but the authors pushed back, noting i.a. that the paper includes GPT davinci-003, which can allay some of this concern.\n\nIn conclusion, the reviews suggest that the paper is reasonably sound and was perceived as moderately-to-strongly exciting by the reviewers."
            }
        },
        "id": "ZavPcyA0m8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AjGXZIgvIb",
        "replyto": "AjGXZIgvIb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2052/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533077,
        "cdate": 1696707533077,
        "tmdate": 1701465451853,
        "mdate": 1701465451853,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors in this paper study the effectiveness of ChatGPT for multiple tasks in a diverse set of non-English languages. \n\nPros: \n* The reviewers have found the paper is well written and very interesting to read. \n* The paper is the first to study and report ChatGPT’s performance on 37 different languages on 7 NLP tasks. \n* It is not surprising but the findings that the ChatGPT demonstrates poor performance on various non-English tasks and languages, will be valuable to the community.\n\nCons:\n* The reviewers have pointed out that the paper will significantly improve with inclusion of results from different LLMs and LLMs with varying sizes. While I agree with the reviewers, I believe that the large-scale multilingual analysis of ChatGPT will be valuable to the community."
            }
        },
        "id": "MXAEvzUypI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "Ai0oBKlJP2",
        "replyto": "Ai0oBKlJP2",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4788/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598515,
        "cdate": 1696707598515,
        "tmdate": 1701465541769,
        "mdate": 1701465541769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In the context of multilingual masked language models, this paper focuses on understanding the importance of specific training (i.e. fine-tuning) data samples at test time (referred to as cross-lingual data sharing). More specifically, it measures and analyzes the importance of the training samples and their language distribution using TracIn (Prudi et al. 2020) for the multilingual and cross-lingual performance of XLM-R across three tasks.\n\nReasons to accept:\n- Very well-written paper (with beautiful plots) that addresses an under-studied question: the importance of specific samples seen during fine-tuning at test time in the multilingual and cross-lingual settings.\n- Many settings and dimensions of the problem are analyzed and discussed, such as sample importance in the multilingual setting, in the zero-shot cross-lingual setting, and the impact of fine-tuning dynamics on cross-lingual data sharing.\n\nReasons to Reject:\n- None\n\nSuggestion:\n- Given the emergence of LLMs as an essential framework in NLP, discussing how this method could be applied to pretrained LLMs (without task-specific fine-tuning) and their multilingual generalization would be highly beneficial for the paper."
            }
        },
        "id": "Ng8zgzADgO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AgsLcJ9KaX",
        "replyto": "AgsLcJ9KaX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2066/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533495,
        "cdate": 1696707533495,
        "tmdate": 1701465452446,
        "mdate": 1701465452446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper examines the issue of potential bias and unfairness in personalized text generation models. Specifically, it looks at personalized explanation generation for recommendations. The authors find that biases in the user-written training data can lead models to associate different levels of linguistic quality with different user attributes. This results in unfair treatment when serving users with different protected attributes. To address this problem, the paper proposes a new framework called COFFEE to optimize counterfactual fairness constraints during training. The key ideas are disentangling user attribute representations and using policy learning to impose fairness rewards.\n\nThe proposed framework COFFEE shows promise in mitigating quality disparities for personalized explanation generation. In particular, experiments on explanation generation datasets demonstrate the system's ability to reduce quality disparities across user attributes. \nLooking at the reviews, the reviewers generally acknowledge the importance of studying counterfactual fairness for personalized text generation, particularly in the area of explanation systems. They find the technical contributions of disentangled representations and policy learning to be novel and promising. However, concerns exist around justifying quality metrics as proxies for human preferences and quantifying the harmfulness of observed disparities. The reviewers consistently seek more evidence that differential quality is inherently problematic rather than simply representing user preference differences. Additionally, two reviewers raise doubts about assumptions in the counterfactual inference module, suggesting sensitive attributes may not be fully disentangled from representations by removing them from the input. The framing also overstates the generalizability of the approach beyond the specific explanation generation setting based on the experiments shown. While the reviewers are in agreement that the paper is technically sound overall, there is a general recommendation for authors to enhance the problem formulation, making explicit the required assumptions, and situating the work within the broader literature on algorithmic fairness. In my opinion, the authors have provided satisfactory clarification on most technical concerns, which equally address my own worries about the work. The authors also demonstrated a sufficient understanding of general fairness literature and methodology. I think the technical approach appears novel and promising.\n\nIn summary, the paper tackles an important emerging problem at the intersection of fairness, personalization, and natural language generation. The initial results are positive, but further analysis of the assumptions and framework generalizability would strengthen the contribution."
            }
        },
        "id": "jcNcw2lHzM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AfnJBOXfAU",
        "replyto": "AfnJBOXfAU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1270/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707508999,
        "cdate": 1696707508999,
        "tmdate": 1701465425802,
        "mdate": 1701465425802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are positive about the paper. They find the task well-motivated and the paper well-written. All reviewers rate the soundness of the paper high and two of the three are highly excited to see the paper published (the third is ambivalent). The reviewers raised issues that seem easy to fix before the camera-ready. All reviewers consider the paper reproducible."
            }
        },
        "id": "oE0Q69KqDu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AfEowGM3qG",
        "replyto": "AfEowGM3qG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2487/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543216,
        "cdate": 1696707543216,
        "tmdate": 1701465466675,
        "mdate": 1701465466675,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents an extensive performance benchmark of 44 recent state-of-the-art Large Language Models on the task of Text Simplification (TS), which they call BLESS. The evaluation is based on three different datasets with original sentences/documents and their humanly written simplifications: ASSET (Wikipedia texts), MED-EASt (short medical texts), NEWSELA (professionally rewritten news article simplifications). The 44 models cover both open and closed sourced models, a range of different LLM architecture families, pretraining objectives and parameter sizes.\n\n\nReasons To Accept:\n- The set of Large Language Models evaluated is extensive. The authors benchmarked 44 LLMs ranging in size from 60M to 176B parameters and include instruction-tuned models.\n\nReasons To Reject:\n- Though the authors do their own small analysis in Section 5, the bulk of this paper uses existing metrics to evaluate existing models on existing datasets.\n\nThe paper is a very interesting and enjoyable read and to me it seems very sound, detailed, comprehensive (they use 44 models and test on 3 different domain, using both automatic as well as manual/qualitative evaluation) and is well thought-through and structured and clearly written. The sheer number of models and evaluation methods makes it very apparent that furthermore they must have put a lot of effort in all the experiments. Moreover, the task of Text Simplification is interesting and relevant, as is - of course - the inspection of what LLMs really can do and cannot do."
            }
        },
        "id": "9EI1ZcRfWY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AbXA40kggY",
        "replyto": "AbXA40kggY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2924/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552272,
        "cdate": 1696707552272,
        "tmdate": 1701465481211,
        "mdate": 1701465481211,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a suite of probes for understanding when and where linguistic information (syntax, semantics, reasoning) is learnt in the embedded representations of MultiBERT models, including an information theoretic analysis supporting the specific formulation of the probes. The main concern shared by all reviewers is that the paper was often confusing and hard to follow. Some reviewers felt that treatment of highly related work was insufficient, some felt that the description of technical details was hard to follow, and some reviewers felt that the overall motivation/story of the paper was not clearly aligned with the provided results and analysis. If the paper is accepted, it should be substantially edited for clarity based on the feedback from the reviewers. After much clarification by the authors in the rebuttal, reviewers seemed to understand more, and found the results in the paper to be sound. While reviewers agreed that the topic was interesting and relevant, reviewers were not unanimously excited, citing lack of breadth in terms of models analyzed, and to much breadth (and lacking depth) in terms of the analysis performed."
            }
        },
        "id": "DRMSpOmqcx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AajIIYMm0d",
        "replyto": "AajIIYMm0d",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3915/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579144,
        "cdate": 1696707579144,
        "tmdate": 1701465514037,
        "mdate": 1701465514037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "All reviewers agree that the authors present a simple yet effective algorithm to reassess possibly disparate importance of few-shot examples. To determine optimal weights for better In-Context Learning, the proposed algorithm seamlessly utilizes Masked-Self Prediction (MSP) scores and beam search, subsequently differentiating the importance by rescaling attention weights. To elaborate the current version, nevertheless, take into consider the following valuable points from our reviewers.\n\n1)\tClarify the setting and relationship with demonstration selection: The current paper presumes the absence of any held-out validation data. Such “true few-shot setting” must be better clarified on the main draft.\n2)\tStronger baselines: Although using held-out data could overwhelm the effect of using MSP and may not be congruent with a true few-shot setting, it remains pertinent to enrich the paper with additional experimental results. This could involve reweighting with an additional validation set or with example retrieval.\n3)\tAdditional analysis on ordering sensitivity and inference costs: It would be advantageous to illuminate the interplay between the weights and the orders of few-shot example. In addition, considering the method predominantly operates with a higher number of shots, real-time inference cost (beyond asymptotic complexity) becomes relevant. Kindly incorporate some analysis.\n4)\tPotential implications for generative tasks: It would be beneficial to include some insights or preliminary thoughts when we apply the proposed method to generative tasks."
            }
        },
        "id": "ind46vkjgl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AZfRWT1dOa",
        "replyto": "AZfRWT1dOa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission749/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495135,
        "cdate": 1696707495135,
        "tmdate": 1701465409227,
        "mdate": 1701465409227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a simple and elegant modification to masking for domain adaptation in continued pre-training by focusing on masking tokens that are different in the new domain compared to the old domain. The authors also provide good experimental results, outperforming more complicated baselines on two NLP datasets (ACL-ARC, ChemProt) and two multimodal datasets (Social-IQ, TVQA).\n\nThe authors have addressed the reviewer concerns in the comments (clarifications for Reviewer LPF7), results on an additional baseline (Reviewer Yfzq) and additional experiments + clarifications for Reviewer o2x7."
            }
        },
        "id": "LR6vT2UWms",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AZ8sFZtLHD",
        "replyto": "AZ8sFZtLHD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2107/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534618,
        "cdate": 1696707534618,
        "tmdate": 1701465454235,
        "mdate": 1701465454235,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents interviews with 26 NLP practitioners about the current state of the field. While the sample size is small, the study was conducted following proper protocols and included in-depth qualitative analyses of the various topics in the interview, supplemented with qualitative longitudinal analyses of citations, terminology, and authorship patterns in the ACL Anthology."
            }
        },
        "id": "cClvAjEpyT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AYOfbWMRSd",
        "replyto": "AYOfbWMRSd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2155/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535785,
        "cdate": 1696707535785,
        "tmdate": 1701465456022,
        "mdate": 1701465456022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work proposes a new distillation loss which maximizes the cross correlation between individual features at pre-logit activations. Authors apply their method on various BERT models fine-tuned on downstream tasks. Reviewers agree on the novelty of the method and find the results promising. Authors also provide further ablations and comparison to related work during the rebuttal."
            }
        },
        "id": "IwNX3VnE7Y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AXY8GJzm2K",
        "replyto": "AXY8GJzm2K",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3401/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561863,
        "cdate": 1696707561863,
        "tmdate": 1701465496098,
        "mdate": 1701465496098,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a method to learn to map wearable IMU data to the same space as text and video data, and shows that the method can be used for retrieval tasks across text, IMU and video modalities.\n\nStrengths:\n- All reviewers highlighted that aligning IMU with other modalities is an interesting idea and not explored before.\n- Most reviewers commended strong results on the benchmarks, which show that the method works.\n- Most reviewers agree that the method is a low-resource way to do media search (the wearable device can be low-power and low bandwidth).\n\nWeaknesses that may have been addressed in rebuttal:\n- One reviewer cited lack of comparison to prior motion datasets. The authors mentioned that this was because prior datasets are too small (<2 hours activity data), but they will run the experiments and include these results anyway at the request of the reviewer.\n- R b6Gx raised a number of concerns and did not elaborate which were addressed during rebuttal. From my reading, it seems the concerns are addressed.\n\nOverall, there is consensus that the paper studies an interesting and novel idea, and proposes a method that works, as substantiated by the experiments. \n\nOverall: Sound and moderately exciting."
            }
        },
        "id": "C0jUKSI4zU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AU2Oq0z4xA",
        "replyto": "AU2Oq0z4xA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4241/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696708181996,
        "cdate": 1696708181996,
        "tmdate": 1701465525175,
        "mdate": 1701465525175,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Reject"
            },
            "comment": {
                "value": "The authors introduce ViLTP, an innovative OCR-free vision-language pretraining objective. This model seeks to cohesively integrate text and layout information from document images, aiming to bolster performance in downstream tasks like document classification and extraction. All reviewers concur that, despite its novel pretraining objective, ViLTP exhibits only marginal empirical performance, particularly in QA tasks. Moreover, the analysis lacks crucial baselines, notably other contemporary foundational models for document comprehension.\n\nWhile the authors have presented a comprehensive rebuttal, complemented by additional experiments, I am of the opinion that it doesn't fully address the concerns raised by the reviewers."
            }
        },
        "id": "JSHRV8C2KZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ARtBIBAmNR",
        "replyto": "ARtBIBAmNR",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission4207/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585430,
        "cdate": 1696707585430,
        "tmdate": 1711124192674,
        "mdate": 1711124192674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper attempts to make utterances more empathetic by prompting GPT-4 with few-shot samples and dialogue act labels. The authors show that it is unnecessary to provide source-target pairs when prompting GPT-4 and dialogue act labels might be a help.\n\npros:\n- It studies a novel direction on bringing dialogue acts for empathy style transfer. The proposed targeting prompting and conditioning on the dialogue act of the source sentence works great. The overall evaluation is extensive.\n\ncons:\n- Empathy is very difficult to be captured by automatic metrics, hence human evaluation is a must to verify the improvements. However, the authors only report automatic metrics. Moreover, the difference of the automatic scores between approaches are small. Therefore it is hard to tell whether the approach in the paper is effective or not.\n- It is unclear what are the qualified few-shot examples selected for target prompting(see more details in the questions). It seems many factors here impact the performance: few-shot example selection(number of examples, diversity, distribution of empathy scores), prompt templates etc. The discussion to control those factors is not well-organized, it is not convincingly clear that claim the proposed prompting method is better than others.\n-  The idea that the transfer between \"apathetic\" and \"empathetic\" is distinct from the transfer between \"unempathetic\" and \"empathetic\" with regard to the issue setting in sec. 3.1 may be based on intuition and prior understanding of empathy and related styles. Theoretical support for this supposition and an explanation of how empathy styles are arranged on a continuous, real-valued scale would be helpful, though. Without more support, this idea may be questioned and criticized."
            }
        },
        "id": "7mv0XkqD7d",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AQiuwWLvim",
        "replyto": "AQiuwWLvim",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2201/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536832,
        "cdate": 1696707536832,
        "tmdate": 1701465457696,
        "mdate": 1701465457696,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers generally agreed that the paper was well written and the experiments are sound. Reviewer X3Lx appreciated the user-level dimension to scoring. Reviewer FazH also pointed out that the results reduce the number of annotations required for subjective tasks, while keeping the accuracy level.\n\nReviewer X3Lx had most notable concerns including the lack of comparisons in findings.The authors mentioned conducting some analyses but suggested that they did it find useful insights. It was still a little unclear. Reviewer 66GU mentioned that they do not have a concern about a lack of a baseline. Reviewer FazH also thought that leaving testing of the method on a broader variety of datasets with more diverse annotators as a future work."
            }
        },
        "id": "TrPeQmbFRF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AJDSZ2YVI6",
        "replyto": "AJDSZ2YVI6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2128/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535170,
        "cdate": 1696707535170,
        "tmdate": 1701465455097,
        "mdate": 1701465455097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a framework to systematically evaluate factual knowledge of LLMs with KGs in generic and specific domains. All reviewers appreciated that the analysis in the paper is comprehensive, covering various LLMs, question types, and domains. Reviewers highlighted how the approach takes into account abstentions by the LLMs.\n\nReviewers also expressed some concerns about the experiment setup. For instance, it's not clear if the evaluation might be conflating models' ability to follow instructions with factual knowledge. Another issue that reviewers raised is whether the proposed benchmark also has ‘leakage problems’ similar to past benchmarks. One reviewer pointed out that automatically generating questions from knowledge graphs has been explored in past work (see reviews for references) and shouldn’t be considered a major contribution of this paper."
            }
        },
        "id": "7YaSKPtxAc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AGVANImv7S",
        "replyto": "AGVANImv7S",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5105/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707607055,
        "cdate": 1696707607055,
        "tmdate": 1701465550069,
        "mdate": 1701465550069,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers are generally appreciative of the novel idea of representing visual knowledge as code and using curriculum-based learning to guide the vision-language models training process in different complexity stages. The ViStruct Suite is seen as potentially useful for training and benchmarking future visual structure extraction models.\n\nHowever, one key remaining concern is the limited number and outdated nature of the pre-trained models used for evaluation. The reviewers also suggest further improvements, such as better exposition, detailed analyses of ViStruct Suite and the curriculum learning design.\n\nGiven its novelty and potential impact on future work, all reviewers are pretty excited about this work."
            }
        },
        "id": "OC91Cwijaa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AEkFAAprvF",
        "replyto": "AEkFAAprvF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission842/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497444,
        "cdate": 1696707497444,
        "tmdate": 1701465412503,
        "mdate": 1701465412503,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agree this work demonstrates a well motivated and rigorously tested set of methods for prompt compression, with very promising results! Reviewers indicate their concerns around comparable baselines and evaluation metrics were answered in the rebuttal."
            }
        },
        "id": "z2ZUGikQcE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ADsEdyI32n",
        "replyto": "ADsEdyI32n",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1427/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707513095,
        "cdate": 1696707513095,
        "tmdate": 1701465430679,
        "mdate": 1701465430679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a data augmentation technique for Multi-hop Fact Verification which generates diverse counterfactuals via a proposed  Explain-Edit-Generate architecture. The experimental results highlights the potential of the approach in generating linguistically diverse counterfactual data while preserving the logical relationships. The writing requires further polishing (starting from the abstract, but this is minor could be addressed in the current version). Several points were clarified during the discussion phase which I strongly encourage the authors to incorporate into their papers."
            }
        },
        "id": "NaUQ7omyjQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ADHMUuN7CE",
        "replyto": "ADHMUuN7CE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5813/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618748,
        "cdate": 1696707618748,
        "tmdate": 1701465567633,
        "mdate": 1701465567633,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a generalized Left-Corner Transformation which eliminates left recursion in left-corner transformations yet can produce bijectively equivalent grammars. The authors also provide a detailed correctness of their bijevtive equivalence and prove to be close to the “speculation” transformation of Eisner and Blatz (2007). Empirical comparison with the selective left-corner transformation of the work of Johnson and Roark (2000) show that their approach can reduce the size of output grammar significantly over eight languages."
            }
        },
        "id": "sIxdChvwoB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AD0o090nDJ",
        "replyto": "AD0o090nDJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5513/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614409,
        "cdate": 1696707614409,
        "tmdate": 1701465561408,
        "mdate": 1701465561408,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "All the reviewers have agreed that this paper has proposed a very interesting new task to detect the trustworthiness of extracted tuples.\n\nHowever, many concerns have also been raised regarding to the experiments. And it seems that through in-depth rebuttal, these concerns remain to a large degree. \n\nGenerally, this paper anchors a new direction that worths further research."
            }
        },
        "id": "rr5ITdJkiO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "ACogU4OVFK",
        "replyto": "ACogU4OVFK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2991/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553635,
        "cdate": 1696707553635,
        "tmdate": 1701465483573,
        "mdate": 1701465483573,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a scientific claim verification model based on a GNN model operating on causal structure. The proposed model constructs a tree representing causal structure for both claim and evidence, and compares the two structures to predict the label. The proposed model outperforms the current state of the art (T5) on HealthVER claim dataset.\n\nThe shared perspective of the reviewers include:\n* the topic is very interesting\n* the proposed model is well-motivated and the performance is extremely promising\n* the work presented here is not only interesting within the scope of the paper, but it opens the door to research such as more fine-grained structures or in general the topic of reasoning over casual structures in claim verification\n\nThe issues of concern for all reviewers seem to be aspects of the paper or the analysis which can be addressed in a camera-ready, such as implementation using LLMs. In response, the authors seem to have thoroughly addressed each point clearly. \n\nBased on the comments by the reviewers, I would hope the authors dedicate time to improve the overall clarity in the writing in a final version of the paper."
            }
        },
        "id": "wkbzGqvtaG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AAuVIl8Aeo",
        "replyto": "AAuVIl8Aeo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission331/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484904,
        "cdate": 1696707484904,
        "tmdate": 1701465394947,
        "mdate": 1701465394947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes to adapt the embedding layer of multilingual language models for language-specific downstream tasks by averaging existing embeddings of tokens in the target language vocabulary to obtain better initialization. The approach seems to lead to pretty significant improvements while reducing parameter count."
            }
        },
        "id": "QkoLbG5N4t",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AAnYBhWKRv",
        "replyto": "AAnYBhWKRv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3457/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563167,
        "cdate": 1696707563167,
        "tmdate": 1701465498305,
        "mdate": 1701465498305,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper highlights the issue of inconsistent tokenization between the input and output sequences in extractive QA with BPE tokenization (e.g. as done in the BART pretrained model). The authors then suggest a method to ensure consistency between the input and output which improves performance across several datasets. The highlighted problem is important to be aware of as it can be found in many extractive tasks where generative models are applied and the suggested method seems to be a good remedy (although not very exciting). The suggested experiments and results are sound. Given the above, this paper may be a good fit fir either Findings or the main conference."
            }
        },
        "id": "ajdsUu28Q8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "AAYXFyvNbr",
        "replyto": "AAYXFyvNbr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4052/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582049,
        "cdate": 1696707582049,
        "tmdate": 1701465518374,
        "mdate": 1701465518374,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors provide an interesting dataset and benchmarks for representing commonsense simple situations as executable text-based games. \nThe 2 out of 3 reviewers selected \"Strong\" for soundness score (1 selected \"Good\")\nReproducibility has also enough score"
            }
        },
        "id": "zXkCghpbVq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "A6FGmwsH7x",
        "replyto": "A6FGmwsH7x",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3974/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580445,
        "cdate": 1696707580445,
        "tmdate": 1701465515833,
        "mdate": 1701465515833,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a prompt selection method called “SKILL-KNN” that uses in-context learning ability of LLMs and power of prompting LLMs to generate skill based representations. \n\nPros:\n\nMotivation and writing is clear\n \n\nProposed method does not require training/finetuning\n \nExperimental results demonstrate that potential effectiveness of the method \n\nCons:\n\nBetter explanations as to why their proposed embedding methods (based on pretrained embeddings) are not biased by surface natural language features that are unimportant for the target task. \n\nReliance on manually annotated demonstrations to prompt a frozen LLM in order to generate skill-based representations - The authos need to provide some more explanations as to how cognitively loaded this task can be and how easy it is to generalize across domains \n\nThe original work will also benefit from incorporating the additional discussion points/experiments conducted during rebuttal."
            }
        },
        "id": "KY8eJ6N6lE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "A68W11vA8o",
        "replyto": "A68W11vA8o",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission460/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488349,
        "cdate": 1696707488349,
        "tmdate": 1701465399769,
        "mdate": 1701465399769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers appreciated the idea and the results shared in this paper but had mixed excitement scores. I believe that the suggested edits would be helpful to improve the current version of the paper. The authors are encouraged to: \n\n- include the results added during the rebuttal and add a comprehensive analysis,\n\n- better situate their contribution and correct the references as suggested by R1,\n\n- explain how term variations are handled (see comment by R3 after the rebuttal),\n\n- The limitations are the ethics sections should also be moved to the main text.|This work proposed improved methods for the dialogue medical information extraction (DMIE) task, resulting in non-negligible improvements over the current best-performing systems on the corresponding dataset. Specifically, they develop a heterogenous graph modeling procedure to account for interactions between the different extraction types and attention models to augment the input representation to better include dialogue context and relationships between medical terms in the dialogue. Additional ablation studies are performed to elucidate additional relevant issues and add to understanding of the dynamics of the proposed method.\n\n== Quality == The strongest element of the paper is the strength of the primary empirical results. The proposed method makes intuitive sense and is shown to perform well on the DMIE datasets, pushing the SotA forward.  Additionally, the paper is well-structured and easy to understand. Both reviewer 4JHj and DNoq suggest additional experiments that were provided during the rebuttal period. I would recommend including some of these results in the camera ready version (even if in an appendix if necessary) as they further support the efficacy of the proposed method.\n\n== Clarity == Overall, the paper is well-motivated, well-structured, and easy to understand. While it would benefit from general polishing of the writing, it is well-written overall, but i recommend adding the additional results, references, and discussion as recommended by the reviewers.\n\n== Originality == The authors claims methodological innovation, but it is really a case of applying more recent methodologies to this particular application. While this is still important (and they got it to work), they are not describing new methods that are easily adaptable to other problems, etc. This, it is a targeted innovation.\n\n== Significance == Medical information extraction is an important and potentially impactful problem. From an application perspective, the proposed method achieves strong empirical results, pushing the SotA forward and thus will minimally be used as a baseline for future work. However, it is a domain-specific improvement using existing methods and doesn't make algorithmic advances that would be useful to the broader community."
            }
        },
        "id": "3SjkRU3n85",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "A2oBdekFgv",
        "replyto": "A2oBdekFgv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5537/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614821,
        "cdate": 1696707614821,
        "tmdate": 1701465561948,
        "mdate": 1701465561948,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes MaNtLE, a model-agnostic natural language explainer which generates explanations for subsets of examples in structured classification tasks. According to the initial reviews, this paper conducts comprehensive experiments (studying explanations in several settings and performing both automatic and human evaluations). Also, the method is efficient at inference time without requiring input perturbation. However, there were some concerns raised, especially on the evaluation part (e.g., the missing NLE baselines and the unfair comparison to the baselines). During the rebuttal phase, the authors well addressed several questions and concerns of the reviewers. In particular, the authors argued that thousands of synthetic tasks MaNtLE saw (which seem unfair to other baselines) were used for pre-training MaNtLE, which happened only once, and this pre-trained model will be released upon publication so public users can leverage it in an efficient way. The authors also presented additional results comparing MaNtLE to a WT5-style model, served as an NLE baseline. Both models were pre-trained on the same number of synthetic tasks, and MaNtLE still outperformed WT5 in terms of simulatability. It has to be noted that, according to the authors' argument, MaNtLE and WT5 are not directly comparable by nature as the former explains a set of examples while the latter explains an individual example. The newly presented results were possible due to a simple aggregation method the authors used to enable the comparison."
            }
        },
        "id": "gijNM8aPyA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "A0xVOahTiw",
        "replyto": "A0xVOahTiw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2369/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540655,
        "cdate": 1696707540655,
        "tmdate": 1701465463028,
        "mdate": 1701465463028,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This adversarial-attack paper describes how perturbations can significantly affect accuracy when well-known models like RoBERTa, BART, FLAN-T5, and GPT do natural-language inference tasks, showing that there is a lack of logical consistency in these models.\n\nPros:\n- The problem statement is well-focused, and the approach and scope is appropriate for a short paper.\n- There is sufficient information to reproduce this work; the data and source code will be openly available.\n- There are extensive and detailed experiments.\n\nCons:\n- The paper uses an LLM to generate the perturbations, which may cause some to doubt their quality; but the authors assessed the quality of a sample of these generated perturbations and found no hallucinations.\n- The review copy of the paper was somewhat hard to follow in places due to its heavy reliance on the appendix, which was itself hard to follow in places due to the formatting. Restructuring and/or reformatting somewhat may improve readability."
            }
        },
        "id": "GUD4AoyYUl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9zZWPEo8et",
        "replyto": "9zZWPEo8et",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5023/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605297,
        "cdate": 1696707605297,
        "tmdate": 1701465547611,
        "mdate": 1701465547611,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a novel method, \"Decomposed Prompt Tuning via Low-Rank Reparameterization,\" aiming to enhance prompt tuning efficiency for large pre-trained language models. By decomposing the soft prompt into low-rank matrices, the proposed approach reduces trainable parameters in the prompt matrix. The approach is experimentally validated using the SuperGLUE benchmark across various-sized T5 models.\n\nGenerally, the reviewers responded positively to the parameter reduction achieved by the method while maintaining similar or better performance. Additionally, the use of low-rank reparameterization seems well motivated in this domain.\n\nThere were concerns raised about the lack of comparison to other adapter-based approaches like LoRA. In the rebuttal, the authors argue that these approaches are less parameter efficient than prompt tuning. However, this argument feels a bit weak to me for multiple reasons. First, the authors compare against Fine-Tuning, which requires training the entire model (many more parameters than a LoRA). Second, and more importantly in my mind, the motivation to compare different parameter efficient fine-tuning methods is to see the trade-off across the additional parameters, computational cost, and performance of each method.\n\nFor instance, while LoRAs may require more parameters, all of these methods likely contribute to a fraction of the storage and computation requirements of the larger LLM they are augmenting. This makes the reduction in memory (even a 10x reduction) harder to quantify without additional results to better contextualize it. The authors may include some additional empirical observations (and comparison to other existing methods) along these lines to better show how this work is positioned in this space."
            }
        },
        "id": "9Exlm3icKA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9z2yznFVw5",
        "replyto": "9z2yznFVw5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1488/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515567,
        "cdate": 1696707515567,
        "tmdate": 1701465432310,
        "mdate": 1701465432310,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper benchmarks prompting for zero-shot TOD using DST and Policy prompt mechanisms. Using ChatGPT, the paper demonstrates SOTA performance on zero-shot TOD using schema-guided prompting. \n\nThis is primarily a benchmarking paper and does not introduce any core technical innovation. However, reviewers appreciated the overall prompting framework. There were some concerns around complexity of the overall prompting setup, towards which, the authors clarified on the necessities of the modules in certain scenarios. \n\nThe paper argues schema-guidance to be better than few-shot prompting. As such, discussing about the paper - \"Show, Don’t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue\" might be useful."
            }
        },
        "id": "x9sI2m4pg7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9s7QooDInQ",
        "replyto": "9s7QooDInQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission245/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482851,
        "cdate": 1696707482851,
        "tmdate": 1701465392711,
        "mdate": 1701465392711,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper address the problem of instability of prompt tuning approaches. The paper addresses an important problem as prompt-tuning is a very popular choice to fine-tune LLMs for specific tasks without incurring in prohibitive costs. The author propose an approach to introduce perturbations in the input or embedding spaces in order to stabilize the model. The reviewers agree that the idea of the paper is sound and that the contribution is relevant to the problem. The experiments proposed by the authors are comprehensive and demonstrate that the approach is working to reduce the instability of prompt tuning training."
            }
        },
        "id": "jffHN6Mzn6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9rWqOgvGpc",
        "replyto": "9rWqOgvGpc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission282/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483697,
        "cdate": 1696707483697,
        "tmdate": 1701465393624,
        "mdate": 1701465393624,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a multi-view curriculum learning framework, which include characterizing data samples using linguistic complexity measures and weighting these samples during model training based on their complexity scores. The paper also explores the impact of linguistic complexity measures on various NLP tasks and provides insights into how different linguistic indices affect model performance at different training stages. Reviewers have found the proposed approach well motivated, interesting and innovative; therefore, I recommend acceptance of this paper. Please also address the comments by reviewers on limitations."
            }
        },
        "id": "KKpLY0XJxr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9r8WwpJv7M",
        "replyto": "9r8WwpJv7M",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4477/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707590484,
        "cdate": 1696707590484,
        "tmdate": 1701465532555,
        "mdate": 1701465532555,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper effectively argues the need for decoupling ethical policies from large language models (LLMs) training, presenting an innovative framework for defining ethical policies for LLM prompts. It also delivers a novel dataset of moral dilemmas. However, there is hesitation around whether the model is truly performing ethical reasoning, despite evidence that it's output aligns with expected responses to dilemmas. The second review points out issues with clarity and some methodological shortcomings, particularly a lack of automatic policy selection. Authors presents a good effort addressing these concerns in their rebuttals which has helped improve understanding of some shortcomings and objectives of the research."
            }
        },
        "id": "izB4cFh7dR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9qydIw5ux1",
        "replyto": "9qydIw5ux1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3671/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567389,
        "cdate": 1696707567389,
        "tmdate": 1701465506147,
        "mdate": 1701465506147,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All the reviewers agree that the paper presents an interesting method and that it is well written. The reviewers found some of the ideas novel and did not have many criticisms. \n\nOne of the reviewers questioned the use of a commercial machine translation, but without evaluating its performance. In the rebuttal the authors carried out a small scale evaluation showing that the errors introduced by MT are minimal and likely to have limited impact on the results.\n\nThe lack of comparison with baselines was  criticised by one reviewer because without them it is difficult to know how good the methods are. The lack of baselines is justified by the authors by the novelty of the task. However, they could have tried at least some naive baselines. \n\nOverall, the reviews indicate that this is a strong paper which contains some novel ideas especially given that it is a short paper which cannot pack too much information."
            }
        },
        "id": "ULGBfsalXP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9edEJfhOFL",
        "replyto": "9edEJfhOFL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission510/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489579,
        "cdate": 1696707489579,
        "tmdate": 1701465401644,
        "mdate": 1701465401644,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work introduces a method for enhancing paraphrase generation using quantized prompts. The method employs a vector-quantizer to encode prompts and generate paraphrases from a codebook of prompt templates that are updated with a k-mean algorithm. However, the novelty of the method and the evidence for the claims about the syntactic information captured by the learned prompts are questionable. Overall, this is important research top in NLP. It also provides comprehensive experiments."
            }
        },
        "id": "D2f5EzwnAx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9cALtYoAEy",
        "replyto": "9cALtYoAEy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission271/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483496,
        "cdate": 1696707483496,
        "tmdate": 1701465393365,
        "mdate": 1701465393365,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates the phenomenon of zero-shot neural machine translation (ZS-NMT) performance variations across different language directions and seeks to understand the underlying factors. It introduces a new dataset, EC40, and conducts extensive experiments involving 40 languages, focusing on the influence of English-centric translation capacity, vocabulary overlap, and linguistic properties. \n\nPros:\n\t\t\n* Novel Dataset and In-Depth Analysis: The introduction of the EC40 dataset is highlighted as an important contribution to the paper. The extensive experiments conducted on this dataset offer valuable insights into ZS-NMT performance variations.\n\t\t\n* Thorough Investigation: The paper explores the factors contributing to ZS-NMT performance variations comprehensively, considering English-centric translation capacity, vocabulary overlap, and linguistic properties. This in-depth analysis provides a holistic understanding of the issue.\n\t\t\n* Identification of Off-Target Issue: The paper suggests that the off-target issue is a symptom rather than the root cause of inadequate ZS-NMT performance, offering a novel perspective on the problem.\n\n\nCons:\n\t\t\n* Concept Clarification: Reviewer 1 points out that some key concepts used in the paper, such as \"spurious correlation,\" \"off-target issue,\" \"correlation and regression analysis,\" and \"sentence-level off-target rate,\" are not properly introduced, making the paper challenging to follow for non-experts.\n\t\t\n* Metric Choice: Reviewer 2 questions the use of string-matching metrics (Bleu, Chrf++, and SpBleu) and suggests considering the use of COMET, although it's noted that COMET may not cover all the languages involved in the study.\n\t\t\n* Lack of Novel Phenomena: Reviewer 3 notes that while the paper delves into ZS-NMT variations, the three identified factors are intuitive and have been observed in previous works. There is a need for more exploration of the relationship between high variation and low average ZS performance.\n\nIn summary, the paper presents a comprehensive analysis of ZS-NMT performance variations, supported by a new dataset, EC40. While it makes valuable contributions to the field, addressing the need for concept clarification, handling appendix content, and further exploring the relationship between high variation and low performance could enhance the paper's quality and impact."
            }
        },
        "id": "Yzi00YhFqW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9V0M45lJAs",
        "replyto": "9V0M45lJAs",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1890/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529505,
        "cdate": 1696707529505,
        "tmdate": 1701465445955,
        "mdate": 1701465445955,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new method (a tree topological probe) to probe syntactic/hierarchical structure learned by BERT in an unsupervised manner. The authors use their findings to also enhance fine-tuning performance of BERT.\n\nPros: \n- Reviewers found the paper has strong potential and addresses an important problem in the probing area.\n- They found the proposed method interesting and appreciated the theoretical contributions.\n- They did not have any critique of the method itself, though this may be due to a lack of understanding (see below).\n\nCons: \n- Multiple reviewers felt that only experimenting on BERT-Large was a limitation. During the author response, the authors provided more results on RoBERTa-Large and Albert-Large to alleviate this concern.\n- [**important**] Multiple reviewers highlighted that the paper was lacking clarity & very difficult to read. They felt the paper was missing a core narrative structure to motivate the work, and that the necessary changes would take more than a minor revision to fix.\n\nReviewers had some disagreement over scores. Taking all of the above into account, I have adjudicated in favor of the 2 reviewers in agreement (but with overall lower scores). I think the paper has some very interesting ideas and seems generally sound, but certain parts are confusing & may need more clarification."
            }
        },
        "id": "gfydgGOHUy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9S0MFwEkc3",
        "replyto": "9S0MFwEkc3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission178/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707481130,
        "cdate": 1696707481130,
        "tmdate": 1701465389836,
        "mdate": 1701465389836,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes Parrot, a training method to improve narrative reading comprehension, where the reading comprehension task is focused on a story. \n\nTo train, Parrot finds two narratives of the same story, generates questions about the first, then trains the model to answer the questions using the second narrative as context. \n\nThe model is pretrained on parallel narratives from NarraSum, then evaluated \"zero-shot\" on d FairytaleQA and NarrativeQA. Results show improved results compared to Vicuna-13B and ChatGPT. Results are still worse than fine-tuning on the training set of each dataset (which is not surprising given the zero-shot evaluation), but get closer. \n\nDuring the discussion period, the authors augmented their ROUGE-based evaluation with additional human evaluation, which was consistent with the automatic metric.\n\nOne thought for future work: the automatic question generation had a lot of moving pieces with engineered rules, makes, templates, question types, named-entity generators... etc. While this seems to work, I would expect all of this machinery to be replaceable with prompting for an LLM, making the method simpler and more robust."
            }
        },
        "id": "oDNilohKFc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9RugvdmIBa",
        "replyto": "9RugvdmIBa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4476/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707590410,
        "cdate": 1696707590410,
        "tmdate": 1701465532570,
        "mdate": 1701465532570,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an ILP formulation for selecting exemplars -- for prompting LLMs in a task of hybrid (text+tables) QA. The idea is interesting and novel. Moreover, it allows for a better control of selected exemplars in terms of their diversity, length/number or other constraints. The code will be publicly available. All the reviewers agree that this contribution is sound and exciting.\n\nHowever, there are some presentation issues. The main issue is the appendix (reviewers #1 and #3): it is very large and contains essential information. It should be reorganized to make the paper self-sufficient and easy to follow. Minor presentation issues (easily fixable) have been noted by the reviewers."
            }
        },
        "id": "hOHyhVybDH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9RFBVLwiOn",
        "replyto": "9RFBVLwiOn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3578/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565569,
        "cdate": 1696707565569,
        "tmdate": 1701465502239,
        "mdate": 1701465502239,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper describes the creation of a large-scale dataset for adverse Drug Event Extraction, within the biomedical domain, encompassing multiple attributes. The dataset is then used employed in an experimental evaluation of some Language Models (LMs). The results show that extracting structural information from the biomedical dataset is still challenging. The study also shows the potential high impact for aiding pharmacovigilance. Reviewers and authors constructively engaged during the rebuttal/discussion phase leading to an potential enhancement of the work. It is highly recommended that the authors integrate reviewers comments.\n\n**Pros**\n- the topic addressed is highly relevant for real-world application in pharmacovigilance reporting, and timely;\n- the large dataset produced is valuable for research in this field;\n- the methodology adopted is robust and well defined;\n- readily applicability in the pharmacovigilance reporting applications.\n\n**Cons**\n- the breakdown performance for each attribute is not provided in the paper, but authors seem to be ready to include such details in the revised paper;\n- the motivation for framing the issue as a sequence-to-sequence task is weak and should be better motivated.\n- the definition of the evaluation metric appears to be unclear"
            }
        },
        "id": "vM0WX37twG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9OPtgQlxVD",
        "replyto": "9OPtgQlxVD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2766/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548940,
        "cdate": 1696707548940,
        "tmdate": 1701465475537,
        "mdate": 1701465475537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In this work, the authors tackle the longstanding CL task of frame identification (matching of words in context with semantic frames they induce) and propose an approach based on reshaping of a representation space to relfect the topology of frame-based similarity.\n\nThe approach, in principle very similar to approaches that convert language models into sentence encoders, consists of: (1) dual encoding (separate encoding of the context of the target word and the sentential example(s) that instantiate the frame); (2) contrastive learning (contrastive loss effectively reshapes the encoder so that the proximity of vectors of sentential instances reflects their alignment in terms of the semantic frame they correspond to, i.e., which they induce); and (3) coarse-to-fine curriculum learning in which the difficulty of negative examples is gradually increased (i.e., one starts with randomly selected, and likely very different semantic frames as negatives in early stages of training and then progresses to use semantically closer frames as negatives in later stages). The authors show empirically that all three components are important for the reported performance gains.   \n\nAll reviewers agree that the work is sound and identify no errors in evaluation protocols. The proposed approach is meaningful and yields gains, although the gains are not particularly large. The reviewers -- and I fully support this view are somewhat more skeptical of the methodological novelty: all three components, namely, the bi-encoding, contrastive learning, and curriculum with increasingly difficult negatives are very widely and commonly used techniques, widely used in sentence-level classification and ranking tasks. From this point of view, the work can be seen as an application of this well-known framework to the task of frame identification (where instances are also sentences or quasi-sentences). \n\nOverall, I find this work to be sound and valuable for the CL community and the narrower community that focuses on frame identification, but not particularly exciting for the wider NLP audience."
            }
        },
        "id": "fQlj6BfGSo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9NGR4GdLII",
        "replyto": "9NGR4GdLII",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3924/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579428,
        "cdate": 1696707579428,
        "tmdate": 1701465514288,
        "mdate": 1701465514288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a significant contribution to the field of dialogue systems by introducing a novel approach to constructing multi-session dialogues and providing a machine-generated corpus. The resources and insights offered in this paper can be expected to facilitate future studies in this area."
            }
        },
        "id": "gRB6sUrsj0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9LPJK81xy1",
        "replyto": "9LPJK81xy1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1573/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518045,
        "cdate": 1696707518045,
        "tmdate": 1701465434854,
        "mdate": 1701465434854,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes an adapter-based method for aligning pre-trained vision and text encoders (ViT and BERT respectively) by inserting gated Adapter units within the frozen encoders and training them with a contrastive loss. \n\nTwo reviewers are positive about the paper, while one reviewer is negative about the paper. Overall, reviewers found that (1) the paper does a good job of applying ideas from parameter-efficient fine-tuning tuning methods to CLIP training; (2) paper is well written and structured; (3) experiments are comprehensive and ablation study is solid. The reviewer who gave a low score mainly complained about (1) lack of novelty; and (2) only test using the proposed method to train CLIP on two languages. \n\nGenerally, the AC is positive about the paper, and agrees that the negative comments did not hold, and that reviewer also did not participate in the discussion to back up his scores."
            }
        },
        "id": "GddaZwJglO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9K1urVN7ti",
        "replyto": "9K1urVN7ti",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1476/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515257,
        "cdate": 1696707515257,
        "tmdate": 1701465431918,
        "mdate": 1701465431918,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new method for conversational recommendation, which integrates recommendation and generation with gated knowledge distillation.\n\nThe authors carefully addressed most concerns (e.g., missing crucial baselines and unconvincing experimental results) from the reviewers in the rebuttal. The authors are encouraged to add all the new experimental results in their paper if accepted. The authors should also make it extremely clear that they have revised the evaluation metric. It would be nice to add results corresponding to the original metric in the appendix as well."
            }
        },
        "id": "NTe6gHqsZd",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9HjxuDwTNG",
        "replyto": "9HjxuDwTNG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4759/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597936,
        "cdate": 1696707597936,
        "tmdate": 1701465540928,
        "mdate": 1701465540928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a new dataset of multimodal storytelling (images from the story, the text itself of the story, and sound effects/music that are part of the story. From the demo page, it seems as though the sound is predominantly music). The dataset is quite large at 27k stories with 19 images per story and 984 hours of sound, and is derived from Hollywood movies. The paper also presents some benchmark tasks for the dataset such as retrieval and audio generation from image/text.\n\nPros:\nThe dataset is novel in its purpose and scope, and is to be made publicly available\n\nCons:\n-Some of the technical details for how similarity scores are not clear (see comments by reviewer Lmj8)\n-The experiments don't include results from pre-trained models like CLAP or wav2Clip\n-It is not clear how the proposed dataset offers any advantages over a dataset of videos + captions/narrations\n-It is also not clear why speech/dialog was removed from the soundtracks. The author's response to reviewer sX7X on this point doesn't really explain what is more compelling about the music/background sound with the movie dialog removed, compared to the full original audio"
            }
        },
        "id": "SYXbsBrwxk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9HbJGoe4a8",
        "replyto": "9HbJGoe4a8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4917/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601537,
        "cdate": 1696707601537,
        "tmdate": 1701465544766,
        "mdate": 1701465544766,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces methods for creating synthetic training data to improve opinion summarization, addressing both general and aspect-based summarization. The paper demonstrates improvements over strong baselines in various evaluation metrics, supported by human assessment. However, concerns were raised in the reviews about the introduction of numerous hyperparameters, potential limitations in generalizability, the absence of a baseline comparison with Coop in unsupervised general opinion summarization, and the need for a more comprehensive discussion of language model-based methods in the post-ChatGPT era. Additionally, issues related to presentation clarity, ambiguities, and details in various sections were noted, along with recommendations for a fairer comparison and a clearer motivation regarding faithfulness issues in other synthetic datasets. Despite these issues, reviewers were satisfied by most of the responses of the authors during the rebuttal period."
            }
        },
        "id": "aDGAtCDMmV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9GxP2Kw8IC",
        "replyto": "9GxP2Kw8IC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5760/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618091,
        "cdate": 1696707618091,
        "tmdate": 1701465566538,
        "mdate": 1701465566538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the problem of multi-modal rumor detection on social media, especially focusing on predicting with incomplete modality information. The method is based on constrastive learning for fusing the features across modalities and knowledge distillation from a teacher model trained for both modalities to improve performance when modalities are missing.\n\nThe reviewers appreciated that the paper introduced a new method for this task, especially dealing with the incomplete modality problem. Some of the reviewers also noticed that paper is well written and ablation experiments are provided.\n\nAll reviewers considered as one of the key negative points to use more than a single data set for each of the two languages, and ideally larger ones. The authors have provided additional results in the response on more data sets."
            }
        },
        "id": "svEq7N3FJz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9F6h0oIYsP",
        "replyto": "9F6h0oIYsP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission502/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489341,
        "cdate": 1696707489341,
        "tmdate": 1701465401370,
        "mdate": 1701465401370,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new interactive system, WEAVER, to help/guide NLP model testers. The system generates concepts from a large language model (LLM) that are diverse but relevant to a task. The idea is that the generated concepts can provide additional tests for modeling testing. The authors' experiments show that WEAVER can help testers explore similar topics.\n\n[Paper Clarity]: Both reviewers rtMG and Mdj9 raise concerns about the paper's clarity. In particular, reviewers rtMG have issues with the lack of technical detail around how the proposed system works. Whereas reviewer Mdj9 concerns are with regard to details not mentioned in the paper around the validity of new concepts, the authors's choice to use ConceptNet, and the authors' optimization objective. \n\n[Traditional Software Engineering Testing]:\nBoth reviewers hoyj and Mdj9 raise concerns about how the proposed approach is related to traditional software engineering testing that provides quick testing/error analysis."
            }
        },
        "id": "C3uoM12SH3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9EYaUfyRYk",
        "replyto": "9EYaUfyRYk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2034/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532628,
        "cdate": 1696707532628,
        "tmdate": 1701465451265,
        "mdate": 1701465451265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new solution to gather negative synthetic data to train an LM for zero-shot commonsense QA. For this, they use conceptualization/ abstraction over Commonsense KB triples to enhance knowledge coverage and in guiding an effective negative sampling strategy to mitigate false-negatives in the generated synthetic data. Their method gets empirical gains over data distilled directly from LLMs and they provide insightful analysis for on-the-fly generated CKB. This is a useful framework with good results, backed by good experimental analysis.\n\nIt would be interesting to see the generality of their conceptualization based pretrained LM by studying if it applies to commonsense generation tasks such as script generation (it should be within the scope as they use ATOMIC). For generalization of the approach w.r.t. other KBs, please include the new results in the camera ready. It would also be interesting to see how conceptualization can be used to guide LLM prompts.\n\nIn summary, this is a useful data augmentation technique that can yield good results on other tasks in the future (remains to be seen)."
            }
        },
        "id": "a5YbpwzUU8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9EYS2EEqFq",
        "replyto": "9EYS2EEqFq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission517/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489679,
        "cdate": 1696707489679,
        "tmdate": 1701465401822,
        "mdate": 1701465401822,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a semi-parametric approach, called kNN-CM for text classification tasks by integrating a non-parametric memory for neighborhood retrieval to a pre-trained parametric text classifier to explore the importance of querying neighbors to solve classification tasks. Reviewers agree that the paper is well-written and appreciate the comprehensive experimental evaluation conducted in the paper. The experiments cover a diverse set of classification tasks and provide persuasive results.\n\nHowever, reviewers express concerns about the contribution of the approach, as kNN-based methods have been studied extensively in the context of language models and machine translation. I see the authors already addressed these concerns during the rebuttal phase. I believe integrating these explanations and providing more insights into the unique aspects of their kNN-CM approach to clarify the contribution will address the potential concerns in this direction. Another point that lacks clarity and has not been raised by the reviewers is whether the applicability of kNN-CM is limited to masked language models or if it can also be employed with autoregressive models. I think clarifying this point is also important.\n\nOne additional aspect worth noting is the authors' mention of potential extensions for the proposed approach in conversation modeling and continual learning. However, the paper does not delve into the specifics of how kNN-LM could be applied in these contexts. I would recommend either providing more detailed insights on these potential extensions or considering the removal of this reference if further elaboration is not possible. This would help to maintain clarity and avoid raising expectations without sufficient follow-through."
            }
        },
        "id": "hFR6wndanV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9BuTdxSfIO",
        "replyto": "9BuTdxSfIO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4726/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597218,
        "cdate": 1696707597218,
        "tmdate": 1701465539825,
        "mdate": 1701465539825,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes cross-modality data augmentation techniques to tackle the data scarcity issues while training end-to-end sign language translation models. In particular, the paper proposes to use cross-modal mix-up and cross-modality knowledge distillation. The experimental results demonstrate the superiority of the proposed method.\n\nThe reviewers liked the method description and analysis. They asked several qualitative and quantitative evaluation questions, but the authors clarified them during the rebuttal. Finally, reviewers pointed out that while the techniques are novel in sign language translation, they are well-studied in speech translation, limiting their excitement."
            }
        },
        "id": "tqFK3hEGxC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "9Ax0pyaLgh",
        "replyto": "9Ax0pyaLgh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission421/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487221,
        "cdate": 1696707487221,
        "tmdate": 1701465398335,
        "mdate": 1701465398335,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Evaluating machine-generated image captions against reference captions has proven to be a difficult task. This paper proposes simply asking LLMs to score machine-generated captions versus reference captions, and shows that LLM performance aligns closely with human judgments.\n\nPros:\n- Focused problem statement, appropriate scope for a short paper.\n- Much better alignment with human judgment than previous metrics. Evaluating image captioning is an important task, and this work brings a methodology that outperforms existing metrics by a significant margin when it comes to aligning with human judgement.\n- These findings are also relevant for other text evaluation challenges; the approach here is could be generalized beyond image captioning in future work (to be clear this is not necessary for this short paper). This is exciting.\n- Well-written, easy to follow.\n\nCons:\n- Using an LLM in the loop can be costly, so this metric may not be something that can practically be applied at training time. The authors  argue, sufficiently convincingly, that this will change over time and indeed is already changing with recent open-source models that have become available after the EMNLP deadline (and ACL's contemporaneous work policy).\n- The authors also propose having the LLM explain its scores, but it's unclear if these explanations are trustworthy. This is ultimately somewhat irrelevant in the sense that the alignment with human judgment is what matters, however, as long as the explanations are not relied upon for anything.\n- Depending on whether the LLM used to score is local or black-box+API-based, reproducibility of this metric may be concerning; for example, the model underlying an API call to a black-box model may change, even during scoring for a large set of captions, which could wreak havoc upon the results. This concern can be mitigated by using a stable model for every run."
            }
        },
        "id": "sBAuLgHjDP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "99msyVXHEq",
        "replyto": "99msyVXHEq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3946/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579914,
        "cdate": 1696707579914,
        "tmdate": 1701465515031,
        "mdate": 1701465515031,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper shows the limitations in progressive training for speech translation (where external text translation data is used during fine-tuning), and then proposes an approach to address this shortcoming. The reviewers agreed that the experiments were sound in showing the problems associated with progressive training as well as the improvements that can be achieved by the new approach. However, the reviewers also had concerns that further analyses could have improved some of the claims in the paper. Some reviewers also had concerns about the novelty of the work, leading to overall ambivalent excitement. Nevertheless, the overall soundness is good."
            }
        },
        "id": "PAJW07bwCL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8zQ77tPTMR",
        "replyto": "8zQ77tPTMR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5542/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707614908,
        "cdate": 1696707614908,
        "tmdate": 1701465562074,
        "mdate": 1701465562074,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a membership inference attack method called MoPe (Model Perturbations Attack) to detect if a certain text is in the training data of a pretrained LLMs. The proposed method assumes access to model parameters. \n\nPros: \n* The authors demonstrate that the proposed method is very effective in identifying if a given text is in the training data with a high confidence, compared to loss-based attacks. \n* The paper is well written and very timely. I believe this direction of work will lead to our improved understanding of security, safety and trust related issues with LLMs. \n\nCons: \n* The reviewers have identified two major weaknesses in the paper: 1) The method assumes access to model parameters, and 2) the paper does not provide or discuss potential solutions or mitigation strategies to address the risks identified in this paper. \n\nThe authors have responded to both limitations adequately in their rebuttals. Despite these two limitations, the reviewers have agreed that the proposed setup is worth exploring and the paper makes a valuable contribution which will be of interest to the community and to the development of LLMs."
            }
        },
        "id": "hAQDubEb9o",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8xyd9i1XLb",
        "replyto": "8xyd9i1XLb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2036/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532714,
        "cdate": 1696707532714,
        "tmdate": 1701465451293,
        "mdate": 1701465451293,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a method for multi-hop knowledge base question answering. The idea is to use information from related relations and entities to improve reasoning over the KG. The paper provides experimental evidence and analysis supporting their method. However, reviewers also pointed out some issues regarding important baselines & comparisons that were missing."
            }
        },
        "id": "TD7OWiYF99",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8uSB79mZks",
        "replyto": "8uSB79mZks",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5428/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707612806,
        "cdate": 1696707612806,
        "tmdate": 1701465559158,
        "mdate": 1701465559158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a new method called InstOptima, which treats instruction generation as a multi-objective optimization problem. InstOptima randomly selects an operator (definition formulation/definition crossover/example mutation/example crossover) to evolve the instruction by feeding the objectives back. The reviewers did raise a few concerns and during the rebuttal period, the authors provided further explanations and experimental results, which solved the concerns to some extent. A critical though non-technical issue is that once accepted, the authors need to carefully reorganize the content into 5 pages so as to come up with a self-contained version."
            }
        },
        "id": "KFLwaPMFV9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8oy8hUeem9",
        "replyto": "8oy8hUeem9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5874/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707620356,
        "cdate": 1696707620356,
        "tmdate": 1701465568818,
        "mdate": 1701465568818,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers recognized the contribution of this paper to low resource NER settings, and the efficacy of the proposed data augmentation on NER benchmarks. The reviewers also suggested additional baseline comparisons and related work which provide important comparisons for the specific application, which were added during the rebuttal discussion. Reviewers also had several questions about the methodology, scalability, and more precise description of the approach which should be addressed in revision. This paper will likely be of moderate interest to the EMNLP community with contributions to few-shot learning and data augmentation strategies."
            }
        },
        "id": "e2QqlrNIMb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8mJujVetQv",
        "replyto": "8mJujVetQv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1164/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506375,
        "cdate": 1696707506375,
        "tmdate": 1701465422313,
        "mdate": 1701465422313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method for more efficient Transformer architectures by employing token pruning and token combining.  It uses a token importance function, but rather than rely directly on it, uses fuzzy set membership to help determine a token's relative importance in relation to the distribution of importance scores.\n\nLike other reviewers, I had trouble understanding the motivation for the fuzzy logic methods in this approach.  The explanations offered in the rebuttal were somewhat helpful in this regard, but they were not properly reflected in the paper itself, meaning the lack of clarity is a real issue.\n\nThe main issue seems to be the inability to use the importance score directly.  Fuzzy logic is introduced to compensate for a weakness in the importance score, but it would have been nice to see a comparison with other methods that compute this score differently, thereby obviating the need for a mitigation in the first place.  This leads into the valid criticism that more comparison in the results with other sparse attention methods would have better contextualized this work and shown *its* importance."
            }
        },
        "id": "hWZGjqs9h0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8l2m7jctGv",
        "replyto": "8l2m7jctGv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4120/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583488,
        "cdate": 1696707583488,
        "tmdate": 1701465520970,
        "mdate": 1701465520970,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Important data generation pipeline using LLMs. Intelligent use of search API to aid the generation of information-seeking dialog. It'd be valuable to community if the framework/pipeline/code is open-sourced for reproduction and further extension."
            }
        },
        "id": "TMv1LWLVOo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8iB0FJmOfV",
        "replyto": "8iB0FJmOfV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission388/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486275,
        "cdate": 1696707486275,
        "tmdate": 1701465397042,
        "mdate": 1701465397042,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a novel alignment learning framework that relies on synthetic feedback rather than extensive human annotations or pre-aligned large language models. By utilizing reward modeling with synthetic feedback and reinforcement learning, the proposed method shows improved performance in alignment benchmarks compared to recent open-sourced models. The reviewers are generally positive towards the contribution of this paper. I therefore would recommend acceptance of this paper to the main conference."
            }
        },
        "id": "PUhj0kReIB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8gYRHspcxK",
        "replyto": "8gYRHspcxK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2246/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537771,
        "cdate": 1696707537771,
        "tmdate": 1701465458896,
        "mdate": 1701465458896,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a query plan language to decompose sql queries. The problem is text-to-sql translation / parsing. The method is sound. I find the decomposition strategy to be sound, and it could potentially bring some interpretability down the road. The results demonstrate the model is effective.\n\nThe reviewers pointed out a few suggestions to strengthen the paper: running experiments with more (and commonly used) datasets, and more importantly, analysis describing what kind of queries their method is most effective with (which they shared in the discussion, so it would be easy to add it to the camera-ready version).\n\nNot running experiments with papers published in 2023 is not negative, as they are considered contemporaneous. This is minor and easily fixable, but claiming p-value < 0.06 goes against the norm. The results are just not statistically significant (and that is probably fine). Finally, the authors are committing to release their implementation, so it is hard to justify anything but a high score for Reproducibility."
            }
        },
        "id": "kOA451YKq3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8e9aFrksRq",
        "replyto": "8e9aFrksRq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2104/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534524,
        "cdate": 1696707534524,
        "tmdate": 1701465454062,
        "mdate": 1701465454062,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Overall, the paper is acknowledged for its technical soundness, clear explanations, and insightful interpretation of results. The reviewers suggest providing more details about the dataset, exploring the use of meta-temporal features, addressing ethical and societal implications, and expanding the discussion to encompass other languages."
            }
        },
        "id": "DHElPMGS5A",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8cRL5fPwUI",
        "replyto": "8cRL5fPwUI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3854/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707577872,
        "cdate": 1696707577872,
        "tmdate": 1701465511855,
        "mdate": 1701465511855,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper delves into verbal humor detection, examining English humor datasets and the performance of various models on them. Nine datasets from previous literature were collected, and an additional one was created by the authors for out-of-domain data testing. These datasets were combined to diversify humorous texts, enhancing model transferability. RoBERTa-based models were compared with 3rd-party classifiers like ColBERT and large language models (LLMs) in zero-shot scenarios. Results indicated that RoBERTa models excel when trained and tested on identical data, especially on the combined dataset. LLMs, however, showed competitive results across datasets without merely memorizing jokes. The authors believe their findings can advance computational humor and have made all datasets publicly accessible.\n\nThe paper offers a thorough exploration of English verbal humor detection datasets and evaluates current NLP model performances on them. It provides valuable insights into model performance based on word distributions in training data, suggesting models might predict based on words rather than genuine humor understanding. The datasets used have been cleaned, unified, and made publicly available. The authors have standardized prior humor detection collections, enabling method comparison, and have shared this standardized collection. The paper is detailed in its implementation and conducts extensive experiments with multiple baselines. The contribution of a new dataset and standardized splits for existing ones is seen as beneficial for practitioners. The paper's evaluation of zero-shot LLMs is also deemed valuable for understanding their joke comprehension capabilities.\n\nThe main weaknesses have been discussed during the rebuttal phase and are listed below.\nThe paper, while investigating NLP model performance on humor datasets, presents seemingly obvious results, such as fine-tuned models performing best on their specific datasets. The additional dataset curated by the authors lacks annotations, limiting its utility for humor detection. There's a disconnect between the paper's content and its mention of humor detection in conversational scenarios. The research is limited in its experimentation, focusing only on two classifiers (RoBERTa and Naïve Bayes) and two LLMs (Flan-U2 and chatGPT). The paper could benefit from a deeper exploration into the linguistic or theoretical facets of humor. The introduction lacks depth, and the experiments could be more detailed. The manuscript's presentation of results is mainly numerical without a comprehensive interpretation, making it less engaging for readers."
            }
        },
        "id": "XIEL2JyzZl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8Y9G7579DP",
        "replyto": "8Y9G7579DP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4017/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581272,
        "cdate": 1696707581272,
        "tmdate": 1701465517041,
        "mdate": 1701465517041,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper pointed out problems with current datasets and 2 benchmarks on visually rich document recognition tasks (NER, EL< ROP). The authors propose a new method for token path prediction and explain their approach and revised datasets for those tasks. The proposal of a reading order invariant labeling scheme TPP for VDU is well motivated and is novel,"
            }
        },
        "id": "WmqyMaFAQG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8WXwPUBFEb",
        "replyto": "8WXwPUBFEb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1698/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707524101,
        "cdate": 1696707524101,
        "tmdate": 1701465438742,
        "mdate": 1701465438742,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a method for reranking entity candidates for zero-shot Entity Linking.\n\nAll three reviewers acknowledge the importance of the cross-entity comparison for candidate ranking and Reviewers kXSc and cEiX also recognise the significant improvement to the state of the art by this work. \n\nReviewers kXSc and cEiX pointed out different ways in which the scope work and the manuscript itself are limited. The authors have responded positively to these points by providing new results and suggesting structural changes to the paper that would shift more original content (rather than background information) into the main paper. The issue regarding the scope pointed out by Reviewer cEiX (reranking in zero-shot with a single dataset) remains, but I think this mostly affects the excitement for rather than the soundness of the work.\n\nAfter an extensive conversation with Reviewer WPJY, we were able to dill down to a few specific issues that they identified in the current work (poor motivation, low novelty, unfair comparisons). I agree with the reviewer on the latter - the change of encoder from BERT to RoBERTa means that previous works (other than the re-implemented BLINK) are at a disadvantage (and that an ablation study with BERT as an encoder would be a plus), but I don't think it's detrimental to the soundness of the work. Regarding the other two issues, my opinion is that they are not as severe as the reviewer presented them in their review and discussion and that neither impacts the soundness of the work."
            }
        },
        "id": "allltNBN1V",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8VK9XXgFHp",
        "replyto": "8VK9XXgFHp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2943/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552592,
        "cdate": 1696707552592,
        "tmdate": 1701465482015,
        "mdate": 1701465482015,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work considers the conversational search setting. Primarily, the work proposes an MTL approach which simultaneous considers topic continuity and question reformulation (instead of independently).\n\n**Pros**: Reviewers tend to agree the task and motivations/hypotheses are well-defined with generally convincing experiments (among those conducted). Some reviewers also comment on interesting takeaways from the work.\n\n**Cons**: After rebuttal, there is a shared sentiment among a few reviewers that the scope of the experiments is limited (1 language, missing perturbations among important variables like larger models and retrieval system, etc.). Authors do provide some new experiments, but they do not address all concerns. For one reviewer, this issue is compounded by substantial similarity to another MTL paper looking at classification/generation of implicit discourse relations. While this work is arguably very similar, and other MTL approaches abound in the current literature, the authors suggest the importance of the conversational setting in the current work. Besides these concerns on soundness and scope, reviewers are generally ambivalent - there is no champion."
            }
        },
        "id": "KfS9YuwnYq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8UWPQboDq9",
        "replyto": "8UWPQboDq9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2968/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707553106,
        "cdate": 1696707553106,
        "tmdate": 1701465482700,
        "mdate": 1701465482700,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "All reviewers appreciate the work that went into creating the dataset and see the importance of studying complex compounds. However, all reviewers would also welcome clearer explanations, especially of the dataset and annotation procedure, which the authors replied to positively in their rebuttal. All reviewers acknowledged the rebuttals and one reviewer even went into lengthy back-and-forth discussions and increased their scores. Average soundness is at 3.3, and excitement at 3, where the reviewer with the lowest confidence scores providing the lowest scores.\nOne reviewer mentions that the method could be applied to other languages. It is very nice to see a contribution on a less-studied language such as Sanskrit, but other languages could indeed benefit from such analyses as well. The dataset pointed to does not exactly correspond to the authors needs, but perhaps the authors would like to check the links provided at the bottom that are relevant for bracketing English noun compounds, and the accompanying data (just structure, not labeled relations).\n\nhttps://aclanthology.org/R15-1094.pdf, https://aclanthology.org/W15-0112/, and David Vadas. 2009. Statistical Parsing of Noun Phrase Structure. Ph.D. thesis."
            }
        },
        "id": "I4HO4xsLDp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8Rif7M7Z6A",
        "replyto": "8Rif7M7Z6A",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2841/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550576,
        "cdate": 1696707550576,
        "tmdate": 1701465478303,
        "mdate": 1701465478303,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a machine reading comprehension dataset for Hebrew, HeQ, consisting of approximately 30K question-answer pairs, used to demonstrate comprehension. It likewise details the collection and annotation pipelines, a revised evaluation metric (TLNLS, which is better suited for the complex morphological patterns exhibited by Hebrew), benchmarking experiments, as well as comparisons to an existing machine reading comprehension dataset ParaShoot. Reviewers praised the paper’s clarity and motivation, its literature review, and the fact that it presents a high-quality resource with few to no viable alternatives. They did raise individual concerns in their reviews, however (e.g. extensibility of the proposed evaluation metric TLNLS to other morphologically rich languages and lack of human evaluation regarding TLNLS). Reviewer cQ9w raised several additional concerns, commenting the paper would benefit from (1) added details pertaining to the dataset, e.g. human evaluation, statistics, evaluation of diversity, (2) more detailed comparison to Parashoot, and (3) detailed error analysis of the models trained on HeQ. In their rebuttals, the authors addressed all of these concerns extensively, sharing the quantitative and qualitative results of their error analysis and empirical results from their comparisons between Parashoot and HeQ.\n\nTo summarize, this paper calls for a fair number of revisions, but much of the work required for these revisions is already complete, as demonstrated in the author rebuttals. What remains is to incorporate this work in writing and ensure that all reviewers comments and concerns have been addressed."
            }
        },
        "id": "BeNRXsNUE6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8POQ904HEc",
        "replyto": "8POQ904HEc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4098/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583128,
        "cdate": 1696707583128,
        "tmdate": 1701465520241,
        "mdate": 1701465520241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper explores the logical reasoning capabilities (or lack of thereof) of pre-trained language models, specifically in their comprehension and handling of multi-nested boolean logic. It addresses the disparity between language models and human reasoning skills and introduces a method to improve the models' logical proficiency. The study introduces a new benchmark and a curriculum-learning approach based on it. Reviewers found the work well-structured, clear and thorough, and having strong and comprehensive experimental results. Reviewers raised some minor issues concerning how catastrophic forgetting of the vanilla language modelling task could impact the performance of resulting models, and also the size of LLMs used. This issues were addressed during the rebuttals."
            }
        },
        "id": "yTuvaw1Dy6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8PNFSDJ3md",
        "replyto": "8PNFSDJ3md",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission443/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487919,
        "cdate": 1696707487919,
        "tmdate": 1701465399225,
        "mdate": 1701465399225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper addresses the current research gap in authorship attribution, authorship verification, and AI-generated text detection, being primarily written texts, despite potential applications for spoken text analyses. The authors present the following contributions.\n\n- They curate existing speech corpora and new human-spoken datasets.\n- They generated spoken text datasets from three popular LLMs and demonstrated the data quality assessment.\n- They evaluate traditional authorship analysis methods on the resulting HANSEN benchmark datasets.\n\nReasons To Accept:\n- The presented benchmark datasets introduce novel tasks of spoken text authorship analysison a large collection of diverse datasets in terms of speech contexts, formality levels, and topics.\n- This paper clearly demonstrates via experiments how baseline and SOTA methods perform on spoken text tasks.\n- The analysis section reveals that the spoken texts benchmark provides interesting research gaps for further research, such as that models trained on written texts do not perform well on spoken text test data, an observation worthy of further investigation.\n- The paper is clearly written.\n- The authors provide a very valuable literature review in the introduction and related work sections.\n\nReasons To Reject: \n- It is unclear in the main paper how the quality assessment of LLM-generated data and curation of existing datasets is performed."
            }
        },
        "id": "ArtIGaPejH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8NFU2kLql3",
        "replyto": "8NFU2kLql3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2024/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532503,
        "cdate": 1696707532503,
        "tmdate": 1701465450935,
        "mdate": 1701465450935,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents data augmentation techniques to generate comparable corpora for using machine translation for code translation (e.g. from Python into Java). The paper is well written, and the experimental results show that their approach outperforms CodeT5.\n\nAs the paper is heavily based on previous work, the contributions of the present work and its novelty, as compared to the previous studies, should be made more  clear.  The confusion regarding the experimental design should be clarified. Statistical significance testing should be added to the paper (thank you for providing the numbers in the author response)."
            }
        },
        "id": "ycPX5h0sM5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8NA76tz7Jj",
        "replyto": "8NA76tz7Jj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission603/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491857,
        "cdate": 1696707491857,
        "tmdate": 1701465404912,
        "mdate": 1701465404912,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an approach for multilingual question generation and question answering, comprising knowledge graphs and textual inputs. The paper is well positioned against the state of the art and can be seen as a (substantial, non-incremental) extension of Rebuffel et al.\n\nThe paper is solid and interesting. It addresses an important multilingual task and the model yields excellent results. Section 7 (Results) merits a special mention, as it discusses results from multiple different perspectives and offers a very good understanding of the system's behaviour. All the reviewers have appreciated the paper's \"excitement\".\n\nHowever, the paper is extremely hard to read. It addresses three phenomena (multilinguality, kg/nlp and qg-qa) and thus offers a rather complex architecture with multiple rationales/subtasks/modules/.. I (AC) have some difficulty understanding individual parts -- and all the reviewers, I believe, have encountered the same: all the three reviewers have asked multiple clarification questions (addressed thoroughly by the authors in the rebuttal). I believe therefore that the presentation issue should be addressed, following very constructive comments: put more emphasis on the bigger picture (reviewer URPa), highlight use-case/impact (reviewer HqCQ), have a more principled discussion of modules and their roles (reviewers zbSx, URPa) etc."
            }
        },
        "id": "PYc2gDqrz0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8LSuy5nNmz",
        "replyto": "8LSuy5nNmz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1926/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530216,
        "cdate": 1696707530216,
        "tmdate": 1701465447491,
        "mdate": 1701465447491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work provides a survey of papers on sentiment analysis and provides a sociotechnical perspective on the area, discussing the issues and challenges. These challenges include the lack of a concrete definition of “sentiment analysis” used within NLP and across NLP and social science, challenges in existing sentiment analysis models, such as biases and limited generalizability among others. Lastly, it provides four pieces of advice and 10 related questions to mitigate the challenges it discusses.\n\nReviewers appreciated that this work provides a succinct summary of the task of sentiment analysis, a careful discussion of its issues, and a practical guideline for (at least partially) mitigating the issues. The presentation is also clear, which is especially important for this type of paper. \n\nThe reviewers do not seem to have major concerns but noted that a main figure presenting an overview of the survey would be helpful, some terms like “framework” can be better clarified, and the limitations section can provide a deeper discussion, among others."
            }
        },
        "id": "efe69mao0k",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8L5SA7ENI4",
        "replyto": "8L5SA7ENI4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission284/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707483727,
        "cdate": 1696707483727,
        "tmdate": 1701465393687,
        "mdate": 1701465393687,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary:\n\nThe paper presents a novel “key-info-first” continuous diffusion model and an information-entropy aware noise schedule for improving open-ended non-autoregressive text generation. The paper is generally well written, and the proposed approach seems sound. Experiments show positive results against selected baselines using automatic metrics like perplexity, BLEU, ROUGE, and BERTScore. No human evaluations were conducted to boost the claims. Additional experiments with discrete diffusions and other datasets were provided during rebuttal period, showing potential for an improved revised paper. \n\nStrengths:\n\n1. Proposing a new diffusion process that is sensible for text generation by prioritizing keywords first, showing improved performance through automatic metrics. \n2. Proposing an information-entropy aware noise schedule\n3. Experiments with automatic metrics, in addition to some qualitative examples, demonstrating the effectiveness of the proposal compared to selected baselines. \n\n\nWeaknesses: \n\n1. The overall presentation and writing should be improved based on reviewers’ comments. \n2. The paper needs to incorporate the additional experiments and analyses provided during the rebuttal period in the final revision.  \n3. Missing human evaluation"
            }
        },
        "id": "MXMRiWO78J",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8IrFLWRvuW",
        "replyto": "8IrFLWRvuW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1993/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531659,
        "cdate": 1696707531659,
        "tmdate": 1701465449768,
        "mdate": 1701465449768,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents an attack on dense retrieval systems, introducing a method to inject poisoned passages into the top-k documents retrieved by these systems. The attack method is thoroughly explained and extensively tested on multiple QA datasets and two different retrieval models, with additional evaluation using the BEIR benchmark for transferring attacks. The paper is well-written and structured, offering clear explanations of the method and its results, making it easy to follow.\n\nOne of the paper's significant strengths is its clear presentation of the attack method, its objectives, and the detailed explanation of how the method operates. The extensive empirical studies conducted by the authors provide strong evidence of the attack's effectiveness, enhancing the paper's overall quality. Additionally, the paper goes beyond the attack itself by including basic experiments for defense strategies, demonstrating a comprehensive approach to addressing the issue.\n\nHowever, some concerns were raised by reviewers. One reviewer pointed out similarities between the attack method and the existing HotFlip technique, seeking clarification regarding the technical contributions of the paper. Additionally, the paper lacks comparisons to state-of-the-art (STOA) works in the field. The paper's focus on a specific type of dense retrieval architecture, the bi-encoder model, should be made clear in the abstract and introduction, and the paper should acknowledge other relevant architectures and cite appropriate literature. Lastly, the evaluation on the BEIR dataset should consider the similarity of queries in different collections relative to the training data to provide a more accurate assessment of poisoning vulnerability."
            }
        },
        "id": "fY8lc1xxv0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8FgdMHbW27",
        "replyto": "8FgdMHbW27",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5103/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606987,
        "cdate": 1696707606987,
        "tmdate": 1701465550029,
        "mdate": 1701465550029,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a novel parameter-sharing method based on the Matrix Product Operator (MPO) for scaling pre-trained language models (PLMs) to deeper model depths. The work emphasizes sharing central tensors of parameter matrices across all layers to achieve efficient deep Transformer architectures. The paper also includes theoretical analysis for why the proposed approach may address observed training instability of deeper PLMs.\n\nGenerally, the reviewers liked this approach as it seems broadly applicable to many Transformer-based architectures and there is some consensus that these are often over-parameterized making parameter sharing seem justified. There was some skepticism that parameter sharing can actually lead to improved performance (or similar performance using fewer parameters). The authors provided additional experiments which appeared to satisfy the reviewers."
            }
        },
        "id": "IcFn43nZ1P",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8FXeFY5487",
        "replyto": "8FXeFY5487",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1784/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527158,
        "cdate": 1696707527158,
        "tmdate": 1701465441562,
        "mdate": 1701465441562,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a dialect adaptation to a number of English dialects without the need of a high-accuracy dialect identification systems. They propose DADA, a modular approach to adapt Standard American English (SAE) trained models to different dialect variants by composing adapters which handle specific linguistic features obtained from several linguistic rules. They trained on nearly 200 feature adapters that distinguish SAE and other English dialects. \n\nAll the reviewers appreciated the incorporation of the linguistic rules to the model, and the promising experiments, and the modularity of adapters incorporated in the model. There are a few concerns about how they obtained the 200 linguistic features which the authors adequately responded to. Another issue was the small improvement in performance. Overall, this is a great contribution for incorporating other english dialects."
            }
        },
        "id": "tECeYBDydZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8ElstW3DUT",
        "replyto": "8ElstW3DUT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2217/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537207,
        "cdate": 1696707537207,
        "tmdate": 1701465458137,
        "mdate": 1701465458137,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method for multi-class few-shot and semi-supervised classification with masked language models. Previous approaches use manually defined verbalizers that map from class labels to tokens in the MLM’s vocabulary, but this does not scale well to datasets with more classes. The method instead adds a classification head on top of the distribution over tokens outputted by the MLM head. Experiments are on text classification tasks and often combine the method with self-training on unlabeled examples. Reviewers found the method appropriate and appreciated the strong empirical results and additional analysis. As pointed out by reviewer jFV1, the experiments are not the most thorough: only covering one pretrained model and a fixed number of labels per class. It also isn’t justified why going through the vocabulary is necessary at all as opposed to applying the classification head to the [MASK] hidden state directly. Additionally, the experiments filtered out classes from the datasets with very few examples per class, which I find strange because this is precisely the setting where we want good few-shot learning. Lastly, as reviewer iK6f suggests, the paper could be clearer about the problem being solved: while the paper largely discusses few-shot learning, the results for the pure few-shot setting (with no unlabeled data) are often worse than the single word label baseline. Instead, the method really shines in the semi-supervised setting, but there isn’t much analysis on why it benefits more from self-training than other approaches. To summarize, the method is well-motivated and produces some strong results, but the paper could be clearer about when and why the method works."
            }
        },
        "id": "MJ4p0RzOdT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8DKrruapZ5",
        "replyto": "8DKrruapZ5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2885/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551480,
        "cdate": 1696707551480,
        "tmdate": 1701465480046,
        "mdate": 1701465480046,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a new method for selecting auxiliary languages to improve machine translation. Instead of using traditional language families, they compute similarity between each pair of languages with Fisher Information Matrix, so that languages will be considered similar if they overlap a lot in terms of which parameters have high variance, and use this to compute pseudo-families based on the empirical data. The technique seems like it should be also applicable for other situations that would benefit from measuring language similarity.\n\nThe reviewers appreciated that the apprach is straightforward and effective."
            }
        },
        "id": "AnBLtTz0sx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8CQ0DUuSAK",
        "replyto": "8CQ0DUuSAK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5680/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616692,
        "cdate": 1696707616692,
        "tmdate": 1701465564975,
        "mdate": 1701465564975,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents new sets of German encoder-only language models using a cross-domain dataset. The model is based on DeBERTa architecture, and it achieves an impressive performance on almost all the NLP tasks evaluated on compared to previous models (GBERT, GELECTRA, GottBERT). \n\nThe reviewers found the models development to be useful especially since it achieves impressive result and since the authors planned to release their code to the community. Also, their results highlight the use of a cross-domain/heterogeneous dataset for model pre-training. There is a small concern about limited contributions of the paper apart from the cross-domain corpus, and building a new set of models, but the issues have been addressed by the authors."
            }
        },
        "id": "6jUXE7sTdZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8B9mL26NDT",
        "replyto": "8B9mL26NDT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5849/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707619870,
        "cdate": 1696707619870,
        "tmdate": 1701465568434,
        "mdate": 1701465568434,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a unique approach to unsupervised paraphrase generation by blending discrete and continuous representation. This method stands out from earlier unsupervised techniques, aiming to produce diverse syntactical paraphrases without the need for translation corpora while maintaining entity accuracy. At its heart, the methodology utilizes a VQ-VAE architecture combined with a self-supervised pseudo-data construction approach trained on non-parallel corpora. For assessment purposes, the research employed the WikiAnswers and Quora datasets and introduced the Entity-Score metric to measure the precision and recall of entities.\nRegarding the topic and contributions, both reviewers concur that the paper presents a fresh perspective on unsupervised paraphrase generation, with a particular emphasis on maintaining entity integrity. They both underscore the innovative use of a VQ-VAE architecture and acknowledge the introduction of the Entity-Score metric as a significant advancement.\nIn terms of strengths, there's a unanimous sentiment about the promising nature of the experimental results. Both reviewers laud the novel approach and express appreciation for the Entity-Score metric, particularly highlighting the thorough evaluation bolstered by human assessments.\nOn the flip side, clarity appears to be an issue. The reviewers both identify areas in the paper, such as the methodology of the pseudo-data constructor and the treatment of entities, that could benefit from more explicit elaboration. They recommend additional evaluations using renowned datasets or models to further validate the approach.\nAdditionally, while Reviewer 3 praises the clarity and structure of the paper, Reviewer 2, though not explicitly critical, refrains from complimenting its writing style.\nOverall, despite recognizing the paper's strengths and contributions, the reviewers have different areas of focus and depth in their critiques.\n\nNB.: The assessment based on the two reviews should suffice for now. Decisions can still be made from those reviews, or if deemed necessary, another reviewer could be approached to provide further insights at a later time."
            }
        },
        "id": "gFzKpjcfa8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8AKBcTXEd3",
        "replyto": "8AKBcTXEd3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5129/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707607602,
        "cdate": 1696707607602,
        "tmdate": 1701465550704,
        "mdate": 1701465550704,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper creates a dataset of label descriptions which can help models improve zero-shot classification.\n\nThe idea is interesting and experiments sufficiently demonstrate the usefulness.\nThe reviewers feel that most of the paper is clear and easy to follow.\n\nThe baselines and comparison to existing methods could be further improved, following specific suggestions from the reviewers.\nCertain parts of the method could be clarified, as pointed out by reviewer ZfZP."
            }
        },
        "id": "VaXimmfn68",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8851TT2R0l",
        "replyto": "8851TT2R0l",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission411/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486989,
        "cdate": 1696707486989,
        "tmdate": 1701465398018,
        "mdate": 1701465398018,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an empirical study on the effect of pixel-level text representations on multilingual translation models. Building on best practices established in previous work, which was trained one language at a time, this work shows that multilingual pixel representations can improve data efficiency and cross-lingual transfer to unseen scripts. Reviewers are excited about the approach as an alternative to subword models that enables positive transfer for low resource languages, and they have few concerns that were not adequately addressed by the response."
            }
        },
        "id": "VLzwMkZ64J",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "87WEkTIVSh",
        "replyto": "87WEkTIVSh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission25/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477145,
        "cdate": 1696707477145,
        "tmdate": 1701465384464,
        "mdate": 1701465384464,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a benchmark for evaluating multilingual (four European languages from different families) dialect-to-standard normalization. The main task is to convert a broad phonetic transcription of dialect(s) into orthography of the standard variety. The authors evaluated several seq-2seq and MT systems and different splits and normalizations on existing datasets.\n\nReasons To Accept:\n- The paper compiles a multilingual dataset, and conducts consistent benchmarking across modeling methods, with detailed error analyses. The obtained results are comparable with previous works, while being more unified and observable across language\n- Providing different splits of the same dataset to test different scenarios. This is usually rare in many NLP papers but is very important and usually highlights any artificial gains/losses in systems that can go unseen. Even though in this specific work, the splits did not affect the performance of the system, it is still helpful for future system evaluations.\n- Appropriateness of the baselines and the different configurations for the systems used for this task.\n- The multilingualism of the benchmark.\n\n\nReasons To Reject:\n- The paper focuses on 4 European languages, which the authors already acknowledge in their limitations, and as such it is difficult to predict how well the proposed methodology would work in other languages.\n- The paper introduces a multilingual dataset, compiled entirely of previous works on dialectal research. Regarding this aspect, the paper makes marginal contributions, as the employed corpora are derived completely from previous works, albeit with a few modifications in splitting and preprocessing."
            }
        },
        "id": "jdIYFcaqQt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "8752c2KVwd",
        "replyto": "8752c2KVwd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2858/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550941,
        "cdate": 1696707550941,
        "tmdate": 1701465478994,
        "mdate": 1701465478994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All the reviewers agree on the fact that the paper provides valuable and novel contributions to research in countering hate speech messages online. A corpus of hateful tweets towards individual public figures with related articles is created, and annotated with useful annotations to identify and analyze counterspeech. The experimental set-up looks also solid and sound.\nThe main concerns (not answered in the rebuttal) are related to the fact that data was constructed with only specific patterns of hate, which may introduce biases, as well as missing details about the annotation instructions and guidelines. A discussion on how far the results provided in the paper are representative in general is also missing."
            }
        },
        "id": "oUmmmfcIut",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "855dPxyaex",
        "replyto": "855dPxyaex",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3175/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557541,
        "cdate": 1696707557541,
        "tmdate": 1701465489300,
        "mdate": 1701465489300,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper shows that high correlations between system rankings from metrics and humans can be spurious, in the presence of one confounding QC. The arguments in the paper are convincing. The authors should consider addition of results from partial correlation, which they claim shows similar findings to strengthen the paper."
            }
        },
        "id": "JGjZwOrogA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "83m634EuTW",
        "replyto": "83m634EuTW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3428/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562501,
        "cdate": 1696707562501,
        "tmdate": 1701465497166,
        "mdate": 1701465497166,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a novel parallel corpus that transfers text between the northern and central Vietnamese dialects, addressing a gap in NLP research with respect to the central Vietnamese dialect. While the potential impact of this work, the corpus creation process, and the inclusion of benchmarking experiments deserve praise, there are concerns over style and presentation. Certain sections are difficult to follow such as the Abstract and Discussion, and sometimes the terminology used is unclear. Some details are also vague or missing, e.g. how were the models fine-tuned and what were the detailed results from the text-image retrieval task. These concerns were addressed by the authors in their rebuttals, however, where they acknowledged the areas that need to be reworked and that many of the missing details can be found in the appendices (but should and would be moved into the main body of the paper).\n\nTo summarize, the revisions required to address paper content concerns can be accomplished quite readily, but substantial revisions may be required to ensure the writing is clear and up to standard."
            }
        },
        "id": "fNbXSPRFOQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "80ZDEuEJVC",
        "replyto": "80ZDEuEJVC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5797/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707618567,
        "cdate": 1696707618567,
        "tmdate": 1701465567296,
        "mdate": 1701465567296,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This papers presents a benchmark aiming at facilitating the evaluation of editing knowledge in multimodal LLMs, specifically the reliability, locality, and generality evaluation metrics are the focus of this paper. Two tasks are included Image  Captioning and VQA. Data and code will be made available.\nA general synthesis of the various results would help the reader to have a clear understanding of the results ans insights."
            }
        },
        "id": "m4SPONTktt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7wJhlDMNH7",
        "replyto": "7wJhlDMNH7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1273/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509040,
        "cdate": 1696707509040,
        "tmdate": 1701465425866,
        "mdate": 1701465425866,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes an approach to document-level NMT that uses a rhetorical structure theory (RST) parser to capture discourse information. It explores different methods for adding this information to the encoder, notably a custom attention mechanism that uses the parse to control the context available to different layers. This is shown to outperform a wide range of baselines across different test settings.\n\nReviewers noted that the paper tackles a significant problem, and found the approach novel,  well-motivated, and interesting, with  positive results from extensive experiments, including ablation studies. The main weakness is the reliance on the RST parser, which is not guaranteed to be free from errors, and which limits source languages to those for which such a parser is available. Some reviewers also felt that analysis specifically linking quality improvements to linguistic aspects of the context would have strengthened the paper.\n\nThis is substantial work that represents a very strong baseline for document-level NMT. Although it’s hard to see it as the future of document-level MT - given the pipelined design, specially-modified architecture, and reliance on custom linguistic components - it is nonetheless an interesting point for comparison and analysis."
            }
        },
        "id": "jneEcJOvX1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7vR0fWRwTX",
        "replyto": "7vR0fWRwTX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2890/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551646,
        "cdate": 1696707551646,
        "tmdate": 1701465480369,
        "mdate": 1701465480369,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates how tool-assisted strategies impact LLM performance. It provides comprehensive empirical evaluation comparing strategies that use tools and those that do not leverage any tools, using multiple LMs, benchmarks, and tools. The findings, particularly that the current tool usage approaches are not as good as they are portrayed, and that they also incur extra costs, are interesting, novel, timely, and can benefit the community for future research on tool-assisted efforts with LLMs.\n\nConcerns include the organisation and readability of the paper, and the limited types of tools that are investigated. The author responses provide further insights on the latter that have the potential to increase the scope of the contributions of this paper."
            }
        },
        "id": "yFEZxKS3Co",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7umLwqBbvw",
        "replyto": "7umLwqBbvw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1834/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528454,
        "cdate": 1696707528454,
        "tmdate": 1701465443889,
        "mdate": 1701465443889,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a large curated dataset GPTSumm, collected using documents from Pile as the source and GPT3.5 summaries. The paper studies the performance of a summarization model distilled from this dataset. Both these are valuable contributions for future work on model distillation for summarization. The evaluation of the model is missing any human evaluation or discussion/analysis of factuality of generated summaries (standard now for summarization papers); these will strengthen the claims in the paper."
            }
        },
        "id": "UlYsFNk5ym",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7s8KOmvdJc",
        "replyto": "7s8KOmvdJc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2857/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550929,
        "cdate": 1696707550929,
        "tmdate": 1701465478968,
        "mdate": 1701465478968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes reducing gender bias in multilingual MT models by fine-tuning the encoder with an additional \"gender-aware contrastive loss\". \n\nThe results on different languages show that the approach reduces gender bias while retaining translation quality.\n\nThe analysis explaining the strengths of the proposed approach is also presented. \n\n\n\nRevisions discussed during the authors' response should be applied. \n\nAlso, dealing only with binary gender should be mentioned in limitations (while including non-binary it is very easy in English, for most of the other languages there are no clearly defined rules (yet))."
            }
        },
        "id": "6ha67GxDfU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7rjkSqMJ5n",
        "replyto": "7rjkSqMJ5n",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1362/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707511130,
        "cdate": 1696707511130,
        "tmdate": 1701465428489,
        "mdate": 1701465428489,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper aims to recover the real harmfulness of chatbot conversations even when some users (a.k.a. trolls) intentionally provide incorrect feedback. The proposed method, based on LCA, is significantly more efficient than common cross-validation practices (but also has higher data requirements) and more robust to scenarios where trolls are the majority, provided that they are consistent in their behaviour. Hence, an interesting aspect of this paper is that it presents a somewhat counter-intuitive result. However, the paper also suffers from some weaknesses: the test data size is very limited, which somewhat puts into question the validity of the conclusions. Most crucially, the effectiveness of the method relies on somewhat strong assumptions: namely, 1) that trolls are \"diligent\", i.e. consistent and 2) that the safe class is in the majority of the utterances. It remains unclear how realistic these assumptions are, without any quantitative evidence from real-world scenarios in support. For these reasons, this paper might be considered for Findings."
            }
        },
        "id": "hmFuSfMtBA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7qCuicCunf",
        "replyto": "7qCuicCunf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5137/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707607928,
        "cdate": 1696707607928,
        "tmdate": 1701465551087,
        "mdate": 1701465551087,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper aims at assessing the legal reasoning capabilities of ChatGPT and introduces a novel dataset annotated on the bases of the IRAC methods. The experimental results reported offer useful insights into the limitations of current technology. While one reviewer was concerned about the limited coverage of the data set to only two countries and a few cases, it still offers an interesting example for future research. However, the paper needs improvements.  \n\n**Pros** \n\n- The dataset is valuable and has the potential to help research on legal NLU;  \n\n- the topic is timely and highly relevant not only for NLP \n\n- the paper is clear and easy to follow; \n\n- evaluation experiments are adequate and methodologically sound, although not sufficient to generalize the claims to other LLMs; \n\n**Cons** \n\n- the discussion of related works could better motivate the soundness of the IRAC methods as guiding principles for analysis;  \n\n- the coverage of legal cases is limited, albeit perhaps sufficient to support the claims of this paper. A critical comparison with the other larger datasets mentioned by reviewers would be beneficial; \n\n- the experimental set up should be better motivated and explained, to avoid misunderstandings;"
            }
        },
        "id": "vk4jyFuyPE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7okuG5JhaM",
        "replyto": "7okuG5JhaM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3182/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557691,
        "cdate": 1696707557691,
        "tmdate": 1701465489508,
        "mdate": 1701465489508,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are generally agreed on both their feelings of soundness and excitement, giving high ratings for both categories. The authors are addressing a practical challenge, using methods whose performance will be interesting to many other researchers.\n\nConcerns about clarity recur across the reviews, and this will be important to address for a final version of the paper. Some sections of the paper were confusing to the reviewers and needed additional clarification from the authors during the rebuttal. These confusions were all cleared up, but that additional explanation should be incorporated into the paper.\n\nThe reviewers agreed that the experiments were sound and compelling. One reviewer had additional questions about the choice of comparison models, but this was clarified by the rebuttal (and as stated above, these misunderstandings can be easily addressed by clarifying a final version of the paper)."
            }
        },
        "id": "EyIiUKwWCw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7jYZd05yjJ",
        "replyto": "7jYZd05yjJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3951/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580007,
        "cdate": 1696707580007,
        "tmdate": 1701465515214,
        "mdate": 1701465515214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In this paper, the authors propose to optimize set-level metrics for in-context learning to achieve better coverage of salient aspects. It proposed a BERTScore-based set metric and provides the proof of its submodularity. Experiments with several conventional LLMs and generation tasks show that the proposed metric can result in better in-context learning performance.\n\nStrength:\n1. The research problem studied in this paper (demonstration selection) is very important.\n2. The proposed method is also simple and does not require any training\n3. Extensive experiments with various LLMs and tasks.\n4. Overall clear presentation with concrete examples.\n\nWeakness:\n1. Would be good to provide significance tests and evaluation of efficiency.\n2. Lack of comparison to related area such as active learning [Batch Active Learning at Scale]"
            }
        },
        "id": "CTtkBZR3lO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7hSVLwNbWT",
        "replyto": "7hSVLwNbWT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4226/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585709,
        "cdate": 1696707585709,
        "tmdate": 1701465524669,
        "mdate": 1701465524669,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "There is consensus amongst the reviewers that the work is sound and well written. The majority of the reviewers agree that the experimental design and analysis is sufficiently comprehensive, and that all the claims made in the paper are well-supported. The general excitement is moderate, with the text of the reviews (and reasons to accept) not particular championing for the idea and its novelty itself. Overall, the positives of the paper outweigh the weaknesses which were not addressed in the rebuttal, and the paper has _good technical soundness_ and _ambivalent excitement_.\n\nThe following is a summary of the strengths, weaknesses and scores across the three reviews:\n\n**Strengths:**\n\n- Paper is well written (**m4pp**, **TY8m**, **McsH**)\n- Experimental design is sufficient in backing the claims made in the paper (**m4pp**)\n- Adaptation of proposed technique (meta-learning approach) to in-context learning is especially pertinent, and shows the efficacy of the proposed methodology in low-resource settings (**TY8m**)\n- Analysis of cross-lingual alignment is comprehensive (**McsH**, **m4pp**)\n\n**Weaknesses:**\n\n- Baselines are limited and too simple (**m4pp**)\n\t- Rebuttal somewhat addresses this weakness\n- Generalization of the findings in general is not sufficiently shown (**McsH**)\n\t- e.g. two models, one of which was not explicitly trained on non-english\n\n**Scores in decreasing order of confidence:**\n\n|      | Soundness | Excitement | Reproducibility | Confidence |\n|------|-----------|------------|-----------------|------------|\n| m4pp | 4         | 3          | 4               | 4          |\n| McsH | 3         | 3          | 4               | 3          |\n| TY8m | 3         | 4          | 4               | 2          |"
            }
        },
        "id": "94IsF7GEtG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7gIhLGqyph",
        "replyto": "7gIhLGqyph",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3674/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567470,
        "cdate": 1696707567470,
        "tmdate": 1701465506197,
        "mdate": 1701465506197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In this paper, the authors formulate an NLP specific definition for the dual use and propose a checklist to be reflected on in ethical considerations of NLP tools. This checklist can be used in addition to a datasheet or model card or other related artefacts to make a complete assessment. The work identifies a major gap in our current discourse of harmful impact of language technologies, and lays foundational work towards defining and checking for the same.\nThe survey of the NLP practitioners is particularly interesting as a contribution and helps reflect on the state of models being released. The checklist however seems a little under detailed. Some more thought on what one can do with the answers recorded, as well as the agency and knowledge of an NLP researcher to reliably answer them are also points that could be reflected upon in the paper some more, since the checklist is also counted by the authors as a core contribution. \nAll reviewers in their reviews and comments post rebuttal concur that the paper makes foundational progress towards consideration of dual use in NLP, and with the minor adjustments recommended (such as expansions of terms such as 'secondary unintended harms', image and text typos, mistyped citations in line 354) will consist of significant insights for the community."
            }
        },
        "id": "ajaIMaAyiZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7fdIbXjRSp",
        "replyto": "7fdIbXjRSp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3505/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564085,
        "cdate": 1696707564085,
        "tmdate": 1701465499492,
        "mdate": 1701465499492,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Model generated code evaluation is a really hard problem and this work is a very interesting and useful contribution to that issue. While the authors go to good lengths to justify the value of the model, it is still perhaps a matter of judgement whether the score coming from CodeBERTScore is more meaningful than other metrics. It's my opinion that code execution in environments with tests is the gold standard for code evaluation, but this is a very high burden for many important questions."
            }
        },
        "id": "EskEln7yyb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7cXoueVCoL",
        "replyto": "7cXoueVCoL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4907/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601367,
        "cdate": 1696707601367,
        "tmdate": 1701465544520,
        "mdate": 1701465544520,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new approach of continual learning in the context of multilingual neural machine translation (MNMT). It utilizes newly available parallel data for zero-shot translation without harming the already well-performed directions by a two-stage strategy: 1) learning new knowledge from the new data by introducing a new module, and 2) compressing and transferring the acquired knowledge back into the original model. The outcomes, as demonstrated using various open-source MNMT models, improve supervised and zero-shot translation, surpassing the performance of several strong baselines.\n\nPros: \n- To my knowledge, this represents a novel approach within the research domain of continual learning for MNMT. It offers several advantages, such as enhancing both zero-shot and supervised translations without introducing additional parameters post-training. These improvements are substantiated by strong results. The concept is innovative and intriguing. The experimental design demonstrates thoughtfulness and is supported by reasonable justifications.\n- Should the paper incorporate the rebuttals, it would serve as an effective mean to enhance MNMT in light of newly available parallel data.\n- The paper conducts a concise yet comprehensive literature review.\n- It provides a thorough reporting and comparison of various baselines, resulting in a robust conclusion regarding the proposed method.\n- Well-written.\n\nCons:\n- There were some unclear details, but these have been thoroughly addressed in the rebuttal phase."
            }
        },
        "id": "QmwCYSBRLR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7Z1F0h7gWq",
        "replyto": "7Z1F0h7gWq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5048/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605858,
        "cdate": 1696707605858,
        "tmdate": 1701465548509,
        "mdate": 1701465548509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an approach to learn text classifiers based on interacting with an LLM to annotate a small amount of training examples, with the goal of generating class descriptions for use in a prompting-based approach. They find that their approach outperforms other prompting-based approaches and conduct a user study that indicates positive reception of the approach.  \n\nSoundness: \n\nReviewers give fair \"soundness\" ratings with 3/3/4 though list several concerns in their written reviews. \n\nReviewers voice concerns regarding the technical details of the experimental evaluation, namely the focus on only one public benchmark (WOC) and the lack of justification for the specific prompts used. Additionally, since the proposed approach relies on interactivity with humans, this naturally limits reproducibility. The authors however try to mitigate the lack or reproducibility by releasing much information on the conducted study. \n\n\nExcitement: \n\nReviewers also give fair \"excitement\" ratings with 3/3/4. \n\nOne limiting factor excitement are presumed difficulties in scaling the proposed approach to many classes, or classes with fine-grained distinctions, in which case a potentially very large amount of interactions would be necessary with the LLM. The authors acknowledge this limitation in their response and will add discussion on this. \n\nThe novelty is questioned by some reviewers, but the author response alleviates this by pointing out that their approach outperforms other prompting-based approach significantly, albeit only evaluated on one benchmark dataset and one small proprietary dataset."
            }
        },
        "id": "MQ421UvOFo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7YluNq3HQQ",
        "replyto": "7YluNq3HQQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4129/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583681,
        "cdate": 1696707583681,
        "tmdate": 1701465521244,
        "mdate": 1701465521244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates the feasibility of Transformers to approximate CKY algorithms. The authors evaluate their Transformer-based  CKY approximations on two data sets: standard benchmarks and a synthetic data set. This paper also introduces some changes in Transformer-based approximation, including a cross-entropy loss over possible constituents for spans, an inductive bias, decoding prediction from gradients etc. The main findings are 1): their Transformer-based  CKY approximation outperforms and runs faster over CKY algorithms on half of the standard benchmark sets; 2) however, experiments on more ambiguous synthetic data suggest their approach is not able to fully capturing the CKY. Those findings are beneficial to our parsing community."
            }
        },
        "id": "UhaqqTggwK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7UvOkmrB8V",
        "replyto": "7UvOkmrB8V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2275/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707538403,
        "cdate": 1696707538403,
        "tmdate": 1701465459729,
        "mdate": 1701465459729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work constructs a polite interpersonal psychotherapy dialogue system by conditioning system responses on the user's gender, age, persona, and sentiment.\n\nContributions:\n - The (synthetic) dataset collected is a valuable resource for future work.\n\nWeakness:\n - As presented it is not clear how significant the results are, there is a lack of analysis of some details, and in relation to the main claim, which is the importance of politeness the examples do not seem to show a specific connection to gender, age, and persona."
            }
        },
        "id": "ndnjH4B0TR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7UVOFuNk27",
        "replyto": "7UVOFuNk27",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5279/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610270,
        "cdate": 1696707610270,
        "tmdate": 1701465554946,
        "mdate": 1701465554946,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors introduce a new dialogue task called DIALGUIDE and provide a human-annotated dataset for this task. The experiments conducted in the paper demonstrate that fine-tuning a model on the DIALGUIDE dataset can improve the model's ability to generate coherent, diverse, and safe responses.  The experimental results indicate promising outcomes in terms of response quality and safety."
            }
        },
        "id": "bpGb4IMPq3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7TKKvwyQef",
        "replyto": "7TKKvwyQef",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2168/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535996,
        "cdate": 1696707535996,
        "tmdate": 1701465456454,
        "mdate": 1701465456454,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary of the paper: RWKV is the largest RNN model trained to date in NLP that rivals transformers in performance. The results show that the model has impressive performance, making it a worthwhile subject of further study.\n\nThis manuscript has a lot of positives. The idea presented in the paper is very ambitious and relevant to the NLP community. The proposed method has comparable training speed as compared to transformers with much faster inference and lower memory footprint. \n\nSome of the core criticisms of the paper are on the empirical evaluations and the paper not being well written. Multiple details are missing including experiments such as actual compute time comparison, evaluation beyond 4K token length to showcase the use of RNN style method. RNN style methods trade-off accuracy and compute time (because of information bottleneck), an evaluation of this trade-off would be an interesting addition."
            }
        },
        "id": "vhcre9ppLH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7SaXczaBpG",
        "replyto": "7SaXczaBpG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1949/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530590,
        "cdate": 1696707530590,
        "tmdate": 1701465448063,
        "mdate": 1701465448063,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces 14 sentiment analysis datasets, comprised of Tweets from 14 African languages. It is thorough and detailed as indicated by its Soundness scores (two 4s and one 3), describing not only the data collection and annotation processes and experimental benchmarks but also the specific challenges associated with curating each dataset. Reviewers acknowledge the positive and far-reaching impact these discussions can have on future research in low-resource NLP, as indicated by the Excitement scores (two 4s, one 3). In addition to its contributions to the field of sentiment analysis, this paper may be of interest to those working on inter-annotator agreement topics, as full annotations are released alongside the majority vote label (Reviewer TvNi). Likewise, discussion of language-specific challenges and probable solutions may be of consequence to those interested in data collection methodologies. Minor individual concerns were raised by each reviewer (e.g. missing details regarding fine tuning, precision and recall, reasons for removing offensive Tweets), but all were adequately resolved during the author rebuttal period.\n\nIn light of this, only minor revisions, addressing reviewers’ comments and questions, need to be made to ensure this paper is camera ready."
            }
        },
        "id": "m1k2ahNFH5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7RzRbVXWPN",
        "replyto": "7RzRbVXWPN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission11/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476732,
        "cdate": 1696707476732,
        "tmdate": 1701465383920,
        "mdate": 1701465383920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces PromptAV, a novel approach that uses Large Language Models (LLMs) for the task of authorship verification, incorporating chain-of-thought to prompt the verification process. Reviewers recognized the significance and interest in the paper's focus on authorship verification. While acknowledging the paper's contributions, reviewers also raised some valid concerns, which the authors addressed during the rebuttal stage and committed to resolving in the final version."
            }
        },
        "id": "ll7epjW2Kk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7QSvLXXHQt",
        "replyto": "7QSvLXXHQt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1435/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707513558,
        "cdate": 1696707513558,
        "tmdate": 1701465430958,
        "mdate": 1701465430958,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes method for cross-lingual event detection. The method has promise for solving the cross-lingual event detection task, performing zero-shot event detection; experiments suggest improvement over the state-of-the-art models. \nIt needs a refinement with respect to the following: generalizability of the method for the other datasets; comparisons with more potential techniques; and a more clearly stated motivation."
            }
        },
        "id": "4XyaWCJymB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7QSa2w5Wai",
        "replyto": "7QSa2w5Wai",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4765/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598157,
        "cdate": 1696707598157,
        "tmdate": 1701465541105,
        "mdate": 1701465541105,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a tool to visualize a forward pass of GPT-based models as an interactive flow graph, and performs an analysis of the information flow inside the attention mechanism.\nIt is therefore part demo paper and part interpretability paper. Since it was submitted to the interpretability track, the recommendation is for that track.\n\nThere were some concerns around reproducibility, which were (mostly) taken away during discussion, as authors promised to release the code. The paper could still also be improved to provide more details on the approach.\n\nPros:\n- The reviewers unanimously thought the tool to visualize information flow is the biggest contribution of the paper, and so the demo track might have been a better place for it. \n- They also do see value in the analysis, especially regarding layer norm.\n\nCons:\n- The analysis is limited to one dataset (CounterFact) and one model (GPT).\n- The accessibility of the tool needs improvement.\n- The assumption that less activated neurons are less important might not be true.\n\nAll in all it's not easy to make a recommendation given the hybrid nature of the paper (part demo paper, part analysis paper). Given that the reviewers quite unanimously positive regarding the above pros and on soundness and excitement, the paper *could* be accepted in current form. But if it is not, I would recommend the authors to either resubmit it as a demo paper or to a workshop, or to make the paper stronger by extending the analysis."
            }
        },
        "id": "dkRCcA7dnT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7O9bTjLgTQ",
        "replyto": "7O9bTjLgTQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1020/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502716,
        "cdate": 1696707502716,
        "tmdate": 1701465417983,
        "mdate": 1701465417983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies whether robustness to input noise, as conferred by “attacks” involving noised input during training, can be transferred across languages in multilingual NMT. Experimental results indicate that this can indeed be the case, and further analyses characterize the conditions under which it occurs.\n\nReviewers felt that the study was novel and well motivated, and were mostly convinced by the findings and analyses indicating positive results. They noted some downsides, including a tradeoff between robustness and quality (which is however confined to the language under attack), some problems with the clarity and conciseness of the presentation, and a lack of further analysis to account for some of the experimental results.\n\nThis is a solid contribution. As the authors point out, a big benefit to their findings is that robustness to a high-resource language can be transferred to a lower-resource language without needing to explicitly attack the data in that language, which might be problematic for various reasons (including the potential to reduce output quality that might already be marginal). The paper has the potential to generate interesting follow-up work, in particular to try to solve the robustness / quality tradeoff for languages subjected to attacks."
            }
        },
        "id": "cxpHTA5GUj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7MmYaN93lb",
        "replyto": "7MmYaN93lb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2813/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549973,
        "cdate": 1696707549973,
        "tmdate": 1701465477325,
        "mdate": 1701465477325,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a new technique for record linkage that uses the visual features of characters, rather than simple edit distance, arguing that this addresses a common problem in OCRed historical text. It uses a clever and novel combination of existing tools (DINO ViT, contrastive loss) to solve a real problem that applied researchers have. While the paper does not make a novel contribution in loss function or model architecture, the combination of tools and the task framing are novel and the paper will be useful. \n\nOne reviewer raised concerns about replicability or whether the process will be easy to use, but another reviewer and the authors point out that the pipeline will be available as a Python package, which will be useful to applied researchers.\n\nThe reviewers find that the paper is somewhat repetitive at times and an architecture figure would be helpful, but the authors have agreed to revise the paper to address these issues.\n\nReviewers raise a number of clarification questions, comments about terminology, and requests for minor extensions/additional citations. The authors thoughtfully respond to each of these and propose changes to the paper that will address them."
            }
        },
        "id": "JvPqDPaCAv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7LBhEJ1DII",
        "replyto": "7LBhEJ1DII",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2351/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540176,
        "cdate": 1696707540176,
        "tmdate": 1701465462313,
        "mdate": 1701465462313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a structured approach to Legal Judgment Prediction, emphasizing interpretability and explainability. It introduces a new dataset, SLJA, based on the concept of syllogistic reasoning. This dataset encompasses four tasks: article retrieval (AR), criminal element generation (CEG), article interpretation generation (AIG), and legal judgment prediction (LJP). The data was annotated using predictions from ChatGPT, which were then post-edited by crowdworkers. The paper evaluates the performance of various Large Language Models (LLMs) on this dataset, aiming to provide an in-depth analysis of their capabilities in legal judgment tasks.\n\nThe paper offers a systematic method to incorporate interpretability into AI-driven legal decisions. The introduction of the SLJA benchmark dataset is a significant contribution, and the annotation process employed is robust. The dataset's value is further enhanced by human corrections, ensuring its reliability. The paper is well-written, making it easily comprehensible. The use of LLMs for legal judgment analysis is timely and necessary, and the paper's syllogistic reasoning approach provides a novel perspective on legal judgment analysis. The evaluation of multiple LLMs on legal tasks further underscores the paper's significance.\n\nThe experimental setup is limited, relying solely on LLMs, which may not fully demonstrate the dataset's value. The use of LLMs, especially for tasks with multiple labels like LJP, raises concerns due to their unpredictability and slower inference speeds. Existing state-of-the-art LJP models have reportedly achieved better performance, making the choice of LLMs questionable. The paper could benefit from exploring other dominant theories for analyzing legal judgment, like the two-stage system, and expanding its scope beyond criminal law. Comparisons with classical methods and LLMs fine-tuned with legal data could provide a more comprehensive evaluation."
            }
        },
        "id": "OcWJlEb4fC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7Jis2yiiEZ",
        "replyto": "7Jis2yiiEZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5149/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608092,
        "cdate": 1696707608092,
        "tmdate": 1701465551244,
        "mdate": 1701465551244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Program behavior diagnosis is a very challenging and important problem, and the authors have made an interesting and impactful contribution to the problem space. While their approach has false positives and negatives, these seem comparable even to some static analysis methods I have seen for detecting bugs."
            }
        },
        "id": "2iwvFcXwpT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7IcVI11lkO",
        "replyto": "7IcVI11lkO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission48/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477808,
        "cdate": 1696707477808,
        "tmdate": 1701465385273,
        "mdate": 1701465385273,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The main conclusions of the reviews and the post-rebuttal discussions:\n\n2/ 3 reviewers consider the paper sound (scores 1, 3, 4)\n2/ 3 reviewers find the paper exciting (scores 1, 4, 3)\nFrom reading the rebuttal and seeing the scores above, I find that the reviewers consider strong points for soundness the following:\n\n- The paper observes that the current transcompiler (code translation) models still make different elementary syntax mistakes, and the current metrics like BLEU and CodeBLEU may not expose that in an interpretable way. \n- To deal with this issue, the paper proposes a new set of unit tests and metrics to investigate the model abilities in translating the elementary syntax element.\n- The idea of decomposing a unit test into syntax elements and computing the element failure rate is novel and inspiring. The NLP community could benefit from a similar approach when testing models to generate non-code texts.\n\nReviewers had questions regarding dataset construction, evaluation, and how it could be extended to a new programming language. The authors answered these questions in the rebuttal and the reviewers accepted their answers and gave constructive feedback for improving the paper."
            }
        },
        "id": "KrkDPeHQDy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7IB8gZRptd",
        "replyto": "7IB8gZRptd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2807/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549834,
        "cdate": 1696707549834,
        "tmdate": 1701465477095,
        "mdate": 1701465477095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "KCTS, a knowledge-constrained decoding method, guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS. KCTS is a plug-and-play, model-agnostic decoding method that can effectively reduce hallucinations in natural language generation without incurring high training costs or causing catastrophic forgetting.\nThe proposed approach is shown to be effective in both automatic and human evaluations."
            }
        },
        "id": "C8tA4ILlTO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7H45HfXsJb",
        "replyto": "7H45HfXsJb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3037/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554864,
        "cdate": 1696707554864,
        "tmdate": 1701465485243,
        "mdate": 1701465485243,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors propose a novel usage of LLMs towards efficient schema subsetting in large databases. It also introduces large benchmarks for this purpose which can be used in future work. The reviewers concur on the utility of this work. It is also noted that additional experiments for their proposed technique with benchmarks not developed by the authors themselves would enrich the paper. However, this is a minor recommended update to the existing work. I would also recommend light restucturing of the work to highlight the most important results and numbers, as that gets a little lost in this existing presentation."
            }
        },
        "id": "HqUUFAOC4k",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7Gy8FXaTv6",
        "replyto": "7Gy8FXaTv6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5361/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611562,
        "cdate": 1696707611562,
        "tmdate": 1701465557284,
        "mdate": 1701465557284,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors propose a new dataset on Arabic QA for the Climate Change domain by automatically translating English in-domain data. All reviewers agree on the soundness and the excitement of the paper. The authors provided answers to the different questions raised by reviewers during the author response period."
            }
        },
        "id": "2EL2pn9eji",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7GxY4WVBzc",
        "replyto": "7GxY4WVBzc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1906/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529806,
        "cdate": 1696707529806,
        "tmdate": 1701465446712,
        "mdate": 1701465446712,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a dataset for the task of interpreting responses to yes/no questions asked on Twitter. This can often be challenging because the responses may not explicitly contain the words \"yes\" or \"no\", and the interpretation might require commonsense knowledge. The authors created a dataset of 4K+ QA pairs and interpretations representing this task. The reviewers generally agreed that the dataset can serve as a valuable resource. Reviewer 8ozY had concerns about the motivation, experimental methodology, and the organization of the paper, but the authors' responses seem convincing to me.\n\nThere are existing datasets for this task, e.g.: BoolQ, but not in this domain (social media, particularly Twitter), making this a potentially useful resource."
            }
        },
        "id": "OLVXn4WQvT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7FaWK7HpKK",
        "replyto": "7FaWK7HpKK",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1137/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505748,
        "cdate": 1696707505748,
        "tmdate": 1701465421490,
        "mdate": 1701465421490,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors extend the prototypical network approach to NLP to obtain an inherently interpretable model and propose a learnable weighting scheme for computing semantic similarity. They evaluate their approach on two text classification datasets (RT, AGNEWS) atop one encoder model (SBERT) and show that their proposed weighting scheme improves upon baseline similarity metrics. They further show that prototypical networks improve upon faithfulness — as estimated through comprehensiveness and sufficiency compared to a baseline.\n\nThe reviewers agree that the paper is well written and easy to follow (pffd, L24G). The reviewers further commend the simplicity, usefulness and relevance of the proposed method (AbxD, L24G, pffd).\n\nThe main concerns raised by the reviewers were (1) the lack of an analysis of the “interpretability tax” (L24G, pffd), (2) an unclear experimental setup (AbxD) and (3) a narrow experimental evaluation (L24G).\nIn the discussion period, the authors have addressed issues (1, 2, 3) with detailed comments and additional experiments, which they have committed to add to their paper — the author response has thus alleviated these concerns of the reviewers.\n\nAnother concern raised by the reviewers (L24G, AbxD) was the lack of relatively novel baselines, as the work of Lei et al is relatively outdated — as more novel baselines would help contextualize the contributions of the authors’ work. Some more recent and very relevant inherently interpretable methods which were not cited by the authors and can also be used as baselines are [1], [2]. While due to the speed of the field it is possible to miss relevant work, these papers are relatively well known in the field of explainable NLP and their omission is somewhat of a concern.\n\nFurthermore, reviewer AbxD accurately notes that the paper might benefit from some hedging with respect to the strength and generality of claims made. Apart from reviewer AbxD’s comment, in the abstract (and in the conclusion) the authors claim that “… our proposed method not only improves predictive performance…” — however this improved predictive performance holds with respect to the baselines without the learned weighting scheme, and not in general. The experimental scope should be clearly delineated in scope of the paper.\n\nThe conclusion also does not mention the extension of prototypical networks to NLP, which should be mentioned as a contribution. \n\n[1] Jain, Sarthak, et al. \"Learning to faithfully rationalize by construction.” (2020)\n\n[2] Bastings, Jasmijn, Wilker Aziz, and Ivan Titov. \"Interpretable neural predictions with differentiable binary variables.\" (2019)."
            }
        },
        "id": "S2DLu1vcKP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7FXgefa9lU",
        "replyto": "7FXgefa9lU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1603/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707519489,
        "cdate": 1696707519489,
        "tmdate": 1701465435818,
        "mdate": 1701465435818,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method to do dialog discourse parsing (DDP), which is distinct from other forms of discourse parsing and benchmarks.  (The authors present this as a \"fundamental NLP task\", but that would mean that most other NLP tasks require it, which is not quite the case in practice.) The method includes a combination of both bottom-up and top-down parsing, producing a pair of graphs that need joint decoding.  This is aided by a singular triangular window (SWT) mechanism to provide soft constraints on the way links are established between utterances.\n\nThe reviewers rightly pointed out the results are an improvement but rather marginal.  Also, the analysis on the Molweni task/dataset could be improved.  There were also concerns about reproducibility and comparison to GNN's, some of which were addressed in rebuttal."
            }
        },
        "id": "FGSuoohA2j",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7F5w5AQrv7",
        "replyto": "7F5w5AQrv7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3339/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560791,
        "cdate": 1696707560791,
        "tmdate": 1701465494450,
        "mdate": 1701465494450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper aims to use scene graph to improve the VL models. The scene graph supervises the vision branch via two tasks: 1. direct supervise with token prediction. 2. serves to construct hard negatives in the language branch. The overall paper quality is good."
            }
        },
        "id": "g2rnsXl4T4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7DueCuvmgM",
        "replyto": "7DueCuvmgM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission18/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707476953,
        "cdate": 1696707476953,
        "tmdate": 1701465384202,
        "mdate": 1701465384202,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a framework for selecting demonstration instances for in-context learning of generating SQL queries from text descriptions with LLMs. Unlike existing approaches, which focus on in-domain in-context learning, which requires hand-annotated in-domain examples, the proposed framework ODIS (1) selects the most useful demonstrations from other domains and (2) generates synthetic in-domain demonstrations. The experimental results reported by the authors demonstrate gains over state-of-the-art approaches on two different cross-domain text-to-SQL benchmarks (although the size of the gains is drastically different between the two datasets). \n\nWhile the retrieval of NLQ(uestion)-to-SQL demonstrations from other domains is already an established practice, the authors show, through a series of soundly executed ablation experiments, that it is the combination of retrieval out-of-domain demonstrations and synthesized in-domain examples that yields optimal performance. The gains over zero-shot NLQ-to-SQL have been reported for three LLMs (Codex in the submission, and then additionally ChatGPT and LLama in the rebuttal), indicating that ODIS is a robust approach, agnostic to the backbone LLM. A particularly valuable aspect of this work, which would be informative for future research efforts in NLQ-to-SQL, is the initial analysis of the distribution of SQL constructs in the in-domain demonstrations and the corresponding effectiveness on in-context learning performance. \n\nWhile the work is methodologically sound (especially the aforementioned analysis), the extent of novelty of both (1) similarity/coverage-based retrieval of out-of-domain examples as well (2) synthesis of in-domain examples from the NLQs with LLMs, is debatable, and subjective impressions of what is \"novel enough\" do permeate the reviews. I would, however, hold one thing against the empirical thoroughness of this work: lack of empirical comparison against (a) gold in-domain few-shot in-context learning (as the reviewer TqYu says: what happens if you annotate 10-30 gold in-domain examples) and (b) few-shot fine-tuning (in some parameter-efficient manner, e.g., with QLoRA, with a backbone stronger than the somewhat outdated T5), in addition to only in-context learning. While the authors argue that labeling 10-30 gold examples for 20 or so domains of Spider is \"extremely expensive\", and that this would not be realistic in the real-world with very many datasets, I'd beg to differ: any developer who wants/expects their database to be extensively queries by non-experts would most likely gladly create more than 30 NLQ-to-SQL examples to facilitate their database/app usage. This is why I'm not entirely sure that zero-shot NLQ-to-SQL with LLMs is the most competitive baseline the authors could have adopted -- if annotating a small number of NLQ-SQL pairs (e.g., 30) surpasses the performance of ODIS, then I'd question that ODIS would be widely used and have a large impact. That said, the analysis part that focuses on identifying in-domain SQLs that are more useful, is unaffected by these concerns. And also, these concerns don't seem to be the issue of this work alone, but the whole body of work on cross-domain transfer for NLQ-to-SQL generation.  \n\nOverall, I find this work to be sound and somewhat novel."
            }
        },
        "id": "aIozMZKpSx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7D4TPisEBk",
        "replyto": "7D4TPisEBk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission807/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496440,
        "cdate": 1696707496440,
        "tmdate": 1701465411167,
        "mdate": 1701465411167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces Token-Level Masking (TLM), a novel regularization technique for training Transformer models. TLM involves masking tokens in the attention layer, forcing models to exploit partial neighbor information for improved representation. Extensive experiments on various NLP tasks demonstrate the effectiveness of TLM compared to existing methods.\n\nReviewers had several concerns, including the need for a more detailed explanation of TLM's motivation and its applicability to larger models. They questioned the lack of hyperparameter search details, the choice of specific models for experiments, and the variation in tested model variants. Additionally, they sought clarification on the selection of specific tasks and models for ablation studies. The authors clarified TLM's motivation, addressed the issue of hyperparameters, and committed to providing results for larger models. They explained their choices for specific tasks and models in ablation studies and expressed willingness to conduct more experiments to address baseline comparisons. The authors have demonstrated a proactive approach to improving the paper in the revised version."
            }
        },
        "id": "Jh2M7ZfLRm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "7CTp2gwqin",
        "replyto": "7CTp2gwqin",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1809/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527643,
        "cdate": 1696707527643,
        "tmdate": 1701465442587,
        "mdate": 1701465442587,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a simple idea that improves learning of document embeddings. The idea is to apply standard a standard contrastive learning technique (operating at the below-document section level) that instills 'global' (i.e., document-level) understanding which in turn then enhances learning document embeddings. Contrastive learning operates, as usual, on the sets of positive and negative pairs, where the positive instances are sections sampled from the same document and negative instances are samples randomly taken from unrelated documents. Given that this is a short paper, it does a good coverage of tasks related to learning good document embeddings and it shows some improvements over standard document embedding models (e.g., Longformer, BigBird, Contriever). As mentioned (and also clearly stated by the authors), the idea is very simple, but it yields consistent gains in the three evaluation tasks. \n\nI agree with one of the reviewers that the paper should have ideally investigated the idea of contrastive learning also with non-random (e.g., topically more similar) negative samples which resembles the idea of using hard negatives in standard contrastive learning frameworks. The paper also does not disclose which exact contrastive objective/loss is used (as there are plenty of different options on how to implement the basic contrastive idea, and it is also possible to combine different losses).\n\nHowever, my main and strong concern is that, although the gains over the chosen baselines are salient, the paper does not provide a good comparison to all the relevant baselines and does not cover those recent methods and baselines in future work. The lack of these comparisons reduces the potential impact and it might negatively affect the novelty of the proposed method (again, noting its sheer simplicity). For instance, a body of work aims to improve (long) document embeddings by running specific contrastive learning techniques and the authors should ideally acknowledge and compare against the following papers at least:\n- https://arxiv.org/abs/2305.16031\n- https://aclanthology.org/2022.emnlp-main.802\n- https://arxiv.org/abs/2103.14542\n- https://aclanthology.org/2021.emnlp-main.109\n- https://aclanthology.org/2021.findings-emnlp.327.pdf"
            }
        },
        "id": "tecLfhEXKL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "77h6pSkw4N",
        "replyto": "77h6pSkw4N",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission579/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491177,
        "cdate": 1696707491177,
        "tmdate": 1701465403996,
        "mdate": 1701465403996,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a taxonomy that can be used to design prompts for LLMs to perform \"complex\" tasks. The authors describe the taxonomy and argue for it's usefulness, including a use case of meta-review generation. There are some concerns about the definition of a complex task, which is somewhat ambiguous in the paper, which have been answered in the rebuttal. Furthermore, there are some concerns about the practical usefulness of the taxonomy, which the authors assert is useful but it cannot be proven due to anonymity, meaning that this cannot be considered for the purposes of the review."
            }
        },
        "id": "pqANbcFpBx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "73kjtIZ4pt",
        "replyto": "73kjtIZ4pt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2545/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544483,
        "cdate": 1696707544483,
        "tmdate": 1701465468625,
        "mdate": 1701465468625,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviews have a consensus on positive evaluations for both soundness and excitement.\n\nRegarding excitement, the proposed method of training an NER model from noisy data is novel.  In the line of framing NER as constituency parsing, the method introduces uncertain nodes in the constituency tree and trains tree-structured CRFs with MC-Dropout to measure uncertainty.\n\nAs for soundness, the paper provides extensive comparisons with several baselines on 4 datasets and their noisy variants with respect to 3 annotation error types.  More descriptions on the quality of test sets (e.g., the corrections that were made) would improve the soundness of evaluation."
            }
        },
        "id": "9LTw6fG8vO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "71Lz8HW3NE",
        "replyto": "71Lz8HW3NE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2773/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549071,
        "cdate": 1696707549071,
        "tmdate": 1701465475826,
        "mdate": 1701465475826,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "An unsupervised contrastive learning strategy for representation learning, even with interesting results, the paper lacks comprehensive experiments and a sound explanation of graph structure in the method section."
            }
        },
        "id": "tQ6CobxRK2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6zSuMMtUjO",
        "replyto": "6zSuMMtUjO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2753/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548653,
        "cdate": 1696707548653,
        "tmdate": 1701465475133,
        "mdate": 1701465475133,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper tackles the word-level auto-completion task (WLAC), useful for computer-assisted translation. The authors propose a new method that, unlike previous approaches, can use target right-context in additional to left context, and handles the fact that either left and/or right context may be empty or incomplete. They experiment with pretraining and co-training on the MLM task along with WLAC to account for the fact that language modelling is done with incomplete target context. They also integrate an experiment with code-switched Pinyin-Chinese character data. They achieve better performance than the benchmark model, perform ablation studies on the model components and perform better on low-frequency words, which are the most important in the task.\n\nThe method surpasses the previous state-of-the-art results and although it builds on previous work, introduces a couple of novel elements (character-level encoding for the target prefix, pretraining and multi-task fine-tuning, iterative decoding process with an non-autoregressive MLM model). Ablations are included and the experiments appear sound. It is also important that the method improves results for low-frequency words. The main missing detail is the decoding speed and complexity, which would be important for real-world CAT applications. The authors provide some details about this in their rebuttal. This discussion should be added to the paper and would benefit from a comparison with other methods."
            }
        },
        "id": "O2nn74VfPA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6wj8Xczqkn",
        "replyto": "6wj8Xczqkn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1232/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507956,
        "cdate": 1696707507956,
        "tmdate": 1701465424578,
        "mdate": 1701465424578,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree this work proposes a useful evaluation resource, and a surprising negative result in how poorly ChatGPT performs on “no-evidence” questions.\nWhile the analysis is limited and the findings are only for ChatGPT, for a short paper this work provides useful resources and informative insights to practitioners using ChatGPT for question answering in ambiguous settings."
            }
        },
        "id": "2ZvJhr6R6o",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6tW1WEHIJe",
        "replyto": "6tW1WEHIJe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3191/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557815,
        "cdate": 1696707557815,
        "tmdate": 1701465489693,
        "mdate": 1701465489693,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a valuable resource to mitigate gender bias in machine translation in the form of GeNTE, an English to Italian test set for gender-neutral translation. It also investigates existing reference-based metrics for evaluating the quality of neutral translations and proposes a reference-free method that addresses the formers’ shortcomings. The strengths of the paper lie in its novelty and thoroughness, as indicated by its Soundness (three 4s, one 3) and Excitement (three 4s, one 3) scores. Multiple reviewers praised the clarity of the introduction, the motivation (particularly the human subjects survey on attitudes toward gender-neutral language), and discussion of limitations, though individual concerns were raised in regards to methodology, e.g. the use of regexes in data collection, the assumption that gender-neutral lexical items remain neutral in context, etc. The authors nevertheless addressed each of these concerns in their rebuttals to reviewer satisfaction.\n\nIn light of this, only minor revisions, addressing reviewers’ comments and questions, need to be made to ensure this paper is camera ready."
            }
        },
        "id": "V9JcUxntIC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6srsYdjLnV",
        "replyto": "6srsYdjLnV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4539/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707592608,
        "cdate": 1696707592608,
        "tmdate": 1701465534149,
        "mdate": 1701465534149,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers appreciated that the paper applies empirical data analysis methods to investigate linguistic phenomena, resulting better insights into the underlying linguistic phenomena (which could in turn be used to improve modeling of code-switching in data). The reviewers also note that the paper does an excellent job of introducing all of the relevant concepts, making the paper both clear and self-contained, even for a general NLP audience."
            }
        },
        "id": "yIG2dT1ddH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6pPCKWzYw4",
        "replyto": "6pPCKWzYw4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3987/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580718,
        "cdate": 1696707580718,
        "tmdate": 1701465516192,
        "mdate": 1701465516192,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes and approach to improve LLM-based re-ranking performance by including even a single example in the prompt. This improvement makes a significant impact under specific conditions that a detailed analysis investigates. \n\nThe paper is sound and well written. The authors were very effective, in my view, in motivating and explaining the value of the approach proposed (see details in the reviews and rebuttals). I think the paper will make an interesting contribution to the conference."
            }
        },
        "id": "KbPKdZPJQ7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6ne78DBkxl",
        "replyto": "6ne78DBkxl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4402/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707588958,
        "cdate": 1696707588958,
        "tmdate": 1701465530585,
        "mdate": 1701465530585,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a method relying on EGAT (edge graph attention networks) for hierarchical discourse dependency parsing. Several experiments and comparisons are reported showing that the method outperforms comparable methods on several standard corpora. Further analysis shows that the method is good for complex longer discourse dependencies/structures. All the reviewers agree on these. \n\nThe paper contributes to the field because there's relatively few works on discourse dependency parsing in general, or models that . Besides, the experimentation is good, sound and broad. However, there's an incremental feeling to the main ideas in that they basically adapt some existing methodology to an existing problem."
            }
        },
        "id": "fyG6wy43SD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6nLdWdTeos",
        "replyto": "6nLdWdTeos",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3205/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558111,
        "cdate": 1696707558111,
        "tmdate": 1701465490175,
        "mdate": 1701465490175,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a multilingual extension of the English HolisticBias dataset covering 50 languages, created by manual translation. This dataset is then used to quantify the gender bias in machine translation, and the authors find that MT models tend to favour masculine forms across languages.\n\nThe reviewers find that the proposed resource will be valuable for studying and analyzing demographic biases in multilingual machine translation systems and multilingual sentence encoders. Most of the paper was found to be clearly written and easy to follow. Reviewers remarked, however, that some crucial references on biases in MT were missing, and that some parts of the paper could benefit from additional clarifications. The requested changes should be easy to accommodate in the camera-ready version."
            }
        },
        "id": "tEzo4MWPYw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6muz29kMQu",
        "replyto": "6muz29kMQu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission491/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489119,
        "cdate": 1696707489119,
        "tmdate": 1701465401041,
        "mdate": 1701465401041,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a unique dataset focused on hate and offensive speech in Korean. This dataset is distinct due to its detailed labeling system that measures varying degrees of offensiveness, aiming to capture both explicit and implicit offensive language. The data is sourced from a major South Korean news portal, and its annotations are designed to identify the target of the hate speech, the rationale behind it, and the level of offensiveness. To ensure the quality of annotations, the dataset employs a cognitive reflection test to filter out potentially biased annotators. The dataset comprises news comments, and a significant number of annotators (405) from a crowdsourcing platform have been involved in the annotation process. The paper also explores the application of this dataset in hate speech detection tasks, using advanced models. A case study within the research indicates a higher occurrence of hate speech in hard news comments compared to soft news. However, there are concerns about the dataset's sourcing and the inter-annotator agreement process.\n\nThe paper presents a novel dataset for hate speech classification in Korean, filling a gap in the research landscape. The dataset's uniqueness is further emphasized by its detailed annotation scheme, which captures the implicit nuances of hate speech. This scheme is not only innovative in the categories it annotates but also in its hierarchical organization. Another significant contribution is the use of the cognitive reflection test (CRT) to filter out biased or skewed annotators, ensuring the quality and reliability of the dataset. The paper is well-organized, with clear presentation and structure, making it accessible to readers. The research demonstrates strong performance in the hate speech detection task, providing promising results that can serve as benchmarks for future studies. Overall, the paper's contributions, particularly the introduction of a new resource for Korean and the thoughtful consideration of potential biases in annotation, make it a valuable addition to the field.\n\nOne of the main issues is that the authors' discussion on implicit hate speech is unconvincing. They overlook the fact that many hate speech studies are based on social media data, where explicit hate is often removed. Their definition of implicitness, equating it to irony or sarcasm, is not universally accepted and lacks a thorough discussion. Moreover, the paper doesn't differentiate between implicit and explicit forms of hate in its annotations or evaluations.\n\nThe authors and the reviewers interacted during the rebuttal period to fix some other issues indicated by the reviewers."
            }
        },
        "id": "iflSl7Mwbt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6mZIF4OxSq",
        "replyto": "6mZIF4OxSq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2689/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547611,
        "cdate": 1696707547611,
        "tmdate": 1701465473446,
        "mdate": 1701465473446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents GlobalBench: a benchmark that measures global progress in NLP. \nGlobalBench improved upon existing multilingual benchmarks such as XTREME or XGLUE. It supports 6671 languages and 17 tasks. \nInstead of aggregating accuracy metrics like it's done in most benchmarks, GlobalBench provides a multi-view task-specific dashboard based on three main metrics:\n- Performance at a given task (e.g. BLEU for Machine Translation)\n- Utility that tracks the downstream utility of a system based on how many speakers can benefit from it and how many languages can benefit from it. \n- Equity: Based on the Gini index, it measures how equitable, in terms of language coverage, state-of-the-art NLP technologies is\n\nWith Utility, Globalbench provides a leaderboard that incentivizes utility. It ranks all the 6671 supported languages based on a measure of utility that balances demographic and linguistic utility, identifying the languages for which progress is the most needed. \nNot only does utility incentivize performance improvement at existing task and dataset, but it also incentivizes the addition of new dataset and new task. \n\nReasons to accept: \n- This paper provides a usable resource to promote measured empirical progress of NLP systems on potentially all languages worldwide (R1, R2 and R3).\n- The design choices for Globalbench are novels and thoroughly described  \n- Specifically, the integration of utility and equity metrics provides a clear incentive to make progress on under-served languages\n\nReasons to Reject: \n- A testable anonymous GlobalBench would have been ideal to make the contribution even more concrete."
            }
        },
        "id": "5dmUvK4ccL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6mPs06irie",
        "replyto": "6mPs06irie",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5116/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707607377,
        "cdate": 1696707607377,
        "tmdate": 1701465550386,
        "mdate": 1701465550386,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The new task of using reasoning for Indirect queries in the task of object detection is very interesting. The novelty of the task that the paper addresses is exciting. The authors have resolved many of the reviewers' questions with the rebuttal. I hope that these results will be properly reflected in the camera-ready version. Reviewer Hwbg makes a good point, but I don't think it is practical to cover the various data sets proposed by the reviewer. It is important that the paper is validated to be at least adequate to the claims made in the paper.\n\nPros:\nThe task is new and unique.\nThe paper is clearly written.\nThe results sound convincing.\n\nCons:\nMany new results were presented during the rebuttal."
            }
        },
        "id": "ewbeft55dH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6lXuQBMsyM",
        "replyto": "6lXuQBMsyM",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission719/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494405,
        "cdate": 1696707494405,
        "tmdate": 1701465408282,
        "mdate": 1701465408282,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a novel framework named Bi-ACL, aimed at addressing two key issues in multilingual Neural Machine Translation (MNMT), specifically data imbalance and representation degeneration.\n\nPros:\n- Combine several popular (and easy to implement) techniques into a novel framework for enhancing MNMT. These improvements demonstrate consistency across various data conditions.\n- Utilizes readily available resources that are more accessible than parallel data, namely target-side monolingual corpora and bilingual dictionaries.\n- Addresses the challenge of extremely low-resource translation directions (alongside with mid- and well-resource).\n- The paper is presented in a clear and comprehensible manner.\n\nCons:\n- The rationale behind the combination of these techniques is not sufficiently elucidated. The individual techniques themselves are not novel.\n- Some minor ambiguities/errors in the paper require clarification/correction, which was addressed during the rebuttal phase thanks to the reviewers' feedbacks."
            }
        },
        "id": "urcYSFnfph",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6jik3wCbTr",
        "replyto": "6jik3wCbTr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3650/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566999,
        "cdate": 1696707566999,
        "tmdate": 1701465505483,
        "mdate": 1701465505483,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes to add a component to LLMs that constructs a \"belief graph\", repair inconsistencies, and produce faithful reasoning chains drawn from a belief system with significantly improved consistency. All reviewers praised the work, highlighting the provided consistency gains, exposition, and analyses. Some concerns were inherent to whether the methodology proposed in this paper could transfer to other domains, about the choice of the datasets, and whether the belief graphs faithfully reflect how the model operates internally. The authors provided clarifications in their rebuttal. Overall, the reviewers found the paper a significant contribution to NLP, which could open interesting research avenues. Two reviewers out of three recommend it for a best paper award."
            }
        },
        "id": "yKRGkldiWi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6j7JZnEzf4",
        "replyto": "6j7JZnEzf4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4013/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581236,
        "cdate": 1696707581236,
        "tmdate": 1701465517020,
        "mdate": 1701465517020,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a self-distillation method for training non-autoregressive models. The idea is simple, effective and well-evaluated with extensive experiments."
            }
        },
        "id": "nAy5cc2Tt8",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6i98agKoZ1",
        "replyto": "6i98agKoZ1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission541/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490328,
        "cdate": 1696707490328,
        "tmdate": 1701465402769,
        "mdate": 1701465402769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper addresses an important question that when an LLM is trained with outdated information, it fails to generate correct responses for time-sensitive questions. The paper introduces a factual direction prediction component to determine whether there is a temporal mismatch between the training data and test data. All reviewers appreciated the clarity of the problem space and solution, and solid experiments."
            }
        },
        "id": "HMF6QZsNGg",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6eBgIRnlGA",
        "replyto": "6eBgIRnlGA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3994/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707580825,
        "cdate": 1696707580825,
        "tmdate": 1701465516422,
        "mdate": 1701465516422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper is centered on estimating the number of bots (percentage) within a certain community. The proposed method is an ensemble on top of existing approaches and then confidence is calibrated to identify if a particular account is a bot or not. \n\nThe proposed approach is interesting although it shows some limitations and drawbacks:\n1) It is not clear how the community has been defined in the evaluation set. \n2) Related work should be extended by considering more recent works on data mining field.\n3) The authors need to explain how they ensure any sampling of bots for both experimental analysis."
            }
        },
        "id": "QaS0fDjYaT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6dyvFZLRX8",
        "replyto": "6dyvFZLRX8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4998/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707604331,
        "cdate": 1696707604331,
        "tmdate": 1701465546882,
        "mdate": 1701465546882,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Paper studies positional encodings of encoder only models for NLP (BERT) and proposes probing tasks and formulation to investigate symetry and locality of the positional encodings. Reviewers raised some concerns about the experimental design and formulation."
            }
        },
        "id": "LjgazFAuhC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6c2s6HddQ4",
        "replyto": "6c2s6HddQ4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3245/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559009,
        "cdate": 1696707559009,
        "tmdate": 1701465491820,
        "mdate": 1701465491820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Reject"
            },
            "comment": {
                "value": "An overview of OOD detection in NLP is provided in this paper. The paper is well-written, and the proposed taxonomy seems reasonable.\n\nThe paper raises the following concerns among reviewers:\n* The survey is relatively narrow in focus and contains flaws in flows and structures; lack of a survey on other types of shift (i.e., non-semantic shift from different domains/styles); lack of discussion of pros and cons of the datests and metrics; no clear categorization of methods and metrics; \n* An empirical evaluation of the selection of which method to use is not included in the survey. It may not be fair to include empirical results in the survey work, but I think a clearer guideline is needed for incorporating new research into your taxonomy.\n* There is a discrepancy between the proposed taxonomy and the previous study [1,2].\n\n[1] Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey. \n[2] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges."
            }
        },
        "id": "fGqKRilOds",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6YQ1uh9IGG",
        "replyto": "6YQ1uh9IGG",
        "number": 2,
        "invitations": [
            "EMNLP/2023/Conference/Submission61/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478113,
        "cdate": 1696707478113,
        "tmdate": 1701465385706,
        "mdate": 1701465385706,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents an analysis of multilingual speech-to-text translation representations, with a focus on analyzing similarity of sentence-level multilingual representations via Singular Value Canonical Correlation Analysis (SVCCA). There is some divergence in numerical scores, but the overall tone of the review text is similar across all 3 reviews. Reviewers are enthusiastic about this type of multilingual representation analysis and its general usefulness, but they also have some concerns and suggestions for these specific experiments. In particular, two reviewers mentioned concerns about the validity of drawing conclusions regarding the similarity of languages with very small amounts of data, indicating the existing discussion on data quantity may need to be revised. The authors seem amenable to the suggested revisions, and have already provided some new results."
            }
        },
        "id": "8fyQ84vP5g",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6WcsVlZE5I",
        "replyto": "6WcsVlZE5I",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3098/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556067,
        "cdate": 1696707556067,
        "tmdate": 1701465486890,
        "mdate": 1701465486890,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates the utility of incorporating knowledge into Large Language Models for the task of ad hoc dataset retrieval. The authors conducted a systematic investigation of implicit and explicit knowledge-enhanced methods for ad hoc dataset retrieval on two test collections. The main contribution is that the authors evaluated a field-weighted BM25 baseline against dense retrieval approaches against approaches based on entities extracted using established entity linking methods and against approaches that combine dense retrieval and entities. Experimental results show that combined methods can achieve a better performance.\n\nThe approach presented is sound and the paper is clear and well written. The arguments are well supported by results and analysis."
            }
        },
        "id": "KXFcdO3MJJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6UklbMESHZ",
        "replyto": "6UklbMESHZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5339/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611199,
        "cdate": 1696707611199,
        "tmdate": 1701465556753,
        "mdate": 1701465556753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper focuses on open-world relation extraction problem, and emphasizes novel classes including long-tailed relations. Experiments are generally solid, with large model-based analysis. All 3 reviewers reached a consensus on strong soundness and excitement."
            }
        },
        "id": "IKvFBB3Qj0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6RuXWFEQzg",
        "replyto": "6RuXWFEQzg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission841/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497425,
        "cdate": 1696707497425,
        "tmdate": 1701465412510,
        "mdate": 1701465412510,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper describes the introduction of IEKG, a commonsense knowledge graph extending ATOMIC for the figurative meaning of idiomatic expressions. After describing the annotation process, the authors present some experiments on the benefits on training LMs on IEKG with a knowledge tuple completion task, showing that all LMs fine-tuned with IEKG manage to outperform a BART Comet baseline. Additionally, they present an extrinsic evaluation of the resource, showing that IEKG injection leads to better performances both in a natural language inference and in a context continuation task.\n\nAll the reviewers agree on the usefulness of the proposed resource and appreciate the careful description of the annotation process and of the steps taken for quality control. The experimental results seem solid, with the incorporation of IEKG leading to consistent improvements in tuple completion and in the two downstream tasks.\nA possible weakness that has been mentioned (R2 and R3) is that the authors could have evaluated their model on a larger number of tasks/datasets (although I must say that I am already positively impressed by the experiments included in the current version of the paper, which also includes additional model evaluation in the Appendix)."
            }
        },
        "id": "E9qYT83JlO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6RQTvSLbgi",
        "replyto": "6RQTvSLbgi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2212/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537128,
        "cdate": 1696707537128,
        "tmdate": 1701465458021,
        "mdate": 1701465458021,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Pros: \n- This paper introduces two new multilingual document image classification datasets: MULTIEURLEX-DOC and WIKI-DOC\n- \"The paper reports and analyzes challenges and limitations posed by these datasets which would be inspiring for future research.\"\n\nCons:\n- The details on the proposed dataset and experimental setup are highlighted to be confusing and can be made clearer."
            }
        },
        "id": "6wSh8kBLMu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6P32h3LTC1",
        "replyto": "6P32h3LTC1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2121/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535000,
        "cdate": 1696707535000,
        "tmdate": 1701465454855,
        "mdate": 1701465454855,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a new method for reducing subjective bias in text without the need for parallel data using a cycle-consistent adversarial network. The methodology proposed is novel and the method is both conceptually useful and demonstrates an improved performance over prior methods. The paper is well-written. There are minor points that could have been improved -- the method was only trained on Wikipedia; hence, it is unclear how it would fair on other domains. The method has though been tested on a variety of domains. Moreover, the reviewers raise the point that the baselines that the authors compare to are quite old. This is though because there are no more recent baseline methods which operate on non-parallel data. It does make it easier to propose a method which outperforms prior work though."
            }
        },
        "id": "QHfjq7Labe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6LkytBaTy9",
        "replyto": "6LkytBaTy9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4228/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585770,
        "cdate": 1696707585770,
        "tmdate": 1701465524774,
        "mdate": 1701465524774,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies the behavior of models on human-constructed vs machine generated prompts, finding that machine generated prompts are processed in a qualitatively different way compared to human-constructed prompts. The findings in this study are very intuitive, and all reviewers appreciated the analysis in the paper."
            }
        },
        "id": "Nh9pcg9jaN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6KyZrSp8y3",
        "replyto": "6KyZrSp8y3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1866/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529030,
        "cdate": 1696707529030,
        "tmdate": 1701465445193,
        "mdate": 1701465445193,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a study comparing the performance of different combinations of domain-expert models against a big model trained on a combination of the data of all domains. All the reviewers agree that the work is sound. One common concern raised by the reviewers is that results are only presented on one language pair (en-ja), however the authors pointed out that according to the guidelines this should be enough for a a short paper and I agree with that. It presents some initial experiments that can trigger further research.\n\nIt is also worth noting that there was some confusion among the reviewers concerning the access to references. The authors addressed these issues in their response and I want to reiterate this here: MBR and QE do *not* have access to the references, thus they are fair methods to include in the comparison. Oracle does have access to the references, and therefore it is marked as such and provides an upper bound to the performance of the model. This does not compromise the soundness of the paper.\n\nThe weak point of this work in the excitement. None of the reviewers was overly enthusiastic, and I tend to agree with their judgement. There are no new methods presented in the paper, and it mainly consists of the experimental comparison between the presented ones. It can constitute a useful resource for practitioners, but I'm a bit reluctant to allocate a slot in the main conference for it. I think Findings would be a better destination, where it can constitute a reference to trigger further research."
            }
        },
        "id": "IEK5LLN7QQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6Jqa4YmUMf",
        "replyto": "6Jqa4YmUMf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission648/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492887,
        "cdate": 1696707492887,
        "tmdate": 1701465406409,
        "mdate": 1701465406409,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel method for misinformation detection using large language models (LLMs), substantiated by extensive experiments. The authors employ a prompting technique and find ChatGPT effective in identifying its generated misinformation. The paper's clarity and dataset quality evaluation have been criticized, but the authors present a reasonably good rebuttal. Despite some remaining concerns around the paper's assumptions, it offers timely and crucial insights into misinformation detection. However, it needs further refinement, particularly in experimental design and results presentation. Despite minor points needing attention, the paper usefully contributes to the misinformation detection field."
            }
        },
        "id": "bfdOcgSsSX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6DMhUhx5oy",
        "replyto": "6DMhUhx5oy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission591/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491532,
        "cdate": 1696707491532,
        "tmdate": 1701465404397,
        "mdate": 1701465404397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces GEEK, a method that employs multi-step reasoning and retrieval from external knowledge sources to answer implicit questions. Such questions typically pose challenges for LLMs due to outdated parametric knowledge. GEEK incorporates both question decomposition and a halting condition. On StrategyQA, GEEK achieves a new state of the art of 78.17%, up from the previous 77.73%, while using slightly fewer parameters. The approach is well-motivated and intriguing. Given the rising prominence of LLMs, this paper could be of significant value to the community. We strongly encourage the authors to address and refine the areas highlighted by the reviewers."
            }
        },
        "id": "WQkvpHfllc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "6DKS4tb387",
        "replyto": "6DKS4tb387",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4893/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707600340,
        "cdate": 1696707600340,
        "tmdate": 1701465544103,
        "mdate": 1701465544103,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces the COVIDET-APPRAISALS dataset, focusing on the appraisal dimensions of emotions in Reddit posts. It also evaluates large language models (LLMs) on this dataset. While the paper makes significant contributions to the understanding of cognitive appraisal in emotions, there are areas that need attention.\n\nPros in Reviews:\n- The paper presents a novel and valuable dataset, COVIDET-APPRAISALS, with detailed annotations and rationales for 24 cognitive appraisal dimensions.\n- It addresses an important and underexplored task in NLP, emotion cognitive appraisal, filling a gap in the field.\n- The paper conducts extensive evaluations and analyses of state-of-the-art LLMs on the task, shedding light on the challenges involved.\n- There is a strong correlation on the subset of data annotated by two labels, demonstrating the reliability of the annotations.\n\nCons in Reviews:\n- The dataset size is relatively small, comprising only 241 Reddit posts. The paper lacks a clear rationale for why understanding subjective cognitive appraisals is essential, limiting the broader context of the research's significance.\n- The claim that COVIDET-APPRAISALS is \"the most comprehensive dataset to date\" lacks support and a comparison with other similar datasets.\n- The paper experiments with LLMs under a zero-shot setup, missing the opportunity to explore other techniques like in-context learning or few-shot fine-tuning. Comparing LLM performance with smaller supervised models could provide additional insights.\n- The experimental setup could be improved for consistency. The use of one or two annotators for labeling and variations in prompting for LLMs should be better justified.\n- The human evaluation setup may be misleading, as it filters outputs prior to evaluation, potentially overestimating system performance. It also makes comparisons with future works challenging.\n- The exclusion of three appraisal dimensions from the final corpus is questionable, given the relatively low NA rates, and NA prediction is considered part of the task.\n\nIn summary, the paper contributes significantly to the understanding of emotion cognitive appraisal and presents a valuable dataset, but it should address concerns about dataset size, support claims regarding dataset comprehensiveness, explore different experimental setups, and reconsider the exclusion of certain appraisal dimensions. Despite all these problems, the authors have written good rebuttals regarding the main criticism. Looking at the work that would need to go into the paper to address all those comments, I would argue for a similar rating in-line with those of the reviewers."
            }
        },
        "id": "1xxSO6OpKh",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "68A4GE4nqf",
        "replyto": "68A4GE4nqf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2354/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540223,
        "cdate": 1696707540223,
        "tmdate": 1701465462376,
        "mdate": 1701465462376,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper aims to validate where morphology and syntax are encoded in pre-trained monolingual BERT models. The authors leverage Centered Kernel Alignment to assess the similarity between weight matrices in BERT and conclude that indeed syntax is encoded in the middle layers of BERT and morphology is more akin to be encoded by the attention layer, though less strongly. The authors also convincingly demonstrate that syntax remains encoded in the middle layers across typologically different languages.\n\nThe main contributions of the paper are a methodology to assess where specific linguistic phenomena are encoded in pre-trained models and an experimental validation on where morphology and syntax are encoded in multiple (including on low-resource languages) pre-trained monolingual BERT models. As a consequence of their experiments, the authors arrive at a conclusion analogous to Tenney et al. (2019) on where BERT appears to encode syntactical information and thereby independently validate the earlier results with a slightly different methodological appraoch. A (side) contribution of the work is the addition of missing features in WALS which the authors leverage for their experiments.\n\nAll in all the paper is in a good shape. Some additional discussion and anaylsis may be added but this is not a major issue."
            }
        },
        "id": "3idXqDUSjp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "65teZsn7HR",
        "replyto": "65teZsn7HR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3535/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564760,
        "cdate": 1696707564760,
        "tmdate": 1701465500591,
        "mdate": 1701465500591,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents models to detect explicit discourse connectives and alternative lexicalisations, as well as to classify their senses (based on PDTB3). The models are applied on texts of various genres and domains. The authors found out that n-gram patterns are stronger predictors than unigram ones.\nThe paper has several cons: (1) working not only on explicit connectives, but also alternative lexicalisations, which so far has received very little attention;  (2) using sense flows to identify rhetorical patterns of different genres and domains is novel; (3) using various levels of PDTB sense hierarchy; (4) in general a well-structured and a well-described paper enabling the replication of this work.\nBut it has also some disadvantages: the minor ones include some flaws (e.g. missing references or typos). The major ones include: (1) a lack of clarity in the subtasks or their connection; (2) comparison with the baseline and (3) a terminological confusion in terms of rhetorical analysis. \nHowever, the authors actively participated in the discussion and their rebuttal sounds convincing.r"
            }
        },
        "id": "Kl1aUShrNL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "63UKbaiyAe",
        "replyto": "63UKbaiyAe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3138/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556860,
        "cdate": 1696707556860,
        "tmdate": 1701465488261,
        "mdate": 1701465488261,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work focuses on increasing the parameter efficiency of prompt tuning through the use of sparsely selected prompts. Authors managed to fit many interesting results in a short paper and provide convincing evidence during the rebuttal. All reviewers agree on the usefulness of the method and experimental evidence (which is almost at a long-paper level). I encourage authors to add the relevant work and comparison discussed during rebuttal (e.g. Adamix, Attempt)."
            }
        },
        "id": "fSx1reH5hA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5x5Vxclc1K",
        "replyto": "5x5Vxclc1K",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3341/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560860,
        "cdate": 1696707560860,
        "tmdate": 1701465494480,
        "mdate": 1701465494480,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a novel hierarchical contrastive learning approach for the zero-shot slot filling task. It introduces token-level contrastive learning and uses Gaussian embedding to improve generalizability in unseen domains. Reviewers appreciates the comprehensive nature of the paper and finds the idea of token-level contrastive learning innovative. The detailed ablation studies make the work convincing. However, they have some questions about the method and suggest more clarification and analysis. Reviewers acknowledges the solid experiments and good performance achieved. However, they point out a lack of MRC-based SF baselines, unclear descriptions, and a missing citation. They also have questions about the loss computation and entity-level representations. \n\nBased on these reviews, the paper is considered of good soundness and shows some originality. However, there are concerns regarding clarity and missing references. The reviewers find the results and ablation studies compelling. The pros of the paper include the clear introduction, innovative token-level contrastive learning, and comprehensive ablation studies. The cons include the lack of clarification in the method section, missing citation for the contrastive loss, and unclear descriptions. Overall, the paper has potential but needs some revisions and clarifications."
            }
        },
        "id": "OLIUs23eI7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5vOHRbLNE7",
        "replyto": "5vOHRbLNE7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission760/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707495340,
        "cdate": 1696707495340,
        "tmdate": 1701465409544,
        "mdate": 1701465409544,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work describes and evaluates a novel and interesting benchmark for lateral thinking composed of 1,100 puzzles accompanied by high-quality annotations.\n\nAll three reviewers agree that the soundness of the proposal is either good or strong and there is a clear consensus on the excitement being strong for all three too. To the extent possible, I would like to see the reviewers questions & concerns incorporated into the final version of the paper in order to improve its clarity. These include: further explanations on the difficulty of the task itself and the evaluation metrics chosen."
            }
        },
        "id": "2sWYh4RqL0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5uZQ6spv9u",
        "replyto": "5uZQ6spv9u",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5180/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608656,
        "cdate": 1696707608656,
        "tmdate": 1701465552136,
        "mdate": 1701465552136,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work investigates a reason for the zero-shot failure of large-scale Vision-Language Models (such as CLIP) on compositional VQA questions, showing these models treat the image and the text inputs as two separate \"bags of concepts\" and attempt to fill in the other missing concept crossmodally, and that this issue is more prevalent in models trained with contrastive loss as opposed to an autoregressive loss. The authors further propose a fine-tuning based solution to mitigate this problem.\nThe work provides enough experiments using various models (including CLIP, BLIP, BLIP-2 and OFA), and more generally provides some interesting insights and inspiring findings for future work. The paper is well-written in an exposatory style and clearly structured."
            }
        },
        "id": "Tpu9XZJOwz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5sGLPiG1vE",
        "replyto": "5sGLPiG1vE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2171/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536094,
        "cdate": 1696707536094,
        "tmdate": 1701465456602,
        "mdate": 1701465456602,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors analyze how NLG models represent uncertainty. The authors argue that NLG systems should capture variation within human language production.For every generation prompt text, the authors compare properties of generations obtained through repeated model sampling, with multiple human productions— finding that uncertainty is lower when the task setting is more constrained. All authors appreciated the motivation of this work and recognize the need for improved calibration methods."
            }
        },
        "id": "3vmNRSbo0u",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5o4a4OjhQW",
        "replyto": "5o4a4OjhQW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3289/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707559750,
        "cdate": 1696707559750,
        "tmdate": 1701465492964,
        "mdate": 1701465492964,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes the IBKD method in order to make pre-trained language models (PLMs) more accessible by distilling them into smaller representation models. The topic studied is an interesting topic and the proposed method based on information bottleneck is meaningful. Reviewers have concerns on the insufficient experiments or missing details. During the rebuttal period, the authors have replied to these concerns with more details and experimental results, which lead to overall positive scores from reviewers."
            }
        },
        "id": "rexN1ve6qt",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5nHLFcj7Y9",
        "replyto": "5nHLFcj7Y9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2676/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547331,
        "cdate": 1696707547331,
        "tmdate": 1701465473106,
        "mdate": 1701465473106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Two reviewers provided strong reviews, giving scores of 4 or even 5 for soundness and excitement.\n\nOn the other hand, R2 (UZhg), in the original review and subsequent discussion with the authors and a co-reviewer, argues that the paper is fundamentally out of scope for this conference and the track it was submitted to. The authors provided relevant arguments and evidence against this view, which I consider overall compelling. I thus discounted the first, second, and fourth paragraphs listed under \"Reasons To Reject\" in Review UZhg as justifications for their low soundness score. On the other hand, I consider their low excitement score to be justified by the review.\n\nI conclude that there are no major soundness concerns. Excitement was very mixed across the three reviewers.\n\nStrengths and weaknesses including the following were mentioned:\n\nStrengths:\n\n- task and evaluation were carefully designed (R1, R3)\n- well-written (R1)\n\nWeaknesses:\n\n- lack of detail on annotation process (R1)\n- unclear what the results say about the inner workings and what implications they have for NLP (R2)\n- corpus was not available for review (R2)\n- limited focus of research question (R3)\n- human evaluators were not given training tasks (R3)"
            }
        },
        "id": "UIuaBcUd4Y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5kV1ZwKMeQ",
        "replyto": "5kV1ZwKMeQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission963/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500353,
        "cdate": 1696707500353,
        "tmdate": 1701465416191,
        "mdate": 1701465416191,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes using a single LLM to, given a query, generate keywords for retrieval and then produce the final answer in one pass. Constrained decoding is adeptly implemented, using the FM-index, to ensure that the documents contain the specified keywords. Experiments validate the efficacy of the proposed method on Open-NQ (as presented in the paper), as well as on WebQuestions and CuratedTrec in a zero-shot manner, as recommended by the reviewers (details reported in the rebuttal). The paper is excellently presented with clear motivation. We strongly urge the authors to address and refine the areas pinpointed by the three reviewers."
            }
        },
        "id": "qEyNLkZ8Tg",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5jc17fMzqf",
        "replyto": "5jc17fMzqf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2072/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707533688,
        "cdate": 1696707533688,
        "tmdate": 1701465452851,
        "mdate": 1701465452851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree this work tackles an important problem, proposes interesting new methods, and achieves promising results. The authors promise to add new experiments that address the primary concerns, primarily around the scope of models and datasets, which would extend the generalizability of the conclusions, and would improve this work's appeal as a contribution worthy of greater discussion in the community."
            }
        },
        "id": "Rb32IpExHj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5ZHznxXCIb",
        "replyto": "5ZHznxXCIb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission607/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491979,
        "cdate": 1696707491979,
        "tmdate": 1701465405062,
        "mdate": 1701465405062,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The manuscript presents a contribution in the field of knowledge distillation, proposing the CEMAL method, which distills math word problem-solving capabilities from large language models (LLMs) into smaller student models, which has proven previously to have problems with CoT. This approach, inspired by educational science principles like knowledge tracing and personalized learning, addresses common limitations of existing methods and outperforms them in both in-distribution and out-of-distribution settings. The manuscript provide a thorough experimental evaluation. The main concerns is the presentation of findings and highlighting exact contributions in relation to existing techniques. We encourage authors, as stated in the rebuttal to address these concerns to make their manuscript more accessible to the community."
            }
        },
        "id": "onhGGsqPew",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5UW6Mivj9M",
        "replyto": "5UW6Mivj9M",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1995/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707531692,
        "cdate": 1696707531692,
        "tmdate": 1701465449853,
        "mdate": 1701465449853,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a benchmark for theory of mind conversations, which is novel and timely wrt to the current pace of LLM development. Authors in their rebuttal also confirm the intention to release this benchmark for general use ASAP, which is the most valuable part. Contingent on this release, the paper can be very helpful for LLM evaluations. I would also urge authors to release a datasheet, detailing aspects like cost of data construction, intended usage, etc. This recurs in reviewers’ questions and underlines how any reader or user will have the same questions. \nAdditionally, like reviewers point out, some of the discussions, specifically around anthropomorphism and limitations wrt it should appear much earlier in the paper to establish the context of benchmark usage.\nWith these updates, the work would be of great use to the community."
            }
        },
        "id": "xbeCpKUVSW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5TEfD2GBUc",
        "replyto": "5TEfD2GBUc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1400/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707512261,
        "cdate": 1696707512261,
        "tmdate": 1701465429803,
        "mdate": 1701465429803,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper review evaluates the performance of the GPT-4 model in the context of radiology reports, comparing its performance against state-of-the-art radiology-specific models. The paper also investigates different prompting strategies and provides a detailed evaluation framework. The results demonstrate that GPT-4 either matches or surpasses the performance of specialized models, suggesting its potential as a versatile foundation for various specialized tasks. The paper also highlights the importance of advanced prompting for certain tasks.\n\nAll three reviewers acknowledge the value of the paper's extensive evaluations and analyses on GPT-4's performance in the radiology domain. They also appreciate the clarity and thoroughness of the paper. The reviewers raise concerns regarding the novelty of the work, the cost and accessibility of the GPT-4 model, and the need for comparison with other open-source large language models. They also suggest the inclusion of more advanced prompting strategies, qualitative evaluation by human experts, and an ethical statement section.\n\nOverall, the paper presents valuable insights into the application of GPT-4 in the radiology domain, presenting findings that could inspire further research in the area, and the evaluation framework proposed could be beneficial to the community. It is recommended that the authors take into consideration the reviewers' suggestions for improvement in future work."
            }
        },
        "id": "CshM4yDSFw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5QNpjtdjD8",
        "replyto": "5QNpjtdjD8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1849/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528722,
        "cdate": 1696707528722,
        "tmdate": 1701465444655,
        "mdate": 1701465444655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a simple PTQ pipeline for the LLM quantization. Reviewers initially had concerns about missing experiments, references, and limited novelty. The authors provided a rebuttal to address them. Based on the unanimous agreement among reviewers, the AC deemed this paper should be accepted."
            }
        },
        "id": "t7ptKJKPHB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5PvFFNRTbp",
        "replyto": "5PvFFNRTbp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1882/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529359,
        "cdate": 1696707529359,
        "tmdate": 1701465445689,
        "mdate": 1701465445689,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper's comprehensive evaluation of existing Biomedical Entity Linking models is agreed upon by all reviewers, that the framework is a potentially valuable tool benefiting the development of future BioEL methods.\n\nDespite these strengths, the reviewers simultaneously acknowledged certain shortcomings. These include a lack of a precise explanation of the evaluation framework and the audience limitation due to the specific focus of the paper.\n\nIn summary, the paper is sound and offers significant insights regarding the BioEL task. However, it is recommended that the authors undertake a revision to address the issues and suggestions raised by the reviewers. Specifically, there is a need for additional clarity regarding some of their claims about current methods' struggles, as well as an accurate reporting of previously published SOTA results for every dataset. Further, a detailed outline of the evaluation framework is necessary."
            }
        },
        "id": "goTqmFcjLO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5Ob6DsDv2V",
        "replyto": "5Ob6DsDv2V",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2162/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535914,
        "cdate": 1696707535914,
        "tmdate": 1701465456240,
        "mdate": 1701465456240,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a method using fast-slow contrastive learning with a memory buffer for continual learning.  Experimenta result prove the effectiveness of this approach.\n\nAll reviewers find the information theoretic analysis insightful and interesting. The experimental results (with the additional results provided in response) are adequate. \n\nThe discussion and writing could be improved (e.g. motivation of fast-slow contrastive learning). The authors may want to discuss related work (e.g. Co2L, OCM) and incorporate the response. We highly encourage authors add additional results provided in response."
            }
        },
        "id": "mN0y0VOk49",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5NMl0TYLey",
        "replyto": "5NMl0TYLey",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1313/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707510186,
        "cdate": 1696707510186,
        "tmdate": 1701465427151,
        "mdate": 1701465427151,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a new method for video paragraph captioning without event annotations called Sparse Frame Grouping, using modifications of contrastive learning methods to improve model precision and reduce repetition.\n\nReviewers generally found the problem important and that the proposed method improves performance with sound experimental evaluation. The main concerns revolved around more ablation experiments, analysis of learned model's segmentation capabilities, more comparisons, and scale of improvements. The authors clarified the majority of the problems. Reviewer UFoE had some remaining concerns regarding writing and presentation, especially on describing the method and triplet selection in contrastive learning, which I encourage the authors to carefully revise."
            }
        },
        "id": "ad09NWRVie",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5K2fiOlcGG",
        "replyto": "5K2fiOlcGG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2301/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539041,
        "cdate": 1696707539041,
        "tmdate": 1701465460605,
        "mdate": 1701465460605,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agreed that the novel task of attitude and theme-guided rebuttal generation based on argumentation is interesting and the proposed approach is sound. They highlighted some drawbacks in their reviews like the limited human evaluation, the lack of details about it, and the lack of deeper discussion about the potential biases on the dependence of the dataset on the model performance. The reviewers appreciated the author rebuttal which clarified most of the issues."
            }
        },
        "id": "ZBLPsXzrae",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5IFMe8TuSy",
        "replyto": "5IFMe8TuSy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1032/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503373,
        "cdate": 1696707503373,
        "tmdate": 1701465418379,
        "mdate": 1701465418379,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree that the main strength of the paper is the proposed conceptual framework for translating binaries, and the dataset created to evaluate it. That said, the paper does not compare against existing baselines for the function similarity comparison task. The paper over-claims some of the contributions and novelty, but based on rebuttal response, the authors intend to change it in the revision, including clearly contrasting the work and highlighting novelty over the Usenix 2023 paper."
            }
        },
        "id": "pShtmdYYmf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5EHI2FGf1D",
        "replyto": "5EHI2FGf1D",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission582/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491234,
        "cdate": 1696707491234,
        "tmdate": 1701465404128,
        "mdate": 1701465404128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers agree this work proposes an interesting and useful new method for retrieval/QA from long documents, promising significant token efficiency, without sacrificing overall accuracy. The authors’ commitments to updating the paper with new results and analysis, requested by reviewers, will improve the applicability and clarity of this method to practitioners."
            }
        },
        "id": "cchFS4rFVc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5DUhBxRqKR",
        "replyto": "5DUhBxRqKR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2335/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539728,
        "cdate": 1696707539728,
        "tmdate": 1701465461640,
        "mdate": 1701465461640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper studies inverse scaling behavior over the course of pre-training using a series of Pythia models. They find that for a set of tasks that demonstrate inverse scaling laws (performance decreases as model size increases), models exhibit a decreasing performance over of the course of training. Reviewers have requested more extensive experiments to support the claims made in the paper and authors provide more comprehensive results and discussions in the rebuttal. Thus I recommend acceptance of this paper."
            }
        },
        "id": "gb2A2aAAjP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5BWvVIa5Uz",
        "replyto": "5BWvVIa5Uz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3937/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707579771,
        "cdate": 1696707579771,
        "tmdate": 1701465514786,
        "mdate": 1701465514786,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Summary (adapted from reviewer Ri3q): This paper presents a corpus for 350+ languages, evaluates several off-the-shelf language identification methods on a test set they created, proposes a hierarchical language identification system added on top of an existing language identifier, and trains new machine translation models and evaluates them on an existing dataset.\n\nAll reviewers were positive regarding the value of the new corpus developed in this paper.\n\nReviewers raised some questions regarding the details of the experiments, the vast majority of which seem to be addressed well in the authors’ response. While the paper should be updated to reflect all the additional information provided during the discussion period, these are mostly clarifications and the addition of some useful but not necessarily essential baselines. The paper submitted for review was still sound, but these additions will enhance its soundness.\n\nThe submitted paper also did not clearly justify why the specific hierarchical model needed to be used, in comparison to other models. This was clarified in the authors’ response as well.\n\nOverall, the corpus and the models represent significant contributions for the field of language ID for less-resourced languages."
            }
        },
        "id": "VIVzvWSpCR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "5Az3d5TkMJ",
        "replyto": "5Az3d5TkMJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3470/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563372,
        "cdate": 1696707563372,
        "tmdate": 1701465498597,
        "mdate": 1701465498597,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary:\nThis paper addresses a challenging open-vocabulary Named Entity Recognition (NER) problem, which involves recognizing new entity types based solely on their surface names/descriptions and annotations of base types. The approach formulates OVNER as a semantic matching task and presents a two-stage method. Initially, it pretrains dual encoders on context-type pairs with distant annotations for alignment using contrastive learning. Subsequently, it fine-tunes the cross-encoder with supervision on base types.\n\nReason To Accept:\n1.This paper is well-structured and provides clear motivation. It introduces a novel approach to address a significant challenge in NER, specifically the discovery of new entity types. The acceptance of this work could serve as a valuable contribution to future research in the NLP community.\n2.The research is comprehensive and reasonably robust. The paper includes a thorough analysis of the proposed method and its limitations, along with suggestions for future research directions.\n3.The paper's strengths lie in its proposal of a novel and scalable two-stage method for open-vocabulary named entity recognition, a challenging and intriguing task within natural language processing.\n4.The approach is thoroughly validated through extensive experiments, with evaluations conducted across multiple datasets. Additionally, the paper promotes reproducibility by releasing pre-trained models and code.\n\nReason To Reject:\n1.The paper lacks a clear connection to prior related research in the field and is critical of previous research directions. It is essential to acknowledge certain limitations in the proposed approach, such as the necessity of providing descriptions.\n2.Some experiments need to be considered. 1) The paper only compares with discriminative methods, it is expected to compare with generative methods or LLMs which might perform better in some cases. 2) The paper primarily focuses on the English language, leaving uncertainty about the generalizability of the proposed method to other languages.\n3.Some details and analysis are lack. 1) A comprehensive analysis of the computational complexity and efficiency of the proposed method is lacking, potentially limiting its practical applicability in real-world scenarios. 2) The paper does not delve into an in-depth examination of how different hyperparameters affect the method's performance, hindering our understanding of its robustness."
            }
        },
        "id": "jIZZVHPmFe",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "59gI2XQPmH",
        "replyto": "59gI2XQPmH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission38/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477494,
        "cdate": 1696707477494,
        "tmdate": 1701465384947,
        "mdate": 1701465384947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a novel regularization approach targeting representation collapse during fine-tuning, complementing existing techniques that regularize model weights/gradients. The experiments are well-structured, extensive, and solid, covering OOD evaluation, label perturbation, probing representation collapse, representation diversity, etc. The paper is generally well-written and easy to follow. However, this paper only compares the proposed methods with models published before 2021. It lacks discussion of recent parameter-efficient fine-tuning strategies like fixed sparse masking and low-rank adaptation. Besides, the paper mentions that REPINA consistently outperforms baselines on most tasks. However, it could provide a more detailed comparison, including specific performance metrics and a deeper analysis of why REPINA performs better."
            }
        },
        "id": "rLyHldGp3M",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "58jpJdPgKi",
        "replyto": "58jpJdPgKi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4706/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707596630,
        "cdate": 1696707596630,
        "tmdate": 1701465539106,
        "mdate": 1701465539106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers appreciated that the paper tackles an interesting and important problem. The paper also extensively evaluates their proposed approach. However, there were several concerns about how the work compares to different baselines (some of which the authors rebut are not applicable, esp. when the backbone model is private). \n\nThis is definitely an interesting direction and I encourage the authors to revise and improve their draft—for instance, it could benefit from clearer and more detailed presentation (reviewers note that they find it hard to follow, and some of the contributions are not immediately clear). The paper could also discuss some related work that was missed in this iteration.|Reviewers acknowledged the paper's engagement with a significant and intriguing problem. They also commended the comprehensive evaluation of the proposed approach. Nevertheless, concerns arose regarding the comparisons with various baselines, with the authors noting that some are not applicable, particularly when the backbone model is private.\n\nThis direction is undeniably captivating, and I encourage the authors to refine and enhance their manuscript. Improvements could include a more lucid and detailed presentation to aid comprehension, addressing reviewers' difficulties in following the content and clarifying certain contributions. Additionally, the paper should consider incorporating relevant missed related work."
            }
        },
        "id": "nOZvOWBVTG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "57yfvVESPE",
        "replyto": "57yfvVESPE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission527/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489944,
        "cdate": 1696707489944,
        "tmdate": 1701465402267,
        "mdate": 1701465402267,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an active learning based framework FreeAL to distill knowledge from LLM to SLM where LLM serves as an active annotator and SLM as the student model. The proposed framework eliminates the need for human-labeled supervision. There is a unanimous consensus among reviewers that this is a sound approach for knowledge distillation without human annotations and the experiment result is very solid and positive. The rebuttal has effectively addressed reviewers' concerns by adding more analysis and baseline comparisons. We encourage the authors to take the feedback to revise the final version."
            }
        },
        "id": "RQDrHc3kNb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "56UYArtXyA",
        "replyto": "56UYArtXyA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3678/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567556,
        "cdate": 1696707567556,
        "tmdate": 1701465506376,
        "mdate": 1701465506376,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a two-step approach to automatically generating radiology reports, where the first stage focuses on extracting content from the image and the second stage verbalizing the content to a specific style. Although the reviewers agreed that the proposed approach is straightforward and convincing, a strong concern remained regarding the fact that the experiment results did not substantiate the core claim of disentangling the content and style of radiology reports, and the potential risk of missing important content while verbalizing it to different styles."
            }
        },
        "id": "ihH461ODX2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "54WhV6RTzi",
        "replyto": "54WhV6RTzi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1280/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509279,
        "cdate": 1696707509279,
        "tmdate": 1701465426063,
        "mdate": 1701465426063,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper works on the MMT problems. It focuses on enhancing the multimodal interactions to achieve a better results. The proposed method is to connect MMT and VQA. Overall, the paper achieves both technical novelty (might be limited due to the reviewer's feedback) and empirical improvements.\n\nA concern from the reviewing process is that the paper does not include the results from some 2022 papers. It's hard to fully stand with author's point that these results are just \"missing references\", since some of the paper achieves higher results. Higher previous results does not directly change the paper's evaluation, but it might change the claims made with this paper. \n\nIt would be great if the author can have an update over the paper claims as well (e.g., the claim of \"state-of-the-art\" in abstract; I want to mention once more that not claiming SotA would not hurt the paper performance, but a claim with uncertainty could). Thus it can resolve confusions for potential readers."
            }
        },
        "id": "uOGnH1ITBF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "51gbtl2VxL",
        "replyto": "51gbtl2VxL",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3676/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567495,
        "cdate": 1696707567495,
        "tmdate": 1701465506251,
        "mdate": 1701465506251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors consider a sequence to sequence approach for key information extraction, taking multi-model data for encoding and yielding the output structures directly, thereby avoiding OCR errors. This is a dedicated study for solving a practical problem. While there are existing sequence to sequence methods, the authors address several important issues in prompt design, generation tasking and OCR errors etc."
            }
        },
        "id": "n82dK4Tbaz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "51BB1xOWq1",
        "replyto": "51BB1xOWq1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission883/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498339,
        "cdate": 1696707498339,
        "tmdate": 1701465413522,
        "mdate": 1701465413522,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper targets question-answering on structured tables, TableQA for short. Previous work has proposed various approaches and used different logical forms for a specific table due to the diverse types of tables, such as relational tables, hierarchical tables, and databases. This paper instead presents a unified framework for TableQA. This unification occurs in 3 levels:\n 1. Various types of tables are represented as a multi-index pandas `DataFrame`.\n 2. Python serves as a generic intermediate representation of natural language.\n3. Powerful code generation models are leveraged to generate Python code in a few-shot manner.\n\nIn addition, this framework allows customs API functions to extend the Pandas functionality and incorporate external knowledge. Experimental results on few-shot HiTab, Spider, and WikiTQ demonstrate the framework's performance to some extent.\n\n**Positive:** The reviewers have liked the simplicity and novelty of the work.\n\n**Negatives:** More extensive comparison with the relevant baselines and elaborating the rationale behind the choice of the baselines (VuHH, 87ms): the authors have agreed. They promised to elaborate on the existing baselines and add more. Similarly, it has raised concerns about the limited choice of baselines, which seem arbitrary. Given the number of questions I see in the reviews, there is a need to improve the writing of the paper. I hope the authors revise their work for future revisions. \n\n\n**Given the \"soundness\" scores, I support accepting this paper. I am also leaning toward suggesting it for the \"main track.\"**"
            }
        },
        "id": "pLDLO76MHF",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "50rXrJNqHQ",
        "replyto": "50rXrJNqHQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2671/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547228,
        "cdate": 1696707547228,
        "tmdate": 1701465472946,
        "mdate": 1701465472946,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a supervised angular-based contrastive learning for multimodal sentiment analysis and shows how the proposed framework can attain more discriminate and generalized representation from each modality and overcome any bias towards a single modality by enhancing partial modality representation after fusion. The authors show the efficacy of the proposed framework with widely used sentiment datasets: CMU-Mosi and CMU-Mosei and compare with some state-of-the-art model performances.\n\nThe challenges addressed in the paper are well-researched, and the proposed framework brings together the ideas and techniques to apply them in MSA. \n\nThe reviews received for the paper are mixed. The majority of the reviewers point out the following concerns:\n– The paper has a very weak related section, missing recent and old approaches proposed to solve these challenges. \n– The presented idea is a simple extension and lacks sufficient novelty.\n– Some insights are not well-explained in the paper.\n\n\nThe authors addressed some of the concerns in the rebuttal and added a new baseline. The author committed to improve the related work section in future versions of the paper.\n\nOverall the framework proposed for SA, is a simple solution that shows comparable performance compared to  UniMSE (2022) and MIMM (2021).  The framework and the results in the paper can of be of interest to the researchers. We recommend that the authors take the suggestions from the reviews to further strengthen their work."
            }
        },
        "id": "AtrzbCtAW9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4wAKqlfV5t",
        "replyto": "4wAKqlfV5t",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2328/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539625,
        "cdate": 1696707539625,
        "tmdate": 1701465461423,
        "mdate": 1701465461423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work investigates compositional generalization in seq2seq models, and aims to disentangle the factors that enable a model to generalize compositionally.\n\nThe submission received 4 reviews. Overall, the reviewers agree that the soundness of the proposed work is sufficient to support its main arguments, though all point out additional points that should be addressed in a revision. These points include adding further motivation for focusing on the specific tasks and datasets and a justification for the complexity metric used, and improving the clarity of several sections. While the work is judged to be largely sound and a good contribution, it is less clear how exciting the resulting insights will be to the EMNLP audience."
            }
        },
        "id": "BlqNznlwvf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4uylA0mUkk",
        "replyto": "4uylA0mUkk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4042/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707581867,
        "cdate": 1696707581867,
        "tmdate": 1701465518113,
        "mdate": 1701465518113,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The work provides a dataset and framework for interactive facial image editing through dialogue.\n\n**Pros**: Reviewers agree the dataset will be a good resource to the community. The majority of reviewers also agree the dialog state tracking solution is also a new (if incremental) contribution that is well defended empirically.\n\n**Cons**: Some reviewers raise concerns that the work is incremental compared to previous research (e.g., talk-to-edit and the dialogue/image generation components)."
            }
        },
        "id": "DnpHL832KS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4toYWE7g6U",
        "replyto": "4toYWE7g6U",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission870/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498024,
        "cdate": 1696707498024,
        "tmdate": 1701465413130,
        "mdate": 1701465413130,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a sentence embedding technique that considers the operability and compositionality of sentences, computes semantic similarity, and offers easy interpretability. The research direction is intriguing, the writing is clear, and the experiments are extensive. We hope that the camera-ready version will address areas the reviewers found unclear or lacking, ensuring the manuscript is valuable for a wide range of readers."
            }
        },
        "id": "v0uL8mWPuO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4to6zjnEQV",
        "replyto": "4to6zjnEQV",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4653/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707595286,
        "cdate": 1696707595286,
        "tmdate": 1701465537737,
        "mdate": 1701465537737,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers found the paper to be sound, and appreciated the experimental results, such as the result that the vocabulary-trimmed multilingual model shows less bias than a monolingual model trained from scratch.\n\nHowever, the reviewers ultimately questioned the value of the contribution since 1) it may excessively harm the model in real-world settings, where data is not always cleanly in a single language, and 2) the memory footprint could also be reduced simply by avoiding naively loading the entire embedding table into memory."
            }
        },
        "id": "DzhcGnwrLf",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4sgXjFtnqg",
        "replyto": "4sgXjFtnqg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission158/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707480639,
        "cdate": 1696707480639,
        "tmdate": 1701465389105,
        "mdate": 1701465389105,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper investigates the impact of fine-tuning on outlier dimensions providing two insights. First, outlier dimensions in pretraining persist in fine-tuned models and second, a single outlier dimension can provide reasonable performance on a downstream task. The paper provides a nice novel insight that is interesting and well-presented.\n\nProposed points of improvement are (1) that the aspect that results may not generalise should be discussed: the rebuttal provides additional experiments and mentions similarities between models. (2) Analyses remain shallow and (3) address what conclusions can be drawn from the observation that a couple of outlier dimensions are sufficient. \n\nIt can not be expected of a short paper, where an interesting observation that leads to new research directions is sufficient, to address all of the above. Given sufficient space, it would indeed be important to discuss what can be expected in terms of generalization and what remains for future research (if it only applies to some LMs, it's interesting as well) and what it means that even one outlier dimension can be fine-tuned for a task: should that even be the case?"
            }
        },
        "id": "NGAULdHyBE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4nQN6Z6OY3",
        "replyto": "4nQN6Z6OY3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3242/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558921,
        "cdate": 1696707558921,
        "tmdate": 1701465491641,
        "mdate": 1701465491641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a method that splits multilingual multimodal understanding tasks into two stages:  English caption generation and cross-lingual natural language understanding.\n\nPros:  The proposed approach is intuitive and achieves SOTA results in the part of the experimental studies\n\nCons: The method is hard to generalize for tasks beyond XNLI due to this two stage setup. Also the method does not give better average results for both XVNLI and MaRVL\n\nAdditionally the draft needs to be updated with more experimental details like use of single-language at a time modeling, baselines and other details shared during the author discussion period."
            }
        },
        "id": "ztYK6mYTNw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4kuLaebvKx",
        "replyto": "4kuLaebvKx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission244/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482817,
        "cdate": 1696707482817,
        "tmdate": 1701465392640,
        "mdate": 1701465392640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces the concept of gated detoxifiers as a solution to improve the text quality of the existing guided decoder of LM for non-toxic text generation. The proposed solution is designed to be model-agnostic and aims to preserve the linguistic quality of the text while reducing toxicity. \n\nPros:\n1. Addresses a significant problem of improving text quality in non-toxic text generation.\n2. The proposed method is model-agnostic. \n3. Positive experimental results in retaining model performance and reducing computational overhead.\n\nCons:\n1. Limited novelty with similarities to existing methods. \n2. Lack of clarity. Two reviewers point out that the term \"detoxifier\" is confused. It doesn't reflect the main purpose of the paper. Reviewers also mention that the technical and implementation details were missed. \n3. Writing and presentation need refinement. Reviewers 1, 2 and 4 point out many language errors."
            }
        },
        "id": "PEeLEqbwTr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4k5BcBYKAS",
        "replyto": "4k5BcBYKAS",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2791/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549518,
        "cdate": 1696707549518,
        "tmdate": 1701465476626,
        "mdate": 1701465476626,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new hierarchical structure Hi-ArG to represent argumentations. It can retain more semantics within arguments at the intra-argument level and record relations between arguments at the inter-argument level. To exploit Hi-ArG, they propose several pre-training tasks to augment a text-graph model with the information from the argumentation graphs. Experiments on two argumentation tasks, Key Point Matching and Claim Extraction with Stance Classification, have shown that further pre-training and fine-tuning improve performance.\n\nIn general, the authors found the proposed Hi-ArG to be an interesting method, the claim that it retains more semantics within the arguments at the intra-/inter-argument level is well-supported and the intensive experiment shows the effectiveness of proposed methods and the effects of each proposed component.\n\nThe more significant reasons to reject seem to be related to clarification and requests for additional results.\n* Requests for clarification: the rebuttal provided a decent expansion on the issues brought up by reviewers.\n* Requests for additional results: the authors provided additional results for one request (results on LLMs on the two evaluation tasks), however other requests were deferred to future work."
            }
        },
        "id": "9EN5geYwEP",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4dJMzjIR2k",
        "replyto": "4dJMzjIR2k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2713/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547975,
        "cdate": 1696707547975,
        "tmdate": 1701465473979,
        "mdate": 1701465473979,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores a method for biomedical NER which introduces a synonym generalization framework to improve dictionary-based approaches for this problem. The reviews have minor concerns about the clarity for some details, but the rebuttal has addressed them. The authors are encouraged to update the paper according to the rebuttal and discussion with the reviewers."
            }
        },
        "id": "nLIhFzXukV",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4akD4Z2BBg",
        "replyto": "4akD4Z2BBg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission718/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494393,
        "cdate": 1696707494393,
        "tmdate": 1701465408244,
        "mdate": 1701465408244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agree that the paper is sound and exciting. Several reviewers mention the paper's writing is easy to understand and the paper is generally well-written. The paper promises to release its tools and data, which will help other researchers reproduce and build upon this work. Reviewers generally do not find issues with the paper. There was one concern about computational requirements, but the authors addressed the concerns during the author response period."
            }
        },
        "id": "TidLKmyiej",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4aBxFtqRNa",
        "replyto": "4aBxFtqRNa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission800/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496223,
        "cdate": 1696707496223,
        "tmdate": 1701465410838,
        "mdate": 1701465410838,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Considering ethics issues pointed by  Ethics Chairs, I would suggest rejection of this paper.|Ethical concerns seemed to have been resolved by authors and ethics committee (only acknowledgement was needed). Reviewers seem to agree that this work's proposed methods are novel and have merits. Some concerns were still flagged but overall they were positive."
            }
        },
        "id": "VdWly4XyKM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4WrqZlEK3K",
        "replyto": "4WrqZlEK3K",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2836/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550532,
        "cdate": 1696707550532,
        "tmdate": 1701465478338,
        "mdate": 1701465478338,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an interesting approach that recycles parts of hypotheses in N-best lists produced via beam search to ultimately generate a higher-quality response. The method is evaluated on paraphrase generation, summarization, and commonsense generation and yields improved performance compared to beam search and other baselines. All reviewers praise the idea behind the method, as it is intuitive and clever to combine token-level quality estimation with constrained decoding. Criticisms of the approach include the computational expense (the authors provide some timing comparisons in their response to 8gyk, but they include no details on experimental/hardware settings so it is hard to draw meaningful conclusions from that). There were other concerns about experimetnal conditions (e.g. beam size) that were mostly resolved through discussion. Overall, it seems like a solid paper, but I would hope the authors provide a thorough analysis in the next version of how much more expensive this approach is in terms of time compared to normal beam search with a variety of batch sizes."
            }
        },
        "id": "CRymY9GmDr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4PPT1An0kY",
        "replyto": "4PPT1An0kY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission239/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707482721,
        "cdate": 1696707482721,
        "tmdate": 1701465392469,
        "mdate": 1701465392469,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All reviewers felt generally positively about both the soundness and excitement of the paper, describing the proposed pre-training dataset as \"real world... valuable\", \"extensive\", and \"diverse\" with the evaluation of the new proposed model as \"comprehensive\", showing its effectiveness on several downstream datasets, and appreciated the analysis-based suggestions for future work on the dataset and task. Some smaller reviewer concerns about presentation/clarity of details, and ablations, were adequately addressed in the author response."
            }
        },
        "id": "J8ofbvL9yB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4MjZNeTCqZ",
        "replyto": "4MjZNeTCqZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4808/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707598859,
        "cdate": 1696707598859,
        "tmdate": 1701465542313,
        "mdate": 1701465542313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Overall, the reviewers find the proposed approach valuable for improving complex reasoning tasks but raise concerns about its novelty, robustness, and the need for broader comparisons in the experimental evaluation. The authors have provided a fair amount of discussion to the reviewers' concerns in the rebuttal phase."
            }
        },
        "id": "l7fzGwrJID",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4M4U3uC3Iy",
        "replyto": "4M4U3uC3Iy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission722/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494414,
        "cdate": 1696707494414,
        "tmdate": 1701465408375,
        "mdate": 1701465408375,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes to merge experts of mixture of experts models into one expert during inference, which improves inference efficiency and task performance\n\n**Pros**:\n- the proposed expert merging technique is simple drop-in replacement to MOE models to effectively improve inference efficiency and task performance.\n- The authors provide additional positive results on larger-scale models and on more generative tasks to demonstrate the effectiveness of the approach.\n\n**Cons**: I find that the work doesn't exhibit notable drawbacks, and the concerns and questions raised in the reviews have been adequately resolved."
            }
        },
        "id": "IwnPxe6tme",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4KRiWsfOwn",
        "replyto": "4KRiWsfOwn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1905/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529786,
        "cdate": 1696707529786,
        "tmdate": 1701465446690,
        "mdate": 1701465446690,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper investigates the application of non-autoregressive transformers (NATs) to document-level MT. It proposes an alignment method for ensuring that generated text respects sentence boundaries, and compares NATs to standard autoregressive transformers, finding that the former produce poorer-quality document-level translations.\n\nReviewers felt that this was a substantial and novel contribution that establishes useful benchmark results for document-level NATs. They appreciated the comprehensive evaluation with different recent NAT architectures, and the honest comparison that demonstrates underperformance. Negatives included the use of small datasets for only one language pair, lack of statistical significance, and insufficient exploration of known techniques for speeding up the autoregressive baselines.\n\nThis is a solid contribution that establishes a useful new baseline technique and corresponding benchmark result for document-level work with NATs. It will serve as a good point of reference for people hoping to extend this line of work. The main downside is the somewhat narrow scope of the evaluation, but this is unlikely to seriously affect the conclusions.\n\nI would like to highlight the use of BLEU rather than more accurate neural metrics such as COMET and BLEURT. Since this is typical of broad areas of MT research, it would be unfair to penalize this particular paper, but this is something that should change in the future, for high-resource settings."
            }
        },
        "id": "3Iv7chkfEO",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4JpybEffzH",
        "replyto": "4JpybEffzH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission423/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487283,
        "cdate": 1696707487283,
        "tmdate": 1701465398421,
        "mdate": 1701465398421,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents TAGNet, a new model for knowledge graph completion which addresses the limitations of a previous model that propagates “empty messages” and “redundant messages”. This is achieved by only aggregating paths in a fixed window for each source-target pair. The model is evaluated on two standard knowledge graph completion datasets (FB15k-237 and WN18RR).\n\nThe reviewers agree that the paper is sound. They also agree that the proposed model is carefully designed to deal with the presented limitations of previous models (such as NBFNet and A*Net). Furthermore, the model appears to be empirically stronger than previous methods. However, the reviewers raised concerns about the time complexity of the model and asked for a comparison of the running time of TAGNet with that of the other models. Those concerns were partially addressed in the response. Finally, the reviewers also raised concerns about the significance of the contribution and the lack of novelty with respect to NBFNet.\n\nOverall, it is my opinion that the work is interesting and technically solid. I also agree with various concerns that were raised in the reviews, but I think that these concerns are minor."
            }
        },
        "id": "hBVjgfPDXa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4Jnjap7NSx",
        "replyto": "4Jnjap7NSx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4512/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707591950,
        "cdate": 1696707591950,
        "tmdate": 1701465533456,
        "mdate": 1701465533456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes to use multilingual commonsense knowledge to augment dialogue generation. It creates a dataset spanning seven languages through translation and introduces a baseline model, MCK-T5, built upon the foundation of mT5. An evaluation of this model on the curated dataset demonstrates a notable enhancement in dialogue systems for low-resource languages when high-resource language knowledge is integrated.\n\nWhile the reviewers did express several concerns about the experiments, the majority of these concerns have been adequately addressed in the rebuttal.|This paper proposes to use multilingual commonsense knowledge to augment dialogue generation. It creates a dataset spanning seven languages through translation and introduces a baseline model, MCK-T5, built upon the foundation of mT5. An evaluation of this model on the curated dataset demonstrates a notable enhancement in dialogue systems for low-resource languages when high-resource language knowledge is integrated.\n\nWhile the reviewers did express several concerns about the experiments, the majority of these concerns have been adequately addressed in the rebuttal."
            }
        },
        "id": "aqBkDjD7w3",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4IubiozIFH",
        "replyto": "4IubiozIFH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1008/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502398,
        "cdate": 1696707502398,
        "tmdate": 1701465417676,
        "mdate": 1701465417676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a new dataset and pipeline model for the task of party-specific legal document summarization, focused on leases. The goal is to produce summaries for each party containing specific information regarding obligations, entitlements, and prohibitions.\n\nReviewers agree that this an interesting use case of tailored summaries. They also agree that the description provided for the pipeline apporach is adequate. The data annotation procress is interesting and, while some concerns were raised regarding the suitability of the annotators, these were clarified in the authors' rebuttal. Authors are encouraged to include this additional information in the paper.\n\nSome concerns were raised regarding the suitabilty of the dataset for training models due to its size. The authors' rebuttal clarified that, indeed, the size is a limitation for training end-to-end models, and that's why they proposed a different pipeline-based approach. This is also the reason why no fully-supervised baselines were included in the comparison. Finally, other concerns were raised regarding relevant  statements that could be missing, either due to not explicitly mentioning the party involved, or because \"importance\" is subjective. Interesting clarifications on these details were provided, as well as acknoledgments of limitations of the paper. Authors are encouraged to include them in the paper."
            }
        },
        "id": "cWrUEWfVTS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4GmujJSuq0",
        "replyto": "4GmujJSuq0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4094/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583031,
        "cdate": 1696707583031,
        "tmdate": 1701465520026,
        "mdate": 1701465520026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a method to optimize the PTQ of LLMs. Initially, reviewers had several concerns regarding the clarity, less novelty, and unclear results comparison. But the authors have addressed them, therefore, I recommend acceptance of this paper."
            }
        },
        "id": "CNkdUF4k7Q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4Ggw1DsgRQ",
        "replyto": "4Ggw1DsgRQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5344/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611295,
        "cdate": 1696707611295,
        "tmdate": 1701465556779,
        "mdate": 1701465556779,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a novel approach, Mixture of Soft Prompts (MSP), for controllable data generation using Large Language Models (LLMs) in the context of multi-attribute natural language understanding tasks. Instead of relying on LLMs for direct prediction, MSP leverages LLMs as data augmentation tools, enhancing the training of smaller, domain-specific models. The paper demonstrates the effectiveness of MSP through comprehensive experiments and shows it outperforms baselines on multi-attribute NLU tasks.\n\nThe reviewers raised concerns about (1) the model size used in the experiments and whether truly large LLMs might not require MSP; (2)\nthe absence of evidence showcasing limitations in large LLMs for multi-attribute predictions; (3) the lack of a comparison with prompt engineering strategies like simple chain-of-thought prompting or prompt-chaining.\n\nThe authors have partially addressed the first concern by gaining access to GPT-4 and reporting that even GPT-4 does not outperform MSP. However, they have not fully addressed the second and third concerns, as they have not provided evidence or comparisons with prompt engineering strategies. Thus, while the paper presents a novel approach, some concerns raised by the reviewers remain unaddressed."
            }
        },
        "id": "sjCvXwYx8c",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4FDx4KMZnu",
        "replyto": "4FDx4KMZnu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3831/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707576950,
        "cdate": 1696707576950,
        "tmdate": 1701465511183,
        "mdate": 1701465511183,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "While the reviews have a consensus on positive soundness, there is a discrepancy in the excitement evaluation.\n\nRegarding soundness, the paper conducts extensive experiments on 8 datasets from 2 languages, supporting the effectiveness of the proposed method.  The paper would be more insightful if it provided an error analysis.  All the reviewers see the paper is well-written.\n\nAs for excitement, the proposed span-based NER model is novel.  The result of improving recall has a practical impact.  I would lean toward the slightly positive side in excitement, mainly because of how the proposed method addresses two problems with existing span-based NER models."
            }
        },
        "id": "DHfeTpjn5Y",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4EXbwN9Ezw",
        "replyto": "4EXbwN9Ezw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission640/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492712,
        "cdate": 1696707492712,
        "tmdate": 1701465406093,
        "mdate": 1701465406093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a method for unsupervised style transfer, using prefix-tuning of GPT-2 with 2 types of prefixes: style, content, and task. The proposed approach was properly evaluated using both automatic metrics and human evaluation, as well as ablation studies, but it was not evaluated against recent and similar models. Another concern is that the work seems incremental and the results are only slightly better than the baselines, and the illustrated examples are probably cherry-picked to favor the proposed model. In addition, it was only evaluated on sentiment task (for transferring sentiment from positive no negative and vice versa), so it's not clear how well it would work for other types of style transfer. Overall, while the paper is well written and the approach is somewhat sound, the paper needs extensive evaluations to validate the method for different style transfer tasks compared to other recently proposed approaches."
            }
        },
        "id": "DRJmcHLlwk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4AiERjB5JD",
        "replyto": "4AiERjB5JD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission529/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707489967,
        "cdate": 1696707489967,
        "tmdate": 1701465402312,
        "mdate": 1701465402312,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel framework, CP-BCS (Control Flow Graph and Pseudo Code Guided Binary Code Summarization), for the task of assembly code summarization. The authors build a dataset containing assembly code-summary pairs across three computer systems with different optimization levels. CP-BCS uses a bidirectional instruction-level control flow graph and pseudo code to learn the execution behavior and logic semantics of binary functions. The method is evaluated on three different binary optimization levels for three different computer architectures, showing considerable improvement in the efficiency of reverse engineering.\n\nMain Contributions:\n\nThe paper introduces the task of binary code summarization.\nThe authors construct a parallel dataset with assembly code-summary pairs across different system architectures.\nA new framework, CP-BCS, is proposed, which uses a multi-source encoder strategy to leverage control flow graphs and pseudo code for binary code summarization.\nThe authors provide comprehensive dataset statistics across a broad range of datasets and computer architectures.\n\nReasons for Acceptance:\n\nThe introduction of the binary code summarization task fills a gap in current research.\nThe construction and release of a comprehensive assembly code summarization dataset will facilitate further development in this field.\nThe CP-BCS method shows superior performance to baseline models, improving the efficiency of reverse engineering.\nThe methodology is well-described and partitioned into various components, including the assembly instruction encoder, BI-CFG Encoder, Pseudo Code Encoder, and Summary Decoder.\nThe paper provides a thorough evaluation of CP-BCS on different binary optimization levels and computer architectures."
            }
        },
        "id": "yGnpDuLutv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4AcHxGE6M4",
        "replyto": "4AcHxGE6M4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3634/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566647,
        "cdate": 1696707566647,
        "tmdate": 1701465504878,
        "mdate": 1701465504878,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agreed that this is a solid paper with well-structured and detailed experiments, as well as clear articulation of the findings. They also found that the results around different LLMs being differenly successful in dealing with negation to be interesting. The work is timely as it helps us better understand today's popular LMs with impressive (but error-prone) chain-of-thought abilities.\n\nThe reviewers pointed out some ways to improve presentation (e.g., splitting Tables 2 and 3), which the authors acknowledged and agree to."
            }
        },
        "id": "QzXLmSr0qY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "49HfhYU9S6",
        "replyto": "49HfhYU9S6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1489/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515580,
        "cdate": 1696707515580,
        "tmdate": 1701465432313,
        "mdate": 1701465432313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers were enthusiastic about the paper and agreed that the methodology was intuitive and sufficiently novel, and the presentation of the results were clear. The rebuttal phase helps clarify some of the confusion and answer outstanding questions. As one reviewer pointed out, the paper may need additional work to situate it against existing work and choosing appropriate comparators. The authors response indicates that this merits some discussion in the revised version. There is strong interest in this line of work and the excitement on the proposed approaches is high."
            }
        },
        "id": "CATjBcVtD1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "46WcPRhRwG",
        "replyto": "46WcPRhRwG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission714/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707494295,
        "cdate": 1696707494295,
        "tmdate": 1701465408220,
        "mdate": 1701465408220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates into \"annotator sensitivity\", i.e., how the wording of instructions affect the annotations collected.\n\nAll reviewers agreed this is an interesting and novel topic to look into, but also all raised some concerns on study design and evaluation. In specific:\n1. The use of models are not up-to-date\n2. The evaluation metric used seem a bit shallow (focusing on model accuracy but not other asspects like annotator-agreements)\n3. The investigation focuses on only one type of intervention which makes the claim of \"annotator sensitivity\" a bit too broad.\n4. Multiple artifacts may affect the result, noise of the annotations. \n\nPersonally, I find the topic refreshing, and while the study is not perfect, it is sound enough and will spark interest and discussion among NLP researchers. As such, I recommend accepting to Findings track, but would recommend the author to significantly revise the paper as they suggest in the rebuttal.\n\nI would also like to point out a relevant work on instruction bias:\nParmar, Mihir, et al. \"Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions.\" EACL 2023"
            }
        },
        "id": "UNeEypc6Mr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "45uZxlMLol",
        "replyto": "45uZxlMLol",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3480/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563615,
        "cdate": 1696707563615,
        "tmdate": 1701465498902,
        "mdate": 1701465498902,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This journal paper presents a novel framework called Chain-of-Thought Tuning (CoTT) for solving natural language understanding (NLU) tasks. \nCoTT is designed to decompose NLU tasks into two sequential sub-tasks and incorporates prompt tuning to enable step-by-step reasoning in masked language models (MLMs). It also includes ablation studies and highlights the potential for increased explainability of the model by introducing an intermediate step. The integration of CoTT to NLU tasks is well-motivated and provides fresh perspectives for the field. \nHowever, the significant components of the proposed frameworks are minor adaptations from previous methods. The evaluation only covers two types of tasks and uncovers how effective the chain-of-thought style technique is for other NLU tasks. \nMoreover, the reported performance gain of the proposed framework is marginal compared to simple Prompt Tuning approaches. \nTherefore, a more comprehensive evaluation of diverse NLU tasks would strengthen the proposed framework's generalizability.\n\nOverall, the paper is well-written, easy to follow, and demonstrates the transferability of CoT concepts from language models to masked language models."
            }
        },
        "id": "WEKxlunUiD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "43SOcneD8W",
        "replyto": "43SOcneD8W",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2612/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545831,
        "cdate": 1696707545831,
        "tmdate": 1701465470751,
        "mdate": 1701465470751,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a user-centric approach to assist qualitative researchers by providing automated agent generated code suggestions.\n\nReviewers agreed that:\n1. The paper presented a comprehensive study\n2. The conceptual task of qualitative coding is well-defined\n3. The dataset is useful.\n\nOriginally reviewers WMQc and ULRd had some concerns about the connection between this and prior work, and the validity of the user study, which the authors sufficiently addressed in rebuttal.\n\nI recommend accepting the paper into the Main track, and encourage the authors to add a figure illustrating the workflow, extending the related work session, and providing clarification to the human eval."
            }
        },
        "id": "at2dgOZRpD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "42LIoV0C1h",
        "replyto": "42LIoV0C1h",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3631/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707566587,
        "cdate": 1696707566587,
        "tmdate": 1701465504725,
        "mdate": 1701465504725,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper brings merits to Many-to-Many Multimodal Summarization, which is interesting. The model part is also reasonable. The major weakness is marginal results make it hard to justify how much vision can contribute to this task."
            }
        },
        "id": "kTI9SzxYdw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "42Cc5s71zl",
        "replyto": "42Cc5s71zl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2359/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707540379,
        "cdate": 1696707540379,
        "tmdate": 1701465462635,
        "mdate": 1701465462635,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Problem: This paper targets the task of question answering on complex tables. For example, those containing multi-level headers and many merged cells. \n\nProposed solution: They convert single-turn questions to multi-turn prompts that convey information about the schema of the table and the gist of its content. This boosts the prompting performance and resolves the max input token limitation of GPT-3.5. Experimental results on HiTab and AIT-QA demonstrated the effectiveness of the proposed approach. \n\n\n- From the title, it is unclear what aspect of LLMs led to these gains (scale of models, scale of pre-training data, RLHF?) The authors have proposed to revise their title. \n- More insight is needed for Section 4.3. The authors promise to add more analysis to the document. \n- Details of other prompting techniques need to be included. The authors promise to add the missing details in the text.\n\n\nOverall, the reviewers have no concerns regarding the \"soundness\" of the paper. So, I think this paper got accepted to either Finding or the main track. However, given the limited \"excitement,\" I am **not** willing to defend its acceptance to the main track.  \n\n----------------------------------------------\n\nAside: Leaving these formatting suggestions for the authors for their revision: (1) the fonts across most figures/tables are too small. (2) \"(Pan et al., 2023) pointed out\" should be \"Pan et al., (2023) pointed out\" since the citation is used in the subject of your sentence."
            }
        },
        "id": "Y6KzulG9ES",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "4272bEn4Q0",
        "replyto": "4272bEn4Q0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3020/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554418,
        "cdate": 1696707554418,
        "tmdate": 1701465484579,
        "mdate": 1701465484579,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper argues that causal attention within in-context demos of limits the ICL ability of language models, and the authors propose a simple technique to prepend the demos to the original demos (i.e. demonstration replay) so that each demo example can observe the information of later ones. A sliding window approach is applied to avoid duplicated information in the attention. Experiments on several traditional classification datasets demonstrate the effectiveness of the proposed approach. The pros and cons summarized by the reviewers and the AC are listed below.\n\n### Pros\n1. \"The limitation of causal language modeling in the ICL proposed in the paper is thought-provoking.\"\n2. \"The proposed techniques are simple, easy to implement, and intuitive.\"\n3. \"The effectiveness of the proposed methodology is demonstrated through various experiments.\"\n\n### Cons\n> 1. \"The sequence length will be expanded by using repeated demonstration.\"  \n\nThis point has been raised by multiple reviewers. While the authors have responded in the rebuttal that the KV cache could be pre-computed, storing these precomputed caches still incurs extra cost (maybe relatively minor). I agree with the authors in the rebuttal that the time cost is not increasing with a sliding window approach given that the KV cache is precomputed.  \n\n> 2. \"On the other hand, 4-shots RdSca ICL cannot outperform regular 7-shots, according to Figure 2, which suggests that using regular ICL might still be the best choice with the same amount of computation budget.\"\n\nThis point is concerning. I do agree with the authors that 4-shot RdSca ICL should be more efficient than regular 7-shots given precomputation. However, a detailed analysis of memory and time efficiency of RdSca ICL is necessary since multiple reviewers are worried about its efficiency -- the 4-shot RdSca ICL is not equivalent on efficiency to regular 4-shots because the first demo of RdSca still attends the other 3 demos while the regular one does not.\n\n> 3. \"The selected benchmark seems a bit outdated. Just like people use MMLU/BigBench to evaluate LLM instead of SST/CB, Such LLM-based ICL approaches need to be validated on MMLU/BigBench to be more convincing.\"\n\nIn summary, all the reviewers agree that this paper is sound, but they did not find this paper exciting enough with some concerns as above and results on slightly outdated benchmarks."
            }
        },
        "id": "OgX2NVQhHc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "41vXNjZbIn",
        "replyto": "41vXNjZbIn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1436/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707514339,
        "cdate": 1696707514339,
        "tmdate": 1701465431078,
        "mdate": 1701465431078,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a novel prompt design strategy for text-to-SQL with in-context learning. The proposed approach first retrieves demonstrations for a test instance by generating preliminary SQL query and both diversity and similarity between demonstrations and the test instance are considered. This work introduces approaches to include schema-related knowledge in prompts, and ensemble approaches to aggregate results to tackle the sensitivity issue.\n\nCombination of all aforementioned information, outperforms the SOTA of in-context learning methods and fine-tuned methods. The findings of this work could benefit the community for studying text-to-SQL."
            }
        },
        "id": "bsxDgH96nn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "40NCUv4I2R",
        "replyto": "40NCUv4I2R",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1965/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530967,
        "cdate": 1696707530967,
        "tmdate": 1701465448574,
        "mdate": 1701465448574,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main contribution of this paper is a cross-lingual question answering/retrieval dataset for 10 African languages (+ English and French), generated through automatic and human translation. The dataset is claimed to be the first of its kind for these languages.\n\nThe reviewers agree that the corpus building corpus and the benchmarking and analyses are generally clearly presented. The resulting resource is expected to be very valuable. On the other hand, the reviewers found the structure of the paper a bit unintuitive and were missing some details. Also, some decisions should be better justified. However, these issues mostly concern the presentation of the paper and can be addressed in the camera-ready version."
            }
        },
        "id": "VoV6sg6JVr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3ymHqvobHJ",
        "replyto": "3ymHqvobHJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1852/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528776,
        "cdate": 1696707528776,
        "tmdate": 1701465444617,
        "mdate": 1701465444617,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper shows that cross-attention similarity in a multilingual NMT system is correlated with translation quality, indicating that higher quality is linked to knowledge transfer. It also demonstrates gains on lower-resourced languages from an auxiliary loss function that encourages representational similarity in multi-way parallel data.\n\nReviewers were mostly convinced by the analysis, finding the proposed cross-attention similarity metric novel, and the demonstrated link between knowledge transfer and quality credible. They also highlighted the positive results from use of the auxiliary representational loss function. They had reservations about potential lack of generality due to reliance on multi-way parallel data, the fact that the similarity metric depends on parallel data, and experiments on into-English directions only. They also raised questions about a number of smaller technical issues.\n\nThis seems to be a nice addition to work on multilingual NMT. The demonstration that knowledge transfer is a key determinant of quality, while not surprising, is a useful and solid contribution. Similarly, the loss function, while roughly in line with previous proposals, is novel in exploiting multi-way parallel data, which recent work has shown to be highly beneficial. The authors did an excellent job of addressing reviewers’ questions, and I don’t have any major concerns about the paper’s technical merits."
            }
        },
        "id": "qLyiB9iErU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3u3kXSeVvR",
        "replyto": "3u3kXSeVvR",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1632/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521229,
        "cdate": 1696707521229,
        "tmdate": 1701465436770,
        "mdate": 1701465436770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Improving collaboration using clarification questions and thus reducing uncertainty in contained information. There are some concerns that reviewers raised concerns that have not been addressed, and some reviews have questions about its novelty."
            }
        },
        "id": "vKPiU85j2z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3qG4r6FGWD",
        "replyto": "3qG4r6FGWD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3720/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707568630,
        "cdate": 1696707568630,
        "tmdate": 1701465507629,
        "mdate": 1701465507629,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a benchmark that is a valuable contribution in that it could/should become a standard for evaluating navigation agents on the more specific characteristics introduced here (responding to help, keeping track of history, cooperating with the helper). The insights and novel attention-mask schemes are also good contributions to building better multi-modal navigation agents. The authors provided detailed answers to alleviate most reviewer concerns, added additional baselines to compare to, and added more analysis of the datasets used to address questions and concerns of the reviewers. This paper, and the evaluation benchmark that comes with it, is a good contribution to the language/RL/navigation field."
            }
        },
        "id": "TJP251giAv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3qF5MqUl3Y",
        "replyto": "3qF5MqUl3Y",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission340/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485186,
        "cdate": 1696707485186,
        "tmdate": 1701465395622,
        "mdate": 1701465395622,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a novel method that allows adapting ASR systems (transducers and enc-dec) to dictionaries containing rare words and phrases without retraining the original ASR model. The method uses kNN retrieval and TTS to obtain speech for rare words. The idea is well-presented and achieves consistent gains on three datasets. The authors will also release their entity-rich speech dataset to facilitate further research into dictionary-based ASR adaptation.\n\nThe authors addressed all reviewers' comments, especially regarding computation cost and comparison with additional methods. All reviewers are excited about this paper and assigned high soundness scores."
            }
        },
        "id": "ENXAOieGSI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3pvdo2yHXq",
        "replyto": "3pvdo2yHXq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1950/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530647,
        "cdate": 1696707530647,
        "tmdate": 1701465448132,
        "mdate": 1701465448132,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper addresses a new problem of high practical relevance in education, namely how the costs of calls to LLMs can be reduced by using previous results of LLM calls in a knn fashion. The paper proposes a simple yet effective solution to a timely problem which seems appropriate for a short paper, although reviewers criticize some technical details (how to update the cache exactly, how to handle new labels) and a somewhat limited evaluation on a single dataset only."
            }
        },
        "id": "RxPNlGwmtk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3l9zUuFo9m",
        "replyto": "3l9zUuFo9m",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3192/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557934,
        "cdate": 1696707557934,
        "tmdate": 1701465489702,
        "mdate": 1701465489702,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "In this work, the authors propose PARROT, a instruction-guided technique to improve the translation abilities of open-sourced LLMs. Translation data is rewritten in instruction-style in three different ways: 1) Using standard translation instructions 2) Using contrastive instructions and 3) Using error-guided instructions. The authors find translation instructions to be very helpful and error-guided instructions to further improve performance. However, contrastive instructions do not yield performance improvements.They also find LoRA-based finetuning to be more effective for high resource language pairs.\n\nR2 and R3 have both rated this work highly on soundness (4/4). R1 listed a number of limitations including the need for more baseline comparisons (esp with larger models), the difference between error-guided and chain-of-thought based methods and the main motivation behind the work. While R1's comments came quite late during the rebuttal window, the authors have offered a fairly thorough response with additional experiments."
            }
        },
        "id": "lk0BM2yIbA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3k5GFJEGem",
        "replyto": "3k5GFJEGem",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2327/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539553,
        "cdate": 1696707539553,
        "tmdate": 1701465461397,
        "mdate": 1701465461397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper titled \"TAPTAP: Generating High-Quality Training Data for Tabular Prediction\" presents a method called TAPTAP for improving the quality of training data for tabular prediction tasks. The proposed method involves pretraining a language model on public tabular datasets using static prompts, fine-tuning the language model, and data sampling. The resulting synthetic data helps address four common challenges in tabular prediction: privacy protection, low resource regime, missing value imputation, and imbalanced classification. The paper presents experimental results on 12 publicly available datasets that demonstrate the effectiveness of the proposed method in improving the performance of these tasks. Overall, the reviewers think that the work has done well in this task, and also has some concerns about the writing, claim. For example, the position of this paper has been questioned by one reviewer, some contribution might be overclaimed, and more experimental explanations and discussions are needed, too."
            }
        },
        "id": "6MSECKDsr6",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3gdG9upo7e",
        "replyto": "3gdG9upo7e",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission321/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484710,
        "cdate": 1696707484710,
        "tmdate": 1701465394777,
        "mdate": 1701465394777,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper utilized dense retrieval to address discriminative NLU tasks. The proposed task is well motivated and may yield some more real-world impact. The paper is well written in general. Reviewers praised the comprehensive experiments with consistent improvements of the proposed model over baselines. On the other hand, two reviewers raised concerns about limited technical novelty of the proposed method given the existing work. The readability of the paper can also be improved. Overall, I think this paper proposes a solid approach (while not super innovative) to a well-motivated task with convincing experiments. I hope the reviewers' comments on the weaknesses will help improve the quality of the next version of the paper."
            }
        },
        "id": "4d80Ungv3V",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3e8rcsIO7H",
        "replyto": "3e8rcsIO7H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5173/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608567,
        "cdate": 1696707608567,
        "tmdate": 1701465551938,
        "mdate": 1701465551938,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates a LLM-based approach for predicting drug-drug  interactions (DDI). The key idea of the proposed model is to mutually learn adequate prompts that leverage descriptive information from external resources, in the one hand, and DDI prediction score using a reinforcement learning policy in the other hand. \n\nThe reviewers agree that the overall contribution is interesting and innovative, presenting a new research angle on a well-known prediction task for health, namely DDI.\nEven if the evaluation part looks reasonable, there have been some concerns about the limited input length, as well as the lack of appropriate ablation study scenarios to fully support the soundness of the results (e.g., DDI predictor / and w/ textual features such as name, truncated vs. full description). \n\nOverall, the discussion phase among the reviewers expressed a positive feeling about the paper and its potential and even its generalizability to other tasks."
            }
        },
        "id": "OGyKGytsfj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3dNeNpmyiO",
        "replyto": "3dNeNpmyiO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2702/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707547869,
        "cdate": 1696707547869,
        "tmdate": 1701465473875,
        "mdate": 1701465473875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates a unified graph pre-training method for online communities. The reviewers appreciated the novelty, method, experiments, and analysis. The authors are recommended to address the reviewers' concerns in the camera ready, if the paper is accepted."
            }
        },
        "id": "faloelUODA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3aF1Rv3dHG",
        "replyto": "3aF1Rv3dHG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5035/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707605641,
        "cdate": 1696707605641,
        "tmdate": 1701465548093,
        "mdate": 1701465548093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The authors present a fairly simple new baseline system for the important problem of KnowledgeBased-VisualQA that achieves near SOTA performance on a popular benchmark. \n\nReasons to accept:\n- The proposed method is simple and intuitive.\n- The conducted experiments are thorough, effectively substantiating the efficacy of the devised mechanisms.\n- The analysis and the ablation are robust."
            }
        },
        "id": "vstUL4QyYy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3XDDWCu8CF",
        "replyto": "3XDDWCu8CF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3895/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578616,
        "cdate": 1696707578616,
        "tmdate": 1701465513205,
        "mdate": 1701465513205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes a new task (and a dataset) in the field of natural language understanding called \"Poem Summarization\". \n\nThe reviewers agree on the soundness of the proposal being good-strong, while the excitement causes more variability (mediocre-strong). Taking into account the novelty of the proposal, and that the reasons to reject have nothing to do with the excitement of the proposal, I take it being rather high. The reviewers raised reasonable and constructive comments and concerns that the authors acknowledge and will include in the final version, resulting in an improved work. The main ones being: \n \n- Further dataset metrics\n- Further analyses on figurative patterns that LLMs do not capture\n- Details on curation processes\n\nWorking on a single language, should, in my opinion, do not preclude the publication of the work. Inasmuch as the code and the dataset will be made publicly available along with the train, validation and test split once accepted, I also believe reproducibility should not be an issue."
            }
        },
        "id": "cXpd24LjxA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3Uu4rZ6hLI",
        "replyto": "3Uu4rZ6hLI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4061/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582256,
        "cdate": 1696707582256,
        "tmdate": 1701465518827,
        "mdate": 1701465518827,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The manuscript studies how to balance training of retrieval-based Language Models between utility (represented by perplexity) and privacy tradeoff. The comprehensive study reveals that kNN-LMs are more susceptible to leaking private information than parametric model as more private data and fewer public data are incorporated into the datastore. The authors propose how to quantitatively mitigate the leakage issue, either by sanitizing the training data or by mixing public and private data. All reviewers agree that this paper provides valuable framework that can systematically explore privacy leakage for a class of retrieval-augmented LLMs and mitigating solutions. To further improve the study, it is encouraged to navigate other types of retrieval-augmented language models and other types of utility, especially multi-task solving capabilities."
            }
        },
        "id": "WnFRtO3N8O",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3RTpKMVg0P",
        "replyto": "3RTpKMVg0P",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2747/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707548560,
        "cdate": 1696707548560,
        "tmdate": 1701465474959,
        "mdate": 1701465474959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors present a pixel sequence-to-sequence model that allows them to translate text in images with an end-to-end model, instead of a cascade between OCR and translation. Experiments show improvements over the cascade on both in- and out-of-domain test sets. The reviewers initially had many concerns with the presentation and experiments, but it seems like most of these have been resolved during the response. The most pressing remaining concern is the realism of the training and test sets, which are formed from black text on a white background, calling into question the generality of the in-image translations that are tested in this paper. The authors correctly point out that, as this is a new task, there are no better datasets, and that this does address a common scenario of images consisting primarily of text. But all three reviewers still seem somewhat disappointed. Given the framing of the paper, it would be much stronger with more realistic data."
            }
        },
        "id": "WTjpx5Text",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3RS2T9EPjI",
        "replyto": "3RS2T9EPjI",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3530/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564652,
        "cdate": 1696707564652,
        "tmdate": 1701465500416,
        "mdate": 1701465500416,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers were unanimous in their thoughts about this paper as being exciting and has potential to be a strong addition to the program."
            }
        },
        "id": "Dt8gO01FhG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3QzTzulZwY",
        "replyto": "3QzTzulZwY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2782/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549315,
        "cdate": 1696707549315,
        "tmdate": 1701465476295,
        "mdate": 1701465476295,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "As the invlved discussion with reviewrs shows, this is interesting and thought provoking work, however, there is consensus that the evalution as presented does not fully alleviate all the concerns of the reviewers.  The authors have been generous with the reviewers and put in the time to dilligently address the concerns raised, including imporving and expanding the human evaluation and the set of models tested, listing of data sets and more meaningful evaluation of the significance of their results. \n\nI'd like to thank both the authors and reviwers for their engadement to improve this work."
            }
        },
        "id": "gxJnlYXa6D",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3QibSyz6Qt",
        "replyto": "3QibSyz6Qt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission589/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491414,
        "cdate": 1696707491414,
        "tmdate": 1701465404382,
        "mdate": 1701465404382,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This submission studies a meaningful and interesting problem about whether LLMs like ChatGPT has the ability to perform passage ranking. The authors have put forth a two-fold proposition in their work. Firstly, they propose permutation generation as a technique to instruct LLMs to directly generate permutations of a set of passages. Secondly, they present a novel permutation distillation approach which aims to replicate the ranking capabilities of LLMs in a smaller, specialized model. This distillation technique showcases superior efficacy and efficiency in its performance. All concerns from reviewers have been resolved by the discussion during the rebuttal."
            }
        },
        "id": "2ucsWBup39",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3Q6LON8y2I",
        "replyto": "3Q6LON8y2I",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3543/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564946,
        "cdate": 1696707564946,
        "tmdate": 1701465500944,
        "mdate": 1701465500944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes an approach to predict backchannels via multi-task training and viewing the problem similarly to dialogue act prediction. The reviewers generally like the novelty of the approach and agree that there are improvements over SOTA, but also comment that those improvements are marginal. The authors reply with additional experiments demonstrating the statistical significance of their results. For these reasons I believe this would be a good candidate for Findings."
            }
        },
        "id": "5GDFTTDZ7r",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3OvLxe9n9S",
        "replyto": "3OvLxe9n9S",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3403/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561929,
        "cdate": 1696707561929,
        "tmdate": 1701465496179,
        "mdate": 1701465496179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a large-scale realistic dataset, DIsh NamE Recognition (DINER), comprising  3k dishes and 228k recipes. This paper also presents the associated task (predicting the name of the dish given the recipe instructions) to evaluate compositional generalization. \nExperiments are well conducted and evaluated various models on this task (fine tuning T5, in-context learning LLM).\nThe article could benefit from the addition of corpus statistics. It would also be good to clarify the fact that it's in Chinese (which doesn't detract from the interest of the corpus) and that the conclusions may not apply."
            }
        },
        "id": "CWgPA5VSXN",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3Nq9KRcvx5",
        "replyto": "3Nq9KRcvx5",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3200/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558031,
        "cdate": 1696707558031,
        "tmdate": 1701465489951,
        "mdate": 1701465489951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a dataset on challenging the open-knowledge of VL models. The purpose of this dataset is clear to me, and I like the OKVQA comparisons. As a dataset paper, the paper's method part also nicely elucidates the status of existing approaches, which compares the zero-shot, fine-tuning (i.e., like using model's internal KB), and explicit KB. The overall discussion went positive on the decision of this paper."
            }
        },
        "id": "WccUQSsMVi",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3MEV3aIDDq",
        "replyto": "3MEV3aIDDq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2309/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539172,
        "cdate": 1696707539172,
        "tmdate": 1701465460782,
        "mdate": 1701465460782,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a human-annotated Event Dependency Relation dataset.\n\nThe three reviewers agree on the strong soundness of the work (4) and moderate to strong excitement (4-3-4). The reviewers have provided several suggestions for improvement and questions that should be incorporated in the final version of the manuscript if accepted. Among these, I find the following particularly relevant: \n- Inter-annotator Agreement to better demonstrate the quality of EDeR - and any other quality statistics (such as as the one described in the rebuttal).\n- Typos, grammar, style, and presentation improvements\n\nAlthough I agree that a statement such as \" the novelty of the paper is not enough\" is unsupported, I believe that including in the paper the novelty highlights summarized in the rebuttal could improve the final version."
            }
        },
        "id": "J4Oz0Yt9QG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3LdaPmAnji",
        "replyto": "3LdaPmAnji",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1716/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707524977,
        "cdate": 1696707524977,
        "tmdate": 1701465439255,
        "mdate": 1701465439255,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper’s major contribution is a novel math word problem dataset in English and Korean characterized by diversity at various levels: in problem types, in lexical patterns, in language narratives, in solution forms, which can help better evaluate the actual linguistic and reasoning performance of LLMs. The authors conducted experiments with various pre-trained language models and found that the math word problem-solving task remains challenging. Reviewers converged on soundness and found the work novel and reproducible. Overall, the paper does not show major weaknesses and may improve results and findings in this area. \n\n**Pros.** \n\n- The paper is well written, and argumentation is sound and clear; \n\n- A novel gold dataset of math word problem of higher complexity and diversity on various aspects; \n\n- The dataset addresses gaps in previous works and has the potential to enhance future research in the field; \n\n- The methodology seems transferable to other languages. \n\n**Cons:**\n\nNo a serious weaknesses to highlight. However, the rationale of the choices made for the comparison with other data sets could be more clearly explained/ motivated."
            }
        },
        "id": "zWvEwulCyL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3LIUMrCKrv",
        "replyto": "3LIUMrCKrv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4935/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707601849,
        "cdate": 1696707601849,
        "tmdate": 1701465545100,
        "mdate": 1701465545100,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a study that is able to perform end-to-end multilingual entity linking. The reviewers are generally positive on this work, especially given it's a first end-to-end approach to such a problem. One concern from the reviewers is that the evaluation can be further strengthened (e.g., with more evaluation metrics). Overall this is a good short paper that contains interesting research progress that is worth sharing with the community."
            }
        },
        "id": "ERisiLvbC1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3JP1Jsng4G",
        "replyto": "3JP1Jsng4G",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1031/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707503253,
        "cdate": 1696707503253,
        "tmdate": 1701465418326,
        "mdate": 1701465418326,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors proposed a new methodology for continual learning, which introduces an importance-based gradient soft-masking on top of SupSup to enhance transfer capacity. The paper itself in general is well-structured and easy to understand. In terms of weaknesses, there was disagreement among the reviewers ranging from the presentation only having minor presentation flaws, to criticism about the applicability of the proposed method, concerns about the impact of reduced data on the baselines results that needs to be further discussed, and one reviewer arguing lack of novelty, that the methodology was hard to follow, and raising that results concerning improved transfer ability and replay-based methods are not shown in the experiments. Moreover, one reviewer suggested the authors may consider including results for settings with resource disparities. In response, the authors clarify their setting and its applicability, addressed the criticism about the impact of the size of the dataset on the baselines and also that about resource disparities, provided additional experiments including a replay-based method, and addressed concerns related to novelty, clarity and provided additional results to highlight the improved transfer capabilities of the proposed methodology. It is worth noting that the authors presented a comprehensive rebuttal including detailed responses to the reviewers concerns and questions and included substantial new experimental results. Moreover, the reviewers who raised concerns engaged in discussion with the authors. However, one of the reviewers still insisted that the proposed approach is incremental and of limited value to the NLP community."
            }
        },
        "id": "x3AQOZiL21",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3JBKnkUACW",
        "replyto": "3JBKnkUACW",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4836/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599177,
        "cdate": 1696707599177,
        "tmdate": 1701465542886,
        "mdate": 1701465542886,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper investigates the impact of different prompts on the behavior of ChatGPT based on the TREC Misinformation dataset. They report the bias induced in the model applied to the healthcare domain.\n\nAll reviewers agreed that the paper studies an important topic, and \"support safety concerns in healthcare sector\". Reviewers asked some specific questions about the study method and the statistical significance of the results, which I believe the authors sufficiently addressed in their rebuttals. Note that one reviewer who rated the paper most negatively did not update their score, but did comment that \"I think the paper is good to go now.\"\n\nI recommend accepting the paper into the Main track, and encourage the authors to add a figure illustrating the workflow, extending the related work session, and providing clarification to the human eval."
            }
        },
        "id": "AFJagRPGdA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3I1A9xAI8S",
        "replyto": "3I1A9xAI8S",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2653/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546817,
        "cdate": 1696707546817,
        "tmdate": 1701465472277,
        "mdate": 1701465472277,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper evaluates the generation quality of interpolation-based retrieval-augmented language models, exemplified by kNN-LM, which interpolates the predictions of the LM with the predictions based on the most relevant retrievals from the prefix. They find that while such methods improve overall perplexity, the do not improve the quality of open-ended text generation, using both automatic metrics and human evaluations. They provide analyses and plausible explanation of the perplexity decrease reported in previous research. The paper is well written and the experiments well constructed, with insightful analysis of the existing interpolation-based kNN-LM model that can guide better evaluation of similar models in the future."
            }
        },
        "id": "8r4d1zTFBX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3FNrGv5MKb",
        "replyto": "3FNrGv5MKb",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4328/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707587549,
        "cdate": 1696707587549,
        "tmdate": 1701465528107,
        "mdate": 1701465528107,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents INSTRUCTOPENWIKI, a large-scale instruction-following open-world IE dataset, and the creation of the PIVOINE model, an instruction tuning model based on BLOOM. The reformulation of open-world IE into a linearized JSON format is a promising solution to the task, though the open-world naming is overclaimed and misleading. The main concerns are with data leakage, though this issue seem to be addressed by the authors in the rebuttal."
            }
        },
        "id": "0GGGLZSGhu",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3F1qEXWKFE",
        "replyto": "3F1qEXWKFE",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2251/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537892,
        "cdate": 1696707537892,
        "tmdate": 1701465459050,
        "mdate": 1701465459050,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents a large dataset (100k dialogues) for dialogue quality assessment; the dialogs are in Chinese and are annotated on six dimensions: grammaticality, relevance, consistency, empathy, proactivity and informativeness, and covers six different domains. The dataset is annotated manually despite its large size. The reviewers appreciate the analyses conducted on the dataset. The reviewers however point to additional work that should be cited and have questions about the annotation process and the annotators, to which the authors replied satisfactorily in the response (The annotators are full-time employees who have continuously annotated the dialogue quality for WenYiWen for over a year, and all of them hold a college degree at least. They have undergone training to annotate the dialogue quality to meet the requirements before the formal annotation. Thus, we regard them as the experienced annotators. Moreover, the quality controllers are those annotators with higher annotation accuracy. The annotation team comprises 26 employees, including 14 females and 12 males.)"
            }
        },
        "id": "rR0q89gzUl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3ErwybEDgt",
        "replyto": "3ErwybEDgt",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3313/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707560291,
        "cdate": 1696707560291,
        "tmdate": 1701465493668,
        "mdate": 1701465493668,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper analyzes design decisions for sparse feedforward layers in large language models. It presents a unified framework that connects sparse mixtures of experts with sparse neural memory and empirically compares the two methods along with new variants that take elements of each. Inspired by their analysis, the authors present a new routing method called avg-k that outperforms previous methods. \n\nReviewers generally found the paper clear and appreciated the strong empirical results, comprehensive analysis, and insights from the new conceptual framework. However, a central concern with the paper brought up by all reviews is potential lack of scalability to settings where model parallelism is necessary (models in the paper are up to 850M parameters and are not trained with model parallelism):\n* The finding that a small block size (essentially activating a larger number of smaller experts) works better given the same compute budget may not hold because communication costs will increase. \n* The lack of load balancing in Avg-K will become more of a problem because speed will be limited by the devices receiving more tokens. While the paper does argue that load balancing should combine well with avg-k, there aren’t experiments (even at smaller scale) to justify this.\n* Possibly due to the lack of load balancing, avg-k works poorly compared to baselines when using a larger block size, but that might be the more realistic setting when thinking about a model-parallel version.\n\nIn the rebuttal, the authors argue that while they believe there exist implementation details or method variants that could reduce communication overhead, the engineering required to develop an efficient model-parallel implementation would be very high. Instead, they see the value of the work in presenting their unified framework and in providing findings (e.g that Avg-K with a small block size is FLOP-efficient) that could be built upon in future engineering-focused research (e.g. adding load balancing and reducing communication overhead). While I do think the limitations may diminish the impact of the paper, I don’t think we should limit research on language model sparsity only to researchers with huge engineering budgets, and I believe the findings still could be of interest to the community."
            }
        },
        "id": "0FaPCE6MRk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3EcjsgPq74",
        "replyto": "3EcjsgPq74",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission429/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487427,
        "cdate": 1696707487427,
        "tmdate": 1701465398598,
        "mdate": 1701465398598,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work focuses on parameter-efficient tuning (PET) and the impact of the pretrained model size on PET methods and design differences. The authors found that as the pretrained model scale increases, the performance differences among PET methods decrease. They explore the impact of model scale in terms of the position and the number of tunable parameters.   \n\nThe authors present a comprehensive study including several tasks and different PET methods/designs in the form of a unified framework (APET) for analysis. As reviewers commented, the paper presents empirical findings based on solid experimental results that are helpful for researchers who study PET methods. Furthermore, I believe authors will incorporate the reviewers suggestions and points together with their adiditonal results to the final version of the paper."
            }
        },
        "id": "B66NJ9NPTM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3CIQIYNGlp",
        "replyto": "3CIQIYNGlp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission464/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707488412,
        "cdate": 1696707488412,
        "tmdate": 1701465399824,
        "mdate": 1701465399824,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper studies text-image retrieval and proposes a method whereby text and images are mapped into a sparse, interpretable space where individual dimensions correspond to wordpiece tokens. This is in contrast to models such as CLIP that utilize dense and less interpretable embeddings. Nevertheless, the paper demonstrates performance improvements over CLIP using its interpretable embeddings.\n\nPros (from reviewers):\nThe proposed method is novel and appealing from both an interpretability standpoint as well as a retrieval accuracy standpoint\nThe experiments are extensive and convincing\nThe limitations and future work are given adequate consideration in the text\nThe paper is clear and well-written\n\nCons (from reviewers)\nThe paper uses a closed-source 1B example training set\nThe multi-stage training procedure of the model is not described in as much detail as it could be\nIt is unclear from the experiments where the performance gains over CLIP are coming from. Is it due to the well-tuned training recipe, or from the sparse nature of the proposed embeddings themselves?"
            }
        },
        "id": "c0QLuJq3bM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "3AxESAk0Re",
        "replyto": "3AxESAk0Re",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission296/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484049,
        "cdate": 1696707484049,
        "tmdate": 1701465394008,
        "mdate": 1701465394008,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Four reviewers gave the Soundness scores 3/4/2/4 and Excitement Scores 4/4/2/3. The authors have responded to all four reviewers and all reviewers acknowledged the author responses.\nAll reviewers recognise the contribution of this paper on addressing idiomatic MT, one of the bottlenecks in MT field, especially  in French, Finnish, and Japanese. \nMajority of the reviewers suggest this is a good and solid work on Soundness, but one reviewer raised many concerns regarding the synthetic experiment, dataset quality control, methods justification, and paper coherence. \nThere are active discussions between reviewers and authors, especially on comments from the lower review scores.\nOverall this is an interesting paper with solid work. It shall appear in main / findings. \nAppendix content can be reorganised to save spaces and reduce number of pages. I hope the authors can address this."
            }
        },
        "id": "NRVESyJWzR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "38k1q1yyCe",
        "replyto": "38k1q1yyCe",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4715/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707597024,
        "cdate": 1696707597024,
        "tmdate": 1701465539438,
        "mdate": 1701465539438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new approach to conjunction recognition. The method consists of two steps: (1) identification of coordinators and (2) filling in relevant arguments through sequence labeling. The method outperforms regular parsers on three datasets and improves the performance of the downstream task (information extraction).\n\nThe reviewers agreed while ranking **soundness** of this paper as strong (scores 4, 4, and 5). Among the strengths of this submission, they listed a *straightforward architecture* and *informative error analysis*. \nIn the first round, the reviewers had many questions regarding the details of the proposed method. However, the answers from the authors clarified most of the issues. Therefore, it is strongly recommended that these explanations and clarifications be included in the camera-ready version.\n\nRegarding **excitement**, the reviewers chose more neutral scores (3, 3, and 4). On the one hand, they were convinced that the paper provides a *good solution* to the described problem. The proposed model achieves *state-of-the-art performance* and tackles concrete shortcomings of other approaches. On the other hand, conjunction recognition is a very specialized task. Therefore, the paper needs to be clear and provide all the necessary details to reach a broad audience. The reviewers found that this level of detail needs to be improved.\n\nTo sum up, it is a sound submission about a particular task. It requires clarification and details that can be added in the camera-ready version. When the paper is clearer, it will also make its readers more excited."
            }
        },
        "id": "a9OMLEmKZ2",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "377w7agYKC",
        "replyto": "377w7agYKC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3392/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707561770,
        "cdate": 1696707561770,
        "tmdate": 1701465495927,
        "mdate": 1701465495927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Wasserstein distance is an attractive method for comparing probability distributions because it can easily be applied in NLP for tasks involving comparing documents (eg, classification). Its computational cost remains a bottleneck and various methods have been proposed to accelerate it. This paper proposes an improvement over one of those methods (tree Wasserstein distance), which is beneficial when applied to high-dimensional problems (as is normally the case in NLP).\n\nReviewers pointed out the originality of the approach as well as the empirical results (gains are substantial) as strengths. There were lingering questions around some experimental decisions, in particular regarding the use of different baselines for different datasets and the relative performance of some proposed combinations. The authors addressed those questions - their presence highlighted by various reviewers seem to indicate that those are point that should be clarified in the manuscript."
            }
        },
        "id": "2yvm9DyXPk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "33aJCNQV1C",
        "replyto": "33aJCNQV1C",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1201/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707507229,
        "cdate": 1696707507229,
        "tmdate": 1701465423539,
        "mdate": 1701465423539,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The main strength of this paper lies in coming up with a simple yet effective idea (of multiple LLMs communicating their thoughts to each other) and demonstrating consistent gains across 9 datasets and multiple CoT baselines. The reviewers found the experiments extensive and the analysis valuable. Moreover, the additional data provided by the authors during the rebuttal period was helpful in clarifying some open questions/concerns; I would strongly suggest that the authors include them in the revised version of the paper as they have indicated they would like to do.\n\nOne concern about the work is that the idea of multiple agents or LLMs communicating is not novel. E.g., Decomposed Prompting (ICLR-2023), ReAct (ICLR-2023), etc., have considered solving compositional problems by composing steps from simpler individual agents. That said, the way communication across multiple agents is used in the proposed setup is sufficiently different from prior uses."
            }
        },
        "id": "tjFBs4ZMas",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "30kbnyD9hF",
        "replyto": "30kbnyD9hF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission339/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707485122,
        "cdate": 1696707485122,
        "tmdate": 1701465395624,
        "mdate": 1701465395624,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a novel multimodal framework, CORECT for emotion recognition in conversations. The proposed architecture consists of two major component to capture the local context features in utterance-level along with the cross-modal global context features in conversation-level. The authors shows some modest gain in performance of CORECT when compared with other multimodal ERC research using two widely used datasets. \n\nAll the reviewers agree that the current study provides sufficient support for its claim and offers meaningful advancements over existing emotion recognition methods. However, the reviewers have identified some minor issues, such as:\n1. An insufficient discussion regarding the limitations of the proposed framework.\n2. Lack of complexity analysis, more focusing on computational efficiency of the model for practical implementation among others.\n\nThe authors have addressed some of these concerns in the comment and in the rebuttal.  By addressing the reviewer’s comments, the paper quality will improve substantially. \nI believe this paper will be interesting to the Speech community."
            }
        },
        "id": "H6LEGxI7J1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2z9o8bMQNd",
        "replyto": "2z9o8bMQNd",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5074/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707606368,
        "cdate": 1696707606368,
        "tmdate": 1701465549164,
        "mdate": 1701465549164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes methods to improve the instruction-following capabilities of the Alpaca model (learned through distilling from OpenAI models via self-instruct). The proposed method includes probabilistic ranking and contextual ranking that are sequentially applied to finetuning LLMs. The proposed method is evaluated on Super NI, LMentry, and Vicuna QA datasets and demonstrate its effectiveness."
            }
        },
        "id": "CbLKRnGcFc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2z4s0W375H",
        "replyto": "2z4s0W375H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2913/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552083,
        "cdate": 1696707552083,
        "tmdate": 1701465480968,
        "mdate": 1701465480968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors write a convincing position paper against utilization of editing techniques to modify model output to be more desirable. Posthoc approaches can have unchecked effects on other axes of model knowledge and generation, and requires more reflections, as identified by authors. While reviewers point out that the alternate approaches to editing that authors contribute are not substantial, I don’t think that is needed or the central point of the paper. As a lot of work quickly evolves to do more ‘editing’, this work comes as a very timely counterpoint and will benefit the community.\nI would encourage the authors to reflect further on the alternative they propose as well as the need of ‘factual checking’ to contextualize their position."
            }
        },
        "id": "wC8SG4A0Nl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2wFVkTDGOZ",
        "replyto": "2wFVkTDGOZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2037/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707532740,
        "cdate": 1696707532740,
        "tmdate": 1701465451373,
        "mdate": 1701465451373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a method to evaluate the relationship between degree expressed by modifiers and their sensitivity to sentence polarity. The authors draw on the artificial language learning paradigm from psycholinguistics and apply it to BERT, finding that the model’s generalization patterns align with linguistic observations relating degree and polarity sensitivity.\n\nThe reviewers express that the paper is clear and presents a meaningful synthesis between linguistics and NLP. The reviewers also express that the methodological setup is novel, interesting, inspiring. The reviewers raised some issues pertaining to clarity, much of which has been addressed in discussion.\n\nMy main hesitation, also mentioned in some of the reviews, is the focus of the paper’s experiments solely on the BERT model, which imposes some limitations on the generality and impact of the conclusions. However, based on the merits and interest of the methodology, and the interdisciplinary value, I recommend that the paper be accepted at least to Findings, and potentially to the main conference."
            }
        },
        "id": "Vj21IQeusk",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2qKRa94sow",
        "replyto": "2qKRa94sow",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1655/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707522122,
        "cdate": 1696707522122,
        "tmdate": 1701465437593,
        "mdate": 1701465437593,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates if you can write prompts for LLMs in pseudocode rather than natural language and investigates how well this works. The authors discussion period had a spirited discussion with the authors and the reviewers where the reviewers' concerns were mostly resolved; this discussion shows that the results are interesting and surprising and left the reviewers convinced that the results were indeed sound."
            }
        },
        "id": "CCCkF8XUzr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2prcotJejU",
        "replyto": "2prcotJejU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1500/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707515825,
        "cdate": 1696707515825,
        "tmdate": 1701465432604,
        "mdate": 1701465432604,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces an approach to mitigating entity bias in pre-trained language models by employing causal interventions structured around a specific causal model. These interventions replace original entities with neighboring ones to reduce biasing information while maintaining semantic relevance. Experimental results show significant improvements, especially in out-of-distribution performance across different datasets and modes of operation (white-box and black-box settings).\n\nI agree with the reviewers that the approach to construct a prompt that is effective for debiasing models is likely of significance to future work in the field. The reviewers are generally of the view that the paper is methodically sound but may only appeal to a narrow audience. Their constructive feedback has highlighted areas that need further clarification in the paper as the authors have also appreciated in their responses.\n\nTwo reviewers were very appreciative of the experimental setup --- both the empirical results as well as the ablation studies. One reviewer had little to say on their reasons to accept but also pointed out that some of these issues could be specific to the prompts used, raising concerns with the experimental setup. The prompts shared in examples in the paper are fairly generic and likely what an average user would input so I don't see any issues with the experiment design as such, but it might be a worthy exploration to see whether such entity bias is prompt dependent or not. \n\nAside from the great suggestions offered in the reviews, I would also point the authors to Section 2.1 in [1] (which, to my knowledge, is one of the first papers to anonymize entities to prevent entity bias) which also follows a similar thought process to mitigate entity bias by modifying the corpora. Though there is not a 1:1 match between the two approaches, they are similar enough to warrant a discussion in this paper's related work.\n\n[1] Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., & Blunsom, P. (2015). Teaching machines to read and comprehend. Advances in neural information processing systems, 28."
            }
        },
        "id": "2M7I2VkEjx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2mxzS2Xv2e",
        "replyto": "2mxzS2Xv2e",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission131/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707479980,
        "cdate": 1696707479980,
        "tmdate": 1701465388296,
        "mdate": 1701465388296,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a new benchmark \"CRAB\" -- Causal Reasoning Assessment Benchmark, which is used to evaluate causal understanding of events in real-world narratives. The dataset contains 2,730 pairs of real-world events, which are based on a set of 20 stories (while it seems small, the annotations are non-trivial and 7-10 annotators have annotated each instance). The performances of GPT models on this dataset suggest that these causality tasks are not very good, highlighting that this is an open task. \nThe paper also contains analyses to provide insights into what makes the task difficult."
            }
        },
        "id": "bj8ZRgHMcR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2lI1pVL6aj",
        "replyto": "2lI1pVL6aj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3447/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562863,
        "cdate": 1696707562863,
        "tmdate": 1701465497869,
        "mdate": 1701465497869,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This is a very interesting paper that proposes an approach to automatically extract cultural norms from multilingual conversations. A strong reason to accept this is that this paper presents an effective and innovative solution to an under-explored, important problem which will broaden the NLP field by looking beyond the cultural norms of English-spoken western societies. \n\nWhile there were concerns about the novelty of the work, pointing out that previous work in IE can be applied here, authors addressed that concern in a convincing way that social norms cannot be described in a word or phrase which is what IE does. The concerns about the language models used and the narrow scope of the data are valid, but I think the authors' replies are sufficient. I would like to see additional experiments done with open-source large language models, as well as discussions about extracting social norms from other sources.\n\nOverall, this is fascinating work which will lead to further research in this important topic."
            }
        },
        "id": "ZAbNu7e0sn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2kSufHoYEi",
        "replyto": "2kSufHoYEi",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2827/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550250,
        "cdate": 1696707550250,
        "tmdate": 1701465477869,
        "mdate": 1701465477869,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper describes a new text generation metric (T5Score) trained on the signals used by both generative and discriminative metrics. The motivation is clear. The main objective of this framework is to learn evaluation metrics based on the assumption that generative and discriminative objectives can work in concert to train a better evaluator (having the best of the two worlds).\nThe paper is well written. \nThis new framework is evaluated on 5 datasets and tasks and the new metric is compared to a various of existing metrics (BLEU, ROUGE, COMET, BERTScore, BLEURT, PRISM and BARTScore). These experiments show that this new metric outperforms leading metrics on segment-level outputs."
            }
        },
        "id": "EoFJRsGnqL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2jibzAXJzH",
        "replyto": "2jibzAXJzH",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3010/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554129,
        "cdate": 1696707554129,
        "tmdate": 1701465484208,
        "mdate": 1701465484208,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The article presents a new method for projecting annotations, which improves on the performance of the state of the art. The reviewers suggested a number of experiments to better understand and analyse the results, which I think are worthwhile, which is why I think this article is only “borderline”."
            }
        },
        "id": "UiliA7kE8r",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2hYi3mXxqf",
        "replyto": "2hYi3mXxqf",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3604/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565988,
        "cdate": 1696707565988,
        "tmdate": 1701465503236,
        "mdate": 1701465503236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Paper Topic And Main Contributions:\n* This paper proposes a multi-view temporal graph enhanced reasoning framework for temporal reasoning over time-involved documents, which explicitly models the temporal relationships among facts. The authors conduct extensive experiments on two time-invoved QA datasets.\n\nReasons to accept:\n* The topic is interesting and the solution is clear. The results show that the proposed method outperforms baselines.\n\nReasons to reject:\n* Some details in the graph construction part are not clearly described. The authors explain more details in the rebuttal.\n* The supervised baselines are not up-to-date. The authors add results in the rebuttal that show the superiority over additional, newer methods for long-input decoding: LED and LongT5."
            }
        },
        "id": "44PFH86LcK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2c3u5YDUUy",
        "replyto": "2c3u5YDUUy",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2900/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551833,
        "cdate": 1696707551833,
        "tmdate": 1701465480622,
        "mdate": 1701465480622,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes a framework for analyzing the effect of a dataset on a model's linguistic abilities, as quantified by probing task performance. The framework allows us to investigate if certain datasets have undesirable effects on the model's linguistic abilities. An interesting finding from the work is that some datasets negatively influence abilities that seem unrelated to the task nature.\n\nOverall, the reviewers agree that the soundness of the proposed work is sufficient to support its main arguments, though several reviewers point out additional points that should be addressed in a revision. Specifically, a common note among reviewers is to clarify the results in Tables 2-4 relating to individual effects and display a measure of the variance in a more prominent way. Another limitation that is brought up is that downstream task performance is not reported. Even though downstream task performance is not the focus of the work, it's important to report as the degree of encoded linguistic abilities may relate to the degree of fine-tuning success. We recommend that these points are addressed in a revision. On the positive side, reviewers praised the clarity of the writing and thought the research question to be of significance to the EMNLP audience."
            }
        },
        "id": "O1ybPGDOoB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2bBIY12n43",
        "replyto": "2bBIY12n43",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4422/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707589351,
        "cdate": 1696707589351,
        "tmdate": 1701465531229,
        "mdate": 1701465531229,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper aims to reduce the cost of MLPs found in Transformer architectures by decomposing the fully connected layers into smaller parallel components. Results demonstrate improved performance/faster runtime. Reviewers mostly agree on the importance of this research direction and find the proposed method promising. However they point out the lack of comparison with related work (sparse-MoEs) and ablations; which impacts the soundness of the claims made in the paper."
            }
        },
        "id": "5CRw283y6X",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2b7aSGxb6M",
        "replyto": "2b7aSGxb6M",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1650/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707521844,
        "cdate": 1696707521844,
        "tmdate": 1701465437400,
        "mdate": 1701465437400,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an extensive study of NMT systems that exploit context beyond the current sentence. It identifies several challenges to progress in this area, including sparsity of context signals and the need for accurate metrics. It proposes a simple framework based on paragraph-level translations, and presents baseline results on a new public dataset.\n\nReviewers were generally positive, finding the paper clearly written, and the analyses comprehensive and well organized, with solid experiments. The released dataset and benchmarks were also appreciated. On the negative side, one reviewer felt that the paper didn’t break new ground, and that the proposed paragraph-to-paragraph framework was overly simplistic.\n\nMoving beyond sentence-level translation is an important problem for the field, and this paper goes further than previous studies in analyzing reasons for the relative lack of progress. Given the rather confused state of the art in this area, starting afresh with a clear and simple benchmark and dataset - armed with a detailed set of empirical observations about the signals available from extra-sentence context - seems like an excellent strategy. I think this work has the potential to make a large impact."
            }
        },
        "id": "lGDxmG4I4u",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2anfut5geh",
        "replyto": "2anfut5geh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2580/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545189,
        "cdate": 1696707545189,
        "tmdate": 1701465469724,
        "mdate": 1701465469724,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a task-adaptive tokenization method and shows its efficacy on long-form generation in mental health. The method is simple and easy to integrate with the potential to be applicable across several downstream tasks and languages. The paper can be further strengthened by adding results on other tasks and by adding the analysis as provided in the rebuttal."
            }
        },
        "id": "CgHbcKOjBa",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2YEY9SPVEA",
        "replyto": "2YEY9SPVEA",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1885/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529401,
        "cdate": 1696707529401,
        "tmdate": 1701465445784,
        "mdate": 1701465445784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper \"FACTIFY 3M: A Multimodal Dataset for Fact Verification\" introduces an intriguing and valuable contribution to the field of fact verification. It addresses the pressing issue of disinformation and its pervasive impact on society, particularly in the context of multimodal disinformation disseminated through images and videos on social media platforms. The dataset, FACTIFY 3M, is the central focus of the paper, consisting of 3 million samples encompassing textual claims, paraphrased claims, associated images, pixel-level image heatmaps, 5W question-answering pairs, and adversarial fake news stories. The primary objective of FACTIFY 3M is to support research endeavors related to multimodal fact verification and enhance interpretability through visual question-answering. The paper also emphasizes the dataset's potential applications, including detecting adversarial attacks, verifying complex facts, and assisting journalists in fact-checking.\n\nIn summary, the paper introduces a promising and relevant dataset in the field of multimodal fact verification. Its strengths lie in its comprehensiveness, the critical societal issue it addresses, and its focus on interpretability. However, to strengthen its case for acceptance, the paper should consider augmenting its content with concrete examples and experimental results demonstrating the practical value of FACTIFY 3M for researchers and practitioners engaged in fact verification tasks."
            }
        },
        "id": "deR7uKtVcv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2XDbDwNlTn",
        "replyto": "2XDbDwNlTn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2115/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707534858,
        "cdate": 1696707534858,
        "tmdate": 1701465454545,
        "mdate": 1701465454545,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This work addresses the problem of speech translation from spoken Swiss German dialects into written Standard German and investigates the performance in many different scenarios to demonstrate the effects of dialect diversity empirically.\nThe reviewers agree on its substance but do not fully acknowledge its “technical” contributions.\n\nHere is a list of the pros and cons of this paper.\n* Pros\n- Thorough experimental comparison in different scenarios (xjs8, LM7U)\n- Valuable for future studies in speech translation for low-resource languages/dialects without a standard written form (qYnP, LM7U)\n- Detailed analyses and discussions on the differences among dialects (xjs8, LM7U)\n* Cons\n- Limited focus on Swiss German, which is not necessarily generalizable to other languages/dialects (xjs8); It is not so serious unless there are any studies working on very similar problems."
            }
        },
        "id": "HM8CGuLQaq",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2X5RXTOsLU",
        "replyto": "2X5RXTOsLU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3493/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563888,
        "cdate": 1696707563888,
        "tmdate": 1701465499242,
        "mdate": 1701465499242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work proposes constructing multi-domain Dialog State Tracking (DST) models using a divide-and-conquer (DAC) DST paradigm along with multi-domain dialogue synthesis from existing single-domain dialogues.\n\nContributions:\n - The paper proposes two novel methods, including divide-and-conquer paradigm and data synthesis framework, to address the key challenges of building multi-domain models without multi-domain data.\n - The method is data-centric and thus generic for various models.\n - Experiments demonstrate the effectiveness of the proposed methods."
            }
        },
        "id": "6LrjUEZCgR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2WZ4Wp1OSo",
        "replyto": "2WZ4Wp1OSo",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5446/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707613187,
        "cdate": 1696707613187,
        "tmdate": 1701465559658,
        "mdate": 1701465559658,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposed a new auxiliary task for vision-and-language navigation, and achieves effective improvement over the baseline. I especially appreciate the honesty in both the paper (e.g., using a stronger baseline with latest CLIP model) and the discussions (e.g., acknowledge that 100% masking = img-goal navigation). Overall, most of the reviewer's concern are resolved during the discussion. \n\nP.s., I encourage the authors to incorporate the proposed changes in their final summary, and also resolve the confusion raised by reviewers. Some confusions come from writing (not from results). For these, I did not see a plan of changes in the final summary but I think that these clarifications are important. A paper is born for readers to learn, thus clear writing is also important as the strong results."
            }
        },
        "id": "Q5DMxwhlKn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2UJvVc8gnP",
        "replyto": "2UJvVc8gnP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1127/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707505561,
        "cdate": 1696707505561,
        "tmdate": 1701465421254,
        "mdate": 1701465421254,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers are positive about the soundness of the paper. The work is timely, addresses an important question in transfer learning, and the analysis is thorough. The writing is easy to understand. The reviewers raised some issues about the experimental design, some of which the authors addressed during the author response period.\n\nOverall, the reviewers believe the paper is sound and are excited to see it published."
            }
        },
        "id": "zeQ02s4zkv",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2U9hDBaOCn",
        "replyto": "2U9hDBaOCn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2566/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544901,
        "cdate": 1696707544901,
        "tmdate": 1701465469270,
        "mdate": 1701465469270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper studies the task of learning interpretable style embeddings. To achieve this, the paper presents a synthetic data generation protocol from GPT-3 (using hand-crafted prompts) which is then used to train style representations. The style embeddings are shown to be competitive with recent approaches, while having a better interpretability. The use of LLM in the context of this goal is interesting and creative. I do share very similar concerns raised by cLTM, as the quality of the StyleGenome could have gone through a more rigorous evaluation, but the resource (to my opinion as the main contribution of this work) as-is has merits and potential for use by the community."
            }
        },
        "id": "MOFoAGLPiE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2TtN6DqjWa",
        "replyto": "2TtN6DqjWa",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission414/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707487083,
        "cdate": 1696707487083,
        "tmdate": 1701465398184,
        "mdate": 1701465398184,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses the challenge of efficiently utilizing Large Language Models (LLMs) as data creators for natural language understanding tasks. It introduces a unified data creation pipeline that requires only a single formatting example and applies to a wide range of tasks. The paper emphasizes the cost-effectiveness of instruction-following LLMs in generating data and demonstrates that models trained with LLM-generated data outperform those trained with human-labeled data on out-of-distribution evaluation while maintaining comparable performance on in-distribution tasks.\n\nReviewer 1 raised concerns about the lack of results on training LLMs with the generated examples and the paper's focus on NLU tasks. The authors responded by explaining their focus on NLU tasks and leaving the use of generated data for LLM fine-tuning as future work.\nReviewer 3 expressed the need for comparisons with other LLM-based data generation methods, additional technical details, and clarification on various aspects of the paper. The authors addressed these concerns by agreeing to include sub-optimal performance comparisons, provide detailed hyperparameter settings, and explain various processes in the final paper.\n\n**Note:** The review and scores of reviewer \"@6qx1\" need to be ignored due to the low-quality review."
            }
        },
        "id": "IsKs3H2JfB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2Rdfdri2oT",
        "replyto": "2Rdfdri2oT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5189/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707608759,
        "cdate": 1696707608759,
        "tmdate": 1701465552503,
        "mdate": 1701465552503,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Four reviewers gave the Soundness scores 3/3/4/3 and Excitement Scores 3/4/3/2. The authors have responded to all four reviewers and all reviewers acknowledged the author's responses.\nAll reviewers tend to agree that this paper has positive Soundness in addressing context-aware MT Evaluation, with a minimum score 3 - good level. However, most reviewers do not seem to be excited for this paper to appear in the main conference. It shall appear in the findings if possible."
            }
        },
        "id": "3IqM2UVtGS",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2O39az85g6",
        "replyto": "2O39az85g6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5603/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707615742,
        "cdate": 1696707615742,
        "tmdate": 1701465563470,
        "mdate": 1701465563470,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The manuscript introduces a novel guided decoding during generation using a discriminator. While the idea of using a discriminator in generation is not novel (example Kumar et al 2023), this is a promising research area and how the discriminator should incorporated is still an open question which this work try to address. The propose method GRACE achieves strong results, on multiple test sets compared to other decoding approaches. However, as highlighted by authors, the discriminator is run after each token generated. For example Kumar et al 2023 to avoid the significant over head of running a discriminator they leverage the encoding from the original LLM. Since this will impact feasibility of work the manuscript would have benefitted from a dedicated section.\n\n\nKumar et al 2023, \"Controlled Text Generation with Hidden Representation Transformations\""
            }
        },
        "id": "7RtSr2Vyek",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2MiTZxLFA9",
        "replyto": "2MiTZxLFA9",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4059/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707582208,
        "cdate": 1696707582208,
        "tmdate": 1701465518613,
        "mdate": 1701465518613,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "**Pros**: the methods is novel and result is impressive as all reviewers agreed.\n\n**Cons**: there are confusions or understanding issues raised by multiple reviewers. They all grasp the confusing parts after author discussion though. Hope the authors could include the explanations in the updated version of the paper.\n\nOverall, This short paper proposed a novel framework with training dynamics to filter low-quality data for multi-choice selection problem. The results is impressive compared to multiple baselines."
            }
        },
        "id": "ZABqEaAA2a",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2MXXycs2T6",
        "replyto": "2MXXycs2T6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1853/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707528806,
        "cdate": 1696707528806,
        "tmdate": 1701465444698,
        "mdate": 1701465444698,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a Bayesian-based method for hallucination detection in LLMs. Reviewers agree on the merit of the proposal and mostly raise minor issues."
            }
        },
        "id": "4NiMsS5xGT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2MDPYm3FPl",
        "replyto": "2MDPYm3FPl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3015/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554321,
        "cdate": 1696707554321,
        "tmdate": 1701465484407,
        "mdate": 1701465484407,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper proposes a new model for guideline learning/refinement to enhance in-context learning. The idea is to provide a few training examples and a set of rules (guidelines). If the model makes an incorrect prediction, it updates the rules. The updated rules are then used for model inference.\n\nThe paper makes significant contributions to an active area of research and would be of considerable interest to the NLP community. The idea pursued in the paper is intuitive and clear, and it is convincingly demonstrated.\n\nThe paper lacks a mandatory \"Limitations\" section, which violates the EMNLP policy and could result in desk rejection. However, the authors have included text for Limitations section as part of the discussion and will include in the next revision. Moreover, as part of the discussion, the authors conducted additional experiments.The paper is well-written and easy to follow, with only a few typos and missing references."
            }
        },
        "id": "qay1njyrIW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2KTvN4Edvl",
        "replyto": "2KTvN4Edvl",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3546/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707564985,
        "cdate": 1696707564985,
        "tmdate": 1701465501037,
        "mdate": 1701465501037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a new model for Universal Information Extraction (UIE) named RexUIE, which concatenates extraction results from the previous iteration with the extraction schema. The proposed methods are evaluated across 17 different datasets and 6 different tasks, such as extracting entities, relations, events, sentiment, and quadruples and quintuples.\n\nOverall, the proposed model demonstrates a minor performance gain over the baseline in some cases when the entire training dataset is used (full-shot training), but achieves higher performance gain under few-shot settings. The paper is well-written and easy to follow, and the proposed idea is novel. There are a few missing references, and a table does not follow the EMNLP template, but these issues will be fixed in the final version."
            }
        },
        "id": "AxkFZdUOXQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2IfYI3dkX7",
        "replyto": "2IfYI3dkX7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission644/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707492805,
        "cdate": 1696707492805,
        "tmdate": 1701465406264,
        "mdate": 1701465406264,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviews have a consensus on slightly positive soundness and excitement.\n\nWith respect to excitement, the proposed idea of introducing chunks as an intermediate representation for Open Information Extraction (OIE) is novel.  The applicability of the proposed method is limited because chunk annotations may not be readily available in non-English languages or low-resource settings.\n\nRegarding soundness, the experiments show that the proposed method achieves state-of-the-art performance in four OIE datasets.  The core hypothesis (why employing chunks as an intermediate representation is beneficial to OIE) is not well presented."
            }
        },
        "id": "QIFCelELhA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2FDty4mLqP",
        "replyto": "2FDty4mLqP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2960/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707552933,
        "cdate": 1696707552933,
        "tmdate": 1701465482425,
        "mdate": 1701465482425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All the reviewers agree that the paper presents a well motivated approach, which is is also evaluated in a satisfactory manner and provides strong results. Not only did all reviewers agree on the soundness of the work, but also provided high excitement scores. This paper is an easy recommendation for the main conference.\n\nHowever I would like to highly encourage the authors to pay attention to the recommendations of the reviewers to improve the presentation of the paper, as well as to the questions for clarification. Specifically, reviewer 4EMT pointed out that the paper may be hard to understand for people not completely familiar with WLAC. Please take the advice of all the reviewers into account when preparing the final submission."
            }
        },
        "id": "DAzDVQBZFo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "2AF1OrD7Y1",
        "replyto": "2AF1OrD7Y1",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2343/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707539987,
        "cdate": 1696707539987,
        "tmdate": 1701465461936,
        "mdate": 1701465461936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors present a method for readability assessment. The key concept involves using proper prompting to induce features from a large model. The features are then combined with other features such as linguistic feature. They are combined via orthogonal projection layers. The authors observed improvements over state-of-the-art methods on four corpora. \n\nAll reviewers agree that the paper is clearly written with results seem to be easily reproducible. The evaluation on the four corpora is solid. \nThere is certain ambivalence of excitement from the reviewers. \n\nGiven that many NLP tasks have to deal with data scarcity challenge as the readability assessment problem, the presented approach of using prompting to generate feature from LLMs is interesting and can lead to further explorations."
            }
        },
        "id": "hP3hG0XVmp",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "27HNeESZQF",
        "replyto": "27HNeESZQF",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2244/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537727,
        "cdate": 1696707537727,
        "tmdate": 1701465458748,
        "mdate": 1701465458748,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper is well motivated and well written. The dataset is described well. The authors have considered an exhaustive set of approaches in the experimentation setup and have conducted a detailed analysis. There are new and possibly reusable insights with the use of Swin Transformers and training language models from scratch. \n\nSome points to be improved:\n- Add conventional methods like CNN+LSTM in the experimental analsysis\n- Discuss negative results as well on  the synthetic data generation process  \n- Hard to identify the core novelty proposed in the technique:"
            }
        },
        "id": "GAF2vwuZzc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "266rF9DyWk",
        "replyto": "266rF9DyWk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1575/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518071,
        "cdate": 1696707518071,
        "tmdate": 1701465434924,
        "mdate": 1701465434924,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper explores whether the listener’s non-verbal signal – mainly gaze, follows entropy rate constancy in dyadic conversation. Moreover, the paper also investigates if there is any alignment between the information density of speech content and the listener’s gaze behavior.\nThe study is inspired by Xu et al.'s 2022 study, which found that ERC applies to non-verbal gestures in monologues, this paper extends to test the principle to the listener's perspective in conversations.\n\nWhile the reviews are mixed, most reviewers concur that the results presented are inconclusive. The paper could benefit from more comprehensive explanations and deeper analysis, especially concerning dialogue positions and gaze sequences. Despite being a short paper, the paper should aim for self-contained explanations, detailing the models used rather than solely referencing other works. Additionally, the paper lacks adequate justification for certain design and experimental choices, such as the removal of outliers, and should clarify the reliability and generalizability of the results (e.g., in Fig 4). The authors have acknowledged some of these issues and committed to addressing them in future iterations of the paper.\n\nGiven that the paper's research findings and methodologies hold some interest to the community, we recommend that the authors refine their work for the next version, taking the reviewers' feedback into account."
            }
        },
        "id": "BKZ7mm36tc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "219K9bcUgC",
        "replyto": "219K9bcUgC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3178/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707557609,
        "cdate": 1696707557609,
        "tmdate": 1701465489341,
        "mdate": 1701465489341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Summary:\nThis paper begins by creating a new dataset for multimodal fine-grained entity typing, building upon FIGER. The authors introduce a multimodal object-level visual context network, which employs attention layers to fuse textual information and object-level visual data, facilitating the capture of fine-grained semantic information. Comparative experiments with several existing textual-level models demonstrate that the integration of object-level visual information enhances performance.\n\nReason To Accept:\n1.A new dataset has been created for the multimodal fine-grained entity typing task, and the proposed model demonstrates a significant improvement in performance through the incorporation of object-level visual information. The MFIGER dataset holds promise as a valuable benchmark for future research and is a beneficial contribution to the community.\n2.The motivation behind this work is clear. It marks the first attempt to introduce the multimodal fine-grained entity typing task, and the paper is well-written and easy to follow. \n3.The reported results showcase substantial improvements: 11.27% improvement in total level, 1.91% in coarse level, and 10.46% in fine-grained level strict accuracy over baselines on MFIGER. Additionally, the attention visualization enhances the interpretability of the proposed model.\n\nReason To Reject:\n1.The use of cross-modal attention layers for fusing object information is a common practice in multi-modal information extraction. It's advisable to reference relevant literature in the missing references section. Additionally, the hybrid classification layer appears to align with previous work in the field.\n2.Several comparison experiments with baselines are lacking. 1)  For other text-only methods, it would be valuable to assess whether VinVL features can enhance text features through attention, ensuring a fairer comparison. 2) The experiments are conducted on a single dataset, and it's advisable to implement more multi-modal baselines for a comprehensive performance comparison.\n3.Missing ablation study on the latent type classifier. It is necessary to investigate whether the inclusion of the latent type classifier contributes to an improvement in classification accuracy."
            }
        },
        "id": "1tSfqQm5jz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1tZxE1WPKz",
        "replyto": "1tZxE1WPKz",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1726/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707525401,
        "cdate": 1696707525401,
        "tmdate": 1701465439625,
        "mdate": 1701465439625,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The reviewers found that the topic (of categorizing and analyzing numerical reasoning abilities of models) to be important and the empirical study extensive and systematic. There were some questions raised regarding the need for yet another taxonomy of numerical problems. The authors, in their response, clarified in detail important differences from prior work in the area. This would be very helpful to include in the revised version of the paper (as the authors indicated they would).\n\nWhile it would be even better to expand the set of models used further to other table-specific models, I think the current coverage is broad enough to provide meaningful value to the readers.\n\nThe findings seem important and valuable enough to share with the community. At the same time, the paper lacks broad excitement, in part due to the existence of some prior efforts on analyzing numerical skills and in part due to the nature of this work being analysis-oriented rather than proposing a novel methodology that pushes the state of the art forward."
            }
        },
        "id": "XNLLhie1Z4",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1qJgZUAc8j",
        "replyto": "1qJgZUAc8j",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1717/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707525032,
        "cdate": 1696707525032,
        "tmdate": 1701465439394,
        "mdate": 1701465439394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviews have not reached a consensus on both soundness and excitement.\n\nRegarding soundness, there is a discrepancy in whether the empirical observations (especially Table 1) adequately support the effectiveness of the proposed method.  I see the effectiveness is well verified, because the proposed method uses segment-level contexts and outperforms state-of-the-art  (Lu and Ng, 2021b) using segment-level contexts, while rivaling state-of-the-art (Xu et al., 2022) using document-level contexts.\n\nAs for excitement, the proposed prompt-based method for within-document event coreference resolution is novel.  I would value this novelty, because to my knowledge this paper is the first paper that employs prompting techniques for event coference resolution."
            }
        },
        "id": "oVmQ09zgb0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1pxxAJwBXj",
        "replyto": "1pxxAJwBXj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1730/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707525670,
        "cdate": 1696707525670,
        "tmdate": 1701465439819,
        "mdate": 1701465439819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper presents an interesting, basic and effective method to identify whether models enclose information from training data. The method can work on a blackbox model and provides extensive experiments on multiple dataset. \n\nApart from minor improvements in the writing, reviewers point out that the method is tested on small models and it could be the case that it works less well for larger ones, so this is a nice first step, and that the choice of summarisation where most other papers investigate machine learning, it is harder to compare to other methods. \n\nOverall, the paper presents a nice first step that can be explored further (does it also work with larger models, how does it compare to other methods when applied to ML) in the future."
            }
        },
        "id": "6qrJ9kAOjT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1mGD6ZLTwv",
        "replyto": "1mGD6ZLTwv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission886/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707498439,
        "cdate": 1696707498439,
        "tmdate": 1701465413652,
        "mdate": 1701465413652,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper proposes a very simple approach for obtaining sentence embeddings: combine representations from different layers (and not necessarily from the last layer only, which is the common procedure). Consistent improvements are reported across many datasets and tasks. There is a consensus among the reviewers on the simplicity and effectiveness of the approach. There were some concerns on the writing of the paper and computational complexity; the authors are expected to work on these. The issues raised by the reviewer YvLi on the limited experiments and interpretability studies are not valid, given the focused scope of this short paper. The SentEval benchmark (8 datasets) is a standard benchmark usually used in the domain. The authors also report results on the STS benchmark (6+ datasets). Interpretability analysis is indeed out of scope."
            }
        },
        "id": "lfE3inFKVy",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1kmIDTfQ4N",
        "replyto": "1kmIDTfQ4N",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5383/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611992,
        "cdate": 1696707611992,
        "tmdate": 1701465558092,
        "mdate": 1701465558092,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explore to transform a encoder only model (in this case an XLM-R) into a generator model. This aims to use these models multilingual encoding capabilities. It is done with a “Semantic-Guided Alignment-then-Denoising (SGA) approach to adapt an encoder to a multilingual generator”. This allows the model to use a small amount of new parameters for this adaption. The reviewers agree that the tackled problems is relevant to the field, and that the proposal is exciting. The main issues found where related with a unclear writing and some terminology confusions. Also the related work should be improved."
            }
        },
        "id": "WLmZ6xtq0k",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1iQMzgmKeD",
        "replyto": "1iQMzgmKeD",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2533/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707544162,
        "cdate": 1696707544162,
        "tmdate": 1701465468105,
        "mdate": 1701465468105,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes to address two types of hallucinations due to question-level and model-level that are beyond the existing self-consistency detection method. Reviewers appreciate that this is a nice contribution to hallucination detection which demonstrates strong performances. The author also provides further results on GPT-4 and PaLM 2 in their rebuttal to show it can consistently deliver improvements. In addition to these new results, we strongly encourage the authors to take the feedback to improve the paper over computational costs, its usefulness for open-source LLMs, and the unresolved challenge of detecting non-factual responses through inconsistency."
            }
        },
        "id": "ll23zlhekW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1gUUznQgVC",
        "replyto": "1gUUznQgVC",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2508/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543567,
        "cdate": 1696707543567,
        "tmdate": 1701465467158,
        "mdate": 1701465467158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper is well-written, and propose an innovative method for document-level relation extraction. Presentation should be improved according to reviewers’ comments."
            }
        },
        "id": "bZL838dQiY",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1faXw8rfeq",
        "replyto": "1faXw8rfeq",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2596/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545500,
        "cdate": 1696707545500,
        "tmdate": 1701465470269,
        "mdate": 1701465470269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper discusses a self-learning strategy for fine-tuning relatively small LM for Question Answering tasks using only unlabelled data. \nThe proposed solution is relatively simple but sound and intuitive and the results observed on multiple datasets are remarkable.\n\nThere is enough substance for a short paper.\nRelated works should be better discussed."
            }
        },
        "id": "JpizJlLjP7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1cKjvlvR7Z",
        "replyto": "1cKjvlvR7Z",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3896/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578689,
        "cdate": 1696707578689,
        "tmdate": 1701465513226,
        "mdate": 1701465513226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "Reviewers agree this work offers an interesting and topical method for leveraging training data \"notes\"/experience at inference-time. Though some are concerned its contributions may be incremental, it does introduce nice augmentations on existing work for a short paper. Its results indicate strong potential, however, there are also universal concerns with understanding crucial details of the method and experimental setup, as well as missing related work. After significant discussion, I hope the authors will heed the advice of the reviewers and agree to (a) fill in missing details from their methodology, (b) add the requested modifications to the related work, and (c) augment the discussion with their additional experiments and carefully contextualize their quantitative results given the small train/test sizes (which also appears in prior work). With these changes the work will be much improved."
            }
        },
        "id": "iVi2Y92cts",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1Xht3SKAoY",
        "replyto": "1Xht3SKAoY",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission866/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707497935,
        "cdate": 1696707497935,
        "tmdate": 1701465413036,
        "mdate": 1701465413036,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This study introduces an entity-level sentiment analysis dataset tailored for the financial domain. Alongside the dataset, the authors present baseline experiments utilizing various models, including BERT, FinBERT, and ChatGPT.\n\nReviewer R2TV appreciated the dataset and its development process. However, reviewer raised valid concerns about the ethics statement and potential implications of the model. These concerns can be addressed in the limitations section, emphasizing that the direct application of the models' decisions could have adverse effects. The authors have already indicated in their rebuttal that an ethics statement will be incorporated, which should be added.\n\nReviewer NjN2 appreciated the detailed description of the data collection and annotation process. To further enhance the paper's readability, it is advised to elaborate on the concerns regarding data size and distribution."
            }
        },
        "id": "Q94onrV7mD",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1WJoJPXwiG",
        "replyto": "1WJoJPXwiG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1581/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707518397,
        "cdate": 1696707518397,
        "tmdate": 1701465435261,
        "mdate": 1701465435261,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Quality: The paper presents an interesting and relevant task. The authors have conducted experiments to back up their claims, however the experiments are lacking details in the current version of the work. Clarifications that were provided in the rebuttals should be added in to the final version of the paper. \n\nClarity: The paper is grammatically sound and well written. However, some reviewers suggested better organization to make the task definition more clear and easier to follow. \n\nOriginality: The paper proposes partisan event detection in news articles, which can be seen as a novel task since most works focus on sentence or document level ideology classification. Additionally, a joint latent model is introduced for solving this task. All the reviewers liked the idea and motivation of the work. \n\nSignificance: Computational political science researchers in NLP will be interested in this work. Detection of media bias is also incredibly important in today's society. However, the lack of experimental details, low inter-annotator agreement on a small dataset (~800 events, 2 political issues), marginal improvement over a minimal baseline, and limited discussion of the application of this work towards understanding framing and bias in the media are considerable drawbacks. \n\nPros: (1) Proposing an important societal task of partisan event detection, which can be seen as novel compared to typical sentence-level (or higher) ideology detection. \n(2) A latent model for event detection, which can also be leveraged in an unsupervised setting. \n(3) A new dataset of 50 articles on 2 political issues, with annotation provided for 828 events.\n\nCons: (1) Underspecified experimental settings which led to reviewer confusion about the task and evaluation. While this was clarified to the reviewers' (bwkA & MVtQ) satisfaction in rebuttal, this missing information is critical and should be included in the final version of the paper. \n(2) The weak performance of the model and lack of further application is a concern to the reviewers. The random baseline performance is quite low and the latent model has marginal improvement. \n(3) The dataset is very small. Of 50 articles, 3035 events were extracted. Of these, 828 events were labeled and used in this work. Also, the reported inter-annotator agreement of 0.43 (and 52.83 in rebuttal) is too low. \n(4) Lack of discussion on the link between ideology, agenda setting, and framing bias; specifically more explanation is needed for how event selection bias contributes to the analysis of media bias. Inclusion of rebuttal points to Reviewer MVtQ could strengthen this area. (5) In the author's rebuttals to Reviewer MVtQ & 9YKN: Please note that the Broockman and Kalla term partisan coverage filtering is essentially framing (here, at the event level). Also, the proposed task of topic-specific stance clustering is again framing of events. The meta-reviewer recommends further reading into the interplay of political ideology, framing, and media bias to strengthen the contributions of this work. \n\nOverall, the reviewers really like the idea and motivation of the proposed task. Inclusion of the details that were provided in the author's rebuttals would greatly strengthen the paper."
            }
        },
        "id": "pdqWp5H7Gb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1VsVZm4DLg",
        "replyto": "1VsVZm4DLg",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2908/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551999,
        "cdate": 1696707551999,
        "tmdate": 1701465480892,
        "mdate": 1701465480892,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper tackles the problem of continual relation extraction, particularly the problem of inferring new relations which have not appeared so far, but are analogous to the already seen relations. There approach is based on treating this as a question-answering problem and using LLMs for this task. The problem is interesting. All of the reviewers raised some concerns about the evaluation and raised the issue that the approach does not show great improvements over the considered baselines. However, I think the reviewers addressed this somewhat in the author response. I would urge the authors to focus on this aspect in the next version."
            }
        },
        "id": "l0JmSQ74KE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1UCopEeGz7",
        "replyto": "1UCopEeGz7",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission332/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707484967,
        "cdate": 1696707484967,
        "tmdate": 1701465395096,
        "mdate": 1701465395096,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main concern raised by the reviewers is the lack of novelty. While Adapter and LoRA results for the financial domain are new and provide a data point for PEFT vs full-finetuning, the paper does not contain new ideas.\n\nThe soundness of the paper is mixed: some reviewers mention some specific problems in the methodology, and the authors highlight that they do a full grid search to find the right hyperparameter, thus providing a strong methodology. Some other issues remain partially unaddressed such as the reproduction of a core result. All in all, this is a borderline paper that could benefit from further revision and review. In this state, I would only recommend acceptance to findings if there is there are enough slots in the proceeding."
            }
        },
        "id": "nNfRl5kviX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1Sn1dpNaP3",
        "replyto": "1Sn1dpNaP3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1798/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707527467,
        "cdate": 1696707527467,
        "tmdate": 1701465442126,
        "mdate": 1701465442126,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper presents a comprehensive analysis of recent approaches for augmenting pre-trained language models with retrievers (kNN-LM, REALM, DPR + FiD, Contriever + ATLAS, and FLAN-T5) and why they fail in reasoning (EntailmentBank and StrategyQA). The main findings are: \n1. the existing similarity scores are inadequate  in retrieving adequate evidence for reasoning, \n2. LLMs often fail to perform reasoning even with the golden evidence, and \n3. with imperfect retrievers, their performances are degraded.\n4. Larger LLMs have  better QA and Multi-step retrieval performances.\n\nStrength:\n1. The paper is well-written and well-executed.\n2. The authors thoroughly analyze the issues in each component and provide insightful findings with their examples.\n\nWeakness:\n1. Lack of novel findings \n2. all of the models are only evaluated without any fine-tuning. Although it is good to know the zero-shot performance, knowing their limitation after fine-tuning would be much more helpful."
            }
        },
        "id": "RfaxiYKeVG",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1RVUxlrFJZ",
        "replyto": "1RVUxlrFJZ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4488/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707590926,
        "cdate": 1696707590926,
        "tmdate": 1701465532930,
        "mdate": 1701465532930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper explores using a sandwich architecture around BERT (a BERTwich) to improve performance for non-standard text. Experiments are performed in English and Swiss German to validate performance, and ablation studies are performed to determine the importance of the two halves of the sandwich.\n\nThe discussion period was very active, and while there are some comparison experiments reported in the discussion that perhaps should have been in the original paper, the reviewers generally found the work sound (scores of 3/3/4, note that reviewer FYgr said they would increase their score to 4 but could not successfully edit it, so I reflect that here). Excitement for the approach varied more (2/3/4), especially given the strong performance of prior work on this task when evaluated appropriately (see next paragraph).\n\nThe authors reported in the discussion period that there may not be an advantage to this approach over the better-known LoRA. However, I do believe there is still a benefit to publishing this work as it provides an alternative approach that in future experiments or different settings may provide an advantage.\n\nWhen revising the paper, the authors must add the experiments described in the discussion, and it would be very beneficial to try to find any benefit to this approach over LoRA, either quantitatively or qualitatively. The authors may want to consider whether the focus of the paper should be on methods for success in the nonstandard text tasks they explore (exploring both LoRA and sandwich approaches with equal interest) or whether they still want to focus primarily on the sandwich approach."
            }
        },
        "id": "DzcAcGgop7",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1PXPP9Gzgc",
        "replyto": "1PXPP9Gzgc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4704/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707596535,
        "cdate": 1696707596535,
        "tmdate": 1701465538837,
        "mdate": 1701465538837,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper addresses the issue of distinguishing the \"Universum class\" from interest classes in classification tasks. It introduces a closed-boundary classification method with a rule-based probability estimation approach to classify the Universum and target classes. The proposed COOLU method outperforms several benchmarks like BS, A-GCN, TaMM, AC-MIMLLN, SpanNER, and SCAPT. The approach handles the miscellaneous class effectively, offering approximately a 1% improvement in F1/accuracy measures in NER, relation extraction, and sentiment analysis experiments.\n\nThe problem addressed is important in many real-world scenarios, and the results presented in the paper are promising. However, the main issue with the work lies in its clarity, as indicated in the discussion, which required several additional details to fully appreciate the paper and this research."
            }
        },
        "id": "1FWtkdjtF5",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1N5Ia3KLX8",
        "replyto": "1N5Ia3KLX8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3691/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707567794,
        "cdate": 1696707567794,
        "tmdate": 1701465506800,
        "mdate": 1701465506800,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This study introduces a dataset comprising 4,043 memes labeled as either abusive or non-abusive. Additionally, benchmark results using various models are provided.\n\nReviewers zToY and UVRc both expressed appreciation for the dataset's contribution. Reviewer xN2X raised concerns about the multitask results, which the authors addressed in their rebuttal. The issue of \"comparative analysis with existing benchmarks\" raised by Reviewer UVRc was also tackled during the rebuttal.\n\nTo enhance the paper, please ensure all feedback from the reviewers is addressed."
            }
        },
        "id": "jgr1TKntN0",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1IRFq6qdke",
        "replyto": "1IRFq6qdke",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1940/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707530411,
        "cdate": 1696707530411,
        "tmdate": 1701465447777,
        "mdate": 1701465447777,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces a model for generating synthetic eye movements on text using a discrete diffusion model with adjustments that surpass human performance. While the primary contribution is methodological, the experimental evidence supports the paper's main claim. This work is anticipated to have broad applications across various disciplines due to the challenge of obtaining real gaze data. Synthetic eye movements can benefit cognitive research in areas like readability, grammaticality, sentiment analysis, and more. The improvements over previous state-of-the-art methods are substantial, potentially establishing this approach as a new standard for the task. Both AC and 3 reviewers acknowledged the contribution of this paper. \n\nHowever, one notable concern is the absence of qualitative comparisons with previous methods. Partly due to the novelty of this research. While the paper demonstrates the superior performance of the ScanDL model, it lacks qualitative insights into how it differs from earlier techniques that also employ diffusion models. Understanding the strengths and differences in performance compared to human eye movements could enhance the paper's arguments for adopting a diffusion model for this task.\n\nIn summary, the paper's methodological contribution is significant, establishing a new state-of-the-art in generating human-like eye movements. The model's performance is well-supported by experiments, and the paper is well-written with clear motivation and interesting analysis. Additionally, the psycholinguistic analysis adds valuable insights into the alignment of the model's outputs with human reading behavior. In my mind, the paper could benefit from further exploration of the diversity of readers in the training set and a clearer explanation of how scanpaths are handled during inference. Despite some minor concerns exists, the detailed rebuttal addressed most of the problems raised by reviewers.\n\nConsider the original paper, the reviewer’s comments, and the rebuttal, as the area chair, would like to to recommend acceptance of the paper and invite the author to take an oral presentation in EMNLP."
            }
        },
        "id": "UY0hE5xq4m",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1CaBi9kEng",
        "replyto": "1CaBi9kEng",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1868/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707529065,
        "cdate": 1696707529065,
        "tmdate": 1701465445295,
        "mdate": 1701465445295,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces a method for injecting human values into large language models (LLMs) and focuses on predicting opinions and behaviors. The reviewers find the paper well-written (uZ69, FesA) and the topic of research exciting and timely topics (uZ69). Most reviewers acknowledge the novelty and interest in the paper's idea of injecting human values into LLMs and its potential societal impact. Reviewers vyUS and FesA commend the extensive experiments conducted to demonstrate the method's efficacy. Reviewer uZ69 appreciates the well-written and clear presentation of the paper and the sound experimental setting.\n\nThe reviewers have highlighted some concerns and suggestions. Reviewer vyUS raises concerns about the performance of the VILLAMAag model compared to the baseline and suggests a more in-depth analysis of the labeling results. They also recommend ethical discussions regarding potential societal harm. Reviewer FesA notes that the paper's novelty is limited, mainly relying on fine-tuning, and suggests the need for proposing novel techniques. Reviewer 1Yni suggests refining the expression of the contributions, conducting significance tests to verify further the effectiveness of the Value Injection Method (VIM), and including hyper-parameter details for LLMs. \n\nIn summary, the reviewers collectively acknowledge the potential of the paper's approach. To further improve the article, address concerns related to performance analysis and include more experimental details to help future paper readers. Also, add a discussion about the ethical implications of the proposed approach."
            }
        },
        "id": "DYhLATbaZc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "1BMj6opwbj",
        "replyto": "1BMj6opwbj",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1289/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707509466,
        "cdate": 1696707509466,
        "tmdate": 1701465426374,
        "mdate": 1701465426374,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "There is a perfect agreement among reviewers about the soundness and excitement of this paper. Some reasons to reject do not have to do with soundness or excitement about the paper (research question is too niche, neglect of cinematic elements, literature on alignment, …), but may be explored in future work.\n\nPaper Topic And Main Contributions:\n\nThis paper introduces a novel approach to analyzing the alignment between literary works and their film adaptations by employing the Smith-Waterman sequence alignment algorithm and sentence-BERT-based similarity measures. It presents three datasets, including two with manual annotations, and evaluates the alignment technique across 40 adaptations. The study delves into various aspects of the adaptation process, shedding light on patterns like narrative faithfulness, retention of book elements, dialog representation, and gender portrayal. These findings hold significant practical implications for advancing computational linguistic analysis and enhancing our understanding of the adaptation process, making it a valuable contribution to the field.\n\nReasons To Accept:\n\n* Novelty\n    * The paper addresses an underexplored area in narrative NLP, potentially catalyzing more research in this domain due to its valuable datasets and comprehensive experimental comparisons for alignment and faithfulness.\n    * The inclusion of curated datasets enriches the research community's resources for further exploration of the intricate relationship between textual sources and cinematic adaptations.\n    * The proposed alignment method demonstrates superior performance compared to established baselines, indicating the paper's potential to advance the field of NLP.\n* Quality of the Methodology\n    * The paper investigates a compelling question regarding book-film adaptation and offers interesting empirical data, contributing valuable insights to both scholarly and practical discussions in screenwriting and adaptation studies.\n    * The authors have navigated the ethical challenge of copyright while ensuring reproducibility by providing a well-considered step-by-step replication strategy.\n    * The experimental design is robust, enhancing the credibility of the presented results.\n* Quality of the writing\n    * The paper is well-written and clear, making it accessible to a broad audience.\n\nReasons To Reject:\n\n* The heavy reliance on textual similarity measures oversimplifies the complex film adaptation process, potentially leading to misconstrued assessments of adaptation quality, as it doesn't consider factors like narrative tempo, emotional subtleties, and creative choices that significantly influence film adaptations.\n* The chosen techniques for alignment and faithfulness assessment (e.g., Smith-Waterman, SBERT, GloVE, Hamming distance) may be outdated, and more recent or tailored methods might yield better results. This suggests a potential need for a more up-to-date approach to ensure the paper's findings remain relevant.\n* The provided datasets, especially the gold alignments, are relatively small, which may limit the assessment of the effectiveness of the chosen techniques. While copyright limitations and resource constraints are acknowledged, larger datasets could provide more robust and reliable results."
            }
        },
        "id": "zyuRQ35REC",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "19uudhc1s8",
        "replyto": "19uudhc1s8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission801/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496259,
        "cdate": 1696707496259,
        "tmdate": 1701465410900,
        "mdate": 1701465410900,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents a U-shaped scaling curve that identifies the \"inverse scaling\" behavior may not hold when the model continues to scale. Authors hypothesizes that medium-sized models are confused by task distractors, e.g. repetitive patterns, famous quotes, etc. In addition, authors find that one-shot prompts/chain-of-thought can mitigate these distractors. The findings in this paper are very interesting and experiments are well-executed."
            }
        },
        "id": "YFCNrS0rkb",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "19sGqVUxQw",
        "replyto": "19sGqVUxQw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3146/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707556988,
        "cdate": 1696707556988,
        "tmdate": 1701465488402,
        "mdate": 1701465488402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper investigates the Nearest Neighbor Machine Translation (kNN-MT) method, which has shown success in domain adaptation tasks. The authors aim to provide a theoretical understanding of why kNN-MT works and compare it to entire-model fine-tuning through empirical experiments. \n\nPros:\n\n* Novel Perspective: The paper introduces a novel perspective by describing kNN-MT as a specific case of model fine-tuning, shedding light on the underlying mechanisms of kNN-MT. This theoretical insight adds depth to the understanding of the method.\n\t\t\n* Detailed Analysis: Reviewer 1 appreciates the detailed mathematical explanation of the methods, and Reviewer 3 commends the authors for providing new insights, conducting solid experiments, and offering a comprehensive analysis. The paper's in-depth analysis contributes to its strength.\n\t\t\n* Empirical Results: The paper includes extensive empirical experiments, comparing kNN-MT and entire-model fine-tuning. The results are well-presented and provide valuable insights into the performance of these methods in different domains.\n\t\t\n* Word-Level Analysis: The paper goes further by conducting word-level analysis, focusing on the recall of in-domain low-frequency words. This adds granularity to the evaluation and highlights the limitations of kNN-MT, which can be addressed with additional adapter layers.\n\n\nCons:\n\t\t\n* Limited Comparison: Reviewer 1 suggests that the authors could have compared their proposed approach with more recent methods, which could provide a more comprehensive understanding of its strengths and weaknesses.\n\t\t\n* Performance Trade-offs: Reviewer 2 notes that while the proposed approach with adapters improves performance, it comes at the cost of slower inference speed and the need for adapter fine-tuning, which are important practical considerations that should be mentioned.\n\n\nIn summary, the paper offers valuable insights into kNN-MT, providing a novel perspective on its working mechanism and conducting thorough empirical and word-level analysis. The paper's strengths lie in its detailed analysis and experimental results. Addressing the suggestions regarding further comparisons and practical considerations could enhance the paper's completeness."
            }
        },
        "id": "4rGNXPjutw",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "18skb5S2Gv",
        "replyto": "18skb5S2Gv",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1607/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707519689,
        "cdate": 1696707519689,
        "tmdate": 1701465435984,
        "mdate": 1701465435984,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents simple yet efficient methods that can be used to compare word usage across different contexts (historical, native vs. non-native etc.). The methodology is sound and uncomplicated and the results are compelling. \n\nWork that identifies simple techniques that allow us to capture significant linguistic properties and better understand the structure of human language will make a useful contribution to this track."
            }
        },
        "id": "DUt52Hi0KL",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "16ZOs6YPDT",
        "replyto": "16ZOs6YPDT",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission47/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477784,
        "cdate": 1696707477784,
        "tmdate": 1701465385245,
        "mdate": 1701465385245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper addresses the problem of making LLMs understand 2d molecular structure They propose MoICA which contains (i) a cross-modal projector to connect a graph encoder's representation space and an LM's text space and (ii) a uni-modal adapter to help the LLM to adapt to downstream tasks. They show that MolCA significantly outperforms existing methods on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval.\n\nI found this to be a very interesting paper (mainly because it deals with very unique modalities). The reviewers are very positive about this work and have not raised any serious concerns. They agree that the approach is novel and the evaluation is comprehensive. There was a concern about the comparison with related word was not directly comparable but this has been addressed in the response.\n\nOverall, the paper is sound and exciting. I request the authors to include the additional experiments and clarifications provided during the rebuttal into the final version of the paper."
            }
        },
        "id": "4xQFtPBCRK",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "14WRhMNq7H",
        "replyto": "14WRhMNq7H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3595/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707565857,
        "cdate": 1696707565857,
        "tmdate": 1701465503053,
        "mdate": 1701465503053,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper introduces an efficient debiasing method at the prediction time, evaluating it on the Emotion Recognition in Conversations (ERC) task. The reviewers are all excited about the originality and usefulness of this method, and the reported results in this paper. Some parts of the paper need further rewriting and improvement and the reviewers must work on that."
            }
        },
        "id": "3v1GNv9RVn",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "10iYooV68H",
        "replyto": "10iYooV68H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3239/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558835,
        "cdate": 1696707558835,
        "tmdate": 1701465491477,
        "mdate": 1701465491477,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "# Meta Review\n\nThis paper revisits the entropy rate constancy hypothesis, by Genzel and Charniak (2002), investigating it using modern language models.\nThey use autoregressive language models (3-grams and GPTs) to compute the average surprisal of words in a document for several document positions; they do this under a number of experimental conditions.\nThey then analyse whether those average surprisal values follow a constant, decreasing, or increasing trend (mostly in a qualitative manner).\n\nThe reviewers agree this is an important research question to investigate.\nThey also agree that this paper’s detailed analyses—including a number of models, datasets, and preprocessing steps—make a significant contribution which might spark debates and future work.\nThe reviewers, however, also point out that the paper lacks a deeper analysis of its results, which makes some of its contributions slightly unclear.\nOverall, I agree with the reviewers that this work is interesting and that its analyses might spark future debate.\nI also agree with the reviewers that a \"deeper dive\" into some of its results would strengthen the paper.\n\n# Typos and Other Comments\n\nLine 46: I believe that Zipf only observed that frequent words were shorter, and that Piantadosi et al. observed that contextually predictable sentences are shorter.\n\nEq 9: I think these should be log-probabilities, instead of probabilities.\n\nLine 264: “per-word probability across”. Again, should this be log-probability instead of probability?\n\nEq 10: I think this is missing a $\\frac{1}{|W|}$, right?\n\nRelated Work Section: I think briefly mentioning the similarities/differences between this work and Xu and Reitter (2018) here could be useful.\n\nLine 460: “our work also provides the first analysis of entropy rate constancy in a language besides English”. See Genzel and Charniak (2003).\n\n# References\n\nGenzel and Charniak. 2003. Variation of Entropy and Parse Trees of Sentences as a Function of the Sentence Number."
            }
        },
        "id": "ke4FOlP5St",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "106xRbVC4k",
        "replyto": "106xRbVC4k",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4210/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585467,
        "cdate": 1696707585467,
        "tmdate": 1701465524294,
        "mdate": 1701465524294,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes a calibration method on a seq2seq model for fine-grained entity typing, which can give label confidence during inference time after the model is trained to generate fine-grained types. The main idea is to transform conditional log-probability into a calibrated confidence value, which shows promising results in experiments. Although some reviewers are concerned about the novelty and contribution of this paper, the paper is well-organized and clearly stated."
            }
        },
        "id": "KAbz4K9lfo",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0u3O7Ju21x",
        "replyto": "0u3O7Ju21x",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4860/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707599948,
        "cdate": 1696707599948,
        "tmdate": 1701465543538,
        "mdate": 1701465543538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes joint training and decoding strategies for Propbank and VerbNet-based semantic role labeling through the incorporation of SemLink. Experiments show SRL predictions to be on par or better than previous approaches. \n\nSoundness:\nAll reviewers agree on their \"soundness\" score of 4, i.e. that the presented experiments are comprehensive, the discussion is informative, and the presented results support the conclusions of the paper. This points to the potential of using two different SRL formalisms to support each other in decoding. \n\nHowever, it is also pointed out that some improvements are rather marginal. While Tables 1 and 2 present experiments averaged over three runs, they do not present a standard deviation score. Some of the bolded results in these tables are most certainly not statistically significant (for instance WSJ Joint decoding in Table 1 for PB increasing from 89.10 to 89.12). Still, as authors point out in their response, depending on the dataset and SRL type, some improvements are larger. \n\nAnother challenge to soundness is that only a single dataset is considered that only covers 56% of the CoNLL05 predicates. The authors discuss in their response how this might bias the results. \n\nExcitement: \nAll reviewers agree on an \"excitement\" score of 3. The main challenge to excitement is the narrow focus of the paper on two particular SRL formalisms linked by SemLink, the focus on a single dataset with derived VN predicates, and the moderate improvements in decoding scores. The authors argue in their response that their results in principle may transfer to other SRL resources like FrameNet, but no supporting experiments are presented in the paper. \n\nClarity:\nReviewer agree that the paper is interesting to read, but could be improved for readers who are not familiar with details on the formalisms (more examples, better explanation of SemLink, concepts like \"SEML(u)\", etc.) The reviewers provide some clarification in their response. Should the paper be accepted, I would recommend clarifying these aspects (and perhaps even working with a Figure to better illustrate the overlap between the two formalisms.)"
            }
        },
        "id": "RuMQz9Fzea",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0tEed0ZiFX",
        "replyto": "0tEed0ZiFX",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission24/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707477082,
        "cdate": 1696707477082,
        "tmdate": 1701465384461,
        "mdate": 1701465384461,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper creates a large human-annotated dataset and models for \"question-answer database retrieval\", where given a question, retrieval takes place of the relevant answers from a pre-existing database with question-answer pairs. \n\nPros:\n- Introduces a valuable, solidly annotated data set that will be helpful for research in question-answer retrieval.\n- The data set doesn't only allow for comparing new questions with a set of existing questions, but also for looking at the answers to those pre-existing questions in order to determine whether a given pre-existing (question+answer) pair could be relevant to the new question.\n- Dataset and newly trained models are released.\n\nCons:\n- Presentation could stand to be improved somewhat; details around the experiments aren't as clear as the other sections. \n- Some inter-annotator measures are missing.\nAs per the review thread, authors would be addressing these in the camera-ready."
            }
        },
        "id": "2fscAQLnKM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0sDieI5GJh",
        "replyto": "0sDieI5GJh",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4236/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707585872,
        "cdate": 1696707585872,
        "tmdate": 1701465524953,
        "mdate": 1701465524953,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools.\nThis paper introduces Self-ICL---a simple framework which bootstraps LMs' intrinsic capabilities to perform zero-shot ICL. The paper presents the first attempt for true zero-shot ICL that does not require any external data from real distribution or pre-defined label sets.\n\nPros:\n\nWell motivated problem, relates to an interesting and relevant topic\n\nSimple intuitive solution of Using the generated pseudo-inputs to improve zero-shot ICL performance\n\nThe paper presents the first attempt for true zero-shot ICL that does not require any external data from real distribution or pre-defined label sets.\n\nCons:\n\nGeneralizability of their solution. The paper only uses InstructGPT. The paper may also evaluate the proposed idea on some other models such as LLaMA, Falcon, Flan-T5\n\nNeeds to also compare with the methods in Table 1 (AutoCOT, Z_ICL, SG_ICL)  when benchmarking in Table 2\n\nMore analysis/ablations on the robustness of the method - e.g. if LLM generates incorrect few shot examples etc"
            }
        },
        "id": "6xHlxNxMjE",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0n92zm014A",
        "replyto": "0n92zm014A",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3894/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578612,
        "cdate": 1696707578612,
        "tmdate": 1701465513041,
        "mdate": 1701465513041,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes a new representation learning framework for closed-domain tasks, wherein representations are trained to capture implicit graph structure within the modelled data. The framework is shown to lead to improved performance on named entity modelling on two benchmarks. \n\nBeyond simply demonstrating state of the art results, the paper is also interesting as the results may lead to further discussion of what knowledge structures entity embeddings encode. The paper is well-written and clear, and as such provides a good starting point for such a discussion."
            }
        },
        "id": "ExdJGoGNKX",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0lE7w8RJDw",
        "replyto": "0lE7w8RJDw",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2453/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707542540,
        "cdate": 1696707542540,
        "tmdate": 1701465465650,
        "mdate": 1701465465650,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This is a survey paper on factual knowledge probing for pretrained language models. It categorizes and reviews probing methods and datasets. It also discusses knowledge retention and prompt optimization in further detail.\n\nThe paper is fairly comprehensive in reviewing past work in an important and well-studied area. However, reviewers have pointed out that it is quite similar to other recent surveys. There are indeed differences between this paper and each of the individual previous papers especially in terms of scope and depth. However, this paper provides incremental insights over recent surveys taken together."
            }
        },
        "id": "BmCErLyxRz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0kseDcA5Nm",
        "replyto": "0kseDcA5Nm",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2635/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707546340,
        "cdate": 1696707546340,
        "tmdate": 1701465471527,
        "mdate": 1701465471527,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper presents a method for generating consistent NER labels across documents using the intuition that entities that share the same surface form are very likely to have the same NER label.\n\nAll three reviewers recognise the soundness of the approach and highlight the evaluation on multiple domains as an important contribution. Equally, all three reviewers are satisfied that the experiments demonstrate the increased performance of the proposed solution. \n\nWhile any remaining technical issues raised by Reviewers 3NNy and G1FQ seem to have been addressed by the authors, the general consensus in the discussion was that the paper was rather incremental in nature which kept the reviewers from being more excited about it.  As the discussion with Reviewer G1FQ suggests, including some of the material currently found in the appendices would make this paper this paper more robust and the authors have proposed a plan to do so already."
            }
        },
        "id": "UQNOKhTo1z",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0juZSwZLA4",
        "replyto": "0juZSwZLA4",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3448/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707563015,
        "cdate": 1696707563015,
        "tmdate": 1701465497948,
        "mdate": 1701465497948,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors used ChatGPT to create new data both with paraphrasing and with zero-shot generation, and compare it to seven other algorithms.\nThe 2 out of 3 reviewers selected \"Good\" for soundness and reproducibility seem to have a problem."
            }
        },
        "id": "0GTPLbXLnA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0isMLQIUpQ",
        "replyto": "0isMLQIUpQ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission538/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707490217,
        "cdate": 1696707490217,
        "tmdate": 1701465402601,
        "mdate": 1701465402601,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The main contributions of this paper are porting subword-level policies for Simultaneous Machine Translation to the word level and proposing fusion with a language model.\n\nAltogether the reviewers agree that the paper is technically sound. There are some concerns about the presentation, and one reviewer believes that it will have limited impact due to the main idea being trivial. \nHowever, given that (some) recent previous work has failed to explore this path, I believe that this only strengthens the merit of publishing the paper."
            }
        },
        "id": "28fkHWOIGW",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0ii51brFyn",
        "replyto": "0ii51brFyn",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission65/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707478243,
        "cdate": 1696707478243,
        "tmdate": 1701465385926,
        "mdate": 1701465385926,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "All reviewers agreed that the proposed approach for few-shot NER is sound and useful. The results on two benchmarks show clear benefits, some surpassing current results in the existing literature.\n\nHowever, there are some raised legitimate concerns.\nIn particular, reviewers are concerned about the lack of clarity in the explanation of the causal model and details on the model component (e.g., sample reweighting component), some of which are partially addressed during the rebuttal phase. It is suggested the authors provide explicit connections to existing causal intervention approaches and use more case studies and visualizations to illustrate their approach.\n\nRegarding presentation, the reviewers pointed out several typos, grammatical errors, and inconsistencies in the paper that need to be addressed."
            }
        },
        "id": "REwwu3aKvM",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0iRgUfkwp3",
        "replyto": "0iRgUfkwp3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3714/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707568417,
        "cdate": 1696707568417,
        "tmdate": 1701465507387,
        "mdate": 1701465507387,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This submission studies news recommendation, which is an important application of text mining and text retrieval. This submission focuses on dual-encoder architecture and aims to address its two challenges: irrelevant word distraction and weak dual-encoder interaction. Two components are proposed to address these two challenges. However, this submission lacks discussion and comparison with recent work and needs further experiments on datasets beyond MIND such as Addressa (its textual content can be obtained by request)."
            }
        },
        "id": "jgmklSg7os",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0hyn6MJmnP",
        "replyto": "0hyn6MJmnP",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3041/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707554964,
        "cdate": 1696707554964,
        "tmdate": 1701465485477,
        "mdate": 1701465485477,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "All the reviewers rated the paper high on soundness as well as excitement. The paper was commended for well-thought motivation and being clearly written. All reviewers highlighted the potential impact of MQuaLE, a dataset of multihop questions collected by the authors to test the capabilities of knowledge editing techniques. \n\nReviewers acknowledged the significance of the findings reported in the paper. For instance, the experiments in the paper demonstrate that existing methods of knowledge editing might be able to recall individual edited facts but are unable to successfully use these updated facts to answer multi-hop questions using them. The paper also proposes MeLLo, a new memory-based approach to store all the facts externally, that outperforms past knowledge editing methods on MQuaLE.\n\nThere were a couple of concerns raised by the reviewers. For instance, the setup might be too simplistic and amenable to the retriever used in MeLLO. Another concern was the absence of prompt templates for the main experiments. Authors should consider addressing such questions in future versions of the paper."
            }
        },
        "id": "9VPOZVXi7F",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0hTPJBnncc",
        "replyto": "0hTPJBnncc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2205/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707536938,
        "cdate": 1696707536938,
        "tmdate": 1701465457813,
        "mdate": 1701465457813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The work presents a dataset of Change My View (CMV) subreddit text annotated with argumentative discourse unit (ADU) typology by Morio et al. (2019). It argues that the ordering of the ADUs affect the persuasiveness and supports its claim with experimental results on determining whether a user has changed their view given a brand of comments.\n\nReviewers appreciated the study of ADU type arrangement patterns and strategies, leading to insights allowing tangible gains in persuasiveness prediction experiments. They also thought a new CMV dataset is good update to the existing one published in 2016.\n\nA primary reviewer concern was the lack of experiment results using the latest models. While the authors provide additional results using LLaMA-2 (7B) in the rebuttal, I agree with the reviewers, and in fact the authors, that reframing the paper to focus on the analysis of argument arrangement will lead to a bigger impact in the community. Also, this work adopts the ADU typology from Morio et al. (2019). However, the typology is a minimally modified version of Park et al. (2015)’s typology. Consequently, the authors cite it, as well, for proper attribution."
            }
        },
        "id": "kEEGLLRoJA",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0eWQVWvPgu",
        "replyto": "0eWQVWvPgu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5366/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707611661,
        "cdate": 1696707611661,
        "tmdate": 1701465557450,
        "mdate": 1701465557450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes to incorporate background knowledge into stance detection models, which is retrieved from Wikipedia and by prompting ChatGPT. To increase reproducability, the authors have additionally added ablation results with the open-source LLM LLaMA-2 in their rebuttal. They perform experiments on several stance benchmarks and for different levels of supervision, and show empirically that their proposed setting works well. The reviewers agree that the paper is well executed, though the approach is relatively pedestrian, as it is in line with a lot of contemporary work on prompting ChatGPT."
            }
        },
        "id": "FDdxEQEDB1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0duz9dhwRc",
        "replyto": "0duz9dhwRc",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3244/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707558986,
        "cdate": 1696707558986,
        "tmdate": 1701465491747,
        "mdate": 1701465491747,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper introduces a novel human evaluation framework, FFAEVAL, to compare the performance of various open-domain dialogue systems. Recognizing the limitations of automatic evaluation metrics and the time-consuming nature of traditional human evaluations, the authors propose a Free-For-All Ranking method that allows evaluators to conduct single-blind, multi-turn dialogues with multiple dialogue systems simultaneously. The paper offers a valuable contribution to designing effective and efficient evaluation methods for dialogue systems."
            }
        },
        "id": "vKxOBh7Oau",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0bderX6zwr",
        "replyto": "0bderX6zwr",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5673/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707616585,
        "cdate": 1696707616585,
        "tmdate": 1701465564792,
        "mdate": 1701465564792,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper contributes a conversational machine reading comprehension dataset in Chinese. The performance of models, both commercial (in zero-shot and few shot ICL setting) and open (few and many shot finetuned) was evaluated on the dataset, showing room for improvement. The reviewers all believe (and I agree) that the resource is valuable, and that the task setting is realistic and challenging: conversations grounded in multiple documents, and answers not necessarily being spans in the input contexts. The authors provided additional results and analysis to some of the concerns raised by the reviewers, particularly prompt sensitivity and error analysis of ChatGPT on this dataset."
            }
        },
        "id": "qb2VmwpeK9",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0b2chPXfVG",
        "replyto": "0b2chPXfVG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2617/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707545970,
        "cdate": 1696707545970,
        "tmdate": 1701465470901,
        "mdate": 1701465470901,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposed a unified method to address a number of tasks involving entities and relations. The main contribution is the elegance of the model, paired with very positive results in some scenarios. However, the generalizability of the method is not fully proved as demonstrated by mixed results in some scenarios."
            }
        },
        "id": "FCKOuMcqLI",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0aiFUPYan3",
        "replyto": "0aiFUPYan3",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1188/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707506960,
        "cdate": 1696707506960,
        "tmdate": 1701465423129,
        "mdate": 1701465423129,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "In this paper, the authors address the challenge of extracting information from documents containing visual and textual elements. They focus specifically on the task of identifying entities in form-like documents that have a rich visual layout. The authors argue that existing multimodal information extraction systems, such as LayoutLM, have limitations in effectively utilizing fine-grained visual features. To overcome this limitation, they propose a new objective named visually asymmetric consistency learning (VANCL), which can be used in conjunction with any multimodal encoder backbone. The experimental results have shown the effectiveness of the proposed method. Although some reviewers have raised some concerns about some statements in the experiment, all of them generally agree with the effectiveness of the proposed method."
            }
        },
        "id": "nkdgtpmQZU",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0W2aSP6y3x",
        "replyto": "0W2aSP6y3x",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3432/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707562593,
        "cdate": 1696707562593,
        "tmdate": 1701465497368,
        "mdate": 1701465497368,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This short paper introduces a dataset of dialogues which are annotated as to whether they adhere to social norms. The dataset contains English and Chinese dialogues. The dialogues are synthetic - they were generated using ChatGPT and then annotated. The authors also evaluate whether ChatGPT can recognize norm violations and find that it does not do very well on that task. \nThe reviewers appreciate the cross-cultural social norms challenge that this dataset can help to address. \nThe reviewers also point out some shortcomings with respect to missing citations, unsupported claims on CoT reasoning effects and unsupported claims regarding the quality of the proposed dataset. However, the authors convincingly address these points in their rebuttal, which should flow into a final version of this paper."
            }
        },
        "id": "wXPscQzFYz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0VQImEvjPJ",
        "replyto": "0VQImEvjPJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3912/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707578974,
        "cdate": 1696707578974,
        "tmdate": 1701465513876,
        "mdate": 1701465513876,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper proposes ClimateBERT-NetZero, a natural language classifier for detecting corporate and national net zero and emission reduction targets from the text. The key contribution is a new expert-annotated dataset of 3.5K text samples categorized as NetZero, Reduction, or NoTarget claims. Authors have provided experiments which demonstrate ClimateBERT outperforms baselines when fine-tuned on this data. Additionally, they provided two use cases highlighting the potential to extract and analyze climate commitments at scale.\n\nThe motivation to assess ambiguous sustainability claims is timely and important. Reviewers praise the novelty of the task and release of models/data but raise issues about dataset details and reproducibility. In particular, Reviewer 2 gave very comprehensive feedback on framing the work and recycled interventions. In response, the authors sufficiently clarify the annotation process, data statistics, and human-in-the-loop mechanisms. Additional analyses also confirm model performance on real-world transcripts.\n\nPersonally, I think that the task is technically trivial, however, the significance and impact of the studies are meritorious. Moreover, I agree with the reviewers that the paper is fundamentally sound even though I am of the opinion that the paper would benefit from more discussion situating the work amidst greenwashing typologies and nuances of claim verification as also implied by one of the reviewers. Also, the current heavy focus on introducing the dataset leaves the overall contributions feeling somewhat incremental. Providing sample texts and better highlighting incremental contributions over prior climate NLP would strengthen the manuscript.  Finally, more robust framing, analysis, and comparisons to prior climate NLP could strengthen the clarity and impact."
            }
        },
        "id": "sKZpQWzfwx",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0ULLuIRdcu",
        "replyto": "0ULLuIRdcu",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1027/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707502971,
        "cdate": 1696707502971,
        "tmdate": 1701465418171,
        "mdate": 1701465418171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "# Overview\n\nThis paper investigates the functional form of the surprisal–RT relationship in seven languages. Specifically, it uses likelihood ratio tests to compare linear mixed effect models with: only baseline predictors, baseline + surprisal, baseline + surprisal + surprisal squared. It finds that surprisal helps predict RTs in the 7 studied languages. It also finds that surprisal squared further helps predict RTs in 3 of these languages.\n\n# Meta-review\n\nThe three reviewers and I agree this paper tackles an important question, and that its cross-linguistic contribution is important and timely. Further, its methods and results are clearly presented.\n\nThe reviewers, however, point out two important limitations in this paper. First, the used crosslinguistic corpora differ substantially in size, genre, and used eyetracking measure; this limits the extent to which conclusions can be drawn. Second, the analysis did not control for frequency effects. (I highlight two more methodological issues below.) \n\nThe authors already promised to run an extra analysis with frequency effects for CR (if accepted), and to de-emphasize any claim that language is the main factor influencing the shape of the surprisal effect. I would also suggest they extend the \"Eyetracking corpora\" paragraph in their limitations section to highlight the issues pointed out by reviewer 1TwT.\n\n# Detailed Comments\n\nWhile I enjoyed reading this paper, and mostly agree with the three reviewers about its positive aspects, I highlight a few methodological issues below. I believe fixing those could help strengthen this paper.\n\n* **Spillover effects**: While one of the reviewers highlights the way in which this effect was computed as a reason to accept, I think it's actually negative. Specifically, the authors seem to choose spillover effect windows based on likelihood ratio tests using models which only had access to word length. If a spillover effect is significant for surprisal, but not for word length, it will thus not be included in the model. This might have impacted the analysis on Japanese, which at the moment uses a spillover window of 0. There seems to be a large enough number of tokens in most analysed languages for spillover effects to be set to zero by the model fitting procedure itself (if they are insignificant), so I would recommend rerunning experiments (at least for Japanese) with a fixed spillover effect window.\n\n* **Model training**: Models were trained for 7 epochs or until a validation loss of 3.5 nats was reached. Given that not all languages are \"equally hard\" to language-model (Cotterell et al., 2018; Mielke et al., 2019), stopping the training early (instead of training until convergence) may unfairly harm models in some languages.\n\n\n* **Frequency effects**: This paper does not control for frequency effects as a baseline feature, using Shain (2019) as a justification. A more recent work by the same author (Shain, 2023) reaches the opposite conclusion. This new paper, however, only came out *after* the EMNLP submission deadline.\n\nThe authors did already promise to add an analysis including frequency effects for camera ready, in case the paper is accepted. I would personally also appreciate seeing an analysis using constant spillover effect windows (specially for Japanese), and, if possible, models trained to convergence.\n\nAs an aside, in one of their responses, the authors state that Wilcox et al. (2023) tested on small \"sentence corpus\"-style datasets. I believe Wilcox et al. (2023) used the MECO dataset (similarly to de Varda et al. 2022; 2023), which has paragraph-level entries.\n\n# References\n\nCotterell et al. (2018). Are All Languages Equally Hard to Language-Model?\n\nMielke et al. (2019). What Kind of Language Is Hard to Language-Model?\n\nShain (2019). A large-scale study of the effects of word frequency and predictability in naturalistic reading\n\nShain (2023). Word Frequency and Predictability Dissociate in Naturalistic Reading"
            }
        },
        "id": "97YopOD2oZ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0SIyWZEOmJ",
        "replyto": "0SIyWZEOmJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2520/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543871,
        "cdate": 1696707543871,
        "tmdate": 1701465467570,
        "mdate": 1701465467570,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers agree that the paper is sound and moderately exciting. The routing approach presented is novel and yields good results. The experiments, although limited to a set of few models, are relatively extensive and include all necessary ablations. I recommend acceptance to either the main conference of findings."
            }
        },
        "id": "vnQ7kUGDCl",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0SF6Kr1lrx",
        "replyto": "0SF6Kr1lrx",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2848/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707550674,
        "cdate": 1696707550674,
        "tmdate": 1701465478517,
        "mdate": 1701465478517,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper treats black-box text attack as an unsupervised text generation problem and proposes a search and learning framework (ATGSL) with three adversarial attack methods (ATGSL-SA, ATGSL-BM, ATGSL-FUSION). ATGSL is significantly superior to the most advanced methods in terms of attack efficiency and adversarial text quality. On the other hand, the comparisons may not be fair due to uncertainty over the constraints."
            }
        },
        "id": "4NBRfTXCys",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0Rdp7a3y2H",
        "replyto": "0Rdp7a3y2H",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission575/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707491097,
        "cdate": 1696707491097,
        "tmdate": 1701465403927,
        "mdate": 1701465403927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper introduces Reinforcement Learning (RL) based models for Query-focused Summarization (QFS) inspired by RL in natural language generation. The proposed work employs rewards through ROUGE, BLEU, and Semantic Similarity. It addresses conflicts with Teacher Forcing through Scheduled Sampling. Additionally, the paper introduces a novel reward based on the Cluster Hypothesis, contributes the RPEDT dataset for passage embedding model training, and presents the RQFT dataset for QfS method analysis. The experimental results demonstrate the effectiveness of the proposed method in QFS. Please add baseline results on the new dataset for Fan et al. (2019)."
            }
        },
        "id": "h9tfoVpVrj",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0OtGfwj8eB",
        "replyto": "0OtGfwj8eB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission5329/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707610942,
        "cdate": 1696707610942,
        "tmdate": 1701465556303,
        "mdate": 1701465556303,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces in-context PVI as an efficient alternative to traditional PVI calculations, particularly beneficial for LLMs without the need for fine-tuning, and measures PVI only through prompting. The empirical analysis provides insights into the reliability and potential applications of in-context PVI. \n\nReviewers appreciate the efficiency and novelty of the in-context PVI method and the novel generalization of v-usable information from fine-tuning to in-context learning, which is seen as a valuable contribution. The comprehensive empirical analysis conducted in the study adds substantial credibility to the concept and establishes its reliability. The reviewers also express that the empirical results indicate a positive correlation between in-context PVI and accuracy, as well as inter-annotator agreement, suggesting its potential as a replacement for PVI.  \n\nHowever, the reviewers express concerns about the lack of a direct comparison between in-context PVI and traditional PVI. I agree with the suggestions by reviewers to directly measure the correlation between these two methods and report statistical significance, which would provide a more robust evaluation. I also suggest the authors to have a more in-depth discussion of the relationship between the two measures. \n\nOverall, the paper offers an innovative and efficient approach to PVI calculation and presents valuable empirical insights. I suggest authors addressing the concerns regarding method comparison and adding statistical significance for experimental results to strengthen the paper's quality."
            }
        },
        "id": "6b8Krc5Jv1",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0ODPaEbHxG",
        "replyto": "0ODPaEbHxG",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1457/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707514813,
        "cdate": 1696707514813,
        "tmdate": 1701465431468,
        "mdate": 1701465431468,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper presents an approach to mitigating bias in text classifiers by inducing Wasserstein independence between representations learned to predict target labels and sensitive attributes. The paper is well-motivated and the experiment setup is thorough. The rebuttal addresses most of concerns."
            }
        },
        "id": "FXUTCeiiAm",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0M2m9GUTLN",
        "replyto": "0M2m9GUTLN",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2804/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707549772,
        "cdate": 1696707549772,
        "tmdate": 1701465476953,
        "mdate": 1701465476953,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper introduces a multimodal SpeechGPT. The paper also creates a SpeechInstruct dataset. Reviewers see merit in these contributions, despite the architecture itself not bringing novelty."
            }
        },
        "id": "S4iJnyxi0P",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0LXEvcD3dB",
        "replyto": "0LXEvcD3dB",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission380/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707486116,
        "cdate": 1696707486116,
        "tmdate": 1701465396820,
        "mdate": 1701465396820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The reviewers express mixed views towards the paper that proposes a new prompt-based framework for taxonomy completion. \n\nCollectively, the strengths of the paper include a sound approach to the taxonomy completion task using prompt learning. Reviewers appreciated the extensive experiments on benchmark taxonomy datasets, the presentation of two innovative variants of TacoPrompt, and the solid empirical results, with significant improvement over previous methods.\n\nHowever, questions have been raised about the novelty of using the Masked Language Model (MLM) and prompts in the taxonomy completion task, as the MLM/generation formulation has been studied for other IE tasks. Also, further comparisons with other techniques is required for a comprehensive evaluation"
            }
        },
        "id": "8T7T13mMjc",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0KYSlQdMu6",
        "replyto": "0KYSlQdMu6",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2874/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707551295,
        "cdate": 1696707551295,
        "tmdate": 1701465479692,
        "mdate": 1701465479692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "The paper adapts Integrated Gradients to siamese encoders (which can be useful e.g. in tasks that learn similarity judgements for two texts). For this, the authors introduce “integrated jacobians” that summarize feature interactions between the two inputs in a matrix form.\n\nThe reviewers agreed that the method and its theoretical justification are solid. They also appreciated that the authors showed how it can be used in practice and discussed the potential weaknesses (e.g., the choice of the baseline input r, etc.) There were concerns about applicability of the method to early layers and other architectures, but those were resolved during discussion. Overall, the reviewers are positive about the paper."
            }
        },
        "id": "ff5FOviNrR",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0JepdeBcDk",
        "replyto": "0JepdeBcDk",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3838/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707577178,
        "cdate": 1696707577178,
        "tmdate": 1701465511372,
        "mdate": 1701465511372,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper propose a calibration method to mitigate the bias towards high confidence predictions. The experimental results of this paper show empirical evidence of substantial performance gains, even with just a few training samples. Overall this incremental, simple, yet effective method is a perfect fit for a shot paper. The main concern that the authors should address are the ones expressed by rev. 3ZFj on mismatch of the empirical results with some claims. It would also help to include the monolingual contributions into the title, as the multilingual focused title is misleading (rev. Pst8)."
            }
        },
        "id": "BDa7MPmysJ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0GO8Dtl8lJ",
        "replyto": "0GO8Dtl8lJ",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission4118/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707583485,
        "cdate": 1696707583485,
        "tmdate": 1701465520905,
        "mdate": 1701465520905,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper expands Word Embbedding Association Test (WEAT) by translating from English to 24 languages. Overall, this paper makes good contributions by 1) proposing a new resource for bias and fairness evaluation through multiple dimensions, 2) empirical results on pretrained masked language models on wide range of languages. During the rebuttal, multiple concerns were raised about presentation clarity (e.g., flows of the results section), but we believe this is an easy fix to make the paper stronger. In summary, the AC is looking forward to the proposed datasets to be contributed to the community."
            }
        },
        "id": "kw0yHpAIrQ",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0EQ4z8n5rp",
        "replyto": "0EQ4z8n5rp",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2145/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707535537,
        "cdate": 1696707535537,
        "tmdate": 1701465455674,
        "mdate": 1701465455674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The paper investigates dense retrieval in a zero-shot setting, with a focus on identifying factors that impact model performance. It examines four key variables: source query set, source document set, data scale, and bias in the target dataset, and provides an in-depth analysis of how these variables influence model performance. The paper also highlights the potential impact of bias on evaluation. Overall, it offers a comprehensive study of dense retrieval in a zero-shot context.\n\nThere are several strengths to this paper. It fills a gap in the research by providing a detailed and systematic analysis of dense retrieval in a zero-shot setting. The experiments conducted are robust and provide substantial evidence to support the findings. The paper is well-structured and well-written, making it accessible to the reader.\n\nHowever, there are a few areas of concern. Reviewers suggests that the paper could have proposed new training strategies based on the insights gained from the analysis, rather than solely reviewing existing methods. Additionally, given the recent surge in pre-training-based dense retrieval models, the insights provided by the paper may not be as groundbreaking as they would have been a year ago. To address this, the authors might consider offering more innovative training strategies based on their findings.\nFurthermore, there are questions raised regarding the allocation of queries from NQ and positive samples from the MSMARCO document set. This allocation could potentially result in queries without accurate answers, which should be addressed or clarified in the paper."
            }
        },
        "id": "QI1Mra9gjH",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0DyJbE93XO",
        "replyto": "0DyJbE93XO",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission1753/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707526345,
        "cdate": 1696707526345,
        "tmdate": 1701465440449,
        "mdate": 1701465440449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This submission studies the Community Question Answering (CQA) problem and adopts the pre-training technique to enhance the representation of questions and answers. In particular, this submission adopts a contrastive learning with some specific design to inject personalized preferences. Nevertheless, the technical novelty is limited and lacks experiments beyond the StackExchange platform. In addition, the performance improvement in many cases is marginal. Besides, the writing of this submission should be improved."
            }
        },
        "id": "5oWsHdvyoz",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0DkaimvWs0",
        "replyto": "0DkaimvWs0",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission793/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707496054,
        "cdate": 1696707496054,
        "tmdate": 1701465410558,
        "mdate": 1701465410558,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "This paper proposes to address named entity-related hallucinations through an adaptive margin ranking loss function and two alignment methods 1)  entity-sentence alignment and 2) entity-reference alignment to reduce halluciation in abstractive text summarization. The results seem to be promising based on BART-base model evaluated on CNNDM and XSum. \n\nPros: \n- Work touches on a very relevant and important topic of improving factuality\n- Introduces novel adaptive margin ranking loss function and alignment methods.\n- Analysis showing promising results\n- Well-documented experiment details (Appendix)\n\nCons: \n- The choice of baselines and datasets raises concerns about the generalizability of the proposed methods to other domains and larger, modern architectures.\n- The performance improvements seen are relatively small and based on a limited number of samples (100), leading to questions about the significance of the results.\n- \"Given that the authors had access to state-of-the-art compute (GPUs with 80GB memory) the choice of baselines seems insufficient\"\n\nIn general, reviewers express enthusiasm regarding the proposed methods. However, they pinpoint two significant concerns: the perceived weakness in the evaluation process and the unjustified choice of using BART-base as the backbone for the system. In its current form, while the submission retains some element of excitement, it does not meet the threshold for acceptance at the main conference. Addressing these concerns, possibly by incorporating other baseline models could potentially elevate the work to the required standard. On the other hand, I think the work meets the standard to be accepted in the findings. \n\nI also found the justification of BART-base is not fully convincing to me, even bigger models does not work out, models like T5-small could be alternatives compared to test on a single model:\n\n\"With GPUs with 80GB memory, the choice of baselines seems insufficient.\n\nThe used GPUs/80GB in this paper are shared resources. It is infeasible for us to fully utilize them for training over several days at any given time. Meanwhile, small-sized pre-trained models such as pre-trained BART-base are used and accepted for abstractive text summarization research in several recent prior works including the cited prior works in this paper. As our objectives in this paper are to address specific hallucinations instead of competing overall performance, we think that the BART-base is a reasonable choice for the purpose.\n\""
            }
        },
        "id": "dv7WWWpVHB",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "0C5C70C3n8",
        "replyto": "0C5C70C3n8",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2234/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707537590,
        "cdate": 1696707537590,
        "tmdate": 1701465458524,
        "mdate": 1701465458524,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This work analyzes the strength and limitations of knowledge graphs (knowledge tuples) compare with knowledge sequences (unstructured textual contents) for knowledge grounded dialogue.\n\nContributions:\n - The analysis provides valuable insights for building knowledge-grounded conversation applications\n\nWeakness:\n - Missing references and the paper's message is sometime hard to follow."
            }
        },
        "id": "HPHjpfRn9q",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "06oozRd4jU",
        "replyto": "06oozRd4jU",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission3047/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707555110,
        "cdate": 1696707555110,
        "tmdate": 1701465485555,
        "mdate": 1701465485555,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Findings"
            },
            "comment": {
                "value": "The authors mostly agree that the submission addresses an important paradigm in NLP, namely in-context learning (ICL), and make an interesting connection between it and active learning, they identify a phenomenon termed template bias which consists of a systematic shift in the distribution of outputs in few-shot ICL due to the template itself, and propose a calibration procedure to mitigate it, the proposed methodology is convincing (calibration and ICL selection), the information gain method demonstrates substantial performance improvements, moreover, it is efficient and forgoes the need for large annotated datasets. However, some weaknesses were also pointed out: the method is restricted to classification tasks, information gains are quantified independently for each sample, however, joint estimation may lead to additional performance gains, the description of the proposed method could use some additional details, the components of the proposed approach are either unsurprising or not new, importantly, rather than penalizing it, the reviewer suggests further discussion, and in the experiments, some baselines are missing (e.g., diversity sampling and clustering) and there is potential for bias in the results. In response, the authors argue that their goal is having a no parameter tuning scheme in favor of data efficiency, that a comprehensive discussion of competing methods will be included in the revision, they acknowledge that max entropy may not be the best strategy for selection and the potential for bias but elaborate on how their information gain framework is likely to counter such bias. Further, the authors agreed to expand the discussion of the proposed methodology in the revision and authors provide additional results for one-shot experiments. Though the reviewers engaged with the authors in the discussion period, one of the reviewers stated that their soundness sore will be increased, however, not reflected in the final score. Nevertheless, such an increase was taken into account."
            }
        },
        "id": "vXzDzHBFCT",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "05vb8rwGct",
        "replyto": "05vb8rwGct",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission2521/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707543856,
        "cdate": 1696707543856,
        "tmdate": 1701465467688,
        "mdate": 1701465467688,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "content": {
            "title": {
                "value": "Paper Decision"
            },
            "decision": {
                "value": "Accept-Main"
            },
            "comment": {
                "value": "This paper explores how the model compression impacts the models' robustness on minority subgroups. The authors present experiments with 18 models compressed by different methods or settings, on 3 datasets (the last one is added during the discussion period). \n\nAs the reviewers mentioned, the paper includes a large list of models including knowledge distillation, post-training quantization, quantization-aware training, structured pruning, and vocabulary transfer. This is comprehensive and helpful for future studies. In terms of datasets, the authors provided further context for the dataset selection during the discussion period. Furthermore, they presented another set of experiments on a new dataset addressing reviewers' feedback.\n\nHowever, also should be noted that reviewers pointed out a lack of analysis on why certain compression methods/models work better than others such as TinyBert. Although the authors attempted to address this during the discussion period, the paper would still benefit from a deeper analysis."
            }
        },
        "id": "RTWVv2OnNr",
        "signatures": [
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "EMNLP/2023/Conference",
            "EMNLP/2023/Conference/Program_Chairs"
        ],
        "forum": "01wSNY5T60",
        "replyto": "01wSNY5T60",
        "number": 1,
        "invitations": [
            "EMNLP/2023/Conference/Submission966/-/Decision",
            "EMNLP/2023/Conference/-/Edit"
        ],
        "domain": "EMNLP/2023/Conference",
        "tcdate": 1696707500449,
        "cdate": 1696707500449,
        "tmdate": 1701465416267,
        "mdate": 1701465416267,
        "license": "CC BY 4.0",
        "version": 2
    }
]