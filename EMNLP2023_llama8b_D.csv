id,Paper_Topic_and_Main_Contributions,Reasons_to_accept,Reasons_to_reject,Questions_for_the_Authors,Soundness,Excitement,Reproducibility,Ethical_Concerns,Reviewer_Confidence,Justification_for_Ethical_Concerns,Missing_References,Typos_Grammar_Style_and_Presentation_Improvements,cdate,ddate,details,domain,forum,invitations,license,mdate,nonreaders,number,odate,pdate,readers,replyto,signatures,tcdate,tmdate,writers,response
tS0GnXilHq,"This paper explores diverse techniques for approximating two-layer neural networks (NNs). To begin with, the authors introduce a comprehensive framework that unifies the Top-K activation function, Mixtures of Experts (MoEs), and product-key memories (PKMs). By thoroughly analyzing their approach, they subsequently present enhancements for both MoEs and PKMs. The empirical investigations reveal that the proposed MoEs perform competitively when compared to the dense Transformer-XL, underscoring the applicability of MoEs to language models of varying scales.","1. The exploration of approximating two-layer feedforward networks (FFNs) is both novel and captivating. Notably, the authors provide a comprehensive perspective that encompasses various methods, and they also conduct a comprehensive comparison of the most notable MoE variants.
2. The motivation behind the paper is distinctly articulated and substantiated. The seamless progression from motivation to theoretical analysis, and ultimately to the proposal of the method, is presented in a coherent and natural manner.
3. The proposed approach is effectively supported by empirical experiments, demonstrating its ability to attain performance on par with dense networks.","1. The proposed $\sigma$-MoE framework requires further justification. The specific design approach for $W_3$ and the uniform initialization of weight matrices are notable contributions. The paper should delve into the rationale behind these design choices and their benefits, particularly in terms of their impact on the performance of the MoE model.

2. The empirical investigations provided appear somewhat limited in scope. Given that all experiments are exclusively conducted on WikiText-103 and Enwik8, which share similar data distributions, it would be prudent to expand the experimental scope to include other datasets. This would provide additional support for the performance claims of the $\sigma$-MoE model.

3. While the paper introduces novel elements through the MoE variant design, the novelty level might be constrained. To enhance the clarity of the novelty introduced beyond the MoE variant design, it's advisable to provide further elaboration and illustration.

","1. It would be valuable to include additional experiments on diverse text datasets such as PTB and C4. Expanding the experimental evaluation beyond Enwik8 and WikiText-103 can provide a more comprehensive understanding of the proposed approach's performance across various text domains and scales.
2. Could you provide further clarification and elaboration on the design principles underlying the $\sigma$-MoE model? ","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1692010000000,,,EMNLP/2023/Conference,zM3mlyflTt,"['EMNLP/2023/Conference/Submission3542/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],4,,,"['everyone', 'EMNLP/2023/Conference/Submission3542/Reviewer_d9Y9']",zM3mlyflTt,['EMNLP/2023/Conference/Submission3542/Reviewer_d9Y9'],1692010000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3542/Reviewer_d9Y9']","No, this peer review does not specifically suggest that the authors refer to any new literature. The reviewer mentions some areas where the authors could improve their paper, but none of these suggestions involve referencing additional literature. Instead, they ask for clarification on certain design principles and methods used by the authors in their paper."
LzJ6Vg1zuU,The paper presents a technique for zero-shot intent classification. The authors make use of a BERT model finetuned on the NLI task and a dependency parser to discover new intents not seen before.,"Creativity: The authors present a creative pipeline that combines several components to predict new intents in a zero-shot setting.

Experiments: For many experiments, the authors show results for several different methods, comparing to a variety of LLMs.","Simplistic approach: The method presented in Algorithm 1 just extracts words from the sentence. If the intent word is not explicitly expressed in the sentence, this method will be incapable of generating the correct intent.

Lack of baseline in Table 4: The authors only present various settings for their model. I'm not familiar with this research area, so I have no idea if there are approaches in previously published work that outperform this method that the authors have left out.

Marginal improvement in Table 4: The difference in results for each approach are very small, so the benefit of the proposed method does not seem large.

Interpretability of remaining results: It's hard to compare the performance to the LLMs because they only use cosine distance. It's clear the model outperforms in semantic similarity (according to the semantic encoder models used), but for more trustworthy results, a small sample of human evaluations should be used as well to be sure that this method outperforms the LLMs in the zero-shot setting. Another option would be to modify the LLM experiment such that label F1 scores could be produced (use a verbalizer to map LLM output to intent classes).","1. What is the frequency of examples in the dataset where the intent is explicitly mentioned in the sentence? If this is almost all of the cases, then my first reason to reject is not important. If there are a lot of examples without the intent mentioned, this method is fundamentally limited compared to LLMs which can generalize better than this approach (generate an intent without the intent being mentioned explicitly).

2. Are there any baselines that you could compare to for zero-shot intent classification? If so, why didn't you include them in Table 4?

3. What is the test set for Table 4?","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"2: Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,,1691098741122,,,EMNLP/2023/Conference,zdMislOLTv,"['EMNLP/2023/Conference/Submission117/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461009534,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission117/Reviewer_ihi6']",zdMislOLTv,['EMNLP/2023/Conference/Submission117/Reviewer_ihi6'],1691098741122,1701461009534,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission117/Reviewer_ihi6']","Based on the provided peer review, it seems that the author does suggest referring to additional literature for certain aspects of the research. Specifically, they mention:

* Human evaluations should be used alongside semantic similarity metrics to ensure the proposed method outperforms LLMs (which implies referencing existing work on human evaluation methods in NLP)
* Label F1 scores could be produced by using a verbalizer to map LLM output to intent classes (which suggests referencing literature on verbalization techniques and their applications)

However, the peer review does not specifically mention referring to any new or additional literature that was not already discussed in the original paper. The reviewer's suggestions are more focused on addressing methodological issues and improving the presentation of results.

If I had to infer, I would say that the author might suggest referencing existing work on zero-shot intent classification, NLI tasks, or dependency parsing, but this is not explicitly stated."
4WJ8SxwMk5,"This work focuses on the task of detecting partisan vs counter-partisan events in news reporting. The authors aggregate 103 news stories, each with 2 news articles from opposing ideologies, end extract events from them using a RoBERTa-based model. They then annotate (a) each article as ideologically leaning towards left/right and on a Likert scale (1-5, left-right) and (b) each identified event in a news article as partisan/counter-partisan/neutral. After a qualitative analysis of their annotations, they test two RoBERTa-based models on two tasks: (A) article-level ideology prediction and (B) partisan event detection -- for (B) they also try ChatGPT. 

The paper is well-written, albeit with a few clarification points needed primarily on the annotation process (ยง3). The major contribution is the introduction of a novel dataset, which can be useful for future research in this area. My major concern though is the lack of error and qualitative analysis of the results, which is quite important for this type of work: which are the most challenging cases for the best-performing model? Which cases are more easily captured? Is there a correlation between qualitative characteristics (e.g., sentiment) and the accuracy of the models? Incorporating a section on this type of analysis would greatly benefit the paper, as it can guide future work on this task/dataset.",- The introduction of a novel dataset.,"- Lack of error and qualitative analysis of the results, which can set up the future research directions on this task.
- A few missing details/clarification points are needed, primarily in section 3 (annotation process) which is the most important.","- L134: ""We manually inspect each story [...] differ"": Please provide more details. Who inspected each story, which were the annotation guidelines/the inspectors' background, IAA, etc.

- L136: Which is the definition of ""events from TimeML""?

- L138: How was the 89.31 F1 score calculated? On which test set? Is there a performance drop expected when applied on the PARTISAN EVENTS dataset?

- L174: ""we held individual weekly meetings [...] if there was ambiguity"": could you provide an example use case in an appendix possibly? Was this part of the training or part of the actual annotation task? If the latter is the case, could this introduce bias in the annotations?

- ยง3.2: Clarify early on this section that each annotation was performed by two students and not by all of them.

- ยง3.2: You mention that the IAA on ""*stories'* relative ideology"" is 91%; but on ยง3.1, you mention that ""At the *article* level, the annotator determines the relative ideological ordering"". Do these two quoted pieces of text refer to the same annotation? If so, please be consistent on your terminology (i.e., ""story"" vs ""article level"" annotation).

- L186: ""a significant difference in their absolute ideologies"": please provide the exact absolute value used.

- ยง5: Is Task A a two- or a five-class prediction task?

- Results section: it is unclear to my why you have omitted the neutral class. Surely the other two classes are more interesting to your task, but it is very important for the reader to see the overall picture of the results.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- L240: research vs reach?
- Table 3: add ""."" at the end
- L253: ""hurts the model performance"" for Task A",1691142932140,,,EMNLP/2023/Conference,zrBrl2iQUr,"['EMNLP/2023/Conference/Submission5006/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461325906,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5006/Reviewer_wiNC']",zrBrl2iQUr,['EMNLP/2023/Conference/Submission5006/Reviewer_wiNC'],1691142932140,1701461325906,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5006/Reviewer_wiNC']","Based on the peer review, there is no clear indication that the authors need to refer to new literature that was not already discussed in the original paper. The reviewer's main concerns and suggestions focus on clarifying certain points, providing more details and explanations about the methodology and results, and improving the presentation of the paper.

However, one possible implication is that the authors may want to consult relevant literature on error analysis and qualitative evaluation of natural language processing models, as the reviewer suggests that incorporating a section on this type of analysis would greatly benefit the paper. This might involve looking at existing work on evaluating the performance of NLP models on specific tasks, such as detecting partisan or biased language.

More specifically, some potential literature that the authors may want to consult includes:

* Studies on error analysis and qualitative evaluation of NLP models (e.g., [1], [2])
* Research on detecting partisan or biased language in news articles (e.g., [3], [4])
* Work on using TimeML for event extraction (e.g., [5])

However, these are not necessarily new or novel sources that the authors need to reference; rather, they may want to consult existing work in these areas to improve their own methodology and presentation."
zV1gFrvzZ5,"The authors have developed an efficient, sparse method for correlating textual and visual data within a unified conceptual framework. The most important novelty of the paper, in my opinion, is to cluster the textual tokens into concepts, and align the visual and textual representations in that space.","Interesting idea.
Important problem.
The solution can be applied to different types of visual-textual applications, for example phishing detection on social media, recommendation systems, and etc.
Superior results.
Well written.","KNN clustering to find concepts given words, makes the approach biased towards the word embeddings of he up-stream models. It can be claimed that the success of the networks is mostly achieved by the initial words embeddings, before clustering.","1- How this method can handle rare and frequent concepts?
2- Is the cluster center selected as the representative?
3- Is there a way to take advantage of the cooccurrence of cluster words?
4- Why the Dense space similarity needed?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691222367311,,,EMNLP/2023/Conference,zeGXjQYhXz,"['EMNLP/2023/Conference/Submission4649/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461307354,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission4649/Reviewer_VnYa']",zeGXjQYhXz,['EMNLP/2023/Conference/Submission4649/Reviewer_VnYa'],1691222367311,1701461307354,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4649/Reviewer_VnYa']","No, this peer review does not explicitly suggest the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer provides constructive criticisms and questions for the authors to consider, but does not mention any specific papers or references that they should consult. However, it can be implied that the authors may need to address some of the limitations mentioned in the review by referencing relevant literature, such as word embeddings research or co-occurrence analysis."
mb7JHUgvGM,This paper introduces a method called 'Holistic Inter-Annotator Agreement' to compute the (percentage) agreement between each pair of annotators on the semantically most similar sentences they have both annotated. This allows the authors to compute what annotators are most similar to a particular annotator. This ranking is shown to correlate well with the ranking obtained using standard IAA metrics such as Cohen's K. ,"1. The paper is clearly built on a great deal of expertise about annotation, in particular in large-scale annotation projects. It raises some very good questions about the limitations of coefficients of agreement in such large-scale annotation projects, and illustrates some very good annotation practices - it could be useful to others aiming to run such a project.

2. The method itself makes a lot of sense.

3. the results obtained with the proposed method are analysed to a great depth.","1. The main problem I have with this paper is that it's not completely clear to me what is the problem that the authors are trying to solve. My understanding is that they are trying to come up with a more useful way of measuring the actual agreement between annotators not just on the few examples they both annotate, but I am not completely sure this is right. 

2. A more general problem is that others have realized that coefficients of agreement are really a limited metric - especially in case of subjective judgments - and have tried to devise more informative approaches, although nobody as far as I know has addressed the specific issue tackled in this paper. I am especially thinking of the work on probabilistic models of annotation by Passonneau and Carpenter, and their TACL 2014 paper. In that paper, the authors argue convincingly that a single number cannot be sufficient  for the in-depth  analysis of annotation that the authors of this paper have in mind. The type of analysis they propose involve building models of each annotator, and of each item, that do allow for more insightful comparisons. I would encourage the authors to have a look at these methods and perhaps think about generalizing them. ","1. Do I understand your objective correctly? Is your objective in effect to compute agreement between annotators on a larger scale, i.e., not only on the sentences they all annotate?

2. If the answer to the above is 'yes', how are you proposing to assess whether the obtained Holistic IAA value is sufficient to your purposes?","5: Excellent: This study is one of the most thorough I have seen, given its type.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Rebecca J. Passonneau and Bob Carpenter. 2014. The Benefits of a Model of Annotation. Transactions of the Association for Computational Linguistics, 2:311โ326.",,1691170000000,,,EMNLP/2023/Conference,zIgc1Qeceh,"['EMNLP/2023/Conference/Submission4608/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission4608/Reviewer_Wmuj']",zIgc1Qeceh,['EMNLP/2023/Conference/Submission4608/Reviewer_Wmuj'],1691170000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4608/Reviewer_Wmuj']","Yes, the peer review suggests that the authors should be aware of and possibly refer to the work by Rebecca J. Passonneau and Bob Carpenter (2014) on probabilistic models of annotation, particularly their TACL 2014 paper. The reviewer mentions this as a related approach to the method proposed in the paper, suggesting that it could provide more insightful comparisons and is worth considering for generalizing or building upon."
aQtWnkoUAX,"This paper analyzes the mechanism of relative positional embeddings and shows that the relative positional dependence of attention emerges due to some factors. Besides, the word embedding is also a factor that enables inference based on relative position for the attention strongly concentrated on the adjacent tokens. ",This paper introduces an interesting work on the relationship between self-attention and position embeddings. The factors proposed in the paper are based on several experiments and the results are persuasive. ,"There are some problems in this paper

1.  The relationships between different sections are not clear and authors should give a clear description of the relationship in the introduction. In the Introduction, these factors are put in Section 4. However, Section 3 also shows the factors of RPE. Besides Section 4 focuses on nearby tokens and Section 5 focuses on adjacent tokens, while adjacent tokens can be viewed as one part of nearby tokens. It is better to combine Section 4 and 5.

2. Authors should make the equation more prescriptive. Line 244, $X$a->$X_a$, $Y$b->$Y_b$. Line 329, $q$s->$q=[q_n]$, $k$s->$k=[k_n]$ ","

",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691233546126,,,EMNLP/2023/Conference,zpayaLaUhL,"['EMNLP/2023/Conference/Submission1988/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461134495,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1988/Reviewer_5Jmf']",zpayaLaUhL,['EMNLP/2023/Conference/Submission1988/Reviewer_5Jmf'],1691233546126,1701461134495,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1988/Reviewer_5Jmf']","No, this peer review does not explicitly suggest referring to any additional literature. It focuses on providing feedback on the paper's structure and content, suggesting improvements such as clarifying relationships between sections, combining certain sections, and making equations more prescriptive. The reviewer also provides an assessment of the paper's soundness, excitement, reproducibility, ethical concerns, and their confidence in the review."
tvbDgCH8Ns,The paper proposes an adapter-based multi-attribute text style transfer model with the parallel and stacked connection configurations. The authors focusing on an issue of the current PLM fine-tuning where the training examples are limited and the PLM contains a huge amount of parameters which may lead to overfitting.,"The paper reads very smoothly and is an enjoyable read. The model is explained clearly and the paper is well-structured.

The method proposed is a new application scenario extension of the Adapter framework.","The experiments only utilized one backbone model, potentially leading to an evaluation bias concerning the model's effectiveness. To address this concern, the author should investigate and assess the impact of employing various backbones on the performance of the adapters.

The human evaluation lacks specific quality details, such as the number of examples used for evaluation and the number of workers hired to conduct the evaluations. Including these details is essential to ensure transparency and replicability of the evaluation process. 

The comparison with prior works is insufficient, raising concerns about the effectiveness of the proposed model. 

Missing some related references, e.g., FGIM, Wang et al., 2019.",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation.,,1691110000000,,,EMNLP/2023/Conference,z9l6nHpTyT,"['EMNLP/2023/Conference/Submission1404/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1404/Reviewer_f2rF']",z9l6nHpTyT,['EMNLP/2023/Conference/Submission1404/Reviewer_f2rF'],1691110000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1404/Reviewer_f2rF']","Yes, the peer review suggests referring to specific literature that is not already discussed in the original paper. The reviewer mentions ""Missing some related references, e.g., FGIM, Wang et al., 2019"" which implies that the authors should include these references in their paper."
MQwLC7aBqC,"This paper proposes a task called incremental novel slot detection that continually detects new slots, labels new slots, and retrains the slot detection model for the next-round detection. The authors adapt the SNIPS and ATIS, two classical slot-filing datasets to fulfill this setting. They proposed to combine contrastive learning and noise-induced adversarial learning for novel type prediction and propose a Query Enhanced Knowledge Distillation to alleviate the catastrophe-forgetting problem during incremental learning.  Experimental results show the efficacy of the proposed framework when compared with a novel-type prediction baseline and two continual learning baselines. ","1. Propose a research problem that is more relevant to the realistic setting, i.e., incremental slot detection (both out-of-distribution and in-distribution), which requires continually detecting new slot values and deploying the new slot prediction models. This could be an interesting research topic for online dialog system development.

2. For slot detection, the paper utilizes learnable query vectors for feature matching and sequence prediction and use Hungarian algorithm to optimize the labeled triples. the same query can also be used to retrieve some representative data from the training set for incremental learning to avoid the catastrophic forgetting problem. Knowledge distillation is also used for fast model adaptation. This framework could potentially be useful and efficient in real dialog systems.","1. Lacking strong baselines. The incremental learning for dialog systems is a classical topic, many different approaches, like architecture-based, memory-based, and data-retrieval-based are proposed in previous work[1], and also compared with strong multi-task learning baseline.  Since this paper focuses on incremental learning, it needs more comparison with different baselines.

2. Lacking more complex benchmarks to verify the effectiveness of the method. Current benchmarks are small and toylike. More realistic dialog datasets like MultiWOZ, SGD, and dataset on DialogStudio should be considered.  In addition, ChatGPT or GPT-4 results on this benchmark should be included for more comparison. This may not be required, but it's crucial to demonstrate how hard and challenging the new task is.

[1] Madotto A, Lin Z, Zhou Z, et al. Continual learning in task-oriented dialogue systems[J]. arXiv preprint arXiv:2012.15504, 2020.
","1. Table 2. For the Snips dataset, why the NSD results in a decrease from 5% to 10% and an increase from 10% to 15%?

2. Line 374. Step 2, Why replace the text token with MASK, and slot values belonging (T_p<i) are labeled with O? Should it be T_p>i?

3. Line 190 After a new slot value is detected at the current stage, the novel slots will be viewed as in-domain types. Does that mean only the set of the new slot names becomes available? Or all the labeled data for those new slots are available?
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Open-vocabulary slot-filling:

1. Rastogi A, Zang X, Sunkara S, et al. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset[C]//Proceedings of the AAAI conference on artificial intelligence. 2020, 34(05): 8689-8696.

2. Rastogi A, Hakkani-Tรผr D, Heck L. Scalable multi-domain dialogue state tracking[C]//2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017: 561-568.

3. Bapna A, Tur G, Hakkani-Tur D, et al. Towards zero-shot frame semantic parsing for domain scaling[J]. arXiv preprint arXiv:1707.02363, 2017.

4. Dai Y, Zhang Y, Liu H, et al. Elastic CRFs for Open-Ontology Slot Filling[J]. Applied Sciences, 2021, 11(22): 10675.


Incremental dialog learning

1. Wang W, Zhang J, Li Q, et al. Incremental learning from scratch for task-oriented dialogue systems[J]. arXiv preprint arXiv:1906.04991, 2019.

2. Madotto A, Lin Z, Zhou Z, et al. Continual learning in task-oriented dialogue systems[J]. arXiv preprint arXiv:2012.15504, 2020.

3. Kim G, Xiao C, Konishi T, et al. Open-World Continual Learning: Unifying Novelty Detection and Continual Learning[J]. arXiv preprint arXiv:2304.10038, 2023.
","1.  The contribution of the paper is not very focused. The new task, or the bipartite matching for NSD, or the query-enhanced data retrospection?  Better to focus on one aspect and add extensive experiments or introduce new realistic datasets. 

2. Line 28 citation ?

3. Line 34 research -> researcher

4. Line 256  what is w_b ? is it w_s?
",1691020000000,,,EMNLP/2023/Conference,z69tlSxAwf,"['EMNLP/2023/Conference/Submission3931/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3931/Reviewer_Mmoy']",z69tlSxAwf,['EMNLP/2023/Conference/Submission3931/Reviewer_Mmoy'],1691020000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3931/Reviewer_Mmoy']","Yes, the peer review suggests that the authors should refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers under ""Missing_References"" section that they think would be relevant to the topic of the paper:

* For open-vocabulary slot-filling:
	+ Rastogi et al., 2020
	+ Rastogi et al., 2017
	+ Bapna et al., 2017
	+ Dai et al., 2021
* For incremental dialog learning:
	+ Wang et al., 2019
	+ Madotto et al., 2020
	+ Kim et al., 2023

These papers are related to the topics of open-vocabulary slot-filling and incremental dialog learning, which are relevant to the paper being reviewed. The reviewer is suggesting that the authors consider referencing these papers in their work to provide a more comprehensive overview of the field."
POnuqpw0mh,"This paper proposed a method to do zero-shot intent classification, it can be applied to BERT-based transformer models. The method contains two stages, where for stage-1, the dependency parser is used to get potential intents and in stage-2 the zero-shot classification is performed for final output. Experiments are done on public datasets to verify the effectiveness of the proposed method.",The paper designed a method as a BERT adapter to handle the zero-shot intent discovery task. The model has been evaluated on two datasets and achieved state-of-the-art performance.,"The contribution of the paper is not very clear, how does this method compare with other existing language model adapters.
More ablation study could be done to prove the effectiveness of components in the model architecture.
","1. How does this method compare with other existing language model adapters.
2. What are the tunable parameters and what are the frozen parameters in the model?
3. What is the size of the trainable parameters in the proposed method?
4. The model has been used in English and Italian, can experiments be added to  one more language to better prove the multilingual ability? 
","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"Here are some minor notes that the author may consider:
1. In line-184, building a pipeline [that is] able to handle unseen classes.
2. As the proposed method is a two-stage pipeline, it could be better if stage-1 and stage-2 can be clearly illustrated in the pipeline figure.
",1691103200789,,,EMNLP/2023/Conference,zdMislOLTv,"['EMNLP/2023/Conference/Submission117/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461009433,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission117/Reviewer_cqfr']",zdMislOLTv,['EMNLP/2023/Conference/Submission117/Reviewer_cqfr'],1691103200789,1701461009433,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission117/Reviewer_cqfr']","No, the peer review does not suggest referring to specific literature not already discussed in the original paper. The reviewer only mentions one specific point of comparison with other existing language model adapters, but it is phrased as a question for the authors (How does this method compare with other existing language model adapters?), implying that they expect the authors to provide more information on this topic rather than referencing external literature."
1oyU2ZDkEj,"This paper mainly discusses the behavior of attention mechanisms in pre-trained language models, specifically focusing on the impact of position embeddings on attention. The authors conduct a series of experiments to analyze the relationship between attention and position embeddings, and also investigate the effect of different model architectures and languages on attention behavior. 
For example๏ผthe authors find that 1) learnable absolute position embedding contains sinusoid-like waves, 2) attention heads are able to extract periodic components from the position embedding in the hidden states, and 3) the self-attention mechanism is responsible for adjusting the phase of the periodic components in both the query and the key, thus influencing the direction of attention.
These findings provide a better understanding of the mechanisms underlying position encoding in attention, which can inform future model design and development.","1. Thorough analysis of attention mechanisms. The paper's findings on the impact of position embeddings on attention is particularly valuable. 
2. Clear presentation of experimental results and findings. 
3. The authors' use of theoretical interpretations and visualization techniques to illustrate attention behavior is particularly helpful in conveying their findings to readers.","1. The authors' findings are related to the models and training objectives used in their study, and may not be entirely generalizable. However, the detection and analysis methods proposed by the authors can be applied to other models.",,"5: Excellent: This study is one of the most thorough I have seen, given its type.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1690971413694,,,EMNLP/2023/Conference,zpayaLaUhL,"['EMNLP/2023/Conference/Submission1988/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461134594,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1988/Reviewer_8B3x']",zpayaLaUhL,['EMNLP/2023/Conference/Submission1988/Reviewer_8B3x'],1690971413694,1701461134594,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1988/Reviewer_8B3x']","No, the peer review does not suggest the authors to refer to specific literature that are not already discussed in the original paper. The reviewer praises the thoroughness and clarity of the analysis, and highlights the value of the findings. However, they do mention some limitations, such as the potential lack of generalizability of the results, but this is more related to the interpretation and application of the research rather than suggesting additional literature to be referenced."
STv0V5gHPA,"This paper proposed a framework that consists of a retriever and a generator. Within, the retriever aims to obtain similar samples according to the similarity score calculated by questions, SQLs and graphs built from schemas. The constructed similar samples as positive samples combined with the negative samples are employed for representation learning for the generator via contrastive learning. The experiments indicate that equipping the proposed ReFSQL can bring an obvious improvement to the backbone methods","1. This paper proposes a retriever-generator framework to improve the representation via contrastive learning.
2. Good experimental results. The baselines equipped with the proposed ReFSQL achieve obvious promotions, especially in Spider (which seems achieving SOTA compared with the leaderboard).","1. Poor writing, bad typesetting, and the figures are not vector diagrams.
2. How can the motivation ""To further bridge the gap between specific and general knowledge"" be implemented according to the proposed contrastive learning whose optimization is only minimizing the margin of the representation between similar samples?
3. Most of the recent baselines in Spider build interaction graphs to joint the representation of the questions and schemas (RATSQL, LGESQL, etc.). What are the advantages of the margin methods in the retriever part that split Query+SQL and Schema in two stages, and the self-designed ""Interaction Graph Construction"" methods? There is no analysis in the Methodology and no comparison in the Experiment.
4. Fine-grained ablation tests need to be provided.",,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691117310178,,,EMNLP/2023/Conference,zWGDn1AmRH,"['EMNLP/2023/Conference/Submission1338/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461097686,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1338/Reviewer_wb9q']",zWGDn1AmRH,['EMNLP/2023/Conference/Submission1338/Reviewer_wb9q'],1691117310178,1701461097686,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1338/Reviewer_wb9q']","No, the peer review does not explicitly suggest referring to additional literature. The reviewer mentions a few issues with the paper, such as poor writing and lack of analysis or comparison with recent baselines, but they do not mention any gaps in the theoretical framework that would require citing more literature. However, they do question how the motivation ""To further bridge the gap between specific and general knowledge"" is implemented by the proposed contrastive learning method, which could be seen as a hint to explore related work on this topic."
bPH9oGRRXy,"This paper presents a source-sentence reordering method to narrow the gap between typologically-distant languages in cross-lingual transfer. They learn reordering rules based on Universal Dependencies and apply them at all levels of the syntactic tree. Extensive experiments show the effectiveness of the proposed approach in enhancing cross-lingual transfer, particularly in the context of low-resource languages. ","1.	Extensive experiments on different tasks (UD paring, relation classification, and semantic parsing) and settings (zero-shot and few-shot), which show the necessity of reordering. 
2.	Detailed analyses on different architectures. 
","1.	Insufficient comparison with related works.
First, the differences with the most related work (Rasooli and Collins, 2019) should be clearer. Their approach is reported to leverage rich syntactic information, as stated on line 190, which is controversial with the expression โsuperficial statisticsโ in the line 192. Second, it is insufficient to solely compare the proposed methodology with that of Rasooli and Collins (2019). It would be prudent to also consider similar work, such as that of Liu et al. (2020a).

2.	The effectiveness of these two settings (STANDARD and ENSEMBLE) varies depending on the tasks and languages. As shown in table 5, ENSEMBLE outperforms STANDARD for the majority of languages, with the notable exceptions of Japanese and Italian. A similar phenomenon is observed for Thai and Irish, as indicated in Table 2, warranting further investigation.
",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1.	Line 265: โbased on of the UDโ -> โbased on the UDโ
2.	The average scores for both typologically-distant and structurally-similar languages should be included in Table 2 for comparison. Additionally, it may be beneficial to employ the average reordering time per sentence as a quantitative measure of structural distance.
",1690300000000,,,EMNLP/2023/Conference,z8gM4ZfK8l,"['EMNLP/2023/Conference/Submission4095/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4095/Reviewer_gucq']",z8gM4ZfK8l,['EMNLP/2023/Conference/Submission4095/Reviewer_gucq'],1690300000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4095/Reviewer_gucq']","Yes, the peer review suggests that the authors should refer to specific literature that are not already discussed in the original paper. The reviewer mentions that it would be prudent for the authors to also consider ""similar work"" such as Liu et al. (2020a) in addition to the most related work of Rasooli and Collins (2019). This implies that the authors should expand their literature review to include other relevant studies, not just the ones already mentioned in the paper."
CHrUCXf8aU,"The paper proposes a novel  large-scale Chinese lexical substitution (LS) dataset created by human-machine collaboration. The key contributions are:

- Construction of a large-scale Chinese LS dataset, CHNLS, containing 33,695 instances across 3 text genres. Significantly larger than prior English datasets.
- Presentation of 4 LS methods, including dictionary, embedding, BERT and paraphraser-based approaches.
- An ensemble method combining the 4 approaches that outperforms individual methods on CHNLS evaluation.
- Quantitative and qualitative analysis showing the high coverage and quality of CHNLS.","- Addresses the lack of Chinese LS datasets and enables future research for this under-studied language. 
- The collaborative annotation approach is creative, efficient and results in higher coverage compared to solely human annotation. Could inform future dataset creation.
- Comprehensive experiments demonstrate the utility of the dataset and the effectiveness of the proposed ensemble method. Thorough quantitative and qualitative analysis.","- While larger than prior datasets, CHNLS still only covers 3 genres of Chinese text. More diversity could be beneficial.
- Some subjectivity and noise inevitable during human evaluation of machine-generated substitutes. Inter-annotator agreement is unclear.
- Ensemble approach is relatively simple. More sophisticated methods could be explored for combining multiple LS techniques.
- Limited analysis of how well methods generalize across the 3 genres. More cross-genre evaluation would be informative.
- The dataset quality should be improved. For example, in wiki_test_gold.txt, line 1, the second substitute ""ๆข"" is in correct. The authors should do double check to make sure the substitute is correct.","- Did you apply any quality assurance measures been taken during the data annotation process? How is the consistency of the annotators' work ensured?
- It's recommended to increase the diversity of data to make this dataset more effective and compelling","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691168724506,,,EMNLP/2023/Conference,zaBPb6Pu21,"['EMNLP/2023/Conference/Submission3500/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461233431,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3500/Reviewer_24p3']",zaBPb6Pu21,['EMNLP/2023/Conference/Submission3500/Reviewer_24p3'],1691168724506,1701461233431,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3500/Reviewer_24p3']","No, the peer review does not specifically suggest referring to additional literature. The reviewer provides a detailed evaluation of the paper's strengths and weaknesses, but does not mention any gaps in the existing knowledge or references that need to be addressed by citing new sources. The focus is on providing constructive feedback on the paper itself, rather than suggesting additional research or references."
xg3MAC1hDT,"This paper introduces PHD, a pixel-based language model for analyzing historical documents without using OCR. The main contributions are:

1. Proposes a novel method to generate synthetic scans that resemble historical documents for pretraining. This helps address the scarcity of large historical scan datasets.

2. Pretrains PHD on a combination of synthetic scans and real historical newspapers from the 18th-19th centuries.

3. Evaluates PHD on image reconstruction, clustering, language understanding tasks like GLUE, and question answering on both SQuAD and a real historical QA dataset.

4. Provides evidence that PHD can effectively understand language and has potential for assisting with NLP tasks involving historical documents.

5. Releases the datasets, models, and code to facilitate future research.

Overall, this paper explores using recent advances in pixel-based language modeling to process historical scans directly at the pixel level. This allows bypassing the OCR stage which can introduce noise when applied to historical documents. The proposed pretraining methodology and evaluations demonstrate the promise of this approach for historical document analysis.","This paper explores an interesting new direction and provides a thorough evaluation of the proposed techniques. Releasing the datasets and models could catalyze more work in this area.
1. Well-written paper that clearly explains the motivation, proposed approach, experiments, results, and limitations.
2. Novel application of pixel-based language models to historical document analysis, bypassing the need for OCR. This is an interesting new direction for processing historical texts.
3. Releases new datasets, models, and code to facilitate research in this area. The historical QA dataset created from real newspaper ads could be valuable for the community.","1. Most of the pretraining data is modern text, not historical. More diverse historical data could help the model better adapt to that domain. Previous work, such as Donut, DiT, Dessurt, and LayoutLM (v1, v2, v3), pre-trained their models on IIT-CDIP. IIT-CDIP is a large-scale scanned document corpus used for pre-training language models.

2. The evaluation tasks, apart from one historical QA dataset, predominantly involve modern text. More historical evaluation data could better assess performance. 

3. There are also some document understanding benchmarks, such as DocVQA (also used in Donut, DiT, Dessurt, and LayoutLM v1, v2, v3), which can be used to evaluate the question-answering performance of models.

4. As the paper mentions, evaluating the pixel-based completions is challenging. More robust quantitative evaluation methods are needed.

5. OCR techniques continue to improve over time. At some point, OCR quality may be sufficient to apply standard NLP pipelines to historical texts without needing to bypass OCR.",The previous studies utilized datasets such as IIT-CDIP and DocVQA for pre-training and evaluation. Can you discuss why you did not consider them?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"- Lewis, David D. et al. โBuilding a test collection for complex document information processing.โ Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval (2006): n. pag.

- Mathew, Minesh et al. โDocVQA: A Dataset for VQA on Document Images.โ 2021 IEEE Winter Conference on Applications of Computer Vision (WACV) (2020): 2199-2208.",,1691220000000,,,EMNLP/2023/Conference,zIb2DlqBxm,"['EMNLP/2023/Conference/Submission1679/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1679/Reviewer_NET7']",zIb2DlqBxm,['EMNLP/2023/Conference/Submission1679/Reviewer_NET7'],1691220000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1679/Reviewer_NET7']","Yes, the peer review suggests that the authors should refer to specific literature that is not already discussed in the original paper. The reviewer mentions two papers:

1. ""Building a test collection for complex document information processing"" by Lewis et al. (2006)
2. ""DocVQA: A Dataset for VQA on Document Images"" by Mathew et al. (2020)

The reviewer suggests that these papers could be relevant to the authors' work, particularly in relation to the use of IIT-CDIP and DocVQA datasets for pre-training and evaluation. The reviewer mentions that previous studies have utilized these datasets, and therefore it would be beneficial for the authors to discuss why they did not consider them."
smD28y6BcO,"This work evaluates models' logical reasoning in the MCQ setting by probing it with additional questions about the reasoning behind selecting or eliminating individual choices. To this end, the authors build a new dataset, RULE. They start with questions from the ReClor dataset and annotate rationales for selecting and eliminating each of its answer choices. They then generate questions like for each choice which has one of the generated rationales as the answer.

These additional (sub-)questions have 2 key features:

1. They are in the same MCQ format as the original question. So one can probe the models' reasoning on the original question by these additional questions.
2. They are contrastive (minimally different), i.e., all subquestions share the same passage and answer choices, but they have different answers depending on the subquestion. This feature prevents models from taking simple shortcuts.

The authors have ensured the annotations are of high quality by human validation, and have also established a very high human score on the task.

Finally, the authors benchmark a large number of recent (few-shot, fine-tuned, with/without CoT) models on ReClor and RULE and demonstrate that all the models struggle and are behind humans. In particular, they find that models are extremely bad at selecting why a given choice is incorrect.

Finally, they explore (i) model-generated rationales and find humans are better at this task. (ii) using human-written rationales to improve models and find that selective subquestions help, eliminative ones hurt.","- This is a very well-done study and evaluation design. I particularly liked the two features highlighted in the summary above.
- This is a nice high-quality resource, and should be helpful for people exploring logical reasoning models.
- I was impressed by the number and diversity of the models that they have benchmarked for this new task (both fine-tuned and few-shot).","I don't see any reason to reject this work.
","Suggestion:

A. I have a different hypothesis about why the score for eliminative subquestions is so much worse than selective subquestions: It is because the models ignore the word ""not"". It would be interesting to do an experiment to test this: Compare model predictions for eliminative subquestions with and without the word ""not"" and see if the scores are close and individual responses correlate. Although not necessary, it'd be a good addition to the paper (at least the appendix, if not the main paper).",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,5: Could easily reproduce the results.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"(tangentially relevant): https://arxiv.org/abs/2204.05212 and https://arxiv.org/abs/2210.10860v1 also collect explanations for each answer choice, albeit for a different purpose. Unlike this work, their explanations for wrong answer argue for why answer should be accepted as opposed to eliminated.

L040: missing reference
",,1691220000000,,,EMNLP/2023/Conference,zByqDt16qZ,"['EMNLP/2023/Conference/Submission266/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission266/Reviewer_i432']",zByqDt16qZ,['EMNLP/2023/Conference/Submission266/Reviewer_i432'],1691220000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission266/Reviewer_i432']","No, this peer review does not suggest that the authors of the paper refer to specific literature that are not already discussed in the original paper. In fact, it mentions two tangentially relevant papers (https://arxiv.org/abs/2204.05212 and https://arxiv.org/abs/2210.10860v1) as a way of providing context for understanding the broader research landscape, but does not suggest that these papers should be cited in the original paper itself."
gJTMXmoGmD,"This paper is based on Induction Augmented Generation Framework that uses LLMs for the implicit reasoning approach. The framework outperforms baselines for Retrieval Augmented Generation and ChatGPT on two Open domain tasks.

Overall Contributions : 
1) Novel inductive prompting method which improves the factuality of knowledge elicited from LLMs.
2)  A GPT implementation of the framework that improves over strong baseline models and ChatGPT.
3) A TAILBACK optimization algorithm that trains the inductor which allows IAG-Student to outperform Retrieval Augmented Generation baselines.","The description of the methodology and building up the framework was well explained.
Evaluation conducted on two large Open-domain QA benchmark datasets.
","1) Although the Student Inductor model is shown to surpass the benchmark, the explanation and the underlying working principle was a bit hard to follow.
2) For tailback differential beam searching is used, but it is hard to follow what are the steps pertaining to it.","1) Can you explain the tailback workflow using a pseudo code with detailed steps? Currently, it's a bit difficult to follow.
2) Are there any plan to switch to later GPT versions from GPT-3? What would be the implications in terms of evaluation outcomes ?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691732126505,,,EMNLP/2023/Conference,zwqDROxClj,"['EMNLP/2023/Conference/Submission1595/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461113139,[],4,,,"['everyone', 'EMNLP/2023/Conference/Submission1595/Reviewer_1Ny9']",zwqDROxClj,['EMNLP/2023/Conference/Submission1595/Reviewer_1Ny9'],1691732126505,1701461113139,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1595/Reviewer_1Ny9']","No, this peer review does not suggest the authors to refer to specific literature that are not already discussed in the original paper. The reviewer mentions areas where the explanation and methodology were hard to follow (points 1 and 2 under ""Reasons_to_reject""), but they do not recommend reading any additional literature. Their questions for the authors (points 1 and 2) aim to clarify specific points related to the framework's implementation and potential future work, rather than recommending further research or citing outside sources."
9nIcwKQ0aO,"This paper proposes a news article dataset consisting of 206 news articles annotated for partisan and counter-partisan events in the articles. Partisan events are defined as events reporting of which benefit the stance of the news article and counter-partisan events are events that are reported with partisan events to make the news article seem neutral.

The authors proposed a human annotation framework for annotating such events by focusing on reported entities and sentiment toward them. The authors identified the ideologies of the news articles and the partisanship of the events using a few off-the-shelf classifiers and ChatGPT. The authors also presented some qualitative analysis of the partisan events and reported how different polarities use such events. For example, in the right-biased media, counter-partisan events appear towards the end of the news articles.","[A1] This work proposes a novel dataset for analyzing partisan news media reporting of events.

[A2] The human annotation framework is well-described and it seems easy to replicate.","[R1] The concept of the partisan event as it is reported in the paper seems conceptually very similar to previously studied information bias (Fan et al., 2019) (discussed by the authors in the related works section) and subframes (Roy and Goldwasser, 2020) (not discussed by the authors in the paper). Hence, I am not totally convinced about the novelty of the work rather it seems like an incremental extension of the previous studies.

[R2] The number of news articles annotated is very few (only 206). Hence, it is hard to say that any trends found in this small amount of data are generalizable to the whole news media.",[Q1] What are the high-level discussion topics in the news articles studied in this paper?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1690670000000,,,EMNLP/2023/Conference,zrBrl2iQUr,"['EMNLP/2023/Conference/Submission5006/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission5006/Reviewer_5vXQ']",zrBrl2iQUr,['EMNLP/2023/Conference/Submission5006/Reviewer_5vXQ'],1690670000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5006/Reviewer_5vXQ']","Yes, the peer review suggests that the authors of the paper may want to refer to additional literature that is not already mentioned in the original paper. Specifically, the reviewer mentions two studies:

1. Fan et al. (2019) - which discusses information bias
2. Roy and Goldwasser (2020) - which discusses subframes

The reviewer notes that these studies are conceptually related to the partisan events described in the paper, but are not mentioned by the authors. The reviewer suggests that including a discussion of these papers could strengthen the novelty of the work and address concerns about its incremental nature."
ttwwsOhAIT,"This paper introduces the Induction-Augmented Generation framework, which integrates inductive knowledge statements for Open-Domain QA tasks to enhance implicit reasoning. The framework proposes two models: IAG-GPT and IAG-student. IAG-GPT generates inductive knowledge statements using GPT-3 and utilizes both generated knowledge prompts and retrieved documents for QA tasks. Since IAG-GPT is dependent on the GPT-3 API, IAG-student is trained to distill GPT-3.",The proposed approach achieved state-of-the-art performance by integrating inductive knowledge into the prompt on the CSQA and Strategy QA datasets. The presentation of quality examples (such as Table 6 and 7) further supports the validity of the work.,"* The consistency in reporting the results needs to be done. Figure 3-4 and Table 2-4 uses StrategyQA dev which makes it hard to compare with Table 1 StrategyQA test baselines.  
* Table 2 shows that knowledge statements generated with inductive prompting support QA performance. However, to fully verify the effectiveness of inductive knowledge statements, additional comparisons need to be made on CSQA dev with 15 retrieved documents versus CSQA dev with 10 retrieved documents and 5 inductive knowledge statements. On StrategyQA, a comparison between 10 retrieved documents and 5 retrieved documents with 5 inductive knowledge statements needs to be conducted.","* Is the concatenation style of Retrieval Only in IAG the same as FiD [1]? If it is FiD-style, then there will be N number of passages to encode with the query. If not, as shown in concept figure 2, there will be a single long input which is a concatenation of the query and all N passages for IAG-GPT and IAG-student. It needs more clarification because in the knowledge fusion experiment, it seems like all of the question, knowledge statements, and retrieved documents are concatenated into a single long input.
* On the CSQA dev dataset, it is stated that top-5 snippets are used (Line 423). However, in Line 465, on CSQA, it uses 10 retrieved documents. Is the top-5 snippets transformed into 10 retrieved documents? Or does it use top-10 snippets?
* Inductive prompting generates implicit knowledge through in-context learning of 5 demonstrations. How are trivial and CoT prompting done? The examples are shown in Table 7, but it is not clear how the prompts are formed.
* Are there cases where inductive knowledge statements are hallucinated?

[1] Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"* In Line 467, naming of the dataset should be consistent
	* sudden acronym for StrategyQA -> SQA
* Including the model size could strengthen your Table 1.",1691458202749,,,EMNLP/2023/Conference,zwqDROxClj,"['EMNLP/2023/Conference/Submission1595/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461113300,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1595/Reviewer_i5uE']",zwqDROxClj,['EMNLP/2023/Conference/Submission1595/Reviewer_i5uE'],1691458202749,1701461113300,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1595/Reviewer_i5uE']","No, the peer review does not suggest that the authors should refer to any additional literature. The reviewer is mainly asking for clarification on certain points and suggesting ways to strengthen the presentation of the results, but they do not mention any omissions or gaps in the literature that need to be addressed by referring to other studies."
bjd85sbyKN,"This paper proposes the shared space to alleviate the problem by the representation mismatches among the modalities (i.e., video and text).","The best performances in three benchmarks in video-text retrieval
Sufficient experiments on the retrieval dataset","[-] Poor writing. To use the word 'Sparse', it is necessary to first specify the sparsity on what (e.g., quantities, distribution?). When reading the Introduction, I didn't know the sparsity of what and also didn't know why the concept of sparsity is needed or what it does.

[-] If the contributions of this paper are based on the representation enhancement of shared spaces about heterogeneous modalities, it is more convincing to validate the approach in several multi-modal video language tasks such as video question answering, video-grounded reasoning/dialogue. Why only experiments on text-video retrieval? Haven't you verified the effect on several other multi modal tasks?

[-] Does the author can visualize the effectiveness of the proposed method?",see above,1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.,"2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691146794935,,,EMNLP/2023/Conference,zeGXjQYhXz,"['EMNLP/2023/Conference/Submission4649/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461307472,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4649/Reviewer_UH8H']",zeGXjQYhXz,['EMNLP/2023/Conference/Submission4649/Reviewer_UH8H'],1691146794935,1701461307472,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4649/Reviewer_UH8H']","No, this peer review does not suggest the authors to refer to specific literature that are not already discussed in the original paper. The reviewer provides feedback on the paper's content and methodology, but does not recommend any additional literature for the authors to consult. Instead, the reviewer asks questions of the authors, such as ""Can you visualize the effectiveness of the proposed method?"" and points out areas where the paper could be improved or more convincing, but does not suggest that they should read anything specific outside of their own work."
n1Zz9dJ9U6,"Positional encoding is important in Transformer architecture, and until a few years ago, learnable absolute position embedding (APE) was often used (e.g., BERT, RoBERTa, GPT-2).

Clark+'19 reported that some attention heads in BERT attend to context words according to their relative positions; Ravishankar&Sรธgaard+'21 reported that some columns of absolute position embeddings are periodic; Chang+'22 reported that the position information is encoded in a hidden representation while remaining periodic.
However, it is not clear how periodicity is used in models, which is what this paper attacks.

Specifically, this paper showed that several attention heads in RoBERTa realize an attention pattern that depends on relative position by extracting the periodic components derived from APE from the hidden representation with shifting the phase in query and key transformation.","- Analyses use convincing and solid mathematical tools.
- Mechanism for realizing relative position-dependent attention patterns from absolute position embedding is really interesting.","- Limited Experiments
  - Most of the experiments (excluding Section 4.1.1) are limited to RoBERTa-base only, and it is unclear if the results can be generalized to other models adopting learnable APEs. It is important to investigate whether the results can be generalized to differences in model size, objective function, and architecture (i.e., encoder, encoder-decoder, or decoder). In particular, it is worthwhile to include more analysis and discussion for GPT-2. For example, I would like to see the results of Figure 2 for GPT-2.
  - The input for the analysis is limited to only 100 or 200 samples from wikitext-2. It would be desirable to experiment with a larger number of samples or with datasets from various domains.
- Findings are interesting, but no statement of what the contribution is and how practical impact on the community or practical use. (Question A).
- Results contradicting those reported in existing studies (Clark+'19) are observed but not discussed (Question B).
- I do not really agree with the argument in Section 5 that word embedding contributes to relative position-dependent attention patterns. The target head is in layer 8, and the changes caused by large deviations from the input, such as only position embedding, are quite large at layer 8. It is likely that the behavior is not such that it can be discussed to explain the behavior under normal conditions. Word embeddings may be the only prerequisites for the model to work properly rather than playing an important role in certain attention patterns.
- Introduction says to analyze ""why attention depends on relative position,"" but I cannot find content that adequately answers this question.
- There is no connection or discussion of relative position embedding, which is typically employed in recent Transformer models in place of learnable APE (Question C).","- A. What is the substantial contribution to the community from these findings? For example, could they lead to any ideas to improve the Transformer architecture?
- B. Clark+'19 reported that most heads pay little attention to themselves and that there are heads that focus heavily on adjacent tokens, especially in the earlier layers. However, this paper shows that there are several heads that focus on themselves and that heads focusing heavily on adjacent token heads were found in various layers. What is the reason for this discrepancy?
- C. Nowadays, relative position embedding is often used instead of APE. Can this paper provide insights into the reasons for the superiority of relative position embedding? For example, can we interpret that relative position embedding can capture other language information richer than APE because there is no need to extract relative positions in the query or key transformation matrices?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"- In addition to Clark+'19, Kovaleva+'19 should be mentioned as a representative attention analysis
    - Kovaleva+, Revealing the Dark Secrets of BERT (EMNLP2019)","- Caption of Figure 1: Attention weights of the first 15 tokens -> Attention weights for the first 15 tokens of an input text? If it is a special input text, please explain explicitly.
- ll.223-227: I cannot follow what you meant. Please provide a clearer explanation.
- ll.263-265: Should mention the agreement with Lin+'19 results here
- (minor) l.200: Consider changing the variable name PE, since PE appears to be the product of two matrices P and E.",1691240000000,,,EMNLP/2023/Conference,zpayaLaUhL,"['EMNLP/2023/Conference/Submission1988/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission1988/Reviewer_fPAJ']",zpayaLaUhL,['EMNLP/2023/Conference/Submission1988/Reviewer_fPAJ'],1691240000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1988/Reviewer_fPAJ']","Yes, the peer review suggests that the authors of the paper should refer to additional literature that is relevant to their study but not mentioned in the original paper. Specifically:

* The reviewer mentions that ""In addition to Clark+'19, Kovaleva+'19 should be mentioned as a representative attention analysis"" and provides the reference: ""Kovaleva+, Revealing the Dark Secrets of BERT (EMNLP2019)"".

This suggests that the authors should add a reference to Kovaleva et al.'s paper in their work to provide further context and background information on attention analysis."
LKMFvBsLIp,"The authors propose an induction-augmented framework that utilizes inductive knowledge derived from LLMs and the retrieved documents for better implicit reasoning. Specifically, they enhance the conventional RAG with an inductor that generates inductive knowledge.

The authors propose an IAG-GPT model which directly utilizes GPT-3 and IAG-Student which is first trained via knowledge distillation with GPT-3 pseudo labels, and then optimized through a differentiable beam search algorithm.

The experiments show that IAG-GPT has significant advantages over ChatGPT and performs extremely well on CSQA2.0 and StrategyQA. IAG-Student outperforms RAG baselines.
","This paper addresses a non-trivial problem for implicit reasoning by introducing an inductor with the assistance of LLMs.  

The authors conduct extensive experiments and the experiment results are reasonable.

","1. The generalization ability of the model is a major concern. 
- The model assigns inductive information to every question, even when some questions do not require it. A potential solution could be implementing a question classifier to identify the type of question and determine whether inductive information is necessary for a particular query.
- The strict structure/formulation of the prompt, especially the Knowledge part, is another issue (lines 245-247). 

2. Another minor issue is that there is a huge gap between the performance of IAG-GPT and IAG-Student, which makes the distilled model and its corresponding algorithm less convincing. More experiments on larger models are expected. 

","1. In the IAG-student algorithm, the generator is first trained followed by the inductor, will finetuning the generator together with the inductor help?

2. In line 228, How to calculate the mu and sigma for the distribution?

",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691035545695,,,EMNLP/2023/Conference,zwqDROxClj,"['EMNLP/2023/Conference/Submission1595/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461113393,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1595/Reviewer_HrRJ']",zwqDROxClj,['EMNLP/2023/Conference/Submission1595/Reviewer_HrRJ'],1691035545695,1701461113393,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1595/Reviewer_HrRJ']","No, this peer review does not suggest the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several concerns and potential solutions, but these are based on the information presented in the paper itself (e.g., the use of a question classifier) rather than referencing external literature."
7XKdnhSQ5p,"The paper โChinese Lexical Substitution: Dataset and Methodโ presents a novel approach for the creation of annotated Lexical Substitution datasets, focusing on the Chinese language to address the lack of an existing Chinese data source for LS. In addition, the authors propose a new ensemble method, combining four classes of LS methods. In doing so, the authors evaluate both the dataset they have created, as well as the effectiveness of their ensemble approach.","The contributions presented by the authors in this paper are many.

Firstly, the authors highlights shortcomings of existing LS datasets, particularly pointing to their relatively small scale and lack of coverage. While this has been pointed out in the literature before (e.g., SwordS), the authors justify that the issues still persist. Moreover, the lack of LS resources for the Chinese language is highlighted.

The new annotation scheme presented is highly original. Motivated by the issue of small-scale datasets, the authors propose a method that allows for larger-scale corpus creation via the automation of existing methods. The quality of these outputs is ensured by the placement of human-made decisions at the end of the pipeline. The creation and execution of this process is well-described and transparent.

The introduction of the CHNLS dataset is likewise a great contribution. This is supplemented by descriptive statistics of the dataset, as well as a discussion of the evaluation that was run to measure the quality and coverage.

The experiments run using the novel ensemble method are demonstrated by using well-known LS metrics from LS07, as well as the presentation of illustrative examples.

The paper is concluded by a meaningful discussion of the results, particularly in comparison to existing datasets and methods.
","Besides the occasional grammatical or stylistic errors (some highlighted below), there are only a few points of weakness exhibited by the paper.

For example, the details of the ensemble method are a bit lacking. The exact construction of the ensemble, as well as the scheme used to weigh the individual methodsโ scores, is not fully elucidated.

In addition, the proposed ensemble is only evaluated on the newly created dataset, making it hard to compare against other existing methods, such as those that have been evaluated on English data.

Finally, while the top 10 substitutes tables are interesting, they are quite cluttered and slightly unclear. Who determined the substitutes in red? Moreover, it is difficult to interpret the results where no translation is provided.","Question A: can you explain why some translations are left out of the tables?

Question B: were any inter-annotator agreement statistics calculated?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,5: Could easily reproduce the results.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"Line 38: this and the following sentence should be one sentence, yet they are split.

455: belonging -> belong 

The naming of Appendix B presumably needs to be changed from the default.",1691753659316,,,EMNLP/2023/Conference,zaBPb6Pu21,"['EMNLP/2023/Conference/Submission3500/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461233326,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3500/Reviewer_o7zy']",zaBPb6Pu21,['EMNLP/2023/Conference/Submission3500/Reviewer_o7zy'],1691753659316,1701461233326,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3500/Reviewer_o7zy']","No, the peer review does not suggest the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions one example of a related work (SwordS) as part of the reasons to accept the contributions of the paper, but this is not an additional reference required for further investigation. The rest of the comments focus on evaluating the quality and clarity of the paper, pointing out areas for improvement, and suggesting minor changes to the text."
lopURGm6td,The paper studied the effects of partisan and counter-partisan events in news reporting across different media outlets. A newly annotated dataset (PARTISAN EVENTS) is provided. Experiments on partisan event detection with a variety of models demonstrate the difficulty of the proposed task.,"(1) The proposed partisan and counter-partisan event detection task is new and interesting 
(2) A task-corresponding dataset is built for research purpose 
(3) The paper is well written and easy to follow
","(1) The novelty of the paper is limited. 
(2) It is mainly a case-study paper. No new methods and techniques are proposed. The experiments only show the difficulty of the proposed task. 
",,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691219925647,,,EMNLP/2023/Conference,zrBrl2iQUr,"['EMNLP/2023/Conference/Submission5006/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461325806,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission5006/Reviewer_ZkKA']",zrBrl2iQUr,['EMNLP/2023/Conference/Submission5006/Reviewer_ZkKA'],1691219925647,1701461325806,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5006/Reviewer_ZkKA']","No, this peer review does not suggest the authors to refer to specific literature that is not already discussed in the original paper. The reviewer's main concerns are with the novelty and contributions of the paper, its methodology, reproducibility, and overall impact, rather than asking for additional literature or references to be added."
y5Ecijt5fr,"This paper presents Adapter-TST, a straightforward yet effective and light-weight method for addressing the challenging task of multiple-attribute textual style transfer (TST). In contrast to single-attribute TST, which focuses on changing one stylistic property at a time, Adapter-TST deals with simultaneously altering multiple stylistic propertiesโsuch as sentiment, tense, voice, formality, and politeness. The Adapter-TST approach involves inserting simple neural adapters (consisting a feed-forward down-project layer, followed by a nonlinear activation function, followed by a feed forward up-project layer, along with a skip-connection layer between two projection layers) into a pre-trained network, like a pre-trained BART model, to capture and change diverse attribute information. The weights (parameters) of the original network are kept frozen during training, and only the additional adapter layers are trained. Adapter-TST offers two configurations, ""parallel"" and ""stack,"" enabling models to perform compositional style transformation (text editing).

In order to validate the effectiveness of Adapter-TST, the authors employ BART-Large as their backbone model and conduct experiments on the Yelp and StylePTB datasets. Their results indicate that Adapter-TST often performs on par with, not outperforms, several baseline methods, including BackTrans, CrossAlign, DualRL, and StyleTransformer, along certain evaluation metrics. Although human evaluation shows positive outcomes for Adapter-TST, the improvements do not appear to be statistically significant.","* Textual style transfer with multiple attributes remains a relatively unexplored and challenging task within the field of NLP. This paper focuses on this intriguing problem and introduces an intuitive, effective, and light-weight approach to tackle it. 
* The paper demonstrates a clear motivation for studying the problem of multiple-attribute textual style transfer and effectively outlines their research objectives. Moreover, the contributions of their work are well-defined, and the paper is written in a coherent manner overall, making it easy to understand both the arguments and results presented.
* This work has the potential to be of interest to not only the TST community but also the broader NLG community, as it proposes a parameter-efficient method for compositional text editing. (That said, the paper lacks clarity regarding the amount of fine-tuning data required to achieve satisfactory task performance and the overall generalizability of the proposed Adapter-TST approach. Further investigation on these aspects would be rather beneficial to better understand the practical implications of this method.)
* On the whole, the experimental setup seems to be sound and thorough.","* While this limitation is not a significant reason for rejection, I believe that the authors could enhance the credibility of their proposed approach by demonstrating its generalizability and robustness. Currently, the focus is solely on one true multi-attribute TST dataset (StylePTB) and one simple model type (BART-Large). The inclusion of an additional dataset would strengthen their claims and offer a clearer demonstration of Adapter-TSTโs efficacy. Moreover, a more comprehensive analysis of their findings is needed in my opinion, as the paper, in its current form, offers only a surface-level discussion of the results.
* This is a relatively minor concern, but I fear that the paper does not sufficiently make reference to the recent studies on textual style transfer and consider stronger baselines for the Yelp dataset.
* In my opinion, the authors missed an opportunity to include two other simple yet potentially strong baselines in their evaluation: an Alpaca (or LLaMa) model with and without adapters. Considering these instruction-tuned models could have provided valuable insights even under a zero-shot setting. Comparing their proposed approach against such baselines could have further enriched the analysis and strengthened the overall study.","* Question A. Could you please provide more details about the training of the adapter layers? For instance, how many epochs did you train your models? How important is the classification loss? 
* Question B. How crucial are the parallel connections? Do you think that stack connections might be enough to perform compositional text editing?
* Question C. Have you considered and conducted experiments using other models, in addition to BART-Large? 
* Question D. Would your proposed method demonstrate favorable performance if trained on a combination of multiple textual style transfer datasets, such as sentiment, formality, grammar correction, and others?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"**TST Literature**
* Eric Malmi, Aliaksei Severyn, and Sascha Rothe. 2020. Unsupervised Text Style Transfer with Padded Masked Language Models. arXiv preprint arXiv:2010.01054.
* Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A Recipe for Arbitrary Text Style Transfer with Large Language Models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 837848, Dublin, Ireland. Association for Computational Linguistics.
* Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022. Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2195โ2222, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
* Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Maheswaran. 2019. ""Transforming"" Delete, Retrieve, Generate Approach for Controlled Text Style Transfer. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3269โ3279.
* Shamik Roy, Raphael Shu, Nikolaos Pappas, Elman Mansimov, Y. Zhang, Saab Mansour and Dan Roth. โConversation Style Transfer using Few-Shot Learning.โ ArXiv abs/2302.08362 (2023).
* Kalpesh Krishna, Deepak Nathani, Xavier Garcia, Bidisha Samanta, and Partha Talukdar. 2022. Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7439โ7468, Dublin, Ireland. Association for Computational Linguistics.

**LLaMa-Adapter**
* Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao and Yu Jiao Qiao. โLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention.โ ArXiv abs/2303.16199 (2023).","* In general, the paper is well-written and easy to follow; however, there is room for improvement in the readability and flow of Sections 3.2, 4.2, and 4.3.
* L170: 2019).To leverage โ 2019. To leverage // Space needed between the period and the word โToโ
* In Table 2, the abbreviations ""BS"" and ""G"" are not explicitly defined, leaving readers uncertain about their meanings. It appears that ""G"" possibly represents the geometric mean of ACC, BERTscore, 434, and 1/PPL. 
* However, the rationale behind choosing the geometric mean of these specific metrics remains unclear. It would be beneficial for the authors to provide further explanation and intuition behind this choice to help readers better understand the motivation and significance of using the geometric mean as a composite metric.",1691200000000,,,EMNLP/2023/Conference,z9l6nHpTyT,"['EMNLP/2023/Conference/Submission1404/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission1404/Reviewer_2k94']",z9l6nHpTyT,['EMNLP/2023/Conference/Submission1404/Reviewer_2k94'],1691200000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1404/Reviewer_2k94']","Yes, the peer review suggests that the authors should refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers under the section ""Missing_References"" which include:

* Eric Malmi, Aliaksei Severyn, and Sascha Rothe. 2020. Unsupervised Text Style Transfer with Padded Masked Language Models. arXiv preprint arXiv:2010.01054.
* Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A Recipe for Arbitrary Text Style Transfer with Large Language Models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 837-848, Dublin, Ireland. Association for Computational Linguistics.
* Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022. Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2195โ2222, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
* Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Maheswaran. 2019. ""Transforming"" Delete, Retrieve, Generate Approach for Controlled Text Style Transfer. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3269โ3279.
* Shamik Roy, Raphael Shu, Nikolaos Pappas, Elman Mansimov, Y. Zhang, Saab Mansour and Dan Roth. โConversation Style Transfer using Few-Shot Learning.โ ArXiv abs/2302.08362 (2023).
* Kalpesh Krishna, Deepak Nathani, Xavier Garcia, Bidisha Samanta, and Partha Talukdar. 2022. Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7439โ7468, Dublin, Ireland. Association for Computational Linguistics.
* Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao and Yu Jiao Qiao. โLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention.โ ArXiv abs/2303.16199 (2023).

The reviewer suggests that the authors should consider citing these papers to provide a more comprehensive view of the literature on text style transfer, as some of them might be relevant to their work."
LTvE66R8hh,"The paper introduces an adapter-based approach for the multiple-attribute text style transfer task.  In short, the processed method Adapter-TST utilizes a series of adapters to model different types of attribute information. As a parameter-efficient method, Adapter-TST achieves better performance with a very low number of training parameters compared to the previous method.","1. The paper was well-written and easy to follow.
2. How to use parametric efficient methods in style transfer tasks is an important field worth exploring.","1. The contribution of the whole paper is limited.  Using adapter-based PLMs is a common strategy for text generation tasks, even those involving multi-attribute-based generation tasks, such as [2][3]. At the same time, the adapter used in this paper is not significantly different from the previous common methods, nor is it optimized for tasks.
2. This paper designed the method only with BART as the backbone. For this reason, I think it might be difficult to call this particular approach a ""parameter-efficient framework"" because its generality has yet to be proven.
3. There are serious deficiencies and possible unfair comparisons in the experimental part. The main reasons are as follows:
3.1 Baselines selected in the experiment are seriously missing. The experiment only compares the proposed method with the non-parameter-efficient method Styletransformer, while other parameter-efficient methods are ignored, especially prompt-learning-based methods, such as[1][2][3]. Although these methods are not applied to style transfer, it is clear that they are general-purpose and can be easily migrated to this task, and [2][3] also perform a similar multi-attribute text generation task while using adapter-based PLMs as baselines.
3.2 Unfair comparison with baselines.  The backbone of Adapter-TST is BART-Large while the backbone of the baseline is Transformer,  it is difficult to determine whether the performance gains are due to the use of PLMs or the proposed approach.

References:

[1] Prefix-Tuning: Optimizing Continuous Prompts for Generation. 

[2] Controllable Natural Language Generation with Contrastive Prefixes.

[3] Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation.",See Reasons To Reject,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,See Reasons To Reject,,1691130000000,,,EMNLP/2023/Conference,z9l6nHpTyT,"['EMNLP/2023/Conference/Submission1404/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1404/Reviewer_Jb8X']",z9l6nHpTyT,['EMNLP/2023/Conference/Submission1404/Reviewer_Jb8X'],1691130000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1404/Reviewer_Jb8X']","Yes, the peer review suggests that the authors should refer to specific literature that are not already discussed in the original paper. The reviewer mentions several references ([1][2][3]) as examples of parameter-efficient methods and multi-attribute text generation tasks with adapter-based PLMs, which were not mentioned or compared in the original paper."
VpCXP2olFf,"This paper presents a new method to improve cross-lingual transfer based on source-side word reordering (operated at the level of dependency subtrees). The authors do so by estimating word order preferences from data, then casting them as constraints to an SMT solver to identify the optimal reordering. The method is experimented on 3 tasks and on typologically diverse languages. Results are ambivalent (usually beneficial but detrimental for some language pairs) but the authors propose a mitigation measure (concatenating original and reordered data) that is successful. Experiments are extended to the few-shot scenario and to comparison among architectures, which yields new insights on the generalizability of the method as well as on properties of existing algorithms.","[Ac-A] The proposed method is thought-enriching and opens new research avenues.

[Ac-B] The authors experiment with a large variety of languages (typologically diverse), tasks, settings (zero-shot, few-shot, different architectures, etc.), which yields more comprehensive and generalizable results.

[Ac-C] Much appreciated the initiative to include a longer discussion on the method's shortcomings in the annex (beyond the ""Limitations"" paragraph).

[Ac-D] Annexes provide comprehensive experimental details and results, including standard deviations, which is very good methodology.","**[Re-A]** The idea underlying this contribution is quite interesting (using a SMT solver to improve upon prior similar work), but it also seems that the authors missed an important related work (Aufrant et al. 2016, see Missing references), and that if accounting for it they would possibly have made some choices differently, and interpreted some of their results differently.

In particular, Equation (2) shows that they discretize the statistics measured empirically (turning a slight preference into a deterministic hard constraint). Why? Aufrant et al. (2016) have precisely discussed (and implemented accordingly) how it is beneficial to have ""*smooth transformations (with mean preference rate objectives and error margins)*"", in other words to avoid deterministic word order, because it is a linguistic fact that for some languages the word order preference is not necessarily deterministic for a given label pair. And they also show how deterministic reordering can be detrimental in practice, by losing useful (balanced) information in the source treebank.

Impact:
- [Re-A1] The argument made on line 331 (that ENSEMBLE works better because statistical estimation creates imperfection) appears dubious in light of that prior work. On the contrary: it is because the proposed method overlooks the statistical nature of word order preference, that ENSEMBLE works better. Indeed, when source has mostly prenominal adjectives and the target has a very slight preference for postnominal, then STANDARD contains 100% postnominal, whereas ENSEMBLE contains half prenominal / half postnominal (= much closer to the target)โฆ which is exactly what Aufrant et al. advocated for. This sheds a completely new light on ENSEMBLE, and significantly changes the interpretation of the results.
- [Re-A2] Same for the case of performance decrease in case of close languages: a simple interpretation is that close languages have already similar word order ratios for a given label pair, and the discretized constraints move them further. This is exactly the ""French to Italian"" case analyzed by Aufrant et al. So these observations may just be an artefact of the choice to discretize the constraints, not an evidence of the applicability of reordering in general depending on the language pair.
- [Re-A3] And for the remark line 506 on MTOP results opposite to Multilingual-TOP ones: since language sets are different, that again may just be an artefact of the fact that target languages on one side or the other have more deterministic or more balanced word order preferences.

The authors acknowledge on line 1039 that there may be an issue with this discretization, but the analysis does not go as far as observing how that impacts (and possibly invalidates) some of their own conclusions and analyses in the paper. Actually, the mention of ""statistical validity"" (line 1041) looks as if the authors consider that either ""nmod < amod"" or ""amod < nmod"" is the appropriate constraint and when measuring P=0.51 it only prevents to identify which one is, NOT that the appropriate one would indeed be ""half of each"".

**[Re-B]** There is also an issue with the realism of the targeted scenarios. Often when working on low-resourced scenarios, there are irremediable issues that prevent from being fully realistic (e.g. evaluation can only be done when sufficient data exists, so not for actually low-resourced languages). So it is fully OK to assume that such work is done in best-effort mode. But this does not mean discarding the corresponding issues as being irrelevant, but rather to acknowledge them and acknowledge they are not solvable. Plus, care must be taken that the proposed methods would actually be meaningful in a real low-resourced scenario.

More precisely:
- [Re-B1] Regarding the pre-requisite for an UD treebank to estimate the POCs, line 359 accurately observes that UD is available for many languages anyway. However, treebank availability does not mean availability of a treebank with size comparable (hence the same reliability of POC estimation) to the large treebanks used in Table 5 for French, Spanish, or German. So this presumably overestimates a lot the quality of the POCs estimated in actually low-resourced settings. In particular, the treatment made of footnote 6 is questionable: precisely this is a real-world scenario of low-resourced language, so the conclusion ""impossible to extract"" raises major questions on the applicability of the method to actual low-resourced cases.
- [Re-B2] The scenario for estimating POCs without a treebank also does not seem very convincing. If using annotation projection to produce a treebank in the target language, why only using it for estimating POCs (and then training on a treebank from a different language), rather than directly using the projected trees as training treebank? And same for the other tasks, if there is parallel data to do annotation projection, then isn't it appropriate to project the available TOP & RC annotations through that parallel corpus, instead of resorting to a convoluted approach through another corpus and through data transforms? Or has this issue already been thought of, and were there specific reasons to prefer that approach?

**[Re-C]** Finally there is a number of places where the analysis is too shallow, or sometimes completely missing:
- [Re-C1] For the relation-classification task, lines 524-534 only describe results, without any comment on what can be concluded from those results.
- [Re-C2] The remark line 502 on English vs French overlooks that English and French are indeed related, but have nevertheless a number of important typological differences (pre/postnominal adjectives for instance), so it is unsurprising to observe gains. What is the concept of ""typologically distant"" used here?
- [Re-C3] Only English is used as source language, which is clearly not neutral when considering typology (English being rather atypical among the 7000 languages, regarding typology). Surely these experiments are computationally expensive, so experimenting with many sources was probably not doable. But the motivations and impact of that choice would have deserved at least a comment.
- [Re-C4] In Table 5, a number of s2s results are so low that it is hard to consider that the model does anything (or at least, it does not outperform a crude heuristic such as ""dependency to the left neighbour, with the majority-class as relation label""). This raises many questions on how to interpret the score increase, so this would have at least deserved a comment (more than just calling it ""subpar"").","[Question A] Lines 75-80, I don't understand the point made here. Isn't ""use pre-nominal adjectival modification"" the same as ""high probability for adjectival modifiers to precede the headword""? What difference did the authors want to make here?

[Question B] Line 280 states ""assuming that the target language does not have determiners"", but it is unclear how this information has been used in the given example (has it?). How is the absence of determiners accounted for in the method? (Side remark related to [Re-A] above: Aufrant et al. also considered the case of absent determiners)

[Question C] Line 300 mentions an approach to estimate POCs without a treebank, but it is not clear where in the paper this is experimented with. For which languages has this method been used, and in which Table are the corresponding results?

[Question D] In the ยง4.2.1 experiment, what has been done exactly for the Thai data? Because line 390 states ""never use the same dataset"", but both line 381 and Table 6 mention Thai-PUD. Which one is correct?

[Question E] Line 447 mentions the use of TAC English for train and Trans-TAC for test, but Trans-TAC is already a translation of TAC English, so there may be leaking. Is it the case that the Trans-TAC data used is solely translated from the **test** split of TAC English? Can you clarify and justify why this is sound evaluation?

[Question F] For the few-shot scenario (line 518 and Table 4), is the experiment conducted with the models based on XLM or mT5?

[Question G] Line 1025, how does it help mitigating conflicts to discard the irrelevant constraints? If those constraints are not applicable to that subtree (because root labels do not match), then they do not take part in the computation of the solver, right? So why would they be the ones preventing the solver from finding an ordering?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Missing a key reference: Aufrant et al. (2016), Zero-resource Dependency Parsing: Boosting Delexicalized Cross-lingual Transfer with Linguistic Knowledge

For section ยง2 it is appropriate to mention, because a) it has anteriority over Liu et al. (2020a) on the POS-based language model idea, and b) it also includes an instance of the ""using WALS"" approach but with source-side (training-time) reordering, as this work does (whereas Meng et al. 2019 is at inference time).

But perhaps more importantly, it already provides answers to some of the issues encountered in this work: how reordering can be detrimental, why avoiding deterministic reorderings, etc. See [Re-A] above.","- Line 56, it is not discussed what ""old"" and ""new"" mean here, so it is hard to understand what phenomenon is referred to. This seems very marginal compared to the topic of the paper, so maybe not worth a long discussion, but is it possible to rephrase in a more explicit manner?
- Typo line 107: suggest --> suggests
- Typo lines 167, 221, 251, 259, 267: constrains --> constraints
- Line 261 (and possibly elsewhere), sub-tree --> subtree (to match the spelling in the rest of the paper)
- Typo line 265: on of --> on
- Typo line 256: the the --> the
- Typo line 490: the the --> the
- Typo line 531: then --> than
- Table 2 formatting: it would be useful to display differently the results that are an increase and a decrease (to see more easily which languages are concerned). Use for instance bold, underline, decorate with an asterisk afterโฆ at least for Table 2 since this is explicitly discussed at line 474, but if possible for other Tables as well.
- Tables 4 & 5: use the same precision for all numbers (e.g. 67.0 instead of 67 for Fr-300-Base)
- Typo line 587: tends --> trends
- Typo line 594: suggest --> suggests
- Typo line 604: able boost --> able to boost
- Typo line 620: results --> result
- Typo line 628: suggests --> suggest
- Typo line 633: remove ""more""
- Line 655, ""multiply-parallel"" is awkward, usually this is called ""multi-parallel""",1691240000000,,,EMNLP/2023/Conference,z8gM4ZfK8l,"['EMNLP/2023/Conference/Submission4095/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission4095/Reviewer_W6Rf']",z8gM4ZfK8l,['EMNLP/2023/Conference/Submission4095/Reviewer_W6Rf'],1691240000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4095/Reviewer_W6Rf']","Yes, the peer review suggests that the authors should refer to Aufrant et al. (2016) and discuss its findings on reordering methods for dependency parsing, which may provide insights into some of the issues encountered in this work.

The review also mentions several typos and grammatical errors throughout the paper, which should be corrected to improve the clarity and readability of the text.

Additionally, the reviewer suggests rephrasing certain sentences to make them more explicit and easier to understand.

Overall, the peer review is positive about the paper's contributions but raises some concerns about its methodology, interpretation of results, and presentation. The authors should address these points to strengthen their argument and improve the paper's overall quality."
LohakslYF7,"This paper introduces a multi-grained sparse learning framework designed to acquire a shared, aligned sparse space for the purpose of video-text retrieval tasks.

The authors adopt a supervised approach to learning and continuously updating the shared sparse space for text and video representations. This is achieved through the incorporation of the proposed similarity and alignment losses. Furthermore, the paper suggests incorporating a multi-grained similarity approach in the context of video retrieval tasks.","The conducted experiments aptly showcase the effectiveness of the proposed method. Notably, this paper stands out for its comprehensive ablation study and meticulous analysis of each individual module.
","In essence, this paper presents an integration of global video-text and local frame-text elements within both the introduced sparse space and the original dense space, all aimed at enhancing video retrieval. However, it's worth noting that the impact of the introduced sparse space alone is not thoroughly elucidated in the analysis. The method itself should be multi-space multi-grained learning framework for video retrieval.
","A. Table 6 showcases that the most optimal performance is attained by employing the multi-space multi-grained similarity computation. Nonetheless, it is important to underscore that the analysis regarding the influence of the introduced sparse space does not encompass its individual performance outcomes.

B. Referring to your assertion of achieving the state-of-the-art (SOTA) status, it might be appropriate to reconsider. To the best of my knowledge, several other published methods have surpassed your results, including CLIP-ViP [1], Cap4Video [2], DRL [3], and TemPVL [4].

[1] Xue, Hongwei, et al. ""Clip-vip: Adapting pre-trained image-text model to video-language representation alignment.""
[2] Wu, Wenhao, et al. ""Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?""
[3] Wang, Qiang, et al. ""Disentangled representation learning for text-video retrieval.""
[4] Ma, Fan, et al. ""Temporal perceiving video-language pre-training.""","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691750000000,,,EMNLP/2023/Conference,zeGXjQYhXz,"['EMNLP/2023/Conference/Submission4649/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission4649/Reviewer_Vvmb']",zeGXjQYhXz,['EMNLP/2023/Conference/Submission4649/Reviewer_Vvmb'],1691750000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4649/Reviewer_Vvmb']","Yes, the peer review suggests that the authors should refer to specific literature that are not already discussed in the original paper. In particular, the reviewer mentions several papers ([1]-[4]) that have achieved better results than the current paper's claimed state-of-the-art performance, and asks the authors to reconsider their assertion of achieving SOTA status. The reviewer also provides references for these papers, indicating that they are notable works in the field that should be taken into account when evaluating the contribution of the current paper."
gN2femrVDO,"The paper introduces a new inter annotator agreement metric,  HolisticIAA, based on sentence embedding similarity. The metric is compared against two traditional IAA metrics, Cohen's k and Krippendorffโs alpha, for the annotation process of one dataset on persuasion classification of text snippets.","The proposed metric allows to compute annotator agreement even if the annotators did not label the same documents, i.e., by pooling from the corpus similar sentences to those annotated by the annotators (using sentence embeddings) and computing label agreement on that set instead","Inter annotator agreement statistics are useful to measure the reliability of an annotation scheme, i.e, that the coders (aka annotators) have internalized the coding instructions s.t. a sufficient level of agreement can be observed), but are not informative about the quality of a dataset. Agreement is flawed for many reasons, e.g., agreement in mistakes, agreement due to label biases, large chance agreement in datasets with skewed classes, headline measurements with no information about the quality of the individual labels, and many more. 

Also, I am not convinced about using as a form of evaluation the correlation with an existing IAA metric. Mainly because these metrics are already biased in some form, e.g., the kappa paradox.

Lastly, the quality of the sentence embeddings and the similarity thresholds seem central to the success of the proposed metric, however, their selection is treated rather lightly in the paper.",I would like to see addressed the concerns raised under 'Reasons to reject',4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Inter-coder agreement for computational linguistics -> offers a good read into the utility and shortcomings of IAA. 

The paper above was updated and can be found part of a larger study into statistical methods for annotation analysis, part of this recent book: Statistical methods for annotation analysis

The aforementioned book starts by presenting coefficients of agreement, useful for assessing the reliability of an annotation scheme, then moves on to measures of dataset and annotator quality, to probabilistic models of annotation, in particular. On the latter topic, the following resources are also good starting points:

A recent EACL tutorial on 'Aggregating and Learning from Multiple Annotators' and these two TACL papers: The Benefits of a Model of Annotation, and Comparing Bayesian models of annotation","line 537 -> Table* 6
line 646 -> without in advance, or early",1691090000000,,,EMNLP/2023/Conference,zIgc1Qeceh,"['EMNLP/2023/Conference/Submission4608/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4608/Reviewer_Jcj8']",zIgc1Qeceh,['EMNLP/2023/Conference/Submission4608/Reviewer_Jcj8'],1691090000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4608/Reviewer_Jcj8']","Yes, this peer review suggests that the authors should refer to additional literature on inter-coder agreement for computational linguistics (specifically a paper called ""Inter-coder agreement for computational linguistics"") as it offers a good read into the utility and shortcomings of IAA."
jYwtGQBcSp,"This paper tackles the zero-shot intent detection tasks and proposes a two-stage zero-shot bert adapters (Z-BERT-A), which first leverages a dependency parser to extract a set of potential intents, then uses NLI methods relying on Bert models to classify on the candidate classes. Experimental results show this method can outperform a wide variety of baselines in both known intents zero-shot classification and unseen intent discovery.","1. This paper focus on important tasks of both known intents zero-shot classification and unseen intent discovery, and can leverages dependency parsers to enhance the intent generation process.
2. Experimental results show the proposed methods are effective in zero-shot intent detection.
","1. This work is better suited as a demo track paper, rather than a regular long paper.
2. The idea of using NLI to handle zero-shot learning tasks are quite common.
","In section 4, the intent generation process generates the candidates novel class for intent classification, but I wonder for a set of unseen classes, how to normalize same intention with different utterances, and how to determine the total number of new intent classes?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1690806939386,,,EMNLP/2023/Conference,zdMislOLTv,"['EMNLP/2023/Conference/Submission117/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461009639,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission117/Reviewer_wrS3']",zdMislOLTv,['EMNLP/2023/Conference/Submission117/Reviewer_wrS3'],1690806939386,1701461009639,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission117/Reviewer_wrS3']","No, this peer review does not explicitly suggest the authors to refer to specific literature that are not already discussed in the original paper. The reviewer's comments are focused on evaluating the strengths and weaknesses of the paper, asking clarifying questions about certain aspects, and providing feedback on the soundness, excitement, reproducibility, and ethical concerns of the work."
QfmhxfSjSl,"1. This paper introduces CHNLS, a benchmark for Chinese lexical substitution (LS).
2. CHNLS contains 33,695 instances and 144,708 substitutes, encompassing various domains such as News, Novel, and Wikipedia.
3. The dataset exhibits both high quality and extensive coverage.
4. CHNLS may promote the development of lexical substitution in Chinese.

","1.The first benchmark for Chinese lexical substitution (LS).
2. The mainstream models have undergone comprehensive evaluation.
3. being clear and well-written

","1. Lack elucidation of certain pertinent indicators. such as ""best-m,"" ""oot,"" and ""oot-m,""
2. Chinese translator to translate english sentences into Chinese may introduce noise.
3. Lack  baseline test for  LLMs.","Question A: Have you tried using chatGPT or other LLMs to produce data? 

","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",No,No,,1691772620474,,,EMNLP/2023/Conference,zaBPb6Pu21,"['EMNLP/2023/Conference/Submission3500/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461233221,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3500/Reviewer_tGuY']",zaBPb6Pu21,['EMNLP/2023/Conference/Submission3500/Reviewer_tGuY'],1691772620474,1701461233221,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3500/Reviewer_tGuY']","No. The review does not suggest that the authors should refer to any additional literature. In fact, it mentions ""No"" under ""Justification for Ethical Concerns: No"" and ""Missing References: No"". This implies that the reviewer has carefully checked the paper's references and did not identify any missing ones."
iNf3nsHt5m,"The paper presents an interesting approach to reordering for multilingual NLP, which instead of reordering the words of sentence, it reorders the sub-trees of its universal dependencies parse tree. The proposed method is shown to outperform previous approaches both in zero- and few-shots settings.","[1] The proposed method is simple, effective, and particularly suited to post-hoc interpretability.

[2] The paper considers both few-shot and zero-shot experiments.

[3] The ""Related work"" section is very informative and clear. Personally, I do not work on machine translation, and I really enjoyed quality of the overview. 

[4] Interestingly, the proposed method is associated with a higher performance gain in the seq2seq setting, highlighting a difficulty of encoder-decoder architectures in dealing with variable word order patterns.","[1] I think that the study could greatly benefit from an objective quantification of the typological / word-order distance between the languages considered. One of the point I found most interesting about this work is that there seems to be an increase in performance for languages that are distant from English (e.g., Japanese, Hindi), and a decrease in performance (sometimes) for languages that are close to English (e.g., Spanish, German, especially with mT5). It would be great to assess this trend more formally, with a metric of typological similarity (e.g., using WALS). 

[2] The authors state that their approach is suited for interpretability; however, the way POCs can be interpreted is never truly addressed in the body of the paper. It would have been a very nice follow-up analysis/appendix.","[1] l. 477 What about Irish? There seems to be an advantage of your method there.

[2] ll. 478-479 ""No noticeable effect is observed for structurally closer languages."" However there is once again a decrease in performance in Arabic, and a terrible drop in Persian!

[3] ll. 512-513 What do you mean by significant? Did you test it with some statistical test? One option could be the McNemar test on paired nominal data. It is true that the difference is numerically small, but statistical significance (p-values) depend on the variance of the samples. Very small differences can be statistically significant with large sample sizes. 

[4] ll. 528-529 How do you explain the drop in performance in Hindi and Telugu? It's in contrast with your previous observation, where more typologically distant languages benefited the most from your approach.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,5: Could easily reproduce the results.,No,"2: Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,"Joshi et al., 2020. The State and Fate of Linguistic Diversity and Inclusion in the NLP World
- Could be useful for the case you're making in the introduction","[1] Highlighting the best results per condition in bold in the tables would increase readability a lot!

[2] Why is table 2 discussed before table 1?",1690400000000,,,EMNLP/2023/Conference,z8gM4ZfK8l,"['EMNLP/2023/Conference/Submission4095/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission4095/Reviewer_YA2U']",z8gM4ZfK8l,['EMNLP/2023/Conference/Submission4095/Reviewer_YA2U'],1690400000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4095/Reviewer_YA2U']","Based on the peer review, it seems that the reviewer suggests the authors to refer to Joshi et al., 2020 (The State and Fate of Linguistic Diversity and Inclusion in the NLP World) as a useful reference for their case in the introduction.

Additionally, the reviewer mentions that the authors may want to consider referring to literature on typological similarity metrics, such as WALS (World Atlas of Language Structures), which could be used to formally assess the trend mentioned by the reviewer."
BpSd8T2hop,"
This paper deals with a practical problem, namely the regular updates of the UMLS, which involve the integration of new terminological vocabularies. The papers proposes a methodology which could be used to speed up the updates, by automatically finding whether
a new term is a potential synonymous of a term already in the UMLS, or it can be attributed to a novel concept, not yet present in the UMLS.

The paper proposes a different conceptualization of the problem compared to previous approaches. Rather then simply evaluating the similarity of a new term to existing terms, the paper proposes an approach where each new term is assessed in relation to the entire
UMLS, and either a concept is found to which the term can be assigned, or the term is maked as novel, and a new concept will have
to be created. I am not completely convinced that this different conceptualization can be considered innovative, as it seems to me to
be entirely derivable from the original one.


","
Well developed study of the problem of UMLS update, which is framed as a problem similar but not identical to (biomedical) entity linking.
Well designed experimental setup, with one particular UMLS update used as a reference for training/development/testing, and other
updates used for further testing of the results. Interesting combination of rule-based and BERT-based methods. Good evaluation.
","
The problem tackled by the authors is extremely specific, and has a very narrow application, although the methods could probably be generalized to similar problems of knowledge base update.

The evaluation metrics are not sufficiently clearly described, but it might be just a question of providing a more formal definition.","
Please provide a more accurate description of the evaluation metrics in section 4.2.  I suggest to use formulas.

In particular the ""ranking accuracy"", which is a central metric in the paper because it is the one where the most
improvements are seen, is very superficially defined. It is not clear if it is a proper ranking metric, as the name
suggest, or a measure of accuracy. If it is a ranking metric, please explain in which sense it provides a measure
of the quality of the ranking. If it is not, please name it differently. 

I also struggled a bit to understand the definition of New Concept Precision"" because it is not immediately
obvious what is the different between ""correct new concept predictions"" and ""true new concept atoms"", but
I believe I understood it. 

In any case, formulas would help understanding and remove some ambiguity.

Figure 2, not clear what the ""New Concept"" as input to the Re-Ranker represents.




","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"
Relevant to biomedical entity linking, section 2.2

Furrer L, Cornelius J, Rinaldi F. Parallel sequence tagging for concept recognition. BMC Bioinformatics. 2022 Mar 24;22(Suppl 1):623. doi: 10.1186/s12859-021-04511-y. PMID: 35331131; PMCID: PMC8943923.","
line 127-129 emphasis inappropriate in my view
",1691180000000,,,EMNLP/2023/Conference,z9CqYTwOiO,"['EMNLP/2023/Conference/Submission3586/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3586/Reviewer_SKdh']",z9CqYTwOiO,['EMNLP/2023/Conference/Submission3586/Reviewer_SKdh'],1691180000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3586/Reviewer_SKdh']","Yes, the peer review suggests that the authors should consider referring to additional literature on biomedical entity linking, specifically the paper ""Parallel sequence tagging for concept recognition"" by Furrer et al. (2022), as it is relevant to their work but was not mentioned in the original paper."
ECM8Hkmxdl,"This paper studies one cognitive bias, primacy effect, in ChatGPT, which tends to select labels that appear earlier in the context. And they find that ChatGPT's performance is sensitive to the order of labels in the prompt and they tend to select labels in earlier positions.",1. Their evaluation of primacy effect in ChatGPT is interesting.,"1. Performing experiments on more tasks might make the claim stronger, for example, other classification tasks or even generation tasks like QA/summarization.
2. It's better to shuffle the labels more times to prove the primacy effect.",Does Chain-of-thoughts prompting which increase the number of words in the answer improve such cognitive bias?,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Mitigating Label Biases for In-context Learning
Symbol tuning improves in-context learning in language models",,1691110000000,,,EMNLP/2023/Conference,zEJFYWWmbG,"['EMNLP/2023/Conference/Submission649/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701460000000,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission649/Reviewer_SvBZ']",zEJFYWWmbG,['EMNLP/2023/Conference/Submission649/Reviewer_SvBZ'],1691110000000,1701460000000,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission649/Reviewer_SvBZ']","No, the peer review does not suggest that the authors should refer to specific literature that is not already discussed in the original paper. The peer review mentions ""Missing_References"" but lists only two references: ""Mitigating Label Biases for In-context Learning"" and ""Symbol tuning improves in-context learning in language models"". These references seem to be related to the topic of the paper, but they are likely suggested as additional resources that the authors could consult to strengthen their study, rather than new literature that is not already discussed."
LIuV4fZqAO,"The paper's key contributions are as follows:

1. The paper proposes an inductive prompting method inspired by cognitive functions of inductive reasoning. This method guides language models (LLMs), specifically GPT-3, to generate knowledge statements that establish reasoning paths. 

2.  The paper introduces the Induction-Augmented Generation (IAG) framework, which enhances the traditional Retrieval-Augmented Generation (RAG) architecture. 

3. The paper presents two variants of the IAG framework. IAG-GPT leverages the inductive knowledge statements sampled from GPT-3 as evidence for the generator.","This paper offers several strengths and benefits:

1.The paper introduces a novel approach, Induction-Augmented Generation (IAG), which effectively combines inductive reasoning with language generation for answering implicit reasoning questions. 

2.  Implicit reasoning questions pose a significant challenge for open-domain question answering systems. By focusing on this challenge, the paper contributes to solving an important problem in the field of NLP, advancing the state of the art in understanding and generating reasoned answers.

3. The paper presents a well-defined framework (IAG) and a detailed methodology for integrating inductive knowledge into the answer generation process.","there are also some potential weaknesses:

1.  The proposed Induction-Augmented Generation (IAG) framework involves multiple components, including retrieval, induction, and generation, which might make it challenging for researchers to reproduce and implement the approach. 

2.  The paper heavily relies on external language models, such as GPT-3, for generating inductive knowledge and improving performance. This reliance raises concerns about the availability, cost, and access to these models, which could limit the adoption of the proposed approach by researchers with limited resources or access.


3.  While the paper highlights successful cases where inductive knowledge enhances answer prediction, it does not thoroughly analyze or discuss cases where the approach might fail or provide incorrect answers. Understanding the limitations and potential pitfalls of the IAG framework is crucial for its safe and reliable application.",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,,,1691695718152,,,EMNLP/2023/Conference,zwqDROxClj,"['EMNLP/2023/Conference/Submission1595/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461113219,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission1595/Reviewer_bY1q']",zwqDROxClj,['EMNLP/2023/Conference/Submission1595/Reviewer_bY1q'],1691695718152,1701461113219,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1595/Reviewer_bY1q']","No, this peer review does not suggest the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer's comments focus on evaluating the strengths and weaknesses of the paper itself, without mentioning any external sources or literature that should be referenced by the authors. The reviewer provides their assessment of the paper's contributions, soundness, excitement, reproducibility, and ethical concerns based on the content presented within the paper."
