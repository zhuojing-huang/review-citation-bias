id,Paper_Topic_and_Main_Contributions,Reasons_to_accept,Reasons_to_reject,Questions_for_the_Authors,Soundness,Excitement,Reproducibility,Ethical_Concerns,Reviewer_Confidence,Justification_for_Ethical_Concerns,Missing_References,Typos_Grammar_Style_and_Presentation_Improvements,cdate,ddate,details,domain,forum,invitations,license,mdate,nonreaders,number,odate,pdate,readers,replyto,signatures,tcdate,tmdate,writers,response,binary_label,suggested_years,paper_info
9QA0K7TTcH,"The paper describes a party-specific legal document summarization system, moving from a newly-annotated dataset to a system that seems useful to help understand leases.

Overall, I found this paper to be pretty strong, with my main concern being regarding the reliability of the annotation process.","Overall, I am firmly on the fence for this paper. Overall I liked it and found it informative, but am not sure if the underlying annotation reliability issue is a showstopper at this time.

- Problem is well chosen, but motivation has room for improvement

Legal documents touch a wide variety of people, many of whom are ill-prepared to understand and execute contracts correctly. Improvements in allowing people to do so has obvious societal benefit

- Paper describes the pipeline well

I think I could work with a student and stand up something similar","Overall, while I am on the fence and this section is longer than the ""reasons to accept,"" I think all but the first concern (annotation reliability) are fixable within this round of review.

- Not clear that the data annotations are reliable

L142-163 describe how the dataset is composed of US contracts, but Appendix A.1 mentions that the lawyers annotating these contracts are based in India. While this experimental design decision makes sense as a cost-control mechanism, there are substantial differences in the legal systems of these two jurisdictions. This means it bears some discussion about how this potential mismatch between annotator and data corpus might affect the labelling and downstream systems that learn from this labelling. Similarly, it isn't clear that this data set size is sufficient to train a high-capacity model.

- Not clear that (or how) this extends to more complicated legal frameworks.

The paper restricts itself to lease agreements, which is fine (around L90). However, the paper should contain some discussion about generalization questions beyond what we find around L680 in the limitations section. As I read the paper, I was brainstorming a bit about examples beyond ""employment agreements,"" because those share the 2-party asymmetric role structure of a lease agreement. In particular, I thought about multilateral trade agreements, which are multi-party and receive far more scrutiny than housing rental contracts due to the higher stakes. As is, I don't envision many prospective renters or landlords using such a system beyond curiosity. While I recognize that expanding the universe of contracts studied will be laborious, this concern can be dealt with more cheaply by painting a vision early in the paper of the future where diverse types of legal documents have tools to assist laypersons parse said documents, then present this paper as a necessary step along that path.

- Expand on several concepts...

1. ContraSUM methodology: Most of the novelty in this paper lies in the methodology to handle 2-party contracts with content categorization and so on. Parts of the document focusing on results that are peculiar to housing leases are less interesting and are candidates to shorten (e.g., around L240).
2. Why the BWS scheme described around line 180 is important: I recognize the savings that comes from providing a partial ordering in this way, but it is not clear why this ordering is important in the first place. It seems like Appendix figures 4 and 5 list the sentences in an order based on this importance scores. However, it does not appear to be used for filtration or anything further. Given the design decision that the user will observe all the prohibitions/entitlements/obligations anyway, it seems plausible just list the detected sentences in the order that they appear in the document. In so doing, the labelling effort would be substantially lighter.
3. Content categorizer baselines: Providing 1-2 sentences saying what the model is and by what measure the prior work deemed it ""best performing"" around L446 would be helpful to a reader. 
4. Footnote 3: It isn't clear to me as a reader what the conceptual separation between entitlement and permission categories are. This whole section would benefit from greater clarity on the definition of all of these categories, as they are critical to understanding the paper.","There are some questions in my review, the answers will not particularly impact my scoring of the paper. I mostly pose questions in reviews to expose areas I found confusing as a reader or help prompt others' thinking.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"[Wilson] Wilson et al. The Creation and Analysis of a Website Privacy Policy Corpus. ACL 2016. https://aclanthology.org/P16-1126/

...has also curated a dataset based on legal expert annotations.

[Fok] Fok et al. Scim: Intelligent Skimming Support for Scientific Papers. ACM IUI 2023. https://dl.acm.org/doi/abs/10.1145/3581641.3584034

...presents an extractive summarization system for helping humans process academic documents using a slightly different taxonomy but a similar overall approach.","- The bit about LLMs around L560 felt a bit hollow

As written, the paper seems to insinuate that ChatGPT DID produce output on this problem (or perhaps output on the meta-problem of being asked if it is capable of doing this problem), and that this answer indicated unsatisfactory performance for various reasons. If that is the case, the paper should say more about what the ""hallucinations"" looked like and so on. If that is not the case, the paper should clarify the non-pursuit of that line of inquiry for the provided reasons, as opposed to the pursuit of that line of inquiry and discovery that it was not promising. 

- Consider reorganizing Introduction and related work.

While I know placing related work before conclusion is conventional by some standards, it is not my preferred formatting. In this case, it has caused background to blend into both introduction and related work. Further, the last line of section 8 is moving into methodology, suggesting that the paragraph contains a nice segue from this section into what is currently section 4.",1691808547111,,,EMNLP/2023/Conference,4GmujJSuq0,"['EMNLP/2023/Conference/Submission4094/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461271907,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission4094/Reviewer_at5K']",4GmujJSuq0,['EMNLP/2023/Conference/Submission4094/Reviewer_at5K'],1691808547111,1701461271907,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4094/Reviewer_at5K']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wilson et al. (2016) - ""The Creation and Analysis of a Website Privacy Policy Corpus"" (ACL 2016)
2. Fok et al. (2023) - ""Scim: Intelligent Skimming Support for Scientific Papers"" (ACM IUI 2023)

These citations might be necessary because the reviewer mentions that Wilson et al. have also curated a dataset based on legal expert annotations, which is relevant to the paper's topic of legal document summarization. Similarly, Fok et al.'s work on an extractive summarization system for academic documents using a similar approach could be relevant to the paper's methodology. The reviewer suggests that including these references could provide additional context and support for the paper's contributions.",1,"2016, 2016, 2023, 2023",The Creation and Analysis of a Website Privacy Policy Corpus
9QA0K7TTcH,"The paper describes a party-specific legal document summarization system, moving from a newly-annotated dataset to a system that seems useful to help understand leases.

Overall, I found this paper to be pretty strong, with my main concern being regarding the reliability of the annotation process.","Overall, I am firmly on the fence for this paper. Overall I liked it and found it informative, but am not sure if the underlying annotation reliability issue is a showstopper at this time.

- Problem is well chosen, but motivation has room for improvement

Legal documents touch a wide variety of people, many of whom are ill-prepared to understand and execute contracts correctly. Improvements in allowing people to do so has obvious societal benefit

- Paper describes the pipeline well

I think I could work with a student and stand up something similar","Overall, while I am on the fence and this section is longer than the ""reasons to accept,"" I think all but the first concern (annotation reliability) are fixable within this round of review.

- Not clear that the data annotations are reliable

L142-163 describe how the dataset is composed of US contracts, but Appendix A.1 mentions that the lawyers annotating these contracts are based in India. While this experimental design decision makes sense as a cost-control mechanism, there are substantial differences in the legal systems of these two jurisdictions. This means it bears some discussion about how this potential mismatch between annotator and data corpus might affect the labelling and downstream systems that learn from this labelling. Similarly, it isn't clear that this data set size is sufficient to train a high-capacity model.

- Not clear that (or how) this extends to more complicated legal frameworks.

The paper restricts itself to lease agreements, which is fine (around L90). However, the paper should contain some discussion about generalization questions beyond what we find around L680 in the limitations section. As I read the paper, I was brainstorming a bit about examples beyond ""employment agreements,"" because those share the 2-party asymmetric role structure of a lease agreement. In particular, I thought about multilateral trade agreements, which are multi-party and receive far more scrutiny than housing rental contracts due to the higher stakes. As is, I don't envision many prospective renters or landlords using such a system beyond curiosity. While I recognize that expanding the universe of contracts studied will be laborious, this concern can be dealt with more cheaply by painting a vision early in the paper of the future where diverse types of legal documents have tools to assist laypersons parse said documents, then present this paper as a necessary step along that path.

- Expand on several concepts...

1. ContraSUM methodology: Most of the novelty in this paper lies in the methodology to handle 2-party contracts with content categorization and so on. Parts of the document focusing on results that are peculiar to housing leases are less interesting and are candidates to shorten (e.g., around L240).
2. Why the BWS scheme described around line 180 is important: I recognize the savings that comes from providing a partial ordering in this way, but it is not clear why this ordering is important in the first place. It seems like Appendix figures 4 and 5 list the sentences in an order based on this importance scores. However, it does not appear to be used for filtration or anything further. Given the design decision that the user will observe all the prohibitions/entitlements/obligations anyway, it seems plausible just list the detected sentences in the order that they appear in the document. In so doing, the labelling effort would be substantially lighter.
3. Content categorizer baselines: Providing 1-2 sentences saying what the model is and by what measure the prior work deemed it ""best performing"" around L446 would be helpful to a reader. 
4. Footnote 3: It isn't clear to me as a reader what the conceptual separation between entitlement and permission categories are. This whole section would benefit from greater clarity on the definition of all of these categories, as they are critical to understanding the paper.","There are some questions in my review, the answers will not particularly impact my scoring of the paper. I mostly pose questions in reviews to expose areas I found confusing as a reader or help prompt others' thinking.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"[Wilson] Wilson et al. The Creation and Analysis of a Website Privacy Policy Corpus. ACL 2016. https://aclanthology.org/P16-1126/

...has also curated a dataset based on legal expert annotations.

[Fok] Fok et al. Scim: Intelligent Skimming Support for Scientific Papers. ACM IUI 2023. https://dl.acm.org/doi/abs/10.1145/3581641.3584034

...presents an extractive summarization system for helping humans process academic documents using a slightly different taxonomy but a similar overall approach.","- The bit about LLMs around L560 felt a bit hollow

As written, the paper seems to insinuate that ChatGPT DID produce output on this problem (or perhaps output on the meta-problem of being asked if it is capable of doing this problem), and that this answer indicated unsatisfactory performance for various reasons. If that is the case, the paper should say more about what the ""hallucinations"" looked like and so on. If that is not the case, the paper should clarify the non-pursuit of that line of inquiry for the provided reasons, as opposed to the pursuit of that line of inquiry and discovery that it was not promising. 

- Consider reorganizing Introduction and related work.

While I know placing related work before conclusion is conventional by some standards, it is not my preferred formatting. In this case, it has caused background to blend into both introduction and related work. Further, the last line of section 8 is moving into methodology, suggesting that the paragraph contains a nice segue from this section into what is currently section 4.",1691808547111,,,EMNLP/2023/Conference,4GmujJSuq0,"['EMNLP/2023/Conference/Submission4094/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461271907,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission4094/Reviewer_at5K']",4GmujJSuq0,['EMNLP/2023/Conference/Submission4094/Reviewer_at5K'],1691808547111,1701461271907,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4094/Reviewer_at5K']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wilson et al. (2016) - ""The Creation and Analysis of a Website Privacy Policy Corpus"" (ACL 2016)
2. Fok et al. (2023) - ""Scim: Intelligent Skimming Support for Scientific Papers"" (ACM IUI 2023)

These citations might be necessary because the reviewer mentions that Wilson et al. have also curated a dataset based on legal expert annotations, which is relevant to the paper's topic of legal document summarization. Similarly, Fok et al.'s work on an extractive summarization system for academic documents using a similar approach could be relevant to the paper's methodology. The reviewer suggests that including these references could provide additional context and support for the paper's contributions.",1,"2016, 2016, 2023, 2023",Scim Intelligent Skimming Support for Scientific Papers
wHvluZ2pQW,"This paper focuses on unsupervised text style transfer, which involves modifying the style of input sentences while preserving their content, without relying on parallel data. Building on the framework of (Dai et al., 2019), the authors aim to utilize pre-trained language models (such as GPT-2) and prompt learning techniques to construct generators and discriminators. Furthermore, the paper adopts a recursive approach to improve the interactions between the input sentence and GPT-2. The proposed method is evaluated on Yelp and IMDb datasets, and the experimental results demonstrate its effectiveness.","- Proposing a new method based on GPT-2 for unsupervised text style transfer.
- Introducing three different types of prefixes to encode task-specific information, target style, and content details, thereby providing more comprehensive information to the model compared to the embeddings used in previous works.
- Adopting a recursive approach to improve the interactions between the input sentence and GPT-2.
- Providing comprehensive results, conducting ablation studies, and including subjective evaluations from human assessors to gain a deeper understanding of the proposed approach.","- This paper presents incremental work. The proposed method is highly similar to the framework of (Dai et al., 2019) and incorporates prompt learning techniques that have been previously proposed.
- The evaluation of the proposed method is limited to sentiment transfer tasks. It would be beneficial to explore the performance of the proposed method on other tasks (such as formality transfer) and datasets.
- The experiments in this paper only compare a small number of baselines. It would be advantageous to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019].","- Can the authors provide more details on the specific differences between their proposed method and the framework presented in (Dai et al., 2019)? How does the incorporation of prompt learning techniques contribute to the proposed method?
- While the evaluation of the proposed method focuses on sentiment transfer tasks, are there any plans to explore its performance on other tasks, such as formality transfer? If so, what are the potential challenges and considerations in adapting the method to different tasks?
- The experiments in this paper only compare a limited number of baselines. Are there any specific reasons for choosing these particular baselines? Would it be possible to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019], to provide a more comprehensive comparison?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"- Style Transfer as Unsupervised Machine Translation. Zhang et al., 2018.
- A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer. Luo et al., 2019.",,1691051892382,,,EMNLP/2023/Conference,4AiERjB5JD,"['EMNLP/2023/Conference/Submission529/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461038107,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']",4AiERjB5JD,['EMNLP/2023/Conference/Submission529/Reviewer_cYMW'],1691051892382,1701461038107,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']","Yes

The suggested papers or references not cited in the manuscript are:
1. DelRetri [Li et al., 2018]
2. Template [Li et al., 2018]
3. UnsuperMT [Zhang et al., 2018]
4. DualRL [Luo et al., 2019]
5. Style Transfer as Unsupervised Machine Translation (already cited as Zhang et al., 2018, but mentioned as a missing reference)
6. A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer (already cited as Luo et al., 2019, but mentioned as a missing reference)

These citations might be necessary because the reviewer suggests that the experiments in the paper only compare a limited number of baselines. Including additional baselines, such as DelRetri, Template, UnsuperMT, and DualRL, would provide a more comprehensive comparison and help to better evaluate the proposed method. The reviewer also mentions that the evaluation of the proposed method is limited to sentiment transfer tasks and suggests exploring its performance on other tasks, which might require citing relevant papers in those areas.",1,"2018, 2018, 2018, 2019, 2018, 2019",DelRetri 
wHvluZ2pQW,"This paper focuses on unsupervised text style transfer, which involves modifying the style of input sentences while preserving their content, without relying on parallel data. Building on the framework of (Dai et al., 2019), the authors aim to utilize pre-trained language models (such as GPT-2) and prompt learning techniques to construct generators and discriminators. Furthermore, the paper adopts a recursive approach to improve the interactions between the input sentence and GPT-2. The proposed method is evaluated on Yelp and IMDb datasets, and the experimental results demonstrate its effectiveness.","- Proposing a new method based on GPT-2 for unsupervised text style transfer.
- Introducing three different types of prefixes to encode task-specific information, target style, and content details, thereby providing more comprehensive information to the model compared to the embeddings used in previous works.
- Adopting a recursive approach to improve the interactions between the input sentence and GPT-2.
- Providing comprehensive results, conducting ablation studies, and including subjective evaluations from human assessors to gain a deeper understanding of the proposed approach.","- This paper presents incremental work. The proposed method is highly similar to the framework of (Dai et al., 2019) and incorporates prompt learning techniques that have been previously proposed.
- The evaluation of the proposed method is limited to sentiment transfer tasks. It would be beneficial to explore the performance of the proposed method on other tasks (such as formality transfer) and datasets.
- The experiments in this paper only compare a small number of baselines. It would be advantageous to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019].","- Can the authors provide more details on the specific differences between their proposed method and the framework presented in (Dai et al., 2019)? How does the incorporation of prompt learning techniques contribute to the proposed method?
- While the evaluation of the proposed method focuses on sentiment transfer tasks, are there any plans to explore its performance on other tasks, such as formality transfer? If so, what are the potential challenges and considerations in adapting the method to different tasks?
- The experiments in this paper only compare a limited number of baselines. Are there any specific reasons for choosing these particular baselines? Would it be possible to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019], to provide a more comprehensive comparison?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"- Style Transfer as Unsupervised Machine Translation. Zhang et al., 2018.
- A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer. Luo et al., 2019.",,1691051892382,,,EMNLP/2023/Conference,4AiERjB5JD,"['EMNLP/2023/Conference/Submission529/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461038107,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']",4AiERjB5JD,['EMNLP/2023/Conference/Submission529/Reviewer_cYMW'],1691051892382,1701461038107,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']","Yes

The suggested papers or references not cited in the manuscript are:
1. DelRetri [Li et al., 2018]
2. Template [Li et al., 2018]
3. UnsuperMT [Zhang et al., 2018]
4. DualRL [Luo et al., 2019]
5. Style Transfer as Unsupervised Machine Translation (already cited as Zhang et al., 2018, but mentioned as a missing reference)
6. A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer (already cited as Luo et al., 2019, but mentioned as a missing reference)

These citations might be necessary because the reviewer suggests that the experiments in the paper only compare a limited number of baselines. Including additional baselines, such as DelRetri, Template, UnsuperMT, and DualRL, would provide a more comprehensive comparison and help to better evaluate the proposed method. The reviewer also mentions that the evaluation of the proposed method is limited to sentiment transfer tasks and suggests exploring its performance on other tasks, which might require citing relevant papers in those areas.",1,"2018, 2018, 2018, 2019, 2018, 2019",Template 
wHvluZ2pQW,"This paper focuses on unsupervised text style transfer, which involves modifying the style of input sentences while preserving their content, without relying on parallel data. Building on the framework of (Dai et al., 2019), the authors aim to utilize pre-trained language models (such as GPT-2) and prompt learning techniques to construct generators and discriminators. Furthermore, the paper adopts a recursive approach to improve the interactions between the input sentence and GPT-2. The proposed method is evaluated on Yelp and IMDb datasets, and the experimental results demonstrate its effectiveness.","- Proposing a new method based on GPT-2 for unsupervised text style transfer.
- Introducing three different types of prefixes to encode task-specific information, target style, and content details, thereby providing more comprehensive information to the model compared to the embeddings used in previous works.
- Adopting a recursive approach to improve the interactions between the input sentence and GPT-2.
- Providing comprehensive results, conducting ablation studies, and including subjective evaluations from human assessors to gain a deeper understanding of the proposed approach.","- This paper presents incremental work. The proposed method is highly similar to the framework of (Dai et al., 2019) and incorporates prompt learning techniques that have been previously proposed.
- The evaluation of the proposed method is limited to sentiment transfer tasks. It would be beneficial to explore the performance of the proposed method on other tasks (such as formality transfer) and datasets.
- The experiments in this paper only compare a small number of baselines. It would be advantageous to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019].","- Can the authors provide more details on the specific differences between their proposed method and the framework presented in (Dai et al., 2019)? How does the incorporation of prompt learning techniques contribute to the proposed method?
- While the evaluation of the proposed method focuses on sentiment transfer tasks, are there any plans to explore its performance on other tasks, such as formality transfer? If so, what are the potential challenges and considerations in adapting the method to different tasks?
- The experiments in this paper only compare a limited number of baselines. Are there any specific reasons for choosing these particular baselines? Would it be possible to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019], to provide a more comprehensive comparison?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"- Style Transfer as Unsupervised Machine Translation. Zhang et al., 2018.
- A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer. Luo et al., 2019.",,1691051892382,,,EMNLP/2023/Conference,4AiERjB5JD,"['EMNLP/2023/Conference/Submission529/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461038107,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']",4AiERjB5JD,['EMNLP/2023/Conference/Submission529/Reviewer_cYMW'],1691051892382,1701461038107,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']","Yes

The suggested papers or references not cited in the manuscript are:
1. DelRetri [Li et al., 2018]
2. Template [Li et al., 2018]
3. UnsuperMT [Zhang et al., 2018]
4. DualRL [Luo et al., 2019]
5. Style Transfer as Unsupervised Machine Translation (already cited as Zhang et al., 2018, but mentioned as a missing reference)
6. A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer (already cited as Luo et al., 2019, but mentioned as a missing reference)

These citations might be necessary because the reviewer suggests that the experiments in the paper only compare a limited number of baselines. Including additional baselines, such as DelRetri, Template, UnsuperMT, and DualRL, would provide a more comprehensive comparison and help to better evaluate the proposed method. The reviewer also mentions that the evaluation of the proposed method is limited to sentiment transfer tasks and suggests exploring its performance on other tasks, which might require citing relevant papers in those areas.",1,"2018, 2018, 2018, 2019, 2018, 2019",UnsuperMT 
wHvluZ2pQW,"This paper focuses on unsupervised text style transfer, which involves modifying the style of input sentences while preserving their content, without relying on parallel data. Building on the framework of (Dai et al., 2019), the authors aim to utilize pre-trained language models (such as GPT-2) and prompt learning techniques to construct generators and discriminators. Furthermore, the paper adopts a recursive approach to improve the interactions between the input sentence and GPT-2. The proposed method is evaluated on Yelp and IMDb datasets, and the experimental results demonstrate its effectiveness.","- Proposing a new method based on GPT-2 for unsupervised text style transfer.
- Introducing three different types of prefixes to encode task-specific information, target style, and content details, thereby providing more comprehensive information to the model compared to the embeddings used in previous works.
- Adopting a recursive approach to improve the interactions between the input sentence and GPT-2.
- Providing comprehensive results, conducting ablation studies, and including subjective evaluations from human assessors to gain a deeper understanding of the proposed approach.","- This paper presents incremental work. The proposed method is highly similar to the framework of (Dai et al., 2019) and incorporates prompt learning techniques that have been previously proposed.
- The evaluation of the proposed method is limited to sentiment transfer tasks. It would be beneficial to explore the performance of the proposed method on other tasks (such as formality transfer) and datasets.
- The experiments in this paper only compare a small number of baselines. It would be advantageous to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019].","- Can the authors provide more details on the specific differences between their proposed method and the framework presented in (Dai et al., 2019)? How does the incorporation of prompt learning techniques contribute to the proposed method?
- While the evaluation of the proposed method focuses on sentiment transfer tasks, are there any plans to explore its performance on other tasks, such as formality transfer? If so, what are the potential challenges and considerations in adapting the method to different tasks?
- The experiments in this paper only compare a limited number of baselines. Are there any specific reasons for choosing these particular baselines? Would it be possible to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019], to provide a more comprehensive comparison?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"- Style Transfer as Unsupervised Machine Translation. Zhang et al., 2018.
- A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer. Luo et al., 2019.",,1691051892382,,,EMNLP/2023/Conference,4AiERjB5JD,"['EMNLP/2023/Conference/Submission529/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461038107,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']",4AiERjB5JD,['EMNLP/2023/Conference/Submission529/Reviewer_cYMW'],1691051892382,1701461038107,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']","Yes

The suggested papers or references not cited in the manuscript are:
1. DelRetri [Li et al., 2018]
2. Template [Li et al., 2018]
3. UnsuperMT [Zhang et al., 2018]
4. DualRL [Luo et al., 2019]
5. Style Transfer as Unsupervised Machine Translation (already cited as Zhang et al., 2018, but mentioned as a missing reference)
6. A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer (already cited as Luo et al., 2019, but mentioned as a missing reference)

These citations might be necessary because the reviewer suggests that the experiments in the paper only compare a limited number of baselines. Including additional baselines, such as DelRetri, Template, UnsuperMT, and DualRL, would provide a more comprehensive comparison and help to better evaluate the proposed method. The reviewer also mentions that the evaluation of the proposed method is limited to sentiment transfer tasks and suggests exploring its performance on other tasks, which might require citing relevant papers in those areas.",1,"2018, 2018, 2018, 2019, 2018, 2019",DualRL 
wHvluZ2pQW,"This paper focuses on unsupervised text style transfer, which involves modifying the style of input sentences while preserving their content, without relying on parallel data. Building on the framework of (Dai et al., 2019), the authors aim to utilize pre-trained language models (such as GPT-2) and prompt learning techniques to construct generators and discriminators. Furthermore, the paper adopts a recursive approach to improve the interactions between the input sentence and GPT-2. The proposed method is evaluated on Yelp and IMDb datasets, and the experimental results demonstrate its effectiveness.","- Proposing a new method based on GPT-2 for unsupervised text style transfer.
- Introducing three different types of prefixes to encode task-specific information, target style, and content details, thereby providing more comprehensive information to the model compared to the embeddings used in previous works.
- Adopting a recursive approach to improve the interactions between the input sentence and GPT-2.
- Providing comprehensive results, conducting ablation studies, and including subjective evaluations from human assessors to gain a deeper understanding of the proposed approach.","- This paper presents incremental work. The proposed method is highly similar to the framework of (Dai et al., 2019) and incorporates prompt learning techniques that have been previously proposed.
- The evaluation of the proposed method is limited to sentiment transfer tasks. It would be beneficial to explore the performance of the proposed method on other tasks (such as formality transfer) and datasets.
- The experiments in this paper only compare a small number of baselines. It would be advantageous to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019].","- Can the authors provide more details on the specific differences between their proposed method and the framework presented in (Dai et al., 2019)? How does the incorporation of prompt learning techniques contribute to the proposed method?
- While the evaluation of the proposed method focuses on sentiment transfer tasks, are there any plans to explore its performance on other tasks, such as formality transfer? If so, what are the potential challenges and considerations in adapting the method to different tasks?
- The experiments in this paper only compare a limited number of baselines. Are there any specific reasons for choosing these particular baselines? Would it be possible to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019], to provide a more comprehensive comparison?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"- Style Transfer as Unsupervised Machine Translation. Zhang et al., 2018.
- A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer. Luo et al., 2019.",,1691051892382,,,EMNLP/2023/Conference,4AiERjB5JD,"['EMNLP/2023/Conference/Submission529/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461038107,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']",4AiERjB5JD,['EMNLP/2023/Conference/Submission529/Reviewer_cYMW'],1691051892382,1701461038107,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']","Yes

The suggested papers or references not cited in the manuscript are:
1. DelRetri [Li et al., 2018]
2. Template [Li et al., 2018]
3. UnsuperMT [Zhang et al., 2018]
4. DualRL [Luo et al., 2019]
5. Style Transfer as Unsupervised Machine Translation (already cited as Zhang et al., 2018, but mentioned as a missing reference)
6. A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer (already cited as Luo et al., 2019, but mentioned as a missing reference)

These citations might be necessary because the reviewer suggests that the experiments in the paper only compare a limited number of baselines. Including additional baselines, such as DelRetri, Template, UnsuperMT, and DualRL, would provide a more comprehensive comparison and help to better evaluate the proposed method. The reviewer also mentions that the evaluation of the proposed method is limited to sentiment transfer tasks and suggests exploring its performance on other tasks, which might require citing relevant papers in those areas.",1,"2018, 2018, 2018, 2019, 2018, 2019",Style Transfer as Unsupervised Machine Translation 
wHvluZ2pQW,"This paper focuses on unsupervised text style transfer, which involves modifying the style of input sentences while preserving their content, without relying on parallel data. Building on the framework of (Dai et al., 2019), the authors aim to utilize pre-trained language models (such as GPT-2) and prompt learning techniques to construct generators and discriminators. Furthermore, the paper adopts a recursive approach to improve the interactions between the input sentence and GPT-2. The proposed method is evaluated on Yelp and IMDb datasets, and the experimental results demonstrate its effectiveness.","- Proposing a new method based on GPT-2 for unsupervised text style transfer.
- Introducing three different types of prefixes to encode task-specific information, target style, and content details, thereby providing more comprehensive information to the model compared to the embeddings used in previous works.
- Adopting a recursive approach to improve the interactions between the input sentence and GPT-2.
- Providing comprehensive results, conducting ablation studies, and including subjective evaluations from human assessors to gain a deeper understanding of the proposed approach.","- This paper presents incremental work. The proposed method is highly similar to the framework of (Dai et al., 2019) and incorporates prompt learning techniques that have been previously proposed.
- The evaluation of the proposed method is limited to sentiment transfer tasks. It would be beneficial to explore the performance of the proposed method on other tasks (such as formality transfer) and datasets.
- The experiments in this paper only compare a small number of baselines. It would be advantageous to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019].","- Can the authors provide more details on the specific differences between their proposed method and the framework presented in (Dai et al., 2019)? How does the incorporation of prompt learning techniques contribute to the proposed method?
- While the evaluation of the proposed method focuses on sentiment transfer tasks, are there any plans to explore its performance on other tasks, such as formality transfer? If so, what are the potential challenges and considerations in adapting the method to different tasks?
- The experiments in this paper only compare a limited number of baselines. Are there any specific reasons for choosing these particular baselines? Would it be possible to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019], to provide a more comprehensive comparison?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"- Style Transfer as Unsupervised Machine Translation. Zhang et al., 2018.
- A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer. Luo et al., 2019.",,1691051892382,,,EMNLP/2023/Conference,4AiERjB5JD,"['EMNLP/2023/Conference/Submission529/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461038107,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']",4AiERjB5JD,['EMNLP/2023/Conference/Submission529/Reviewer_cYMW'],1691051892382,1701461038107,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission529/Reviewer_cYMW']","Yes

The suggested papers or references not cited in the manuscript are:
1. DelRetri [Li et al., 2018]
2. Template [Li et al., 2018]
3. UnsuperMT [Zhang et al., 2018]
4. DualRL [Luo et al., 2019]
5. Style Transfer as Unsupervised Machine Translation (already cited as Zhang et al., 2018, but mentioned as a missing reference)
6. A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer (already cited as Luo et al., 2019, but mentioned as a missing reference)

These citations might be necessary because the reviewer suggests that the experiments in the paper only compare a limited number of baselines. Including additional baselines, such as DelRetri, Template, UnsuperMT, and DualRL, would provide a more comprehensive comparison and help to better evaluate the proposed method. The reviewer also mentions that the evaluation of the proposed method is limited to sentiment transfer tasks and suggests exploring its performance on other tasks, which might require citing relevant papers in those areas.",1,"2018, 2018, 2018, 2019, 2018, 2019",A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer
URwVSgR3D7,"This paper proposes an unsupervised text style transfer method which is an adversarial framework consisting of a generator and a discriminator. Both the generator and discriminator are using GPT-2 as base models. Moreover, the authors use prefix-tuning to finetune the models. They design shared prefix, style prefix, and content prefix to encode corresponding information of the input sequence. The proposed method outperforms baselines in terms of both automatic evaluation and human evaluation from the aspects of style, content, and fluency. ","1.	This work is clearly motivated to do text style transfer and proposes a quite new and interesting.
2.	The adversarial framework of the methodology is reasonable. The prefixes are quite significant according to the experimental results. Moreover, the losses designed for the generator are quite thoroughly considered.
3.	The statement of the methodology and the structure of the paper are good. The implementation is comprehensive. The results are solid.
","1.	Baselines in this paper are limited. There are no baselines from 2022. There a also no baselines of editing-based [1] or prompt-based methods [2].
2.	There is no analysis of the three losses to train the generator.

[1] Machel Reid and Victor Zhong. 2021. LEWIS: Leven- shtein editing for unsupervised text style transfer. In Findings of ACL-IJCNLP, pages 3932–3944. 
[2] Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022. Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2195–2222.
",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691154088643,,,EMNLP/2023/Conference,4AiERjB5JD,"['EMNLP/2023/Conference/Submission529/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461038013,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission529/Reviewer_p4xo']",4AiERjB5JD,['EMNLP/2023/Conference/Submission529/Reviewer_p4xo'],1691154088643,1701461038013,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission529/Reviewer_p4xo']","Yes

The suggested papers or references not cited in the manuscript are:
1. Machel Reid and Victor Zhong. 2021. LEWIS: Levenshtein editing for unsupervised text style transfer.
2. Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022. Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models.

These citations might be necessary because the reviewer mentions that the baselines in the paper are limited, specifically noting the absence of baselines from 2022 and editing-based or prompt-based methods. The reviewer references these two papers as examples of relevant work that is not included in the manuscript, suggesting that they could be useful for comparison or to demonstrate the novelty of the proposed approach.",1,"2021, 2022, 2022",LEWIS Levenshtein editing for unsupervised text style transfer 
URwVSgR3D7,"This paper proposes an unsupervised text style transfer method which is an adversarial framework consisting of a generator and a discriminator. Both the generator and discriminator are using GPT-2 as base models. Moreover, the authors use prefix-tuning to finetune the models. They design shared prefix, style prefix, and content prefix to encode corresponding information of the input sequence. The proposed method outperforms baselines in terms of both automatic evaluation and human evaluation from the aspects of style, content, and fluency. ","1.	This work is clearly motivated to do text style transfer and proposes a quite new and interesting.
2.	The adversarial framework of the methodology is reasonable. The prefixes are quite significant according to the experimental results. Moreover, the losses designed for the generator are quite thoroughly considered.
3.	The statement of the methodology and the structure of the paper are good. The implementation is comprehensive. The results are solid.
","1.	Baselines in this paper are limited. There are no baselines from 2022. There a also no baselines of editing-based [1] or prompt-based methods [2].
2.	There is no analysis of the three losses to train the generator.

[1] Machel Reid and Victor Zhong. 2021. LEWIS: Leven- shtein editing for unsupervised text style transfer. In Findings of ACL-IJCNLP, pages 3932–3944. 
[2] Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022. Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2195–2222.
",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691154088643,,,EMNLP/2023/Conference,4AiERjB5JD,"['EMNLP/2023/Conference/Submission529/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461038013,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission529/Reviewer_p4xo']",4AiERjB5JD,['EMNLP/2023/Conference/Submission529/Reviewer_p4xo'],1691154088643,1701461038013,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission529/Reviewer_p4xo']","Yes

The suggested papers or references not cited in the manuscript are:
1. Machel Reid and Victor Zhong. 2021. LEWIS: Levenshtein editing for unsupervised text style transfer.
2. Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022. Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models.

These citations might be necessary because the reviewer mentions that the baselines in the paper are limited, specifically noting the absence of baselines from 2022 and editing-based or prompt-based methods. The reviewer references these two papers as examples of relevant work that is not included in the manuscript, suggesting that they could be useful for comparison or to demonstrate the novelty of the proposed approach.",1,"2021, 2022, 2022",Prompt-and-Rerank A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models
SZJQKBrGK0,"This work focuses on generating complete summaries for binary functions, particularly stripped binary. To fully exploit the semantics of assembly code, authors present a control flow graph and pseudo code guided binary code summarization model, CP-BCS. CP-BCS uses a bidirectional instruction-level control flow graph and pseudo code with expert knowledge to learn binary function execution behavior and logic semantics. The authors evaluate CP-BCS on three different binary optimization levels (O1, O2, and O3) for three different computer architectures (X86, X64, 025 and ARM).","- The Introduction and Related Works sections present some summary of limitations of existing works, as well as contributions of CP-BCS. 
- The Methodology portions partitions the various components of the architecture well, with components of the: assembly instruction encoder, BI-CFG Encoder, Pseudo Code Encoder, and Summary Decoder. 
- Dataset statistics are provided for broad range of datasets and computer architecture","- There is lack of motivation as to why the problem of investigation is important. For example, the authors may consider answering questions like what the benefits are with generating complete summaries for binary functions from an efficiency or accuracy point of view.  Alternatively, what are the different applications that can be benefited with solving this problem. 
- In the Methodology section, it would be helpful if the authors could introduce a Problem Formulation section to formally describe the problem to be solved and any relevant assumptions and constraints. ","- While different components of the model are described in the Methodology section, there should also be some motivation as to why an autoencoder based architecture is used e.g., why not consider an encoder only architecture with an classier such as MLP component?  
- The size of the datasets are in the thousands, which seem to be quite small. As such, I wonder what the scalability of the proposed approach is. Can the authors provide a section regarding the time complexity of their model? ",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"In the Related Works section, there is a recent work on understanding and capturing semantic meaning from code through a program-derived semantics graph. The authors should reference this work in this section: 
- (NeurIPS-CAP 2020) Software Language Comprehension using a Program-Derived Semantics Graph, https://arxiv.org/abs/2004.00768
",,1690904403654,,,EMNLP/2023/Conference,4AcHxGE6M4,"['EMNLP/2023/Conference/Submission3634/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461242835,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3634/Reviewer_bpsr']",4AcHxGE6M4,['EMNLP/2023/Conference/Submission3634/Reviewer_bpsr'],1690904403654,1701461242835,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3634/Reviewer_bpsr']","Yes

The suggested paper is:
- (NeurIPS-CAP 2020) Software Language Comprehension using a Program-Derived Semantics Graph, https://arxiv.org/abs/2004.00768

This citation is suggested because the reviewer believes it is a relevant recent work on understanding and capturing semantic meaning from code, which is closely related to the topic of the submitted paper. The reviewer thinks that including this reference in the Related Works section would be beneficial, as it would provide a more comprehensive overview of existing research in the field. The reviewer's comment implies that the authors may have missed an important contribution to the area of program semantics, and that citing this work would strengthen the manuscript's discussion of related research.",1,"2020, 2020",Software Language Comprehension using a Program-Derived Semantics Graph
kTrEhe6fsZ,"The authors propose Chain-of-Thought Tuning framework in an attempt to unify the benefits of Masked Language Modeling (MLM) and Chain-of-thought (CoT) in two NLU tasks: hierarchical classification and relation classification. Specifically, the framework is divided into 2 steps (1) Intermediate Step Generation, (2) Target Outcome Prediction via Intermediate Step generated in step (1). Additional Counterfactual-based Contrastive Learning (CCL) and Probability Rectification (PR) are also proposed to effectively integrate intermediate steps into the overall framework.","1. The overall structure of the work as well as motivations for the integration of CoT and individual components is  well written.

2. The adaptation of CoT to NLU tasks is quite interesting and provides fresh perspectives for NLU tasks.","1. The reported performance gain of the proposed framework is marginal when compared to the improvements introduced by simple Prompt Tuning approaches. For instance,for Table 3, out of 2.7% gain over Roberta backbone on ReTACRED, prompting tuning (i.e. HardPrompt) already achieves the gain of 1.7%. 

2. The scope of the study is under-specified. It seems that the work focuses on injecting CoT- based approach to small-scale Language Models. If that is not the case, additional relevant CoT baselines for in-context learning of Large Language Models (for text-003 and ChatGPT) are missing in Table 2 and 3 (See Question A).

3. The major components of the proposed frameworks are CCL and PR. Both of them are incremental over the previous methods with minor adaptation for CoT-based prompting proposal.","A. What is the performance of different CoT variants [1,2,3] of LLMs for the two evaluated tasks? 

B. Why is SoftPrompt not included in Table 3?

C. The interpretation of Figure 3 seems incomplete. For instance, how to determine reasonable decision scope for CoTT based on Figure 3 (Line 592-594)?

D.For CCL, what is the size of the counterfactual step I* (Line 304)? Is there only a single negative sample for CCL? Since CL benefits from larger batch sizes and more negative samples, it would make more sense to enlarge I* for CCL.

[1] Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 2022. 

[2] Wang et al., Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023.

[3] Fu et al., Complexity-Based Prompting for Multi-step Reasoning. ICLR 2023.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"Line 135:  the final chosen answers

Line 519 and 524: , -> .

Tense consistency across the manuscript is recommended.",1691206266874,,,EMNLP/2023/Conference,43SOcneD8W,"['EMNLP/2023/Conference/Submission2612/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461176632,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2612/Reviewer_AVcC']",43SOcneD8W,['EMNLP/2023/Conference/Submission2612/Reviewer_AVcC'],1691206266874,1701461176632,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2612/Reviewer_AVcC']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 2022.
2. Wang et al., Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023.
3. Fu et al., Complexity-Based Prompting for Multi-step Reasoning. ICLR 2023.

These citations might be necessary because the reviewer is asking the authors to provide the performance of different CoT variants of LLMs for the two evaluated tasks (hierarchical classification and relation classification) and notes that relevant CoT baselines for in-context learning of Large Language Models are missing in Tables 2 and 3. The reviewer specifically mentions that the scope of the study is under-specified and that additional relevant CoT baselines for large language models (such as text-003 and ChatGPT) are missing. The suggested papers appear to be relevant to the topic of Chain-of-Thought prompting and its application to large language models, which is a key aspect of the manuscript.",1,"2022, 2023, 2023",Chain-of-Thought Prompting Elicits Reasoning in Large Language Models 
kTrEhe6fsZ,"The authors propose Chain-of-Thought Tuning framework in an attempt to unify the benefits of Masked Language Modeling (MLM) and Chain-of-thought (CoT) in two NLU tasks: hierarchical classification and relation classification. Specifically, the framework is divided into 2 steps (1) Intermediate Step Generation, (2) Target Outcome Prediction via Intermediate Step generated in step (1). Additional Counterfactual-based Contrastive Learning (CCL) and Probability Rectification (PR) are also proposed to effectively integrate intermediate steps into the overall framework.","1. The overall structure of the work as well as motivations for the integration of CoT and individual components is  well written.

2. The adaptation of CoT to NLU tasks is quite interesting and provides fresh perspectives for NLU tasks.","1. The reported performance gain of the proposed framework is marginal when compared to the improvements introduced by simple Prompt Tuning approaches. For instance,for Table 3, out of 2.7% gain over Roberta backbone on ReTACRED, prompting tuning (i.e. HardPrompt) already achieves the gain of 1.7%. 

2. The scope of the study is under-specified. It seems that the work focuses on injecting CoT- based approach to small-scale Language Models. If that is not the case, additional relevant CoT baselines for in-context learning of Large Language Models (for text-003 and ChatGPT) are missing in Table 2 and 3 (See Question A).

3. The major components of the proposed frameworks are CCL and PR. Both of them are incremental over the previous methods with minor adaptation for CoT-based prompting proposal.","A. What is the performance of different CoT variants [1,2,3] of LLMs for the two evaluated tasks? 

B. Why is SoftPrompt not included in Table 3?

C. The interpretation of Figure 3 seems incomplete. For instance, how to determine reasonable decision scope for CoTT based on Figure 3 (Line 592-594)?

D.For CCL, what is the size of the counterfactual step I* (Line 304)? Is there only a single negative sample for CCL? Since CL benefits from larger batch sizes and more negative samples, it would make more sense to enlarge I* for CCL.

[1] Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 2022. 

[2] Wang et al., Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023.

[3] Fu et al., Complexity-Based Prompting for Multi-step Reasoning. ICLR 2023.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"Line 135:  the final chosen answers

Line 519 and 524: , -> .

Tense consistency across the manuscript is recommended.",1691206266874,,,EMNLP/2023/Conference,43SOcneD8W,"['EMNLP/2023/Conference/Submission2612/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461176632,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2612/Reviewer_AVcC']",43SOcneD8W,['EMNLP/2023/Conference/Submission2612/Reviewer_AVcC'],1691206266874,1701461176632,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2612/Reviewer_AVcC']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 2022.
2. Wang et al., Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023.
3. Fu et al., Complexity-Based Prompting for Multi-step Reasoning. ICLR 2023.

These citations might be necessary because the reviewer is asking the authors to provide the performance of different CoT variants of LLMs for the two evaluated tasks (hierarchical classification and relation classification) and notes that relevant CoT baselines for in-context learning of Large Language Models are missing in Tables 2 and 3. The reviewer specifically mentions that the scope of the study is under-specified and that additional relevant CoT baselines for large language models (such as text-003 and ChatGPT) are missing. The suggested papers appear to be relevant to the topic of Chain-of-Thought prompting and its application to large language models, which is a key aspect of the manuscript.",1,"2022, 2023, 2023",Self-Consistency Improves Chain of Thought Reasoning in Language Models 
kTrEhe6fsZ,"The authors propose Chain-of-Thought Tuning framework in an attempt to unify the benefits of Masked Language Modeling (MLM) and Chain-of-thought (CoT) in two NLU tasks: hierarchical classification and relation classification. Specifically, the framework is divided into 2 steps (1) Intermediate Step Generation, (2) Target Outcome Prediction via Intermediate Step generated in step (1). Additional Counterfactual-based Contrastive Learning (CCL) and Probability Rectification (PR) are also proposed to effectively integrate intermediate steps into the overall framework.","1. The overall structure of the work as well as motivations for the integration of CoT and individual components is  well written.

2. The adaptation of CoT to NLU tasks is quite interesting and provides fresh perspectives for NLU tasks.","1. The reported performance gain of the proposed framework is marginal when compared to the improvements introduced by simple Prompt Tuning approaches. For instance,for Table 3, out of 2.7% gain over Roberta backbone on ReTACRED, prompting tuning (i.e. HardPrompt) already achieves the gain of 1.7%. 

2. The scope of the study is under-specified. It seems that the work focuses on injecting CoT- based approach to small-scale Language Models. If that is not the case, additional relevant CoT baselines for in-context learning of Large Language Models (for text-003 and ChatGPT) are missing in Table 2 and 3 (See Question A).

3. The major components of the proposed frameworks are CCL and PR. Both of them are incremental over the previous methods with minor adaptation for CoT-based prompting proposal.","A. What is the performance of different CoT variants [1,2,3] of LLMs for the two evaluated tasks? 

B. Why is SoftPrompt not included in Table 3?

C. The interpretation of Figure 3 seems incomplete. For instance, how to determine reasonable decision scope for CoTT based on Figure 3 (Line 592-594)?

D.For CCL, what is the size of the counterfactual step I* (Line 304)? Is there only a single negative sample for CCL? Since CL benefits from larger batch sizes and more negative samples, it would make more sense to enlarge I* for CCL.

[1] Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 2022. 

[2] Wang et al., Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023.

[3] Fu et al., Complexity-Based Prompting for Multi-step Reasoning. ICLR 2023.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"Line 135:  the final chosen answers

Line 519 and 524: , -> .

Tense consistency across the manuscript is recommended.",1691206266874,,,EMNLP/2023/Conference,43SOcneD8W,"['EMNLP/2023/Conference/Submission2612/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461176632,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2612/Reviewer_AVcC']",43SOcneD8W,['EMNLP/2023/Conference/Submission2612/Reviewer_AVcC'],1691206266874,1701461176632,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2612/Reviewer_AVcC']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 2022.
2. Wang et al., Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023.
3. Fu et al., Complexity-Based Prompting for Multi-step Reasoning. ICLR 2023.

These citations might be necessary because the reviewer is asking the authors to provide the performance of different CoT variants of LLMs for the two evaluated tasks (hierarchical classification and relation classification) and notes that relevant CoT baselines for in-context learning of Large Language Models are missing in Tables 2 and 3. The reviewer specifically mentions that the scope of the study is under-specified and that additional relevant CoT baselines for large language models (such as text-003 and ChatGPT) are missing. The suggested papers appear to be relevant to the topic of Chain-of-Thought prompting and its application to large language models, which is a key aspect of the manuscript.",1,"2022, 2023, 2023",Complexity-Based Prompting for Multi-step Reasoning
dNHoyU6gwL,"This paper describes a novel approach to perform qualitative annotation. Previous research mostly aimed to create a system that does the annotation process fully automatically, while this paper aims to provide ""qualitative code suggestion"" so that the annotation process becomes a collaboration between human and machine. Furthermore, the authors aim for their approach to not only suggest pre-defined codes, but also detect and suggest novel, rare, codes that have not been defined. It should also be mentioned that the authors introduce the Quoding dataset, which is the only open-source dataset for this type of task.

The results show that their human-centric approach to qualitative coding can be of great assistance to annotators. However, the system is not capable of performing the novel code detection subtask.","- Potentially helpful tool to many researchers that perform qualitative coding in some capacity
- Introduction of an open-source dataset for this task
- Extensive analysis of the results (section 8 is quite valuable)","While the authors do a good job of reporting and describing their results, I do have some questions related to clarity and methodological choices.

- Questionable to what degree this task is novel (therefore missing relevant baselines) (see ""Missing References"").
- Evaluation does not seem most relevant for this task. The main purpose of this system is to help annotators do their work more efficiently. Do the suggestions make annotation more efficient (in terms of annotation time), and do they decrease Annotation Errors by the annotators when used, for instance?","- §2 (On deductive vs. inductive coding) --> What would make inductive coding fundamentally different? I would think that inductive coding just adds an extra step of exploratory analysis where codes are defined before moving on to the official annotation. However, this annotation itself would then not be different from deductive coding.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"To me it seems like this proposed task largely overlaps to a large degree with human-in-the-loop annotation, which is a well-studied topic. Tools exist, like Prodigy for instance (Montani & Honnibal, 2018). And also previous ACL-research, such as:

Yu et al. (2015). LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
Liu et al. (2019). Deep reinforcement active learning for human-in-the-loop person re-identification.
Fan et al. (2019). An interactive visual analytics approach for network anomaly detection through smart labeling.
Gesller et al. (2022). Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data.
Mendes et al. (2023). Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments.
Klie et al. (2020). From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains.

It would have been nice if this work had been discussed in this paper to compare previous approaches on this topic to their approach, and maybe also to provide relevant baselines to compare their QCS to.",,1691006493166,,,EMNLP/2023/Conference,42LIoV0C1h,"['EMNLP/2023/Conference/Submission3631/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461242463,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']",42LIoV0C1h,['EMNLP/2023/Conference/Submission3631/Reviewer_ULRd'],1691006493166,1701461242463,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']","Yes

The suggested papers or references not cited in the manuscript are:
1. Montani & Honnibal (2018) - Prodigy
2. Yu et al. (2015) - LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
3. Liu et al. (2019) - Deep reinforcement active learning for human-in-the-loop person re-identification
4. Fan et al. (2019) - An interactive visual analytics approach for network anomaly detection through smart labeling
5. Gesller et al. (2022) - Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data
6. Mendes et al. (2023) - Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments
7. Klie et al. (2020) - From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains

These citations might be necessary because the reviewer believes that the proposed task in the manuscript overlaps with the well-studied topic of human-in-the-loop annotation. The reviewer suggests that discussing these previous approaches and comparing them to the authors' approach would provide relevant baselines and context, potentially strengthening the manuscript. The reviewer also questions the novelty of the task, implying that the authors should demonstrate how their work differs from or builds upon existing research in the field.",1,"2018, 2015, 2019, 2019, 2022, 2023, 2020",Prodigy
dNHoyU6gwL,"This paper describes a novel approach to perform qualitative annotation. Previous research mostly aimed to create a system that does the annotation process fully automatically, while this paper aims to provide ""qualitative code suggestion"" so that the annotation process becomes a collaboration between human and machine. Furthermore, the authors aim for their approach to not only suggest pre-defined codes, but also detect and suggest novel, rare, codes that have not been defined. It should also be mentioned that the authors introduce the Quoding dataset, which is the only open-source dataset for this type of task.

The results show that their human-centric approach to qualitative coding can be of great assistance to annotators. However, the system is not capable of performing the novel code detection subtask.","- Potentially helpful tool to many researchers that perform qualitative coding in some capacity
- Introduction of an open-source dataset for this task
- Extensive analysis of the results (section 8 is quite valuable)","While the authors do a good job of reporting and describing their results, I do have some questions related to clarity and methodological choices.

- Questionable to what degree this task is novel (therefore missing relevant baselines) (see ""Missing References"").
- Evaluation does not seem most relevant for this task. The main purpose of this system is to help annotators do their work more efficiently. Do the suggestions make annotation more efficient (in terms of annotation time), and do they decrease Annotation Errors by the annotators when used, for instance?","- §2 (On deductive vs. inductive coding) --> What would make inductive coding fundamentally different? I would think that inductive coding just adds an extra step of exploratory analysis where codes are defined before moving on to the official annotation. However, this annotation itself would then not be different from deductive coding.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"To me it seems like this proposed task largely overlaps to a large degree with human-in-the-loop annotation, which is a well-studied topic. Tools exist, like Prodigy for instance (Montani & Honnibal, 2018). And also previous ACL-research, such as:

Yu et al. (2015). LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
Liu et al. (2019). Deep reinforcement active learning for human-in-the-loop person re-identification.
Fan et al. (2019). An interactive visual analytics approach for network anomaly detection through smart labeling.
Gesller et al. (2022). Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data.
Mendes et al. (2023). Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments.
Klie et al. (2020). From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains.

It would have been nice if this work had been discussed in this paper to compare previous approaches on this topic to their approach, and maybe also to provide relevant baselines to compare their QCS to.",,1691006493166,,,EMNLP/2023/Conference,42LIoV0C1h,"['EMNLP/2023/Conference/Submission3631/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461242463,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']",42LIoV0C1h,['EMNLP/2023/Conference/Submission3631/Reviewer_ULRd'],1691006493166,1701461242463,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']","Yes

The suggested papers or references not cited in the manuscript are:
1. Montani & Honnibal (2018) - Prodigy
2. Yu et al. (2015) - LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
3. Liu et al. (2019) - Deep reinforcement active learning for human-in-the-loop person re-identification
4. Fan et al. (2019) - An interactive visual analytics approach for network anomaly detection through smart labeling
5. Gesller et al. (2022) - Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data
6. Mendes et al. (2023) - Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments
7. Klie et al. (2020) - From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains

These citations might be necessary because the reviewer believes that the proposed task in the manuscript overlaps with the well-studied topic of human-in-the-loop annotation. The reviewer suggests that discussing these previous approaches and comparing them to the authors' approach would provide relevant baselines and context, potentially strengthening the manuscript. The reviewer also questions the novelty of the task, implying that the authors should demonstrate how their work differs from or builds upon existing research in the field.",1,"2018, 2015, 2019, 2019, 2022, 2023, 2020",LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
dNHoyU6gwL,"This paper describes a novel approach to perform qualitative annotation. Previous research mostly aimed to create a system that does the annotation process fully automatically, while this paper aims to provide ""qualitative code suggestion"" so that the annotation process becomes a collaboration between human and machine. Furthermore, the authors aim for their approach to not only suggest pre-defined codes, but also detect and suggest novel, rare, codes that have not been defined. It should also be mentioned that the authors introduce the Quoding dataset, which is the only open-source dataset for this type of task.

The results show that their human-centric approach to qualitative coding can be of great assistance to annotators. However, the system is not capable of performing the novel code detection subtask.","- Potentially helpful tool to many researchers that perform qualitative coding in some capacity
- Introduction of an open-source dataset for this task
- Extensive analysis of the results (section 8 is quite valuable)","While the authors do a good job of reporting and describing their results, I do have some questions related to clarity and methodological choices.

- Questionable to what degree this task is novel (therefore missing relevant baselines) (see ""Missing References"").
- Evaluation does not seem most relevant for this task. The main purpose of this system is to help annotators do their work more efficiently. Do the suggestions make annotation more efficient (in terms of annotation time), and do they decrease Annotation Errors by the annotators when used, for instance?","- §2 (On deductive vs. inductive coding) --> What would make inductive coding fundamentally different? I would think that inductive coding just adds an extra step of exploratory analysis where codes are defined before moving on to the official annotation. However, this annotation itself would then not be different from deductive coding.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"To me it seems like this proposed task largely overlaps to a large degree with human-in-the-loop annotation, which is a well-studied topic. Tools exist, like Prodigy for instance (Montani & Honnibal, 2018). And also previous ACL-research, such as:

Yu et al. (2015). LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
Liu et al. (2019). Deep reinforcement active learning for human-in-the-loop person re-identification.
Fan et al. (2019). An interactive visual analytics approach for network anomaly detection through smart labeling.
Gesller et al. (2022). Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data.
Mendes et al. (2023). Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments.
Klie et al. (2020). From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains.

It would have been nice if this work had been discussed in this paper to compare previous approaches on this topic to their approach, and maybe also to provide relevant baselines to compare their QCS to.",,1691006493166,,,EMNLP/2023/Conference,42LIoV0C1h,"['EMNLP/2023/Conference/Submission3631/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461242463,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']",42LIoV0C1h,['EMNLP/2023/Conference/Submission3631/Reviewer_ULRd'],1691006493166,1701461242463,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']","Yes

The suggested papers or references not cited in the manuscript are:
1. Montani & Honnibal (2018) - Prodigy
2. Yu et al. (2015) - LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
3. Liu et al. (2019) - Deep reinforcement active learning for human-in-the-loop person re-identification
4. Fan et al. (2019) - An interactive visual analytics approach for network anomaly detection through smart labeling
5. Gesller et al. (2022) - Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data
6. Mendes et al. (2023) - Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments
7. Klie et al. (2020) - From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains

These citations might be necessary because the reviewer believes that the proposed task in the manuscript overlaps with the well-studied topic of human-in-the-loop annotation. The reviewer suggests that discussing these previous approaches and comparing them to the authors' approach would provide relevant baselines and context, potentially strengthening the manuscript. The reviewer also questions the novelty of the task, implying that the authors should demonstrate how their work differs from or builds upon existing research in the field.",1,"2018, 2015, 2019, 2019, 2022, 2023, 2020",Deep reinforcement active learning for human-in-the-loop person re-identification
dNHoyU6gwL,"This paper describes a novel approach to perform qualitative annotation. Previous research mostly aimed to create a system that does the annotation process fully automatically, while this paper aims to provide ""qualitative code suggestion"" so that the annotation process becomes a collaboration between human and machine. Furthermore, the authors aim for their approach to not only suggest pre-defined codes, but also detect and suggest novel, rare, codes that have not been defined. It should also be mentioned that the authors introduce the Quoding dataset, which is the only open-source dataset for this type of task.

The results show that their human-centric approach to qualitative coding can be of great assistance to annotators. However, the system is not capable of performing the novel code detection subtask.","- Potentially helpful tool to many researchers that perform qualitative coding in some capacity
- Introduction of an open-source dataset for this task
- Extensive analysis of the results (section 8 is quite valuable)","While the authors do a good job of reporting and describing their results, I do have some questions related to clarity and methodological choices.

- Questionable to what degree this task is novel (therefore missing relevant baselines) (see ""Missing References"").
- Evaluation does not seem most relevant for this task. The main purpose of this system is to help annotators do their work more efficiently. Do the suggestions make annotation more efficient (in terms of annotation time), and do they decrease Annotation Errors by the annotators when used, for instance?","- §2 (On deductive vs. inductive coding) --> What would make inductive coding fundamentally different? I would think that inductive coding just adds an extra step of exploratory analysis where codes are defined before moving on to the official annotation. However, this annotation itself would then not be different from deductive coding.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"To me it seems like this proposed task largely overlaps to a large degree with human-in-the-loop annotation, which is a well-studied topic. Tools exist, like Prodigy for instance (Montani & Honnibal, 2018). And also previous ACL-research, such as:

Yu et al. (2015). LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
Liu et al. (2019). Deep reinforcement active learning for human-in-the-loop person re-identification.
Fan et al. (2019). An interactive visual analytics approach for network anomaly detection through smart labeling.
Gesller et al. (2022). Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data.
Mendes et al. (2023). Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments.
Klie et al. (2020). From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains.

It would have been nice if this work had been discussed in this paper to compare previous approaches on this topic to their approach, and maybe also to provide relevant baselines to compare their QCS to.",,1691006493166,,,EMNLP/2023/Conference,42LIoV0C1h,"['EMNLP/2023/Conference/Submission3631/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461242463,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']",42LIoV0C1h,['EMNLP/2023/Conference/Submission3631/Reviewer_ULRd'],1691006493166,1701461242463,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']","Yes

The suggested papers or references not cited in the manuscript are:
1. Montani & Honnibal (2018) - Prodigy
2. Yu et al. (2015) - LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
3. Liu et al. (2019) - Deep reinforcement active learning for human-in-the-loop person re-identification
4. Fan et al. (2019) - An interactive visual analytics approach for network anomaly detection through smart labeling
5. Gesller et al. (2022) - Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data
6. Mendes et al. (2023) - Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments
7. Klie et al. (2020) - From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains

These citations might be necessary because the reviewer believes that the proposed task in the manuscript overlaps with the well-studied topic of human-in-the-loop annotation. The reviewer suggests that discussing these previous approaches and comparing them to the authors' approach would provide relevant baselines and context, potentially strengthening the manuscript. The reviewer also questions the novelty of the task, implying that the authors should demonstrate how their work differs from or builds upon existing research in the field.",1,"2018, 2015, 2019, 2019, 2022, 2023, 2020",An interactive visual analytics approach for network anomaly detection through smart labeling
dNHoyU6gwL,"This paper describes a novel approach to perform qualitative annotation. Previous research mostly aimed to create a system that does the annotation process fully automatically, while this paper aims to provide ""qualitative code suggestion"" so that the annotation process becomes a collaboration between human and machine. Furthermore, the authors aim for their approach to not only suggest pre-defined codes, but also detect and suggest novel, rare, codes that have not been defined. It should also be mentioned that the authors introduce the Quoding dataset, which is the only open-source dataset for this type of task.

The results show that their human-centric approach to qualitative coding can be of great assistance to annotators. However, the system is not capable of performing the novel code detection subtask.","- Potentially helpful tool to many researchers that perform qualitative coding in some capacity
- Introduction of an open-source dataset for this task
- Extensive analysis of the results (section 8 is quite valuable)","While the authors do a good job of reporting and describing their results, I do have some questions related to clarity and methodological choices.

- Questionable to what degree this task is novel (therefore missing relevant baselines) (see ""Missing References"").
- Evaluation does not seem most relevant for this task. The main purpose of this system is to help annotators do their work more efficiently. Do the suggestions make annotation more efficient (in terms of annotation time), and do they decrease Annotation Errors by the annotators when used, for instance?","- §2 (On deductive vs. inductive coding) --> What would make inductive coding fundamentally different? I would think that inductive coding just adds an extra step of exploratory analysis where codes are defined before moving on to the official annotation. However, this annotation itself would then not be different from deductive coding.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"To me it seems like this proposed task largely overlaps to a large degree with human-in-the-loop annotation, which is a well-studied topic. Tools exist, like Prodigy for instance (Montani & Honnibal, 2018). And also previous ACL-research, such as:

Yu et al. (2015). LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
Liu et al. (2019). Deep reinforcement active learning for human-in-the-loop person re-identification.
Fan et al. (2019). An interactive visual analytics approach for network anomaly detection through smart labeling.
Gesller et al. (2022). Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data.
Mendes et al. (2023). Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments.
Klie et al. (2020). From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains.

It would have been nice if this work had been discussed in this paper to compare previous approaches on this topic to their approach, and maybe also to provide relevant baselines to compare their QCS to.",,1691006493166,,,EMNLP/2023/Conference,42LIoV0C1h,"['EMNLP/2023/Conference/Submission3631/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461242463,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']",42LIoV0C1h,['EMNLP/2023/Conference/Submission3631/Reviewer_ULRd'],1691006493166,1701461242463,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']","Yes

The suggested papers or references not cited in the manuscript are:
1. Montani & Honnibal (2018) - Prodigy
2. Yu et al. (2015) - LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
3. Liu et al. (2019) - Deep reinforcement active learning for human-in-the-loop person re-identification
4. Fan et al. (2019) - An interactive visual analytics approach for network anomaly detection through smart labeling
5. Gesller et al. (2022) - Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data
6. Mendes et al. (2023) - Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments
7. Klie et al. (2020) - From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains

These citations might be necessary because the reviewer believes that the proposed task in the manuscript overlaps with the well-studied topic of human-in-the-loop annotation. The reviewer suggests that discussing these previous approaches and comparing them to the authors' approach would provide relevant baselines and context, potentially strengthening the manuscript. The reviewer also questions the novelty of the task, implying that the authors should demonstrate how their work differs from or builds upon existing research in the field.",1,"2018, 2015, 2019, 2019, 2022, 2023, 2020",Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data
dNHoyU6gwL,"This paper describes a novel approach to perform qualitative annotation. Previous research mostly aimed to create a system that does the annotation process fully automatically, while this paper aims to provide ""qualitative code suggestion"" so that the annotation process becomes a collaboration between human and machine. Furthermore, the authors aim for their approach to not only suggest pre-defined codes, but also detect and suggest novel, rare, codes that have not been defined. It should also be mentioned that the authors introduce the Quoding dataset, which is the only open-source dataset for this type of task.

The results show that their human-centric approach to qualitative coding can be of great assistance to annotators. However, the system is not capable of performing the novel code detection subtask.","- Potentially helpful tool to many researchers that perform qualitative coding in some capacity
- Introduction of an open-source dataset for this task
- Extensive analysis of the results (section 8 is quite valuable)","While the authors do a good job of reporting and describing their results, I do have some questions related to clarity and methodological choices.

- Questionable to what degree this task is novel (therefore missing relevant baselines) (see ""Missing References"").
- Evaluation does not seem most relevant for this task. The main purpose of this system is to help annotators do their work more efficiently. Do the suggestions make annotation more efficient (in terms of annotation time), and do they decrease Annotation Errors by the annotators when used, for instance?","- §2 (On deductive vs. inductive coding) --> What would make inductive coding fundamentally different? I would think that inductive coding just adds an extra step of exploratory analysis where codes are defined before moving on to the official annotation. However, this annotation itself would then not be different from deductive coding.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"To me it seems like this proposed task largely overlaps to a large degree with human-in-the-loop annotation, which is a well-studied topic. Tools exist, like Prodigy for instance (Montani & Honnibal, 2018). And also previous ACL-research, such as:

Yu et al. (2015). LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
Liu et al. (2019). Deep reinforcement active learning for human-in-the-loop person re-identification.
Fan et al. (2019). An interactive visual analytics approach for network anomaly detection through smart labeling.
Gesller et al. (2022). Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data.
Mendes et al. (2023). Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments.
Klie et al. (2020). From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains.

It would have been nice if this work had been discussed in this paper to compare previous approaches on this topic to their approach, and maybe also to provide relevant baselines to compare their QCS to.",,1691006493166,,,EMNLP/2023/Conference,42LIoV0C1h,"['EMNLP/2023/Conference/Submission3631/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461242463,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']",42LIoV0C1h,['EMNLP/2023/Conference/Submission3631/Reviewer_ULRd'],1691006493166,1701461242463,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']","Yes

The suggested papers or references not cited in the manuscript are:
1. Montani & Honnibal (2018) - Prodigy
2. Yu et al. (2015) - LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
3. Liu et al. (2019) - Deep reinforcement active learning for human-in-the-loop person re-identification
4. Fan et al. (2019) - An interactive visual analytics approach for network anomaly detection through smart labeling
5. Gesller et al. (2022) - Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data
6. Mendes et al. (2023) - Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments
7. Klie et al. (2020) - From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains

These citations might be necessary because the reviewer believes that the proposed task in the manuscript overlaps with the well-studied topic of human-in-the-loop annotation. The reviewer suggests that discussing these previous approaches and comparing them to the authors' approach would provide relevant baselines and context, potentially strengthening the manuscript. The reviewer also questions the novelty of the task, implying that the authors should demonstrate how their work differs from or builds upon existing research in the field.",1,"2018, 2015, 2019, 2019, 2022, 2023, 2020",Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments
dNHoyU6gwL,"This paper describes a novel approach to perform qualitative annotation. Previous research mostly aimed to create a system that does the annotation process fully automatically, while this paper aims to provide ""qualitative code suggestion"" so that the annotation process becomes a collaboration between human and machine. Furthermore, the authors aim for their approach to not only suggest pre-defined codes, but also detect and suggest novel, rare, codes that have not been defined. It should also be mentioned that the authors introduce the Quoding dataset, which is the only open-source dataset for this type of task.

The results show that their human-centric approach to qualitative coding can be of great assistance to annotators. However, the system is not capable of performing the novel code detection subtask.","- Potentially helpful tool to many researchers that perform qualitative coding in some capacity
- Introduction of an open-source dataset for this task
- Extensive analysis of the results (section 8 is quite valuable)","While the authors do a good job of reporting and describing their results, I do have some questions related to clarity and methodological choices.

- Questionable to what degree this task is novel (therefore missing relevant baselines) (see ""Missing References"").
- Evaluation does not seem most relevant for this task. The main purpose of this system is to help annotators do their work more efficiently. Do the suggestions make annotation more efficient (in terms of annotation time), and do they decrease Annotation Errors by the annotators when used, for instance?","- §2 (On deductive vs. inductive coding) --> What would make inductive coding fundamentally different? I would think that inductive coding just adds an extra step of exploratory analysis where codes are defined before moving on to the official annotation. However, this annotation itself would then not be different from deductive coding.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"To me it seems like this proposed task largely overlaps to a large degree with human-in-the-loop annotation, which is a well-studied topic. Tools exist, like Prodigy for instance (Montani & Honnibal, 2018). And also previous ACL-research, such as:

Yu et al. (2015). LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
Liu et al. (2019). Deep reinforcement active learning for human-in-the-loop person re-identification.
Fan et al. (2019). An interactive visual analytics approach for network anomaly detection through smart labeling.
Gesller et al. (2022). Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data.
Mendes et al. (2023). Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments.
Klie et al. (2020). From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains.

It would have been nice if this work had been discussed in this paper to compare previous approaches on this topic to their approach, and maybe also to provide relevant baselines to compare their QCS to.",,1691006493166,,,EMNLP/2023/Conference,42LIoV0C1h,"['EMNLP/2023/Conference/Submission3631/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461242463,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']",42LIoV0C1h,['EMNLP/2023/Conference/Submission3631/Reviewer_ULRd'],1691006493166,1701461242463,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3631/Reviewer_ULRd']","Yes

The suggested papers or references not cited in the manuscript are:
1. Montani & Honnibal (2018) - Prodigy
2. Yu et al. (2015) - LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop
3. Liu et al. (2019) - Deep reinforcement active learning for human-in-the-loop person re-identification
4. Fan et al. (2019) - An interactive visual analytics approach for network anomaly detection through smart labeling
5. Gesller et al. (2022) - Midas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data
6. Mendes et al. (2023) - Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments
7. Klie et al. (2020) - From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains

These citations might be necessary because the reviewer believes that the proposed task in the manuscript overlaps with the well-studied topic of human-in-the-loop annotation. The reviewer suggests that discussing these previous approaches and comparing them to the authors' approach would provide relevant baselines and context, potentially strengthening the manuscript. The reviewer also questions the novelty of the task, implying that the authors should demonstrate how their work differs from or builds upon existing research in the field.",1,"2018, 2015, 2019, 2019, 2022, 2023, 2020",From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains
1cdOVQSBAi,"This work is the first to introduce the comprehensive many-to-many multimodal summarization (M$^3$S) task and contribute a corresponding benchmark dataset. To achieve this, a dual knowledge distillation and target-oriented vision modeling framework (D2TV) is proposed.
Experiments on the M3Sum benchmark with four languages and 16 language directions show that this work builds new state-of-the-art performance, demonstrating the effectiveness of the D2TV approach.","1. This work introduces a new task called M$^3$S and presents a corresponding dataset.
2. The paper is well-written and easy to understand, making it accessible to a broad audience.","1. The motivation behind the Dual Knowledge Distillation module is unclear, and it would be preferable to provide vivid explanations with relevant examples in the Introduction section.
2. Figure 2 is not sufficiently clear, and the DKD and TCO modules are not presented in the MMS model. It is recommended to revise and modify them accordingly.
3. Recent relevant work is not be mentioned, such as ""CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization"".",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"2: Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,,1691053138664,,,EMNLP/2023/Conference,42Cc5s71zl,"['EMNLP/2023/Conference/Submission2359/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461160315,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2359/Reviewer_KUUr']",42Cc5s71zl,['EMNLP/2023/Conference/Submission2359/Reviewer_KUUr'],1691053138664,1701461160315,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2359/Reviewer_KUUr']","Yes

* ""CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization""

The reviewer suggests that the authors should cite this paper because it is recent relevant work that is not mentioned in the manuscript. This implies that the reviewer believes ""CFSum"" is closely related to the topic of multimodal summarization and that citing it would provide a more comprehensive overview of the current state of research in the field, potentially strengthening the authors' argument and contributions.",1,,CFSum A Coarse-to-Fine Contribution Network for Multimodal Summarization
xrpiwh1itz,"The paper explores prompt design strategies for text-to-SQL with in-context learning. The authors propose to retrieve demonstrations with SQL syntactic structure. Given a test instance, the proposed method first generates a preliminary SQL query and uses the preliminary SQL query to retrieve demonstration examples. For the retrieval strategy, the authors propose to consider both the diversity of demonstrations and the similarity between demonstrations and the test instance. The paper also studies how to represent schema-related knowledge in prompt designs in a zero-shot scenario. 
By combining schema-related knowledge and the demonstration retrieval method, the proposed method outperforms the previous state-of-the-art in-context learning methods and fine-tuned methods. The findings in the paper will benefit future study text-to-SQL.

","1. The paper compares various demonstration retrieval strategies for text-to-SQL with in-context learning and reveals that the most preferred demonstrations are to consider both similarity and diversity.
2. The paper also studies how to represent schema-related knowledge in the zero-shot prompt designs. 
3. The experiments on multiple text-to-SQL datasets demonstrate the generalization of the findings.
4. The paper is well-written and easy to follow.","1. The authors should be more careful when saying ""we propose"". (it does not sound like a problem to me once some editing is done).
(1) In Line 224, the authors say ""We propose ... code sequence ... CREATE query"". The CREATE query for database schema was proposed by [1]. The authors should be aware of this work as it is cited in the introduction section.
(2) Retrieve diverse demonstrations for semantic parsing has been proposed in this work [2], although it is not focusing on text-to-SQL.  

[1] Rajkumar, Nitarshan, Raymond Li, and Dzmitry Bahdanau. ""Evaluating the text-to-sql capabilities of large language models."" arXiv preprint arXiv:2204.00498 (2022).
[2] Levy, Itay, Ben Bogin, and Jonathan Berant. ""Diverse demonstrations improve in-context compositional generalization."" arXiv preprint arXiv:2212.06800 (2022).","1. How does the Codex perform when retrieving demonstrations with the oracle SQL query of the test instance?
2. How many experiments are run to achieve the majority voting for one test instance?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"The best fine-tuned system was [1], which was released more than 3 months ahead of the EMNLP2023 deadline.
[1] Li, Haoyang, et al. ""Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.",,1690401149318,,,EMNLP/2023/Conference,40NCUv4I2R,"['EMNLP/2023/Conference/Submission1965/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461133102,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1965/Reviewer_rZ1M']",40NCUv4I2R,['EMNLP/2023/Conference/Submission1965/Reviewer_rZ1M'],1690401149318,1701461133102,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1965/Reviewer_rZ1M']","Yes

The suggested papers or references not cited in the manuscript are:
1. Rajkumar, Nitarshan, Raymond Li, and Dzmitry Bahdanau. ""Evaluating the text-to-sql capabilities of large language models."" arXiv preprint arXiv:2204.00498 (2022).
2. Levy, Itay, Ben Bogin, and Jonathan Berant. ""Diverse demonstrations improve in-context compositional generalization."" arXiv preprint arXiv:2212.06800 (2022).
3. Li, Haoyang, et al. ""Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.

These citations might be necessary because the reviewer points out that the authors should be aware of existing work related to their proposals. Specifically, the CREATE query for database schema was proposed in [1], and retrieving diverse demonstrations for semantic parsing was proposed in [2]. Additionally, the reviewer mentions that the best fine-tuned system was [3], which was released before the submission deadline, suggesting that the authors should be aware of the current state-of-the-art in the field.",1,"2022, 2022, 2022, 2022, 2023",Evaluating the text-to-sql capabilities of large language models 
xrpiwh1itz,"The paper explores prompt design strategies for text-to-SQL with in-context learning. The authors propose to retrieve demonstrations with SQL syntactic structure. Given a test instance, the proposed method first generates a preliminary SQL query and uses the preliminary SQL query to retrieve demonstration examples. For the retrieval strategy, the authors propose to consider both the diversity of demonstrations and the similarity between demonstrations and the test instance. The paper also studies how to represent schema-related knowledge in prompt designs in a zero-shot scenario. 
By combining schema-related knowledge and the demonstration retrieval method, the proposed method outperforms the previous state-of-the-art in-context learning methods and fine-tuned methods. The findings in the paper will benefit future study text-to-SQL.

","1. The paper compares various demonstration retrieval strategies for text-to-SQL with in-context learning and reveals that the most preferred demonstrations are to consider both similarity and diversity.
2. The paper also studies how to represent schema-related knowledge in the zero-shot prompt designs. 
3. The experiments on multiple text-to-SQL datasets demonstrate the generalization of the findings.
4. The paper is well-written and easy to follow.","1. The authors should be more careful when saying ""we propose"". (it does not sound like a problem to me once some editing is done).
(1) In Line 224, the authors say ""We propose ... code sequence ... CREATE query"". The CREATE query for database schema was proposed by [1]. The authors should be aware of this work as it is cited in the introduction section.
(2) Retrieve diverse demonstrations for semantic parsing has been proposed in this work [2], although it is not focusing on text-to-SQL.  

[1] Rajkumar, Nitarshan, Raymond Li, and Dzmitry Bahdanau. ""Evaluating the text-to-sql capabilities of large language models."" arXiv preprint arXiv:2204.00498 (2022).
[2] Levy, Itay, Ben Bogin, and Jonathan Berant. ""Diverse demonstrations improve in-context compositional generalization."" arXiv preprint arXiv:2212.06800 (2022).","1. How does the Codex perform when retrieving demonstrations with the oracle SQL query of the test instance?
2. How many experiments are run to achieve the majority voting for one test instance?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"The best fine-tuned system was [1], which was released more than 3 months ahead of the EMNLP2023 deadline.
[1] Li, Haoyang, et al. ""Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.",,1690401149318,,,EMNLP/2023/Conference,40NCUv4I2R,"['EMNLP/2023/Conference/Submission1965/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461133102,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1965/Reviewer_rZ1M']",40NCUv4I2R,['EMNLP/2023/Conference/Submission1965/Reviewer_rZ1M'],1690401149318,1701461133102,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1965/Reviewer_rZ1M']","Yes

The suggested papers or references not cited in the manuscript are:
1. Rajkumar, Nitarshan, Raymond Li, and Dzmitry Bahdanau. ""Evaluating the text-to-sql capabilities of large language models."" arXiv preprint arXiv:2204.00498 (2022).
2. Levy, Itay, Ben Bogin, and Jonathan Berant. ""Diverse demonstrations improve in-context compositional generalization."" arXiv preprint arXiv:2212.06800 (2022).
3. Li, Haoyang, et al. ""Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.

These citations might be necessary because the reviewer points out that the authors should be aware of existing work related to their proposals. Specifically, the CREATE query for database schema was proposed in [1], and retrieving diverse demonstrations for semantic parsing was proposed in [2]. Additionally, the reviewer mentions that the best fine-tuned system was [3], which was released before the submission deadline, suggesting that the authors should be aware of the current state-of-the-art in the field.",1,"2022, 2022, 2022, 2022, 2023",Diverse demonstrations improve in-context compositional generalization 
xrpiwh1itz,"The paper explores prompt design strategies for text-to-SQL with in-context learning. The authors propose to retrieve demonstrations with SQL syntactic structure. Given a test instance, the proposed method first generates a preliminary SQL query and uses the preliminary SQL query to retrieve demonstration examples. For the retrieval strategy, the authors propose to consider both the diversity of demonstrations and the similarity between demonstrations and the test instance. The paper also studies how to represent schema-related knowledge in prompt designs in a zero-shot scenario. 
By combining schema-related knowledge and the demonstration retrieval method, the proposed method outperforms the previous state-of-the-art in-context learning methods and fine-tuned methods. The findings in the paper will benefit future study text-to-SQL.

","1. The paper compares various demonstration retrieval strategies for text-to-SQL with in-context learning and reveals that the most preferred demonstrations are to consider both similarity and diversity.
2. The paper also studies how to represent schema-related knowledge in the zero-shot prompt designs. 
3. The experiments on multiple text-to-SQL datasets demonstrate the generalization of the findings.
4. The paper is well-written and easy to follow.","1. The authors should be more careful when saying ""we propose"". (it does not sound like a problem to me once some editing is done).
(1) In Line 224, the authors say ""We propose ... code sequence ... CREATE query"". The CREATE query for database schema was proposed by [1]. The authors should be aware of this work as it is cited in the introduction section.
(2) Retrieve diverse demonstrations for semantic parsing has been proposed in this work [2], although it is not focusing on text-to-SQL.  

[1] Rajkumar, Nitarshan, Raymond Li, and Dzmitry Bahdanau. ""Evaluating the text-to-sql capabilities of large language models."" arXiv preprint arXiv:2204.00498 (2022).
[2] Levy, Itay, Ben Bogin, and Jonathan Berant. ""Diverse demonstrations improve in-context compositional generalization."" arXiv preprint arXiv:2212.06800 (2022).","1. How does the Codex perform when retrieving demonstrations with the oracle SQL query of the test instance?
2. How many experiments are run to achieve the majority voting for one test instance?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"The best fine-tuned system was [1], which was released more than 3 months ahead of the EMNLP2023 deadline.
[1] Li, Haoyang, et al. ""Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.",,1690401149318,,,EMNLP/2023/Conference,40NCUv4I2R,"['EMNLP/2023/Conference/Submission1965/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461133102,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1965/Reviewer_rZ1M']",40NCUv4I2R,['EMNLP/2023/Conference/Submission1965/Reviewer_rZ1M'],1690401149318,1701461133102,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1965/Reviewer_rZ1M']","Yes

The suggested papers or references not cited in the manuscript are:
1. Rajkumar, Nitarshan, Raymond Li, and Dzmitry Bahdanau. ""Evaluating the text-to-sql capabilities of large language models."" arXiv preprint arXiv:2204.00498 (2022).
2. Levy, Itay, Ben Bogin, and Jonathan Berant. ""Diverse demonstrations improve in-context compositional generalization."" arXiv preprint arXiv:2212.06800 (2022).
3. Li, Haoyang, et al. ""Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.

These citations might be necessary because the reviewer points out that the authors should be aware of existing work related to their proposals. Specifically, the CREATE query for database schema was proposed in [1], and retrieving diverse demonstrations for semantic parsing was proposed in [2]. Additionally, the reviewer mentions that the best fine-tuned system was [3], which was released before the submission deadline, suggesting that the authors should be aware of the current state-of-the-art in the field.",1,"2022, 2022, 2022, 2022, 2023",Resdsql Decoupling schema linking and skeleton parsing for text-to-sql
XKpj1OuiIx,"Topic: contextual biasing for ASR systems. 

The authors proposed an inference time adaptation algorithm inspired by KNN LM to bias ASR model decoding towards a predefined rare word dictionary, while do not worsen the performance on words not in that rare word dictionary by using a trie for exact word match and using a backup beam. The proposed method leverages speech synthesis systems, and is training-free, which are the main novelty compared to other contextual biasing approaches.

Experiments show that the proposed algorithm works on both encoder-decoder architecture (the Whisper model) and Neural Transducers (the RNN-T model)

In addition, the release of the entity-rich dataset could be a valuable contribution to the community as well.
","1. The idea of organizing memory as a trie and use a backoff beam to avoid spurious matches is elegant, and is shown to avoid worsening the performance on words not in the rare word dictionary
2. the approaches works for both encoder-decoder models and Transducer models. While it's not as straightforward to apply their approach on Transducer models, the authors found tricks that make it work
3. the evaluation is comprehensive - on publicly available dataset, proprietary data set in medical domain, and synthetic entity-rich dataset","1. Latency of the proposed approach, and comparison with other approaches are not studied. I suspect that this approach is faster than neural nets based adaptation approaches that are compared in the paper, but this needs to be shown quantitatively.
2. whether the proposed approach is superior to Le at al 2021a is not clear to me, because PRISM and Le et al. 2021a are only compared once in table 2, and PRISM performs worse than Le et al.2021a. However, the paper claims that ""PRISM achieves superior WER reductions"" in line 467.","1. Based on description in section 4.1 and table 1, I'm confused by line 455 ""despite starting with a much stronger Whisper baseline"", how can it be much stronger if both this paper and Sun et al 2023 are using base.en. 
2. In Table 2, the RNN-T baseline numbers for the three approaches are different, e.g. on test-clean, WERs are 5.5 for Sun et al. 3.7 for Le et al. and 3.5 for this work. How can that be the case, as the RNN-T model is the same, and contextual biasing modules are not added for baseline.
3. For Whisper, there should be a very simple way to achieve contextual biasing. As indicated in figure 1 of the original whisper paper [1] and executed in the AVSR section of [2] (note that [2] is published after the submission of this paper), is to use a predefined dictionary in the prompt to the decoder can bias the model to output words from or related to the words in the predefined dictionary. Using prompt for contextual biasing seems particularly suitable for the experiment in section 4.2 where the evaluation is on Enterprise dataset, because the vocab. is small, and therefore the overhead to context length is small.

[1] Radford, Alec, et al. ""Robust speech recognition via large-scale weak supervision."" International Conference on Machine Learning. PMLR, 2023.

[2] Peng, Puyuan, et al. ""Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization."" Interspeech, 2023.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"Line 35: state-of-art -> state-of-the-art

Line 195: … perform a memory augmented inference to… -> … perform memory augmented inference to…",1690209359231,,,EMNLP/2023/Conference,3pvdo2yHXq,"['EMNLP/2023/Conference/Submission1950/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461132528,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1950/Reviewer_emcA']",3pvdo2yHXq,['EMNLP/2023/Conference/Submission1950/Reviewer_emcA'],1690209359231,1701461132528,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1950/Reviewer_emcA']","Yes

The suggested papers or references not cited in the manuscript are:
1. Radford, Alec, et al. ""Robust speech recognition via large-scale weak supervision."" (referenced as [1] in the review)
2. Peng, Puyuan, et al. ""Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization."" (referenced as [2] in the review)
3. Le et al. 2021a (already cited, but the reviewer is unclear about the comparison with this work)

These citations might be necessary because the reviewer mentions that the authors' claim of ""PRISM achieves superior WER reductions"" is unclear in comparison to Le et al. 2021a, suggesting a need for further clarification or comparison. Additionally, the reviewer points out that using a predefined dictionary in the prompt to the decoder for contextual biasing, as indicated in Radford et al. [1] and executed in Peng et al. [2], seems like a simple and suitable approach, especially for the experiment in section 4.2. This implies that the authors could benefit from discussing or comparing their approach with these existing methods to strengthen their work.",1,,Robust speech recognition via large-scale weak supervision
XKpj1OuiIx,"Topic: contextual biasing for ASR systems. 

The authors proposed an inference time adaptation algorithm inspired by KNN LM to bias ASR model decoding towards a predefined rare word dictionary, while do not worsen the performance on words not in that rare word dictionary by using a trie for exact word match and using a backup beam. The proposed method leverages speech synthesis systems, and is training-free, which are the main novelty compared to other contextual biasing approaches.

Experiments show that the proposed algorithm works on both encoder-decoder architecture (the Whisper model) and Neural Transducers (the RNN-T model)

In addition, the release of the entity-rich dataset could be a valuable contribution to the community as well.
","1. The idea of organizing memory as a trie and use a backoff beam to avoid spurious matches is elegant, and is shown to avoid worsening the performance on words not in the rare word dictionary
2. the approaches works for both encoder-decoder models and Transducer models. While it's not as straightforward to apply their approach on Transducer models, the authors found tricks that make it work
3. the evaluation is comprehensive - on publicly available dataset, proprietary data set in medical domain, and synthetic entity-rich dataset","1. Latency of the proposed approach, and comparison with other approaches are not studied. I suspect that this approach is faster than neural nets based adaptation approaches that are compared in the paper, but this needs to be shown quantitatively.
2. whether the proposed approach is superior to Le at al 2021a is not clear to me, because PRISM and Le et al. 2021a are only compared once in table 2, and PRISM performs worse than Le et al.2021a. However, the paper claims that ""PRISM achieves superior WER reductions"" in line 467.","1. Based on description in section 4.1 and table 1, I'm confused by line 455 ""despite starting with a much stronger Whisper baseline"", how can it be much stronger if both this paper and Sun et al 2023 are using base.en. 
2. In Table 2, the RNN-T baseline numbers for the three approaches are different, e.g. on test-clean, WERs are 5.5 for Sun et al. 3.7 for Le et al. and 3.5 for this work. How can that be the case, as the RNN-T model is the same, and contextual biasing modules are not added for baseline.
3. For Whisper, there should be a very simple way to achieve contextual biasing. As indicated in figure 1 of the original whisper paper [1] and executed in the AVSR section of [2] (note that [2] is published after the submission of this paper), is to use a predefined dictionary in the prompt to the decoder can bias the model to output words from or related to the words in the predefined dictionary. Using prompt for contextual biasing seems particularly suitable for the experiment in section 4.2 where the evaluation is on Enterprise dataset, because the vocab. is small, and therefore the overhead to context length is small.

[1] Radford, Alec, et al. ""Robust speech recognition via large-scale weak supervision."" International Conference on Machine Learning. PMLR, 2023.

[2] Peng, Puyuan, et al. ""Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization."" Interspeech, 2023.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"Line 35: state-of-art -> state-of-the-art

Line 195: … perform a memory augmented inference to… -> … perform memory augmented inference to…",1690209359231,,,EMNLP/2023/Conference,3pvdo2yHXq,"['EMNLP/2023/Conference/Submission1950/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461132528,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1950/Reviewer_emcA']",3pvdo2yHXq,['EMNLP/2023/Conference/Submission1950/Reviewer_emcA'],1690209359231,1701461132528,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1950/Reviewer_emcA']","Yes

The suggested papers or references not cited in the manuscript are:
1. Radford, Alec, et al. ""Robust speech recognition via large-scale weak supervision."" (referenced as [1] in the review)
2. Peng, Puyuan, et al. ""Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization."" (referenced as [2] in the review)
3. Le et al. 2021a (already cited, but the reviewer is unclear about the comparison with this work)

These citations might be necessary because the reviewer mentions that the authors' claim of ""PRISM achieves superior WER reductions"" is unclear in comparison to Le et al. 2021a, suggesting a need for further clarification or comparison. Additionally, the reviewer points out that using a predefined dictionary in the prompt to the decoder for contextual biasing, as indicated in Radford et al. [1] and executed in Peng et al. [2], seems like a simple and suitable approach, especially for the experiment in section 4.2. This implies that the authors could benefit from discussing or comparing their approach with these existing methods to strengthen their work.",1,,Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization
6RqtUVCDfv,"This paper studies the topic of mitigating the cost of LLM inference by delegating simple examples to an inexpensive preliminary classifier.
The primary contribution is an approach for using the LLM predictions to improve the preliminary classifier in an online setting.
The approach employs an embedding-based kNN classifier over past LLM predictions as the preliminary classifier.
The decision of whether to delegate to the LLM is based on whether the distance between the input embedding and the embeddings of the nearest neighbors exceed a learned threshold.
The decision threshold is learned to optimize a discounted objective function (e.g., accuracy) where the discount factor can be tuned to weigh the relative cost of performing inference with the LLM vs. making a mistake.
The paper also contributes experimental results applying the method to the Banking77 intent classification datasets.
The experimental results demonstrate that the proposed system can achieve accuracy within 0.5% (absolute) of the LLM, while only using the LLM to make predictions on 1/3 the number of examples.",a. Performing inference using proprietary LLMs is becoming a substantial operating expense for businesses as well as NLP research labs. The proposed method could be helpful in reducing this cost while maintaining comparable levels of accuracy.,"b. Novelty. The idea of having the preliminary classifier abstain from making predictions (i.e., delegate making predictions to the LLM) resembles a large body of existing work on abstaining/delegating classifiers and early exiting.
   I believe that the novel aspect of this work is that an LLM is being used to generate the labels instead of a human labeler, which, given the current popularity of LLMs, is in my opinion a relatively straightforward modification.
   This accounts for the majority of my excitement score; if there is a novel aspect of the paper that I am missing I would be willing to revise this score upwards.
c. Limited Scope. Experimental results are only reported for a single task, thus it is hard to generalize how well this method may work in other settings. This negatively impacts my evaluation of the paper's soundness.","You write that:
> λ can intuitively be thought of as a currency exchange rate, showing how expensive ρ is in terms of δ (e.g., loss of accuracy in percentage points).

I think that this is an interesting point, but it would be even more interesting if it were grounded in terms of real costs.
I have a few questions about this:

d. In your setting would it be possible to back this out to an estimate of the dollar cost of an additional point of accuracy?

e. How would this compare to the dollar cost of an additional point of accuracy from using human-annotated labels instead of LLM generated labels?

I understand that the second question may be difficult to answer without knowing the cost of collecting Banking77 (it looks like the experimental portion is covered by the results in Figure 3), but if you were able to quantify whether/when an additional point of accuracy is using LLMs in your framework, I think this paper would be much more exciting.
",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"These are a few papers that study the related problems of delegating and abstaining classifiers as well as other cost-reducing approaches like early exiting:
- César Ferri, Peter Flach, and José Hernández-Orallo. 2004. Delegating classifiers. In Proceedings of the twenty-first international conference on Machine learning (ICML '04). Association for Computing Machinery, New York, NY, USA, 37. https://doi.org/10.1145/1015330.1015395
- Tadeusz Pietraszek. 2005. Optimizing abstaining classifiers using ROC analysis. In Proceedings of the 22nd international conference on Machine learning (ICML '05). Association for Computing Machinery, New York, NY, USA, 665–672. https://doi.org/10.1145/1102351.1102435
- B. Barla Cambazoglu, Hugo Zaragoza, Olivier Chapelle, Jiang Chen, Ciya Liao, Zhaohui Zheng, and Jon Degenhardt. 2010. Early exit optimizations for additive machine learned ranking systems. In Proceedings of the third ACM international conference on Web search and data mining (WSDM '10). Association for Computing Machinery, New York, NY, USA, 411–420. https://doi.org/10.1145/1718487.1718538

I think these would be good starting points for a related work section.",Figure 2 - Discounged,1691197638527,,,EMNLP/2023/Conference,3l9zUuFo9m,"['EMNLP/2023/Conference/Submission3192/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461214822,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3192/Reviewer_7Vyf']",3l9zUuFo9m,['EMNLP/2023/Conference/Submission3192/Reviewer_7Vyf'],1691197638527,1701461214822,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3192/Reviewer_7Vyf']","Yes

The suggested papers or references not cited in the manuscript are:
1. César Ferri, Peter Flach, and José Hernández-Orallo. 2004. Delegating classifiers.
2. Tadeusz Pietraszek. 2005. Optimizing abstaining classifiers using ROC analysis.
3. B. Barla Cambazoglu, Hugo Zaragoza, Olivier Chapelle, Jiang Chen, Ciya Liao, Zhaohui Zheng, and Jon Degenhardt. 2010. Early exit optimizations for additive machine learned ranking systems.

These citations might be necessary because the reviewer believes they are relevant to the topic of delegating and abstaining classifiers, as well as other cost-reducing approaches like early exiting, which is closely related to the paper's topic of mitigating the cost of LLM inference. The reviewer suggests that including these references in a related work section would be a good starting point, implying that they would strengthen the paper by providing a more comprehensive overview of existing research in the field.",1,"2004, 2005, 2010",Delegating classifiers 
6RqtUVCDfv,"This paper studies the topic of mitigating the cost of LLM inference by delegating simple examples to an inexpensive preliminary classifier.
The primary contribution is an approach for using the LLM predictions to improve the preliminary classifier in an online setting.
The approach employs an embedding-based kNN classifier over past LLM predictions as the preliminary classifier.
The decision of whether to delegate to the LLM is based on whether the distance between the input embedding and the embeddings of the nearest neighbors exceed a learned threshold.
The decision threshold is learned to optimize a discounted objective function (e.g., accuracy) where the discount factor can be tuned to weigh the relative cost of performing inference with the LLM vs. making a mistake.
The paper also contributes experimental results applying the method to the Banking77 intent classification datasets.
The experimental results demonstrate that the proposed system can achieve accuracy within 0.5% (absolute) of the LLM, while only using the LLM to make predictions on 1/3 the number of examples.",a. Performing inference using proprietary LLMs is becoming a substantial operating expense for businesses as well as NLP research labs. The proposed method could be helpful in reducing this cost while maintaining comparable levels of accuracy.,"b. Novelty. The idea of having the preliminary classifier abstain from making predictions (i.e., delegate making predictions to the LLM) resembles a large body of existing work on abstaining/delegating classifiers and early exiting.
   I believe that the novel aspect of this work is that an LLM is being used to generate the labels instead of a human labeler, which, given the current popularity of LLMs, is in my opinion a relatively straightforward modification.
   This accounts for the majority of my excitement score; if there is a novel aspect of the paper that I am missing I would be willing to revise this score upwards.
c. Limited Scope. Experimental results are only reported for a single task, thus it is hard to generalize how well this method may work in other settings. This negatively impacts my evaluation of the paper's soundness.","You write that:
> λ can intuitively be thought of as a currency exchange rate, showing how expensive ρ is in terms of δ (e.g., loss of accuracy in percentage points).

I think that this is an interesting point, but it would be even more interesting if it were grounded in terms of real costs.
I have a few questions about this:

d. In your setting would it be possible to back this out to an estimate of the dollar cost of an additional point of accuracy?

e. How would this compare to the dollar cost of an additional point of accuracy from using human-annotated labels instead of LLM generated labels?

I understand that the second question may be difficult to answer without knowing the cost of collecting Banking77 (it looks like the experimental portion is covered by the results in Figure 3), but if you were able to quantify whether/when an additional point of accuracy is using LLMs in your framework, I think this paper would be much more exciting.
",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"These are a few papers that study the related problems of delegating and abstaining classifiers as well as other cost-reducing approaches like early exiting:
- César Ferri, Peter Flach, and José Hernández-Orallo. 2004. Delegating classifiers. In Proceedings of the twenty-first international conference on Machine learning (ICML '04). Association for Computing Machinery, New York, NY, USA, 37. https://doi.org/10.1145/1015330.1015395
- Tadeusz Pietraszek. 2005. Optimizing abstaining classifiers using ROC analysis. In Proceedings of the 22nd international conference on Machine learning (ICML '05). Association for Computing Machinery, New York, NY, USA, 665–672. https://doi.org/10.1145/1102351.1102435
- B. Barla Cambazoglu, Hugo Zaragoza, Olivier Chapelle, Jiang Chen, Ciya Liao, Zhaohui Zheng, and Jon Degenhardt. 2010. Early exit optimizations for additive machine learned ranking systems. In Proceedings of the third ACM international conference on Web search and data mining (WSDM '10). Association for Computing Machinery, New York, NY, USA, 411–420. https://doi.org/10.1145/1718487.1718538

I think these would be good starting points for a related work section.",Figure 2 - Discounged,1691197638527,,,EMNLP/2023/Conference,3l9zUuFo9m,"['EMNLP/2023/Conference/Submission3192/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461214822,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3192/Reviewer_7Vyf']",3l9zUuFo9m,['EMNLP/2023/Conference/Submission3192/Reviewer_7Vyf'],1691197638527,1701461214822,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3192/Reviewer_7Vyf']","Yes

The suggested papers or references not cited in the manuscript are:
1. César Ferri, Peter Flach, and José Hernández-Orallo. 2004. Delegating classifiers.
2. Tadeusz Pietraszek. 2005. Optimizing abstaining classifiers using ROC analysis.
3. B. Barla Cambazoglu, Hugo Zaragoza, Olivier Chapelle, Jiang Chen, Ciya Liao, Zhaohui Zheng, and Jon Degenhardt. 2010. Early exit optimizations for additive machine learned ranking systems.

These citations might be necessary because the reviewer believes they are relevant to the topic of delegating and abstaining classifiers, as well as other cost-reducing approaches like early exiting, which is closely related to the paper's topic of mitigating the cost of LLM inference. The reviewer suggests that including these references in a related work section would be a good starting point, implying that they would strengthen the paper by providing a more comprehensive overview of existing research in the field.",1,"2004, 2005, 2010",Optimizing abstaining classifiers using ROC analysis 
6RqtUVCDfv,"This paper studies the topic of mitigating the cost of LLM inference by delegating simple examples to an inexpensive preliminary classifier.
The primary contribution is an approach for using the LLM predictions to improve the preliminary classifier in an online setting.
The approach employs an embedding-based kNN classifier over past LLM predictions as the preliminary classifier.
The decision of whether to delegate to the LLM is based on whether the distance between the input embedding and the embeddings of the nearest neighbors exceed a learned threshold.
The decision threshold is learned to optimize a discounted objective function (e.g., accuracy) where the discount factor can be tuned to weigh the relative cost of performing inference with the LLM vs. making a mistake.
The paper also contributes experimental results applying the method to the Banking77 intent classification datasets.
The experimental results demonstrate that the proposed system can achieve accuracy within 0.5% (absolute) of the LLM, while only using the LLM to make predictions on 1/3 the number of examples.",a. Performing inference using proprietary LLMs is becoming a substantial operating expense for businesses as well as NLP research labs. The proposed method could be helpful in reducing this cost while maintaining comparable levels of accuracy.,"b. Novelty. The idea of having the preliminary classifier abstain from making predictions (i.e., delegate making predictions to the LLM) resembles a large body of existing work on abstaining/delegating classifiers and early exiting.
   I believe that the novel aspect of this work is that an LLM is being used to generate the labels instead of a human labeler, which, given the current popularity of LLMs, is in my opinion a relatively straightforward modification.
   This accounts for the majority of my excitement score; if there is a novel aspect of the paper that I am missing I would be willing to revise this score upwards.
c. Limited Scope. Experimental results are only reported for a single task, thus it is hard to generalize how well this method may work in other settings. This negatively impacts my evaluation of the paper's soundness.","You write that:
> λ can intuitively be thought of as a currency exchange rate, showing how expensive ρ is in terms of δ (e.g., loss of accuracy in percentage points).

I think that this is an interesting point, but it would be even more interesting if it were grounded in terms of real costs.
I have a few questions about this:

d. In your setting would it be possible to back this out to an estimate of the dollar cost of an additional point of accuracy?

e. How would this compare to the dollar cost of an additional point of accuracy from using human-annotated labels instead of LLM generated labels?

I understand that the second question may be difficult to answer without knowing the cost of collecting Banking77 (it looks like the experimental portion is covered by the results in Figure 3), but if you were able to quantify whether/when an additional point of accuracy is using LLMs in your framework, I think this paper would be much more exciting.
",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"These are a few papers that study the related problems of delegating and abstaining classifiers as well as other cost-reducing approaches like early exiting:
- César Ferri, Peter Flach, and José Hernández-Orallo. 2004. Delegating classifiers. In Proceedings of the twenty-first international conference on Machine learning (ICML '04). Association for Computing Machinery, New York, NY, USA, 37. https://doi.org/10.1145/1015330.1015395
- Tadeusz Pietraszek. 2005. Optimizing abstaining classifiers using ROC analysis. In Proceedings of the 22nd international conference on Machine learning (ICML '05). Association for Computing Machinery, New York, NY, USA, 665–672. https://doi.org/10.1145/1102351.1102435
- B. Barla Cambazoglu, Hugo Zaragoza, Olivier Chapelle, Jiang Chen, Ciya Liao, Zhaohui Zheng, and Jon Degenhardt. 2010. Early exit optimizations for additive machine learned ranking systems. In Proceedings of the third ACM international conference on Web search and data mining (WSDM '10). Association for Computing Machinery, New York, NY, USA, 411–420. https://doi.org/10.1145/1718487.1718538

I think these would be good starting points for a related work section.",Figure 2 - Discounged,1691197638527,,,EMNLP/2023/Conference,3l9zUuFo9m,"['EMNLP/2023/Conference/Submission3192/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461214822,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3192/Reviewer_7Vyf']",3l9zUuFo9m,['EMNLP/2023/Conference/Submission3192/Reviewer_7Vyf'],1691197638527,1701461214822,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3192/Reviewer_7Vyf']","Yes

The suggested papers or references not cited in the manuscript are:
1. César Ferri, Peter Flach, and José Hernández-Orallo. 2004. Delegating classifiers.
2. Tadeusz Pietraszek. 2005. Optimizing abstaining classifiers using ROC analysis.
3. B. Barla Cambazoglu, Hugo Zaragoza, Olivier Chapelle, Jiang Chen, Ciya Liao, Zhaohui Zheng, and Jon Degenhardt. 2010. Early exit optimizations for additive machine learned ranking systems.

These citations might be necessary because the reviewer believes they are relevant to the topic of delegating and abstaining classifiers, as well as other cost-reducing approaches like early exiting, which is closely related to the paper's topic of mitigating the cost of LLM inference. The reviewer suggests that including these references in a related work section would be a good starting point, implying that they would strengthen the paper by providing a more comprehensive overview of existing research in the field.",1,"2004, 2005, 2010",Early exit optimizations for additive machine learned ranking systems
HoJBPCGrdx,"The target of this paper is to perform generative pre-training over a bunch of tabular datasets and then synthesize tabular data for downstream tasks. The synthetic data are then labeled by a classifier trained on the real data. Subsequently, the labeled synthetic data and the real data are combined to train a final classifier for the downstream task. 

The authors emphasize the utility of this data synthesis approach in the following aspects:

- Privacy-preserving tabular prediction: replacing the real data with the synthesized data to build the predictive ML models.

- Few-shot tabular prediction: augmenting the real data to boost the predictive ML models in low-data scenarios.

- Missing data imputation: the generative pre-training model can help impute the missing values in the target table.","- The authors conducted a large-scale experiment over a bundle of 450 tabular datasets collected from OpenML and Kaggle. It would benefit the community if the authors could offer this data for future research in tabular learning.

- The TapTap method that has been proposed can be used for pre-training across various tables and fine-tuned to generate for targeting tabular data. The setting is reasonable and brings the promise of copying the success of generative pre-training on text to on tabular datasets.","- This paper sort of overclaims its contribution as ""the first attempt to apply table pre-training to tabular prediction"". There were many attempts in tabular pre-training to improve tabular prediction [1,2,3,4,5]. In my opinion, the major contribution of this paper lies in the cross-table generative pre-training and then generates synthetic tabular data for the target task to augment the downstream machine learning prediction models.

- The position of this paper seems to be suboptimal. While the paper emphasizes its capability in tabular prediction, the main strength of this paper, in my perspective, lies in its tabular data generation capability. From the observation made in Tables 3 and 5, the lift obtained by augmentation tabular data seems to be marginal over the other baselines. By contrast, in Table 2, it is witnessed that the proposed method reaches the lowest performance gap between the model trained on real data and trained on synthetic data, showing its power in generating synthetic data. It is suggested the author dives deeper into its generation capability and perform more in-depth of the fidelity, utility, and privacy of the synthetic data.


---

[1] Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In AAAI, volume 35, pages 6679–6687, 2021.

[2] Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33:11033–11043, 2020.

[3] Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption. In International Conference on Learning Representations, 2022.

[4] Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning. Advances in Neural Information Processing Systems, 34, 2021.

[5] Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables. Advances in Neural Information Processing Systems, 2022, 35: 2902-2915.",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,,,1690851945125,,,EMNLP/2023/Conference,3gdG9upo7e,"['EMNLP/2023/Conference/Submission321/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461023901,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']",3gdG9upo7e,['EMNLP/2023/Conference/Submission321/Reviewer_2Qac'],1690851945125,1701461023901,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']","Yes

The suggested papers or references not cited in the manuscript are:
1. Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning.
2. Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain.
3. Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption.
4. Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning.
5. Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables.

These citations might be necessary because the reviewer argues that the paper ""overclaims its contribution as 'the first attempt to apply table pre-training to tabular prediction'"" and points out that there have been many attempts in tabular pre-training to improve tabular prediction, as evidenced by the listed references. The reviewer suggests that the authors should acknowledge these existing works to provide a more accurate representation of the state of the field and the novelty of their own contribution.",1,,Tabnet Attentive interpretable tabular learning
HoJBPCGrdx,"The target of this paper is to perform generative pre-training over a bunch of tabular datasets and then synthesize tabular data for downstream tasks. The synthetic data are then labeled by a classifier trained on the real data. Subsequently, the labeled synthetic data and the real data are combined to train a final classifier for the downstream task. 

The authors emphasize the utility of this data synthesis approach in the following aspects:

- Privacy-preserving tabular prediction: replacing the real data with the synthesized data to build the predictive ML models.

- Few-shot tabular prediction: augmenting the real data to boost the predictive ML models in low-data scenarios.

- Missing data imputation: the generative pre-training model can help impute the missing values in the target table.","- The authors conducted a large-scale experiment over a bundle of 450 tabular datasets collected from OpenML and Kaggle. It would benefit the community if the authors could offer this data for future research in tabular learning.

- The TapTap method that has been proposed can be used for pre-training across various tables and fine-tuned to generate for targeting tabular data. The setting is reasonable and brings the promise of copying the success of generative pre-training on text to on tabular datasets.","- This paper sort of overclaims its contribution as ""the first attempt to apply table pre-training to tabular prediction"". There were many attempts in tabular pre-training to improve tabular prediction [1,2,3,4,5]. In my opinion, the major contribution of this paper lies in the cross-table generative pre-training and then generates synthetic tabular data for the target task to augment the downstream machine learning prediction models.

- The position of this paper seems to be suboptimal. While the paper emphasizes its capability in tabular prediction, the main strength of this paper, in my perspective, lies in its tabular data generation capability. From the observation made in Tables 3 and 5, the lift obtained by augmentation tabular data seems to be marginal over the other baselines. By contrast, in Table 2, it is witnessed that the proposed method reaches the lowest performance gap between the model trained on real data and trained on synthetic data, showing its power in generating synthetic data. It is suggested the author dives deeper into its generation capability and perform more in-depth of the fidelity, utility, and privacy of the synthetic data.


---

[1] Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In AAAI, volume 35, pages 6679–6687, 2021.

[2] Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33:11033–11043, 2020.

[3] Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption. In International Conference on Learning Representations, 2022.

[4] Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning. Advances in Neural Information Processing Systems, 34, 2021.

[5] Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables. Advances in Neural Information Processing Systems, 2022, 35: 2902-2915.",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,,,1690851945125,,,EMNLP/2023/Conference,3gdG9upo7e,"['EMNLP/2023/Conference/Submission321/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461023901,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']",3gdG9upo7e,['EMNLP/2023/Conference/Submission321/Reviewer_2Qac'],1690851945125,1701461023901,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']","Yes

The suggested papers or references not cited in the manuscript are:
1. Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning.
2. Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain.
3. Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption.
4. Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning.
5. Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables.

These citations might be necessary because the reviewer argues that the paper ""overclaims its contribution as 'the first attempt to apply table pre-training to tabular prediction'"" and points out that there have been many attempts in tabular pre-training to improve tabular prediction, as evidenced by the listed references. The reviewer suggests that the authors should acknowledge these existing works to provide a more accurate representation of the state of the field and the novelty of their own contribution.",1,,VIME Extending the success of self-and semi-supervised learning to tabular domain
HoJBPCGrdx,"The target of this paper is to perform generative pre-training over a bunch of tabular datasets and then synthesize tabular data for downstream tasks. The synthetic data are then labeled by a classifier trained on the real data. Subsequently, the labeled synthetic data and the real data are combined to train a final classifier for the downstream task. 

The authors emphasize the utility of this data synthesis approach in the following aspects:

- Privacy-preserving tabular prediction: replacing the real data with the synthesized data to build the predictive ML models.

- Few-shot tabular prediction: augmenting the real data to boost the predictive ML models in low-data scenarios.

- Missing data imputation: the generative pre-training model can help impute the missing values in the target table.","- The authors conducted a large-scale experiment over a bundle of 450 tabular datasets collected from OpenML and Kaggle. It would benefit the community if the authors could offer this data for future research in tabular learning.

- The TapTap method that has been proposed can be used for pre-training across various tables and fine-tuned to generate for targeting tabular data. The setting is reasonable and brings the promise of copying the success of generative pre-training on text to on tabular datasets.","- This paper sort of overclaims its contribution as ""the first attempt to apply table pre-training to tabular prediction"". There were many attempts in tabular pre-training to improve tabular prediction [1,2,3,4,5]. In my opinion, the major contribution of this paper lies in the cross-table generative pre-training and then generates synthetic tabular data for the target task to augment the downstream machine learning prediction models.

- The position of this paper seems to be suboptimal. While the paper emphasizes its capability in tabular prediction, the main strength of this paper, in my perspective, lies in its tabular data generation capability. From the observation made in Tables 3 and 5, the lift obtained by augmentation tabular data seems to be marginal over the other baselines. By contrast, in Table 2, it is witnessed that the proposed method reaches the lowest performance gap between the model trained on real data and trained on synthetic data, showing its power in generating synthetic data. It is suggested the author dives deeper into its generation capability and perform more in-depth of the fidelity, utility, and privacy of the synthetic data.


---

[1] Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In AAAI, volume 35, pages 6679–6687, 2021.

[2] Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33:11033–11043, 2020.

[3] Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption. In International Conference on Learning Representations, 2022.

[4] Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning. Advances in Neural Information Processing Systems, 34, 2021.

[5] Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables. Advances in Neural Information Processing Systems, 2022, 35: 2902-2915.",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,,,1690851945125,,,EMNLP/2023/Conference,3gdG9upo7e,"['EMNLP/2023/Conference/Submission321/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461023901,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']",3gdG9upo7e,['EMNLP/2023/Conference/Submission321/Reviewer_2Qac'],1690851945125,1701461023901,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']","Yes

The suggested papers or references not cited in the manuscript are:
1. Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning.
2. Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain.
3. Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption.
4. Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning.
5. Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables.

These citations might be necessary because the reviewer argues that the paper ""overclaims its contribution as 'the first attempt to apply table pre-training to tabular prediction'"" and points out that there have been many attempts in tabular pre-training to improve tabular prediction, as evidenced by the listed references. The reviewer suggests that the authors should acknowledge these existing works to provide a more accurate representation of the state of the field and the novelty of their own contribution.",1,,SCARF Self-supervised contrastive learning using random feature corruption
HoJBPCGrdx,"The target of this paper is to perform generative pre-training over a bunch of tabular datasets and then synthesize tabular data for downstream tasks. The synthetic data are then labeled by a classifier trained on the real data. Subsequently, the labeled synthetic data and the real data are combined to train a final classifier for the downstream task. 

The authors emphasize the utility of this data synthesis approach in the following aspects:

- Privacy-preserving tabular prediction: replacing the real data with the synthesized data to build the predictive ML models.

- Few-shot tabular prediction: augmenting the real data to boost the predictive ML models in low-data scenarios.

- Missing data imputation: the generative pre-training model can help impute the missing values in the target table.","- The authors conducted a large-scale experiment over a bundle of 450 tabular datasets collected from OpenML and Kaggle. It would benefit the community if the authors could offer this data for future research in tabular learning.

- The TapTap method that has been proposed can be used for pre-training across various tables and fine-tuned to generate for targeting tabular data. The setting is reasonable and brings the promise of copying the success of generative pre-training on text to on tabular datasets.","- This paper sort of overclaims its contribution as ""the first attempt to apply table pre-training to tabular prediction"". There were many attempts in tabular pre-training to improve tabular prediction [1,2,3,4,5]. In my opinion, the major contribution of this paper lies in the cross-table generative pre-training and then generates synthetic tabular data for the target task to augment the downstream machine learning prediction models.

- The position of this paper seems to be suboptimal. While the paper emphasizes its capability in tabular prediction, the main strength of this paper, in my perspective, lies in its tabular data generation capability. From the observation made in Tables 3 and 5, the lift obtained by augmentation tabular data seems to be marginal over the other baselines. By contrast, in Table 2, it is witnessed that the proposed method reaches the lowest performance gap between the model trained on real data and trained on synthetic data, showing its power in generating synthetic data. It is suggested the author dives deeper into its generation capability and perform more in-depth of the fidelity, utility, and privacy of the synthetic data.


---

[1] Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In AAAI, volume 35, pages 6679–6687, 2021.

[2] Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33:11033–11043, 2020.

[3] Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption. In International Conference on Learning Representations, 2022.

[4] Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning. Advances in Neural Information Processing Systems, 34, 2021.

[5] Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables. Advances in Neural Information Processing Systems, 2022, 35: 2902-2915.",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,,,1690851945125,,,EMNLP/2023/Conference,3gdG9upo7e,"['EMNLP/2023/Conference/Submission321/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461023901,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']",3gdG9upo7e,['EMNLP/2023/Conference/Submission321/Reviewer_2Qac'],1690851945125,1701461023901,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']","Yes

The suggested papers or references not cited in the manuscript are:
1. Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning.
2. Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain.
3. Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption.
4. Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning.
5. Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables.

These citations might be necessary because the reviewer argues that the paper ""overclaims its contribution as 'the first attempt to apply table pre-training to tabular prediction'"" and points out that there have been many attempts in tabular pre-training to improve tabular prediction, as evidenced by the listed references. The reviewer suggests that the authors should acknowledge these existing works to provide a more accurate representation of the state of the field and the novelty of their own contribution.",1,,SubTab Subsetting features of tabular data for self-supervised representation learning
HoJBPCGrdx,"The target of this paper is to perform generative pre-training over a bunch of tabular datasets and then synthesize tabular data for downstream tasks. The synthetic data are then labeled by a classifier trained on the real data. Subsequently, the labeled synthetic data and the real data are combined to train a final classifier for the downstream task. 

The authors emphasize the utility of this data synthesis approach in the following aspects:

- Privacy-preserving tabular prediction: replacing the real data with the synthesized data to build the predictive ML models.

- Few-shot tabular prediction: augmenting the real data to boost the predictive ML models in low-data scenarios.

- Missing data imputation: the generative pre-training model can help impute the missing values in the target table.","- The authors conducted a large-scale experiment over a bundle of 450 tabular datasets collected from OpenML and Kaggle. It would benefit the community if the authors could offer this data for future research in tabular learning.

- The TapTap method that has been proposed can be used for pre-training across various tables and fine-tuned to generate for targeting tabular data. The setting is reasonable and brings the promise of copying the success of generative pre-training on text to on tabular datasets.","- This paper sort of overclaims its contribution as ""the first attempt to apply table pre-training to tabular prediction"". There were many attempts in tabular pre-training to improve tabular prediction [1,2,3,4,5]. In my opinion, the major contribution of this paper lies in the cross-table generative pre-training and then generates synthetic tabular data for the target task to augment the downstream machine learning prediction models.

- The position of this paper seems to be suboptimal. While the paper emphasizes its capability in tabular prediction, the main strength of this paper, in my perspective, lies in its tabular data generation capability. From the observation made in Tables 3 and 5, the lift obtained by augmentation tabular data seems to be marginal over the other baselines. By contrast, in Table 2, it is witnessed that the proposed method reaches the lowest performance gap between the model trained on real data and trained on synthetic data, showing its power in generating synthetic data. It is suggested the author dives deeper into its generation capability and perform more in-depth of the fidelity, utility, and privacy of the synthetic data.


---

[1] Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In AAAI, volume 35, pages 6679–6687, 2021.

[2] Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33:11033–11043, 2020.

[3] Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption. In International Conference on Learning Representations, 2022.

[4] Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning. Advances in Neural Information Processing Systems, 34, 2021.

[5] Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables. Advances in Neural Information Processing Systems, 2022, 35: 2902-2915.",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,,,1690851945125,,,EMNLP/2023/Conference,3gdG9upo7e,"['EMNLP/2023/Conference/Submission321/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461023901,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']",3gdG9upo7e,['EMNLP/2023/Conference/Submission321/Reviewer_2Qac'],1690851945125,1701461023901,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission321/Reviewer_2Qac']","Yes

The suggested papers or references not cited in the manuscript are:
1. Sercan O Arık and Tomas Pfister. Tabnet: Attentive interpretable tabular learning.
2. Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. VIME: Extending the success of self-and semi-supervised learning to tabular domain.
3. Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. SCARF: Self-supervised contrastive learning using random feature corruption.
4. Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. SubTab: Subsetting features of tabular data for self-supervised representation learning.
5. Wang Z, Sun J. Transtab: Learning transferable tabular transformers across tables.

These citations might be necessary because the reviewer argues that the paper ""overclaims its contribution as 'the first attempt to apply table pre-training to tabular prediction'"" and points out that there have been many attempts in tabular pre-training to improve tabular prediction, as evidenced by the listed references. The reviewer suggests that the authors should acknowledge these existing works to provide a more accurate representation of the state of the field and the novelty of their own contribution.",1,,Transtab Learning transferable tabular transformers across tables
ltjmS64896,"This paper introduces a novel method of predicting drug-drug interactions (DDI) using drug descriptions with a particular emphasis on zero-shot prediction (i.e., drugs that have not been seen in the training dataset). The method works using two components: a DDI predictor and an information selector. The DDI predictor is trained using random snippets from the descriptions of two drugs as input. The information selector uses reinforcement learning to identify the sentences in the description that maximizes model performance. The model performance of the proposed method substantially outperforms prior work.","Overall, the experiments in this paper are very well done, with averages reported across random seeds, detailed descriptions of the training, development, and test datasets, computational cost experiments, and a useful case study depicting why the method works.

I really liked the idea of the information selector. The approach could be relevant beyond the descriptions in DrugBank, e.g., PubMed research articles. Ultimately, the approach seems generalizable.","
(Minor) The search for relevant information seems greedy. I imagine there are cases where the greedy addition of information can go haywire, resulting in incorrect predictions. It would be useful to see cases when this happens. I assume using beam search or some other method of optimizing the sequence at inference time could help, but it may not completely mitigate the issue.

(Minor) This approach assumes a large description of the new Drug in a very specific style. However, for real ""new"" drugs, this description may be unavailable or unreliable/incomplete. The current evaluation pipeline assumes that these new descriptions are easily accessible, available, and informative. So, the zero-shot results may be overestimated in practice. A solution to this ""cold-start"" issue may be required for accurate performance. I think the limitations section could be expanded on these additional points. Note: This is probably an issue with prior work as well.

(Very Minor) The use of kappa scores seems a bit out-of-place. Instead of using kappa scores o evaluate if the method is better than random, the proposed could be compared to random, and similar, baselines (e.g., predicting at random, predicting the most common class all the time, etc.), which would be a bit more informative.
",Question A: Would the performance be as high as reported for truly new drugs? When would this method fail?,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"There is substantial research on using reinforcement learning for text selection beyond prompt learning. For example, substantial research has been conducted using reinforcement learning for summarization and paraphrasing. Expanding on the related work section in this area could be useful.


REFERENCES:
Yin, Haiyan, Dingcheng Li, and Ping Li. ""Learning to selectively learn for weakly supervised paraphrase generation with model-based reinforcement learning."" Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.

Lee, Gyoung Ho, and Kong Joo Lee. ""Automatic text summarization using reinforcement learning with embedding features."" Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2017.",,1691073554512,,,EMNLP/2023/Conference,3dNeNpmyiO,"['EMNLP/2023/Conference/Submission2702/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461182438,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2702/Reviewer_Na97']",3dNeNpmyiO,['EMNLP/2023/Conference/Submission2702/Reviewer_Na97'],1691073554512,1701461182438,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2702/Reviewer_Na97']","Yes

The suggested papers or references not cited in the manuscript are:
1. Yin, Haiyan, Dingcheng Li, and Ping Li. ""Learning to selectively learn for weakly supervised paraphrase generation with model-based reinforcement learning."" Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.
2. Lee, Gyoung Ho, and Kong Joo Lee. ""Automatic text summarization using reinforcement learning with embedding features."" Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2017.

These citations might be necessary because the reviewer mentions that there is substantial research on using reinforcement learning for text selection beyond prompt learning, such as in summarization and paraphrasing. The reviewer suggests that expanding on the related work section to include these areas could be useful, implying that the authors could benefit from citing and discussing these relevant papers to provide a more comprehensive background and context for their work.",1,"2022, 2022, 2017",Learning to selectively learn for weakly supervised paraphrase generation with model-based reinforcement learning
ltjmS64896,"This paper introduces a novel method of predicting drug-drug interactions (DDI) using drug descriptions with a particular emphasis on zero-shot prediction (i.e., drugs that have not been seen in the training dataset). The method works using two components: a DDI predictor and an information selector. The DDI predictor is trained using random snippets from the descriptions of two drugs as input. The information selector uses reinforcement learning to identify the sentences in the description that maximizes model performance. The model performance of the proposed method substantially outperforms prior work.","Overall, the experiments in this paper are very well done, with averages reported across random seeds, detailed descriptions of the training, development, and test datasets, computational cost experiments, and a useful case study depicting why the method works.

I really liked the idea of the information selector. The approach could be relevant beyond the descriptions in DrugBank, e.g., PubMed research articles. Ultimately, the approach seems generalizable.","
(Minor) The search for relevant information seems greedy. I imagine there are cases where the greedy addition of information can go haywire, resulting in incorrect predictions. It would be useful to see cases when this happens. I assume using beam search or some other method of optimizing the sequence at inference time could help, but it may not completely mitigate the issue.

(Minor) This approach assumes a large description of the new Drug in a very specific style. However, for real ""new"" drugs, this description may be unavailable or unreliable/incomplete. The current evaluation pipeline assumes that these new descriptions are easily accessible, available, and informative. So, the zero-shot results may be overestimated in practice. A solution to this ""cold-start"" issue may be required for accurate performance. I think the limitations section could be expanded on these additional points. Note: This is probably an issue with prior work as well.

(Very Minor) The use of kappa scores seems a bit out-of-place. Instead of using kappa scores o evaluate if the method is better than random, the proposed could be compared to random, and similar, baselines (e.g., predicting at random, predicting the most common class all the time, etc.), which would be a bit more informative.
",Question A: Would the performance be as high as reported for truly new drugs? When would this method fail?,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"There is substantial research on using reinforcement learning for text selection beyond prompt learning. For example, substantial research has been conducted using reinforcement learning for summarization and paraphrasing. Expanding on the related work section in this area could be useful.


REFERENCES:
Yin, Haiyan, Dingcheng Li, and Ping Li. ""Learning to selectively learn for weakly supervised paraphrase generation with model-based reinforcement learning."" Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.

Lee, Gyoung Ho, and Kong Joo Lee. ""Automatic text summarization using reinforcement learning with embedding features."" Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2017.",,1691073554512,,,EMNLP/2023/Conference,3dNeNpmyiO,"['EMNLP/2023/Conference/Submission2702/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461182438,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2702/Reviewer_Na97']",3dNeNpmyiO,['EMNLP/2023/Conference/Submission2702/Reviewer_Na97'],1691073554512,1701461182438,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2702/Reviewer_Na97']","Yes

The suggested papers or references not cited in the manuscript are:
1. Yin, Haiyan, Dingcheng Li, and Ping Li. ""Learning to selectively learn for weakly supervised paraphrase generation with model-based reinforcement learning."" Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.
2. Lee, Gyoung Ho, and Kong Joo Lee. ""Automatic text summarization using reinforcement learning with embedding features."" Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2017.

These citations might be necessary because the reviewer mentions that there is substantial research on using reinforcement learning for text selection beyond prompt learning, such as in summarization and paraphrasing. The reviewer suggests that expanding on the related work section to include these areas could be useful, implying that the authors could benefit from citing and discussing these relevant papers to provide a more comprehensive background and context for their work.",1,"2022, 2022, 2017",Automatic text summarization using reinforcement learning with embedding features
YJxmn4BbPZ,"- The paper deal with the problem of knowledge-based visual question answering (KB-VQA).
- The authors propose fully white-box framework for KB-VQA based on LLaMA-13B model.
- The proposed framework achieves comparable performance with GPT-3 API-based VQA baselines and does not require any addional training steps.","- If I understood correctly, this work is the first LLM-based reproducible open-source VQA framework. Since VQA is one of the most practical problem in NLP era, this framework can be employed as basis for the future research on VQA. 
- The empirical results show that the proposed VQA framework achieves SOTA peformance which is comparable with existing black-box framework based on GPT-3 for OK-VQA dataset.
- The proposed method is simple and intuitive.
- Nice ablation results (Section 5).","- I think novelty of the proposed method is not so significant. (However, it is not important for this kind of research.)
- Please refer to questions.","- If possible, can you provide peformance after some fine-tuning steps? This can prove wider applicability of the proposed framework.
- [1] propose a new ensemble scheme for QA with few-shot examples. Can you provide performance of your framework equipped with idea of [1] in multi-query ensemble?
- If I understood correctly, we may replace LLaMA-13B model in the overall process of this paper into GPT-4, which is the state-of-the-art large language model. Since, this work utilizes LLM without any finetuning, I expect that GPT-4 can show upper-bound performance which is higher than LLaMA-13B. Even though GPT-4 is black-box model, I want to see the upper bound performance of the framwork.

[1] Replug: Retrieval-augmented black-box language models, Shi et al","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"2: Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,,1691227187661,,,EMNLP/2023/Conference,3XDDWCu8CF,"['EMNLP/2023/Conference/Submission3895/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461258964,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3895/Reviewer_P6LA']",3XDDWCu8CF,['EMNLP/2023/Conference/Submission3895/Reviewer_P6LA'],1691227187661,1701461258964,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3895/Reviewer_P6LA']","Yes

List of suggested papers or references not cited in the manuscript:
1. [1] ""Replug: Retrieval-augmented black-box language models"" by Shi et al

These citations might be necessary because the reviewer is asking the authors to provide performance of their framework equipped with the idea of [1] in multi-query ensemble. This implies that the reviewer thinks the work of Shi et al is relevant to the research presented in the manuscript, and that incorporating their ideas could potentially improve the proposed framework. The reviewer's suggestion to explore this connection indicates that the authors should be aware of and engage with the work of Shi et al, which could involve citing their paper.",1,,Replug: Retrieval-augmented black-box language models
eOZQEH74lU,"This paper focuses on a new task, In-Image Machine Translation (IIMT), and proposes a novel and effective solution. The input and output images are converted into pixel sequences, and the grayscale values are mapped to discrete symbols (1-10). A BPE vocabulary is learned on the symbol sequence, and a sequence-to-sequence Transformer model is used to learn the mapping between the source and target sequences. Experimental results on a created benchmark demonstrate the effectiveness of the proposed method.","- The proposed framework is surprisingly effective in translating the source image to the target image without any textual supervision during training, outperforming the cascaded OCR+MT baseline. It demonstrates that the segmented pixel sequence is a simple yet effective intermediate representation for this task.
- The paper is well written and easy to follow. The authors did detailed analysis of the proposed method.","The created dataset primarily contains images with black text and white backgrounds, which may be simplified compared to real-world scenarios. It is unclear if the proposed method will outperform the cascaded method in more complex scenarios.","- It is not entirely clear how an image with a size of HxW is converted into a sequence of length T in the proposed method. However, it appears that the image is organized as a sequence from top to bottom and then from left to right, as shown in Figure 5. More explanation is needed in the paper to clarify this process.
- The BLEU score of the Gloden NMT model on the newstest-2014 seems slightly higher than results reported in previous NMT paper. What about the details of your NMT model?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,Robust Open­Vocabulary Translation from Visual Text Representations,,1690270618440,,,EMNLP/2023/Conference,3RS2T9EPjI,"['EMNLP/2023/Conference/Submission3530/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461235406,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3530/Reviewer_9Cpu']",3RS2T9EPjI,['EMNLP/2023/Conference/Submission3530/Reviewer_9Cpu'],1690270618440,1701461235406,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3530/Reviewer_9Cpu']","Yes

* ""Robust Open­Vocabulary Translation from Visual Text Representations""

The reviewer suggests citing this paper because it appears to be relevant to the topic of the submitted manuscript, specifically in the area of visual text representations and translation. The reviewer mentions ""Missing_References"" and lists this paper, implying that it is a notable work that the authors should be aware of and potentially cite in their manuscript to provide a more comprehensive overview of the field.",1,,Robust Open­Vocabulary Translation from Visual Text Representations
qXqvw7mtKz,This paper proposes an end-to-end image-to-image machine translation model. The main contribution of this paper is using a pixel sequence-based method to transform source language text images into target language text images.,The experiment results show the proposed method can outperform cascade model on in-domain and out-domain test sets.,"This paper has several shortcomings that should be rejected:
1. The description of the pixel sequence-based in-image machine translation model is unclear, such as 1) how to transform the image into pixel sequence is incomplete, 2) the method of transforming RGB to grayscale map is unclear, and 3) the training and inference details are missing, and the optimization object is not mentioned.
2. The constructed dataset in this paper is quite simple, which just considers the black font and white background image. As so, the experiments and analysis are based on a toy task in the in-image translation area, which has limited influence in real-world applications. Various text image effects should be considered in ablation studies to show the generalization of the proposed methods.
3. There are certain problems with paper writing. Some explanations for variables and symbols are missing. Thus, this paper should take a major revision before publication.
","1. In the task formulation section, what's the meaning of \hat{Y}? Although the readers might guess the meaning of \hat{Y}, the authors should clarify to eliminate misunderstandings.
2. The data construction in this paper just renders texts in the parallel corpora into images with white background, which is quite easy data distribution of text images. The authors mention their method has strong scalability, I wonder whether authors conduct any experiments on other data distribution of text images like changing the font, font colors, background colors, and background images?
3. What's the method of converting an RGB image into a grayscale map in your implementation (Line 186-188 in the paper)? There are multiple methods of converting an RGB image into a grayscale map, what's your implementation? PIL.image.convert(“L”) or pixel averaging through channels or any other methods? If the channel conversion is conducted using open-source packages or through existing methods, please provide the conversion formula, reference citation, or footnote with a reference link to the open-source package.
4. Since the dataset in your work is images with black font and white background of the one-line text. Is the RGB image necessary and do you conduct any ablation study?
5. Figure 5 shows the Visualization of segmentation with different iteration times. Each segmentation results have several missing pixels at the bottom-right corner, what’s the reason for these missing pixels? Furthermore, What about the robustness of the proposed segmentation method? When there are Gaussian noise, Salt-and-pepper noise, and other pixel noises, what are the influences of the segmentation results? Have you conducted any analysis experiments? Experiments and analysis should be conducted in the paper to show the generalization of the proposed pixel sequence segmentation. 
6. In sub-section 6.4, there is no comparison and analysis of end-to-end models with cascade models. As so, error propagation indeed exists in cascade methods, and can your proposed end-to-end methods address this problem? What's the relationship between the analysis of error propagation of the cascade method with your proposed end-to-end method? Furthermore, the case study in Table 5 lacks the results of your proposed end-to-end model, which cannot prove your end-to-end model can reduce the influence of error propagation.
7. For image encoding and decoding, the pixel sequence transformation and pixel sequence segmentation might be one of your contributions. However, there is no comparison with vanilla patch-based image encoding and decoding methods, which is a quite vital baseline for your work.
","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Path-based image encoding should be considered as a vital baseline for this paper:
[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021
","What's the meaning of T in Equation (3)? Meanwhile, the detailed clarification of function F_{ITS}(x) is missing. Subsection 4.2 only introduces 1) transforming the RGB image into a grayscale map and 2) quantizing values in the grayscale map into a character map. I believe that there must have a step 3) transforming character map into pixel sequences but it is missing in the paper. If there is no step 3), the character map of length H\times W (Line 203 in the paper) is different from x_{tok}\in\mathbb{R}^{T\times 1} as in Equation (3).",1690970010711,,,EMNLP/2023/Conference,3RS2T9EPjI,"['EMNLP/2023/Conference/Submission3530/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461235207,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3530/Reviewer_ApGL']",3RS2T9EPjI,['EMNLP/2023/Conference/Submission3530/Reviewer_ApGL'],1690970010711,1701461235207,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3530/Reviewer_ApGL']","Yes

The suggested paper not cited in the manuscript is:
[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021

This citation might be necessary because the reviewer mentions that path-based image encoding, as discussed in the suggested paper, should be considered as a vital baseline for the work presented in the manuscript. The reviewer suggests that comparing the proposed method with vanilla patch-based image encoding and decoding methods, such as those presented in the suggested paper, is essential to evaluate the effectiveness of the proposed approach.",1,2021,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
WDwMonJ5Es,"The authors present a unified toolkit for the evaluation if interactive machine translation approaches. Formally, they define a set of operations (keep, insert, replace, delete, blank filling) and policies (prefix decoding and several variations of simulating in-filling). Given a policy and a set of edits, the backing machine translation systems are queried to provide a step towards an improved translation. In an evaluation, four different models are compared employing the four different policies. Metrics include the model's response time and the average number of turns needed to arrive at the final translation. In simulated experiments the authors show that a traditional transformer model doing prefix-decoding performs best in most metrics, an in-filling approach comes however quite close.
Lastly, the authors present a short human evaluation, where first good correlation is shown to the simulated experiments.","- The unified toolkit for evaluating interactive machine translation is likely to be very useful to compare interactive machine translation approaches.
- The proposed evaluation metrics and abstractions should further help in this regard.",- At times the paper is lacking important details which may hinder replicability.,"- Line 323: I'm not entirely sure what MTPE is: Is the MT output corrected in 1 step? If not, which edit is applied first? In general it would be helpful to have examples for each policy.
- How is prefix decoding implemented in the policies and in the user interface? First removal of the remaining suffix and then blanking?
- It would be intersting to compare the overall editing time in the human evaluation.
- Line 450: What is the variance in the Rand experiments?
- Line 405: What is the success rate in the simulated experiments?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"- ""A Post-editing Interface for Immediate Adaptation in Statistical Machine Translation"" presents a variant of interactive machine translation where phrases of a phrase-based MT system can be moved inidivudally.",- The tables are somewhat too dense.,1689960474909,,,EMNLP/2023/Conference,3QzTzulZwY,"['EMNLP/2023/Conference/Submission2782/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461187665,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2782/Reviewer_UJug']",3QzTzulZwY,['EMNLP/2023/Conference/Submission2782/Reviewer_UJug'],1689960474909,1701461187665,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2782/Reviewer_UJug']","Yes

The suggested paper not cited in the manuscript is:
- ""A Post-editing Interface for Immediate Adaptation in Statistical Machine Translation""

This citation might be necessary because the reviewer mentions it as a relevant work that presents a variant of interactive machine translation, which is the topic of the submitted paper. The reviewer suggests that this reference is missing, implying that it could provide additional context or related work that would strengthen the manuscript.",1,,A Post-editing Interface for Immediate Adaptation in Statistical Machine Translation
BbzMXWGpgE,"This paper explores the potential of LLMs like ChatGPT for relevance ranking. The authors explore query generation, relevance generation, and propose permutation generation + sliding window as different prompting strategies. They also investigate whether the LLM generated results can be distilled into smaller models. They conduct experiments on TREC-DL, BEIR and Mr.TyDi data sets, and a proposed new data set NovelEval and show that LLM can be more effective than supervised rankers like monoT5. They also show that the distilled model performs better than model trained from scratch. ","* Directly using LLM to produce the ranking of multiple documents is a novel and interesting idea and worth further explorations.

* Thorough experiments conducted on multiple data sets, including a newly proposed NovelEval data set.

* The work on distillation makes the technique more applicable in the real world.
","
* Potential unfair comparison

  * Baselines like UPR are essentially QG and thus can be directly applied using the same LLM as the proposed method. Since PG is a major contribution of this paper, it is better to compare QG vs. RG vs. PG using the same LLM. However, in Table 4 there are only partial results reported (missing PG for curie-001 and missing RG for davinci-003). 

  * As suggested in Section 6.5 Line 459, the LLM results seem to be extremely sensitive to the initial passage order of BM25, which other supervised models or methods like monoT5/UPR do not have access to. A potentially better comparison is to compare with an ensemble result of monoT5 + BM25 (or UPR + BM25). Otherwise it is not convincing that permutation generation + sliding window is a better strategy than query generation like UPR. 

  * Most baseline methods only see one query-document pair at a time, while PG sees multiple at a time. Perhaps methods like duoT5 [1] should be more appropriate baselines. 

* The method might be sensitive to different parameters of the sliding window strategy. More experiments can be done to provide guidance on how to set the parameters of the sliding window strategy. 

* Methods might be overfitting to ChatGPT. The gap between LLMs reported in Table 6 is large. Some of them are even lower than or close to BM25, which seems suspicious. It is possible that the prompt employed by the authors are heavily engineered for ChatGPT and hence does not generalize to other LLMs. 
","Question A: Does the supervised learning baseline in Figure 4 use the same RankNet loss? Otherwise it is not clear whether the improvement comes from using the ranking loss or from the teacher labels, since ranking losses can also help improve the model performance [2].",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"
[1] Pradeep, Ronak, Rodrigo Nogueira, and Jimmy Lin. ""The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models."" arXiv preprint arXiv:2101.05667 (2021).

[2] Zhuang, Honglei, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. ""RankT5: Fine-tuning T5 for text ranking with ranking losses."" In SIGIR. 2023.
","Table 6: ND might be a confusing abbreviation for nDCG
",1691227843827,,,EMNLP/2023/Conference,3Q6LON8y2I,"['EMNLP/2023/Conference/Submission3543/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461236227,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3543/Reviewer_Xw3x']",3Q6LON8y2I,['EMNLP/2023/Conference/Submission3543/Reviewer_Xw3x'],1691227843827,1701461236227,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3543/Reviewer_Xw3x']","Yes

The suggested papers or references not cited in the manuscript are:
1. Pradeep, Ronak, Rodrigo Nogueira, and Jimmy Lin. ""The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models."" arXiv preprint arXiv:2101.05667 (2021).
2. Zhuang, Honglei, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. ""RankT5: Fine-tuning T5 for text ranking with ranking losses."" In SIGIR. 2023.

These citations might be necessary because the reviewer suggests that the authors should compare their method with more appropriate baselines, such as duoT5, and also clarify whether the improvement comes from using the ranking loss or from the teacher labels. The reviewer mentions that ranking losses can also help improve model performance, citing the RankT5 paper. Additionally, the expando-mono-duo design pattern paper is suggested as a relevant reference for text ranking with pretrained sequence-to-sequence models.",1,"2021, 2021, 2023",The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models 
BbzMXWGpgE,"This paper explores the potential of LLMs like ChatGPT for relevance ranking. The authors explore query generation, relevance generation, and propose permutation generation + sliding window as different prompting strategies. They also investigate whether the LLM generated results can be distilled into smaller models. They conduct experiments on TREC-DL, BEIR and Mr.TyDi data sets, and a proposed new data set NovelEval and show that LLM can be more effective than supervised rankers like monoT5. They also show that the distilled model performs better than model trained from scratch. ","* Directly using LLM to produce the ranking of multiple documents is a novel and interesting idea and worth further explorations.

* Thorough experiments conducted on multiple data sets, including a newly proposed NovelEval data set.

* The work on distillation makes the technique more applicable in the real world.
","
* Potential unfair comparison

  * Baselines like UPR are essentially QG and thus can be directly applied using the same LLM as the proposed method. Since PG is a major contribution of this paper, it is better to compare QG vs. RG vs. PG using the same LLM. However, in Table 4 there are only partial results reported (missing PG for curie-001 and missing RG for davinci-003). 

  * As suggested in Section 6.5 Line 459, the LLM results seem to be extremely sensitive to the initial passage order of BM25, which other supervised models or methods like monoT5/UPR do not have access to. A potentially better comparison is to compare with an ensemble result of monoT5 + BM25 (or UPR + BM25). Otherwise it is not convincing that permutation generation + sliding window is a better strategy than query generation like UPR. 

  * Most baseline methods only see one query-document pair at a time, while PG sees multiple at a time. Perhaps methods like duoT5 [1] should be more appropriate baselines. 

* The method might be sensitive to different parameters of the sliding window strategy. More experiments can be done to provide guidance on how to set the parameters of the sliding window strategy. 

* Methods might be overfitting to ChatGPT. The gap between LLMs reported in Table 6 is large. Some of them are even lower than or close to BM25, which seems suspicious. It is possible that the prompt employed by the authors are heavily engineered for ChatGPT and hence does not generalize to other LLMs. 
","Question A: Does the supervised learning baseline in Figure 4 use the same RankNet loss? Otherwise it is not clear whether the improvement comes from using the ranking loss or from the teacher labels, since ranking losses can also help improve the model performance [2].",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"
[1] Pradeep, Ronak, Rodrigo Nogueira, and Jimmy Lin. ""The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models."" arXiv preprint arXiv:2101.05667 (2021).

[2] Zhuang, Honglei, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. ""RankT5: Fine-tuning T5 for text ranking with ranking losses."" In SIGIR. 2023.
","Table 6: ND might be a confusing abbreviation for nDCG
",1691227843827,,,EMNLP/2023/Conference,3Q6LON8y2I,"['EMNLP/2023/Conference/Submission3543/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461236227,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3543/Reviewer_Xw3x']",3Q6LON8y2I,['EMNLP/2023/Conference/Submission3543/Reviewer_Xw3x'],1691227843827,1701461236227,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3543/Reviewer_Xw3x']","Yes

The suggested papers or references not cited in the manuscript are:
1. Pradeep, Ronak, Rodrigo Nogueira, and Jimmy Lin. ""The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models."" arXiv preprint arXiv:2101.05667 (2021).
2. Zhuang, Honglei, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. ""RankT5: Fine-tuning T5 for text ranking with ranking losses."" In SIGIR. 2023.

These citations might be necessary because the reviewer suggests that the authors should compare their method with more appropriate baselines, such as duoT5, and also clarify whether the improvement comes from using the ranking loss or from the teacher labels. The reviewer mentions that ranking losses can also help improve model performance, citing the RankT5 paper. Additionally, the expando-mono-duo design pattern paper is suggested as a relevant reference for text ranking with pretrained sequence-to-sequence models.",1,"2021, 2021, 2023",RankT5 Fine-tuning T5 for text ranking with ranking losses
mX0NwNCjol,"This paper introduces a multi-task learning approach for backchannel prediction in dialogues, enhancing textual representations and utilizing self-supervised pre-training for audio encoders. Outperforming previous models by 3% in WF1, improving computational understanding of back-channeling for applications like conversational AI and speech recognition.","1. Proposed a multi-tasking method that jointly models 3 way classification of and 42-way classification of DA in a speech-text multimodal approach
2. Achieve improvement compared with previous methods.","1. Innovations are some how limited, a combination of existing work. Experimental settings/datasets from Ortega, Multi-task from BPM_MT, and pre-trained audio encoder from Ekstedt (VAT). 
2. The improvement from the proposed idea is incremental (<1%), and a significant test is needed to prove its effectiveness.
3. Lack of in-depth analysis and insights.","1. From Table 2, adding the proposed multi-task learning increases merely by 0.6% accuracy (65.5% —> 66.1%); a significant test is needed to prove the effectiveness, such as a single-tail t-test. Most of the improvement comes from using the existing pre-trained audio encoder.
2. In the abstract: ""we further pre-train the audio encoder in a self-supervised,"" the authors actually use the audio pre-trained model to initialize the audio encoder. It seems that the paper does not involve a pre-training stage.
3. Please provide the detailed distribution of the 42 DA classification. There is only 40+1 DA (plus one empty) in the training code, and the statement-non-opinion (68.5%) and statement-opinion (24.2%) is not match the distribution in SwDA [1].
4. Turn-taking is just a subset of dialog act, where in this work, the author frames it as a three-way classification (no-BC, BC-Continuer, BC-Assessment). The author may compare the traditional DA classification method with BCDA_MT. For example, [2] could achieve 83.2% ACC in the 42 DA classification.

[1] https://compprag.christopherpotts.net/swda.html#tags
[2] He, Zihao, et al. ""Speaker Turn Modeling for Dialogue Act Classification."" Findings of the Association for Computational Linguistics: EMNLP 2021. 2021.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"He, Zihao, et al. ""Speaker Turn Modeling for Dialogue Act Classification."" Findings of the Association for Computational Linguistics: EMNLP 2021. 2021.
Skantze, Gabriel. ""Turn-taking in conversational systems and human-robot interaction: a review."" Computer Speech & Language 67 (2021): 101178.",The manuscript is more of a technical report than a research paper. The author should focus on the original part rather than the known technique.,1690870330923,,,EMNLP/2023/Conference,3OvLxe9n9S,"['EMNLP/2023/Conference/Submission3403/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461227108,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3403/Reviewer_ww4h']",3OvLxe9n9S,['EMNLP/2023/Conference/Submission3403/Reviewer_ww4h'],1690870330923,1701461227108,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3403/Reviewer_ww4h']","Yes

The suggested papers or references not cited in the manuscript are:
1. He, Zihao, et al. ""Speaker Turn Modeling for Dialogue Act Classification."" (already included in the manuscript as [2])
2. Skantze, Gabriel. ""Turn-taking in conversational systems and human-robot interaction: a review."" 

These citations might be necessary because the reviewer mentions them as relevant works that the authors could compare their method to, particularly for traditional DA classification methods and turn-taking in conversational systems. The reviewer suggests that comparing the proposed BCDA_MT method to traditional DA classification methods, such as the one in [2], could provide a more comprehensive evaluation of the approach. The Skantze reference is likely suggested to provide additional context on turn-taking in conversational systems, which is related to the backchannel prediction task addressed in the manuscript.",1,,Speaker Turn Modeling for Dialogue Act Classification 
mX0NwNCjol,"This paper introduces a multi-task learning approach for backchannel prediction in dialogues, enhancing textual representations and utilizing self-supervised pre-training for audio encoders. Outperforming previous models by 3% in WF1, improving computational understanding of back-channeling for applications like conversational AI and speech recognition.","1. Proposed a multi-tasking method that jointly models 3 way classification of and 42-way classification of DA in a speech-text multimodal approach
2. Achieve improvement compared with previous methods.","1. Innovations are some how limited, a combination of existing work. Experimental settings/datasets from Ortega, Multi-task from BPM_MT, and pre-trained audio encoder from Ekstedt (VAT). 
2. The improvement from the proposed idea is incremental (<1%), and a significant test is needed to prove its effectiveness.
3. Lack of in-depth analysis and insights.","1. From Table 2, adding the proposed multi-task learning increases merely by 0.6% accuracy (65.5% —> 66.1%); a significant test is needed to prove the effectiveness, such as a single-tail t-test. Most of the improvement comes from using the existing pre-trained audio encoder.
2. In the abstract: ""we further pre-train the audio encoder in a self-supervised,"" the authors actually use the audio pre-trained model to initialize the audio encoder. It seems that the paper does not involve a pre-training stage.
3. Please provide the detailed distribution of the 42 DA classification. There is only 40+1 DA (plus one empty) in the training code, and the statement-non-opinion (68.5%) and statement-opinion (24.2%) is not match the distribution in SwDA [1].
4. Turn-taking is just a subset of dialog act, where in this work, the author frames it as a three-way classification (no-BC, BC-Continuer, BC-Assessment). The author may compare the traditional DA classification method with BCDA_MT. For example, [2] could achieve 83.2% ACC in the 42 DA classification.

[1] https://compprag.christopherpotts.net/swda.html#tags
[2] He, Zihao, et al. ""Speaker Turn Modeling for Dialogue Act Classification."" Findings of the Association for Computational Linguistics: EMNLP 2021. 2021.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"He, Zihao, et al. ""Speaker Turn Modeling for Dialogue Act Classification."" Findings of the Association for Computational Linguistics: EMNLP 2021. 2021.
Skantze, Gabriel. ""Turn-taking in conversational systems and human-robot interaction: a review."" Computer Speech & Language 67 (2021): 101178.",The manuscript is more of a technical report than a research paper. The author should focus on the original part rather than the known technique.,1690870330923,,,EMNLP/2023/Conference,3OvLxe9n9S,"['EMNLP/2023/Conference/Submission3403/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461227108,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3403/Reviewer_ww4h']",3OvLxe9n9S,['EMNLP/2023/Conference/Submission3403/Reviewer_ww4h'],1690870330923,1701461227108,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3403/Reviewer_ww4h']","Yes

The suggested papers or references not cited in the manuscript are:
1. He, Zihao, et al. ""Speaker Turn Modeling for Dialogue Act Classification."" (already included in the manuscript as [2])
2. Skantze, Gabriel. ""Turn-taking in conversational systems and human-robot interaction: a review."" 

These citations might be necessary because the reviewer mentions them as relevant works that the authors could compare their method to, particularly for traditional DA classification methods and turn-taking in conversational systems. The reviewer suggests that comparing the proposed BCDA_MT method to traditional DA classification methods, such as the one in [2], could provide a more comprehensive evaluation of the approach. The Skantze reference is likely suggested to provide additional context on turn-taking in conversational systems, which is related to the backchannel prediction task addressed in the manuscript.",1,,Turn-taking in conversational systems and human-robot interaction a review
89esjCdki7,"The paper proposes VQA dataset that focuses on visual info-seeking questions. The authors evaluate the zero-shot, fine-tuning, and retrieval performance of sota pre-trained models on the dataset.","1. The paper constructs over 1 million visual information-seeking QA pairs using the Wikidata database. Compared to previous works, this dataset covers more fine-grained knowledge and visual understanding, which could potentially accelerate community development.
2. The paper validates the dataset's effectivenessthrough fine-tuning on VLPs.","1. This work has limited innovation, except for proposing a dataset；
2. This  evaluation has too few models，lacks some of the latest multimodal models including SFT such as MiniGPT4, LLaVA, etc.","1. when will the dataset be open source? It has been posted for almost half a year now？
2.  Provide a detailed introduction to the model fine-tuning setting.
3. The scale of the BLIP2 and PALI-X models and the difference in pre-training data are significant, the comparison between these two models unfair.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"The latest multimodal models such as MiniGPT4, LLaVA, mPLUG-Owl etc.",,1691653275340,,,EMNLP/2023/Conference,3MEV3aIDDq,"['EMNLP/2023/Conference/Submission2309/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461156492,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2309/Reviewer_Kp26']",3MEV3aIDDq,['EMNLP/2023/Conference/Submission2309/Reviewer_Kp26'],1691653275340,1701461156492,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2309/Reviewer_Kp26']","Yes

The suggested papers or references not cited in the manuscript are:
1. MiniGPT4
2. LLaVA
3. mPLUG-Owl
4. SFT (likely referring to a specific paper on this topic, but the acronym is not fully expanded)

These citations might be necessary because the reviewer mentions that the evaluation in the paper ""has too few models, lacks some of the latest multimodal models"" and specifically names these models as examples of missing references. The reviewer implies that including these models in the evaluation would strengthen the paper, suggesting that the authors should be aware of and engage with the latest developments in the field, as represented by these models.",1,,MiniGPT4 
89esjCdki7,"The paper proposes VQA dataset that focuses on visual info-seeking questions. The authors evaluate the zero-shot, fine-tuning, and retrieval performance of sota pre-trained models on the dataset.","1. The paper constructs over 1 million visual information-seeking QA pairs using the Wikidata database. Compared to previous works, this dataset covers more fine-grained knowledge and visual understanding, which could potentially accelerate community development.
2. The paper validates the dataset's effectivenessthrough fine-tuning on VLPs.","1. This work has limited innovation, except for proposing a dataset；
2. This  evaluation has too few models，lacks some of the latest multimodal models including SFT such as MiniGPT4, LLaVA, etc.","1. when will the dataset be open source? It has been posted for almost half a year now？
2.  Provide a detailed introduction to the model fine-tuning setting.
3. The scale of the BLIP2 and PALI-X models and the difference in pre-training data are significant, the comparison between these two models unfair.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"The latest multimodal models such as MiniGPT4, LLaVA, mPLUG-Owl etc.",,1691653275340,,,EMNLP/2023/Conference,3MEV3aIDDq,"['EMNLP/2023/Conference/Submission2309/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461156492,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2309/Reviewer_Kp26']",3MEV3aIDDq,['EMNLP/2023/Conference/Submission2309/Reviewer_Kp26'],1691653275340,1701461156492,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2309/Reviewer_Kp26']","Yes

The suggested papers or references not cited in the manuscript are:
1. MiniGPT4
2. LLaVA
3. mPLUG-Owl
4. SFT (likely referring to a specific paper on this topic, but the acronym is not fully expanded)

These citations might be necessary because the reviewer mentions that the evaluation in the paper ""has too few models, lacks some of the latest multimodal models"" and specifically names these models as examples of missing references. The reviewer implies that including these models in the evaluation would strengthen the paper, suggesting that the authors should be aware of and engage with the latest developments in the field, as represented by these models.",1,,LLaVA 
89esjCdki7,"The paper proposes VQA dataset that focuses on visual info-seeking questions. The authors evaluate the zero-shot, fine-tuning, and retrieval performance of sota pre-trained models on the dataset.","1. The paper constructs over 1 million visual information-seeking QA pairs using the Wikidata database. Compared to previous works, this dataset covers more fine-grained knowledge and visual understanding, which could potentially accelerate community development.
2. The paper validates the dataset's effectivenessthrough fine-tuning on VLPs.","1. This work has limited innovation, except for proposing a dataset；
2. This  evaluation has too few models，lacks some of the latest multimodal models including SFT such as MiniGPT4, LLaVA, etc.","1. when will the dataset be open source? It has been posted for almost half a year now？
2.  Provide a detailed introduction to the model fine-tuning setting.
3. The scale of the BLIP2 and PALI-X models and the difference in pre-training data are significant, the comparison between these two models unfair.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"The latest multimodal models such as MiniGPT4, LLaVA, mPLUG-Owl etc.",,1691653275340,,,EMNLP/2023/Conference,3MEV3aIDDq,"['EMNLP/2023/Conference/Submission2309/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461156492,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2309/Reviewer_Kp26']",3MEV3aIDDq,['EMNLP/2023/Conference/Submission2309/Reviewer_Kp26'],1691653275340,1701461156492,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2309/Reviewer_Kp26']","Yes

The suggested papers or references not cited in the manuscript are:
1. MiniGPT4
2. LLaVA
3. mPLUG-Owl
4. SFT (likely referring to a specific paper on this topic, but the acronym is not fully expanded)

These citations might be necessary because the reviewer mentions that the evaluation in the paper ""has too few models, lacks some of the latest multimodal models"" and specifically names these models as examples of missing references. The reviewer implies that including these models in the evaluation would strengthen the paper, suggesting that the authors should be aware of and engage with the latest developments in the field, as represented by these models.",1,,mPLUG-Owl 
89esjCdki7,"The paper proposes VQA dataset that focuses on visual info-seeking questions. The authors evaluate the zero-shot, fine-tuning, and retrieval performance of sota pre-trained models on the dataset.","1. The paper constructs over 1 million visual information-seeking QA pairs using the Wikidata database. Compared to previous works, this dataset covers more fine-grained knowledge and visual understanding, which could potentially accelerate community development.
2. The paper validates the dataset's effectivenessthrough fine-tuning on VLPs.","1. This work has limited innovation, except for proposing a dataset；
2. This  evaluation has too few models，lacks some of the latest multimodal models including SFT such as MiniGPT4, LLaVA, etc.","1. when will the dataset be open source? It has been posted for almost half a year now？
2.  Provide a detailed introduction to the model fine-tuning setting.
3. The scale of the BLIP2 and PALI-X models and the difference in pre-training data are significant, the comparison between these two models unfair.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"The latest multimodal models such as MiniGPT4, LLaVA, mPLUG-Owl etc.",,1691653275340,,,EMNLP/2023/Conference,3MEV3aIDDq,"['EMNLP/2023/Conference/Submission2309/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461156492,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2309/Reviewer_Kp26']",3MEV3aIDDq,['EMNLP/2023/Conference/Submission2309/Reviewer_Kp26'],1691653275340,1701461156492,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2309/Reviewer_Kp26']","Yes

The suggested papers or references not cited in the manuscript are:
1. MiniGPT4
2. LLaVA
3. mPLUG-Owl
4. SFT (likely referring to a specific paper on this topic, but the acronym is not fully expanded)

These citations might be necessary because the reviewer mentions that the evaluation in the paper ""has too few models, lacks some of the latest multimodal models"" and specifically names these models as examples of missing references. The reviewer implies that including these models in the evaluation would strengthen the paper, suggesting that the authors should be aware of and engage with the latest developments in the field, as represented by these models.",1,,SFT
aXBXyh0CEJ,"This paper presents the Event Dependency Relation (EDeR) dataset, which annotates binary argument relations on close to 12,000 pairs of predicates from a subset of documents from the OntoNotes dataset. Relations between predicates are assigned one of four labels: for eventive arguments, one predicate may be semantically **required** by the other or else may be **optional**; and for eventive *non-arguments*, one predicate may nonetheless express a **condition** on the other predicate or else may be fully **independent** of it. The paper also presents baseline models for relation classification on EDeR in both fine-tuned (using standard pretrained LMs) and few-shot settings (using ChatGPT), with experiments showing performance variation across different input representations. Results from additional experiments argue that incorporating event dependency information into training for related tasks (semantic role labeling and coreference resolution) may boost performance on those tasks.","- The dataset seems to be fairly carefully constructed and likely to be of interest to a significant subset of the NLP community, including folks working on event semantics and on information extraction. The fact that it builds on an existing widely used benchmark (OntoNotes) is also a plus.
- The experiments in general seem to accomplish most of what one expects of experiments in a resource paper — namely, establishing an accessible set of baselines and demonstrating the utility of the resource for existing tasks.","I do not in general have significant methodological concerns with the paper; most of my concerns relate to presentation and certain points of clarity (see **Questions for The Authors** and **Typos, Grammar, Style, and Presentation Improvements**), though I'll raise one point about annotator agreement here. While the authors' description of the data collection procedure inspires confidence with respect to the dataset's quality (notably, the fact that the 5% sampled annotations had 95% accuracy with respect to the authors' judgments), no standard agreement metrics are presented on the full subset of the data that is redundantly annotated. This does make it somewhat difficult to get a full sense for its quality.","A. How was the total number of documents for annotation (275) determined? Was this just based on resource constraints or was there some other principle behind it?

B. It is counterintuitive to me that the modified event structures would boost performance on entity coreference resolution. If I understand correctly, by pruning dependency parses based on the event relations, isn't it possible that you are dropping spans that participate in a gold coreference chain? Or does the model still have access to these spans, albeit not as part of the dependency parse? I feel like I must be misunderstanding something here.

C. The authors note that ""For the analysis of the dataset and experiments in the remainder of this paper, we treat the two non-argument labels as one"" (L253-5). Why was this done? Among non-arguments, the distinction between a condition and a wholly independent event is semantically very salient.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"I'm aware of several datasets that are distinct in their aims from the present work but that are worth citing as relevant general background:
- The Basic dataset from the IARPA BETTER program (see [here](https://ir.nist.gov/better/)) contain event structures with eventive arguments (so-called ""referred events""). NB: To my knowledge, there is no publication associated with the Basic datasets specifically, but see [this paper](https://aclanthology.org/2022.lrec-1.384/) for a description of the program and the related Abstract datasets.
- ""Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation"" (O'Gorman et al., 2016) has received a good amount of attention and contains several different types of event-event relations.
- ""Decomposing and Recomposing Event Structure"" (Gantt et al., 2022) annotates the English Web Treebank for event parthood relations as part of its event structure dataset.","Typos / Grammar:

In general, the paper would benefit from a careful proofread. Below are a sampling of suggested edits:
- 142-3: ""Several event-event relations have been proposed for decades"" -> (e.g.) ""Various event relation ontologies have been proposed over the years.""
- 179: ""an optional argument of, or a condition of"" -> ""an optional argument of, a condition of""
- 332: hence -> and so
- 394-6: this description of the rule-based classification method is rather awkward. Please rephrase.
- 529: feature -> features
- figure 6 caption: help -> helps

Presentation:
- Majority class baseline accuracy really should be explicitly presented (even if only in text) for comparison with the results in Table 3 (though I believe it's inferrable from Table 2).
- Table 3 should also include F1 scores
- For clarity, it would be good to have explicit examples of the ""Event-Event-SRL"" and ""Event-Event-SRL-DEP"" input variations presented in 4.2. (This is already done for the other two variants.)
- An analysis of model performance broken down by label, possibly including some qualitative evaluations, would (for my money) be more interesting and informative than the present analysis based on predicate distance and sentence length (though this analysis is still worthwhile).",1690306573707,,,EMNLP/2023/Conference,3LdaPmAnji,"['EMNLP/2023/Conference/Submission1716/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461119546,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1716/Reviewer_R3vw']",3LdaPmAnji,['EMNLP/2023/Conference/Submission1716/Reviewer_R3vw'],1690306573707,1701461119546,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1716/Reviewer_R3vw']","Yes

The suggested papers or references not cited in the manuscript are:
1. The Basic dataset from the IARPA BETTER program
2. ""Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation"" (O'Gorman et al., 2016)
3. ""Decomposing and Recomposing Event Structure"" (Gantt et al., 2022)
4. A paper describing the Abstract datasets from the IARPA BETTER program, referenced as [this paper](https://aclanthology.org/2022.lrec-1.384/)

These citations might be necessary because they provide relevant background information on event structures and event-event relations, which are central to the paper's topic. The reviewer notes that while these datasets and papers have distinct aims from the present work, they are worth citing as relevant general background, suggesting that they could enhance the manuscript's context and credibility.",1,"2016, 2022, 2022","Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation "
aXBXyh0CEJ,"This paper presents the Event Dependency Relation (EDeR) dataset, which annotates binary argument relations on close to 12,000 pairs of predicates from a subset of documents from the OntoNotes dataset. Relations between predicates are assigned one of four labels: for eventive arguments, one predicate may be semantically **required** by the other or else may be **optional**; and for eventive *non-arguments*, one predicate may nonetheless express a **condition** on the other predicate or else may be fully **independent** of it. The paper also presents baseline models for relation classification on EDeR in both fine-tuned (using standard pretrained LMs) and few-shot settings (using ChatGPT), with experiments showing performance variation across different input representations. Results from additional experiments argue that incorporating event dependency information into training for related tasks (semantic role labeling and coreference resolution) may boost performance on those tasks.","- The dataset seems to be fairly carefully constructed and likely to be of interest to a significant subset of the NLP community, including folks working on event semantics and on information extraction. The fact that it builds on an existing widely used benchmark (OntoNotes) is also a plus.
- The experiments in general seem to accomplish most of what one expects of experiments in a resource paper — namely, establishing an accessible set of baselines and demonstrating the utility of the resource for existing tasks.","I do not in general have significant methodological concerns with the paper; most of my concerns relate to presentation and certain points of clarity (see **Questions for The Authors** and **Typos, Grammar, Style, and Presentation Improvements**), though I'll raise one point about annotator agreement here. While the authors' description of the data collection procedure inspires confidence with respect to the dataset's quality (notably, the fact that the 5% sampled annotations had 95% accuracy with respect to the authors' judgments), no standard agreement metrics are presented on the full subset of the data that is redundantly annotated. This does make it somewhat difficult to get a full sense for its quality.","A. How was the total number of documents for annotation (275) determined? Was this just based on resource constraints or was there some other principle behind it?

B. It is counterintuitive to me that the modified event structures would boost performance on entity coreference resolution. If I understand correctly, by pruning dependency parses based on the event relations, isn't it possible that you are dropping spans that participate in a gold coreference chain? Or does the model still have access to these spans, albeit not as part of the dependency parse? I feel like I must be misunderstanding something here.

C. The authors note that ""For the analysis of the dataset and experiments in the remainder of this paper, we treat the two non-argument labels as one"" (L253-5). Why was this done? Among non-arguments, the distinction between a condition and a wholly independent event is semantically very salient.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"I'm aware of several datasets that are distinct in their aims from the present work but that are worth citing as relevant general background:
- The Basic dataset from the IARPA BETTER program (see [here](https://ir.nist.gov/better/)) contain event structures with eventive arguments (so-called ""referred events""). NB: To my knowledge, there is no publication associated with the Basic datasets specifically, but see [this paper](https://aclanthology.org/2022.lrec-1.384/) for a description of the program and the related Abstract datasets.
- ""Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation"" (O'Gorman et al., 2016) has received a good amount of attention and contains several different types of event-event relations.
- ""Decomposing and Recomposing Event Structure"" (Gantt et al., 2022) annotates the English Web Treebank for event parthood relations as part of its event structure dataset.","Typos / Grammar:

In general, the paper would benefit from a careful proofread. Below are a sampling of suggested edits:
- 142-3: ""Several event-event relations have been proposed for decades"" -> (e.g.) ""Various event relation ontologies have been proposed over the years.""
- 179: ""an optional argument of, or a condition of"" -> ""an optional argument of, a condition of""
- 332: hence -> and so
- 394-6: this description of the rule-based classification method is rather awkward. Please rephrase.
- 529: feature -> features
- figure 6 caption: help -> helps

Presentation:
- Majority class baseline accuracy really should be explicitly presented (even if only in text) for comparison with the results in Table 3 (though I believe it's inferrable from Table 2).
- Table 3 should also include F1 scores
- For clarity, it would be good to have explicit examples of the ""Event-Event-SRL"" and ""Event-Event-SRL-DEP"" input variations presented in 4.2. (This is already done for the other two variants.)
- An analysis of model performance broken down by label, possibly including some qualitative evaluations, would (for my money) be more interesting and informative than the present analysis based on predicate distance and sentence length (though this analysis is still worthwhile).",1690306573707,,,EMNLP/2023/Conference,3LdaPmAnji,"['EMNLP/2023/Conference/Submission1716/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461119546,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1716/Reviewer_R3vw']",3LdaPmAnji,['EMNLP/2023/Conference/Submission1716/Reviewer_R3vw'],1690306573707,1701461119546,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1716/Reviewer_R3vw']","Yes

The suggested papers or references not cited in the manuscript are:
1. The Basic dataset from the IARPA BETTER program
2. ""Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation"" (O'Gorman et al., 2016)
3. ""Decomposing and Recomposing Event Structure"" (Gantt et al., 2022)
4. A paper describing the Abstract datasets from the IARPA BETTER program, referenced as [this paper](https://aclanthology.org/2022.lrec-1.384/)

These citations might be necessary because they provide relevant background information on event structures and event-event relations, which are central to the paper's topic. The reviewer notes that while these datasets and papers have distinct aims from the present work, they are worth citing as relevant general background, suggesting that they could enhance the manuscript's context and credibility.",1,"2016, 2022, 2022",Decomposing and Recomposing Event Structure
2acABjlOJD,This paper focus on the continually learning a mixed sequence. They propose two subnetwork discovery with soft-masking to allevaite catastrophic forgetting and importance computation to learn a mixed sequence. They also conduct extensive experiments on different NLP tasks and compare with many methods.,1. The extensive experiments show the effectiveness of the proposed method under the setting of learning mixed sequence continually.,"1. The significance of studying continual learning of a mixed sequence of tasks is unclear. This paper does not provide a formal definition of  ""continual learning of a mixed sequence"" and clearify what is the exact meaning of ""similar tasks"" and ""disimilar tasks"". In a practical scenario of task-incremental learning, the model should not have assumptions about the tasks to be learned. Therefore, I think the setting is strange and unrealistic. 
2. The core idea of the proposed method (e.g., subnetwork discovery and importance score computation) is not innovative. Furthermore, there are no strong connections between the proposed method and the setting of continual learning of a mixed sequence of tasks. It seems that the proposed method can be apply to conventional task-incremental learning settings.
3. The method part of the paper is hard to follow (e.g., Figure 1 is difficult to understand). Besides, the rationale of some model design is quite confusing. 
4. The paper claim that the proposed method encourage knowlege transfer, but the results of some key metrics (e.g., forward/backward transfer) are not shown in the experiments.","Question A: The paper states that ""To our knowledge, only one method has been proposed to learn a sequence of mixed tasks."". Why other task-incremental learning methods are not suitable for learning a sequence of mixed tasks? What are the special designs in your method enable learning a sequence of mixed tasks?
Question B: Why the proposed method is built upon adapter? Is it an empirical choice?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] Ermis B, Zappella G, Wistuba M, et al. Memory efficient continual learning with transformers[J]. Advances in Neural Information Processing Systems, 2022, 35: 10629-10642.

[2] Zheng J, Liang Z, Chen H, et al. Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3602-3615.

[3] Xia Y, Wang Q, Lyu Y, et al. Learn and review: Enhancing continual named entity recognition via reviewing synthetic samples[C]//Findings of the Association for Computational Linguistics: ACL 2022. 2022: 2291-2300.",,1689840686442,,,EMNLP/2023/Conference,3JBKnkUACW,"['EMNLP/2023/Conference/Submission4836/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461317737,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4836/Reviewer_jmje']",3JBKnkUACW,['EMNLP/2023/Conference/Submission4836/Reviewer_jmje'],1689840686442,1701461317737,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4836/Reviewer_jmje']","Yes

The suggested papers or references not cited in the manuscript are:
1. Ermis B, Zappella G, Wistuba M, et al. Memory efficient continual learning with transformers[J]. Advances in Neural Information Processing Systems, 2022, 35: 10629-10642.
2. Zheng J, Liang Z, Chen H, et al. Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3602-3615.
3. Xia Y, Wang Q, Lyu Y, et al. Learn and review: Enhancing continual named entity recognition via reviewing synthetic samples[C]//Findings of the Association for Computational Linguistics: ACL 2022. 2022: 2291-2300.

These citations might be necessary because the reviewer mentions ""Missing_References"" and lists these papers, implying that they are relevant to the topic of continual learning and task-incremental learning, which is the focus of the submitted manuscript. The reviewer likely suggests that the authors should be aware of and possibly cite these works to provide a more comprehensive overview of the current state of research in the field, and to strengthen their own contributions by comparing and contrasting their approach with existing methods.",1,"2022, 2022, 2022, 2022, 2022",Memory efficient continual learning with transformers
2acABjlOJD,This paper focus on the continually learning a mixed sequence. They propose two subnetwork discovery with soft-masking to allevaite catastrophic forgetting and importance computation to learn a mixed sequence. They also conduct extensive experiments on different NLP tasks and compare with many methods.,1. The extensive experiments show the effectiveness of the proposed method under the setting of learning mixed sequence continually.,"1. The significance of studying continual learning of a mixed sequence of tasks is unclear. This paper does not provide a formal definition of  ""continual learning of a mixed sequence"" and clearify what is the exact meaning of ""similar tasks"" and ""disimilar tasks"". In a practical scenario of task-incremental learning, the model should not have assumptions about the tasks to be learned. Therefore, I think the setting is strange and unrealistic. 
2. The core idea of the proposed method (e.g., subnetwork discovery and importance score computation) is not innovative. Furthermore, there are no strong connections between the proposed method and the setting of continual learning of a mixed sequence of tasks. It seems that the proposed method can be apply to conventional task-incremental learning settings.
3. The method part of the paper is hard to follow (e.g., Figure 1 is difficult to understand). Besides, the rationale of some model design is quite confusing. 
4. The paper claim that the proposed method encourage knowlege transfer, but the results of some key metrics (e.g., forward/backward transfer) are not shown in the experiments.","Question A: The paper states that ""To our knowledge, only one method has been proposed to learn a sequence of mixed tasks."". Why other task-incremental learning methods are not suitable for learning a sequence of mixed tasks? What are the special designs in your method enable learning a sequence of mixed tasks?
Question B: Why the proposed method is built upon adapter? Is it an empirical choice?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] Ermis B, Zappella G, Wistuba M, et al. Memory efficient continual learning with transformers[J]. Advances in Neural Information Processing Systems, 2022, 35: 10629-10642.

[2] Zheng J, Liang Z, Chen H, et al. Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3602-3615.

[3] Xia Y, Wang Q, Lyu Y, et al. Learn and review: Enhancing continual named entity recognition via reviewing synthetic samples[C]//Findings of the Association for Computational Linguistics: ACL 2022. 2022: 2291-2300.",,1689840686442,,,EMNLP/2023/Conference,3JBKnkUACW,"['EMNLP/2023/Conference/Submission4836/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461317737,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4836/Reviewer_jmje']",3JBKnkUACW,['EMNLP/2023/Conference/Submission4836/Reviewer_jmje'],1689840686442,1701461317737,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4836/Reviewer_jmje']","Yes

The suggested papers or references not cited in the manuscript are:
1. Ermis B, Zappella G, Wistuba M, et al. Memory efficient continual learning with transformers[J]. Advances in Neural Information Processing Systems, 2022, 35: 10629-10642.
2. Zheng J, Liang Z, Chen H, et al. Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3602-3615.
3. Xia Y, Wang Q, Lyu Y, et al. Learn and review: Enhancing continual named entity recognition via reviewing synthetic samples[C]//Findings of the Association for Computational Linguistics: ACL 2022. 2022: 2291-2300.

These citations might be necessary because the reviewer mentions ""Missing_References"" and lists these papers, implying that they are relevant to the topic of continual learning and task-incremental learning, which is the focus of the submitted manuscript. The reviewer likely suggests that the authors should be aware of and possibly cite these works to provide a more comprehensive overview of the current state of research in the field, and to strengthen their own contributions by comparing and contrasting their approach with existing methods.",1,"2022, 2022, 2022, 2022, 2022",Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition
2acABjlOJD,This paper focus on the continually learning a mixed sequence. They propose two subnetwork discovery with soft-masking to allevaite catastrophic forgetting and importance computation to learn a mixed sequence. They also conduct extensive experiments on different NLP tasks and compare with many methods.,1. The extensive experiments show the effectiveness of the proposed method under the setting of learning mixed sequence continually.,"1. The significance of studying continual learning of a mixed sequence of tasks is unclear. This paper does not provide a formal definition of  ""continual learning of a mixed sequence"" and clearify what is the exact meaning of ""similar tasks"" and ""disimilar tasks"". In a practical scenario of task-incremental learning, the model should not have assumptions about the tasks to be learned. Therefore, I think the setting is strange and unrealistic. 
2. The core idea of the proposed method (e.g., subnetwork discovery and importance score computation) is not innovative. Furthermore, there are no strong connections between the proposed method and the setting of continual learning of a mixed sequence of tasks. It seems that the proposed method can be apply to conventional task-incremental learning settings.
3. The method part of the paper is hard to follow (e.g., Figure 1 is difficult to understand). Besides, the rationale of some model design is quite confusing. 
4. The paper claim that the proposed method encourage knowlege transfer, but the results of some key metrics (e.g., forward/backward transfer) are not shown in the experiments.","Question A: The paper states that ""To our knowledge, only one method has been proposed to learn a sequence of mixed tasks."". Why other task-incremental learning methods are not suitable for learning a sequence of mixed tasks? What are the special designs in your method enable learning a sequence of mixed tasks?
Question B: Why the proposed method is built upon adapter? Is it an empirical choice?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] Ermis B, Zappella G, Wistuba M, et al. Memory efficient continual learning with transformers[J]. Advances in Neural Information Processing Systems, 2022, 35: 10629-10642.

[2] Zheng J, Liang Z, Chen H, et al. Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3602-3615.

[3] Xia Y, Wang Q, Lyu Y, et al. Learn and review: Enhancing continual named entity recognition via reviewing synthetic samples[C]//Findings of the Association for Computational Linguistics: ACL 2022. 2022: 2291-2300.",,1689840686442,,,EMNLP/2023/Conference,3JBKnkUACW,"['EMNLP/2023/Conference/Submission4836/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461317737,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4836/Reviewer_jmje']",3JBKnkUACW,['EMNLP/2023/Conference/Submission4836/Reviewer_jmje'],1689840686442,1701461317737,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4836/Reviewer_jmje']","Yes

The suggested papers or references not cited in the manuscript are:
1. Ermis B, Zappella G, Wistuba M, et al. Memory efficient continual learning with transformers[J]. Advances in Neural Information Processing Systems, 2022, 35: 10629-10642.
2. Zheng J, Liang Z, Chen H, et al. Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3602-3615.
3. Xia Y, Wang Q, Lyu Y, et al. Learn and review: Enhancing continual named entity recognition via reviewing synthetic samples[C]//Findings of the Association for Computational Linguistics: ACL 2022. 2022: 2291-2300.

These citations might be necessary because the reviewer mentions ""Missing_References"" and lists these papers, implying that they are relevant to the topic of continual learning and task-incremental learning, which is the focus of the submitted manuscript. The reviewer likely suggests that the authors should be aware of and possibly cite these works to provide a more comprehensive overview of the current state of research in the field, and to strengthen their own contributions by comparing and contrasting their approach with existing methods.",1,"2022, 2022, 2022, 2022, 2022",Learn and review Enhancing continual named entity recognition via reviewing synthetic samples
g7F0PuXVrL,"This paper proposed a task incremental learning method (TSS) to achieve both forgetting prevention and knowledge transfer on a range of NLP problems. Specifically, TSS finds a sub-network for each task to avoid catastrophic forgetting and soft-mask the gradient to encourage knowledge transfer.","- The paper is well-structured and easy to understand.
- Introducing a parameter importance-based gradient soft-mask on top of the SupSup method has enhanced the method's knowledge transfer capability.
- Empirical study shows that the proposed method is effective and outperforms CL baselines.","The experiment setting raises concerns regarding using small training set sizes for the 40 task datasets involved in the experiments. This approach gives rise to several issues that should be addressed:

1) **Questionable Applicability**: Continual learning methods are commonly applied in scenarios with large training datasets, where the storage of data and training of new models become crucial considerations. However, the experimental setup in the paper employs less than 30k training data, significantly deviating from real-world application scenarios. This poses doubts about the applicability of the findings to practical situations.
2) **Impact on CL Baselines**: Many CL baselines rely on using the training set of the current task to train adapters or perform fine-tuning on pre-trained models. With the training sets being drastically reduced in size, these methods may suffer from insufficient data, resulting in weakened performance.
3) **Missing Replay-based Baseline**: The paper fails to include a replay-based method as a baseline, which is a challenging approach to beat and should be considered for a comprehensive evaluation. 
4) **Ignoring Resource Disparities**: Real-world applications often involve tasks with varying resource availability, directly impacting their relative difficulty. By limiting the training data size uniformly across all tasks, the paper fails to account for the resource differences among them, potentially obscuring important insights.

In conclusion, the experiment setting requires revision to address the aforementioned issues and enhance the validity and applicability of the study's conclusions. Consideration should be given to utilizing more substantial training datasets that align better with real-world scenarios, ensuring the evaluation of CL baselines is based on sufficient data, including the replay-based method as part of the baseline comparisons, and acknowledging and accounting for the resource disparities among different tasks.","1) Why were small training samples (less than 30,000) used from 40 different tasks in the experiments? Did the authors attempt to use all the training samples for tasks in a smaller homogeneous sequence for a fair comparison of TSS against other continual learning methods? Was the relationship between the proposed method's performance and the number of training samples analyzed?

2) Was the method compared with a replay-based baseline given the small size of the training dataset (less than 30,000 samples)?

3) Did the authors analyze the relationship between popup scores of different tasks? Are popup scores closer for similar tasks? How does the proposed initialization and soft gradient masking technique affect the learned popup scores?

4) In Section 3.1.1, why was there no specific explanation for the chosen value of the threshold parameter (ε)? Is this parameter set uniformly for all tasks, or do different tasks have different settings?

5) In Section 3.1.2, what is the calculation method for normalizing and rounding the gradient-based importance using Tanh()? Why do the importance scores after Tanh() activation fall within the [0, 1] range, despite the original range being [-1, 1]? What impact does normalizing the importance values to -μ for parameters that initially had an importance of 0 have on the subsequent importance calculations?

6) In Section 4.2, the authors reported average performance on all datasets. However, since different tasks have different evaluation metrics, why did they choose to average F1, BLEU, and ROUGE-1 scores? Why not normalize task performances before averaging?

7) In Section 4.2, why did the authors use ONE as a control for analyzing Knowledge Transfer (KT) when ONE and TSS have different model architectures? Wouldn't it be more reasonable to compare each task's performance independently with its performance in the task sequence?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1) Konishi, T., Kurokawa, M., Ono, C., Ke, Z., Kim, G., & Liu, B. (2023). Parameter-Level Soft-Masking for Continual Learning. arXiv preprint arXiv:2306.14775.","1) In Equation (3), the subscripts on the left and right sides are inconsistent, and the variable definition for w_{i,j} has not been declared.",1690594443208,,,EMNLP/2023/Conference,3JBKnkUACW,"['EMNLP/2023/Conference/Submission4836/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461317614,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission4836/Reviewer_Naqz']",3JBKnkUACW,['EMNLP/2023/Conference/Submission4836/Reviewer_Naqz'],1690594443208,1701461317614,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4836/Reviewer_Naqz']","Yes

The suggested paper is:
1) Konishi, T., Kurokawa, M., Ono, C., Ke, Z., Kim, G., & Liu, B. (2023). Parameter-Level Soft-Masking for Continual Learning. arXiv preprint arXiv:2306.14775.

This citation might be necessary because the reviewer mentions it under ""Missing_References"", implying that the paper's topic, ""Parameter-Level Soft-Masking for Continual Learning"", is relevant to the submitted manuscript, which proposes a task incremental learning method (TSS) that utilizes a parameter importance-based gradient soft-mask. The reviewer suggests that this reference is missing, indicating that the authors should consider citing it to provide a more comprehensive overview of related work in the field of continual learning.",1,"2023, 2023",Parameter-Level Soft-Masking for Continual Learning
P1tzzK2YUy,"The paper addresses the challenge of Open World Information Extraction (IE) through instruction tuning, presenting a new concept of generating comprehensive entity profiles. Unlike previous work that focuses on specific tasks (e.g., NER, entity linking) within IE, this paper integrates six relevant tasks to create entity profiles. The primary contributions of the paper include the development of the INSTRUCTOPENWIKI, a large-scale instruction-following open-world IE dataset, and the creation of the PIVOINE model, an instruction tuning model based on BLOOM. ","This is the first work in Open-world IE that demonstrates the ability of large language models to be customized to generate complete entity profiles, as opposed to focusing on single tasks. This approach will facilitate future research in the direction of customized and complete OpenIE.

The authors introduce the INSTRUCTOPENWIKI dataset and the PIVOINE model, providing valuable resources for future research in this area. They conduct extensive experiments that offer a comprehensive evaluation of the model's performance, demonstrating its effectiveness in following instructions and extracting entity profiles.

The authors propose a delicate method that uses the time difference between Wiki dumps to construct a strictly open-world test set. This method minimizes potential data leakage and maximizes the completeness of the training set’s annotations.","While the authors claim that the model can follow unseen instructions, the definition of unseen instructions seems to primarily focus on unseen entities, and the model fails to follow cross-instructions, which are also unseen. The extent of what is considered unseen is debatable. For example, the ""2023 ATP Tour"" example in Figure 1, although it doesn’t occur in the training data, “2022 ATP Tour” is included in the training data. This raises concerns that the model may simply mimic what’s encoded in the training data, making the comparison against models that are untrained (e.g., ChatGPT) unfair.","A. How is the evaluation of cross-instructions conducted? The paper shows that the Number + Base Type and Number + Abstract Type instructions have high error rates (Fig 4). Could you clarify what is considered wrong in these cases? Is it the extracted numbers or the types?

B. Given the relation extraction data are created by aligning Wikipedia with Wikidata with distant supervision, some concerns about the dataset were raised. In prior work, Gao et al., (2021) found that distant supervision often brings a lot of noise and causes wrong relations labels. In addition, a pair of head and tail entities might also be related with multiple relations. Can you elaborate on how the relation is being selected and how the dataset's quality is being ensured?

Gao et al., (2021) : Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction 

C: The concept of generating entity profiles is interesting,  however, the evaluations are still conducted using single evaluation metrics, with no comprehensive evaluation of each entity. Is it possible to provide overall score for on entity level?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. In line 182, there appears to be a missing comma at the end.
2. The readability of the paper can be improved by clearly indicating the location of figures and tables, especially when they are in the appendix. Many of the tables and figures are located in the appendix.
3. It would be helpful to elaborate on why focusing on ""desired targets"" is important and clarify the meaning of ""desired targets"" (mentioned in lines 058-063) in the introduction.",1691148957004,,,EMNLP/2023/Conference,3F1qEXWKFE,"['EMNLP/2023/Conference/Submission2251/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461153601,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2251/Reviewer_VgsG']",3F1qEXWKFE,['EMNLP/2023/Conference/Submission2251/Reviewer_VgsG'],1691148957004,1701461153601,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2251/Reviewer_VgsG']","Yes

List of suggested papers or references not cited in the manuscript:
1. Gao et al., (2021) - Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction (already cited in the review, but potentially requiring further discussion)

 Briefly explain why those citations might be necessary:
The reviewer mentions Gao et al., (2021) to highlight concerns about the quality of the relation extraction data created using distant supervision. The reviewer suggests that distant supervision can bring noise and incorrect relation labels, which might impact the dataset's quality. The authors may need to address these concerns and potentially cite additional papers to discuss how they mitigated these issues or to provide further justification for their methodology.",1,"2021, 2021",Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction
M2QTMKFltC,"This paper proposes a new task ""open-world information extraction"" and constructs a dataset InstructOpenWiki for instruction-tuning models to solve this task. Although the task name seemingly includes IE as a whole, it is essentially extracting entity profiles.
They fine-tune an open-source LLM BLOOM on InstructOpenWiki and show that it outperforms non-LLM baselines and vanilla ChatGPT on the task.","- Instruction-tuning for IE is relatively understudied compared to other NLP tasks such as QA or summarization. 
- The weakly-supervised dataset InstructOpenWiki could be a useful resource to help train models for entity-related tasks.
- Error analysis of different failure cases is interesting and helps break down the challenges in this task.","- The new task name is misleading and it is questionable to what degree is the task ""open-domain"". 

The paper reads ""we introduce Open-world Information Extraction (Open-world IE) to accommodate broad and diverse requests related to entity profiles surpassing predefined ontologies’ limits.""  In Table 1, the example shows that the task is essentially entity linking, typing and relation extraction for the target entity. The task could simply be introduced as ""entity profiling"". Using the umbrella term ""Open-world IE"" is misleading since the task does not involve events or coreference, which are also important tasks under IE. 

One of my main concerns is whether the task is truly ""open-domain"".  The definition of entity linking (to link a mention to an entry in a knowledge base) means that it is closed domain in nature. The only difference between GENRE and PIVOINE is that they link to different snapshots of Wikipedia. Both are closed-domain.  For entity typing and relation extraction, note that Wikidata is closed-domain and the dataset is built through linking to Wikidata. While Wikidata has a lot of entity types  (20K as reported for InstructOpenWiki), this is on the same scale as UFET [1]. Moreover, if the entity types were truly open-domain, then F1 would not be an appropriate evaluation metric as it fails to capture similarities between types such as ""singer"", ""musical artist"", and ""musician"". 
For relations, there are only several hundred relations in the dataset. The only open-domain component seems to be the entity description. 

[1] Ultra-Fine Entity Typing (Choi et al., ACL 2018) 

-  The findings of the paper are not insightful. 
Experiments show that fine-tuning an LLM for this instruction-guided IE task works better than non-LLM which has no instruction-following ability and ChatGPT out of the box. This conclusion seems quite obvious. (Before the LLM era, if someone said that their supervised system worked better than their zero-shot / one-shot system it would not be a meaningful finding.)  
I would encourage the authors to go one step further and investigate aspects like ""how much data is needed"" and ""why this data works"". The analysis in Section 5.3  already provides some leads for investigation, for example, if failing to produce a valid JSON output is a problem, then can we solve this part directly?","A. How are the eight popular categories of instructions selected?  

B. Can you clarify what is ""entity priorities in Wikipedia""? Page views?

C. Is unseen ontology equivalent to an unseen entity? For example, 2023 ATP Tour was referred to as an unseen entity, but related entities such as the 2022 APT Tour could be in the training set. 2022 APT Tour would be an entity of the same type in the ontology.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Citation for GENRE paper is wrong. Should be De Cao, Nicola, Gautier Izacard, Sebastian Riedel and Fabio Petroni. “Autoregressive Entity Retrieval.” (ICLR 2021)",,1691226246143,,,EMNLP/2023/Conference,3F1qEXWKFE,"['EMNLP/2023/Conference/Submission2251/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461153488,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2251/Reviewer_YbHk']",3F1qEXWKFE,['EMNLP/2023/Conference/Submission2251/Reviewer_YbHk'],1691226246143,1701461153488,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2251/Reviewer_YbHk']","Yes

The suggested papers or references not cited in the manuscript are:
1. Ultra-Fine Entity Typing (Choi et al., ACL 2018) 
2. The correct citation for the GENRE paper: De Cao, Nicola, Gautier Izacard, Sebastian Riedel and Fabio Petroni. “Autoregressive Entity Retrieval.” (ICLR 2021)

These citations might be necessary because the reviewer mentions them as relevant references to support their arguments. Specifically, the Ultra-Fine Entity Typing paper is mentioned as a point of comparison for the scale of entity types in Wikidata, and the correct GENRE paper citation is necessary to accurately credit the original authors. The reviewer's comments imply that these references could provide additional context or background information to strengthen the manuscript.",1,"2018, 2021",Ultra-Fine Entity Typing 
M2QTMKFltC,"This paper proposes a new task ""open-world information extraction"" and constructs a dataset InstructOpenWiki for instruction-tuning models to solve this task. Although the task name seemingly includes IE as a whole, it is essentially extracting entity profiles.
They fine-tune an open-source LLM BLOOM on InstructOpenWiki and show that it outperforms non-LLM baselines and vanilla ChatGPT on the task.","- Instruction-tuning for IE is relatively understudied compared to other NLP tasks such as QA or summarization. 
- The weakly-supervised dataset InstructOpenWiki could be a useful resource to help train models for entity-related tasks.
- Error analysis of different failure cases is interesting and helps break down the challenges in this task.","- The new task name is misleading and it is questionable to what degree is the task ""open-domain"". 

The paper reads ""we introduce Open-world Information Extraction (Open-world IE) to accommodate broad and diverse requests related to entity profiles surpassing predefined ontologies’ limits.""  In Table 1, the example shows that the task is essentially entity linking, typing and relation extraction for the target entity. The task could simply be introduced as ""entity profiling"". Using the umbrella term ""Open-world IE"" is misleading since the task does not involve events or coreference, which are also important tasks under IE. 

One of my main concerns is whether the task is truly ""open-domain"".  The definition of entity linking (to link a mention to an entry in a knowledge base) means that it is closed domain in nature. The only difference between GENRE and PIVOINE is that they link to different snapshots of Wikipedia. Both are closed-domain.  For entity typing and relation extraction, note that Wikidata is closed-domain and the dataset is built through linking to Wikidata. While Wikidata has a lot of entity types  (20K as reported for InstructOpenWiki), this is on the same scale as UFET [1]. Moreover, if the entity types were truly open-domain, then F1 would not be an appropriate evaluation metric as it fails to capture similarities between types such as ""singer"", ""musical artist"", and ""musician"". 
For relations, there are only several hundred relations in the dataset. The only open-domain component seems to be the entity description. 

[1] Ultra-Fine Entity Typing (Choi et al., ACL 2018) 

-  The findings of the paper are not insightful. 
Experiments show that fine-tuning an LLM for this instruction-guided IE task works better than non-LLM which has no instruction-following ability and ChatGPT out of the box. This conclusion seems quite obvious. (Before the LLM era, if someone said that their supervised system worked better than their zero-shot / one-shot system it would not be a meaningful finding.)  
I would encourage the authors to go one step further and investigate aspects like ""how much data is needed"" and ""why this data works"". The analysis in Section 5.3  already provides some leads for investigation, for example, if failing to produce a valid JSON output is a problem, then can we solve this part directly?","A. How are the eight popular categories of instructions selected?  

B. Can you clarify what is ""entity priorities in Wikipedia""? Page views?

C. Is unseen ontology equivalent to an unseen entity? For example, 2023 ATP Tour was referred to as an unseen entity, but related entities such as the 2022 APT Tour could be in the training set. 2022 APT Tour would be an entity of the same type in the ontology.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Citation for GENRE paper is wrong. Should be De Cao, Nicola, Gautier Izacard, Sebastian Riedel and Fabio Petroni. “Autoregressive Entity Retrieval.” (ICLR 2021)",,1691226246143,,,EMNLP/2023/Conference,3F1qEXWKFE,"['EMNLP/2023/Conference/Submission2251/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461153488,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2251/Reviewer_YbHk']",3F1qEXWKFE,['EMNLP/2023/Conference/Submission2251/Reviewer_YbHk'],1691226246143,1701461153488,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2251/Reviewer_YbHk']","Yes

The suggested papers or references not cited in the manuscript are:
1. Ultra-Fine Entity Typing (Choi et al., ACL 2018) 
2. The correct citation for the GENRE paper: De Cao, Nicola, Gautier Izacard, Sebastian Riedel and Fabio Petroni. “Autoregressive Entity Retrieval.” (ICLR 2021)

These citations might be necessary because the reviewer mentions them as relevant references to support their arguments. Specifically, the Ultra-Fine Entity Typing paper is mentioned as a point of comparison for the scale of entity types in Wikidata, and the correct GENRE paper citation is necessary to accurately credit the original authors. The reviewer's comments imply that these references could provide additional context or background information to strengthen the manuscript.",1,"2018, 2021",Autoregressive Entity Retrieval
0jPql233SV,"This paper provides a unified mathematical view of sparse mixture-of-experts and sparse neural memory. Based on this view and empirical observations, the author proposes a new gating method Avg-k to select memory blocks based on the averaged hidden states of each block. Avg-k achieves substantially better perplexity compared with a strong baseline (Switch Transformer).","1. Strong perplexity improvement over Switch Transformer on out-of-domain evaluation of language modeling.
2. The paper is well-written and easy to follow. 
3. Comprehensive analyses for the hyper-parameters and the baselines. The analyses deepen the understanding of current MoE methods.","1. While the method provides better FLOPs-perplexity trade-off, it is unclear how Avg-K can provide better actual training speed perplexity trade-off due to increased communication cost.
2. It will be better to compare the performance of the proposed method with more recent MoE routing approach such as X-MoE.",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691237125800,,,EMNLP/2023/Conference,3EcjsgPq74,"['EMNLP/2023/Conference/Submission429/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461030440,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission429/Reviewer_mxPW']",3EcjsgPq74,['EMNLP/2023/Conference/Submission429/Reviewer_mxPW'],1691237125800,1701461030440,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission429/Reviewer_mxPW']","Yes

List of suggested papers or references not cited in the manuscript:
1. X-MoE (a recent MoE routing approach)

Those citations might be necessary because the reviewer mentions that ""It will be better to compare the performance of the proposed method with more recent MoE routing approach such as X-MoE."" This implies that the reviewer believes the authors should be aware of and compare their work to the most recent developments in the field, including X-MoE, to provide a more comprehensive evaluation of their proposed method.",1,,X-MoE
ugkjzfgiSw,"This paper provides a study of the performance of different PEFT methods across different scales of models. It describes interesting findings that model scaling can mitigate the effects of insertion positions of PEFT and the trainable parameter is a critical factor for achieving competitive performance for PEFT in comparison to FFT. It is delightful to read this paper, and the presentation, visuals, of this paper is great. However, the APET framework for studying the parameters is not new. Hence, the technical contribution of this paper is not the strength of this paper, but I appreciate the analysis of PEFT across scales for driving PEFT research further.  I will recommend the author to include the detailed performance of each experiment for each task in appendix for providing additional information rather than only providing an averaged number in the main section. The experiments are solid for the discussed 4 PEFT methods for deriving the corresponding findings and the results are as expected from PEFT researchers, but I expect for more PEFT modules to be studied. This work can give more technical contributions to the field if the author could also study the effectiveness of mixture of different insertion methods in a limited parameter budget, layer positions for inserting PEFTs, and the most recommended/universal PEFT solution for large language models. In conclusion, I am learning positive for this work given its experimental solidness, and will recommend for a findings-level of work given the limited technical contribution.","1. The experimental results are solid and as expected.
2. The writing and presentation of this work is great.
3. It gives useful empirical findings for PEFT researchers facing different scales of language models.","1. The technical contribution of this paper is limited.
2. The APET framework is related with other arbitrary PEFT frameworks: UniPELT [1], AutoPEFT [2], S3PET [3], PEFT Design Space [4], NASPEFT [5], but they are not cited nor discussed.
3. The technical part of this paper can be improved by studying the effectiveness of mixture of different insertion methods and the most recommended PEFT solution for large language models.
4. As an analysis paper, I will expect more types of PEFT modules to be studied in this paper, such as Compactor, IA3 [6], LayerNorm Tuning [7], Parallel Adapter. It will be interesting to see if the current conclusion applies to wider ranges of PEFT modules.","1. If the author could address the concerns in the weakness section, I will further improve my scores, especially for point 3, 4. 
2. Prefix-tuning (prompt tuning) can leverage reparameterization, and the parameter amount learned by additional MLP layers can give significant impacts to the performance. Have you considered studying this?

---------------------
### After Rebuttal

I appreciate the response from the author. I am convinced now that the current experiments are solid for delievering the findings. I have increased my score for soundness and keep the excitement score unchanged. I appeciate if the author could include more PEFT results in the camera-ready version.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.

2. Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.

3. Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning. 

4. Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES. 

5. Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. 

6. Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.

7. Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.",Line 123: 2 dots.,1691103002157,,,EMNLP/2023/Conference,3CIQIYNGlp,"['EMNLP/2023/Conference/Submission464/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461033280,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']",3CIQIYNGlp,['EMNLP/2023/Conference/Submission464/Reviewer_fvk4'],1691103002157,1701461033280,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']","Yes

The suggested papers or references not cited in the manuscript are:
1. UniPELT [1] - Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.
2. AutoPEFT [2] - Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.
3. S3PET [3] - Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning.
4. PEFT Design Space [4] - Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES.
5. NASPEFT [5] - Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models.
6. IA3 [6] - Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.
7. LayerNorm Tuning [7] - Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.

These citations might be necessary because the reviewer mentions that the APET framework is related to other arbitrary PEFT frameworks (UniPELT, AutoPEFT, S3PET, PEFT Design Space, NASPEFT), but they are not cited nor discussed in the paper. Additionally, the reviewer suggests studying more types of PEFT modules, such as Compactor, IA3, LayerNorm Tuning, and Parallel Adapter, implying that the authors should be aware of and engage with existing research in these areas. The reviewer believes that including these references and discussing their relationship to the authors' work could strengthen the paper's technical contribution and provide a more comprehensive analysis of PEFT methods.",1,,UniPELT A Unified Framework for Parameter-Efficient Language Model Tuning
ugkjzfgiSw,"This paper provides a study of the performance of different PEFT methods across different scales of models. It describes interesting findings that model scaling can mitigate the effects of insertion positions of PEFT and the trainable parameter is a critical factor for achieving competitive performance for PEFT in comparison to FFT. It is delightful to read this paper, and the presentation, visuals, of this paper is great. However, the APET framework for studying the parameters is not new. Hence, the technical contribution of this paper is not the strength of this paper, but I appreciate the analysis of PEFT across scales for driving PEFT research further.  I will recommend the author to include the detailed performance of each experiment for each task in appendix for providing additional information rather than only providing an averaged number in the main section. The experiments are solid for the discussed 4 PEFT methods for deriving the corresponding findings and the results are as expected from PEFT researchers, but I expect for more PEFT modules to be studied. This work can give more technical contributions to the field if the author could also study the effectiveness of mixture of different insertion methods in a limited parameter budget, layer positions for inserting PEFTs, and the most recommended/universal PEFT solution for large language models. In conclusion, I am learning positive for this work given its experimental solidness, and will recommend for a findings-level of work given the limited technical contribution.","1. The experimental results are solid and as expected.
2. The writing and presentation of this work is great.
3. It gives useful empirical findings for PEFT researchers facing different scales of language models.","1. The technical contribution of this paper is limited.
2. The APET framework is related with other arbitrary PEFT frameworks: UniPELT [1], AutoPEFT [2], S3PET [3], PEFT Design Space [4], NASPEFT [5], but they are not cited nor discussed.
3. The technical part of this paper can be improved by studying the effectiveness of mixture of different insertion methods and the most recommended PEFT solution for large language models.
4. As an analysis paper, I will expect more types of PEFT modules to be studied in this paper, such as Compactor, IA3 [6], LayerNorm Tuning [7], Parallel Adapter. It will be interesting to see if the current conclusion applies to wider ranges of PEFT modules.","1. If the author could address the concerns in the weakness section, I will further improve my scores, especially for point 3, 4. 
2. Prefix-tuning (prompt tuning) can leverage reparameterization, and the parameter amount learned by additional MLP layers can give significant impacts to the performance. Have you considered studying this?

---------------------
### After Rebuttal

I appreciate the response from the author. I am convinced now that the current experiments are solid for delievering the findings. I have increased my score for soundness and keep the excitement score unchanged. I appeciate if the author could include more PEFT results in the camera-ready version.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.

2. Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.

3. Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning. 

4. Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES. 

5. Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. 

6. Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.

7. Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.",Line 123: 2 dots.,1691103002157,,,EMNLP/2023/Conference,3CIQIYNGlp,"['EMNLP/2023/Conference/Submission464/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461033280,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']",3CIQIYNGlp,['EMNLP/2023/Conference/Submission464/Reviewer_fvk4'],1691103002157,1701461033280,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']","Yes

The suggested papers or references not cited in the manuscript are:
1. UniPELT [1] - Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.
2. AutoPEFT [2] - Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.
3. S3PET [3] - Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning.
4. PEFT Design Space [4] - Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES.
5. NASPEFT [5] - Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models.
6. IA3 [6] - Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.
7. LayerNorm Tuning [7] - Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.

These citations might be necessary because the reviewer mentions that the APET framework is related to other arbitrary PEFT frameworks (UniPELT, AutoPEFT, S3PET, PEFT Design Space, NASPEFT), but they are not cited nor discussed in the paper. Additionally, the reviewer suggests studying more types of PEFT modules, such as Compactor, IA3, LayerNorm Tuning, and Parallel Adapter, implying that the authors should be aware of and engage with existing research in these areas. The reviewer believes that including these references and discussing their relationship to the authors' work could strengthen the paper's technical contribution and provide a more comprehensive analysis of PEFT methods.",1,,AutoPEFT Automatic Configuration Search for Parameter-Efficient Fine-Tuning
ugkjzfgiSw,"This paper provides a study of the performance of different PEFT methods across different scales of models. It describes interesting findings that model scaling can mitigate the effects of insertion positions of PEFT and the trainable parameter is a critical factor for achieving competitive performance for PEFT in comparison to FFT. It is delightful to read this paper, and the presentation, visuals, of this paper is great. However, the APET framework for studying the parameters is not new. Hence, the technical contribution of this paper is not the strength of this paper, but I appreciate the analysis of PEFT across scales for driving PEFT research further.  I will recommend the author to include the detailed performance of each experiment for each task in appendix for providing additional information rather than only providing an averaged number in the main section. The experiments are solid for the discussed 4 PEFT methods for deriving the corresponding findings and the results are as expected from PEFT researchers, but I expect for more PEFT modules to be studied. This work can give more technical contributions to the field if the author could also study the effectiveness of mixture of different insertion methods in a limited parameter budget, layer positions for inserting PEFTs, and the most recommended/universal PEFT solution for large language models. In conclusion, I am learning positive for this work given its experimental solidness, and will recommend for a findings-level of work given the limited technical contribution.","1. The experimental results are solid and as expected.
2. The writing and presentation of this work is great.
3. It gives useful empirical findings for PEFT researchers facing different scales of language models.","1. The technical contribution of this paper is limited.
2. The APET framework is related with other arbitrary PEFT frameworks: UniPELT [1], AutoPEFT [2], S3PET [3], PEFT Design Space [4], NASPEFT [5], but they are not cited nor discussed.
3. The technical part of this paper can be improved by studying the effectiveness of mixture of different insertion methods and the most recommended PEFT solution for large language models.
4. As an analysis paper, I will expect more types of PEFT modules to be studied in this paper, such as Compactor, IA3 [6], LayerNorm Tuning [7], Parallel Adapter. It will be interesting to see if the current conclusion applies to wider ranges of PEFT modules.","1. If the author could address the concerns in the weakness section, I will further improve my scores, especially for point 3, 4. 
2. Prefix-tuning (prompt tuning) can leverage reparameterization, and the parameter amount learned by additional MLP layers can give significant impacts to the performance. Have you considered studying this?

---------------------
### After Rebuttal

I appreciate the response from the author. I am convinced now that the current experiments are solid for delievering the findings. I have increased my score for soundness and keep the excitement score unchanged. I appeciate if the author could include more PEFT results in the camera-ready version.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.

2. Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.

3. Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning. 

4. Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES. 

5. Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. 

6. Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.

7. Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.",Line 123: 2 dots.,1691103002157,,,EMNLP/2023/Conference,3CIQIYNGlp,"['EMNLP/2023/Conference/Submission464/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461033280,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']",3CIQIYNGlp,['EMNLP/2023/Conference/Submission464/Reviewer_fvk4'],1691103002157,1701461033280,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']","Yes

The suggested papers or references not cited in the manuscript are:
1. UniPELT [1] - Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.
2. AutoPEFT [2] - Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.
3. S3PET [3] - Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning.
4. PEFT Design Space [4] - Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES.
5. NASPEFT [5] - Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models.
6. IA3 [6] - Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.
7. LayerNorm Tuning [7] - Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.

These citations might be necessary because the reviewer mentions that the APET framework is related to other arbitrary PEFT frameworks (UniPELT, AutoPEFT, S3PET, PEFT Design Space, NASPEFT), but they are not cited nor discussed in the paper. Additionally, the reviewer suggests studying more types of PEFT modules, such as Compactor, IA3, LayerNorm Tuning, and Parallel Adapter, implying that the authors should be aware of and engage with existing research in these areas. The reviewer believes that including these references and discussing their relationship to the authors' work could strengthen the paper's technical contribution and provide a more comprehensive analysis of PEFT methods.",1,,S3PET Sparse Structure Search for Parameter-Efficient Tuning
ugkjzfgiSw,"This paper provides a study of the performance of different PEFT methods across different scales of models. It describes interesting findings that model scaling can mitigate the effects of insertion positions of PEFT and the trainable parameter is a critical factor for achieving competitive performance for PEFT in comparison to FFT. It is delightful to read this paper, and the presentation, visuals, of this paper is great. However, the APET framework for studying the parameters is not new. Hence, the technical contribution of this paper is not the strength of this paper, but I appreciate the analysis of PEFT across scales for driving PEFT research further.  I will recommend the author to include the detailed performance of each experiment for each task in appendix for providing additional information rather than only providing an averaged number in the main section. The experiments are solid for the discussed 4 PEFT methods for deriving the corresponding findings and the results are as expected from PEFT researchers, but I expect for more PEFT modules to be studied. This work can give more technical contributions to the field if the author could also study the effectiveness of mixture of different insertion methods in a limited parameter budget, layer positions for inserting PEFTs, and the most recommended/universal PEFT solution for large language models. In conclusion, I am learning positive for this work given its experimental solidness, and will recommend for a findings-level of work given the limited technical contribution.","1. The experimental results are solid and as expected.
2. The writing and presentation of this work is great.
3. It gives useful empirical findings for PEFT researchers facing different scales of language models.","1. The technical contribution of this paper is limited.
2. The APET framework is related with other arbitrary PEFT frameworks: UniPELT [1], AutoPEFT [2], S3PET [3], PEFT Design Space [4], NASPEFT [5], but they are not cited nor discussed.
3. The technical part of this paper can be improved by studying the effectiveness of mixture of different insertion methods and the most recommended PEFT solution for large language models.
4. As an analysis paper, I will expect more types of PEFT modules to be studied in this paper, such as Compactor, IA3 [6], LayerNorm Tuning [7], Parallel Adapter. It will be interesting to see if the current conclusion applies to wider ranges of PEFT modules.","1. If the author could address the concerns in the weakness section, I will further improve my scores, especially for point 3, 4. 
2. Prefix-tuning (prompt tuning) can leverage reparameterization, and the parameter amount learned by additional MLP layers can give significant impacts to the performance. Have you considered studying this?

---------------------
### After Rebuttal

I appreciate the response from the author. I am convinced now that the current experiments are solid for delievering the findings. I have increased my score for soundness and keep the excitement score unchanged. I appeciate if the author could include more PEFT results in the camera-ready version.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.

2. Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.

3. Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning. 

4. Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES. 

5. Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. 

6. Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.

7. Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.",Line 123: 2 dots.,1691103002157,,,EMNLP/2023/Conference,3CIQIYNGlp,"['EMNLP/2023/Conference/Submission464/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461033280,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']",3CIQIYNGlp,['EMNLP/2023/Conference/Submission464/Reviewer_fvk4'],1691103002157,1701461033280,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']","Yes

The suggested papers or references not cited in the manuscript are:
1. UniPELT [1] - Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.
2. AutoPEFT [2] - Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.
3. S3PET [3] - Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning.
4. PEFT Design Space [4] - Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES.
5. NASPEFT [5] - Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models.
6. IA3 [6] - Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.
7. LayerNorm Tuning [7] - Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.

These citations might be necessary because the reviewer mentions that the APET framework is related to other arbitrary PEFT frameworks (UniPELT, AutoPEFT, S3PET, PEFT Design Space, NASPEFT), but they are not cited nor discussed in the paper. Additionally, the reviewer suggests studying more types of PEFT modules, such as Compactor, IA3, LayerNorm Tuning, and Parallel Adapter, implying that the authors should be aware of and engage with existing research in these areas. The reviewer believes that including these references and discussing their relationship to the authors' work could strengthen the paper's technical contribution and provide a more comprehensive analysis of PEFT methods.",1,,PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES
ugkjzfgiSw,"This paper provides a study of the performance of different PEFT methods across different scales of models. It describes interesting findings that model scaling can mitigate the effects of insertion positions of PEFT and the trainable parameter is a critical factor for achieving competitive performance for PEFT in comparison to FFT. It is delightful to read this paper, and the presentation, visuals, of this paper is great. However, the APET framework for studying the parameters is not new. Hence, the technical contribution of this paper is not the strength of this paper, but I appreciate the analysis of PEFT across scales for driving PEFT research further.  I will recommend the author to include the detailed performance of each experiment for each task in appendix for providing additional information rather than only providing an averaged number in the main section. The experiments are solid for the discussed 4 PEFT methods for deriving the corresponding findings and the results are as expected from PEFT researchers, but I expect for more PEFT modules to be studied. This work can give more technical contributions to the field if the author could also study the effectiveness of mixture of different insertion methods in a limited parameter budget, layer positions for inserting PEFTs, and the most recommended/universal PEFT solution for large language models. In conclusion, I am learning positive for this work given its experimental solidness, and will recommend for a findings-level of work given the limited technical contribution.","1. The experimental results are solid and as expected.
2. The writing and presentation of this work is great.
3. It gives useful empirical findings for PEFT researchers facing different scales of language models.","1. The technical contribution of this paper is limited.
2. The APET framework is related with other arbitrary PEFT frameworks: UniPELT [1], AutoPEFT [2], S3PET [3], PEFT Design Space [4], NASPEFT [5], but they are not cited nor discussed.
3. The technical part of this paper can be improved by studying the effectiveness of mixture of different insertion methods and the most recommended PEFT solution for large language models.
4. As an analysis paper, I will expect more types of PEFT modules to be studied in this paper, such as Compactor, IA3 [6], LayerNorm Tuning [7], Parallel Adapter. It will be interesting to see if the current conclusion applies to wider ranges of PEFT modules.","1. If the author could address the concerns in the weakness section, I will further improve my scores, especially for point 3, 4. 
2. Prefix-tuning (prompt tuning) can leverage reparameterization, and the parameter amount learned by additional MLP layers can give significant impacts to the performance. Have you considered studying this?

---------------------
### After Rebuttal

I appreciate the response from the author. I am convinced now that the current experiments are solid for delievering the findings. I have increased my score for soundness and keep the excitement score unchanged. I appeciate if the author could include more PEFT results in the camera-ready version.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.

2. Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.

3. Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning. 

4. Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES. 

5. Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. 

6. Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.

7. Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.",Line 123: 2 dots.,1691103002157,,,EMNLP/2023/Conference,3CIQIYNGlp,"['EMNLP/2023/Conference/Submission464/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461033280,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']",3CIQIYNGlp,['EMNLP/2023/Conference/Submission464/Reviewer_fvk4'],1691103002157,1701461033280,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']","Yes

The suggested papers or references not cited in the manuscript are:
1. UniPELT [1] - Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.
2. AutoPEFT [2] - Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.
3. S3PET [3] - Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning.
4. PEFT Design Space [4] - Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES.
5. NASPEFT [5] - Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models.
6. IA3 [6] - Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.
7. LayerNorm Tuning [7] - Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.

These citations might be necessary because the reviewer mentions that the APET framework is related to other arbitrary PEFT frameworks (UniPELT, AutoPEFT, S3PET, PEFT Design Space, NASPEFT), but they are not cited nor discussed in the paper. Additionally, the reviewer suggests studying more types of PEFT modules, such as Compactor, IA3, LayerNorm Tuning, and Parallel Adapter, implying that the authors should be aware of and engage with existing research in these areas. The reviewer believes that including these references and discussing their relationship to the authors' work could strengthen the paper's technical contribution and provide a more comprehensive analysis of PEFT methods.",1,,Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models
ugkjzfgiSw,"This paper provides a study of the performance of different PEFT methods across different scales of models. It describes interesting findings that model scaling can mitigate the effects of insertion positions of PEFT and the trainable parameter is a critical factor for achieving competitive performance for PEFT in comparison to FFT. It is delightful to read this paper, and the presentation, visuals, of this paper is great. However, the APET framework for studying the parameters is not new. Hence, the technical contribution of this paper is not the strength of this paper, but I appreciate the analysis of PEFT across scales for driving PEFT research further.  I will recommend the author to include the detailed performance of each experiment for each task in appendix for providing additional information rather than only providing an averaged number in the main section. The experiments are solid for the discussed 4 PEFT methods for deriving the corresponding findings and the results are as expected from PEFT researchers, but I expect for more PEFT modules to be studied. This work can give more technical contributions to the field if the author could also study the effectiveness of mixture of different insertion methods in a limited parameter budget, layer positions for inserting PEFTs, and the most recommended/universal PEFT solution for large language models. In conclusion, I am learning positive for this work given its experimental solidness, and will recommend for a findings-level of work given the limited technical contribution.","1. The experimental results are solid and as expected.
2. The writing and presentation of this work is great.
3. It gives useful empirical findings for PEFT researchers facing different scales of language models.","1. The technical contribution of this paper is limited.
2. The APET framework is related with other arbitrary PEFT frameworks: UniPELT [1], AutoPEFT [2], S3PET [3], PEFT Design Space [4], NASPEFT [5], but they are not cited nor discussed.
3. The technical part of this paper can be improved by studying the effectiveness of mixture of different insertion methods and the most recommended PEFT solution for large language models.
4. As an analysis paper, I will expect more types of PEFT modules to be studied in this paper, such as Compactor, IA3 [6], LayerNorm Tuning [7], Parallel Adapter. It will be interesting to see if the current conclusion applies to wider ranges of PEFT modules.","1. If the author could address the concerns in the weakness section, I will further improve my scores, especially for point 3, 4. 
2. Prefix-tuning (prompt tuning) can leverage reparameterization, and the parameter amount learned by additional MLP layers can give significant impacts to the performance. Have you considered studying this?

---------------------
### After Rebuttal

I appreciate the response from the author. I am convinced now that the current experiments are solid for delievering the findings. I have increased my score for soundness and keep the excitement score unchanged. I appeciate if the author could include more PEFT results in the camera-ready version.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.

2. Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.

3. Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning. 

4. Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES. 

5. Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. 

6. Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.

7. Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.",Line 123: 2 dots.,1691103002157,,,EMNLP/2023/Conference,3CIQIYNGlp,"['EMNLP/2023/Conference/Submission464/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461033280,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']",3CIQIYNGlp,['EMNLP/2023/Conference/Submission464/Reviewer_fvk4'],1691103002157,1701461033280,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']","Yes

The suggested papers or references not cited in the manuscript are:
1. UniPELT [1] - Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.
2. AutoPEFT [2] - Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.
3. S3PET [3] - Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning.
4. PEFT Design Space [4] - Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES.
5. NASPEFT [5] - Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models.
6. IA3 [6] - Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.
7. LayerNorm Tuning [7] - Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.

These citations might be necessary because the reviewer mentions that the APET framework is related to other arbitrary PEFT frameworks (UniPELT, AutoPEFT, S3PET, PEFT Design Space, NASPEFT), but they are not cited nor discussed in the paper. Additionally, the reviewer suggests studying more types of PEFT modules, such as Compactor, IA3, LayerNorm Tuning, and Parallel Adapter, implying that the authors should be aware of and engage with existing research in these areas. The reviewer believes that including these references and discussing their relationship to the authors' work could strengthen the paper's technical contribution and provide a more comprehensive analysis of PEFT methods.",1,,Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning
ugkjzfgiSw,"This paper provides a study of the performance of different PEFT methods across different scales of models. It describes interesting findings that model scaling can mitigate the effects of insertion positions of PEFT and the trainable parameter is a critical factor for achieving competitive performance for PEFT in comparison to FFT. It is delightful to read this paper, and the presentation, visuals, of this paper is great. However, the APET framework for studying the parameters is not new. Hence, the technical contribution of this paper is not the strength of this paper, but I appreciate the analysis of PEFT across scales for driving PEFT research further.  I will recommend the author to include the detailed performance of each experiment for each task in appendix for providing additional information rather than only providing an averaged number in the main section. The experiments are solid for the discussed 4 PEFT methods for deriving the corresponding findings and the results are as expected from PEFT researchers, but I expect for more PEFT modules to be studied. This work can give more technical contributions to the field if the author could also study the effectiveness of mixture of different insertion methods in a limited parameter budget, layer positions for inserting PEFTs, and the most recommended/universal PEFT solution for large language models. In conclusion, I am learning positive for this work given its experimental solidness, and will recommend for a findings-level of work given the limited technical contribution.","1. The experimental results are solid and as expected.
2. The writing and presentation of this work is great.
3. It gives useful empirical findings for PEFT researchers facing different scales of language models.","1. The technical contribution of this paper is limited.
2. The APET framework is related with other arbitrary PEFT frameworks: UniPELT [1], AutoPEFT [2], S3PET [3], PEFT Design Space [4], NASPEFT [5], but they are not cited nor discussed.
3. The technical part of this paper can be improved by studying the effectiveness of mixture of different insertion methods and the most recommended PEFT solution for large language models.
4. As an analysis paper, I will expect more types of PEFT modules to be studied in this paper, such as Compactor, IA3 [6], LayerNorm Tuning [7], Parallel Adapter. It will be interesting to see if the current conclusion applies to wider ranges of PEFT modules.","1. If the author could address the concerns in the weakness section, I will further improve my scores, especially for point 3, 4. 
2. Prefix-tuning (prompt tuning) can leverage reparameterization, and the parameter amount learned by additional MLP layers can give significant impacts to the performance. Have you considered studying this?

---------------------
### After Rebuttal

I appreciate the response from the author. I am convinced now that the current experiments are solid for delievering the findings. I have increased my score for soundness and keep the excitement score unchanged. I appeciate if the author could include more PEFT results in the camera-ready version.",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.

2. Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.

3. Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning. 

4. Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES. 

5. Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. 

6. Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.

7. Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.",Line 123: 2 dots.,1691103002157,,,EMNLP/2023/Conference,3CIQIYNGlp,"['EMNLP/2023/Conference/Submission464/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461033280,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']",3CIQIYNGlp,['EMNLP/2023/Conference/Submission464/Reviewer_fvk4'],1691103002157,1701461033280,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission464/Reviewer_fvk4']","Yes

The suggested papers or references not cited in the manuscript are:
1. UniPELT [1] - Mao Y, Mathias L, Hou R, et al. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.
2. AutoPEFT [2] - Zhou H, Wan X, Vulić I, Korhonen A. AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.
3. S3PET [3] - Hu S, Zhang Z, Ding N, et al. Sparse Structure Search for Parameter-Efficient Tuning.
4. PEFT Design Space [4] - Chen J, Zhang A, Shi X, Li M, Smola A, Yang D. PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES.
5. NASPEFT [5] - Lawton N, Kumar A, Thattai G, Galstyan A, Steeg GV. Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models.
6. IA3 [6] - Liu H, Tam D, Muqeeth M, et al. Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.
7. LayerNorm Tuning [7] - Giannou A, Rajput S, Papailiopoulos D. The Expressive Power of Tuning Only the Norm Layers.

These citations might be necessary because the reviewer mentions that the APET framework is related to other arbitrary PEFT frameworks (UniPELT, AutoPEFT, S3PET, PEFT Design Space, NASPEFT), but they are not cited nor discussed in the paper. Additionally, the reviewer suggests studying more types of PEFT modules, such as Compactor, IA3, LayerNorm Tuning, and Parallel Adapter, implying that the authors should be aware of and engage with existing research in these areas. The reviewer believes that including these references and discussing their relationship to the authors' work could strengthen the paper's technical contribution and provide a more comprehensive analysis of PEFT methods.",1,,The Expressive Power of Tuning Only the Norm Layers
9xEdZdNOJL,"This paper addresses the problem of learning interpretable text and image representations. Unlike the prevailing vision-language models (VLMs) that rely on abstract dense embeddings, this work introduces an interpretable sparse embedding space, namely STAIR, where each dimension associates with a (sub-)word from the BERT vocabulary. This work also proposes a multi-stage training strategy to (1) align learned image and text representations effectively and (2) ground them in meaningful (sub-)words. The empirical results demonstrate that, compared with the CLIP model that utilizes dense embeddings, the embeddings of STAIR are not only more interpretable to humans but also achieve superior performance in image-text retrieval, zero-shot and linear probing visual classification tasks.","* The problem addressed by this paper, i.e., learning interpretable image and text representations, is important, which can facilitate the transparency of large VLMs.
* The proposed methods, i.e., the sparse embedding space and the multi-stage training strategy, are reasonable and effective.
* The experiments are well-designed and the results are convincing, which validate the advantage of STAIR in terms of both interpretability and effectiveness across various downstream vision-language tasks.
* The limitations of this work are properly recognized in the paper, which is accompanied by discussions on potential solutions and future directions.
* The paper is well-written and easy to follow.","* No quantitative evaluation supports the claim that STAIR is more efficient than CLIP in text-image retrieval.
* It is unclear whether STAIR can still enhance the interpretability and downstream performance, when combined with more advanced VLMs, e.g., BLIP[1]. 

[1] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation","* How does STAIR compare with CLIP in terms of actual retrieval speed?
* What is the impact of regularization weights $\lambda$ on interpretability? Is there a trade-off between interpretability and retrieval performance when increasing the regularization weights?
* Could you provide more information about the training cost, e.g., the total training time and the infrastructure being used?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1690271642662,,,EMNLP/2023/Conference,3AxESAk0Re,"['EMNLP/2023/Conference/Submission296/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461022127,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission296/Reviewer_9dcF']",3AxESAk0Re,['EMNLP/2023/Conference/Submission296/Reviewer_9dcF'],1690271642662,1701461022127,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission296/Reviewer_9dcF']","Yes

* BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation

The reviewer suggests citing the BLIP paper because they question whether STAIR can still enhance interpretability and downstream performance when combined with more advanced VLMs like BLIP. This implies that the reviewer thinks the authors should consider comparing their work to or building upon more recent and advanced models in the field, such as BLIP, to strengthen their claims and demonstrate the broader applicability of their approach.",1,,Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
YfiOUgtqg7,"This paper presents a novel method, called ""Exchange-of-Thought,"" to enhance the reasoning capabilities of LLMs (Language Language Models) in mathematical and common-sense reasoning tasks. It si based on the Chain-of-Thought concept, generates intermediate reasoning steps for addressing complex problems. By introducing four communication modes, the paper argues that it enhances the quality and reliability of the answers with respect to the state of the art. Experimental validation shows that this interactive communication approach outperforms other methods.","- The paper addresses a relevant issue: enhancing the reasoning capabilities of LLMs.
- The interactive and multi-agent approach is an interesting and novel proposal for tackling this challenge.
- The results of the experiments are significant and improve the state of the art.","- Lack of reference explaining communication in this context. 
- The paper introduces four communication modes (debate, report, relay, and memory) without sufficient support from literature, despite existing relevant work in argumentation theory. Section 4.2 provides inadequate details and lacks illustrative examples.
- Figure 3 is challenging to understand. The workflow and captions are unclear, and the representation of communication modes on the left side is confusing.
- Figure 4's tabular representation of node agent interactions is not intuitive.",- Is this paper considering communication using the concepts from multi-agent systems?,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"2: Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,"- For communication in multi-agent system and a further exploration of communciation modes: Parsons, S., McBurney, P. (2003). Argumentation-Based Communication between Agents. In: Huget, MP. (eds) Communication in Multiagent Systems. Lecture Notes in Computer Science(), vol 2650. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-540-44972-0_8","Line 103: ""models, We"" -> ""models, we",1690988323503,,,EMNLP/2023/Conference,30kbnyD9hF,"['EMNLP/2023/Conference/Submission339/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461024679,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission339/Reviewer_u6cP']",30kbnyD9hF,['EMNLP/2023/Conference/Submission339/Reviewer_u6cP'],1690988323503,1701461024679,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission339/Reviewer_u6cP']","Yes

The suggested paper is:
- Parsons, S., McBurney, P. (2003). Argumentation-Based Communication between Agents. In: Huget, MP. (eds) Communication in Multiagent Systems. Lecture Notes in Computer Science(), vol 2650. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-540-44972-0_8

This citation might be necessary because the reviewer points out a lack of reference explaining communication in the context of the paper, particularly in relation to the introduction of four communication modes (debate, report, relay, and memory) without sufficient support from literature. The suggested paper appears to provide relevant background in argumentation theory and communication in multi-agent systems, which could help strengthen the authors' proposal and provide a more thorough understanding of the communication modes introduced.",1,2003,Argumentation-Based Communication between Agents
7CLp8eX1Cu,"This paper aims to propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), a novel neural network framework that effectively captures conversation-level cross-modality interactions and utterance-level temporal dependencies in a modality-specific manner for conversation understanding.",-	Reasonable method.,"-	So difficult to follow the contribution of this paper. And it looks like an incremental engineering paper. The proposed method has been introduced in many papers, such as [1] Joshi, A., Bhat, A., Jain, A., Singh, A., & Modi, A. (2022, July). COGMEN: COntextualized GNN-based Multimodal Emotion Recognition. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 4148-4164).
-	The related work should be updated with more recent related works.
-	The experimental section needs some significance tests to further verify the effectiveness of the method put forward in the paper.
-	For the first time appearing in the text, the full name must be written, and abbreviations must be written in parentheses. When it appears in the abstract, it needs to be written once, and when it appears in the main text, it needs to be repeated again, that is, the full name+parentheses (abbreviations) should appear again.
-	Error analysis plays a crucial role in evaluating model performance and identifying potential issues. We encourage the authors to conduct error analysis in the paper and provide detailed explanations of the model's performance under different scenarios. Error analysis will aid in guiding subsequent improvements and expansions of the ERC research.
-	Writing mistakes are common across the overall paper, which could be found in “Typos, Grammar, Style, and Presentation Improvements”.","Question A: Why not compare with some advanced baselines, and what criteria were used to select these baselines?

**I lean towards giving a rating of 2.5 on Excitement.** The performance of the method doesn't seem significantly superior, as observed in various other approaches listed below. Additionally, the paper's contributions appear rather limited, primarily focusing on refining the COGMEN model.

*Shi, T., & Huang, S. L. (2023). MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 14752-14766).*  
*Chen, F., Shao, J., Zhu, S., & Shen, H. T. (2023). Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10761-10770).*  
*Lian, Z., Liu, B., & Tao, J. (2022). Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition. IEEE Transactions on Affective Computing.*","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"Typos, Grammar, Style, and Presentation Improvements:

-	The structure and fluency of the sentences in the paper are poor and require improvement to enhance the quality of the writing. (e.g., “Our social interactions and relationships are all influenced by emotions.”, “MMGCN adopts an undirected graph to effectively fuse multimodal information and capture long-distance contextual and inter-modal interactions”, “Extensive experiments on two popular benchmark datasets, i.e., IEMOCAP and CMU-MOSEI, demonstrate the effectiveness of CORECT”, “we validate that capturing the long-term dependencies, e.g., past relation, improves the performance of multimodal ERC task.”)
-	On page 2, there are some words or punctuation that may have ambiguity. (e.g., “ CORECT is a multimodal ERC model based on GNNs which captures both temporal dependencies and cross-modality interaction in conversational.”) In particular, the above problems appear in many parts of the paper.",1691173038164,,,EMNLP/2023/Conference,2z9o8bMQNd,"['EMNLP/2023/Conference/Submission5074/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461329937,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission5074/Reviewer_Xeu3']",2z9o8bMQNd,['EMNLP/2023/Conference/Submission5074/Reviewer_Xeu3'],1691173038164,1701461329937,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5074/Reviewer_Xeu3']","Yes

The suggested papers or references not cited in the manuscript are:
1. Joshi, A., Bhat, A., Jain, A., Singh, A., & Modi, A. (2022) - already cited, but mentioned as a comparison.
2. Shi, T., & Huang, S. L. (2023) - MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations.
3. Chen, F., Shao, J., Zhu, S., & Shen, H. T. (2023) - Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation.
4. Lian, Z., Liu, B., & Tao, J. (2022) - Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition.

These citations might be necessary because the reviewer mentions that the related work section should be updated with more recent related works, implying that the authors should be aware of and discuss the latest research in the field, including the mentioned papers. The reviewer also compares the proposed method's performance to other approaches, suggesting that the authors should be familiar with and cite these works to provide a more comprehensive understanding of their contribution.",1,"2022, 2023, 2023, 2022",MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations 
7CLp8eX1Cu,"This paper aims to propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), a novel neural network framework that effectively captures conversation-level cross-modality interactions and utterance-level temporal dependencies in a modality-specific manner for conversation understanding.",-	Reasonable method.,"-	So difficult to follow the contribution of this paper. And it looks like an incremental engineering paper. The proposed method has been introduced in many papers, such as [1] Joshi, A., Bhat, A., Jain, A., Singh, A., & Modi, A. (2022, July). COGMEN: COntextualized GNN-based Multimodal Emotion Recognition. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 4148-4164).
-	The related work should be updated with more recent related works.
-	The experimental section needs some significance tests to further verify the effectiveness of the method put forward in the paper.
-	For the first time appearing in the text, the full name must be written, and abbreviations must be written in parentheses. When it appears in the abstract, it needs to be written once, and when it appears in the main text, it needs to be repeated again, that is, the full name+parentheses (abbreviations) should appear again.
-	Error analysis plays a crucial role in evaluating model performance and identifying potential issues. We encourage the authors to conduct error analysis in the paper and provide detailed explanations of the model's performance under different scenarios. Error analysis will aid in guiding subsequent improvements and expansions of the ERC research.
-	Writing mistakes are common across the overall paper, which could be found in “Typos, Grammar, Style, and Presentation Improvements”.","Question A: Why not compare with some advanced baselines, and what criteria were used to select these baselines?

**I lean towards giving a rating of 2.5 on Excitement.** The performance of the method doesn't seem significantly superior, as observed in various other approaches listed below. Additionally, the paper's contributions appear rather limited, primarily focusing on refining the COGMEN model.

*Shi, T., & Huang, S. L. (2023). MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 14752-14766).*  
*Chen, F., Shao, J., Zhu, S., & Shen, H. T. (2023). Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10761-10770).*  
*Lian, Z., Liu, B., & Tao, J. (2022). Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition. IEEE Transactions on Affective Computing.*","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"Typos, Grammar, Style, and Presentation Improvements:

-	The structure and fluency of the sentences in the paper are poor and require improvement to enhance the quality of the writing. (e.g., “Our social interactions and relationships are all influenced by emotions.”, “MMGCN adopts an undirected graph to effectively fuse multimodal information and capture long-distance contextual and inter-modal interactions”, “Extensive experiments on two popular benchmark datasets, i.e., IEMOCAP and CMU-MOSEI, demonstrate the effectiveness of CORECT”, “we validate that capturing the long-term dependencies, e.g., past relation, improves the performance of multimodal ERC task.”)
-	On page 2, there are some words or punctuation that may have ambiguity. (e.g., “ CORECT is a multimodal ERC model based on GNNs which captures both temporal dependencies and cross-modality interaction in conversational.”) In particular, the above problems appear in many parts of the paper.",1691173038164,,,EMNLP/2023/Conference,2z9o8bMQNd,"['EMNLP/2023/Conference/Submission5074/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461329937,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission5074/Reviewer_Xeu3']",2z9o8bMQNd,['EMNLP/2023/Conference/Submission5074/Reviewer_Xeu3'],1691173038164,1701461329937,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5074/Reviewer_Xeu3']","Yes

The suggested papers or references not cited in the manuscript are:
1. Joshi, A., Bhat, A., Jain, A., Singh, A., & Modi, A. (2022) - already cited, but mentioned as a comparison.
2. Shi, T., & Huang, S. L. (2023) - MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations.
3. Chen, F., Shao, J., Zhu, S., & Shen, H. T. (2023) - Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation.
4. Lian, Z., Liu, B., & Tao, J. (2022) - Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition.

These citations might be necessary because the reviewer mentions that the related work section should be updated with more recent related works, implying that the authors should be aware of and discuss the latest research in the field, including the mentioned papers. The reviewer also compares the proposed method's performance to other approaches, suggesting that the authors should be familiar with and cite these works to provide a more comprehensive understanding of their contribution.",1,"2022, 2023, 2023, 2022","Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation "
7CLp8eX1Cu,"This paper aims to propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), a novel neural network framework that effectively captures conversation-level cross-modality interactions and utterance-level temporal dependencies in a modality-specific manner for conversation understanding.",-	Reasonable method.,"-	So difficult to follow the contribution of this paper. And it looks like an incremental engineering paper. The proposed method has been introduced in many papers, such as [1] Joshi, A., Bhat, A., Jain, A., Singh, A., & Modi, A. (2022, July). COGMEN: COntextualized GNN-based Multimodal Emotion Recognition. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 4148-4164).
-	The related work should be updated with more recent related works.
-	The experimental section needs some significance tests to further verify the effectiveness of the method put forward in the paper.
-	For the first time appearing in the text, the full name must be written, and abbreviations must be written in parentheses. When it appears in the abstract, it needs to be written once, and when it appears in the main text, it needs to be repeated again, that is, the full name+parentheses (abbreviations) should appear again.
-	Error analysis plays a crucial role in evaluating model performance and identifying potential issues. We encourage the authors to conduct error analysis in the paper and provide detailed explanations of the model's performance under different scenarios. Error analysis will aid in guiding subsequent improvements and expansions of the ERC research.
-	Writing mistakes are common across the overall paper, which could be found in “Typos, Grammar, Style, and Presentation Improvements”.","Question A: Why not compare with some advanced baselines, and what criteria were used to select these baselines?

**I lean towards giving a rating of 2.5 on Excitement.** The performance of the method doesn't seem significantly superior, as observed in various other approaches listed below. Additionally, the paper's contributions appear rather limited, primarily focusing on refining the COGMEN model.

*Shi, T., & Huang, S. L. (2023). MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 14752-14766).*  
*Chen, F., Shao, J., Zhu, S., & Shen, H. T. (2023). Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10761-10770).*  
*Lian, Z., Liu, B., & Tao, J. (2022). Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition. IEEE Transactions on Affective Computing.*","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"Typos, Grammar, Style, and Presentation Improvements:

-	The structure and fluency of the sentences in the paper are poor and require improvement to enhance the quality of the writing. (e.g., “Our social interactions and relationships are all influenced by emotions.”, “MMGCN adopts an undirected graph to effectively fuse multimodal information and capture long-distance contextual and inter-modal interactions”, “Extensive experiments on two popular benchmark datasets, i.e., IEMOCAP and CMU-MOSEI, demonstrate the effectiveness of CORECT”, “we validate that capturing the long-term dependencies, e.g., past relation, improves the performance of multimodal ERC task.”)
-	On page 2, there are some words or punctuation that may have ambiguity. (e.g., “ CORECT is a multimodal ERC model based on GNNs which captures both temporal dependencies and cross-modality interaction in conversational.”) In particular, the above problems appear in many parts of the paper.",1691173038164,,,EMNLP/2023/Conference,2z9o8bMQNd,"['EMNLP/2023/Conference/Submission5074/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461329937,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission5074/Reviewer_Xeu3']",2z9o8bMQNd,['EMNLP/2023/Conference/Submission5074/Reviewer_Xeu3'],1691173038164,1701461329937,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5074/Reviewer_Xeu3']","Yes

The suggested papers or references not cited in the manuscript are:
1. Joshi, A., Bhat, A., Jain, A., Singh, A., & Modi, A. (2022) - already cited, but mentioned as a comparison.
2. Shi, T., & Huang, S. L. (2023) - MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations.
3. Chen, F., Shao, J., Zhu, S., & Shen, H. T. (2023) - Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation.
4. Lian, Z., Liu, B., & Tao, J. (2022) - Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition.

These citations might be necessary because the reviewer mentions that the related work section should be updated with more recent related works, implying that the authors should be aware of and discuss the latest research in the field, including the mentioned papers. The reviewer also compares the proposed method's performance to other approaches, suggesting that the authors should be familiar with and cite these works to provide a more comprehensive understanding of their contribution.",1,"2022, 2023, 2023, 2022",Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition
wccOCnEAYe,"The paper proposes a new framework for finetuning Instruction Tuning model (Student Model) with a feedback score from Large Language Models (LLMs) as Teacher Model. The framework consists of two ranking methods for diversifying the generation outputs of Student Model (smaller LLM). 

The first method is referred to as Probabilistic Ranking, where Teacher Model (proprietary  GPT4) generates N-ranked diverse responses based log likelihood ranking. Then, Student Model uses the generated examples as ground truth to finetune the model on Instruction Tuning held-in data/task with new loss function: the aggregation of rank-based loss and weighted MLE Cross Entropy loss. The rank-based loss is defined as delta or the gap between the log likelihood of low rank responses versus high rank responses. The ranking loss is adopted from previous study (.., Zhao et al., 2023).

The second method is referred to as Contextual Ranking. That is, Student Model generates N-diverse responses by utilizing ROUGE-L diversity measure. Then, the generated responses are being evaluated by Teacher Model (GPT4). GPT4 is asked to rate the quality of reponses based on evaluation aspects (relevance, level of details, accuracy, etc) of two type of NLG objectives: Open-Ended and Close-Ended text generation.

The framework also explores the ablation and the ensemble of the two ranking methods as a continual process.

The main contribution of the paper is NLP Engineering experiment.","-	The experiments are reasonably executable, excluding the the utilization of propietary LLMs for researchers without API access.
-	The paper uses ablation study to validate the proposed method or framework, including varying different size of finetuning data and briefly discussing the risk of GPT4-based automatic evaluation.","My concerns of this manuscript are mainly two:

- The framework is highly dependent on propietary LLMs, which the authors have also mentioned in Limitation section.

The Student Model is forced to align with the preference of Teacher Model (GPT4), even though the risk of utilizing GPT4 as automatic evaluation metric is known, which authors have also discussed in the Results section. The risk, however, remains an open question. The pros is that the estimated cost for the API is less expensive than crowd-sourcing human. However, it does not necessarily mean that human is fully replaceable by larger LLMs. A feasible solution of those trade-offs is that authors can run human evaluation on few samples in which larger LLM performs differently (unfaithful vs faithful outcomes). For example, on samples where larger LLM has high performance (lower loss) versus samples where LLM has low performance score (higher loss).

- The model evaluation is rather unfair.

The authors compare between (i) models that are not aligned with GPT4 preference and (ii) aligned models (Student Models). So, intuitively the aligned models receive more support and can achieve a better performance results, particularly on held-out benchmark datasets in which GPT4 (Teacher Model) performs reasonably well. RLHF methods are taken directly from previous work’s results (Dubois et al., 2017), so it is not clear whether the resulting scores of PPO methods in Table 1 will be same if they are run on the same computing resource with the remaining models in the paper.  

Side note: 
An example of a more fair evaluation would be: comparing the performance between (i) models that is finetuned with human alignment ranking score vs. (ii) models that is finetuned with larger LLMs scoring. This human alignment setup might not be suitable for large scale finetuning data, so sampling on a smaller pool of data is a necessary precondition.",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9802–9822, Toronto, Canada. Association for Computational Linguistics.",,1690309375810,,,EMNLP/2023/Conference,2z4s0W375H,"['EMNLP/2023/Conference/Submission2913/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461197503,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2913/Reviewer_ANTd']",2z4s0W375H,['EMNLP/2023/Conference/Submission2913/Reviewer_ANTd'],1690309375810,1701461197503,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2913/Reviewer_ANTd']","Yes

The suggested paper is:
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories.

This citation might be necessary because the reviewer mentions ""Missing_References"" and provides this specific paper, implying that it could be relevant to the topic of the manuscript, particularly in the context of investigating the effectiveness and limitations of language models, which is a key aspect of the paper being reviewed. The reviewer's comment suggests that the authors could benefit from considering the findings of this paper, especially regarding the trustworthiness of language models, which is a concern raised in the review about the reliance on proprietary LLMs like GPT4 for evaluation.",1,2023,When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories
yxkCBNHmBI,"The authors present a framework for automatically extracting social norms from English & Chinese conversation datasets. Their framework involves prompting GPT-3 to extract candidate norms, and then self-verify those norms. NORMSAGE outperforms other approaches at norm discovery, as determined by human evaluators in multiple experiments.  ","This paper is well-written and broadens research on social norms to additional perspectives beyond a predominantly white, U.S.-centric one. The experiments and findings are compelling and the results and examples are presented in a clear manner. ","The authors use closed source models (e.g. GPT-3) that may be difficult to reproduce in future studies, as OpenAI models are often updated in unpredictable and non-transparent ways (e.g. “How is ChatGPT's behavior changing over time?” https://arxiv.org/abs/2307.09009). 

A (small) limitation of this paper is that it assumes that a few select TV shows are sufficient to reflect entire cultures’ norms. I say “small” limitation because the author’s approach for extracting norms could be applied to additional data by future work that builds upon this current contribution. 
",Line 250: How do you get the BERT embedding of a norm? (Embeddings are usually at the token-level rather than the phrase- or sentence-level). What is a BERT(n) embedding? ,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,For your limitations section: The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning by Ye & Durrett NeurIPS 2022. ,"Line 53: the citation is missing the year (2022). 

Line 138-139: “better representing the diverse socioethnic or demographic groups” -> “which can better represent diverse socioethnic or demographic groups” 

Line 178: “learnt” -> “learned” 

Change “Muslim vs. Western” in Table 3 to “Pakistani vs. British” as you say “British” in Table 4. The authors do say they define culture based on “cultural references in the source data,” so this change would also assume a Pakistani vs. British comparison captures the dimension that is most emphasized by the writers of that particular show. ",1690861601584,,,EMNLP/2023/Conference,2kSufHoYEi,"['EMNLP/2023/Conference/Submission2827/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461191154,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2827/Reviewer_sQGj']",2kSufHoYEi,['EMNLP/2023/Conference/Submission2827/Reviewer_sQGj'],1690861601584,1701461191154,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2827/Reviewer_sQGj']","Yes

The suggested paper not cited in the manuscript is:
- ""The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning"" by Ye & Durrett, NeurIPS 2022.

This citation might be necessary because the reviewer mentions it in the ""Missing_References"" section, specifically for the limitations section of the paper. The reviewer suggests that this reference could be relevant to the discussion of limitations, possibly because it relates to the reliability of explanations in few-shot prompting, which could be relevant to the authors' use of GPT-3 for extracting social norms.",1,2022,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning
DzSBB3svfU,"This paper proposes T5SCORE, a text generation evaluation metric. T5SCORE ensembles a discriminative one, which is trained on unsupervised data using contrastive learning, and a generative one, which learns a conditional distribution. The proposed metric is evaluated on machine translation task and summarization task.","T5SCORE can evaluate both generation task with or without reference. Due to the fact that it's constructed on mT5, it can evaluate generated texts in different languages.","1. There is no technique novelty in this paper. For the generative metric, it just learns a distribution like traditional unsupervised task. For discriminative metric, contrastive learning is not new. For example, like SESCORE2: Learning Text Generation Evaluation via Synthesizing
Realistic Mistakes, which is also a text generation evaluation metric and used contrastive learning. The author should also cite and compare with this work.
2. The proposed method didn't compare with some new methods like SESCORE, SESCORE2.
3. The experimental results exhibit mixed performance. For example, in table2, BARTScore, COMET, BLEURT also perform best on some tasks, while T5SCORE with different settings perform best on different tasks, such as T5SCORE-B on en-de (sys-k, sys-p) and T5SCORE-L on en-de (seg-k).","1. Is there one separate model for every task and language? If so, I think the potential of mT5 is not fully utilized and the usage of T5SCORE is limited.
2. In 3.3, how to find hypothesis with higher and lower ratings? Are there many2one data pairs in the original dataset for all supervised datasets?
3. Why the author use different model in different settings? In Figure2, T5SCORE-XL is used while in figure 3, T5SCORE-L is used.
4. Why the author didn't use the newest MQM dataset, like MQM-2022?
5. The author should also evaluate some diverse generation task performance to validate the performance of the proposed metric instead of only MT and Summarization.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Not All Errors Are Equal: Learning Text Generation Metrics using Stratified Error Synthesis. Xu et al. ACL 2021.
SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes. Xu et al. ACL 2022.",,1691035695427,,,EMNLP/2023/Conference,2jibzAXJzH,"['EMNLP/2023/Conference/Submission3010/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461203666,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3010/Reviewer_SSiu']",2jibzAXJzH,['EMNLP/2023/Conference/Submission3010/Reviewer_SSiu'],1691035695427,1701461203666,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3010/Reviewer_SSiu']","Yes

The suggested papers or references not cited in the manuscript are:
1. SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes (Xu et al., ACL 2022)
2. Not All Errors Are Equal: Learning Text Generation Metrics using Stratified Error Synthesis (Xu et al., ACL 2021)

These citations might be necessary because the reviewer believes that the proposed method, T5SCORE, lacks technique novelty and that similar work has been done in the past. The reviewer specifically mentions SESCORE2, which also uses contrastive learning, and suggests that the authors should cite and compare their work with this existing research. This comparison is necessary to demonstrate the originality and contribution of the proposed metric, T5SCORE. Additionally, citing ""Not All Errors Are Equal"" could provide further context on learning text generation metrics, making the manuscript more comprehensive and well-researched.",1,"2022, 2021",SESCORE2 Learning Text Generation Evaluation via Synthesizing Realistic Mistakes 
DzSBB3svfU,"This paper proposes T5SCORE, a text generation evaluation metric. T5SCORE ensembles a discriminative one, which is trained on unsupervised data using contrastive learning, and a generative one, which learns a conditional distribution. The proposed metric is evaluated on machine translation task and summarization task.","T5SCORE can evaluate both generation task with or without reference. Due to the fact that it's constructed on mT5, it can evaluate generated texts in different languages.","1. There is no technique novelty in this paper. For the generative metric, it just learns a distribution like traditional unsupervised task. For discriminative metric, contrastive learning is not new. For example, like SESCORE2: Learning Text Generation Evaluation via Synthesizing
Realistic Mistakes, which is also a text generation evaluation metric and used contrastive learning. The author should also cite and compare with this work.
2. The proposed method didn't compare with some new methods like SESCORE, SESCORE2.
3. The experimental results exhibit mixed performance. For example, in table2, BARTScore, COMET, BLEURT also perform best on some tasks, while T5SCORE with different settings perform best on different tasks, such as T5SCORE-B on en-de (sys-k, sys-p) and T5SCORE-L on en-de (seg-k).","1. Is there one separate model for every task and language? If so, I think the potential of mT5 is not fully utilized and the usage of T5SCORE is limited.
2. In 3.3, how to find hypothesis with higher and lower ratings? Are there many2one data pairs in the original dataset for all supervised datasets?
3. Why the author use different model in different settings? In Figure2, T5SCORE-XL is used while in figure 3, T5SCORE-L is used.
4. Why the author didn't use the newest MQM dataset, like MQM-2022?
5. The author should also evaluate some diverse generation task performance to validate the performance of the proposed metric instead of only MT and Summarization.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Not All Errors Are Equal: Learning Text Generation Metrics using Stratified Error Synthesis. Xu et al. ACL 2021.
SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes. Xu et al. ACL 2022.",,1691035695427,,,EMNLP/2023/Conference,2jibzAXJzH,"['EMNLP/2023/Conference/Submission3010/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461203666,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3010/Reviewer_SSiu']",2jibzAXJzH,['EMNLP/2023/Conference/Submission3010/Reviewer_SSiu'],1691035695427,1701461203666,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3010/Reviewer_SSiu']","Yes

The suggested papers or references not cited in the manuscript are:
1. SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes (Xu et al., ACL 2022)
2. Not All Errors Are Equal: Learning Text Generation Metrics using Stratified Error Synthesis (Xu et al., ACL 2021)

These citations might be necessary because the reviewer believes that the proposed method, T5SCORE, lacks technique novelty and that similar work has been done in the past. The reviewer specifically mentions SESCORE2, which also uses contrastive learning, and suggests that the authors should cite and compare their work with this existing research. This comparison is necessary to demonstrate the originality and contribution of the proposed metric, T5SCORE. Additionally, citing ""Not All Errors Are Equal"" could provide further context on learning text generation metrics, making the manuscript more comprehensive and well-researched.",1,"2022, 2021",Not All Errors Are Equal Learning Text Generation Metrics using Stratified Error Synthesis
S0dKHj7ZDc,"The paper proposes to reduce Transformer's computational cost by reducing the computational complexity of feed forward layers, specifically, it splits the large matrix space to several small spaces and uses the Multi-Space Cross method to prevent performance decline. The paper evaluates the accuracy and efficiency on the Long-Range Arena benchmark and shows the proposed MSCFFN can achieve better results with a faster speed. ",The proposed MSCFFN can save more than 60 percent of the computation resources without undercutting model performances.,"The paper fails to compare to related work. Even though the goal of the proposed model is to reduce computational cost of transformers, the method itself is a method to reduce FFN, and the paper fails to introduce related work and compare to them. Please see related work in ""Missing References"" section. ",,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"1. Zhang, Zhengyan, et al. ""Moefication: Transformer feed-forward layers are mixtures of experts."" arXiv preprint arXiv:2110.01786 (2021).
2. Chelombiev, Ivan, et al. ""Groupbert: Enhanced transformer architecture with efficient grouped structures."" arXiv preprint arXiv:2106.05822 (2021).

Please also check the related work section in the above papers.","There should be a space between ""("" and its preceding word (line 041 and 042).",1690989523928,,,EMNLP/2023/Conference,2b7aSGxb6M,"['EMNLP/2023/Conference/Submission1650/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461116264,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1650/Reviewer_iZ74']",2b7aSGxb6M,['EMNLP/2023/Conference/Submission1650/Reviewer_iZ74'],1690989523928,1701461116264,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1650/Reviewer_iZ74']","Yes

The suggested papers or references not cited in the manuscript are:
1. Zhang, Zhengyan, et al. ""Moefication: Transformer feed-forward layers are mixtures of experts."" arXiv preprint arXiv:2110.01786 (2021)
2. Chelombiev, Ivan, et al. ""Groupbert: Enhanced transformer architecture with efficient grouped structures."" arXiv preprint arXiv:2106.05822 (2021)

These citations might be necessary because the reviewer points out that the paper fails to compare to related work, specifically methods that also aim to reduce the computational cost of transformers, such as Moefication and Groupbert. The reviewer suggests that the authors should introduce and compare their proposed method (MSCFFN) to these existing methods to provide a more comprehensive understanding of its effectiveness and novelty.",1,"2021, 2021, 2021, 2021",Moefication: Transformer feed-forward layers are mixtures of experts 
S0dKHj7ZDc,"The paper proposes to reduce Transformer's computational cost by reducing the computational complexity of feed forward layers, specifically, it splits the large matrix space to several small spaces and uses the Multi-Space Cross method to prevent performance decline. The paper evaluates the accuracy and efficiency on the Long-Range Arena benchmark and shows the proposed MSCFFN can achieve better results with a faster speed. ",The proposed MSCFFN can save more than 60 percent of the computation resources without undercutting model performances.,"The paper fails to compare to related work. Even though the goal of the proposed model is to reduce computational cost of transformers, the method itself is a method to reduce FFN, and the paper fails to introduce related work and compare to them. Please see related work in ""Missing References"" section. ",,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"1. Zhang, Zhengyan, et al. ""Moefication: Transformer feed-forward layers are mixtures of experts."" arXiv preprint arXiv:2110.01786 (2021).
2. Chelombiev, Ivan, et al. ""Groupbert: Enhanced transformer architecture with efficient grouped structures."" arXiv preprint arXiv:2106.05822 (2021).

Please also check the related work section in the above papers.","There should be a space between ""("" and its preceding word (line 041 and 042).",1690989523928,,,EMNLP/2023/Conference,2b7aSGxb6M,"['EMNLP/2023/Conference/Submission1650/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461116264,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1650/Reviewer_iZ74']",2b7aSGxb6M,['EMNLP/2023/Conference/Submission1650/Reviewer_iZ74'],1690989523928,1701461116264,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1650/Reviewer_iZ74']","Yes

The suggested papers or references not cited in the manuscript are:
1. Zhang, Zhengyan, et al. ""Moefication: Transformer feed-forward layers are mixtures of experts."" arXiv preprint arXiv:2110.01786 (2021)
2. Chelombiev, Ivan, et al. ""Groupbert: Enhanced transformer architecture with efficient grouped structures."" arXiv preprint arXiv:2106.05822 (2021)

These citations might be necessary because the reviewer points out that the paper fails to compare to related work, specifically methods that also aim to reduce the computational cost of transformers, such as Moefication and Groupbert. The reviewer suggests that the authors should introduce and compare their proposed method (MSCFFN) to these existing methods to provide a more comprehensive understanding of its effectiveness and novelty.",1,"2021, 2021, 2021, 2021",Groupbert: Enhanced transformer architecture with efficient grouped structures
2iyTDg1HJl,"This paper investigates challenges for context-aware neural machine translation. The authors show that discourse phenomena are sparse and that a context-aware transformer model (concatenating the context) doesn't perform well for some of them. They also replace the transformer model with another architecture (MEGA), with limited improvement, and argue for a better document-level translation metric. A paragraph-to-paragraph dataset of Chinese-English novels is also collected.","The analysis, especially in sections 4.1 (discourse phenomena sparsity) and 4.2 (model performance for contextual phenomena), is fairly detailed. We can observe that some phenomena on which the context-aware models are struggling (e.g. tense, discourse markers) are infrequent (table 1).

The experiments are generally sound.","Section 4.3 (""The sentence-level NMT baseline is already competitive"") is potentially misleading. Because BLEU scores are similar between context-aware and context-agnostic models, the authors mention that ""This suggests that [...] context-agnostic models are already capable of delivering high-quality translations."" However, this could also indicate that both types of model generate deficient translations.

High-levels observations in section 4 are often already known (although relevant papers are cited and the specific results/experiments are novel). Data sparsity is mentioned for example in [1]. Limited differences in BLEU scores have (in part) motivated the design of metrics to capture specific discourse phenomena (but these metrics are not perfect).

The collected paragraph-to-paragraph dataset is quite small, so its usefulness is not well established.

[1] Lupo et al. Divide and rule: Effective pre-training for context-aware multi-encoder translation models. ACL. 2022","A. What does synthetic and analytic mean to describe languages?

B. For table 7, what would be the model performances before fine-tuning (i.e trained on WMT17 Zh-En only)?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Zhang and Liu. Paragraph-Parallel based Neural Machine Translation Model
with Hierarchical Attention. Journal of Physics: Conference Series. 2020. https://iopscience.iop.org/article/10.1088/1742-6596/1453/1/012006/pdf
Note: The previous paper appears to be almost the same as Zhang et al. Paragraph-Level Hierarchical Neural Machine Translation. ICONIP. 2019.

[Contemporaneous] Ghussin et al. Exploring Paracrawl for Document-level Neural Machine Translation. EACL. 2023.",Thai et al. Exploring document-level literary machine translation with parallel paragraphs from world literature. EMNLP. 2022. is duplicated in the references.,1691193420936,,,EMNLP/2023/Conference,2anfut5geh,"['EMNLP/2023/Conference/Submission2580/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461174477,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2580/Reviewer_bWHY']",2anfut5geh,['EMNLP/2023/Conference/Submission2580/Reviewer_bWHY'],1691193420936,1701461174477,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2580/Reviewer_bWHY']","Yes

The suggested papers or references not cited in the manuscript are:
1. Lupo et al. Divide and rule: Effective pre-training for context-aware multi-encoder translation models. ACL. 2022 (already cited, but mentioned as relevant to data sparsity)
2. Zhang and Liu. Paragraph-Parallel based Neural Machine Translation Model with Hierarchical Attention. Journal of Physics: Conference Series. 2020.
3. Zhang et al. Paragraph-Level Hierarchical Neural Machine Translation. ICONIP. 2019.
4. Ghussin et al. Exploring Paracrawl for Document-level Neural Machine Translation. EACL. 2023 (marked as [Contemporaneous])

These citations might be necessary because they appear to be relevant to the topic of context-aware neural machine translation and document-level translation. The reviewer mentions that some of the high-level observations in the paper are already known and cites Lupo et al. as an example. The Zhang and Liu, and Zhang et al. papers seem to be related to paragraph-level neural machine translation, which is relevant to the collected paragraph-to-paragraph dataset in the manuscript. The Ghussin et al. paper, marked as [Contemporaneous], suggests that it may have been published around the same time as the manuscript and could be relevant to the discussion on document-level neural machine translation.",1,"2022, 2020, 2019, 2023",Divide and rule: Effective pre-training for context-aware multi-encoder translation models
2iyTDg1HJl,"This paper investigates challenges for context-aware neural machine translation. The authors show that discourse phenomena are sparse and that a context-aware transformer model (concatenating the context) doesn't perform well for some of them. They also replace the transformer model with another architecture (MEGA), with limited improvement, and argue for a better document-level translation metric. A paragraph-to-paragraph dataset of Chinese-English novels is also collected.","The analysis, especially in sections 4.1 (discourse phenomena sparsity) and 4.2 (model performance for contextual phenomena), is fairly detailed. We can observe that some phenomena on which the context-aware models are struggling (e.g. tense, discourse markers) are infrequent (table 1).

The experiments are generally sound.","Section 4.3 (""The sentence-level NMT baseline is already competitive"") is potentially misleading. Because BLEU scores are similar between context-aware and context-agnostic models, the authors mention that ""This suggests that [...] context-agnostic models are already capable of delivering high-quality translations."" However, this could also indicate that both types of model generate deficient translations.

High-levels observations in section 4 are often already known (although relevant papers are cited and the specific results/experiments are novel). Data sparsity is mentioned for example in [1]. Limited differences in BLEU scores have (in part) motivated the design of metrics to capture specific discourse phenomena (but these metrics are not perfect).

The collected paragraph-to-paragraph dataset is quite small, so its usefulness is not well established.

[1] Lupo et al. Divide and rule: Effective pre-training for context-aware multi-encoder translation models. ACL. 2022","A. What does synthetic and analytic mean to describe languages?

B. For table 7, what would be the model performances before fine-tuning (i.e trained on WMT17 Zh-En only)?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Zhang and Liu. Paragraph-Parallel based Neural Machine Translation Model
with Hierarchical Attention. Journal of Physics: Conference Series. 2020. https://iopscience.iop.org/article/10.1088/1742-6596/1453/1/012006/pdf
Note: The previous paper appears to be almost the same as Zhang et al. Paragraph-Level Hierarchical Neural Machine Translation. ICONIP. 2019.

[Contemporaneous] Ghussin et al. Exploring Paracrawl for Document-level Neural Machine Translation. EACL. 2023.",Thai et al. Exploring document-level literary machine translation with parallel paragraphs from world literature. EMNLP. 2022. is duplicated in the references.,1691193420936,,,EMNLP/2023/Conference,2anfut5geh,"['EMNLP/2023/Conference/Submission2580/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461174477,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2580/Reviewer_bWHY']",2anfut5geh,['EMNLP/2023/Conference/Submission2580/Reviewer_bWHY'],1691193420936,1701461174477,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2580/Reviewer_bWHY']","Yes

The suggested papers or references not cited in the manuscript are:
1. Lupo et al. Divide and rule: Effective pre-training for context-aware multi-encoder translation models. ACL. 2022 (already cited, but mentioned as relevant to data sparsity)
2. Zhang and Liu. Paragraph-Parallel based Neural Machine Translation Model with Hierarchical Attention. Journal of Physics: Conference Series. 2020.
3. Zhang et al. Paragraph-Level Hierarchical Neural Machine Translation. ICONIP. 2019.
4. Ghussin et al. Exploring Paracrawl for Document-level Neural Machine Translation. EACL. 2023 (marked as [Contemporaneous])

These citations might be necessary because they appear to be relevant to the topic of context-aware neural machine translation and document-level translation. The reviewer mentions that some of the high-level observations in the paper are already known and cites Lupo et al. as an example. The Zhang and Liu, and Zhang et al. papers seem to be related to paragraph-level neural machine translation, which is relevant to the collected paragraph-to-paragraph dataset in the manuscript. The Ghussin et al. paper, marked as [Contemporaneous], suggests that it may have been published around the same time as the manuscript and could be relevant to the discussion on document-level neural machine translation.",1,"2022, 2020, 2019, 2023",Paragraph-Parallel based Neural Machine Translation Model with Hierarchical Attention
2iyTDg1HJl,"This paper investigates challenges for context-aware neural machine translation. The authors show that discourse phenomena are sparse and that a context-aware transformer model (concatenating the context) doesn't perform well for some of them. They also replace the transformer model with another architecture (MEGA), with limited improvement, and argue for a better document-level translation metric. A paragraph-to-paragraph dataset of Chinese-English novels is also collected.","The analysis, especially in sections 4.1 (discourse phenomena sparsity) and 4.2 (model performance for contextual phenomena), is fairly detailed. We can observe that some phenomena on which the context-aware models are struggling (e.g. tense, discourse markers) are infrequent (table 1).

The experiments are generally sound.","Section 4.3 (""The sentence-level NMT baseline is already competitive"") is potentially misleading. Because BLEU scores are similar between context-aware and context-agnostic models, the authors mention that ""This suggests that [...] context-agnostic models are already capable of delivering high-quality translations."" However, this could also indicate that both types of model generate deficient translations.

High-levels observations in section 4 are often already known (although relevant papers are cited and the specific results/experiments are novel). Data sparsity is mentioned for example in [1]. Limited differences in BLEU scores have (in part) motivated the design of metrics to capture specific discourse phenomena (but these metrics are not perfect).

The collected paragraph-to-paragraph dataset is quite small, so its usefulness is not well established.

[1] Lupo et al. Divide and rule: Effective pre-training for context-aware multi-encoder translation models. ACL. 2022","A. What does synthetic and analytic mean to describe languages?

B. For table 7, what would be the model performances before fine-tuning (i.e trained on WMT17 Zh-En only)?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Zhang and Liu. Paragraph-Parallel based Neural Machine Translation Model
with Hierarchical Attention. Journal of Physics: Conference Series. 2020. https://iopscience.iop.org/article/10.1088/1742-6596/1453/1/012006/pdf
Note: The previous paper appears to be almost the same as Zhang et al. Paragraph-Level Hierarchical Neural Machine Translation. ICONIP. 2019.

[Contemporaneous] Ghussin et al. Exploring Paracrawl for Document-level Neural Machine Translation. EACL. 2023.",Thai et al. Exploring document-level literary machine translation with parallel paragraphs from world literature. EMNLP. 2022. is duplicated in the references.,1691193420936,,,EMNLP/2023/Conference,2anfut5geh,"['EMNLP/2023/Conference/Submission2580/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461174477,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2580/Reviewer_bWHY']",2anfut5geh,['EMNLP/2023/Conference/Submission2580/Reviewer_bWHY'],1691193420936,1701461174477,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2580/Reviewer_bWHY']","Yes

The suggested papers or references not cited in the manuscript are:
1. Lupo et al. Divide and rule: Effective pre-training for context-aware multi-encoder translation models. ACL. 2022 (already cited, but mentioned as relevant to data sparsity)
2. Zhang and Liu. Paragraph-Parallel based Neural Machine Translation Model with Hierarchical Attention. Journal of Physics: Conference Series. 2020.
3. Zhang et al. Paragraph-Level Hierarchical Neural Machine Translation. ICONIP. 2019.
4. Ghussin et al. Exploring Paracrawl for Document-level Neural Machine Translation. EACL. 2023 (marked as [Contemporaneous])

These citations might be necessary because they appear to be relevant to the topic of context-aware neural machine translation and document-level translation. The reviewer mentions that some of the high-level observations in the paper are already known and cites Lupo et al. as an example. The Zhang and Liu, and Zhang et al. papers seem to be related to paragraph-level neural machine translation, which is relevant to the collected paragraph-to-paragraph dataset in the manuscript. The Ghussin et al. paper, marked as [Contemporaneous], suggests that it may have been published around the same time as the manuscript and could be relevant to the discussion on document-level neural machine translation.",1,"2022, 2020, 2019, 2023",Paragraph-Level Hierarchical Neural Machine Translation
2iyTDg1HJl,"This paper investigates challenges for context-aware neural machine translation. The authors show that discourse phenomena are sparse and that a context-aware transformer model (concatenating the context) doesn't perform well for some of them. They also replace the transformer model with another architecture (MEGA), with limited improvement, and argue for a better document-level translation metric. A paragraph-to-paragraph dataset of Chinese-English novels is also collected.","The analysis, especially in sections 4.1 (discourse phenomena sparsity) and 4.2 (model performance for contextual phenomena), is fairly detailed. We can observe that some phenomena on which the context-aware models are struggling (e.g. tense, discourse markers) are infrequent (table 1).

The experiments are generally sound.","Section 4.3 (""The sentence-level NMT baseline is already competitive"") is potentially misleading. Because BLEU scores are similar between context-aware and context-agnostic models, the authors mention that ""This suggests that [...] context-agnostic models are already capable of delivering high-quality translations."" However, this could also indicate that both types of model generate deficient translations.

High-levels observations in section 4 are often already known (although relevant papers are cited and the specific results/experiments are novel). Data sparsity is mentioned for example in [1]. Limited differences in BLEU scores have (in part) motivated the design of metrics to capture specific discourse phenomena (but these metrics are not perfect).

The collected paragraph-to-paragraph dataset is quite small, so its usefulness is not well established.

[1] Lupo et al. Divide and rule: Effective pre-training for context-aware multi-encoder translation models. ACL. 2022","A. What does synthetic and analytic mean to describe languages?

B. For table 7, what would be the model performances before fine-tuning (i.e trained on WMT17 Zh-En only)?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Zhang and Liu. Paragraph-Parallel based Neural Machine Translation Model
with Hierarchical Attention. Journal of Physics: Conference Series. 2020. https://iopscience.iop.org/article/10.1088/1742-6596/1453/1/012006/pdf
Note: The previous paper appears to be almost the same as Zhang et al. Paragraph-Level Hierarchical Neural Machine Translation. ICONIP. 2019.

[Contemporaneous] Ghussin et al. Exploring Paracrawl for Document-level Neural Machine Translation. EACL. 2023.",Thai et al. Exploring document-level literary machine translation with parallel paragraphs from world literature. EMNLP. 2022. is duplicated in the references.,1691193420936,,,EMNLP/2023/Conference,2anfut5geh,"['EMNLP/2023/Conference/Submission2580/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461174477,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2580/Reviewer_bWHY']",2anfut5geh,['EMNLP/2023/Conference/Submission2580/Reviewer_bWHY'],1691193420936,1701461174477,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2580/Reviewer_bWHY']","Yes

The suggested papers or references not cited in the manuscript are:
1. Lupo et al. Divide and rule: Effective pre-training for context-aware multi-encoder translation models. ACL. 2022 (already cited, but mentioned as relevant to data sparsity)
2. Zhang and Liu. Paragraph-Parallel based Neural Machine Translation Model with Hierarchical Attention. Journal of Physics: Conference Series. 2020.
3. Zhang et al. Paragraph-Level Hierarchical Neural Machine Translation. ICONIP. 2019.
4. Ghussin et al. Exploring Paracrawl for Document-level Neural Machine Translation. EACL. 2023 (marked as [Contemporaneous])

These citations might be necessary because they appear to be relevant to the topic of context-aware neural machine translation and document-level translation. The reviewer mentions that some of the high-level observations in the paper are already known and cites Lupo et al. as an example. The Zhang and Liu, and Zhang et al. papers seem to be related to paragraph-level neural machine translation, which is relevant to the collected paragraph-to-paragraph dataset in the manuscript. The Ghussin et al. paper, marked as [Contemporaneous], suggests that it may have been published around the same time as the manuscript and could be relevant to the discussion on document-level neural machine translation.",1,"2022, 2020, 2019, 2023",Exploring Paracrawl for Document-level Neural Machine Translation
v5Lr4qTN7G,"This paper addresses transfer between Swiss German dialects in speech translation from Swiss German to Standard German.  
Specifically, it looks at translation of 7 different Swiss German dialects to German using the STT4SG-350 corpus with the balanced train splits of 34 hours per dialect. Compared are directly finetuning pretrained models (XLS-R and Whisper) on all dialects on a single dialect, or leave one out; and finetuning a transformer model possibly pretrained on SDS-200 (a Swiss German speech translation corpus) on each of these conditions (called ""Trafo""). Two different model sizes are used for XLS-R to check whether model size has an effect (conclusions are shown to be robust across model scales).  
Results suggest that transfer ability roughly corresponds to linguistic distances between dialects derived from DDAGGS features.  
Analysis on the preterite and vocabulary likely to be specific to Swiss German using crawled word lists with semi-manual filtering is conducted (each leading to the conclusion that there is a difference in these two cases, but stated that it is hard to pinpoint why).  
Results suggest that leave one out is typically similar to including all dialects, but with a weaker pretrained model (Trafo) more distantly related dialects suffer as more data is left out, and finetuning on a single dialect is insufficient in all cases. There is a greater gap between all/leave one out/single dialect finetuning for Whisper than XLS-R (presumably because it is trained on less non-English data, and so more sensitive to having less finetuning data?).  Reasons for differences between XLS-R and Whisper are not discussed, and the comparison with Trafo is a bit murky.","- Experimental comparison of speech translation of Swiss German dialects into Standard German, with three different models, and three different transfer/finetuning settings (all, some, one)
- Detailed analysis of differences between dialects with examples","- Narrow scope using existing models and techniques; differences between dialects are the focus, rather than the techniques, making the conclusions potentially helpful to a specific target audience but not necessarily generalizable to other cases, particularly given that there is no task-specific parameter tuning and not a direct comparison between model settings 
- Some experimental details are missing or confusingly stated. A number of basic model/data details are referred to other works and should be included here (at least in the appendix) for clarity instead of requiring lookups in multiple other papers (for example, the size of the Trafo data, the model architecture, the size of the seq2seq decoder). 
- The space could be used more effectively; analysis is at times redundant with the text as it lists what is in figures and tables (for example, significant space is used for heatmaps whose numbers which are then summarized in the text, rather than further discussion of why there may be differences in performance or trends such as between Whisper and XLS-R).","Question A: Could you clarify what data the Trafo model was trained on? (pretraining and finetuning)

Question B: Given the other experiments showing CTC performs significantly better than a seq2seq decoder trained from scratch, why not use CTC here as well for a more appropriate comparison to a model trained directly on/for Swiss German? What is the size of the seq2seq decoder? Given that the target language is a high resource language (Standard German) why not use a pretrained decoder or additional language modelling data? 

Question C: For context for the scores presented here, if a faithful dialectal *transcript* was output rather than the Standard German translation, what would the BLEU scores with the Standard German reference translation be? Given that the vocabulary analysis says some models leave special vocabulary in Swiss German, this would be particularly good to report. 

Question D: Would it be possible to include Standard German in the linguistic distances for context? 

Question E: What were the model vocabularies used? It is said that the preprocessing limits characters to lowercase characters (ASCII characters?) and the German umlauts ä, ö, and ü. Does this mean all (non-Whisper) models use character-level vocabularies, or is this just a normalization step?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"It is not completely clear what data the Trafo model is trained on: 
- Section 4.2 says 'the baseline of Plüss et al 2022 is replicated and fine-tuned on our data', and the balanced training is said to be 34 hours per dialect, and looking up Plüss et al 2022 it is a 200 hour Swiss German to Standard German corpus and the baseline is a 12-6 layer encoder-decoder trained on all of the data. On L373 it says re Trafo ""50 hours of data are little to fine-tune such a model from scratch"" when mentioning that the single dialect fine-tuning resulting in a score of 0.  
- Is it the case that the Trafo model is pretrained on the 200 hours from SDS-200, and then finetuned on each condition? (all dialects, LOO, single dialect). Or, has it been trained from scratch on each condition, using the baseline architecture only from Plüss et al 2022, in which case these models have been trained on between 34 and 276 hours? Should L373 say ""34 hours is too little to fine-tune such a model"" or ""34 hours is too little to train such a model from scratch"" ? If they are in fact fine-tuned, it is surprising that with 200+ hours of Swiss German to Standard German data there are scores of 0?

Table 2: Consider renaming ""Baseline"" to ""All dialects"" for clarity and to help explain why in most cases it performs the best

BLEU scores are typically reported multiplied by 100 to be 0-100, and here presented 0-1; the presented scores are presumably quite high (45-73) and not very low (<1), consider multiplying by 100 to standardize. 

It is stated that model checkpoints are chosen based on WER, but WER is not reported. Consider adding WER for context in the results table, or at least in the appendix. 

BLEU is known to be less appropriate than other metrics such as chrF for dialects which are often marked by differences of 1 to 2 characters, causing potentially large differences in BLEU for small differences in system outputs; consider reporting at least chrF as an additional metric.

Why use the BLEU implementation from Plüss 2023b (which uses NLTK and is not common practice because it is not comparable to other implementations), and not SacreBLEU [(Post 2018)](https://aclanthology.org/W18-6319/)? It should be noted not just that BLEU is calculated using the script provided by (Plüss et al, 2023b) but include ""which uses the NLTK implementation"" so this is clear and scores are not compared with a different implementation.

Typo: L772 ""task-specific specific""",1691234260045,,,EMNLP/2023/Conference,2X5RXTOsLU,"['EMNLP/2023/Conference/Submission3493/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461233036,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3493/Reviewer_xjs8']",2X5RXTOsLU,['EMNLP/2023/Conference/Submission3493/Reviewer_xjs8'],1691234260045,1701461233036,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3493/Reviewer_xjs8']","Yes

The suggested papers or references not cited in the manuscript are:
1. Post 2018 (SacreBLEU) - This is suggested as a more common and comparable implementation of BLEU, implying that the authors should consider using this instead of the NLTK implementation from Plüss 2023b for calculating BLEU scores.

These citations might be necessary because the reviewer suggests that using the SacreBLEU implementation would make the results more comparable to other works, and notes that the current implementation used by the authors (from Plüss 2023b) is not common practice. This implies that the reviewer thinks the authors should be aware of and consider using more standardized metrics and implementations to increase the validity and reproducibility of their results.",1,2018,SacreBLEU
PhGcGR7H3h,"This paper proposes Masked Path Modeling to improve VLN performance. Specifically, this paper uses masked paths to replace the instructions and optimize the single action prediction training objective. Empirically, on R2R, R4R, and RxR datasets, the proposed approach improves the baseline method by around 1-2% in success rate.","1. Good performance on multiple VLN datasets. 
2. The paper is well-written and easy to follow.
3. Reasonable approach to improve the performance.","1. The data collection pipeline is not new. In PREVALENT, they traverse through all the possible paths (with length ranging from 3-6) in the seen environments. Furthermore, PREVALENT generates synthetic instructions for all those unannotated paths. Given the data collection process mentioned in the paper, I don't think any new paths are actually created, thus I'm not sure why MPM helps reduce the data scarcity problem in VLN. Besides, no stats about the collected paths are given in the paper. 
2. Simply adding a pre-training/fine-tuning objective is not novel enough. Similar ideas that optimize the self-supervised image-level loss are explored in ""Improving Vision-and-Language Navigation by Generating Future-View Image Semantics"", where they propose Masked Trajectory Modeling (MTM) that masks out views in a trajectory. 
3. HAMT is a sota approach in 2021. To demonstrate the effectiveness of the proposed approach (i.e., could generalize to different model architecture/training strategy), at least MPM should be applied to ""Think Global, Act Local: Dual-Scale Graph Transformer for Vision-and-Language Navigation"" (CVPR 2022).",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,,,1691106993133,,,EMNLP/2023/Conference,2UJvVc8gnP,"['EMNLP/2023/Conference/Submission1127/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461085632,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1127/Reviewer_otNW']",2UJvVc8gnP,['EMNLP/2023/Conference/Submission1127/Reviewer_otNW'],1691106993133,1701461085632,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1127/Reviewer_otNW']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Improving Vision-and-Language Navigation by Generating Future-View Image Semantics"" (which proposes Masked Trajectory Modeling, MTM)
2. ""Think Global, Act Local: Dual-Scale Graph Transformer for Vision-and-Language Navigation"" (CVPR 2022)

These citations might be necessary because the reviewer believes that the proposed approach (Masked Path Modeling, MPM) is not novel enough and has similarities with existing work, such as MTM. Additionally, the reviewer suggests that applying MPM to a state-of-the-art approach like ""Think Global, Act Local: Dual-Scale Graph Transformer for Vision-and-Language Navigation"" could demonstrate its effectiveness and ability to generalize to different model architectures and training strategies.",1,2022,Improving Vision-and-Language Navigation by Generating Future-View Image Semantics 
PhGcGR7H3h,"This paper proposes Masked Path Modeling to improve VLN performance. Specifically, this paper uses masked paths to replace the instructions and optimize the single action prediction training objective. Empirically, on R2R, R4R, and RxR datasets, the proposed approach improves the baseline method by around 1-2% in success rate.","1. Good performance on multiple VLN datasets. 
2. The paper is well-written and easy to follow.
3. Reasonable approach to improve the performance.","1. The data collection pipeline is not new. In PREVALENT, they traverse through all the possible paths (with length ranging from 3-6) in the seen environments. Furthermore, PREVALENT generates synthetic instructions for all those unannotated paths. Given the data collection process mentioned in the paper, I don't think any new paths are actually created, thus I'm not sure why MPM helps reduce the data scarcity problem in VLN. Besides, no stats about the collected paths are given in the paper. 
2. Simply adding a pre-training/fine-tuning objective is not novel enough. Similar ideas that optimize the self-supervised image-level loss are explored in ""Improving Vision-and-Language Navigation by Generating Future-View Image Semantics"", where they propose Masked Trajectory Modeling (MTM) that masks out views in a trajectory. 
3. HAMT is a sota approach in 2021. To demonstrate the effectiveness of the proposed approach (i.e., could generalize to different model architecture/training strategy), at least MPM should be applied to ""Think Global, Act Local: Dual-Scale Graph Transformer for Vision-and-Language Navigation"" (CVPR 2022).",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,,,1691106993133,,,EMNLP/2023/Conference,2UJvVc8gnP,"['EMNLP/2023/Conference/Submission1127/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461085632,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1127/Reviewer_otNW']",2UJvVc8gnP,['EMNLP/2023/Conference/Submission1127/Reviewer_otNW'],1691106993133,1701461085632,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1127/Reviewer_otNW']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Improving Vision-and-Language Navigation by Generating Future-View Image Semantics"" (which proposes Masked Trajectory Modeling, MTM)
2. ""Think Global, Act Local: Dual-Scale Graph Transformer for Vision-and-Language Navigation"" (CVPR 2022)

These citations might be necessary because the reviewer believes that the proposed approach (Masked Path Modeling, MPM) is not novel enough and has similarities with existing work, such as MTM. Additionally, the reviewer suggests that applying MPM to a state-of-the-art approach like ""Think Global, Act Local: Dual-Scale Graph Transformer for Vision-and-Language Navigation"" could demonstrate its effectiveness and ability to generalize to different model architectures and training strategies.",1,2022,"Think Global, Act Local: Dual-Scale Graph Transformer for Vision-and-Language Navigation"
U3PfniQiQe,"This paper introduces masked path modeling (MPM) for vision-and-language navigation (VLN) to address the data scarcity limitation. They first explore and collect navigation paths in the downstream environments. MPM then learns to reconstruct the executed actions of the masked subpath. Experimental results show that MPM can effectively improve the navigator performance across all R2R, R4R, and RxR datasets.
","+ This paper is well-written and easy to follow.
+ This MPM framework is simple yet effective and seems to be model agnostic with better scalability.","+ The idea of mask-then-reconstruct for VLN has first been proposed by (Hao et al., 2020), where the difference is to predict missing language tokens or actions of the navigation. I am wondering if this achieves the EMNLP novelty bar.
+ For MPM learning, it only considers the visual observation of the navigation path. How to improve the crucial cross-modal perception in this process? Does MPM just understand the action transformation but not the vision-and-language interaction?
+ In L584, they mention that previous methods are limited by the size of VLN data. However, MPM also only collects randomly-sampled paths in the downstream environment. How does MPM extend to a large-scale setting as they claim?",Please see Reasons to Reject,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[ECCV'20] Counterfactual Vision-and-Language Navigation via Adversarial Path Sampler, which also considers data augmentation for VLN.

",,1691383363426,,,EMNLP/2023/Conference,2UJvVc8gnP,"['EMNLP/2023/Conference/Submission1127/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461085548,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1127/Reviewer_Sdnd']",2UJvVc8gnP,['EMNLP/2023/Conference/Submission1127/Reviewer_Sdnd'],1691383363426,1701461085548,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1127/Reviewer_Sdnd']","Yes

The suggested paper is: [ECCV'20] Counterfactual Vision-and-Language Navigation via Adversarial Path Sampler.

This citation might be necessary because the reviewer mentions it as a relevant work that also considers data augmentation for Vision-and-Language Navigation (VLN), which is the topic of the submitted paper. The reviewer includes it in the ""Missing_References"" section, implying that the authors should be aware of this work and potentially cite it to provide a more comprehensive overview of the field.",1,2020,Counterfactual Vision-and-Language Navigation via Adversarial Path Sampler
EVAeRx2AC0,"This work introduces a context-aware machine translation metric, Cont-COMET, built on top of the trained COMET metric used for machine translation evaluation, which has been shown to have better correlations with human judgments compared to string-based matching metrics such as BLEU. In this work, the authors experiment with including previous and next context sentences for both source and target reference sentences, using reference sentences as context for the machine-generated hypothesis translation. All of these are encoded using XLM-R and trained to produce a score, in the same way as COMET. Experiments are conducted using cosine similarity to select relevant sentences from the context since XLM-R can only process up to 512 tokens. Experiments demonstrate some improvements in scores compared to COMET.","As motivated by the paper, more work needs to be done towards having reliable context-aware metrics. So far there have been few model-based trained metrics that have incorporated context, so this work is a welcome step in that direction.","- The improvements demonstrated are marginal, and their advantage over COMET is unclear. COMET still outperforms their proposed metric in several cases. Statistical significance tests are not conducted, so it is unclear if the improvements are significant as claimed.
- Another claim that does not seem to have sufficient backing is that using a similar number of sentences for preceding and subsequent context performs the best. This conclusion seems to be based on a couple of improved results in Table 1, one of which is marginal, while all the rest of the results seem to show the contrary.
- One of the assumptions made for context selection is that sentences similar to the current sentence (based on cosine similarity) are more relevant context. There is a good chance this is unlikely, since useful context often contains information that is not present in the current sentence (e.g. antecedents of pronouns) and is therefore less likely to be similar. 
- There is no language-wise breakdown of results in the paper. It is likely that the usefulness of context differs depending on the source and target languages and it seems to be a fairly big oversight to have a discussion of this missing.",- What kind of sentences were selected as context when cosine similarity was used? Was any manual analysis conducted to check the usefulness of the context?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation (Jiang et al., NAACL 2022)","- It is claimed in the paper that WMT introduced context information for human judgments in 2019. This may need to be double-checked, since the document-level MT task was introduced in 2019; annotators were able to see context during evaluation much earlier.
- It is not clear that the results in Table 1 are for untrained COMET with additional context sentences. This needs to be mentioned more explicitly before presenting the results.
- Please also mention the language pairs in the test data in the main paper and perform statistical significance tests. Having a language-pair-wise breakdown will also help analysis.
- ll 206-208 mention that Vernikos et al (2022) do not provide comprehensive conclusions. It would be helpful to include what conclusions they do provide, and why this paper is more comprehensive in comparison.
- ll 029 were early proposed -> were earlier proposed
- 043-044 while the related document content was often involved in practice -> in which practice? Unclear what this sentence means.",1691193838136,,,EMNLP/2023/Conference,2O39az85g6,"['EMNLP/2023/Conference/Submission5603/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461356241,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission5603/Reviewer_7yHH']",2O39az85g6,['EMNLP/2023/Conference/Submission5603/Reviewer_7yHH'],1691193838136,1701461356241,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5603/Reviewer_7yHH']","Yes

The suggested paper not cited in the manuscript is:
- BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation (Jiang et al., NAACL 2022)

This citation might be necessary because the reviewer mentions it as a missing reference, implying that the work presented in the paper is related to the topic of document-level machine translation and context-aware metrics, which is also the focus of the submitted manuscript. The reviewer suggests that the authors should be aware of and cite relevant existing work in the field to provide a more comprehensive overview of the current state of research.",1,2022,BlonDe An Automatic Evaluation Metric for Document-level Machine Translation
ISCzRoNATK,"This paper proposed a method for automatically detecting and filtering out noisy examples from synthetically constructed data from commonsense knowledge graphs. Inspired by the idea of datasets cartography (Swayamdipta et al. 20), the authors proposed to detect mislabeled examples, false-negative examples and easy-to-learn distractors from the synthetic data, using the confidence measure of the model along the training process. The final models are trained on the dataset after filtering. The authors evaluated the models on five commonsense reasoning datasets, and the results show that the proposed method greatly improved the performance compared to no filtering or filtering using the method from  Swayamdipta et al. 20.",The idea is very intuitive that filtering out noisy data from synthetic data potentially improves the model’s performance. The experimental results also show significant gains across five benchmarks by training on less but higher quality data.,"Important technical details are not clearly explained, which weakens the claims made in this paper, in particular: 
1. I don’t quite understand the motivation for Equation 2 and Equation 3. For equation 2, compare to Swayamdipta et al. 20, you just normalize over one distractor instead of all distractors. Intuitively, this just increased the probability of the correct answer and nothing more. For equation 3, if the correct answer has a high probability, the distractors will have low probabilities, thus these two measures (confidence for the correct answer and distractor) are strongly correlated or basically the same. Why don’t you use the original definition of confidence from Swayamdipta et al. 20? If you stick with the original training dynamics, and apply the easy distractor removal, mislabeled removal and false negative removal, would the model achieve similar performance? 
2. For mislabeled, false-neg and mixed strategy, do you apply these methods on the remaining 33% of data to further filter out low-quality instances? If so, how much data do you filter out for each of the strategies? You should provide a table of statistics for this. 

I am willing to increase my scores if my concerns and questions are addressed.","1. In line 714, it should be resulting lower confidence in ground-truth answer right?
2. In line 722, if you’re using the second-lowest logits, then it should be the harder distractor? 
3. I didn’t understand how is figure 1 computed? If you’re plotting the difference between the logits, what does softmax refer to? Your logits did not come from softmax?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"There is a previous paper that studied training dynamics on synthetic QA pairs from CSKBs, you should consider discussing it in your related work. 

A Study of Zero-shot Adaptation with Commonsense Knowledge
J Zhang, F Ilievski, K Ma, J Francis, A Oltramari - Automated Knowledge Base Construction (AKBC), 2022",,1690417189998,,,EMNLP/2023/Conference,2MXXycs2T6,"['EMNLP/2023/Conference/Submission1853/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461127314,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1853/Reviewer_ukoU']",2MXXycs2T6,['EMNLP/2023/Conference/Submission1853/Reviewer_ukoU'],1690417189998,1701461127314,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1853/Reviewer_ukoU']","Yes

The suggested paper not cited in the manuscript is:
""A Study of Zero-shot Adaptation with Commonsense Knowledge"" by J Zhang, F Ilievski, K Ma, J Francis, A Oltramari, published in Automated Knowledge Base Construction (AKBC), 2022.

This citation might be necessary because the reviewer mentions that there is a previous paper that studied training dynamics on synthetic QA pairs from CSKBs, which is relevant to the topic of the submitted manuscript. The reviewer suggests that the authors should consider discussing this paper in their related work, implying that it could provide valuable context or insights that are not currently included in the manuscript.",1,2022,A Study of Zero-shot Adaptation with Commonsense Knowledge
osx5Ri6afz,"This paper introduces a new paradigm of guideline learning (or more specifically guideline refinement) that can improve in-context learning. Given a few training examples and a guideline consisting of a set of rules, the model will update the rules for wrongly-predicted examples. Then the updated rules will be added into the context for model inference. 
The paper evaluates this framework on the classification component of event extraction and relation extraction tasks and shows that it improves performance.","- Overall, this guideline learning framework is interesting and meaningful as it does not require fine-tuning the entire model. Although the paper only applies the idea to EE and RE, the idea is transferrable to a wide range of tasks.","- **This paper does not include the mandatory Limitation section, and thus should be desk-rejected.** 
- The idea is related to instruction learning/induction, which aims to automatically generate or rewrite the instruction of the task. The guideline in this paper can also be viewed as part of the task instruction, the difference being only a subset of the rules are used for each instance. 
- Few datasets have rule-based guidelines (this might be the reason why not the paper does not include results for more popular RE datasets such as TACRED / DocRED), which limits the application of the framework. 
- Since some of the experiment settings (EE-GL-r, RE-GL-r) rely on randomly sampling in-context examples, the variance should be reported for results.","A. Have the authors thought about the faithfulness of the rules returned by the Reasoner? In Lines 232-233, it is mentioned that R* is the set of rules that ""the LLM refers to as being helpful during reasoning"". Are these rules truly relevant to the instance and are they actually used by the LLM? 

B. ~~Because the rule scoring and rule update rely on the true label y, instance selection can only be done over the instances that have labels.  This might be ok when compared with fully-supervised models, but the authors should note that this is not true few-shot learning (where the assumption is that few-shot examples are provided as input and the model does not have access to any other labels)~~

C. The generalizer is only mentioned in Section 4.3 and not in the methodology part (Section 3).  Can you provide some intuition on why the generalizer is needed or show some results that ablate this component? If the generalizer is critical for the model to work, I'd suggest moving it into Section 3.

D. In Table 2, it seems that guideline learning is not helpful for multi-event documents (75.0 F1 for ICL vs 74.4 F1 for GL), can the authors explain this behavior? 

E. Can the authors think of a way to apply this framework to datasets that do not have readily available guidelines?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Missing related work on instruction tuning, especially using the model to generate instructions, which is similar to generating guidelines.
To list a few: 
- Honovich, Or et al. “Instruction Induction: From Few Examples to Natural Language Task Descriptions.” Annual Meeting of the Association for Computational Linguistics (2022).
- Ye, Seonghyeon et al. “Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners.” ICLR (2023)","Presentation improvements:
- - It would be beneficial to accompany Section 3 with some examples of the prompt and input/output. The current writing is very abstract. For example, can you give an example of a guideline before and after the update? 


Typos: 
- Line 48: fine-tuning LLM faces with the .... issues  -> fine-tuning LLM faces ... issues
- Line 57: promising performances -> promising performance 

- Line 95: different with -> different from 
- Line 374: as all the event is related to -> as all event types are related to 
- Line 377: extra ""we apply guide""",1691272121954,,,EMNLP/2023/Conference,2KTvN4Edvl,"['EMNLP/2023/Conference/Submission3546/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461236392,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3546/Reviewer_6ZCo']",2KTvN4Edvl,['EMNLP/2023/Conference/Submission3546/Reviewer_6ZCo'],1691272121954,1701461236392,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3546/Reviewer_6ZCo']","Yes

The suggested papers or references not cited in the manuscript are:
1. Honovich, Or et al. “Instruction Induction: From Few Examples to Natural Language Task Descriptions.” Annual Meeting of the Association for Computational Linguistics (2022).
2. Ye, Seonghyeon et al. “Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners.” ICLR (2023)

These citations might be necessary because the reviewer mentions that the idea presented in the paper is related to instruction learning/induction, which aims to automatically generate or rewrite the instruction of the task. The reviewer suggests that the guideline in the paper can also be viewed as part of the task instruction, and therefore, citing related work on instruction tuning, especially using the model to generate instructions, would be relevant. This would provide a more comprehensive overview of the existing research in the field and help to contextualize the contributions of the paper.",1,"2022, 2023",Instruction Induction: From Few Examples to Natural Language Task Descriptions
osx5Ri6afz,"This paper introduces a new paradigm of guideline learning (or more specifically guideline refinement) that can improve in-context learning. Given a few training examples and a guideline consisting of a set of rules, the model will update the rules for wrongly-predicted examples. Then the updated rules will be added into the context for model inference. 
The paper evaluates this framework on the classification component of event extraction and relation extraction tasks and shows that it improves performance.","- Overall, this guideline learning framework is interesting and meaningful as it does not require fine-tuning the entire model. Although the paper only applies the idea to EE and RE, the idea is transferrable to a wide range of tasks.","- **This paper does not include the mandatory Limitation section, and thus should be desk-rejected.** 
- The idea is related to instruction learning/induction, which aims to automatically generate or rewrite the instruction of the task. The guideline in this paper can also be viewed as part of the task instruction, the difference being only a subset of the rules are used for each instance. 
- Few datasets have rule-based guidelines (this might be the reason why not the paper does not include results for more popular RE datasets such as TACRED / DocRED), which limits the application of the framework. 
- Since some of the experiment settings (EE-GL-r, RE-GL-r) rely on randomly sampling in-context examples, the variance should be reported for results.","A. Have the authors thought about the faithfulness of the rules returned by the Reasoner? In Lines 232-233, it is mentioned that R* is the set of rules that ""the LLM refers to as being helpful during reasoning"". Are these rules truly relevant to the instance and are they actually used by the LLM? 

B. ~~Because the rule scoring and rule update rely on the true label y, instance selection can only be done over the instances that have labels.  This might be ok when compared with fully-supervised models, but the authors should note that this is not true few-shot learning (where the assumption is that few-shot examples are provided as input and the model does not have access to any other labels)~~

C. The generalizer is only mentioned in Section 4.3 and not in the methodology part (Section 3).  Can you provide some intuition on why the generalizer is needed or show some results that ablate this component? If the generalizer is critical for the model to work, I'd suggest moving it into Section 3.

D. In Table 2, it seems that guideline learning is not helpful for multi-event documents (75.0 F1 for ICL vs 74.4 F1 for GL), can the authors explain this behavior? 

E. Can the authors think of a way to apply this framework to datasets that do not have readily available guidelines?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Missing related work on instruction tuning, especially using the model to generate instructions, which is similar to generating guidelines.
To list a few: 
- Honovich, Or et al. “Instruction Induction: From Few Examples to Natural Language Task Descriptions.” Annual Meeting of the Association for Computational Linguistics (2022).
- Ye, Seonghyeon et al. “Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners.” ICLR (2023)","Presentation improvements:
- - It would be beneficial to accompany Section 3 with some examples of the prompt and input/output. The current writing is very abstract. For example, can you give an example of a guideline before and after the update? 


Typos: 
- Line 48: fine-tuning LLM faces with the .... issues  -> fine-tuning LLM faces ... issues
- Line 57: promising performances -> promising performance 

- Line 95: different with -> different from 
- Line 374: as all the event is related to -> as all event types are related to 
- Line 377: extra ""we apply guide""",1691272121954,,,EMNLP/2023/Conference,2KTvN4Edvl,"['EMNLP/2023/Conference/Submission3546/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461236392,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3546/Reviewer_6ZCo']",2KTvN4Edvl,['EMNLP/2023/Conference/Submission3546/Reviewer_6ZCo'],1691272121954,1701461236392,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3546/Reviewer_6ZCo']","Yes

The suggested papers or references not cited in the manuscript are:
1. Honovich, Or et al. “Instruction Induction: From Few Examples to Natural Language Task Descriptions.” Annual Meeting of the Association for Computational Linguistics (2022).
2. Ye, Seonghyeon et al. “Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners.” ICLR (2023)

These citations might be necessary because the reviewer mentions that the idea presented in the paper is related to instruction learning/induction, which aims to automatically generate or rewrite the instruction of the task. The reviewer suggests that the guideline in the paper can also be viewed as part of the task instruction, and therefore, citing related work on instruction tuning, especially using the model to generate instructions, would be relevant. This would provide a more comprehensive overview of the existing research in the field and help to contextualize the contributions of the paper.",1,"2022, 2023",Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners
ZyhdpawD2C,"This paper proposes an improved version of UIE, the Unified Structure Generation approach. It replaces the structural schema instructor in UIE with an explicit schema instructor where not only the schema terms but also their structures are provided. This paper proposes the idea of recursively extracting aspects instead of doing so separately. The results are significantly improved on most of the evaluated benchmarks.","1. The improvements are significaint.
2. The writing is fluent, making the paper easy to read.
3. The idea of recursive extraction is novel in joint extraction methods with matrix-based tagging schemes.","1. The reasons behind the results improvement might require further discussion. (Please refer to Question B)
2. The requirement for explicit schema may limit the practicality of this method. (Please refer to Question C and D)","Question A: Does the premise for judging how to perform recursive extraction, the Explicit Schema Instructor, implies prior knowledge of all possible schemas?

Question B: Is the explicit schema information also provide to compared methods?  Could the author provide a more clearly explanation about how much improvement is solely due to a more complete schema, and how much is attributed to the advancement in recursive extraction techniques?

Question C: Are there any instance of incomplete schemas in practice? For example, the schema requires the extraction result to have four aspects, but only three of them appear in the utterance. Will the proposed method work correctly in such cases?

Question D: CasRel and TPlinker (please refer to missing references) proposed a matrix-based tagging scheme partly to handle the phenomenon of overlapping relations in the extraction task. (For instance, in the sentence ""A and B boarded the plane together,"" two facts can be extracted: ""A boarded the plane"" and ""B boarded the plane,"" resulting in the plane being used twice). This work strictly follows the structure of an explicit schema for extraction. Does it also have the capability to handle overlapping relations?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"CasRel[1] proposed using consistent tagging schemes to model the extraction of entities and relationships, and TPlinker[2] extended the idea to a unified matrix representation. CasRel was not cited in this paper, and while TPlinker was mentioned at line 269, no corresponding description was provided. Although they did not mention the extraction of multi-granular structures like events, the idea of transforming multi-granular structures into combinations of binary relations has been discussed in NLP research for a long time. Therefore, I suggest introducing CasRel and TPlinker as related works in joint extraction in Section 2.

[1] Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. 2020. A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1476–1488, Online. Association for Computational Linguistics.

[2] Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu, and Limin Sun. 2020. Tplinker: Single-stage joint extraction of entities and relations through token pair linking.","1. I personally think that the discontinuous vertical lines in Table 1 are not proper formatting. (You can find relevant discussions in the documentation of booktab.) If the author indeed wants to use booktab, please remove the vertical lines or hack it to make the lines continuous.",1689934627053,,,EMNLP/2023/Conference,2IfYI3dkX7,"['EMNLP/2023/Conference/Submission644/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461046214,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission644/Reviewer_SsUQ']",2IfYI3dkX7,['EMNLP/2023/Conference/Submission644/Reviewer_SsUQ'],1689934627053,1701461046214,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission644/Reviewer_SsUQ']","Yes

The suggested papers or references not cited in the manuscript are:
1. CasRel by Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang (2020) - ""A Novel Cascade Binary Tagging Framework for Relational Triple Extraction""
2. TPlinker by Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu, and Limin Sun (2020) - ""Tplinker: Single-stage joint extraction of entities and relations through token pair linking""

These citations might be necessary because the reviewer believes that the ideas presented in these papers are relevant to the topic of joint extraction and matrix-based tagging schemes, and could provide a more comprehensive understanding of the research area. Specifically, the reviewer mentions that CasRel and TPlinker proposed a matrix-based tagging scheme to handle overlapping relations, which is related to the topic of the submitted manuscript. By citing these papers, the authors could provide a more thorough discussion of related work and demonstrate a clearer understanding of the research context.",1,"2020, 2020",A Novel Cascade Binary Tagging Framework for Relational Triple Extraction 
ZyhdpawD2C,"This paper proposes an improved version of UIE, the Unified Structure Generation approach. It replaces the structural schema instructor in UIE with an explicit schema instructor where not only the schema terms but also their structures are provided. This paper proposes the idea of recursively extracting aspects instead of doing so separately. The results are significantly improved on most of the evaluated benchmarks.","1. The improvements are significaint.
2. The writing is fluent, making the paper easy to read.
3. The idea of recursive extraction is novel in joint extraction methods with matrix-based tagging schemes.","1. The reasons behind the results improvement might require further discussion. (Please refer to Question B)
2. The requirement for explicit schema may limit the practicality of this method. (Please refer to Question C and D)","Question A: Does the premise for judging how to perform recursive extraction, the Explicit Schema Instructor, implies prior knowledge of all possible schemas?

Question B: Is the explicit schema information also provide to compared methods?  Could the author provide a more clearly explanation about how much improvement is solely due to a more complete schema, and how much is attributed to the advancement in recursive extraction techniques?

Question C: Are there any instance of incomplete schemas in practice? For example, the schema requires the extraction result to have four aspects, but only three of them appear in the utterance. Will the proposed method work correctly in such cases?

Question D: CasRel and TPlinker (please refer to missing references) proposed a matrix-based tagging scheme partly to handle the phenomenon of overlapping relations in the extraction task. (For instance, in the sentence ""A and B boarded the plane together,"" two facts can be extracted: ""A boarded the plane"" and ""B boarded the plane,"" resulting in the plane being used twice). This work strictly follows the structure of an explicit schema for extraction. Does it also have the capability to handle overlapping relations?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"CasRel[1] proposed using consistent tagging schemes to model the extraction of entities and relationships, and TPlinker[2] extended the idea to a unified matrix representation. CasRel was not cited in this paper, and while TPlinker was mentioned at line 269, no corresponding description was provided. Although they did not mention the extraction of multi-granular structures like events, the idea of transforming multi-granular structures into combinations of binary relations has been discussed in NLP research for a long time. Therefore, I suggest introducing CasRel and TPlinker as related works in joint extraction in Section 2.

[1] Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. 2020. A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1476–1488, Online. Association for Computational Linguistics.

[2] Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu, and Limin Sun. 2020. Tplinker: Single-stage joint extraction of entities and relations through token pair linking.","1. I personally think that the discontinuous vertical lines in Table 1 are not proper formatting. (You can find relevant discussions in the documentation of booktab.) If the author indeed wants to use booktab, please remove the vertical lines or hack it to make the lines continuous.",1689934627053,,,EMNLP/2023/Conference,2IfYI3dkX7,"['EMNLP/2023/Conference/Submission644/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461046214,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission644/Reviewer_SsUQ']",2IfYI3dkX7,['EMNLP/2023/Conference/Submission644/Reviewer_SsUQ'],1689934627053,1701461046214,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission644/Reviewer_SsUQ']","Yes

The suggested papers or references not cited in the manuscript are:
1. CasRel by Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang (2020) - ""A Novel Cascade Binary Tagging Framework for Relational Triple Extraction""
2. TPlinker by Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu, and Limin Sun (2020) - ""Tplinker: Single-stage joint extraction of entities and relations through token pair linking""

These citations might be necessary because the reviewer believes that the ideas presented in these papers are relevant to the topic of joint extraction and matrix-based tagging schemes, and could provide a more comprehensive understanding of the research area. Specifically, the reviewer mentions that CasRel and TPlinker proposed a matrix-based tagging scheme to handle overlapping relations, which is related to the topic of the submitted manuscript. By citing these papers, the authors could provide a more thorough discussion of related work and demonstrate a clearer understanding of the research context.",1,"2020, 2020",Tplinker Single-stage joint extraction of entities and relations through token pair linking
jaVUklrno0,"This paper focuses on Open Information Extraction (OIE), which involves extracting structured tuples from unstructured text without a particular ontology. The traditional token-level extraction approach faces challenges in determining the appropriate text spans. To address this, the paper introduces an alternative method called Sentence as Chunk sequence (SaC), where chunks are recognized as relations and arguments. The paper evaluates different choices of chunks for SaC and proposes Chunk-OIE, an end-to-end tagging-based model for sentence chunking and tuple extraction. ",The presentation of the overall method is clear. The method is backed by a series of experiments and ablations. It might be beneficial for the community to see how to incorporate syntactic annotations from different datasets into an open IE model. It can also be seen as a practice of integrating [1] and [2] kind of model with such syntactic information.,"1. The dependency trees introduced in the paper are inherently similar to trees generated from dependency parsing while the authors did not discuss any relations between these two tasks/formulations. 

2. Despite the novel intermediate representation claimed by the authors, the model architecture is basically the same  as (or similar to) prior work [1] and [2]. From the modeling perspective, the contribution is limited.

3. It is not (directly) clear from the paper what are the semantics behind chunks. To some extent, these spans seem to be predicates and entity mentions. While most choices of chunks mentioned are defined at syntax level, it is hard to distinguish the work from prior work using syntactic structures for IE, e.g. [3].

4. The choice of span finding  method should be justified. Potential through references like  [4] or other ablations.
",,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, Relation, and Event Extraction with Contextualized Span Representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5784–5789, Hong Kong, China. Association for Computational Linguistics.

[2] Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2020. Joint Modeling of Arguments for Event Understanding. In Proceedings of the First Workshop on Computational Approaches to Discourse, pages 96–101, Online. Association for Computational Linguistics.

[3] Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li. 2022. Syntactic Multi-view Learning for Open Information Extraction. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4072–4083, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

[4] Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2022. An Empirical Study on Finding Spans. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3976–3983, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
",,1691235777692,,,EMNLP/2023/Conference,2FDty4mLqP,"['EMNLP/2023/Conference/Submission2960/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461200081,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2960/Reviewer_zi3N']",2FDty4mLqP,['EMNLP/2023/Conference/Submission2960/Reviewer_zi3N'],1691235777692,1701461200081,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2960/Reviewer_zi3N']","Yes

The suggested papers or references not cited in the manuscript are:
1. David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, Relation, and Event Extraction with Contextualized Span Representations.
2. Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2020. Joint Modeling of Arguments for Event Understanding.
3. Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li. 2022. Syntactic Multi-view Learning for Open Information Extraction.
4. Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2022. An Empirical Study on Finding Spans.

These citations might be necessary because the reviewer believes that:
- The model architecture is similar to prior work ([1] and [2]), so citing these papers could provide context and acknowledge the similarities.
- The use of syntactic structures for IE is not novel, and citing [3] could help distinguish the current work from prior research.
- The choice of span finding method should be justified through references like [4] or other ablations, indicating that the reviewer thinks the authors should provide more evidence or comparison to existing methods for finding spans.",1,"2019, 2020, 2022, 2022","Entity, Relation, and Event Extraction with Contextualized Span Representations "
jaVUklrno0,"This paper focuses on Open Information Extraction (OIE), which involves extracting structured tuples from unstructured text without a particular ontology. The traditional token-level extraction approach faces challenges in determining the appropriate text spans. To address this, the paper introduces an alternative method called Sentence as Chunk sequence (SaC), where chunks are recognized as relations and arguments. The paper evaluates different choices of chunks for SaC and proposes Chunk-OIE, an end-to-end tagging-based model for sentence chunking and tuple extraction. ",The presentation of the overall method is clear. The method is backed by a series of experiments and ablations. It might be beneficial for the community to see how to incorporate syntactic annotations from different datasets into an open IE model. It can also be seen as a practice of integrating [1] and [2] kind of model with such syntactic information.,"1. The dependency trees introduced in the paper are inherently similar to trees generated from dependency parsing while the authors did not discuss any relations between these two tasks/formulations. 

2. Despite the novel intermediate representation claimed by the authors, the model architecture is basically the same  as (or similar to) prior work [1] and [2]. From the modeling perspective, the contribution is limited.

3. It is not (directly) clear from the paper what are the semantics behind chunks. To some extent, these spans seem to be predicates and entity mentions. While most choices of chunks mentioned are defined at syntax level, it is hard to distinguish the work from prior work using syntactic structures for IE, e.g. [3].

4. The choice of span finding  method should be justified. Potential through references like  [4] or other ablations.
",,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, Relation, and Event Extraction with Contextualized Span Representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5784–5789, Hong Kong, China. Association for Computational Linguistics.

[2] Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2020. Joint Modeling of Arguments for Event Understanding. In Proceedings of the First Workshop on Computational Approaches to Discourse, pages 96–101, Online. Association for Computational Linguistics.

[3] Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li. 2022. Syntactic Multi-view Learning for Open Information Extraction. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4072–4083, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

[4] Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2022. An Empirical Study on Finding Spans. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3976–3983, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
",,1691235777692,,,EMNLP/2023/Conference,2FDty4mLqP,"['EMNLP/2023/Conference/Submission2960/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461200081,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2960/Reviewer_zi3N']",2FDty4mLqP,['EMNLP/2023/Conference/Submission2960/Reviewer_zi3N'],1691235777692,1701461200081,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2960/Reviewer_zi3N']","Yes

The suggested papers or references not cited in the manuscript are:
1. David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, Relation, and Event Extraction with Contextualized Span Representations.
2. Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2020. Joint Modeling of Arguments for Event Understanding.
3. Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li. 2022. Syntactic Multi-view Learning for Open Information Extraction.
4. Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2022. An Empirical Study on Finding Spans.

These citations might be necessary because the reviewer believes that:
- The model architecture is similar to prior work ([1] and [2]), so citing these papers could provide context and acknowledge the similarities.
- The use of syntactic structures for IE is not novel, and citing [3] could help distinguish the current work from prior research.
- The choice of span finding method should be justified through references like [4] or other ablations, indicating that the reviewer thinks the authors should provide more evidence or comparison to existing methods for finding spans.",1,"2019, 2020, 2022, 2022",Joint Modeling of Arguments for Event Understanding 
jaVUklrno0,"This paper focuses on Open Information Extraction (OIE), which involves extracting structured tuples from unstructured text without a particular ontology. The traditional token-level extraction approach faces challenges in determining the appropriate text spans. To address this, the paper introduces an alternative method called Sentence as Chunk sequence (SaC), where chunks are recognized as relations and arguments. The paper evaluates different choices of chunks for SaC and proposes Chunk-OIE, an end-to-end tagging-based model for sentence chunking and tuple extraction. ",The presentation of the overall method is clear. The method is backed by a series of experiments and ablations. It might be beneficial for the community to see how to incorporate syntactic annotations from different datasets into an open IE model. It can also be seen as a practice of integrating [1] and [2] kind of model with such syntactic information.,"1. The dependency trees introduced in the paper are inherently similar to trees generated from dependency parsing while the authors did not discuss any relations between these two tasks/formulations. 

2. Despite the novel intermediate representation claimed by the authors, the model architecture is basically the same  as (or similar to) prior work [1] and [2]. From the modeling perspective, the contribution is limited.

3. It is not (directly) clear from the paper what are the semantics behind chunks. To some extent, these spans seem to be predicates and entity mentions. While most choices of chunks mentioned are defined at syntax level, it is hard to distinguish the work from prior work using syntactic structures for IE, e.g. [3].

4. The choice of span finding  method should be justified. Potential through references like  [4] or other ablations.
",,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, Relation, and Event Extraction with Contextualized Span Representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5784–5789, Hong Kong, China. Association for Computational Linguistics.

[2] Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2020. Joint Modeling of Arguments for Event Understanding. In Proceedings of the First Workshop on Computational Approaches to Discourse, pages 96–101, Online. Association for Computational Linguistics.

[3] Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li. 2022. Syntactic Multi-view Learning for Open Information Extraction. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4072–4083, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

[4] Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2022. An Empirical Study on Finding Spans. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3976–3983, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
",,1691235777692,,,EMNLP/2023/Conference,2FDty4mLqP,"['EMNLP/2023/Conference/Submission2960/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461200081,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2960/Reviewer_zi3N']",2FDty4mLqP,['EMNLP/2023/Conference/Submission2960/Reviewer_zi3N'],1691235777692,1701461200081,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2960/Reviewer_zi3N']","Yes

The suggested papers or references not cited in the manuscript are:
1. David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, Relation, and Event Extraction with Contextualized Span Representations.
2. Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2020. Joint Modeling of Arguments for Event Understanding.
3. Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li. 2022. Syntactic Multi-view Learning for Open Information Extraction.
4. Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2022. An Empirical Study on Finding Spans.

These citations might be necessary because the reviewer believes that:
- The model architecture is similar to prior work ([1] and [2]), so citing these papers could provide context and acknowledge the similarities.
- The use of syntactic structures for IE is not novel, and citing [3] could help distinguish the current work from prior research.
- The choice of span finding method should be justified through references like [4] or other ablations, indicating that the reviewer thinks the authors should provide more evidence or comparison to existing methods for finding spans.",1,"2019, 2020, 2022, 2022",Syntactic Multi-view Learning for Open Information Extraction 
jaVUklrno0,"This paper focuses on Open Information Extraction (OIE), which involves extracting structured tuples from unstructured text without a particular ontology. The traditional token-level extraction approach faces challenges in determining the appropriate text spans. To address this, the paper introduces an alternative method called Sentence as Chunk sequence (SaC), where chunks are recognized as relations and arguments. The paper evaluates different choices of chunks for SaC and proposes Chunk-OIE, an end-to-end tagging-based model for sentence chunking and tuple extraction. ",The presentation of the overall method is clear. The method is backed by a series of experiments and ablations. It might be beneficial for the community to see how to incorporate syntactic annotations from different datasets into an open IE model. It can also be seen as a practice of integrating [1] and [2] kind of model with such syntactic information.,"1. The dependency trees introduced in the paper are inherently similar to trees generated from dependency parsing while the authors did not discuss any relations between these two tasks/formulations. 

2. Despite the novel intermediate representation claimed by the authors, the model architecture is basically the same  as (or similar to) prior work [1] and [2]. From the modeling perspective, the contribution is limited.

3. It is not (directly) clear from the paper what are the semantics behind chunks. To some extent, these spans seem to be predicates and entity mentions. While most choices of chunks mentioned are defined at syntax level, it is hard to distinguish the work from prior work using syntactic structures for IE, e.g. [3].

4. The choice of span finding  method should be justified. Potential through references like  [4] or other ablations.
",,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, Relation, and Event Extraction with Contextualized Span Representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5784–5789, Hong Kong, China. Association for Computational Linguistics.

[2] Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2020. Joint Modeling of Arguments for Event Understanding. In Proceedings of the First Workshop on Computational Approaches to Discourse, pages 96–101, Online. Association for Computational Linguistics.

[3] Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li. 2022. Syntactic Multi-view Learning for Open Information Extraction. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4072–4083, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

[4] Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2022. An Empirical Study on Finding Spans. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3976–3983, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
",,1691235777692,,,EMNLP/2023/Conference,2FDty4mLqP,"['EMNLP/2023/Conference/Submission2960/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461200081,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2960/Reviewer_zi3N']",2FDty4mLqP,['EMNLP/2023/Conference/Submission2960/Reviewer_zi3N'],1691235777692,1701461200081,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2960/Reviewer_zi3N']","Yes

The suggested papers or references not cited in the manuscript are:
1. David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, Relation, and Event Extraction with Contextualized Span Representations.
2. Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2020. Joint Modeling of Arguments for Event Understanding.
3. Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li. 2022. Syntactic Multi-view Learning for Open Information Extraction.
4. Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, and Benjamin Van Durme. 2022. An Empirical Study on Finding Spans.

These citations might be necessary because the reviewer believes that:
- The model architecture is similar to prior work ([1] and [2]), so citing these papers could provide context and acknowledge the similarities.
- The use of syntactic structures for IE is not novel, and citing [3] could help distinguish the current work from prior research.
- The choice of span finding method should be justified through references like [4] or other ablations, indicating that the reviewer thinks the authors should provide more evidence or comparison to existing methods for finding spans.",1,"2019, 2020, 2022, 2022",An Empirical Study on Finding Spans
OoPqTe3KJf,"This paper proposes a new approach for Open Information Extraction (OIE) called Sentence as Chunk sequence (SaC) and an end-to-end BERT-based model called Chunk-OIE for sentence chunking and tuple extraction on top of SaC. The main contribution of this paper is to demonstrate that SaC as an intermediate layer for OIE tuple extraction has better properties than the traditional approach of using a sentence as a token sequence. Additionally, the paper evaluates various chunk choices as SaC and performs a statistical study to understand how these chunks align with OIE gold tuple spans and how boundary alignment impacts overall OIE performance. Finally, the Chunk-OIE model achieves state-of-the-art results on multiple OIE datasets, demonstrating the effectiveness of the SaC approach. The paper also provides publicly available software and pre-trained models for use by the community.

","This paper proposes a new approach for Open Information Extraction (OIE) called Sentence as Chunk sequence (SaC), which is argued to have better properties for OIE than the traditional approach of using a sentence as a token sequence. The proposed approach achieves state-of-the-art performance on OIE datasets.","1. Limited experimentation with non-English languages: The evaluation of the proposed approach is limited to English datasets, and it is unclear how well the approach would perform in non-English languages or low-resource settings.
2. Limited discussion of related work: An important and similar related work UIE [1] is not discussed. I wonder about the differences between UIE and OIE
3. The experiments are limited to open-domain IE tasks, I wonder whether the approach works on traditional NER and RE tasks (for example, CoNLL 03 NER).
4. GPT-based models are proved to be strong on open-domain and 0-shot tasks, but this paper does not compare OIE with these models.

Reference:
[1] Unified Structure Generation for Universal Information Extraction
",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691332844567,,,EMNLP/2023/Conference,2FDty4mLqP,"['EMNLP/2023/Conference/Submission2960/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461199992,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2960/Reviewer_s459']",2FDty4mLqP,['EMNLP/2023/Conference/Submission2960/Reviewer_s459'],1691332844567,1701461199992,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2960/Reviewer_s459']","Yes

List of suggested papers or references not cited in the manuscript:
1. Unified Structure Generation for Universal Information Extraction (UIE) [1]

These citations might be necessary because the reviewer mentions that an important and similar related work, UIE [1], is not discussed in the paper. The reviewer wonders about the differences between UIE and OIE, implying that a comparison or discussion of UIE would be relevant and useful for understanding the contributions and context of the proposed approach. This suggests that the authors should consider citing and discussing UIE to provide a more comprehensive overview of related work in the field.",1,,Unified Structure Generation for Universal Information Extraction
xYqGvFbEXk,"Summary:
This paper extends previous ARA approach (BERT-FP-LBL [1]) by incorporating simple prompting tricks. Multiple experiments are conducted over 4 English and 2 Chinese datasets by comparing to various baselines. 

[1] A unified neural network model for readability assessment with feature projection and length-balanced loss, by Li et al., in EMNLP, 2022.
https://aclanthology.org/2022.emnlp-main.504/","-- The paper is well written.

-- The experiments are good.","-- Limited technical novelties.

-- More comparisons are needed for the proposed approach and the BERT-FP-LBL, including detailed ablation studies.

-- Instead of directly citing the results from [1] for BERT-FP-LBL, it would be more helpful if we could have the finetuned results. ","1- For Table 4 (Page 7), it’s better to have the finetuned results for the BERT-FP-LBL for ALL datasets. Is there any particular reason that you choose not to do that?

2- What checkpoints/pretrained weights did you use for all approaches, especially for the Chinese datasets (e.g., the BERT method in Table 5)?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"Missing references:
- Comprehensive readability assessment of scientific learning resources, by Arshad et al., in IEEE, 2023
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10132466

- Automated assessment of subjective assignments: a hybrid approach, by Birla et al., in 2022
 https://www.sciencedirect.com/science/article/abs/pii/S0957417422006777","1- In Page 7, Table 4, it’s better to add the recall, similar as the previous work [1].
",1691712616116,,,EMNLP/2023/Conference,27HNeESZQF,"['EMNLP/2023/Conference/Submission2244/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461152766,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2244/Reviewer_JPZk']",27HNeESZQF,['EMNLP/2023/Conference/Submission2244/Reviewer_JPZk'],1691712616116,1701461152766,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2244/Reviewer_JPZk']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Comprehensive readability assessment of scientific learning resources"" by Arshad et al., in IEEE, 2023
2. ""Automated assessment of subjective assignments: a hybrid approach"" by Birla et al., in 2022

These citations might be necessary because the reviewer has listed them under ""Missing_References"", implying that they are relevant to the topic of the manuscript and should be included to provide a more comprehensive overview of the existing research in the field. The reviewer's comment suggests that the authors may have overlooked these important papers, and including them could strengthen the manuscript by acknowledging and engaging with the broader literature.",1,"2023, 2022",Comprehensive readability assessment of scientific learning resources 
xYqGvFbEXk,"Summary:
This paper extends previous ARA approach (BERT-FP-LBL [1]) by incorporating simple prompting tricks. Multiple experiments are conducted over 4 English and 2 Chinese datasets by comparing to various baselines. 

[1] A unified neural network model for readability assessment with feature projection and length-balanced loss, by Li et al., in EMNLP, 2022.
https://aclanthology.org/2022.emnlp-main.504/","-- The paper is well written.

-- The experiments are good.","-- Limited technical novelties.

-- More comparisons are needed for the proposed approach and the BERT-FP-LBL, including detailed ablation studies.

-- Instead of directly citing the results from [1] for BERT-FP-LBL, it would be more helpful if we could have the finetuned results. ","1- For Table 4 (Page 7), it’s better to have the finetuned results for the BERT-FP-LBL for ALL datasets. Is there any particular reason that you choose not to do that?

2- What checkpoints/pretrained weights did you use for all approaches, especially for the Chinese datasets (e.g., the BERT method in Table 5)?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"Missing references:
- Comprehensive readability assessment of scientific learning resources, by Arshad et al., in IEEE, 2023
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10132466

- Automated assessment of subjective assignments: a hybrid approach, by Birla et al., in 2022
 https://www.sciencedirect.com/science/article/abs/pii/S0957417422006777","1- In Page 7, Table 4, it’s better to add the recall, similar as the previous work [1].
",1691712616116,,,EMNLP/2023/Conference,27HNeESZQF,"['EMNLP/2023/Conference/Submission2244/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461152766,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2244/Reviewer_JPZk']",27HNeESZQF,['EMNLP/2023/Conference/Submission2244/Reviewer_JPZk'],1691712616116,1701461152766,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2244/Reviewer_JPZk']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Comprehensive readability assessment of scientific learning resources"" by Arshad et al., in IEEE, 2023
2. ""Automated assessment of subjective assignments: a hybrid approach"" by Birla et al., in 2022

These citations might be necessary because the reviewer has listed them under ""Missing_References"", implying that they are relevant to the topic of the manuscript and should be included to provide a more comprehensive overview of the existing research in the field. The reviewer's comment suggests that the authors may have overlooked these important papers, and including them could strengthen the manuscript by acknowledging and engaging with the broader literature.",1,"2023, 2022",Automated assessment of subjective assignments a hybrid approach
AcJg9dCp02,"Readability assessment is the key step for many NLP tasks, including text simplification and educational applications.
Traditional feature-based readability formulas don't capture deep semantic meaning. Deep learning-based approaches capture semantic meaning but require a large dataset to train and finetune.

This paper proposes automatic readability assessment with prompt-based techniques in a large language model, BigBERT to address the two abovementioned challenges.

Toward that end, the authors combined prompt-based and traditional linguistic features in their experiment design. The authors conducted experiments with both English and Chinese language datasets.
Their empirical results and ablation studies demonstrate superior performance compared to baselines. 
","-The proposed method uses prompt-based readability assessment which overcomes the need of a large dataset for finetuning models
- Authors evaluate the proposed approach in English and Chinese language

","First, I would recommend authors to compare their proposed approach with readability formula-based methods, similar to Chakraborty et al. in [6].
Second, to demonstrate the utility of the prompt-based approach, the authors can compare BERT-with prompts [7] and their proposed method: BigBERT-with prompts. Since BigBERT is a larger pre-trained model, the performance improvement over BERt is expected.


Third, for the Chinese Readability, the experiment setting is not correct. The authors can utilize the existing pre-trained Chinese models [1, 2], checkpoints for Chinese [3], and multilingual BERT [4] models.
Specifically, the results on the English dataset in Table 4 are better than the Chinese dataset in Table 5 by a large margin. 

A good example of a multilingual experiment setting is Hollenstein et al. [5]. 
",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"References
1. Yang, Z., Xu, Z., Cui, Y., Wang, B., Lin, M., Wu, D. and Chen, Z., 2022, October. CINO: A Chinese Minority Pre-trained Language Model. In Proceedings of the 29th International Conference on Computational Linguistics (pp. 3937-3949).
2. Cui, Y., Che, W., Liu, T., Qin, B., Wang, S. and Hu, G., 2020, November. Revisiting Pre-Trained Models for Chinese Natural Language Processing. In Findings of the Association for Computational Linguistics: EMNLP 2020 (pp. 657-668).
3. Cino Huggingface Checkpoints https://huggingface.co/hfl/cino-large
4. https://huggingface.co/bert-base-multilingual-cased
4. PHollenstein, N., Pirovano, F., Zhang, C., Jäger, L.A. and Beinborn, L., 2021, June. Multilingual Language Models Predict Human Reading Behavior. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 106-123). Association for Computational Linguistics.
5. Chakraborty, S., Nayeem, M.T. and Ahmad, W.U., 2021, May. Simple or complex? learning to predict the readability of Bengali texts. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 14, pp. 12621-12629).
6. Jiang, T., Jiao, J., Huang, S., Zhang, Z., Wang, D., Zhuang, F., Wei, F., Huang, H., Deng, D. and Zhang, Q., 2022, December. PromptBERT: Improving BERT Sentence Embeddings with Prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (pp. 8826-8837).",,1691721932855,,,EMNLP/2023/Conference,27HNeESZQF,"['EMNLP/2023/Conference/Submission2244/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461152650,[],4,,,"['everyone', 'EMNLP/2023/Conference/Submission2244/Reviewer_b6jz']",27HNeESZQF,['EMNLP/2023/Conference/Submission2244/Reviewer_b6jz'],1691721932855,1701461152650,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2244/Reviewer_b6jz']","Yes

The suggested papers or references not cited in the manuscript are:
1. Chakraborty et al. [6] (already cited, but the reviewer suggests a comparison with their readability formula-based methods)
2. Jiang et al. [7] (PromptBERT: Improving BERT Sentence Embeddings with Prompts, for comparison with the proposed BigBERT-with prompts method)
3. Yang et al. [1] (CINO: A Chinese Minority Pre-trained Language Model)
4. Cui et al. [2] (Revisiting Pre-Trained Models for Chinese Natural Language Processing)
5. Hollenstein et al. [5] (Multilingual Language Models Predict Human Reading Behavior, as an example of a multilingual experiment setting)

These citations might be necessary because the reviewer suggests that the authors should:
- Compare their proposed approach with readability formula-based methods (Chakraborty et al. [6]) to demonstrate its superiority.
- Compare their BigBERT-with prompts method with BERT-with prompts (Jiang et al. [7]) to show the performance improvement due to the larger pre-trained model.
- Utilize existing pre-trained Chinese models (Yang et al. [1], Cui et al. [2]) and multilingual BERT models for the Chinese readability experiment, as the current setting may not be correct.
- Follow the example of Hollenstein et al. [5] for a multilingual experiment setting to improve the validity of their results.",1,,PromptBERT Improving BERT Sentence Embeddings with Prompts 
AcJg9dCp02,"Readability assessment is the key step for many NLP tasks, including text simplification and educational applications.
Traditional feature-based readability formulas don't capture deep semantic meaning. Deep learning-based approaches capture semantic meaning but require a large dataset to train and finetune.

This paper proposes automatic readability assessment with prompt-based techniques in a large language model, BigBERT to address the two abovementioned challenges.

Toward that end, the authors combined prompt-based and traditional linguistic features in their experiment design. The authors conducted experiments with both English and Chinese language datasets.
Their empirical results and ablation studies demonstrate superior performance compared to baselines. 
","-The proposed method uses prompt-based readability assessment which overcomes the need of a large dataset for finetuning models
- Authors evaluate the proposed approach in English and Chinese language

","First, I would recommend authors to compare their proposed approach with readability formula-based methods, similar to Chakraborty et al. in [6].
Second, to demonstrate the utility of the prompt-based approach, the authors can compare BERT-with prompts [7] and their proposed method: BigBERT-with prompts. Since BigBERT is a larger pre-trained model, the performance improvement over BERt is expected.


Third, for the Chinese Readability, the experiment setting is not correct. The authors can utilize the existing pre-trained Chinese models [1, 2], checkpoints for Chinese [3], and multilingual BERT [4] models.
Specifically, the results on the English dataset in Table 4 are better than the Chinese dataset in Table 5 by a large margin. 

A good example of a multilingual experiment setting is Hollenstein et al. [5]. 
",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"References
1. Yang, Z., Xu, Z., Cui, Y., Wang, B., Lin, M., Wu, D. and Chen, Z., 2022, October. CINO: A Chinese Minority Pre-trained Language Model. In Proceedings of the 29th International Conference on Computational Linguistics (pp. 3937-3949).
2. Cui, Y., Che, W., Liu, T., Qin, B., Wang, S. and Hu, G., 2020, November. Revisiting Pre-Trained Models for Chinese Natural Language Processing. In Findings of the Association for Computational Linguistics: EMNLP 2020 (pp. 657-668).
3. Cino Huggingface Checkpoints https://huggingface.co/hfl/cino-large
4. https://huggingface.co/bert-base-multilingual-cased
4. PHollenstein, N., Pirovano, F., Zhang, C., Jäger, L.A. and Beinborn, L., 2021, June. Multilingual Language Models Predict Human Reading Behavior. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 106-123). Association for Computational Linguistics.
5. Chakraborty, S., Nayeem, M.T. and Ahmad, W.U., 2021, May. Simple or complex? learning to predict the readability of Bengali texts. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 14, pp. 12621-12629).
6. Jiang, T., Jiao, J., Huang, S., Zhang, Z., Wang, D., Zhuang, F., Wei, F., Huang, H., Deng, D. and Zhang, Q., 2022, December. PromptBERT: Improving BERT Sentence Embeddings with Prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (pp. 8826-8837).",,1691721932855,,,EMNLP/2023/Conference,27HNeESZQF,"['EMNLP/2023/Conference/Submission2244/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461152650,[],4,,,"['everyone', 'EMNLP/2023/Conference/Submission2244/Reviewer_b6jz']",27HNeESZQF,['EMNLP/2023/Conference/Submission2244/Reviewer_b6jz'],1691721932855,1701461152650,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2244/Reviewer_b6jz']","Yes

The suggested papers or references not cited in the manuscript are:
1. Chakraborty et al. [6] (already cited, but the reviewer suggests a comparison with their readability formula-based methods)
2. Jiang et al. [7] (PromptBERT: Improving BERT Sentence Embeddings with Prompts, for comparison with the proposed BigBERT-with prompts method)
3. Yang et al. [1] (CINO: A Chinese Minority Pre-trained Language Model)
4. Cui et al. [2] (Revisiting Pre-Trained Models for Chinese Natural Language Processing)
5. Hollenstein et al. [5] (Multilingual Language Models Predict Human Reading Behavior, as an example of a multilingual experiment setting)

These citations might be necessary because the reviewer suggests that the authors should:
- Compare their proposed approach with readability formula-based methods (Chakraborty et al. [6]) to demonstrate its superiority.
- Compare their BigBERT-with prompts method with BERT-with prompts (Jiang et al. [7]) to show the performance improvement due to the larger pre-trained model.
- Utilize existing pre-trained Chinese models (Yang et al. [1], Cui et al. [2]) and multilingual BERT models for the Chinese readability experiment, as the current setting may not be correct.
- Follow the example of Hollenstein et al. [5] for a multilingual experiment setting to improve the validity of their results.",1,,CINO A Chinese Minority Pre-trained Language Model 
AcJg9dCp02,"Readability assessment is the key step for many NLP tasks, including text simplification and educational applications.
Traditional feature-based readability formulas don't capture deep semantic meaning. Deep learning-based approaches capture semantic meaning but require a large dataset to train and finetune.

This paper proposes automatic readability assessment with prompt-based techniques in a large language model, BigBERT to address the two abovementioned challenges.

Toward that end, the authors combined prompt-based and traditional linguistic features in their experiment design. The authors conducted experiments with both English and Chinese language datasets.
Their empirical results and ablation studies demonstrate superior performance compared to baselines. 
","-The proposed method uses prompt-based readability assessment which overcomes the need of a large dataset for finetuning models
- Authors evaluate the proposed approach in English and Chinese language

","First, I would recommend authors to compare their proposed approach with readability formula-based methods, similar to Chakraborty et al. in [6].
Second, to demonstrate the utility of the prompt-based approach, the authors can compare BERT-with prompts [7] and their proposed method: BigBERT-with prompts. Since BigBERT is a larger pre-trained model, the performance improvement over BERt is expected.


Third, for the Chinese Readability, the experiment setting is not correct. The authors can utilize the existing pre-trained Chinese models [1, 2], checkpoints for Chinese [3], and multilingual BERT [4] models.
Specifically, the results on the English dataset in Table 4 are better than the Chinese dataset in Table 5 by a large margin. 

A good example of a multilingual experiment setting is Hollenstein et al. [5]. 
",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"References
1. Yang, Z., Xu, Z., Cui, Y., Wang, B., Lin, M., Wu, D. and Chen, Z., 2022, October. CINO: A Chinese Minority Pre-trained Language Model. In Proceedings of the 29th International Conference on Computational Linguistics (pp. 3937-3949).
2. Cui, Y., Che, W., Liu, T., Qin, B., Wang, S. and Hu, G., 2020, November. Revisiting Pre-Trained Models for Chinese Natural Language Processing. In Findings of the Association for Computational Linguistics: EMNLP 2020 (pp. 657-668).
3. Cino Huggingface Checkpoints https://huggingface.co/hfl/cino-large
4. https://huggingface.co/bert-base-multilingual-cased
4. PHollenstein, N., Pirovano, F., Zhang, C., Jäger, L.A. and Beinborn, L., 2021, June. Multilingual Language Models Predict Human Reading Behavior. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 106-123). Association for Computational Linguistics.
5. Chakraborty, S., Nayeem, M.T. and Ahmad, W.U., 2021, May. Simple or complex? learning to predict the readability of Bengali texts. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 14, pp. 12621-12629).
6. Jiang, T., Jiao, J., Huang, S., Zhang, Z., Wang, D., Zhuang, F., Wei, F., Huang, H., Deng, D. and Zhang, Q., 2022, December. PromptBERT: Improving BERT Sentence Embeddings with Prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (pp. 8826-8837).",,1691721932855,,,EMNLP/2023/Conference,27HNeESZQF,"['EMNLP/2023/Conference/Submission2244/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461152650,[],4,,,"['everyone', 'EMNLP/2023/Conference/Submission2244/Reviewer_b6jz']",27HNeESZQF,['EMNLP/2023/Conference/Submission2244/Reviewer_b6jz'],1691721932855,1701461152650,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2244/Reviewer_b6jz']","Yes

The suggested papers or references not cited in the manuscript are:
1. Chakraborty et al. [6] (already cited, but the reviewer suggests a comparison with their readability formula-based methods)
2. Jiang et al. [7] (PromptBERT: Improving BERT Sentence Embeddings with Prompts, for comparison with the proposed BigBERT-with prompts method)
3. Yang et al. [1] (CINO: A Chinese Minority Pre-trained Language Model)
4. Cui et al. [2] (Revisiting Pre-Trained Models for Chinese Natural Language Processing)
5. Hollenstein et al. [5] (Multilingual Language Models Predict Human Reading Behavior, as an example of a multilingual experiment setting)

These citations might be necessary because the reviewer suggests that the authors should:
- Compare their proposed approach with readability formula-based methods (Chakraborty et al. [6]) to demonstrate its superiority.
- Compare their BigBERT-with prompts method with BERT-with prompts (Jiang et al. [7]) to show the performance improvement due to the larger pre-trained model.
- Utilize existing pre-trained Chinese models (Yang et al. [1], Cui et al. [2]) and multilingual BERT models for the Chinese readability experiment, as the current setting may not be correct.
- Follow the example of Hollenstein et al. [5] for a multilingual experiment setting to improve the validity of their results.",1,,Revisiting Pre-Trained Models for Chinese Natural Language Processing 
AcJg9dCp02,"Readability assessment is the key step for many NLP tasks, including text simplification and educational applications.
Traditional feature-based readability formulas don't capture deep semantic meaning. Deep learning-based approaches capture semantic meaning but require a large dataset to train and finetune.

This paper proposes automatic readability assessment with prompt-based techniques in a large language model, BigBERT to address the two abovementioned challenges.

Toward that end, the authors combined prompt-based and traditional linguistic features in their experiment design. The authors conducted experiments with both English and Chinese language datasets.
Their empirical results and ablation studies demonstrate superior performance compared to baselines. 
","-The proposed method uses prompt-based readability assessment which overcomes the need of a large dataset for finetuning models
- Authors evaluate the proposed approach in English and Chinese language

","First, I would recommend authors to compare their proposed approach with readability formula-based methods, similar to Chakraborty et al. in [6].
Second, to demonstrate the utility of the prompt-based approach, the authors can compare BERT-with prompts [7] and their proposed method: BigBERT-with prompts. Since BigBERT is a larger pre-trained model, the performance improvement over BERt is expected.


Third, for the Chinese Readability, the experiment setting is not correct. The authors can utilize the existing pre-trained Chinese models [1, 2], checkpoints for Chinese [3], and multilingual BERT [4] models.
Specifically, the results on the English dataset in Table 4 are better than the Chinese dataset in Table 5 by a large margin. 

A good example of a multilingual experiment setting is Hollenstein et al. [5]. 
",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"References
1. Yang, Z., Xu, Z., Cui, Y., Wang, B., Lin, M., Wu, D. and Chen, Z., 2022, October. CINO: A Chinese Minority Pre-trained Language Model. In Proceedings of the 29th International Conference on Computational Linguistics (pp. 3937-3949).
2. Cui, Y., Che, W., Liu, T., Qin, B., Wang, S. and Hu, G., 2020, November. Revisiting Pre-Trained Models for Chinese Natural Language Processing. In Findings of the Association for Computational Linguistics: EMNLP 2020 (pp. 657-668).
3. Cino Huggingface Checkpoints https://huggingface.co/hfl/cino-large
4. https://huggingface.co/bert-base-multilingual-cased
4. PHollenstein, N., Pirovano, F., Zhang, C., Jäger, L.A. and Beinborn, L., 2021, June. Multilingual Language Models Predict Human Reading Behavior. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 106-123). Association for Computational Linguistics.
5. Chakraborty, S., Nayeem, M.T. and Ahmad, W.U., 2021, May. Simple or complex? learning to predict the readability of Bengali texts. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 14, pp. 12621-12629).
6. Jiang, T., Jiao, J., Huang, S., Zhang, Z., Wang, D., Zhuang, F., Wei, F., Huang, H., Deng, D. and Zhang, Q., 2022, December. PromptBERT: Improving BERT Sentence Embeddings with Prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (pp. 8826-8837).",,1691721932855,,,EMNLP/2023/Conference,27HNeESZQF,"['EMNLP/2023/Conference/Submission2244/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461152650,[],4,,,"['everyone', 'EMNLP/2023/Conference/Submission2244/Reviewer_b6jz']",27HNeESZQF,['EMNLP/2023/Conference/Submission2244/Reviewer_b6jz'],1691721932855,1701461152650,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2244/Reviewer_b6jz']","Yes

The suggested papers or references not cited in the manuscript are:
1. Chakraborty et al. [6] (already cited, but the reviewer suggests a comparison with their readability formula-based methods)
2. Jiang et al. [7] (PromptBERT: Improving BERT Sentence Embeddings with Prompts, for comparison with the proposed BigBERT-with prompts method)
3. Yang et al. [1] (CINO: A Chinese Minority Pre-trained Language Model)
4. Cui et al. [2] (Revisiting Pre-Trained Models for Chinese Natural Language Processing)
5. Hollenstein et al. [5] (Multilingual Language Models Predict Human Reading Behavior, as an example of a multilingual experiment setting)

These citations might be necessary because the reviewer suggests that the authors should:
- Compare their proposed approach with readability formula-based methods (Chakraborty et al. [6]) to demonstrate its superiority.
- Compare their BigBERT-with prompts method with BERT-with prompts (Jiang et al. [7]) to show the performance improvement due to the larger pre-trained model.
- Utilize existing pre-trained Chinese models (Yang et al. [1], Cui et al. [2]) and multilingual BERT models for the Chinese readability experiment, as the current setting may not be correct.
- Follow the example of Hollenstein et al. [5] for a multilingual experiment setting to improve the validity of their results.",1,,Multilingual Language Models Predict Human Reading Behavior
wF5kcuEPNc,"This paper first construct a new dataset for multimodal fine-grained entity typing based on FIGER. They propose a multimodal object-level visual context network, which fuses textual information and object-level visual information through attention layers, to capture fine-grained semantic information. Experiments compared with several existing textual-level models show that fusing object-level visual information can help improve the performance.",A new dataset is constructed for multimodal fine-grained entity typing task and their proposed model shows that adding object-level visual information can improve the performance a lot. The attention visualization helps better understand how object-level visual information works.,"The main concerns of this paper are:
1. Fusing object information by cross-modal attention layeres are commonly used for multi-modal information extraction. Please refer to missing references section. And the hybrid classification layer is also following previous works. 
2. The experiment is only conducted on one dataset. Need to implement more multi-modal baselines for performance comparison. Please see details in question D.","A. For section 3.2 dataset construction. What's the quality of the collected images? Is there any method to evaluate your images quality? Does there exist any noise for the images?
B. Line 302, is the attention calculation following self-attention mechanism, where q is from text and k and v are from images?
C. Line 356-357, could you please explain more about how you dynamically adjust the parameter \lambda? what's the final value for \lambda?
D. For your experiments, all baselines are unimodal. In other words, they only use textual information. There are lots of existing multimodal fusion models, I would like to see how your model compare with those multimodal fusion models. You could refer to the missing references, or even some simpliest fusion techniques such as concatenation, circulant fusion, etc. If there is no such comparison, the paper only proves that adding object-level visual information works better than only using textual information. In general, if the assumption is you have high quality inputs (additional information such as images), it always help improve the performance (regardless of efficiency). In this way, it is hard to get the contribution of your proposed multimodal object-level visual context network. Because in your ablation study, using your attention-based fusion method, the performance improves less than 1 percentage. The most performance contribution is provided by adding additional information for the inputs.
","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Chen, Xiang, Ningyu Zhang, Lei Li, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. ""Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction.""NAACL 2022.
2. Bo Xu, Shizhou Huang, Chaofeng Sha, and Hongya Wang. 2022. MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22). Association for Computing Machinery, New York, NY, USA, 1215–1223. https://doi.org/10.1145/3488560.3498475
3. C. Zheng, Z. Wu, T. Wang, Y. Cai and Q. Li, ""Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning,"" in IEEE Transactions on Multimedia, vol. 23, pp. 2520-2532, 2021, doi: 10.1109/TMM.2020.3013398.
4. Zhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen, Ho-fung Leung, and Qing Li. 2020. Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts. In Proceedings of the 28th ACM International Conference on Multimedia (MM '20). Association for Computing Machinery, New York, NY, USA, 1038–1046. https://doi.org/10.1145/3394171.3413650
",,1690785338026,,,EMNLP/2023/Conference,1tZxE1WPKz,"['EMNLP/2023/Conference/Submission1726/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461120343,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1726/Reviewer_JYT1']",1tZxE1WPKz,['EMNLP/2023/Conference/Submission1726/Reviewer_JYT1'],1690785338026,1701461120343,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1726/Reviewer_JYT1']","Yes

The suggested papers or references not cited in the manuscript are:
1. Chen, Xiang, et al. ""Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction."" NAACL 2022.
2. Bo Xu, et al. ""MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition."" Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22).
3. C. Zheng, et al. ""Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning,"" IEEE Transactions on Multimedia, vol. 23, pp. 2520-2532, 2021.
4. Zhiwei Wu, et al. ""Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts."" Proceedings of the 28th ACM International Conference on Multimedia (MM '20).

These citations might be necessary because the reviewer believes that the authors' approach to fusing object information using cross-modal attention layers is not novel and has been commonly used in previous works. The reviewer suggests that the authors should compare their model with existing multimodal fusion models, such as those presented in the suggested papers, to demonstrate the contribution of their proposed multimodal object-level visual context network. The reviewer argues that simply showing that adding object-level visual information improves performance over textual-only models is not sufficient, as it is expected that additional information would improve performance. A comparison with existing multimodal models would help to isolate the contribution of the authors' specific approach.",1,"2022, 2022, 2021, 2020",Good visual guidance makes a better extractor Hierarchical visual prefix for multimodal entity and relation extraction 
wF5kcuEPNc,"This paper first construct a new dataset for multimodal fine-grained entity typing based on FIGER. They propose a multimodal object-level visual context network, which fuses textual information and object-level visual information through attention layers, to capture fine-grained semantic information. Experiments compared with several existing textual-level models show that fusing object-level visual information can help improve the performance.",A new dataset is constructed for multimodal fine-grained entity typing task and their proposed model shows that adding object-level visual information can improve the performance a lot. The attention visualization helps better understand how object-level visual information works.,"The main concerns of this paper are:
1. Fusing object information by cross-modal attention layeres are commonly used for multi-modal information extraction. Please refer to missing references section. And the hybrid classification layer is also following previous works. 
2. The experiment is only conducted on one dataset. Need to implement more multi-modal baselines for performance comparison. Please see details in question D.","A. For section 3.2 dataset construction. What's the quality of the collected images? Is there any method to evaluate your images quality? Does there exist any noise for the images?
B. Line 302, is the attention calculation following self-attention mechanism, where q is from text and k and v are from images?
C. Line 356-357, could you please explain more about how you dynamically adjust the parameter \lambda? what's the final value for \lambda?
D. For your experiments, all baselines are unimodal. In other words, they only use textual information. There are lots of existing multimodal fusion models, I would like to see how your model compare with those multimodal fusion models. You could refer to the missing references, or even some simpliest fusion techniques such as concatenation, circulant fusion, etc. If there is no such comparison, the paper only proves that adding object-level visual information works better than only using textual information. In general, if the assumption is you have high quality inputs (additional information such as images), it always help improve the performance (regardless of efficiency). In this way, it is hard to get the contribution of your proposed multimodal object-level visual context network. Because in your ablation study, using your attention-based fusion method, the performance improves less than 1 percentage. The most performance contribution is provided by adding additional information for the inputs.
","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Chen, Xiang, Ningyu Zhang, Lei Li, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. ""Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction.""NAACL 2022.
2. Bo Xu, Shizhou Huang, Chaofeng Sha, and Hongya Wang. 2022. MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22). Association for Computing Machinery, New York, NY, USA, 1215–1223. https://doi.org/10.1145/3488560.3498475
3. C. Zheng, Z. Wu, T. Wang, Y. Cai and Q. Li, ""Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning,"" in IEEE Transactions on Multimedia, vol. 23, pp. 2520-2532, 2021, doi: 10.1109/TMM.2020.3013398.
4. Zhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen, Ho-fung Leung, and Qing Li. 2020. Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts. In Proceedings of the 28th ACM International Conference on Multimedia (MM '20). Association for Computing Machinery, New York, NY, USA, 1038–1046. https://doi.org/10.1145/3394171.3413650
",,1690785338026,,,EMNLP/2023/Conference,1tZxE1WPKz,"['EMNLP/2023/Conference/Submission1726/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461120343,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1726/Reviewer_JYT1']",1tZxE1WPKz,['EMNLP/2023/Conference/Submission1726/Reviewer_JYT1'],1690785338026,1701461120343,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1726/Reviewer_JYT1']","Yes

The suggested papers or references not cited in the manuscript are:
1. Chen, Xiang, et al. ""Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction."" NAACL 2022.
2. Bo Xu, et al. ""MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition."" Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22).
3. C. Zheng, et al. ""Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning,"" IEEE Transactions on Multimedia, vol. 23, pp. 2520-2532, 2021.
4. Zhiwei Wu, et al. ""Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts."" Proceedings of the 28th ACM International Conference on Multimedia (MM '20).

These citations might be necessary because the reviewer believes that the authors' approach to fusing object information using cross-modal attention layers is not novel and has been commonly used in previous works. The reviewer suggests that the authors should compare their model with existing multimodal fusion models, such as those presented in the suggested papers, to demonstrate the contribution of their proposed multimodal object-level visual context network. The reviewer argues that simply showing that adding object-level visual information improves performance over textual-only models is not sufficient, as it is expected that additional information would improve performance. A comparison with existing multimodal models would help to isolate the contribution of the authors' specific approach.",1,"2022, 2022, 2021, 2020",MAF A General Matching and Alignment Framework for Multimodal Named Entity Recognition 
wF5kcuEPNc,"This paper first construct a new dataset for multimodal fine-grained entity typing based on FIGER. They propose a multimodal object-level visual context network, which fuses textual information and object-level visual information through attention layers, to capture fine-grained semantic information. Experiments compared with several existing textual-level models show that fusing object-level visual information can help improve the performance.",A new dataset is constructed for multimodal fine-grained entity typing task and their proposed model shows that adding object-level visual information can improve the performance a lot. The attention visualization helps better understand how object-level visual information works.,"The main concerns of this paper are:
1. Fusing object information by cross-modal attention layeres are commonly used for multi-modal information extraction. Please refer to missing references section. And the hybrid classification layer is also following previous works. 
2. The experiment is only conducted on one dataset. Need to implement more multi-modal baselines for performance comparison. Please see details in question D.","A. For section 3.2 dataset construction. What's the quality of the collected images? Is there any method to evaluate your images quality? Does there exist any noise for the images?
B. Line 302, is the attention calculation following self-attention mechanism, where q is from text and k and v are from images?
C. Line 356-357, could you please explain more about how you dynamically adjust the parameter \lambda? what's the final value for \lambda?
D. For your experiments, all baselines are unimodal. In other words, they only use textual information. There are lots of existing multimodal fusion models, I would like to see how your model compare with those multimodal fusion models. You could refer to the missing references, or even some simpliest fusion techniques such as concatenation, circulant fusion, etc. If there is no such comparison, the paper only proves that adding object-level visual information works better than only using textual information. In general, if the assumption is you have high quality inputs (additional information such as images), it always help improve the performance (regardless of efficiency). In this way, it is hard to get the contribution of your proposed multimodal object-level visual context network. Because in your ablation study, using your attention-based fusion method, the performance improves less than 1 percentage. The most performance contribution is provided by adding additional information for the inputs.
","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Chen, Xiang, Ningyu Zhang, Lei Li, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. ""Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction.""NAACL 2022.
2. Bo Xu, Shizhou Huang, Chaofeng Sha, and Hongya Wang. 2022. MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22). Association for Computing Machinery, New York, NY, USA, 1215–1223. https://doi.org/10.1145/3488560.3498475
3. C. Zheng, Z. Wu, T. Wang, Y. Cai and Q. Li, ""Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning,"" in IEEE Transactions on Multimedia, vol. 23, pp. 2520-2532, 2021, doi: 10.1109/TMM.2020.3013398.
4. Zhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen, Ho-fung Leung, and Qing Li. 2020. Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts. In Proceedings of the 28th ACM International Conference on Multimedia (MM '20). Association for Computing Machinery, New York, NY, USA, 1038–1046. https://doi.org/10.1145/3394171.3413650
",,1690785338026,,,EMNLP/2023/Conference,1tZxE1WPKz,"['EMNLP/2023/Conference/Submission1726/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461120343,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1726/Reviewer_JYT1']",1tZxE1WPKz,['EMNLP/2023/Conference/Submission1726/Reviewer_JYT1'],1690785338026,1701461120343,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1726/Reviewer_JYT1']","Yes

The suggested papers or references not cited in the manuscript are:
1. Chen, Xiang, et al. ""Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction."" NAACL 2022.
2. Bo Xu, et al. ""MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition."" Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22).
3. C. Zheng, et al. ""Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning,"" IEEE Transactions on Multimedia, vol. 23, pp. 2520-2532, 2021.
4. Zhiwei Wu, et al. ""Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts."" Proceedings of the 28th ACM International Conference on Multimedia (MM '20).

These citations might be necessary because the reviewer believes that the authors' approach to fusing object information using cross-modal attention layers is not novel and has been commonly used in previous works. The reviewer suggests that the authors should compare their model with existing multimodal fusion models, such as those presented in the suggested papers, to demonstrate the contribution of their proposed multimodal object-level visual context network. The reviewer argues that simply showing that adding object-level visual information improves performance over textual-only models is not sufficient, as it is expected that additional information would improve performance. A comparison with existing multimodal models would help to isolate the contribution of the authors' specific approach.",1,"2022, 2022, 2021, 2020",Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning 
wF5kcuEPNc,"This paper first construct a new dataset for multimodal fine-grained entity typing based on FIGER. They propose a multimodal object-level visual context network, which fuses textual information and object-level visual information through attention layers, to capture fine-grained semantic information. Experiments compared with several existing textual-level models show that fusing object-level visual information can help improve the performance.",A new dataset is constructed for multimodal fine-grained entity typing task and their proposed model shows that adding object-level visual information can improve the performance a lot. The attention visualization helps better understand how object-level visual information works.,"The main concerns of this paper are:
1. Fusing object information by cross-modal attention layeres are commonly used for multi-modal information extraction. Please refer to missing references section. And the hybrid classification layer is also following previous works. 
2. The experiment is only conducted on one dataset. Need to implement more multi-modal baselines for performance comparison. Please see details in question D.","A. For section 3.2 dataset construction. What's the quality of the collected images? Is there any method to evaluate your images quality? Does there exist any noise for the images?
B. Line 302, is the attention calculation following self-attention mechanism, where q is from text and k and v are from images?
C. Line 356-357, could you please explain more about how you dynamically adjust the parameter \lambda? what's the final value for \lambda?
D. For your experiments, all baselines are unimodal. In other words, they only use textual information. There are lots of existing multimodal fusion models, I would like to see how your model compare with those multimodal fusion models. You could refer to the missing references, or even some simpliest fusion techniques such as concatenation, circulant fusion, etc. If there is no such comparison, the paper only proves that adding object-level visual information works better than only using textual information. In general, if the assumption is you have high quality inputs (additional information such as images), it always help improve the performance (regardless of efficiency). In this way, it is hard to get the contribution of your proposed multimodal object-level visual context network. Because in your ablation study, using your attention-based fusion method, the performance improves less than 1 percentage. The most performance contribution is provided by adding additional information for the inputs.
","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. Chen, Xiang, Ningyu Zhang, Lei Li, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. ""Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction.""NAACL 2022.
2. Bo Xu, Shizhou Huang, Chaofeng Sha, and Hongya Wang. 2022. MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22). Association for Computing Machinery, New York, NY, USA, 1215–1223. https://doi.org/10.1145/3488560.3498475
3. C. Zheng, Z. Wu, T. Wang, Y. Cai and Q. Li, ""Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning,"" in IEEE Transactions on Multimedia, vol. 23, pp. 2520-2532, 2021, doi: 10.1109/TMM.2020.3013398.
4. Zhiwei Wu, Changmeng Zheng, Yi Cai, Junying Chen, Ho-fung Leung, and Qing Li. 2020. Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts. In Proceedings of the 28th ACM International Conference on Multimedia (MM '20). Association for Computing Machinery, New York, NY, USA, 1038–1046. https://doi.org/10.1145/3394171.3413650
",,1690785338026,,,EMNLP/2023/Conference,1tZxE1WPKz,"['EMNLP/2023/Conference/Submission1726/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461120343,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1726/Reviewer_JYT1']",1tZxE1WPKz,['EMNLP/2023/Conference/Submission1726/Reviewer_JYT1'],1690785338026,1701461120343,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1726/Reviewer_JYT1']","Yes

The suggested papers or references not cited in the manuscript are:
1. Chen, Xiang, et al. ""Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction."" NAACL 2022.
2. Bo Xu, et al. ""MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition."" Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22).
3. C. Zheng, et al. ""Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning,"" IEEE Transactions on Multimedia, vol. 23, pp. 2520-2532, 2021.
4. Zhiwei Wu, et al. ""Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts."" Proceedings of the 28th ACM International Conference on Multimedia (MM '20).

These citations might be necessary because the reviewer believes that the authors' approach to fusing object information using cross-modal attention layers is not novel and has been commonly used in previous works. The reviewer suggests that the authors should compare their model with existing multimodal fusion models, such as those presented in the suggested papers, to demonstrate the contribution of their proposed multimodal object-level visual context network. The reviewer argues that simply showing that adding object-level visual information improves performance over textual-only models is not sufficient, as it is expected that additional information would improve performance. A comparison with existing multimodal models would help to isolate the contribution of the authors' specific approach.",1,"2022, 2022, 2021, 2020",Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts
LDPP26gk2M,"With the help of a specially curated probe dataset, this paper evaluates the numerical reasoning capabilities of language models (LMs) in the tabular Natural Language Inference (TNLI) task. Specifically, samples from five tabular datasets are converted to the <tabular premise,hypothesis> format and LMs are required to classify if the hypothesis entails or contradicts the table. Moreover, three perturbations are introduced to create adversarial examples or probe examples, such as altering the hypothesis while maintaining the label unchanged. Finally, the authors evaluate three types of LMs with the resulted probe dataset and give detailed experimental analyses. 

The contributions are two-folded. First, the authors introduce a hierarchical taxonomy for numerical reasoning skills and construct a probe dataset based on the proposed taxonomy. Secondly, the authors conduct extensive experiments to evaluate several LMs including SOTA LLMs such as FlanT5 and GPT-3.5.","(1) This paper focuses on the important numerical reasoning skills of LMs especially LLMs, which is a key necessity in many application scenarios.
(2) The authors introduces a new taxonomy for numerical reasoning skills together with a probe dataset for TNLI task.
(3) Comprehensive experiments are conducted across various LMs and several insightful findings are given. ","(1) In consideration of previous work [1], the contribution of this paper seems to be incremental. [1] proposed a taxonomy for numerical reasoning skills and it also introduced pertubations to construct adversarial (probe) examples, which is very similar with this paper. For instance, changing ""192"" to ""one hundred and ninety-two"", which is the same as the ""Numeration"" probes. As a result, this paper looks like an imitation of [1] with incremental adaptation.

(2) Some recent sota LMs are not included in the experiments. For example, the authors choose TAPAS and DeBERTa as representatives of ""LMs for tabular reasoning"". However, new Table pre-training models such as TAPEX[2] were proposed and can not be treated as contemporary works.

[1] Jialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, and Dongmei Zhang. 2022. Towards robust numerical question answering: Diagnosing numerical capabilities of NLP systems. EMNLP 2022.
[2] Liu Q, Chen B, Guo J, et al. TAPEX: Table pre-training via learning a neural SQL executor. ",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"(1) In the caption of Table 1, ""apprimation"" --> "" approximation"". The label of ""Date Flip"" probe example of H1 should be ""C"".
(2) In section 2, ""Figure 1"" should be ""Table 1"" or ""Table 3""?",1691735581278,,,EMNLP/2023/Conference,1qJgZUAc8j,"['EMNLP/2023/Conference/Submission1717/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461119432,[],4,,,"['everyone', 'EMNLP/2023/Conference/Submission1717/Reviewer_9VRp']",1qJgZUAc8j,['EMNLP/2023/Conference/Submission1717/Reviewer_9VRp'],1691735581278,1701461119432,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1717/Reviewer_9VRp']","Yes

The suggested papers or references not cited in the manuscript are:
1. Jialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, and Dongmei Zhang. 2022. Towards robust numerical question answering: Diagnosing numerical capabilities of NLP systems. EMNLP 2022.
2. Liu Q, Chen B, Guo J, et al. TAPEX: Table pre-training via learning a neural SQL executor.

These citations might be necessary because the reviewer suggests that the contributions of the paper seem incremental compared to previous work [1], and some recent state-of-the-art models, such as TAPEX [2], are not included in the experiments. The reviewer implies that the authors should be aware of and cite these relevant works to demonstrate the novelty and completeness of their research.",1,"2022, 2022",Towards robust numerical question answering Diagnosing numerical capabilities of NLP systems 
LDPP26gk2M,"With the help of a specially curated probe dataset, this paper evaluates the numerical reasoning capabilities of language models (LMs) in the tabular Natural Language Inference (TNLI) task. Specifically, samples from five tabular datasets are converted to the <tabular premise,hypothesis> format and LMs are required to classify if the hypothesis entails or contradicts the table. Moreover, three perturbations are introduced to create adversarial examples or probe examples, such as altering the hypothesis while maintaining the label unchanged. Finally, the authors evaluate three types of LMs with the resulted probe dataset and give detailed experimental analyses. 

The contributions are two-folded. First, the authors introduce a hierarchical taxonomy for numerical reasoning skills and construct a probe dataset based on the proposed taxonomy. Secondly, the authors conduct extensive experiments to evaluate several LMs including SOTA LLMs such as FlanT5 and GPT-3.5.","(1) This paper focuses on the important numerical reasoning skills of LMs especially LLMs, which is a key necessity in many application scenarios.
(2) The authors introduces a new taxonomy for numerical reasoning skills together with a probe dataset for TNLI task.
(3) Comprehensive experiments are conducted across various LMs and several insightful findings are given. ","(1) In consideration of previous work [1], the contribution of this paper seems to be incremental. [1] proposed a taxonomy for numerical reasoning skills and it also introduced pertubations to construct adversarial (probe) examples, which is very similar with this paper. For instance, changing ""192"" to ""one hundred and ninety-two"", which is the same as the ""Numeration"" probes. As a result, this paper looks like an imitation of [1] with incremental adaptation.

(2) Some recent sota LMs are not included in the experiments. For example, the authors choose TAPAS and DeBERTa as representatives of ""LMs for tabular reasoning"". However, new Table pre-training models such as TAPEX[2] were proposed and can not be treated as contemporary works.

[1] Jialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, and Dongmei Zhang. 2022. Towards robust numerical question answering: Diagnosing numerical capabilities of NLP systems. EMNLP 2022.
[2] Liu Q, Chen B, Guo J, et al. TAPEX: Table pre-training via learning a neural SQL executor. ",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"(1) In the caption of Table 1, ""apprimation"" --> "" approximation"". The label of ""Date Flip"" probe example of H1 should be ""C"".
(2) In section 2, ""Figure 1"" should be ""Table 1"" or ""Table 3""?",1691735581278,,,EMNLP/2023/Conference,1qJgZUAc8j,"['EMNLP/2023/Conference/Submission1717/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461119432,[],4,,,"['everyone', 'EMNLP/2023/Conference/Submission1717/Reviewer_9VRp']",1qJgZUAc8j,['EMNLP/2023/Conference/Submission1717/Reviewer_9VRp'],1691735581278,1701461119432,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1717/Reviewer_9VRp']","Yes

The suggested papers or references not cited in the manuscript are:
1. Jialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, and Dongmei Zhang. 2022. Towards robust numerical question answering: Diagnosing numerical capabilities of NLP systems. EMNLP 2022.
2. Liu Q, Chen B, Guo J, et al. TAPEX: Table pre-training via learning a neural SQL executor.

These citations might be necessary because the reviewer suggests that the contributions of the paper seem incremental compared to previous work [1], and some recent state-of-the-art models, such as TAPEX [2], are not included in the experiments. The reviewer implies that the authors should be aware of and cite these relevant works to demonstrate the novelty and completeness of their research.",1,"2022, 2022",TAPEX Table pre-training via learning a neural SQL executor
Dvb8ChBS58,"This paper takes a close look at membership inference (MI) attacks for large language models, focusing on summarization tasks. It's well-written, easy to read, and the definitions and notations are clear. The figures in the paper are well-designed, adding to the understanding of the topic.

","The authors present a simple but effective method for MI attacks that works specifically with summarization. Unlike previous methods that focused on classification tasks, this approach doesn't need scores or probabilities and can work with just black-box API access. The method can even be used without the summarized document, making it very useful for real-world situations.

The paper also includes extra tests and analysis, such as how well the method can be transferred to different setups. The authors have thought about how to protect against these kinds of attacks and the balance between keeping data private and making it useful. Overall, this is a solid paper that adds valuable insights to the field of natural language processing and information security. It's a good read for anyone interested in these subjects.","At times the main inference was hard to follow and justify logical jumps, however, the abundance of figures/tables covered those minor discrepancies.","1) Is it possible to extend the authors' method to perform membership inference attacks on autoregressive generative language models? I'm intrigued to explore whether some publicly available models, displaying impressive capabilities, were trained on legally questionable data sets, such as non-permissive codebases. 

2) Have the authors conducted experiments in this direction? If there is existing research pertaining to membership inference attacks on generative models, please consider adding it to the related work section.
",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691186088526,,,EMNLP/2023/Conference,1mGD6ZLTwv,"['EMNLP/2023/Conference/Submission886/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461065238,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission886/Reviewer_9rho']",1mGD6ZLTwv,['EMNLP/2023/Conference/Submission886/Reviewer_9rho'],1691186088526,1701461065238,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission886/Reviewer_9rho']","Yes

List of suggested papers or references:
* Research pertaining to membership inference attacks on generative models (not specified)

The reviewer suggests that the authors consider adding existing research on membership inference attacks on generative models to the related work section. This is because the reviewer is interested in exploring the possibility of extending the authors' method to autoregressive generative language models and wants to know if there are already studies in this direction. The addition of such references would provide a more comprehensive overview of the field and help to contextualize the authors' contributions.",1,,Research pertaining to membership inference attacks on generative models
Qqm4coK7m1,"This paper proposes a new method for obtaining sentence embeddings by combining certain layers of a BERT-based model. The authors address the problem of improving the quality of sentence embeddings, which are widely used in various natural language processing tasks such as text classification, information retrieval, and machine translation.","1. A new method for obtaining sentence embeddings by combining certain layers of a BERT-based model, which outperforms the baseline BERT on various tasks and data sets, achieving state-of-the-art results.
2. An analysis of the effect of different layers and combinations of layers on the quality of sentence embeddings, which provides insights into the inner workings of the BERT model and how it can be fine-tuned for specific tasks.
3. A comparison of the proposed method with other state-of-the-art methods for obtaining sentence embeddings, which shows that the proposed method is competitive and often outperforms them.
","1. The proposed method is based on a relatively simple modification of the BERT model, which may not be novel or significant enough to warrant publication in a top-tier conference like EMNLP.
2. The evaluation of the proposed method is limited to a few standard tasks and data sets, which may not be sufficient to demonstrate its effectiveness in real-world scenarios or to compare it with other state-of-the-art methods, mostly from 2022 and 2023.
3. The paper does not provide a detailed analysis of the interpretability of the proposed method, which could limit its usefulness in some applications where interpretability is important.
4. The paper does not compare the proposed method with other recent methods that have achieved state-of-the-art results on similar tasks, which could provide a more comprehensive evaluation of the proposed method's performance.
",,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"[1] Neelakantan A, Xu T, Puri R, et al. Text and code embeddings by contrastive pre-training[J]. arXiv preprint arXiv:2201.10005, 2022.",,1691032689672,,,EMNLP/2023/Conference,1kmIDTfQ4N,"['EMNLP/2023/Conference/Submission5383/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461345974,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5383/Reviewer_YvLi']",1kmIDTfQ4N,['EMNLP/2023/Conference/Submission5383/Reviewer_YvLi'],1691032689672,1701461345974,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5383/Reviewer_YvLi']","Yes

List of suggested papers or references not cited in the manuscript:
1. Neelakantan A, Xu T, Puri R, et al. Text and code embeddings by contrastive pre-training[J]. arXiv preprint arXiv:2201.10005, 2022.

These citations might be necessary because the reviewer mentions ""Missing_References"" and specifically lists this paper, implying that it is relevant to the topic of the manuscript and should be considered for citation. The reviewer also mentions that the paper does not compare the proposed method with other recent methods that have achieved state-of-the-art results on similar tasks, which suggests that the authors should be aware of and engage with the latest research in the field, including the referenced paper from 2022.",1,"2022, 2022, 2022",Text and code embeddings by contrastive pre-training
YnbKZOx8pH,"This paper proposes a three-stage approach to adapt multilingual encoders for generation tasks. During the first stage, the authors train soft prompts to allow a pre-trained encoder to extract representations; During the second stage, the authors train another series of soft prompts for the same encoder model to do alignment to the target sequence; During the third stage, train another series of soft prompts that iteratively refines the aligned sequence to the target sequence. Experiments on Machine Translation, Question Generation and Story Generation show that their method outperforms training decoder-only models from scratch and and naive soft prompt tuning on multilingual GPT models. ","1. The problem that this paper studies is important: how to modify encoder only models for generations as the non-autoregressive pre-train objective is hard to adapt for generation. The method is simple and generalizable to various tasks since we only need to train a few set of soft prompts.

2. The experiments show that their method can outperform simply initializing the encoder with a pre-trained model, direct training a encoder-decoder model on the task albeit training on much less parameters due to the parameter efficient fine-tuning. 

3. The generation speed is accelerated because compared to decoder only models which needs to pass the generated output many times through the model, encoder only model generates multiple tokens simultaneously and iteratively denoises them. ","1. This paper misses some important work in their discussion: The authors fail to compare with or at least cite some important work that utilizes pre-trained encoders for generation. e.g. BiBERT[1], which is the current state-of-the art on IWSLT 14 De-En translation. The authors also compare to mBART and show that although their method underperforms, they win in generation speed and trained parameters. I want to say that they should compare to doing prompt tuning on mBART as well to make it a fair comparison between number of trained parameters. If it outperforms, then it strengthens their claim. 

2. Motivation Unclear: The authors mention that an important reason to use encoders for generation is because of their speed. However, works [2] have demonstrated that standard encoder-decoder models can also be fast and comparable in performance by using a large encoder and a small decoder. On the other hand, if the argument is that encoder models excel in NLU tasks, then using Pattern Exploit Training ,encoder-decoder models can be comparable to standard encoder models in NLU tasks [3]. My point is, I want to see a more motivating argument for studying how to adapt encoder models for generation aside from performance and efficiency. 
 
3. The paper is not very well written. It does not have a clear distinction between model architecture (encoder, decoder, enc-dec) and training objective (autoregressive, auto-encoding). They use autoregressive (AR) and encoders interchangeably - which should be mentioned in the paper. Some of the facts need modifying: e.g. the authors mention that unlike AR models which can only generate from left to right, NAR models as the advantage of generating in arbitrary order. I don't think this is true since there has been work that generates with permuted orders with encoder-decoder models [4].


[1] BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation (Xu et al., EMNLP 2021)

[2] Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation (Kasai et al., ICLR 2021)

[3] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., JMLR 2020)

[4] GLM: General Language Model Pretraining with Autoregressive Blank Infilling (Du et al., ACL 2022) 
","First see the ""reasons to reject"" section.

A. Line 76. What does ""complicated"" mean right here? If there is a comparison of difficulty, please mention machine translation is complicated than what task. 

B. What is AT and NAT abbreviation for? Should it be AR and NAR?
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"See ""the reasons to reject"" section.","73: surprising latency -> surprisingly high latency

144: with only -> with only training on 

146: tge -> the

151: prompting based -> prompt tuning

217 Abbreviation ""SGA"" first introduced here without any introduction before
",1691014765195,,,EMNLP/2023/Conference,1iQMzgmKeD,"['EMNLP/2023/Conference/Submission2533/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170929,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2533/Reviewer_5rFu']",1iQMzgmKeD,['EMNLP/2023/Conference/Submission2533/Reviewer_5rFu'],1691014765195,1701461170929,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2533/Reviewer_5rFu']","Yes

Here are the suggested papers or references not cited in the manuscript:
1. BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation (Xu et al., EMNLP 2021)
2. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation (Kasai et al., ICLR 2021)
3. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., JMLR 2020)
4. GLM: General Language Model Pretraining with Autoregressive Blank Infilling (Du et al., ACL 2022)

These citations might be necessary because the reviewer believes the authors have missed important work in their discussion, particularly related to utilizing pre-trained encoders for generation and comparisons with existing state-of-the-art models. The reviewer suggests that including these references could strengthen the authors' claims and provide a more comprehensive understanding of the research area. Additionally, the reviewer points out that some of the facts mentioned in the paper need modification, and citing these references could help to clarify and correct these points.",1,"2021, 2021, 2020, 2022","BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation "
YnbKZOx8pH,"This paper proposes a three-stage approach to adapt multilingual encoders for generation tasks. During the first stage, the authors train soft prompts to allow a pre-trained encoder to extract representations; During the second stage, the authors train another series of soft prompts for the same encoder model to do alignment to the target sequence; During the third stage, train another series of soft prompts that iteratively refines the aligned sequence to the target sequence. Experiments on Machine Translation, Question Generation and Story Generation show that their method outperforms training decoder-only models from scratch and and naive soft prompt tuning on multilingual GPT models. ","1. The problem that this paper studies is important: how to modify encoder only models for generations as the non-autoregressive pre-train objective is hard to adapt for generation. The method is simple and generalizable to various tasks since we only need to train a few set of soft prompts.

2. The experiments show that their method can outperform simply initializing the encoder with a pre-trained model, direct training a encoder-decoder model on the task albeit training on much less parameters due to the parameter efficient fine-tuning. 

3. The generation speed is accelerated because compared to decoder only models which needs to pass the generated output many times through the model, encoder only model generates multiple tokens simultaneously and iteratively denoises them. ","1. This paper misses some important work in their discussion: The authors fail to compare with or at least cite some important work that utilizes pre-trained encoders for generation. e.g. BiBERT[1], which is the current state-of-the art on IWSLT 14 De-En translation. The authors also compare to mBART and show that although their method underperforms, they win in generation speed and trained parameters. I want to say that they should compare to doing prompt tuning on mBART as well to make it a fair comparison between number of trained parameters. If it outperforms, then it strengthens their claim. 

2. Motivation Unclear: The authors mention that an important reason to use encoders for generation is because of their speed. However, works [2] have demonstrated that standard encoder-decoder models can also be fast and comparable in performance by using a large encoder and a small decoder. On the other hand, if the argument is that encoder models excel in NLU tasks, then using Pattern Exploit Training ,encoder-decoder models can be comparable to standard encoder models in NLU tasks [3]. My point is, I want to see a more motivating argument for studying how to adapt encoder models for generation aside from performance and efficiency. 
 
3. The paper is not very well written. It does not have a clear distinction between model architecture (encoder, decoder, enc-dec) and training objective (autoregressive, auto-encoding). They use autoregressive (AR) and encoders interchangeably - which should be mentioned in the paper. Some of the facts need modifying: e.g. the authors mention that unlike AR models which can only generate from left to right, NAR models as the advantage of generating in arbitrary order. I don't think this is true since there has been work that generates with permuted orders with encoder-decoder models [4].


[1] BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation (Xu et al., EMNLP 2021)

[2] Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation (Kasai et al., ICLR 2021)

[3] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., JMLR 2020)

[4] GLM: General Language Model Pretraining with Autoregressive Blank Infilling (Du et al., ACL 2022) 
","First see the ""reasons to reject"" section.

A. Line 76. What does ""complicated"" mean right here? If there is a comparison of difficulty, please mention machine translation is complicated than what task. 

B. What is AT and NAT abbreviation for? Should it be AR and NAR?
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"See ""the reasons to reject"" section.","73: surprising latency -> surprisingly high latency

144: with only -> with only training on 

146: tge -> the

151: prompting based -> prompt tuning

217 Abbreviation ""SGA"" first introduced here without any introduction before
",1691014765195,,,EMNLP/2023/Conference,1iQMzgmKeD,"['EMNLP/2023/Conference/Submission2533/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170929,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2533/Reviewer_5rFu']",1iQMzgmKeD,['EMNLP/2023/Conference/Submission2533/Reviewer_5rFu'],1691014765195,1701461170929,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2533/Reviewer_5rFu']","Yes

Here are the suggested papers or references not cited in the manuscript:
1. BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation (Xu et al., EMNLP 2021)
2. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation (Kasai et al., ICLR 2021)
3. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., JMLR 2020)
4. GLM: General Language Model Pretraining with Autoregressive Blank Infilling (Du et al., ACL 2022)

These citations might be necessary because the reviewer believes the authors have missed important work in their discussion, particularly related to utilizing pre-trained encoders for generation and comparisons with existing state-of-the-art models. The reviewer suggests that including these references could strengthen the authors' claims and provide a more comprehensive understanding of the research area. Additionally, the reviewer points out that some of the facts mentioned in the paper need modification, and citing these references could help to clarify and correct these points.",1,"2021, 2021, 2020, 2022","Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation "
YnbKZOx8pH,"This paper proposes a three-stage approach to adapt multilingual encoders for generation tasks. During the first stage, the authors train soft prompts to allow a pre-trained encoder to extract representations; During the second stage, the authors train another series of soft prompts for the same encoder model to do alignment to the target sequence; During the third stage, train another series of soft prompts that iteratively refines the aligned sequence to the target sequence. Experiments on Machine Translation, Question Generation and Story Generation show that their method outperforms training decoder-only models from scratch and and naive soft prompt tuning on multilingual GPT models. ","1. The problem that this paper studies is important: how to modify encoder only models for generations as the non-autoregressive pre-train objective is hard to adapt for generation. The method is simple and generalizable to various tasks since we only need to train a few set of soft prompts.

2. The experiments show that their method can outperform simply initializing the encoder with a pre-trained model, direct training a encoder-decoder model on the task albeit training on much less parameters due to the parameter efficient fine-tuning. 

3. The generation speed is accelerated because compared to decoder only models which needs to pass the generated output many times through the model, encoder only model generates multiple tokens simultaneously and iteratively denoises them. ","1. This paper misses some important work in their discussion: The authors fail to compare with or at least cite some important work that utilizes pre-trained encoders for generation. e.g. BiBERT[1], which is the current state-of-the art on IWSLT 14 De-En translation. The authors also compare to mBART and show that although their method underperforms, they win in generation speed and trained parameters. I want to say that they should compare to doing prompt tuning on mBART as well to make it a fair comparison between number of trained parameters. If it outperforms, then it strengthens their claim. 

2. Motivation Unclear: The authors mention that an important reason to use encoders for generation is because of their speed. However, works [2] have demonstrated that standard encoder-decoder models can also be fast and comparable in performance by using a large encoder and a small decoder. On the other hand, if the argument is that encoder models excel in NLU tasks, then using Pattern Exploit Training ,encoder-decoder models can be comparable to standard encoder models in NLU tasks [3]. My point is, I want to see a more motivating argument for studying how to adapt encoder models for generation aside from performance and efficiency. 
 
3. The paper is not very well written. It does not have a clear distinction between model architecture (encoder, decoder, enc-dec) and training objective (autoregressive, auto-encoding). They use autoregressive (AR) and encoders interchangeably - which should be mentioned in the paper. Some of the facts need modifying: e.g. the authors mention that unlike AR models which can only generate from left to right, NAR models as the advantage of generating in arbitrary order. I don't think this is true since there has been work that generates with permuted orders with encoder-decoder models [4].


[1] BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation (Xu et al., EMNLP 2021)

[2] Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation (Kasai et al., ICLR 2021)

[3] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., JMLR 2020)

[4] GLM: General Language Model Pretraining with Autoregressive Blank Infilling (Du et al., ACL 2022) 
","First see the ""reasons to reject"" section.

A. Line 76. What does ""complicated"" mean right here? If there is a comparison of difficulty, please mention machine translation is complicated than what task. 

B. What is AT and NAT abbreviation for? Should it be AR and NAR?
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"See ""the reasons to reject"" section.","73: surprising latency -> surprisingly high latency

144: with only -> with only training on 

146: tge -> the

151: prompting based -> prompt tuning

217 Abbreviation ""SGA"" first introduced here without any introduction before
",1691014765195,,,EMNLP/2023/Conference,1iQMzgmKeD,"['EMNLP/2023/Conference/Submission2533/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170929,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2533/Reviewer_5rFu']",1iQMzgmKeD,['EMNLP/2023/Conference/Submission2533/Reviewer_5rFu'],1691014765195,1701461170929,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2533/Reviewer_5rFu']","Yes

Here are the suggested papers or references not cited in the manuscript:
1. BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation (Xu et al., EMNLP 2021)
2. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation (Kasai et al., ICLR 2021)
3. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., JMLR 2020)
4. GLM: General Language Model Pretraining with Autoregressive Blank Infilling (Du et al., ACL 2022)

These citations might be necessary because the reviewer believes the authors have missed important work in their discussion, particularly related to utilizing pre-trained encoders for generation and comparisons with existing state-of-the-art models. The reviewer suggests that including these references could strengthen the authors' claims and provide a more comprehensive understanding of the research area. Additionally, the reviewer points out that some of the facts mentioned in the paper need modification, and citing these references could help to clarify and correct these points.",1,"2021, 2021, 2020, 2022",Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 
YnbKZOx8pH,"This paper proposes a three-stage approach to adapt multilingual encoders for generation tasks. During the first stage, the authors train soft prompts to allow a pre-trained encoder to extract representations; During the second stage, the authors train another series of soft prompts for the same encoder model to do alignment to the target sequence; During the third stage, train another series of soft prompts that iteratively refines the aligned sequence to the target sequence. Experiments on Machine Translation, Question Generation and Story Generation show that their method outperforms training decoder-only models from scratch and and naive soft prompt tuning on multilingual GPT models. ","1. The problem that this paper studies is important: how to modify encoder only models for generations as the non-autoregressive pre-train objective is hard to adapt for generation. The method is simple and generalizable to various tasks since we only need to train a few set of soft prompts.

2. The experiments show that their method can outperform simply initializing the encoder with a pre-trained model, direct training a encoder-decoder model on the task albeit training on much less parameters due to the parameter efficient fine-tuning. 

3. The generation speed is accelerated because compared to decoder only models which needs to pass the generated output many times through the model, encoder only model generates multiple tokens simultaneously and iteratively denoises them. ","1. This paper misses some important work in their discussion: The authors fail to compare with or at least cite some important work that utilizes pre-trained encoders for generation. e.g. BiBERT[1], which is the current state-of-the art on IWSLT 14 De-En translation. The authors also compare to mBART and show that although their method underperforms, they win in generation speed and trained parameters. I want to say that they should compare to doing prompt tuning on mBART as well to make it a fair comparison between number of trained parameters. If it outperforms, then it strengthens their claim. 

2. Motivation Unclear: The authors mention that an important reason to use encoders for generation is because of their speed. However, works [2] have demonstrated that standard encoder-decoder models can also be fast and comparable in performance by using a large encoder and a small decoder. On the other hand, if the argument is that encoder models excel in NLU tasks, then using Pattern Exploit Training ,encoder-decoder models can be comparable to standard encoder models in NLU tasks [3]. My point is, I want to see a more motivating argument for studying how to adapt encoder models for generation aside from performance and efficiency. 
 
3. The paper is not very well written. It does not have a clear distinction between model architecture (encoder, decoder, enc-dec) and training objective (autoregressive, auto-encoding). They use autoregressive (AR) and encoders interchangeably - which should be mentioned in the paper. Some of the facts need modifying: e.g. the authors mention that unlike AR models which can only generate from left to right, NAR models as the advantage of generating in arbitrary order. I don't think this is true since there has been work that generates with permuted orders with encoder-decoder models [4].


[1] BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation (Xu et al., EMNLP 2021)

[2] Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation (Kasai et al., ICLR 2021)

[3] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., JMLR 2020)

[4] GLM: General Language Model Pretraining with Autoregressive Blank Infilling (Du et al., ACL 2022) 
","First see the ""reasons to reject"" section.

A. Line 76. What does ""complicated"" mean right here? If there is a comparison of difficulty, please mention machine translation is complicated than what task. 

B. What is AT and NAT abbreviation for? Should it be AR and NAR?
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"See ""the reasons to reject"" section.","73: surprising latency -> surprisingly high latency

144: with only -> with only training on 

146: tge -> the

151: prompting based -> prompt tuning

217 Abbreviation ""SGA"" first introduced here without any introduction before
",1691014765195,,,EMNLP/2023/Conference,1iQMzgmKeD,"['EMNLP/2023/Conference/Submission2533/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170929,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2533/Reviewer_5rFu']",1iQMzgmKeD,['EMNLP/2023/Conference/Submission2533/Reviewer_5rFu'],1691014765195,1701461170929,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2533/Reviewer_5rFu']","Yes

Here are the suggested papers or references not cited in the manuscript:
1. BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation (Xu et al., EMNLP 2021)
2. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation (Kasai et al., ICLR 2021)
3. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., JMLR 2020)
4. GLM: General Language Model Pretraining with Autoregressive Blank Infilling (Du et al., ACL 2022)

These citations might be necessary because the reviewer believes the authors have missed important work in their discussion, particularly related to utilizing pre-trained encoders for generation and comparisons with existing state-of-the-art models. The reviewer suggests that including these references could strengthen the authors' claims and provide a more comprehensive understanding of the research area. Additionally, the reviewer points out that some of the facts mentioned in the paper need modification, and citing these references could help to clarify and correct these points.",1,"2021, 2021, 2020, 2022",GLM: General Language Model Pretraining with Autoregressive Blank Infilling
vpv3PSCSZG,"This paper studies test-time fine-tuning with unlabeled test data. The initial model is the one tuned to follow instructions. In this paper, it uses FLAN-series models. 

The initial model will be fined tuned with unlabeled test data, which is a transductive-learning setup. In the method, the initial model has generate pseudo labels on the test data, then the pseudo labels will be used to fine-tune the model. 

The paper proposes a method which samples several pseudo labels then filter the noisy labels, to make sure the training more robust.

","1. The setting is practical and reasonable. 
Transductive learning should be very useful in practice since it can explore the information of the test data, so it can find out more correct labels on the test data, though it is expensive to refine the model. 

2. The method is sound with enough experimental results, given it is a short paper. 

","Though the setting is practical and reasonable, the novelty seems to be limited. 
Test-time fine-tuning or adaptation has been studied for NLP tasks in the past few years, so the setting studied in this work is not new.

Related work: 
[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension. 

","For the experiments, have you run multiple times for each baseline? ",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension. ",,1691168603730,,,EMNLP/2023/Conference,1cKjvlvR7Z,"['EMNLP/2023/Conference/Submission3896/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461258971,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3896/Reviewer_ruV4']",1cKjvlvR7Z,['EMNLP/2023/Conference/Submission3896/Reviewer_ruV4'],1691168603730,1701461258971,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3896/Reviewer_ruV4']","Yes

The suggested papers or references not cited in the manuscript are:
[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension.

These citations might be necessary because the reviewer mentions that ""Test-time fine-tuning or adaptation has been studied for NLP tasks in the past few years, so the setting studied in this work is not new."" This implies that the authors could have drawn from existing research in this area, and the suggested papers may provide relevant context or comparisons to the authors' work. The fact that the reviewer lists these papers under both ""Related work"" and ""Missing_References"" further emphasizes their potential importance to the manuscript.",1,,Robust Question Answering against Distribution Shifts with Test-Time Adaptation An Empirical Study
vpv3PSCSZG,"This paper studies test-time fine-tuning with unlabeled test data. The initial model is the one tuned to follow instructions. In this paper, it uses FLAN-series models. 

The initial model will be fined tuned with unlabeled test data, which is a transductive-learning setup. In the method, the initial model has generate pseudo labels on the test data, then the pseudo labels will be used to fine-tune the model. 

The paper proposes a method which samples several pseudo labels then filter the noisy labels, to make sure the training more robust.

","1. The setting is practical and reasonable. 
Transductive learning should be very useful in practice since it can explore the information of the test data, so it can find out more correct labels on the test data, though it is expensive to refine the model. 

2. The method is sound with enough experimental results, given it is a short paper. 

","Though the setting is practical and reasonable, the novelty seems to be limited. 
Test-time fine-tuning or adaptation has been studied for NLP tasks in the past few years, so the setting studied in this work is not new.

Related work: 
[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension. 

","For the experiments, have you run multiple times for each baseline? ",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension. ",,1691168603730,,,EMNLP/2023/Conference,1cKjvlvR7Z,"['EMNLP/2023/Conference/Submission3896/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461258971,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3896/Reviewer_ruV4']",1cKjvlvR7Z,['EMNLP/2023/Conference/Submission3896/Reviewer_ruV4'],1691168603730,1701461258971,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3896/Reviewer_ruV4']","Yes

The suggested papers or references not cited in the manuscript are:
[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension.

These citations might be necessary because the reviewer mentions that ""Test-time fine-tuning or adaptation has been studied for NLP tasks in the past few years, so the setting studied in this work is not new."" This implies that the authors could have drawn from existing research in this area, and the suggested papers may provide relevant context or comparisons to the authors' work. The fact that the reviewer lists these papers under both ""Related work"" and ""Missing_References"" further emphasizes their potential importance to the manuscript.",1,,Efficient test time adapter ensembling for low-resource language varieties
vpv3PSCSZG,"This paper studies test-time fine-tuning with unlabeled test data. The initial model is the one tuned to follow instructions. In this paper, it uses FLAN-series models. 

The initial model will be fined tuned with unlabeled test data, which is a transductive-learning setup. In the method, the initial model has generate pseudo labels on the test data, then the pseudo labels will be used to fine-tune the model. 

The paper proposes a method which samples several pseudo labels then filter the noisy labels, to make sure the training more robust.

","1. The setting is practical and reasonable. 
Transductive learning should be very useful in practice since it can explore the information of the test data, so it can find out more correct labels on the test data, though it is expensive to refine the model. 

2. The method is sound with enough experimental results, given it is a short paper. 

","Though the setting is practical and reasonable, the novelty seems to be limited. 
Test-time fine-tuning or adaptation has been studied for NLP tasks in the past few years, so the setting studied in this work is not new.

Related work: 
[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension. 

","For the experiments, have you run multiple times for each baseline? ",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension. ",,1691168603730,,,EMNLP/2023/Conference,1cKjvlvR7Z,"['EMNLP/2023/Conference/Submission3896/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461258971,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3896/Reviewer_ruV4']",1cKjvlvR7Z,['EMNLP/2023/Conference/Submission3896/Reviewer_ruV4'],1691168603730,1701461258971,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3896/Reviewer_ruV4']","Yes

The suggested papers or references not cited in the manuscript are:
[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension.

These citations might be necessary because the reviewer mentions that ""Test-time fine-tuning or adaptation has been studied for NLP tasks in the past few years, so the setting studied in this work is not new."" This implies that the authors could have drawn from existing research in this area, and the suggested papers may provide relevant context or comparisons to the authors' work. The fact that the reviewer lists these papers under both ""Related work"" and ""Missing_References"" further emphasizes their potential importance to the manuscript.",1,,PADA Example-based prompt learning for on-thefly adaptation to unseen domains
vpv3PSCSZG,"This paper studies test-time fine-tuning with unlabeled test data. The initial model is the one tuned to follow instructions. In this paper, it uses FLAN-series models. 

The initial model will be fined tuned with unlabeled test data, which is a transductive-learning setup. In the method, the initial model has generate pseudo labels on the test data, then the pseudo labels will be used to fine-tune the model. 

The paper proposes a method which samples several pseudo labels then filter the noisy labels, to make sure the training more robust.

","1. The setting is practical and reasonable. 
Transductive learning should be very useful in practice since it can explore the information of the test data, so it can find out more correct labels on the test data, though it is expensive to refine the model. 

2. The method is sound with enough experimental results, given it is a short paper. 

","Though the setting is practical and reasonable, the novelty seems to be limited. 
Test-time fine-tuning or adaptation has been studied for NLP tasks in the past few years, so the setting studied in this work is not new.

Related work: 
[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension. 

","For the experiments, have you run multiple times for each baseline? ",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension. ",,1691168603730,,,EMNLP/2023/Conference,1cKjvlvR7Z,"['EMNLP/2023/Conference/Submission3896/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461258971,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3896/Reviewer_ruV4']",1cKjvlvR7Z,['EMNLP/2023/Conference/Submission3896/Reviewer_ruV4'],1691168603730,1701461258971,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3896/Reviewer_ruV4']","Yes

The suggested papers or references not cited in the manuscript are:
[1] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study
[2] Efficient test time adapter ensembling for low-resource language varieties. 
[3] PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. 
[4] Self-supervised test-time learning for reading comprehension.

These citations might be necessary because the reviewer mentions that ""Test-time fine-tuning or adaptation has been studied for NLP tasks in the past few years, so the setting studied in this work is not new."" This implies that the authors could have drawn from existing research in this area, and the suggested papers may provide relevant context or comparisons to the authors' work. The fact that the reviewer lists these papers under both ""Related work"" and ""Missing_References"" further emphasizes their potential importance to the manuscript.",1,,Self-supervised test-time learning for reading comprehension
wQ8ICEpBiv,"The authors present a prompting that utilizes demonstration-like retrieval. I call it ""demonstration-like"" because what is included in the prompt is derived from a demonstration (called relevant experience). Although a potentially novel technique, I found the method is talked about quite abstractly and the paper is missing details that would be necessary for replication. In addition, although the paper includes multiple ablations there are not sufficient baselines to know if this is an overall promising technique. As an example, at least one prompting baseline is missing for CLUTTR which greatly outperforms ExpNote.","Strength 1. It is interesting to include ""experiences"" which are derived from demonstrations rather than demonstrations themselves. That being said, this has been done before with chain-of-thought.

Strength 2. The paper surprisingly shows that using experiences where the model failed (called negatives) is very helpful. Although, on closer inspection, this may be because the failure rate is high and that too much data is excluded when excluding failures.

Strength 3. ExpNote substantially outperforms ablations and two baselines (TeachMe and Reflexion). Although some important prompting baselines may be missing.","Reject 1. There are probably not enough details to reproduce the work without guessing at multiple steps. For example, it is not clear what is a ""word-based retriever"".

Reject 2. ExpNote probably needs negatives for good performance because so many of the initial predictions are negatives. The positives are only 70% in the best case (EMOJI) and can be less than 50% on some data. Plus the data is somewhat small to begin with, making the inclusion of as much of the train data as possible important.

Reject 3. It seems the baselines are not particularly strong. For example, DIVERSE can achieve > 90% accuracy on CLUTTR [1].","Did you consider retrieving relevant demonstrations rather than only using a fixed set of demonstrations? In other words, is it clear that using demonstration as ""experiences"" is more important than simply using the raw demonstrations?","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] Making LLMs Better Reasoners with Step-Aware Verifier by Li et al. 2022 presents a technique for finding more reliable CoT reasoning paths. It has strong results on CLUTTR.

[2] Compositional Semantic Parsing with LLMs by Drozdov et al. 2022 uses demonstrations that include their pre-computed CoT. Also, the demonstration retrieval involves template-like steps that have some similarity with experiences used in ExpNote.

[3] Faithful CoT (Lyu et al) shows that adding explanations to CoT is beneficial.",Why are no decimals shown in Table 1/2? Is the full data being used or is this a sample of 100 test cases?,1691207266721,,,EMNLP/2023/Conference,1Xht3SKAoY,"['EMNLP/2023/Conference/Submission866/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461062547,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission866/Reviewer_ZAJN']",1Xht3SKAoY,['EMNLP/2023/Conference/Submission866/Reviewer_ZAJN'],1691207266721,1701461062547,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission866/Reviewer_ZAJN']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Making LLMs Better Reasoners with Step-Aware Verifier"" by Li et al. (2022)
2. ""Compositional Semantic Parsing with LLMs"" by Drozdov et al. (2022)
3. ""Faithful CoT"" by Lyu et al.

These citations might be necessary because the reviewer mentions that the baselines used in the paper are not particularly strong, and references like ""Making LLMs Better Reasoners with Step-Aware Verifier"" achieve > 90% accuracy on CLUTTR, which could be a relevant comparison. Additionally, ""Compositional Semantic Parsing with LLMs"" and ""Faithful CoT"" seem to use similar techniques, such as demonstrations with pre-computed chain-of-thought and adding explanations to chain-of-thought, which could be relevant to the authors' work on using ""experiences"" derived from demonstrations. The reviewer suggests that including these references could provide a more comprehensive understanding of the authors' contributions and the broader context of their research.",1,"2022, 2022",Making LLMs Better Reasoners with Step-Aware Verifier 
wQ8ICEpBiv,"The authors present a prompting that utilizes demonstration-like retrieval. I call it ""demonstration-like"" because what is included in the prompt is derived from a demonstration (called relevant experience). Although a potentially novel technique, I found the method is talked about quite abstractly and the paper is missing details that would be necessary for replication. In addition, although the paper includes multiple ablations there are not sufficient baselines to know if this is an overall promising technique. As an example, at least one prompting baseline is missing for CLUTTR which greatly outperforms ExpNote.","Strength 1. It is interesting to include ""experiences"" which are derived from demonstrations rather than demonstrations themselves. That being said, this has been done before with chain-of-thought.

Strength 2. The paper surprisingly shows that using experiences where the model failed (called negatives) is very helpful. Although, on closer inspection, this may be because the failure rate is high and that too much data is excluded when excluding failures.

Strength 3. ExpNote substantially outperforms ablations and two baselines (TeachMe and Reflexion). Although some important prompting baselines may be missing.","Reject 1. There are probably not enough details to reproduce the work without guessing at multiple steps. For example, it is not clear what is a ""word-based retriever"".

Reject 2. ExpNote probably needs negatives for good performance because so many of the initial predictions are negatives. The positives are only 70% in the best case (EMOJI) and can be less than 50% on some data. Plus the data is somewhat small to begin with, making the inclusion of as much of the train data as possible important.

Reject 3. It seems the baselines are not particularly strong. For example, DIVERSE can achieve > 90% accuracy on CLUTTR [1].","Did you consider retrieving relevant demonstrations rather than only using a fixed set of demonstrations? In other words, is it clear that using demonstration as ""experiences"" is more important than simply using the raw demonstrations?","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] Making LLMs Better Reasoners with Step-Aware Verifier by Li et al. 2022 presents a technique for finding more reliable CoT reasoning paths. It has strong results on CLUTTR.

[2] Compositional Semantic Parsing with LLMs by Drozdov et al. 2022 uses demonstrations that include their pre-computed CoT. Also, the demonstration retrieval involves template-like steps that have some similarity with experiences used in ExpNote.

[3] Faithful CoT (Lyu et al) shows that adding explanations to CoT is beneficial.",Why are no decimals shown in Table 1/2? Is the full data being used or is this a sample of 100 test cases?,1691207266721,,,EMNLP/2023/Conference,1Xht3SKAoY,"['EMNLP/2023/Conference/Submission866/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461062547,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission866/Reviewer_ZAJN']",1Xht3SKAoY,['EMNLP/2023/Conference/Submission866/Reviewer_ZAJN'],1691207266721,1701461062547,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission866/Reviewer_ZAJN']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Making LLMs Better Reasoners with Step-Aware Verifier"" by Li et al. (2022)
2. ""Compositional Semantic Parsing with LLMs"" by Drozdov et al. (2022)
3. ""Faithful CoT"" by Lyu et al.

These citations might be necessary because the reviewer mentions that the baselines used in the paper are not particularly strong, and references like ""Making LLMs Better Reasoners with Step-Aware Verifier"" achieve > 90% accuracy on CLUTTR, which could be a relevant comparison. Additionally, ""Compositional Semantic Parsing with LLMs"" and ""Faithful CoT"" seem to use similar techniques, such as demonstrations with pre-computed chain-of-thought and adding explanations to chain-of-thought, which could be relevant to the authors' work on using ""experiences"" derived from demonstrations. The reviewer suggests that including these references could provide a more comprehensive understanding of the authors' contributions and the broader context of their research.",1,"2022, 2022",Compositional Semantic Parsing with LLMs 
wQ8ICEpBiv,"The authors present a prompting that utilizes demonstration-like retrieval. I call it ""demonstration-like"" because what is included in the prompt is derived from a demonstration (called relevant experience). Although a potentially novel technique, I found the method is talked about quite abstractly and the paper is missing details that would be necessary for replication. In addition, although the paper includes multiple ablations there are not sufficient baselines to know if this is an overall promising technique. As an example, at least one prompting baseline is missing for CLUTTR which greatly outperforms ExpNote.","Strength 1. It is interesting to include ""experiences"" which are derived from demonstrations rather than demonstrations themselves. That being said, this has been done before with chain-of-thought.

Strength 2. The paper surprisingly shows that using experiences where the model failed (called negatives) is very helpful. Although, on closer inspection, this may be because the failure rate is high and that too much data is excluded when excluding failures.

Strength 3. ExpNote substantially outperforms ablations and two baselines (TeachMe and Reflexion). Although some important prompting baselines may be missing.","Reject 1. There are probably not enough details to reproduce the work without guessing at multiple steps. For example, it is not clear what is a ""word-based retriever"".

Reject 2. ExpNote probably needs negatives for good performance because so many of the initial predictions are negatives. The positives are only 70% in the best case (EMOJI) and can be less than 50% on some data. Plus the data is somewhat small to begin with, making the inclusion of as much of the train data as possible important.

Reject 3. It seems the baselines are not particularly strong. For example, DIVERSE can achieve > 90% accuracy on CLUTTR [1].","Did you consider retrieving relevant demonstrations rather than only using a fixed set of demonstrations? In other words, is it clear that using demonstration as ""experiences"" is more important than simply using the raw demonstrations?","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] Making LLMs Better Reasoners with Step-Aware Verifier by Li et al. 2022 presents a technique for finding more reliable CoT reasoning paths. It has strong results on CLUTTR.

[2] Compositional Semantic Parsing with LLMs by Drozdov et al. 2022 uses demonstrations that include their pre-computed CoT. Also, the demonstration retrieval involves template-like steps that have some similarity with experiences used in ExpNote.

[3] Faithful CoT (Lyu et al) shows that adding explanations to CoT is beneficial.",Why are no decimals shown in Table 1/2? Is the full data being used or is this a sample of 100 test cases?,1691207266721,,,EMNLP/2023/Conference,1Xht3SKAoY,"['EMNLP/2023/Conference/Submission866/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461062547,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission866/Reviewer_ZAJN']",1Xht3SKAoY,['EMNLP/2023/Conference/Submission866/Reviewer_ZAJN'],1691207266721,1701461062547,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission866/Reviewer_ZAJN']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Making LLMs Better Reasoners with Step-Aware Verifier"" by Li et al. (2022)
2. ""Compositional Semantic Parsing with LLMs"" by Drozdov et al. (2022)
3. ""Faithful CoT"" by Lyu et al.

These citations might be necessary because the reviewer mentions that the baselines used in the paper are not particularly strong, and references like ""Making LLMs Better Reasoners with Step-Aware Verifier"" achieve > 90% accuracy on CLUTTR, which could be a relevant comparison. Additionally, ""Compositional Semantic Parsing with LLMs"" and ""Faithful CoT"" seem to use similar techniques, such as demonstrations with pre-computed chain-of-thought and adding explanations to chain-of-thought, which could be relevant to the authors' work on using ""experiences"" derived from demonstrations. The reviewer suggests that including these references could provide a more comprehensive understanding of the authors' contributions and the broader context of their research.",1,"2022, 2022",Faithful CoT
e1CqCpHMV9,"The paper addresses the disparity in sentiment classification within the financial domain by introducing the FinEntity dataset. This dataset annotates financial entities and their corresponding sentiments in financial news, facilitating entity-level sentiment classification. Pre-trained language models fine-tuned with FinEntity outperform zero-/few-shot ChatGPT models in this task. The practical utility of the dataset is demonstrated through a case study on the cryptocurrency market, showcasing stronger correlations between cryptocurrency prices and inferred sentiment. An LSTM model incorporating entity-level sentiment features shows improved accuracy in Bitcoin price prediction. Overall, the FinEntity dataset offers a valuable resource for accurate financial sentiment analysis systems.","1. Clear Documentation: The paper provides transparent and detailed documentation of the data collection and sentiment annotation process, allowing easy dataset reproduction and potential expansion by other researchers.
2. Objective Sentiment Annotation: The method used to assign sentiment labels to each financial entity appears to be objective, ensuring consistency in the dataset construction process.","1. Lack of Innovation: The dataset construction method employed in the paper is relatively simple and lacks innovation, leading to concerns about the originality and novelty of the research.
2. Experimental Report Style: The paper's approach and presentation make it resemble more of an experimental report rather than a research paper, potentially undermining its overall contribution to the field.
3. Ambiguity in Dataset Size: In section 5.1, it is not explicitly clear whether the dataset size used for sequence-level and entity-level models is the same. The distinction between the two models in terms of data utilization needs clarification to ensure a fair comparison.","In section 5.1, the entity-level approach used in the paper is not explicitly clarified regarding the grouping of entities. It is unclear whether entities are grouped based on the target words utilized in the sequence-level approach or if the grouping and aggregation of sentiments are done manually. Additional information on the entity-level approach's methodology would help to understand the process better and ensure the validity of the comparison with the sequence-level approach.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Du, Chi-Han, Ming-Feng Tsai, and Chuan-Ju Wang. ""Beyond word-level to sentence-level sentiment analysis for financial reports."" ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019.

Lin, Sheng-Chieh, et al. ""Self-attentive sentimental sentence embedding for sentiment analysis."" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020.",none,1691117884368,,,EMNLP/2023/Conference,1WJoJPXwiG,"['EMNLP/2023/Conference/Submission1581/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461112386,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1581/Reviewer_NjN2']",1WJoJPXwiG,['EMNLP/2023/Conference/Submission1581/Reviewer_NjN2'],1691117884368,1701461112386,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1581/Reviewer_NjN2']","Yes

The suggested papers or references not cited in the manuscript are:
1. Du, Chi-Han, Ming-Feng Tsai, and Chuan-Ju Wang. ""Beyond word-level to sentence-level sentiment analysis for financial reports."" ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019.
2. Lin, Sheng-Chieh, et al. ""Self-attentive sentimental sentence embedding for sentiment analysis."" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020.

These citations might be necessary because they appear to be related to the topic of sentiment analysis in financial reports, which is the focus of the manuscript. The reviewer likely suggests including them to provide a more comprehensive overview of existing research in the field, potentially strengthening the manuscript's literature review and positioning its contributions within the broader context of sentiment analysis research.",1,"2019, 2019, 2019, 2020, 2020, 2020",Beyond word-level to sentence-level sentiment analysis for financial reports
e1CqCpHMV9,"The paper addresses the disparity in sentiment classification within the financial domain by introducing the FinEntity dataset. This dataset annotates financial entities and their corresponding sentiments in financial news, facilitating entity-level sentiment classification. Pre-trained language models fine-tuned with FinEntity outperform zero-/few-shot ChatGPT models in this task. The practical utility of the dataset is demonstrated through a case study on the cryptocurrency market, showcasing stronger correlations between cryptocurrency prices and inferred sentiment. An LSTM model incorporating entity-level sentiment features shows improved accuracy in Bitcoin price prediction. Overall, the FinEntity dataset offers a valuable resource for accurate financial sentiment analysis systems.","1. Clear Documentation: The paper provides transparent and detailed documentation of the data collection and sentiment annotation process, allowing easy dataset reproduction and potential expansion by other researchers.
2. Objective Sentiment Annotation: The method used to assign sentiment labels to each financial entity appears to be objective, ensuring consistency in the dataset construction process.","1. Lack of Innovation: The dataset construction method employed in the paper is relatively simple and lacks innovation, leading to concerns about the originality and novelty of the research.
2. Experimental Report Style: The paper's approach and presentation make it resemble more of an experimental report rather than a research paper, potentially undermining its overall contribution to the field.
3. Ambiguity in Dataset Size: In section 5.1, it is not explicitly clear whether the dataset size used for sequence-level and entity-level models is the same. The distinction between the two models in terms of data utilization needs clarification to ensure a fair comparison.","In section 5.1, the entity-level approach used in the paper is not explicitly clarified regarding the grouping of entities. It is unclear whether entities are grouped based on the target words utilized in the sequence-level approach or if the grouping and aggregation of sentiments are done manually. Additional information on the entity-level approach's methodology would help to understand the process better and ensure the validity of the comparison with the sequence-level approach.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Du, Chi-Han, Ming-Feng Tsai, and Chuan-Ju Wang. ""Beyond word-level to sentence-level sentiment analysis for financial reports."" ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019.

Lin, Sheng-Chieh, et al. ""Self-attentive sentimental sentence embedding for sentiment analysis."" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020.",none,1691117884368,,,EMNLP/2023/Conference,1WJoJPXwiG,"['EMNLP/2023/Conference/Submission1581/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461112386,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1581/Reviewer_NjN2']",1WJoJPXwiG,['EMNLP/2023/Conference/Submission1581/Reviewer_NjN2'],1691117884368,1701461112386,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1581/Reviewer_NjN2']","Yes

The suggested papers or references not cited in the manuscript are:
1. Du, Chi-Han, Ming-Feng Tsai, and Chuan-Ju Wang. ""Beyond word-level to sentence-level sentiment analysis for financial reports."" ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019.
2. Lin, Sheng-Chieh, et al. ""Self-attentive sentimental sentence embedding for sentiment analysis."" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020.

These citations might be necessary because they appear to be related to the topic of sentiment analysis in financial reports, which is the focus of the manuscript. The reviewer likely suggests including them to provide a more comprehensive overview of existing research in the field, potentially strengthening the manuscript's literature review and positioning its contributions within the broader context of sentiment analysis research.",1,"2019, 2019, 2019, 2020, 2020, 2020",Self-attentive sentimental sentence embedding for sentiment analysis
tjQj1OfYyE,"In this work, the authors introduce a new entity-level sentiment analysis classification dataset focused on Financial Texts. Entity-level sentiment analysis (SA) is a branch of SA where sentiment is associated with specific entities in the text and is not just an overall sentiment classification common in sentence-level sentiment analysis. The authors describe step-by-step how they collected their data and annotated their dataset. Moreover, they propose a specific classification model (FinBERT-CRF) and show that it outperforms other transformed-based models. In a case example, the authors also demonstrate how one may use their dataset to build a model that predicts cryptocurrency prices better than without using the model.","- The dataset created by the authors is interesting, and it is a clear contribution to the community when publicly available, especially since the entity-level dataset is much more scarce than the sentence-level.
- The authors demonstrate care and attention to quality when creating their dataset. Also, they demonstrate a pertinent application of their dataset in a real-world scenario for stock prices forecast.
- The paper is quite direct and easy to follow.","- There is no Ethical Authors did not provide a dedicated ""Ethics Statement"". My main concern is related to the use of 12 undergraduate students for labeling their dataset. It is not clear if this comply with good scientific ethics standards.
- The authors does not carefully state the potential problems and even financial risks one may have if they try do implement and use their approach to forecast currency prices.
- There are overstatements such ""no such public dataset currently exists"". There are many other entity-level datasets, including from SemEval-2022. Authors should clarify who their dataset are unique.",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",Yes,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","Authors did not provide a dedicated ""Ethics Statement"". My main concern is related to the use of 12 undergraduate students for labeling their dataset. It is not clear if this comply with good scientific ethics standards.",,"- Provide references of using conditional random field layer with BERT.
- In table 1 caption, state which metric the numbers mean, is it accuracy? F1 Score?",1691185566619,,,EMNLP/2023/Conference,1WJoJPXwiG,"['EMNLP/2023/Conference/Submission1581/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461112308,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission1581/Reviewer_R2TV']",1WJoJPXwiG,['EMNLP/2023/Conference/Submission1581/Reviewer_R2TV'],1691185566619,1701461112308,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1581/Reviewer_R2TV']","Yes

List of suggested papers or references:
- References for using conditional random field layer with BERT
- SemEval-2022 entity-level datasets

These citations might be necessary because the reviewer mentions that the authors should provide references for using a conditional random field layer with BERT, indicating a gap in the literature review. Additionally, the reviewer corrects the authors' claim that ""no such public dataset currently exists"" by mentioning SemEval-2022 entity-level datasets, suggesting that the authors should be aware of and cite these existing datasets to clarify the uniqueness of their own dataset.",1,"2022, 2022",References for using conditional random field layer with BERT 
tjQj1OfYyE,"In this work, the authors introduce a new entity-level sentiment analysis classification dataset focused on Financial Texts. Entity-level sentiment analysis (SA) is a branch of SA where sentiment is associated with specific entities in the text and is not just an overall sentiment classification common in sentence-level sentiment analysis. The authors describe step-by-step how they collected their data and annotated their dataset. Moreover, they propose a specific classification model (FinBERT-CRF) and show that it outperforms other transformed-based models. In a case example, the authors also demonstrate how one may use their dataset to build a model that predicts cryptocurrency prices better than without using the model.","- The dataset created by the authors is interesting, and it is a clear contribution to the community when publicly available, especially since the entity-level dataset is much more scarce than the sentence-level.
- The authors demonstrate care and attention to quality when creating their dataset. Also, they demonstrate a pertinent application of their dataset in a real-world scenario for stock prices forecast.
- The paper is quite direct and easy to follow.","- There is no Ethical Authors did not provide a dedicated ""Ethics Statement"". My main concern is related to the use of 12 undergraduate students for labeling their dataset. It is not clear if this comply with good scientific ethics standards.
- The authors does not carefully state the potential problems and even financial risks one may have if they try do implement and use their approach to forecast currency prices.
- There are overstatements such ""no such public dataset currently exists"". There are many other entity-level datasets, including from SemEval-2022. Authors should clarify who their dataset are unique.",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",Yes,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","Authors did not provide a dedicated ""Ethics Statement"". My main concern is related to the use of 12 undergraduate students for labeling their dataset. It is not clear if this comply with good scientific ethics standards.",,"- Provide references of using conditional random field layer with BERT.
- In table 1 caption, state which metric the numbers mean, is it accuracy? F1 Score?",1691185566619,,,EMNLP/2023/Conference,1WJoJPXwiG,"['EMNLP/2023/Conference/Submission1581/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461112308,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission1581/Reviewer_R2TV']",1WJoJPXwiG,['EMNLP/2023/Conference/Submission1581/Reviewer_R2TV'],1691185566619,1701461112308,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1581/Reviewer_R2TV']","Yes

List of suggested papers or references:
- References for using conditional random field layer with BERT
- SemEval-2022 entity-level datasets

These citations might be necessary because the reviewer mentions that the authors should provide references for using a conditional random field layer with BERT, indicating a gap in the literature review. Additionally, the reviewer corrects the authors' claim that ""no such public dataset currently exists"" by mentioning SemEval-2022 entity-level datasets, suggesting that the authors should be aware of and cite these existing datasets to clarify the uniqueness of their own dataset.",1,"2022, 2022",SemEval-2022 entity-level datasets
UenKAJre5E,"This paper presents a study of PEFT in financial domain tasks. It addresses two research questions: 1. how do PEFTs perform on financial tasks? 2. what are the pros and cons of using PEFTs? The author conducts adapter-tuning and LoRA-tuning in comparison to fully supervised fine-tuning across BERT-base, FinBERT, and FLANG-BERT on 4 financial domain tasks from the FLUE benchmarks. However, the first research question is not well motivated as PEFTs have shown tremendous success across domains, languages, and even modalities. Therefore, the technical contribution of this paper is minimal, and there is no additional finding on PEFT on top of existing PEFT literature. Moreover, the second research question has been addressed by individual PEFT papers. 

Pfeiffer, J., Goyal, N., Lin, X. V., Li, X., Cross, J., Riedel, S., & Artetxe, M. (2022). Lifting the Curse of Multilinguality by Pre-training Modular Transformers. 

Chronopoulou, A., Peters, M. E., & Dodge, J. (2022). Efficient Hierarchical Domain Adaptation for Pretrained Language Models.
Sung, Y.-L., Cho, J., & Bansal, M. (2022). VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks.


","1. This paper extends the existing PEFT analysis to the financial domain, and indicates competitive performance to fully supervised fine-tuning.
","1. The study of PEFT in a new domain is not motivated and has limited novelty.

2. The study only contains two PEFT approaches, Adapter and LoRA. More interesting comparisons among parallel adapter [1], IA^3, pre-fix tuning, prompt tuning, and many other new PEFT approaches can make the analysis in the finance domain more existing. 

3. The study only focuses on classification-based finance tasks. Generative tasks can lead to more interesting results. Additionally, this study only considers encoder-only models. A more throughout evaluation study of PEFT should consider T5, and GPT-type models.

4. The second research question is not novel.
",1. Can you extend this analysis to more PEFT approaches and types of base models?,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2021). Towards a unified view of parameter-efficient transfer learning. 

[2] Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., & Raffel, C. A. (2022). Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.
","Line 03: they risk -> the risk / their risk
The bold fonts in Table 1 are misleading. The author should not exclude the original results in ranking and should also highlight the FT baseline that achieves the same highest score. ",1690678986013,,,EMNLP/2023/Conference,1Sn1dpNaP3,"['EMNLP/2023/Conference/Submission1798/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461123665,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1798/Reviewer_JNPW']",1Sn1dpNaP3,['EMNLP/2023/Conference/Submission1798/Reviewer_JNPW'],1690678986013,1701461123665,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1798/Reviewer_JNPW']","Yes

The suggested papers or references not cited in the manuscript are:
1. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2021). Towards a unified view of parameter-efficient transfer learning.
2. Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., & Raffel, C. A. (2022). Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.

These citations might be necessary because the reviewer suggests that the study of PEFT in the financial domain is not well-motivated and lacks novelty, and that the analysis only considers two PEFT approaches (Adapter and LoRA) and encoder-only models. The reviewer recommends extending the analysis to more PEFT approaches (such as parallel adapter, IA^3, pre-fix tuning, and prompt tuning) and types of base models (such as T5 and GPT-type models). The suggested papers may provide relevant information on these topics, with He et al. (2021) offering a unified view of parameter-efficient transfer learning and Liu et al. (2022) discussing few-shot parameter-efficient fine-tuning.",1,"2021, 2022, 2021, 2022",Towards a unified view of parameter-efficient transfer learning 
UenKAJre5E,"This paper presents a study of PEFT in financial domain tasks. It addresses two research questions: 1. how do PEFTs perform on financial tasks? 2. what are the pros and cons of using PEFTs? The author conducts adapter-tuning and LoRA-tuning in comparison to fully supervised fine-tuning across BERT-base, FinBERT, and FLANG-BERT on 4 financial domain tasks from the FLUE benchmarks. However, the first research question is not well motivated as PEFTs have shown tremendous success across domains, languages, and even modalities. Therefore, the technical contribution of this paper is minimal, and there is no additional finding on PEFT on top of existing PEFT literature. Moreover, the second research question has been addressed by individual PEFT papers. 

Pfeiffer, J., Goyal, N., Lin, X. V., Li, X., Cross, J., Riedel, S., & Artetxe, M. (2022). Lifting the Curse of Multilinguality by Pre-training Modular Transformers. 

Chronopoulou, A., Peters, M. E., & Dodge, J. (2022). Efficient Hierarchical Domain Adaptation for Pretrained Language Models.
Sung, Y.-L., Cho, J., & Bansal, M. (2022). VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks.


","1. This paper extends the existing PEFT analysis to the financial domain, and indicates competitive performance to fully supervised fine-tuning.
","1. The study of PEFT in a new domain is not motivated and has limited novelty.

2. The study only contains two PEFT approaches, Adapter and LoRA. More interesting comparisons among parallel adapter [1], IA^3, pre-fix tuning, prompt tuning, and many other new PEFT approaches can make the analysis in the finance domain more existing. 

3. The study only focuses on classification-based finance tasks. Generative tasks can lead to more interesting results. Additionally, this study only considers encoder-only models. A more throughout evaluation study of PEFT should consider T5, and GPT-type models.

4. The second research question is not novel.
",1. Can you extend this analysis to more PEFT approaches and types of base models?,"2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2021). Towards a unified view of parameter-efficient transfer learning. 

[2] Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., & Raffel, C. A. (2022). Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.
","Line 03: they risk -> the risk / their risk
The bold fonts in Table 1 are misleading. The author should not exclude the original results in ranking and should also highlight the FT baseline that achieves the same highest score. ",1690678986013,,,EMNLP/2023/Conference,1Sn1dpNaP3,"['EMNLP/2023/Conference/Submission1798/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461123665,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1798/Reviewer_JNPW']",1Sn1dpNaP3,['EMNLP/2023/Conference/Submission1798/Reviewer_JNPW'],1690678986013,1701461123665,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1798/Reviewer_JNPW']","Yes

The suggested papers or references not cited in the manuscript are:
1. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2021). Towards a unified view of parameter-efficient transfer learning.
2. Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., & Raffel, C. A. (2022). Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.

These citations might be necessary because the reviewer suggests that the study of PEFT in the financial domain is not well-motivated and lacks novelty, and that the analysis only considers two PEFT approaches (Adapter and LoRA) and encoder-only models. The reviewer recommends extending the analysis to more PEFT approaches (such as parallel adapter, IA^3, pre-fix tuning, and prompt tuning) and types of base models (such as T5 and GPT-type models). The suggested papers may provide relevant information on these topics, with He et al. (2021) offering a unified view of parameter-efficient transfer learning and Liu et al. (2022) discussing few-shot parameter-efficient fine-tuning.",1,"2021, 2022, 2021, 2022",Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning
N5CJlxdCTb,"This paper examines a few retrieval augmented language models' performance on reasoning intensive tasks: EntailmentBank and StrategyQA. The performance of both the retrievers and the readers are examined. 5 popular models are evaluated without fine-tuning on the EntailmentBank and StrategyQA datasets: kNN-LM, REALM, DPR + FiD, Contriever + ATLAS and Contriever + Flan-T5. 

The main takeaways from the experiments are: (1) The retrievers of these models tend to fail to perfectly retrieve the necessary statements for the reasoning intensive tasks. (2) Even with the gold evidence, the neural readers could not perfectly answer the question. (3) When multi-step reasoning over the statements is needed to come up with the answer, different models perform differently (e.g., Flan-T5 has a better reasoning ability compared with others). (4) Bad retrieval quality can largely affect QA model's performance. (5) Large model usually has a better QA performance. (6) Multi-step retrieval is promising when combined with large language models. "," - The idea of evaluating the retriever and reader of the language models on reasoning intensive tasks is interesting.
 - The authors evaluate 5 recent and representative retrieval augmented neural models, which is informative to other readers."," - One major drawback is that all of the models are only evaluated without any fine-tuning. Although it is good to know the zero-shot performance, knowing their limitation after fine-tuning would be much more helpful. 
 - Some takeaways are already discussed in previous literatures, and the results are reported for similar models, which makes this work less exciting. E.g.,
   * For takeaways (2) and (4) above, similar results are reported in ""Better retrieval may not lead to better question answering"".
   * For takeaway (5) above, ""Chain-of-Thought Prompting Elicits Reasoning in Large Language Models""",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691205896003,,,EMNLP/2023/Conference,1RVUxlrFJZ,"['EMNLP/2023/Conference/Submission4488/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461298234,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission4488/Reviewer_orzP']",1RVUxlrFJZ,['EMNLP/2023/Conference/Submission4488/Reviewer_orzP'],1691205896003,1701461298234,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4488/Reviewer_orzP']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Better retrieval may not lead to better question answering""
2. ""Chain-of-Thought Prompting Elicits Reasoning in Large Language Models""

These citations might be necessary because the reviewer mentions that some of the takeaways from the experiments, such as the limitations of retrieval quality and the performance of large language models, have already been discussed in previous literature. The reviewer specifically mentions that similar results are reported in these two papers, implying that the authors should be aware of and reference this existing work to provide a more comprehensive understanding of their research and to avoid duplicating existing knowledge.",1,,Better retrieval may not lead to better question answering
N5CJlxdCTb,"This paper examines a few retrieval augmented language models' performance on reasoning intensive tasks: EntailmentBank and StrategyQA. The performance of both the retrievers and the readers are examined. 5 popular models are evaluated without fine-tuning on the EntailmentBank and StrategyQA datasets: kNN-LM, REALM, DPR + FiD, Contriever + ATLAS and Contriever + Flan-T5. 

The main takeaways from the experiments are: (1) The retrievers of these models tend to fail to perfectly retrieve the necessary statements for the reasoning intensive tasks. (2) Even with the gold evidence, the neural readers could not perfectly answer the question. (3) When multi-step reasoning over the statements is needed to come up with the answer, different models perform differently (e.g., Flan-T5 has a better reasoning ability compared with others). (4) Bad retrieval quality can largely affect QA model's performance. (5) Large model usually has a better QA performance. (6) Multi-step retrieval is promising when combined with large language models. "," - The idea of evaluating the retriever and reader of the language models on reasoning intensive tasks is interesting.
 - The authors evaluate 5 recent and representative retrieval augmented neural models, which is informative to other readers."," - One major drawback is that all of the models are only evaluated without any fine-tuning. Although it is good to know the zero-shot performance, knowing their limitation after fine-tuning would be much more helpful. 
 - Some takeaways are already discussed in previous literatures, and the results are reported for similar models, which makes this work less exciting. E.g.,
   * For takeaways (2) and (4) above, similar results are reported in ""Better retrieval may not lead to better question answering"".
   * For takeaway (5) above, ""Chain-of-Thought Prompting Elicits Reasoning in Large Language Models""",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1691205896003,,,EMNLP/2023/Conference,1RVUxlrFJZ,"['EMNLP/2023/Conference/Submission4488/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461298234,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission4488/Reviewer_orzP']",1RVUxlrFJZ,['EMNLP/2023/Conference/Submission4488/Reviewer_orzP'],1691205896003,1701461298234,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4488/Reviewer_orzP']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Better retrieval may not lead to better question answering""
2. ""Chain-of-Thought Prompting Elicits Reasoning in Large Language Models""

These citations might be necessary because the reviewer mentions that some of the takeaways from the experiments, such as the limitations of retrieval quality and the performance of large language models, have already been discussed in previous literature. The reviewer specifically mentions that similar results are reported in these two papers, implying that the authors should be aware of and reference this existing work to provide a more comprehensive understanding of their research and to avoid duplicating existing knowledge.",1,,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
6nRorM0wJd,"This paper aims to make a Pretrained Language Model more robust to naturally occurring noise and linguistic variation. To do so, they propose adding additional layers to the start and end of a pretrained language model, using continued pretraining on data which has been synthetically noised using character level noise patterns, and finetuning on similarly noised data. They find that this full procedure helps with English typos and with a Bavarian dialect of German, while only finetuning on noisy data benefits performance in Swiss German. Finally, they perform an analysis of token-level alignment throughout the original and modified models and how this correlates with classifiers trained at different layers of the model as well.","- Assessing the benefits of introducing synthetic noise during both pretraining and finetuning is an interesting question. Understanding how much general robustness can be derived from pretraining on synthetically noised data alone might yield good insights into the ways in which models maintain more broad linguistic knowledge.

- Assessing the effects of the procedure on multiple dialects of German is valuable, since it allows experiments which tease apart the benefits depending on the types of variation which occurs in each dialect.","- The primary novelty of the work appears to be the architectural change, but they don't compare to other widely used architecture modifications for transformers such as Adapters, Prefix-Tuning, or LoRA. The baselines (L0, LF, and regular BERT) all have strictly fewer tunable parameters than the proposed approach, so it seems totally possible to me that the improved results of the sandwich approach are simply from the increased number of parameters especially since the sandwich approach frequently does not outperform even these baseline architectures by a statistically significant margin. It seems unclear that this architecture is a meaningful improvement without more diverse baselines, especially ones which control for parameter count.

- The approach for synthetic introduction of noise (insertion, deletion, replacement, and swapping) primarily address typos with some coverage of low-edit distance dialectal morphology changes. This is a quite small subset of dialectal shifts compared to full lexical shift and syntactic variation. Furthermore, even for typo noise this model is somewhat naive and assigns even probability to noise categories, despite real noise following patterns e.g. the example ""student"" -> ""studebt"" seems reasonable because B and N are neighbors on the keyboard, but ""student"" -> ""studeqt"" would almost never happen. This work would be much stronger if it incorporated and compared to synthetic noise grounded in actual typo patterns and language variation. 

- The model omits many standard details of training which would make it difficult to reproduce.","A) Are there patterns to the cases in German dialects which switch from wrong to right with noisy pretraining? What types of linguistic patterns does this noisy training capture? Maybe this could help explore why the Swiss German performance degrades when noise is introduced.

B) How do you explain the higher similarity at the start of BERT+LF than of L0+BERT?

C) What were the hyperparameters used for experiments? Learning rate, Batch Size, epochs, etc","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Prior works for introducing noise during pretraining and finetuning:

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 619–628, New Orleans, Louisiana. Association for Computational Linguistics.

Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. ArXiv, abs/2202.12312.

Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 543–553, Brussels, Belgium. Association for Computational Linguistics.

Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1916–1920, Minneapolis, Minnesota. Association for Computational Linguistics.

Prior works for modular LLM architectures to handle linguistic variation:

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.

William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English. In Findings of the Association for Computational Linguistics: ACL 2023, pages 813–824, Toronto, Canada. Association for Computational Linguistics.","Figure 1: This figure is quite large, but with not a ton of information. This could be shrunk significantly (stack the 12 internal layers!) to make room for key information such as the settings for both CPT and finetuning training runs.

Line 226: Repeated List of Operations twice

Line 394-396: ""models maintain performance on Standard"" - Where do you show this? It doesn't appear to be in any of the tables in the paper? 

",1691019614161,,,EMNLP/2023/Conference,1PXPP9Gzgc,"['EMNLP/2023/Conference/Submission4704/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461309922,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']",1PXPP9Gzgc,['EMNLP/2023/Conference/Submission4704/Reviewer_FYgr'],1691019614161,1701461309922,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.
2. Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction.
3. Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies.
4. Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text.
5. Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise.
6. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.
7. William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English.

These citations might be necessary because the reviewer believes that the authors' work on introducing synthetic noise during pretraining and finetuning, as well as their exploration of modular LLM architectures to handle linguistic variation, has been previously studied in other papers. The reviewer suggests that the authors should be aware of and reference these prior works to provide a more comprehensive understanding of the research area and to properly contextualize their own contributions. The reviewer also implies that the authors' approach to introducing synthetic noise and their architectural modifications may not be entirely novel, and that prior works may have already explored similar ideas. By citing these papers, the authors can demonstrate their awareness of the existing literature and provide a more nuanced discussion of their research.",1,"2020, 2018, 2022, 2018, 2019, 2020, 2023",BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation Translation and Comprehension
6nRorM0wJd,"This paper aims to make a Pretrained Language Model more robust to naturally occurring noise and linguistic variation. To do so, they propose adding additional layers to the start and end of a pretrained language model, using continued pretraining on data which has been synthetically noised using character level noise patterns, and finetuning on similarly noised data. They find that this full procedure helps with English typos and with a Bavarian dialect of German, while only finetuning on noisy data benefits performance in Swiss German. Finally, they perform an analysis of token-level alignment throughout the original and modified models and how this correlates with classifiers trained at different layers of the model as well.","- Assessing the benefits of introducing synthetic noise during both pretraining and finetuning is an interesting question. Understanding how much general robustness can be derived from pretraining on synthetically noised data alone might yield good insights into the ways in which models maintain more broad linguistic knowledge.

- Assessing the effects of the procedure on multiple dialects of German is valuable, since it allows experiments which tease apart the benefits depending on the types of variation which occurs in each dialect.","- The primary novelty of the work appears to be the architectural change, but they don't compare to other widely used architecture modifications for transformers such as Adapters, Prefix-Tuning, or LoRA. The baselines (L0, LF, and regular BERT) all have strictly fewer tunable parameters than the proposed approach, so it seems totally possible to me that the improved results of the sandwich approach are simply from the increased number of parameters especially since the sandwich approach frequently does not outperform even these baseline architectures by a statistically significant margin. It seems unclear that this architecture is a meaningful improvement without more diverse baselines, especially ones which control for parameter count.

- The approach for synthetic introduction of noise (insertion, deletion, replacement, and swapping) primarily address typos with some coverage of low-edit distance dialectal morphology changes. This is a quite small subset of dialectal shifts compared to full lexical shift and syntactic variation. Furthermore, even for typo noise this model is somewhat naive and assigns even probability to noise categories, despite real noise following patterns e.g. the example ""student"" -> ""studebt"" seems reasonable because B and N are neighbors on the keyboard, but ""student"" -> ""studeqt"" would almost never happen. This work would be much stronger if it incorporated and compared to synthetic noise grounded in actual typo patterns and language variation. 

- The model omits many standard details of training which would make it difficult to reproduce.","A) Are there patterns to the cases in German dialects which switch from wrong to right with noisy pretraining? What types of linguistic patterns does this noisy training capture? Maybe this could help explore why the Swiss German performance degrades when noise is introduced.

B) How do you explain the higher similarity at the start of BERT+LF than of L0+BERT?

C) What were the hyperparameters used for experiments? Learning rate, Batch Size, epochs, etc","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Prior works for introducing noise during pretraining and finetuning:

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 619–628, New Orleans, Louisiana. Association for Computational Linguistics.

Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. ArXiv, abs/2202.12312.

Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 543–553, Brussels, Belgium. Association for Computational Linguistics.

Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1916–1920, Minneapolis, Minnesota. Association for Computational Linguistics.

Prior works for modular LLM architectures to handle linguistic variation:

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.

William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English. In Findings of the Association for Computational Linguistics: ACL 2023, pages 813–824, Toronto, Canada. Association for Computational Linguistics.","Figure 1: This figure is quite large, but with not a ton of information. This could be shrunk significantly (stack the 12 internal layers!) to make room for key information such as the settings for both CPT and finetuning training runs.

Line 226: Repeated List of Operations twice

Line 394-396: ""models maintain performance on Standard"" - Where do you show this? It doesn't appear to be in any of the tables in the paper? 

",1691019614161,,,EMNLP/2023/Conference,1PXPP9Gzgc,"['EMNLP/2023/Conference/Submission4704/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461309922,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']",1PXPP9Gzgc,['EMNLP/2023/Conference/Submission4704/Reviewer_FYgr'],1691019614161,1701461309922,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.
2. Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction.
3. Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies.
4. Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text.
5. Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise.
6. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.
7. William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English.

These citations might be necessary because the reviewer believes that the authors' work on introducing synthetic noise during pretraining and finetuning, as well as their exploration of modular LLM architectures to handle linguistic variation, has been previously studied in other papers. The reviewer suggests that the authors should be aware of and reference these prior works to provide a more comprehensive understanding of the research area and to properly contextualize their own contributions. The reviewer also implies that the authors' approach to introducing synthetic noise and their architectural modifications may not be entirely novel, and that prior works may have already explored similar ideas. By citing these papers, the authors can demonstrate their awareness of the existing literature and provide a more nuanced discussion of their research.",1,"2020, 2018, 2022, 2018, 2019, 2020, 2023",Noising and Denoising Natural Language Diverse Backtranslation for Grammar Correction
6nRorM0wJd,"This paper aims to make a Pretrained Language Model more robust to naturally occurring noise and linguistic variation. To do so, they propose adding additional layers to the start and end of a pretrained language model, using continued pretraining on data which has been synthetically noised using character level noise patterns, and finetuning on similarly noised data. They find that this full procedure helps with English typos and with a Bavarian dialect of German, while only finetuning on noisy data benefits performance in Swiss German. Finally, they perform an analysis of token-level alignment throughout the original and modified models and how this correlates with classifiers trained at different layers of the model as well.","- Assessing the benefits of introducing synthetic noise during both pretraining and finetuning is an interesting question. Understanding how much general robustness can be derived from pretraining on synthetically noised data alone might yield good insights into the ways in which models maintain more broad linguistic knowledge.

- Assessing the effects of the procedure on multiple dialects of German is valuable, since it allows experiments which tease apart the benefits depending on the types of variation which occurs in each dialect.","- The primary novelty of the work appears to be the architectural change, but they don't compare to other widely used architecture modifications for transformers such as Adapters, Prefix-Tuning, or LoRA. The baselines (L0, LF, and regular BERT) all have strictly fewer tunable parameters than the proposed approach, so it seems totally possible to me that the improved results of the sandwich approach are simply from the increased number of parameters especially since the sandwich approach frequently does not outperform even these baseline architectures by a statistically significant margin. It seems unclear that this architecture is a meaningful improvement without more diverse baselines, especially ones which control for parameter count.

- The approach for synthetic introduction of noise (insertion, deletion, replacement, and swapping) primarily address typos with some coverage of low-edit distance dialectal morphology changes. This is a quite small subset of dialectal shifts compared to full lexical shift and syntactic variation. Furthermore, even for typo noise this model is somewhat naive and assigns even probability to noise categories, despite real noise following patterns e.g. the example ""student"" -> ""studebt"" seems reasonable because B and N are neighbors on the keyboard, but ""student"" -> ""studeqt"" would almost never happen. This work would be much stronger if it incorporated and compared to synthetic noise grounded in actual typo patterns and language variation. 

- The model omits many standard details of training which would make it difficult to reproduce.","A) Are there patterns to the cases in German dialects which switch from wrong to right with noisy pretraining? What types of linguistic patterns does this noisy training capture? Maybe this could help explore why the Swiss German performance degrades when noise is introduced.

B) How do you explain the higher similarity at the start of BERT+LF than of L0+BERT?

C) What were the hyperparameters used for experiments? Learning rate, Batch Size, epochs, etc","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Prior works for introducing noise during pretraining and finetuning:

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 619–628, New Orleans, Louisiana. Association for Computational Linguistics.

Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. ArXiv, abs/2202.12312.

Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 543–553, Brussels, Belgium. Association for Computational Linguistics.

Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1916–1920, Minneapolis, Minnesota. Association for Computational Linguistics.

Prior works for modular LLM architectures to handle linguistic variation:

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.

William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English. In Findings of the Association for Computational Linguistics: ACL 2023, pages 813–824, Toronto, Canada. Association for Computational Linguistics.","Figure 1: This figure is quite large, but with not a ton of information. This could be shrunk significantly (stack the 12 internal layers!) to make room for key information such as the settings for both CPT and finetuning training runs.

Line 226: Repeated List of Operations twice

Line 394-396: ""models maintain performance on Standard"" - Where do you show this? It doesn't appear to be in any of the tables in the paper? 

",1691019614161,,,EMNLP/2023/Conference,1PXPP9Gzgc,"['EMNLP/2023/Conference/Submission4704/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461309922,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']",1PXPP9Gzgc,['EMNLP/2023/Conference/Submission4704/Reviewer_FYgr'],1691019614161,1701461309922,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.
2. Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction.
3. Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies.
4. Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text.
5. Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise.
6. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.
7. William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English.

These citations might be necessary because the reviewer believes that the authors' work on introducing synthetic noise during pretraining and finetuning, as well as their exploration of modular LLM architectures to handle linguistic variation, has been previously studied in other papers. The reviewer suggests that the authors should be aware of and reference these prior works to provide a more comprehensive understanding of the research area and to properly contextualize their own contributions. The reviewer also implies that the authors' approach to introducing synthetic noise and their architectural modifications may not be entirely novel, and that prior works may have already explored similar ideas. By citing these papers, the authors can demonstrate their awareness of the existing literature and provide a more nuanced discussion of their research.",1,"2020, 2018, 2022, 2018, 2019, 2020, 2023",Oolong Investigating What Makes Crosslingual Transfer Hard with Controlled Studies
6nRorM0wJd,"This paper aims to make a Pretrained Language Model more robust to naturally occurring noise and linguistic variation. To do so, they propose adding additional layers to the start and end of a pretrained language model, using continued pretraining on data which has been synthetically noised using character level noise patterns, and finetuning on similarly noised data. They find that this full procedure helps with English typos and with a Bavarian dialect of German, while only finetuning on noisy data benefits performance in Swiss German. Finally, they perform an analysis of token-level alignment throughout the original and modified models and how this correlates with classifiers trained at different layers of the model as well.","- Assessing the benefits of introducing synthetic noise during both pretraining and finetuning is an interesting question. Understanding how much general robustness can be derived from pretraining on synthetically noised data alone might yield good insights into the ways in which models maintain more broad linguistic knowledge.

- Assessing the effects of the procedure on multiple dialects of German is valuable, since it allows experiments which tease apart the benefits depending on the types of variation which occurs in each dialect.","- The primary novelty of the work appears to be the architectural change, but they don't compare to other widely used architecture modifications for transformers such as Adapters, Prefix-Tuning, or LoRA. The baselines (L0, LF, and regular BERT) all have strictly fewer tunable parameters than the proposed approach, so it seems totally possible to me that the improved results of the sandwich approach are simply from the increased number of parameters especially since the sandwich approach frequently does not outperform even these baseline architectures by a statistically significant margin. It seems unclear that this architecture is a meaningful improvement without more diverse baselines, especially ones which control for parameter count.

- The approach for synthetic introduction of noise (insertion, deletion, replacement, and swapping) primarily address typos with some coverage of low-edit distance dialectal morphology changes. This is a quite small subset of dialectal shifts compared to full lexical shift and syntactic variation. Furthermore, even for typo noise this model is somewhat naive and assigns even probability to noise categories, despite real noise following patterns e.g. the example ""student"" -> ""studebt"" seems reasonable because B and N are neighbors on the keyboard, but ""student"" -> ""studeqt"" would almost never happen. This work would be much stronger if it incorporated and compared to synthetic noise grounded in actual typo patterns and language variation. 

- The model omits many standard details of training which would make it difficult to reproduce.","A) Are there patterns to the cases in German dialects which switch from wrong to right with noisy pretraining? What types of linguistic patterns does this noisy training capture? Maybe this could help explore why the Swiss German performance degrades when noise is introduced.

B) How do you explain the higher similarity at the start of BERT+LF than of L0+BERT?

C) What were the hyperparameters used for experiments? Learning rate, Batch Size, epochs, etc","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Prior works for introducing noise during pretraining and finetuning:

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 619–628, New Orleans, Louisiana. Association for Computational Linguistics.

Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. ArXiv, abs/2202.12312.

Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 543–553, Brussels, Belgium. Association for Computational Linguistics.

Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1916–1920, Minneapolis, Minnesota. Association for Computational Linguistics.

Prior works for modular LLM architectures to handle linguistic variation:

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.

William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English. In Findings of the Association for Computational Linguistics: ACL 2023, pages 813–824, Toronto, Canada. Association for Computational Linguistics.","Figure 1: This figure is quite large, but with not a ton of information. This could be shrunk significantly (stack the 12 internal layers!) to make room for key information such as the settings for both CPT and finetuning training runs.

Line 226: Repeated List of Operations twice

Line 394-396: ""models maintain performance on Standard"" - Where do you show this? It doesn't appear to be in any of the tables in the paper? 

",1691019614161,,,EMNLP/2023/Conference,1PXPP9Gzgc,"['EMNLP/2023/Conference/Submission4704/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461309922,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']",1PXPP9Gzgc,['EMNLP/2023/Conference/Submission4704/Reviewer_FYgr'],1691019614161,1701461309922,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.
2. Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction.
3. Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies.
4. Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text.
5. Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise.
6. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.
7. William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English.

These citations might be necessary because the reviewer believes that the authors' work on introducing synthetic noise during pretraining and finetuning, as well as their exploration of modular LLM architectures to handle linguistic variation, has been previously studied in other papers. The reviewer suggests that the authors should be aware of and reference these prior works to provide a more comprehensive understanding of the research area and to properly contextualize their own contributions. The reviewer also implies that the authors' approach to introducing synthetic noise and their architectural modifications may not be entirely novel, and that prior works may have already explored similar ideas. By citing these papers, the authors can demonstrate their awareness of the existing literature and provide a more nuanced discussion of their research.",1,"2020, 2018, 2022, 2018, 2019, 2020, 2023",MTNT A Testbed for Machine Translation of Noisy Text
6nRorM0wJd,"This paper aims to make a Pretrained Language Model more robust to naturally occurring noise and linguistic variation. To do so, they propose adding additional layers to the start and end of a pretrained language model, using continued pretraining on data which has been synthetically noised using character level noise patterns, and finetuning on similarly noised data. They find that this full procedure helps with English typos and with a Bavarian dialect of German, while only finetuning on noisy data benefits performance in Swiss German. Finally, they perform an analysis of token-level alignment throughout the original and modified models and how this correlates with classifiers trained at different layers of the model as well.","- Assessing the benefits of introducing synthetic noise during both pretraining and finetuning is an interesting question. Understanding how much general robustness can be derived from pretraining on synthetically noised data alone might yield good insights into the ways in which models maintain more broad linguistic knowledge.

- Assessing the effects of the procedure on multiple dialects of German is valuable, since it allows experiments which tease apart the benefits depending on the types of variation which occurs in each dialect.","- The primary novelty of the work appears to be the architectural change, but they don't compare to other widely used architecture modifications for transformers such as Adapters, Prefix-Tuning, or LoRA. The baselines (L0, LF, and regular BERT) all have strictly fewer tunable parameters than the proposed approach, so it seems totally possible to me that the improved results of the sandwich approach are simply from the increased number of parameters especially since the sandwich approach frequently does not outperform even these baseline architectures by a statistically significant margin. It seems unclear that this architecture is a meaningful improvement without more diverse baselines, especially ones which control for parameter count.

- The approach for synthetic introduction of noise (insertion, deletion, replacement, and swapping) primarily address typos with some coverage of low-edit distance dialectal morphology changes. This is a quite small subset of dialectal shifts compared to full lexical shift and syntactic variation. Furthermore, even for typo noise this model is somewhat naive and assigns even probability to noise categories, despite real noise following patterns e.g. the example ""student"" -> ""studebt"" seems reasonable because B and N are neighbors on the keyboard, but ""student"" -> ""studeqt"" would almost never happen. This work would be much stronger if it incorporated and compared to synthetic noise grounded in actual typo patterns and language variation. 

- The model omits many standard details of training which would make it difficult to reproduce.","A) Are there patterns to the cases in German dialects which switch from wrong to right with noisy pretraining? What types of linguistic patterns does this noisy training capture? Maybe this could help explore why the Swiss German performance degrades when noise is introduced.

B) How do you explain the higher similarity at the start of BERT+LF than of L0+BERT?

C) What were the hyperparameters used for experiments? Learning rate, Batch Size, epochs, etc","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Prior works for introducing noise during pretraining and finetuning:

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 619–628, New Orleans, Louisiana. Association for Computational Linguistics.

Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. ArXiv, abs/2202.12312.

Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 543–553, Brussels, Belgium. Association for Computational Linguistics.

Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1916–1920, Minneapolis, Minnesota. Association for Computational Linguistics.

Prior works for modular LLM architectures to handle linguistic variation:

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.

William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English. In Findings of the Association for Computational Linguistics: ACL 2023, pages 813–824, Toronto, Canada. Association for Computational Linguistics.","Figure 1: This figure is quite large, but with not a ton of information. This could be shrunk significantly (stack the 12 internal layers!) to make room for key information such as the settings for both CPT and finetuning training runs.

Line 226: Repeated List of Operations twice

Line 394-396: ""models maintain performance on Standard"" - Where do you show this? It doesn't appear to be in any of the tables in the paper? 

",1691019614161,,,EMNLP/2023/Conference,1PXPP9Gzgc,"['EMNLP/2023/Conference/Submission4704/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461309922,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']",1PXPP9Gzgc,['EMNLP/2023/Conference/Submission4704/Reviewer_FYgr'],1691019614161,1701461309922,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.
2. Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction.
3. Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies.
4. Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text.
5. Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise.
6. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.
7. William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English.

These citations might be necessary because the reviewer believes that the authors' work on introducing synthetic noise during pretraining and finetuning, as well as their exploration of modular LLM architectures to handle linguistic variation, has been previously studied in other papers. The reviewer suggests that the authors should be aware of and reference these prior works to provide a more comprehensive understanding of the research area and to properly contextualize their own contributions. The reviewer also implies that the authors' approach to introducing synthetic noise and their architectural modifications may not be entirely novel, and that prior works may have already explored similar ideas. By citing these papers, the authors can demonstrate their awareness of the existing literature and provide a more nuanced discussion of their research.",1,"2020, 2018, 2022, 2018, 2019, 2020, 2023",Improving Robustness of Machine Translation with Synthetic Noise
6nRorM0wJd,"This paper aims to make a Pretrained Language Model more robust to naturally occurring noise and linguistic variation. To do so, they propose adding additional layers to the start and end of a pretrained language model, using continued pretraining on data which has been synthetically noised using character level noise patterns, and finetuning on similarly noised data. They find that this full procedure helps with English typos and with a Bavarian dialect of German, while only finetuning on noisy data benefits performance in Swiss German. Finally, they perform an analysis of token-level alignment throughout the original and modified models and how this correlates with classifiers trained at different layers of the model as well.","- Assessing the benefits of introducing synthetic noise during both pretraining and finetuning is an interesting question. Understanding how much general robustness can be derived from pretraining on synthetically noised data alone might yield good insights into the ways in which models maintain more broad linguistic knowledge.

- Assessing the effects of the procedure on multiple dialects of German is valuable, since it allows experiments which tease apart the benefits depending on the types of variation which occurs in each dialect.","- The primary novelty of the work appears to be the architectural change, but they don't compare to other widely used architecture modifications for transformers such as Adapters, Prefix-Tuning, or LoRA. The baselines (L0, LF, and regular BERT) all have strictly fewer tunable parameters than the proposed approach, so it seems totally possible to me that the improved results of the sandwich approach are simply from the increased number of parameters especially since the sandwich approach frequently does not outperform even these baseline architectures by a statistically significant margin. It seems unclear that this architecture is a meaningful improvement without more diverse baselines, especially ones which control for parameter count.

- The approach for synthetic introduction of noise (insertion, deletion, replacement, and swapping) primarily address typos with some coverage of low-edit distance dialectal morphology changes. This is a quite small subset of dialectal shifts compared to full lexical shift and syntactic variation. Furthermore, even for typo noise this model is somewhat naive and assigns even probability to noise categories, despite real noise following patterns e.g. the example ""student"" -> ""studebt"" seems reasonable because B and N are neighbors on the keyboard, but ""student"" -> ""studeqt"" would almost never happen. This work would be much stronger if it incorporated and compared to synthetic noise grounded in actual typo patterns and language variation. 

- The model omits many standard details of training which would make it difficult to reproduce.","A) Are there patterns to the cases in German dialects which switch from wrong to right with noisy pretraining? What types of linguistic patterns does this noisy training capture? Maybe this could help explore why the Swiss German performance degrades when noise is introduced.

B) How do you explain the higher similarity at the start of BERT+LF than of L0+BERT?

C) What were the hyperparameters used for experiments? Learning rate, Batch Size, epochs, etc","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Prior works for introducing noise during pretraining and finetuning:

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 619–628, New Orleans, Louisiana. Association for Computational Linguistics.

Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. ArXiv, abs/2202.12312.

Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 543–553, Brussels, Belgium. Association for Computational Linguistics.

Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1916–1920, Minneapolis, Minnesota. Association for Computational Linguistics.

Prior works for modular LLM architectures to handle linguistic variation:

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.

William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English. In Findings of the Association for Computational Linguistics: ACL 2023, pages 813–824, Toronto, Canada. Association for Computational Linguistics.","Figure 1: This figure is quite large, but with not a ton of information. This could be shrunk significantly (stack the 12 internal layers!) to make room for key information such as the settings for both CPT and finetuning training runs.

Line 226: Repeated List of Operations twice

Line 394-396: ""models maintain performance on Standard"" - Where do you show this? It doesn't appear to be in any of the tables in the paper? 

",1691019614161,,,EMNLP/2023/Conference,1PXPP9Gzgc,"['EMNLP/2023/Conference/Submission4704/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461309922,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']",1PXPP9Gzgc,['EMNLP/2023/Conference/Submission4704/Reviewer_FYgr'],1691019614161,1701461309922,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.
2. Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction.
3. Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies.
4. Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text.
5. Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise.
6. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.
7. William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English.

These citations might be necessary because the reviewer believes that the authors' work on introducing synthetic noise during pretraining and finetuning, as well as their exploration of modular LLM architectures to handle linguistic variation, has been previously studied in other papers. The reviewer suggests that the authors should be aware of and reference these prior works to provide a more comprehensive understanding of the research area and to properly contextualize their own contributions. The reviewer also implies that the authors' approach to introducing synthetic noise and their architectural modifications may not be entirely novel, and that prior works may have already explored similar ideas. By citing these papers, the authors can demonstrate their awareness of the existing literature and provide a more nuanced discussion of their research.",1,"2020, 2018, 2022, 2018, 2019, 2020, 2023",MAD-X An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer
6nRorM0wJd,"This paper aims to make a Pretrained Language Model more robust to naturally occurring noise and linguistic variation. To do so, they propose adding additional layers to the start and end of a pretrained language model, using continued pretraining on data which has been synthetically noised using character level noise patterns, and finetuning on similarly noised data. They find that this full procedure helps with English typos and with a Bavarian dialect of German, while only finetuning on noisy data benefits performance in Swiss German. Finally, they perform an analysis of token-level alignment throughout the original and modified models and how this correlates with classifiers trained at different layers of the model as well.","- Assessing the benefits of introducing synthetic noise during both pretraining and finetuning is an interesting question. Understanding how much general robustness can be derived from pretraining on synthetically noised data alone might yield good insights into the ways in which models maintain more broad linguistic knowledge.

- Assessing the effects of the procedure on multiple dialects of German is valuable, since it allows experiments which tease apart the benefits depending on the types of variation which occurs in each dialect.","- The primary novelty of the work appears to be the architectural change, but they don't compare to other widely used architecture modifications for transformers such as Adapters, Prefix-Tuning, or LoRA. The baselines (L0, LF, and regular BERT) all have strictly fewer tunable parameters than the proposed approach, so it seems totally possible to me that the improved results of the sandwich approach are simply from the increased number of parameters especially since the sandwich approach frequently does not outperform even these baseline architectures by a statistically significant margin. It seems unclear that this architecture is a meaningful improvement without more diverse baselines, especially ones which control for parameter count.

- The approach for synthetic introduction of noise (insertion, deletion, replacement, and swapping) primarily address typos with some coverage of low-edit distance dialectal morphology changes. This is a quite small subset of dialectal shifts compared to full lexical shift and syntactic variation. Furthermore, even for typo noise this model is somewhat naive and assigns even probability to noise categories, despite real noise following patterns e.g. the example ""student"" -> ""studebt"" seems reasonable because B and N are neighbors on the keyboard, but ""student"" -> ""studeqt"" would almost never happen. This work would be much stronger if it incorporated and compared to synthetic noise grounded in actual typo patterns and language variation. 

- The model omits many standard details of training which would make it difficult to reproduce.","A) Are there patterns to the cases in German dialects which switch from wrong to right with noisy pretraining? What types of linguistic patterns does this noisy training capture? Maybe this could help explore why the Swiss German performance degrades when noise is introduced.

B) How do you explain the higher similarity at the start of BERT+LF than of L0+BERT?

C) What were the hyperparameters used for experiments? Learning rate, Batch Size, epochs, etc","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Prior works for introducing noise during pretraining and finetuning:

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 619–628, New Orleans, Louisiana. Association for Computational Linguistics.

Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. ArXiv, abs/2202.12312.

Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 543–553, Brussels, Belgium. Association for Computational Linguistics.

Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1916–1920, Minneapolis, Minnesota. Association for Computational Linguistics.

Prior works for modular LLM architectures to handle linguistic variation:

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.

William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English. In Findings of the Association for Computational Linguistics: ACL 2023, pages 813–824, Toronto, Canada. Association for Computational Linguistics.","Figure 1: This figure is quite large, but with not a ton of information. This could be shrunk significantly (stack the 12 internal layers!) to make room for key information such as the settings for both CPT and finetuning training runs.

Line 226: Repeated List of Operations twice

Line 394-396: ""models maintain performance on Standard"" - Where do you show this? It doesn't appear to be in any of the tables in the paper? 

",1691019614161,,,EMNLP/2023/Conference,1PXPP9Gzgc,"['EMNLP/2023/Conference/Submission4704/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461309922,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']",1PXPP9Gzgc,['EMNLP/2023/Conference/Submission4704/Reviewer_FYgr'],1691019614161,1701461309922,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4704/Reviewer_FYgr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.
2. Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018. Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction.
3. Wu, Z., Papadimitriou, I., & Tamkin, A. (2022). Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies.
4. Paul Michel and Graham Neubig. 2018. MTNT: A Testbed for Machine Translation of Noisy Text.
5. Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019. Improving Robustness of Machine Translation with Synthetic Noise.
6. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.
7. William Held, Caleb Ziems, and Diyi Yang. 2023. TADA : Task Agnostic Dialect Adapters for English.

These citations might be necessary because the reviewer believes that the authors' work on introducing synthetic noise during pretraining and finetuning, as well as their exploration of modular LLM architectures to handle linguistic variation, has been previously studied in other papers. The reviewer suggests that the authors should be aware of and reference these prior works to provide a more comprehensive understanding of the research area and to properly contextualize their own contributions. The reviewer also implies that the authors' approach to introducing synthetic noise and their architectural modifications may not be entirely novel, and that prior works may have already explored similar ideas. By citing these papers, the authors can demonstrate their awareness of the existing literature and provide a more nuanced discussion of their research.",1,"2020, 2018, 2022, 2018, 2019, 2020, 2023",TADA Task Agnostic Dialect Adapters for English
8tHUd0QzMm,This paper proposes a closed-boundary classification method to deal with the miscellaneous class with many clusters. ,"The proposed approach of building closed boundary classifier to deal with Universum class is interesting. 

The proposed method is novel. ","I am surprised that a closed boundary method can do well. Is it because the NER and RE tasks are particularly amenable to such a method? For supervised classification, I am not sure. Can you include some traditional supervised classification methods. 

OOD detection researchers have tried different types of approaches to build closed boundary classifiers. In NLP, there are many papers, e.g., DOC: Deep Open Classification of Text Documents. EMNLP-2017; Deep open intent classification with adaptive decision boundary, AAAI-2021; KNN-contrastive learning for out-of-domain intent classification. ACL2022; Adversarial Self-Supervised Learning for Out-of-Domain Detection, NAACL-2021.  

Your baselines should include some SOTA OOD detection methods. I think kNN with a pre-trained feature extractor should do quite well too. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691105567396,,,EMNLP/2023/Conference,1N5Ia3KLX8,"['EMNLP/2023/Conference/Submission3691/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461247413,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3691/Reviewer_RZMD']",1N5Ia3KLX8,['EMNLP/2023/Conference/Submission3691/Reviewer_RZMD'],1691105567396,1701461247413,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3691/Reviewer_RZMD']","Yes

The suggested papers or references not cited in the manuscript are:
1. DOC: Deep Open Classification of Text Documents (EMNLP-2017)
2. Deep open intent classification with adaptive decision boundary (AAAI-2021)
3. KNN-contrastive learning for out-of-domain intent classification (ACL2022)
4. Adversarial Self-Supervised Learning for Out-of-Domain Detection (NAACL-2021)

These citations might be necessary because the reviewer believes that the authors' proposed method is not thoroughly compared to existing state-of-the-art (SOTA) out-of-distribution (OOD) detection methods. The reviewer suggests including traditional supervised classification methods and some SOTA OOD detection methods, such as kNN with a pre-trained feature extractor, as baselines for a more comprehensive evaluation. The mentioned papers represent relevant approaches to building closed-boundary classifiers and OOD detection, which could provide a more complete context for the authors' contributions and help assess the novelty and effectiveness of their proposed method.",1,"2017, 2021, 2021",Deep Open Classification of Text Documents 
8tHUd0QzMm,This paper proposes a closed-boundary classification method to deal with the miscellaneous class with many clusters. ,"The proposed approach of building closed boundary classifier to deal with Universum class is interesting. 

The proposed method is novel. ","I am surprised that a closed boundary method can do well. Is it because the NER and RE tasks are particularly amenable to such a method? For supervised classification, I am not sure. Can you include some traditional supervised classification methods. 

OOD detection researchers have tried different types of approaches to build closed boundary classifiers. In NLP, there are many papers, e.g., DOC: Deep Open Classification of Text Documents. EMNLP-2017; Deep open intent classification with adaptive decision boundary, AAAI-2021; KNN-contrastive learning for out-of-domain intent classification. ACL2022; Adversarial Self-Supervised Learning for Out-of-Domain Detection, NAACL-2021.  

Your baselines should include some SOTA OOD detection methods. I think kNN with a pre-trained feature extractor should do quite well too. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691105567396,,,EMNLP/2023/Conference,1N5Ia3KLX8,"['EMNLP/2023/Conference/Submission3691/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461247413,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3691/Reviewer_RZMD']",1N5Ia3KLX8,['EMNLP/2023/Conference/Submission3691/Reviewer_RZMD'],1691105567396,1701461247413,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3691/Reviewer_RZMD']","Yes

The suggested papers or references not cited in the manuscript are:
1. DOC: Deep Open Classification of Text Documents (EMNLP-2017)
2. Deep open intent classification with adaptive decision boundary (AAAI-2021)
3. KNN-contrastive learning for out-of-domain intent classification (ACL2022)
4. Adversarial Self-Supervised Learning for Out-of-Domain Detection (NAACL-2021)

These citations might be necessary because the reviewer believes that the authors' proposed method is not thoroughly compared to existing state-of-the-art (SOTA) out-of-distribution (OOD) detection methods. The reviewer suggests including traditional supervised classification methods and some SOTA OOD detection methods, such as kNN with a pre-trained feature extractor, as baselines for a more comprehensive evaluation. The mentioned papers represent relevant approaches to building closed-boundary classifiers and OOD detection, which could provide a more complete context for the authors' contributions and help assess the novelty and effectiveness of their proposed method.",1,"2017, 2021, 2021",Deep open intent classification with adaptive decision boundary 
8tHUd0QzMm,This paper proposes a closed-boundary classification method to deal with the miscellaneous class with many clusters. ,"The proposed approach of building closed boundary classifier to deal with Universum class is interesting. 

The proposed method is novel. ","I am surprised that a closed boundary method can do well. Is it because the NER and RE tasks are particularly amenable to such a method? For supervised classification, I am not sure. Can you include some traditional supervised classification methods. 

OOD detection researchers have tried different types of approaches to build closed boundary classifiers. In NLP, there are many papers, e.g., DOC: Deep Open Classification of Text Documents. EMNLP-2017; Deep open intent classification with adaptive decision boundary, AAAI-2021; KNN-contrastive learning for out-of-domain intent classification. ACL2022; Adversarial Self-Supervised Learning for Out-of-Domain Detection, NAACL-2021.  

Your baselines should include some SOTA OOD detection methods. I think kNN with a pre-trained feature extractor should do quite well too. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691105567396,,,EMNLP/2023/Conference,1N5Ia3KLX8,"['EMNLP/2023/Conference/Submission3691/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461247413,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3691/Reviewer_RZMD']",1N5Ia3KLX8,['EMNLP/2023/Conference/Submission3691/Reviewer_RZMD'],1691105567396,1701461247413,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3691/Reviewer_RZMD']","Yes

The suggested papers or references not cited in the manuscript are:
1. DOC: Deep Open Classification of Text Documents (EMNLP-2017)
2. Deep open intent classification with adaptive decision boundary (AAAI-2021)
3. KNN-contrastive learning for out-of-domain intent classification (ACL2022)
4. Adversarial Self-Supervised Learning for Out-of-Domain Detection (NAACL-2021)

These citations might be necessary because the reviewer believes that the authors' proposed method is not thoroughly compared to existing state-of-the-art (SOTA) out-of-distribution (OOD) detection methods. The reviewer suggests including traditional supervised classification methods and some SOTA OOD detection methods, such as kNN with a pre-trained feature extractor, as baselines for a more comprehensive evaluation. The mentioned papers represent relevant approaches to building closed-boundary classifiers and OOD detection, which could provide a more complete context for the authors' contributions and help assess the novelty and effectiveness of their proposed method.",1,"2017, 2021, 2021",KNN-contrastive learning for out-of-domain intent classification 
8tHUd0QzMm,This paper proposes a closed-boundary classification method to deal with the miscellaneous class with many clusters. ,"The proposed approach of building closed boundary classifier to deal with Universum class is interesting. 

The proposed method is novel. ","I am surprised that a closed boundary method can do well. Is it because the NER and RE tasks are particularly amenable to such a method? For supervised classification, I am not sure. Can you include some traditional supervised classification methods. 

OOD detection researchers have tried different types of approaches to build closed boundary classifiers. In NLP, there are many papers, e.g., DOC: Deep Open Classification of Text Documents. EMNLP-2017; Deep open intent classification with adaptive decision boundary, AAAI-2021; KNN-contrastive learning for out-of-domain intent classification. ACL2022; Adversarial Self-Supervised Learning for Out-of-Domain Detection, NAACL-2021.  

Your baselines should include some SOTA OOD detection methods. I think kNN with a pre-trained feature extractor should do quite well too. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691105567396,,,EMNLP/2023/Conference,1N5Ia3KLX8,"['EMNLP/2023/Conference/Submission3691/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461247413,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3691/Reviewer_RZMD']",1N5Ia3KLX8,['EMNLP/2023/Conference/Submission3691/Reviewer_RZMD'],1691105567396,1701461247413,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3691/Reviewer_RZMD']","Yes

The suggested papers or references not cited in the manuscript are:
1. DOC: Deep Open Classification of Text Documents (EMNLP-2017)
2. Deep open intent classification with adaptive decision boundary (AAAI-2021)
3. KNN-contrastive learning for out-of-domain intent classification (ACL2022)
4. Adversarial Self-Supervised Learning for Out-of-Domain Detection (NAACL-2021)

These citations might be necessary because the reviewer believes that the authors' proposed method is not thoroughly compared to existing state-of-the-art (SOTA) out-of-distribution (OOD) detection methods. The reviewer suggests including traditional supervised classification methods and some SOTA OOD detection methods, such as kNN with a pre-trained feature extractor, as baselines for a more comprehensive evaluation. The mentioned papers represent relevant approaches to building closed-boundary classifiers and OOD detection, which could provide a more complete context for the authors' contributions and help assess the novelty and effectiveness of their proposed method.",1,"2017, 2021, 2021",Adversarial Self-Supervised Learning for Out-of-Domain Detection
5MPfIp19Yo,"This paper delves into the phenomenon of inverse scaling in language models and brings attention to how larger pre-trained models can counter the trends observed by McKenzie et al. The presence of U-shaped curves suggests that relying on inverse scaling laws to predict a model's performance during training may not be accurate, as the performance could decrease, stabilize, or even improve.

Moreover, the study explores different prompting strategies, such as 1-shot in-context learning and chain-of-thought reasoning, aiming to tackle challenging tasks effectively with sufficiently large language models. The experimental results provide supporting evidence that the effect of inverse scaling can be mitigated when models are guided through demonstrations, enabling them to steer clear of ""distractor tasks"" that are concealed within each task in the Inverse Scaling Prize benchmark.","This study challenges the inverse scaling law, which indicated that an increase in pre-training compute leads to a decrease in performance. The authors demonstrate that certain models defy this scaling law, rendering it unsuitable for reliable extrapolation as an indicator of future model performance. Furthermore, the authors illustrate that adequately large language models are able to identify the ""true task"" from a single in-context example, while also presenting possibilities for future research on imitation strategies that eliminate the need for task-specific tuning or example selection.","There are no particular grounds for rejection. However, it would have been interesting if this study had incorporated more non-proprietary models that could be easily reproduced for further investigation.","Question A: Regarding the prompts shown in Figure 5, I am curious about the potential impact of the target label order on the models' performance. Zhao et al. [1] have discussed tendencies like a recency bias (models favoring the last target label) and a ""common token"" bias (frequently occurring answers from the pre-training data). Have you verified whether the labels are evenly distributed in the prompts for each task, ensuring that the correct answer is not disproportionately positioned as the first or second option?

Zhao et al. ""Calibrate Before Use: Improving Few-shot Performance of Language Models."" In Proceedings of the 38th International Conference on Machine Learning (ICML'21). https://proceedings.mlr.press/v139/zhao21c.html

",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,"I have two minor remarks regarding the presentation:

Figure 1: I understand that the purpose of this figure is to compare the trends of PaLM with previous models. However, I find the marker and line size to be slightly too large, making it difficult to discern the general trends of all the models.

Table 1: While I recognize that space constraints might have influenced its placement on the first page, I personally find that featuring something like the ""average"" subplot from Figure 1 would be more captivating for engaging a new reader.",1690748651617,,,EMNLP/2023/Conference,19sGqVUxQw,"['EMNLP/2023/Conference/Submission3146/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461211863,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3146/Reviewer_bS6X']",19sGqVUxQw,['EMNLP/2023/Conference/Submission3146/Reviewer_bS6X'],1690748651617,1701461211863,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3146/Reviewer_bS6X']","Yes

List of suggested papers or references not cited in the manuscript: 
1. Zhao et al. ""Calibrate Before Use: Improving Few-shot Performance of Language Models."" (This paper is actually already cited in the review, but it's possible the reviewer thinks it should be considered more prominently or its findings directly addressed in the manuscript.)

Brief explanation: 
The reviewer mentions Zhao et al. in the context of a specific question about the potential impact of target label order on model performance, referencing concepts like recency bias and ""common token"" bias. The suggestion is implicit in the question, implying that the authors should consider or discuss these biases and possibly how they relate to the findings in the paper, potentially requiring a deeper engagement with Zhao et al.'s work or similar studies on biases in language models. However, since Zhao et al. is already referenced, the main implication is about considering its findings more thoroughly rather than adding a new citation. The review does not explicitly suggest adding new citations beyond engaging more with the referenced paper's concepts.",1,,Calibrate Before Use: Improving Few-shot Performance of Language Models
Ww2Z3tc2Ce,"This paper proposes a novel perspective to understand kNN-MT by describing it as a special case of fine-tuning, specifically a process of meta-optimization on the Output Projection Layer (OPL) of NMT, and establish connections between kNN-MT and model fine-tuning. The
novel perspective on kNN-MT posits that (i) the working mechanism of kNN-MT is to implicitly execute gradient descent on OPL, producing meta-gradients via forward computation based on k-nearest-neighbors, and (ii) explicit fine-tuning on OPL shares a similar gradient format with the meta-gradients obtained by kNN-MT, according to the derivation of back-propagation.","This paper introduces a novel meta-optimization perspective for comprehending kNN-MT (k-Nearest Neighbors Machine Translation), while also forging connections between kNN-MT and general model fine-tuning. Through experimentation on multi-domain datasets, substantial evidence is provided to affirm the validity of this viewpoint. Key findings include: (i) the combination of kNN-MT with adapter-based fine-tuning, which yields translation quality on par with entire-model fine-tuning and exhibits superior performance on out-of-domain test sets; (ii) the identification of a shortcoming in kNN-based models pertaining to low recall of in-domain, low-frequency words, a challenge that can be alleviated through the optimization of representation vectors using lightweight adapter layers.",The authors do not compare their proposed approach with enough recent methods.,"1) A comprehensive comparison is requested between the presented method and the most recent advancements in Fast kNN-MT (https://aclanthology.org/2022.spanlp-1.3.pdf), considering both the translation quality and generation speed as critical parameters for evaluation.
2) An inquiry is raised regarding the effectiveness of the proposed method when applied to kNN-LM (k-Nearest Neighbors Language Models). Should this method prove effective in this context, a thorough discussion on the subject would be deemed valuable and pertinent.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691772845063,,,EMNLP/2023/Conference,18skb5S2Gv,"['EMNLP/2023/Conference/Submission1607/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461113807,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission1607/Reviewer_Gf9v']",18skb5S2Gv,['EMNLP/2023/Conference/Submission1607/Reviewer_Gf9v'],1691772845063,1701461113807,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1607/Reviewer_Gf9v']","Yes

* ""Fast kNN-MT"" (https://aclanthology.org/2022.spanlp-1.3.pdf)

The reviewer suggests that the authors should compare their proposed approach with recent methods, specifically ""Fast kNN-MT"", to evaluate both translation quality and generation speed. This implies that the authors should cite this paper and possibly other recent advancements in the field to provide a more comprehensive comparison and demonstrate the effectiveness of their method. The reviewer's comment implies that the manuscript lacks a thorough comparison with state-of-the-art methods, and including these citations would help to address this limitation.",1,2022,Fast kNN-MT
aMek4F2xfd,"The paper studies the open-ended molecule-to-text generation problem. The key point is to let language models understand the 2-dimensional molecule graph and translating the representations of 2D graphs into 1D soft prompts in the text space, and thus, bridging the gap between the graph encoder’s representation space and the LM’s input space. To this end, the author proposed MolCA and applies a three-stage training pipeline. In the first stage, a cross-modal projector and the graph-encoder are trained to extract the relevant molecule features to given the text, endowing the model with powerful molecule-text retrieval ability. In the second stage, the cross-modal projector is connected to a frozen LM and learn to produce soft prompts that the LM can understand. In the third stage, MolCA is fine-tuned for downstream generation tasks. Empirical results demonstrate that MolCA achieves SoTA results on various tasks, including molecule captioning and molecule-text retrieval.","Existing work exclusively utilizes 1D SMILES to represent molecules, while this paper proposed a a novel molecular language modeling method, which enable LMs to perceive molecules’ 2D graph representations. The model features a cross-modal Q-former to map representations of 2D graphs into the text space of LMs and applies an adapter for efficient downstream-finetuning. 

The author evaluate the proposed method for molecule captioning and IUPAC name prediction  task on PubChem324k and CheBI-20, demonstrating a consistent superiority to the baseline methods.  Additionally, the proposed method also outperform the existing baseline on the Molecule-Text Retrieval task on three benchmark datasets. The author also conducts ablation studies to demonstrate that combing 2D graphs and 1D SMILES leads to improved performance.","Although the author ablate MolCA’s capability of counting FGs types inside molecules to demonstrate the LMs could understand 2D molecular structure, there is a lack of further in-depth analysis of how well the Q-former can attend to the corresponding graph structure conditioning on the textual input. For example, when the model caption a specific function of the given molecule, does the corresponding graph nodes have a high attention value?","How could we improve the Q-former to make MolCA sufficient for practical application? In addition to collect a larger pre-training corpus, do we need other tasks besides molecular captioning to align the representations of 2D molecular structure to text representation space?","5: Excellent: This study is one of the most thorough I have seen, given its type.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"There are some work focusing on Graph-Aware Language Model Pre-Training, e.g., Xie, Han, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N. Ioannidis, Xiang Song, Qing Ping et al. ""Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications."" arXiv preprint arXiv:2306.02592 (2023).",,1691234933156,,,EMNLP/2023/Conference,14WRhMNq7H,"['EMNLP/2023/Conference/Submission3595/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461239583,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3595/Reviewer_YfAf']",14WRhMNq7H,['EMNLP/2023/Conference/Submission3595/Reviewer_YfAf'],1691234933156,1701461239583,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3595/Reviewer_YfAf']","Yes

List of suggested papers or references not cited in the manuscript:
1. Xie, Han, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N. Ioannidis, Xiang Song, Qing Ping et al. ""Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications."" arXiv preprint arXiv:2306.02592 (2023)

These citations might be necessary because the reviewer mentions ""Missing_References"" and specifically points out that there are works focusing on Graph-Aware Language Model Pre-Training, which is relevant to the topic of the submitted paper. The reviewer suggests that the authors should be aware of these works, implying that they could provide additional context, insights, or comparisons to enhance the submitted paper.",1,"2023, 2023",Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications
77NvJu48jU,"This paper proposes Self-ICL, a method for in-context learning in a zero-shot setup. Specifically, the model is first prompted to generate some pseudo-inputs and the model is then asked to respond to these pseudo-inputs to obtain a sequence of pseudo-demonstrations. The pseudo-demonstrations are then concatenated to the test queries in a manner similar to few-shot in-context learning to perform inference. The authors show effectiveness of their method on the BIG-Bench Hard problems with popular GPT models.","- The methodology is sensible, and I also see the value of this problem setting of extending in-context learning to a strictly zero-shot setup; previous works, like AutoCoT, are *transductive* zero-shot as unlabeled data are typically required -- I do have some concerns and questions regarding this, please see *Reasons to Reject* below.

- Experiments are largely thorough, and the choice of the datasets (the entire BIG-Bench Hard suite of 25 tasks) and models is convincing. The outperformance over the baselines seems significant, which is also supported by statistical tests. 
","- Related to the first point I mentioned in *Reasons to Accept*, my concern is as follows: while I see the academic value of the extension to ICL to a strictly zero-shot setup, I wonder whether the authors could clarify the practical significance? 
  - The proposed method requires the user to prompt the LLM to generate pseudo-inputs and answers to *each* of the test questions, and the user is then required to concatenate the pseudo-demos to the test queries. In contrast, competing methods like AutoCoT or [1] generate a set of pseudo-demos *per task* but requires the user to provide an unlabelled dataset (which is cheap). On balance, it seems there is still a significant burden on the users, and since the users now need to prompt for model-generated pseudo-inputs *and* the final predictions, the users need to query the model many more times, so I'm not sure to what extent the claim of ""bridging user's practical need"" is satisfied. 
   
  - Moreover, AutoCoT paper also has a section on the streaming setup where the unlabeled queries arrive in an online fashion; this is a practical enough setup, as we can always collect unlabeled user queries when the model is serving requests. I am requesting the authors to clarify better in what scenarios are their purely zero-shot setup particularly advantageous compared to these previous works?

- Related to what I mentioned above, given that we need to prompt the LLM more times per test query, the method is also more expensive. On the other hand, the baselines, such as zero-shot-CoT and zero-shot-direct, do not take advantage of the higher budget at all. One simple but effective way to improve the performance of the baselines is through self-consistency [2], especially for the CoT tasks. I wonder whether the authors could 1) state how much more expensive their method is compared to zs-cot and zs-direct, and 2) if the baselines are allowed to use the same amount of compute via self-consistency, is their method still better than the baselines? I think some evidence of this could alleviate the concern that the improvement is simply due to more computation used and can be easily bridged with a simpler alternative.

- Not a reason to reject, but simply a request for authors to comment: [1] is considered to be a concurrent work, but it considers a very related problem of using ICL in (transductive) zero-shot (similar to AutoCoT). They focus on a different angle in retrieving the best zero-shot demonstrations using a certainty estimate from self-consistency. For example, will the proposed method by the authors benefit from a similar selection procedure? I'd much appreciate if the authors could include discussions of these more recent related works in a revised version of their manuscript.

### References

[1]  Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.

[2] Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., ... & Zhou, D. (2022, September). Self-Consistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations.",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1690886601572,,,EMNLP/2023/Conference,0n92zm014A,"['EMNLP/2023/Conference/Submission3894/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461259051,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3894/Reviewer_aCDP']",0n92zm014A,['EMNLP/2023/Conference/Submission3894/Reviewer_aCDP'],1690886601572,1701461259051,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3894/Reviewer_aCDP']","Yes

The suggested papers or references not cited in the manuscript are:
1. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting.
2. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., ... & Zhou, D. (2022, September). Self-Consistency Improves Chain of Thought Reasoning in Language Models.

These citations might be necessary because the reviewer mentions that the authors could benefit from discussing related works, such as the concurrent work by Xingchen Wan et al., which focuses on a different angle of using in-context learning in zero-shot settings. The reviewer also suggests that the authors' method might be improved by incorporating self-consistency, as shown in the work by Wang et al. The addition of these references could provide a more comprehensive understanding of the research area and demonstrate how the authors' work relates to and builds upon existing research.",1,"2023, 2022",Better Zero-Shot Reasoning with Self-Adaptive Prompting 
77NvJu48jU,"This paper proposes Self-ICL, a method for in-context learning in a zero-shot setup. Specifically, the model is first prompted to generate some pseudo-inputs and the model is then asked to respond to these pseudo-inputs to obtain a sequence of pseudo-demonstrations. The pseudo-demonstrations are then concatenated to the test queries in a manner similar to few-shot in-context learning to perform inference. The authors show effectiveness of their method on the BIG-Bench Hard problems with popular GPT models.","- The methodology is sensible, and I also see the value of this problem setting of extending in-context learning to a strictly zero-shot setup; previous works, like AutoCoT, are *transductive* zero-shot as unlabeled data are typically required -- I do have some concerns and questions regarding this, please see *Reasons to Reject* below.

- Experiments are largely thorough, and the choice of the datasets (the entire BIG-Bench Hard suite of 25 tasks) and models is convincing. The outperformance over the baselines seems significant, which is also supported by statistical tests. 
","- Related to the first point I mentioned in *Reasons to Accept*, my concern is as follows: while I see the academic value of the extension to ICL to a strictly zero-shot setup, I wonder whether the authors could clarify the practical significance? 
  - The proposed method requires the user to prompt the LLM to generate pseudo-inputs and answers to *each* of the test questions, and the user is then required to concatenate the pseudo-demos to the test queries. In contrast, competing methods like AutoCoT or [1] generate a set of pseudo-demos *per task* but requires the user to provide an unlabelled dataset (which is cheap). On balance, it seems there is still a significant burden on the users, and since the users now need to prompt for model-generated pseudo-inputs *and* the final predictions, the users need to query the model many more times, so I'm not sure to what extent the claim of ""bridging user's practical need"" is satisfied. 
   
  - Moreover, AutoCoT paper also has a section on the streaming setup where the unlabeled queries arrive in an online fashion; this is a practical enough setup, as we can always collect unlabeled user queries when the model is serving requests. I am requesting the authors to clarify better in what scenarios are their purely zero-shot setup particularly advantageous compared to these previous works?

- Related to what I mentioned above, given that we need to prompt the LLM more times per test query, the method is also more expensive. On the other hand, the baselines, such as zero-shot-CoT and zero-shot-direct, do not take advantage of the higher budget at all. One simple but effective way to improve the performance of the baselines is through self-consistency [2], especially for the CoT tasks. I wonder whether the authors could 1) state how much more expensive their method is compared to zs-cot and zs-direct, and 2) if the baselines are allowed to use the same amount of compute via self-consistency, is their method still better than the baselines? I think some evidence of this could alleviate the concern that the improvement is simply due to more computation used and can be easily bridged with a simpler alternative.

- Not a reason to reject, but simply a request for authors to comment: [1] is considered to be a concurrent work, but it considers a very related problem of using ICL in (transductive) zero-shot (similar to AutoCoT). They focus on a different angle in retrieving the best zero-shot demonstrations using a certainty estimate from self-consistency. For example, will the proposed method by the authors benefit from a similar selection procedure? I'd much appreciate if the authors could include discussions of these more recent related works in a revised version of their manuscript.

### References

[1]  Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.

[2] Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., ... & Zhou, D. (2022, September). Self-Consistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations.",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1690886601572,,,EMNLP/2023/Conference,0n92zm014A,"['EMNLP/2023/Conference/Submission3894/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461259051,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3894/Reviewer_aCDP']",0n92zm014A,['EMNLP/2023/Conference/Submission3894/Reviewer_aCDP'],1690886601572,1701461259051,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3894/Reviewer_aCDP']","Yes

The suggested papers or references not cited in the manuscript are:
1. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting.
2. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., ... & Zhou, D. (2022, September). Self-Consistency Improves Chain of Thought Reasoning in Language Models.

These citations might be necessary because the reviewer mentions that the authors could benefit from discussing related works, such as the concurrent work by Xingchen Wan et al., which focuses on a different angle of using in-context learning in zero-shot settings. The reviewer also suggests that the authors' method might be improved by incorporating self-consistency, as shown in the work by Wang et al. The addition of these references could provide a more comprehensive understanding of the research area and demonstrate how the authors' work relates to and builds upon existing research.",1,"2023, 2022",Self-Consistency Improves Chain of Thought Reasoning in Language Models
uwmU0UkHFZ,"*  This paper provide a comprehensive review on factual knowledge probing in Pre-trained language models. They categorize relevant studies based on probing inputs, outputs and target LMs.
*  Moreover, this paper provides interesting discussion about knowledge retention, prompt optimization and obstacles of PLMs-as-KBs.


---

After rebuttal: thanks for providing such a comprehensive response. 
Overall, I appreciate the author's meticulous review and summarization of the entire field.

Regarding the response about differences between other surveys:
> In contrast to all existing surveys, we conduct a systematic analysis of methods for factual knowledge probing, providing a categorization scheme (that is admittedly novel and well-organized). We additionally provide a categorization of factual probing datasets. Furthermore, we discuss recent findings on knowledge retention, the use of optimized prompts, and challenges with corresponding recent solutions to adopting PLMs as KBs, shedding light on several future work directions.

I appreciate the categorization of relevant studies and probing datasets.
However, I still think that this review lacks novel insights: knowledge retention (the conclusion seems to be obvious), the use of optimized prompts can be found in [3], the discussions about LMs-as-KBs can be found in both [1][2][3].

Overall, I'm willing to increase the score on ""Excitement,"" **but I strongly recommend that the author incorporate more novel discussions in the revision**.","* The paper offers a comprehensive review of factual knowledge probing, the paper is well-written and easy to follow.
*  The categorization schema is novel and well-structured, providing a clear approach to organizing relevant studies.
* The article provides a comprehensive summary of the existing probing methods and datasets (as presented in Table 1 and Table 2).","* Regarding the novelty of the study: many discussions can be found in previous reviews with similar arguments, especially in the most intriguing parts, Section 4 and Section 5, where most of the viewpoints (prompt optimization , consistency, knowledge updating, Interpretability, etc.) have already been summarized in other surveys [1][2][3][4]...
* For a comprehensive review, the discussion on future directions and conclusions appears to be too brief, limiting the ability of future researchers to gain deeper insights.

[1] Language Models As or For Knowledge Bases

[2] A Review on Language Models as Knowledge Bases

[3] The Life Cycle of Knowledge in Big Language Models: A Survey

[4] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",* Can you describe in detail the main differences and novelty of this review and the other reviews mentioned above?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691162388638,,,EMNLP/2023/Conference,0kseDcA5Nm,"['EMNLP/2023/Conference/Submission2635/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461177973,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2635/Reviewer_avt8']",0kseDcA5Nm,['EMNLP/2023/Conference/Submission2635/Reviewer_avt8'],1691162388638,1701461177973,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2635/Reviewer_avt8']","Yes

The suggested papers or references not cited in the manuscript are:
[1] Language Models As or For Knowledge Bases
[2] A Review on Language Models as Knowledge Bases
[3] The Life Cycle of Knowledge in Big Language Models: A Survey
[4] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing

These citations might be necessary because the reviewer mentions that many discussions in the manuscript, especially in sections 4 and 5, have already been summarized in these other surveys. The reviewer suggests that the authors incorporate more novel discussions in the revision, implying that they should be aware of and engage with the existing literature, including these specific papers, to add more original insights to their review.",1,,Language Models As or For Knowledge Bases
uwmU0UkHFZ,"*  This paper provide a comprehensive review on factual knowledge probing in Pre-trained language models. They categorize relevant studies based on probing inputs, outputs and target LMs.
*  Moreover, this paper provides interesting discussion about knowledge retention, prompt optimization and obstacles of PLMs-as-KBs.


---

After rebuttal: thanks for providing such a comprehensive response. 
Overall, I appreciate the author's meticulous review and summarization of the entire field.

Regarding the response about differences between other surveys:
> In contrast to all existing surveys, we conduct a systematic analysis of methods for factual knowledge probing, providing a categorization scheme (that is admittedly novel and well-organized). We additionally provide a categorization of factual probing datasets. Furthermore, we discuss recent findings on knowledge retention, the use of optimized prompts, and challenges with corresponding recent solutions to adopting PLMs as KBs, shedding light on several future work directions.

I appreciate the categorization of relevant studies and probing datasets.
However, I still think that this review lacks novel insights: knowledge retention (the conclusion seems to be obvious), the use of optimized prompts can be found in [3], the discussions about LMs-as-KBs can be found in both [1][2][3].

Overall, I'm willing to increase the score on ""Excitement,"" **but I strongly recommend that the author incorporate more novel discussions in the revision**.","* The paper offers a comprehensive review of factual knowledge probing, the paper is well-written and easy to follow.
*  The categorization schema is novel and well-structured, providing a clear approach to organizing relevant studies.
* The article provides a comprehensive summary of the existing probing methods and datasets (as presented in Table 1 and Table 2).","* Regarding the novelty of the study: many discussions can be found in previous reviews with similar arguments, especially in the most intriguing parts, Section 4 and Section 5, where most of the viewpoints (prompt optimization , consistency, knowledge updating, Interpretability, etc.) have already been summarized in other surveys [1][2][3][4]...
* For a comprehensive review, the discussion on future directions and conclusions appears to be too brief, limiting the ability of future researchers to gain deeper insights.

[1] Language Models As or For Knowledge Bases

[2] A Review on Language Models as Knowledge Bases

[3] The Life Cycle of Knowledge in Big Language Models: A Survey

[4] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",* Can you describe in detail the main differences and novelty of this review and the other reviews mentioned above?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691162388638,,,EMNLP/2023/Conference,0kseDcA5Nm,"['EMNLP/2023/Conference/Submission2635/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461177973,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2635/Reviewer_avt8']",0kseDcA5Nm,['EMNLP/2023/Conference/Submission2635/Reviewer_avt8'],1691162388638,1701461177973,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2635/Reviewer_avt8']","Yes

The suggested papers or references not cited in the manuscript are:
[1] Language Models As or For Knowledge Bases
[2] A Review on Language Models as Knowledge Bases
[3] The Life Cycle of Knowledge in Big Language Models: A Survey
[4] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing

These citations might be necessary because the reviewer mentions that many discussions in the manuscript, especially in sections 4 and 5, have already been summarized in these other surveys. The reviewer suggests that the authors incorporate more novel discussions in the revision, implying that they should be aware of and engage with the existing literature, including these specific papers, to add more original insights to their review.",1,,A Review on Language Models as Knowledge Bases
uwmU0UkHFZ,"*  This paper provide a comprehensive review on factual knowledge probing in Pre-trained language models. They categorize relevant studies based on probing inputs, outputs and target LMs.
*  Moreover, this paper provides interesting discussion about knowledge retention, prompt optimization and obstacles of PLMs-as-KBs.


---

After rebuttal: thanks for providing such a comprehensive response. 
Overall, I appreciate the author's meticulous review and summarization of the entire field.

Regarding the response about differences between other surveys:
> In contrast to all existing surveys, we conduct a systematic analysis of methods for factual knowledge probing, providing a categorization scheme (that is admittedly novel and well-organized). We additionally provide a categorization of factual probing datasets. Furthermore, we discuss recent findings on knowledge retention, the use of optimized prompts, and challenges with corresponding recent solutions to adopting PLMs as KBs, shedding light on several future work directions.

I appreciate the categorization of relevant studies and probing datasets.
However, I still think that this review lacks novel insights: knowledge retention (the conclusion seems to be obvious), the use of optimized prompts can be found in [3], the discussions about LMs-as-KBs can be found in both [1][2][3].

Overall, I'm willing to increase the score on ""Excitement,"" **but I strongly recommend that the author incorporate more novel discussions in the revision**.","* The paper offers a comprehensive review of factual knowledge probing, the paper is well-written and easy to follow.
*  The categorization schema is novel and well-structured, providing a clear approach to organizing relevant studies.
* The article provides a comprehensive summary of the existing probing methods and datasets (as presented in Table 1 and Table 2).","* Regarding the novelty of the study: many discussions can be found in previous reviews with similar arguments, especially in the most intriguing parts, Section 4 and Section 5, where most of the viewpoints (prompt optimization , consistency, knowledge updating, Interpretability, etc.) have already been summarized in other surveys [1][2][3][4]...
* For a comprehensive review, the discussion on future directions and conclusions appears to be too brief, limiting the ability of future researchers to gain deeper insights.

[1] Language Models As or For Knowledge Bases

[2] A Review on Language Models as Knowledge Bases

[3] The Life Cycle of Knowledge in Big Language Models: A Survey

[4] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",* Can you describe in detail the main differences and novelty of this review and the other reviews mentioned above?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691162388638,,,EMNLP/2023/Conference,0kseDcA5Nm,"['EMNLP/2023/Conference/Submission2635/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461177973,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2635/Reviewer_avt8']",0kseDcA5Nm,['EMNLP/2023/Conference/Submission2635/Reviewer_avt8'],1691162388638,1701461177973,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2635/Reviewer_avt8']","Yes

The suggested papers or references not cited in the manuscript are:
[1] Language Models As or For Knowledge Bases
[2] A Review on Language Models as Knowledge Bases
[3] The Life Cycle of Knowledge in Big Language Models: A Survey
[4] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing

These citations might be necessary because the reviewer mentions that many discussions in the manuscript, especially in sections 4 and 5, have already been summarized in these other surveys. The reviewer suggests that the authors incorporate more novel discussions in the revision, implying that they should be aware of and engage with the existing literature, including these specific papers, to add more original insights to their review.",1,,The Life Cycle of Knowledge in Big Language Models A Survey
uwmU0UkHFZ,"*  This paper provide a comprehensive review on factual knowledge probing in Pre-trained language models. They categorize relevant studies based on probing inputs, outputs and target LMs.
*  Moreover, this paper provides interesting discussion about knowledge retention, prompt optimization and obstacles of PLMs-as-KBs.


---

After rebuttal: thanks for providing such a comprehensive response. 
Overall, I appreciate the author's meticulous review and summarization of the entire field.

Regarding the response about differences between other surveys:
> In contrast to all existing surveys, we conduct a systematic analysis of methods for factual knowledge probing, providing a categorization scheme (that is admittedly novel and well-organized). We additionally provide a categorization of factual probing datasets. Furthermore, we discuss recent findings on knowledge retention, the use of optimized prompts, and challenges with corresponding recent solutions to adopting PLMs as KBs, shedding light on several future work directions.

I appreciate the categorization of relevant studies and probing datasets.
However, I still think that this review lacks novel insights: knowledge retention (the conclusion seems to be obvious), the use of optimized prompts can be found in [3], the discussions about LMs-as-KBs can be found in both [1][2][3].

Overall, I'm willing to increase the score on ""Excitement,"" **but I strongly recommend that the author incorporate more novel discussions in the revision**.","* The paper offers a comprehensive review of factual knowledge probing, the paper is well-written and easy to follow.
*  The categorization schema is novel and well-structured, providing a clear approach to organizing relevant studies.
* The article provides a comprehensive summary of the existing probing methods and datasets (as presented in Table 1 and Table 2).","* Regarding the novelty of the study: many discussions can be found in previous reviews with similar arguments, especially in the most intriguing parts, Section 4 and Section 5, where most of the viewpoints (prompt optimization , consistency, knowledge updating, Interpretability, etc.) have already been summarized in other surveys [1][2][3][4]...
* For a comprehensive review, the discussion on future directions and conclusions appears to be too brief, limiting the ability of future researchers to gain deeper insights.

[1] Language Models As or For Knowledge Bases

[2] A Review on Language Models as Knowledge Bases

[3] The Life Cycle of Knowledge in Big Language Models: A Survey

[4] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",* Can you describe in detail the main differences and novelty of this review and the other reviews mentioned above?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",5: Could easily reproduce the results.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691162388638,,,EMNLP/2023/Conference,0kseDcA5Nm,"['EMNLP/2023/Conference/Submission2635/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461177973,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2635/Reviewer_avt8']",0kseDcA5Nm,['EMNLP/2023/Conference/Submission2635/Reviewer_avt8'],1691162388638,1701461177973,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2635/Reviewer_avt8']","Yes

The suggested papers or references not cited in the manuscript are:
[1] Language Models As or For Knowledge Bases
[2] A Review on Language Models as Knowledge Bases
[3] The Life Cycle of Knowledge in Big Language Models: A Survey
[4] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing

These citations might be necessary because the reviewer mentions that many discussions in the manuscript, especially in sections 4 and 5, have already been summarized in these other surveys. The reviewer suggests that the authors incorporate more novel discussions in the revision, implying that they should be aware of and engage with the existing literature, including these specific papers, to add more original insights to their review.",1,,Pre-train Prompt and Predict A Systematic Survey of Prompting Methods in Natural Language Processing
NrS4wrQKgN,"The paper deals with the task of document-level named entity recognition. The goal is to ensure the same NER tags are being predicted across all mentions of an entity in a document. To this end, the paper proposed a two-stage approach. In the first stage, the model uses a binary classifier to identify the potential mention spans and stores the surfaces in a key-value memory. In the second stage, the model aggregates the span representations of each unique surface and applies a multi-class classifier MLP to predict the tags. Experiments are done on biomedical, scientific, and general domain NER data sets. The results outperform all baselines considered in this paper.","1. The study is done across a wide variety of domains and shows consistently better performance across all data sets.
2. The method is simple and effective.
",1. The paper does not recognize the computational cost of the span-based approach as opposed to the sequence labeling approach. The complexity of the span-based approach is quadratic. So the binary classifier would need O(n*KC2) predictions. It is a limitation of any span-based approach. But I expected a discussion in the limitations section on this.,,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. “FLERT: Document-Level Features for Named Entity Recognition”, Schweter and Akbik,
2. “Exploiting Global Contextual Information for Document-level Named Entity Recognition”, Wang et al.
",Section 3.3 is hard to read with so many notations and subscripts. Please try to simplify it.,1691186328285,,,EMNLP/2023/Conference,0juZSwZLA4,"['EMNLP/2023/Conference/Submission3448/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461230481,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3448/Reviewer_3NNy']",0juZSwZLA4,['EMNLP/2023/Conference/Submission3448/Reviewer_3NNy'],1691186328285,1701461230481,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3448/Reviewer_3NNy']","Yes

The suggested papers or references not cited in the manuscript are:
1. “FLERT: Document-Level Features for Named Entity Recognition” by Schweter and Akbik
2. “Exploiting Global Contextual Information for Document-level Named Entity Recognition” by Wang et al.

These citations might be necessary because the reviewer mentions ""Missing_References"", implying that the paper could benefit from referencing these works. The reviewer likely believes that these papers are relevant to the topic of document-level named entity recognition and could provide additional context or insights that would strengthen the manuscript. By citing these papers, the authors could demonstrate a more comprehensive understanding of the existing research in the field and potentially improve their own methodology or discussion.",1,,FLERT Document-Level Features for Named Entity Recognition 
NrS4wrQKgN,"The paper deals with the task of document-level named entity recognition. The goal is to ensure the same NER tags are being predicted across all mentions of an entity in a document. To this end, the paper proposed a two-stage approach. In the first stage, the model uses a binary classifier to identify the potential mention spans and stores the surfaces in a key-value memory. In the second stage, the model aggregates the span representations of each unique surface and applies a multi-class classifier MLP to predict the tags. Experiments are done on biomedical, scientific, and general domain NER data sets. The results outperform all baselines considered in this paper.","1. The study is done across a wide variety of domains and shows consistently better performance across all data sets.
2. The method is simple and effective.
",1. The paper does not recognize the computational cost of the span-based approach as opposed to the sequence labeling approach. The complexity of the span-based approach is quadratic. So the binary classifier would need O(n*KC2) predictions. It is a limitation of any span-based approach. But I expected a discussion in the limitations section on this.,,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. “FLERT: Document-Level Features for Named Entity Recognition”, Schweter and Akbik,
2. “Exploiting Global Contextual Information for Document-level Named Entity Recognition”, Wang et al.
",Section 3.3 is hard to read with so many notations and subscripts. Please try to simplify it.,1691186328285,,,EMNLP/2023/Conference,0juZSwZLA4,"['EMNLP/2023/Conference/Submission3448/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461230481,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3448/Reviewer_3NNy']",0juZSwZLA4,['EMNLP/2023/Conference/Submission3448/Reviewer_3NNy'],1691186328285,1701461230481,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3448/Reviewer_3NNy']","Yes

The suggested papers or references not cited in the manuscript are:
1. “FLERT: Document-Level Features for Named Entity Recognition” by Schweter and Akbik
2. “Exploiting Global Contextual Information for Document-level Named Entity Recognition” by Wang et al.

These citations might be necessary because the reviewer mentions ""Missing_References"", implying that the paper could benefit from referencing these works. The reviewer likely believes that these papers are relevant to the topic of document-level named entity recognition and could provide additional context or insights that would strengthen the manuscript. By citing these papers, the authors could demonstrate a more comprehensive understanding of the existing research in the field and potentially improve their own methodology or discussion.",1,,Exploiting Global Contextual Information for Document-level Named Entity Recognition
nhwnujwuzc,"This paper focuses on Document-Level NER, based on the intuition that spans with the same surface form are more likely to have the same entity type. In this paper, the authors propose a 2-stage DL-NER model. In stage 1, they train a binary classifier to classify whether each token is an entity or not. In stage 2, they build a span key-value memory to fuse the features of spans that share the same surface form. They evaluate their model on NER datasets from three domains: biomedical, scientific, and general domain. In these three domains, they show that their model is better than all the baseline models, including other document-level NER models, and also sentence-level NER models. ","The paper is well written. 
The performance is good. Evaluating on different datasets from three domains, and still outperforming. 
","Span-based key-value memory is updated by the matching of the surface form, which does not consider the situation of entity coreference. ",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"You should cite ""Design Challenges and Misconceptions in Named Entity Recognition"" since this paper is the first to suggest global features. ",,1691394620617,,,EMNLP/2023/Conference,0juZSwZLA4,"['EMNLP/2023/Conference/Submission3448/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461230367,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3448/Reviewer_iSfy']",0juZSwZLA4,['EMNLP/2023/Conference/Submission3448/Reviewer_iSfy'],1691394620617,1701461230367,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3448/Reviewer_iSfy']","Yes

* ""Design Challenges and Misconceptions in Named Entity Recognition""

This citation might be necessary because the reviewer mentions that this paper is the first to suggest global features, which is relevant to the topic of the submitted manuscript, specifically the use of a span-based key-value memory to fuse features of spans with the same surface form. The reviewer suggests that the authors should cite this paper, implying that it is an important reference that has contributed to the development of the ideas presented in the manuscript.",1,,Design Challenges and Misconceptions in Named Entity Recognition
l1wmwaRRVf,The paper proposes to use policies like wait-k for simultaneous MT on word level rather than on subword level. The authors additionally suggest to use a language model (possibly with a different subword segmentation than the translation model) to further improve the quality of the translation; operating on word level makes it possible to apply the LM despite the differences in subword segmentation.,"One of the main claims of the paper that so far, the average lagging, one of the main ways of evaluating simultaneous MT systems, has happened on subword level. This claim is not true, and the idea to go from subword level to word level when computing the policy is not new at all either. The remaining part about using the LM is not enough for publication.

So overall, I see no reasons to accept the paper in its current form.","The main claim of the paper that so far the average lagging was computed on subword level and not on word level is simply not true. In fact, SimEval by default uses word level.  This is absolutely necessary to do to have fair comparison between submissions to competitive evaluations/shared tasks like Workshop on Simultaneous Translation, and also Simultaneous speech translation track at the IWSLT conference. 

Also, prior research definitely used word-level adaptive policies, like the phrases/chunks in one of the papers that I listed below - these were defined on the word level. 

Your use of word-level ""chunk attention"" is misleading - you seem to just attend to multiple subwords of the same word - this is not a chunk in any of the established meaning of this word. 

Overall, it is clear that the output of words, not subwords is expected in simultaneous MT, as they have to be presented to a user in form of words. How you read the input, in subwords, words, or directly from speech, or in characters - this is up to the architecture of the system, so there is nothing novel about going from subwords to words there. 

So overall, I see no merit in the paper, as its main claims have no ground.",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","From Line 42: ""The performance analysis and implementation of most SiMT systems, to the best of our knowledge, have been carried out on encoded sequences of source and target subwords, rather than on the original source and target sentences. This has led to two critical issues that need to be addressed.""

This is simply not true! And because of this, the paper has no value! The authors did add ""to the best of our knowledge"", but actually, this is a known fact for everyone working on simultaneous MT. 

=== After the author's rebuttal, I acknowledge that some papers still use BPE-level evaluation of simultaneous MT systems. Still, I believe that it is clear that this is not the right thing to do - yet I don't have any ethical concerns anymore after the authors re-worded the paper.","@inproceedings{papi2022does, title={Does Simultaneous Speech Translation need Simultaneous Models?}, author={Papi, Sara and Gaido, Marco and Negri, Matteo and Turchi, Marco}, booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022}, pages={141--153}, year={2022} }

@article{wilken2020neural, title={Neural Simultaneous Speech Translation Using Alignment-Based Chunking}, author={Wilken, Patrick and Alkhouli, Tamer and Matusov, Evgeny and Golik, Pavel}, journal={IWSLT 2020}, pages={237}, year={2020} }","Line 493: ""all tested latency settings"": according to Fig. 6b, this is not true: in the lowest latency setting of 2, the system without ""chunk attention"" is better.",1691163444258,,,EMNLP/2023/Conference,0ii51brFyn,"['EMNLP/2023/Conference/Submission65/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461005701,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission65/Reviewer_5oLi']",0ii51brFyn,['EMNLP/2023/Conference/Submission65/Reviewer_5oLi'],1691163444258,1701461005701,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission65/Reviewer_5oLi']","Yes

The suggested papers or references not cited in the manuscript are:
1. Papi et al. (2022) - ""Does Simultaneous Speech Translation need Simultaneous Models?""
2. Wilken et al. (2020) - ""Neural Simultaneous Speech Translation Using Alignment-Based Chunking""

These citations might be necessary because the reviewer claims that the main claims of the paper are not true and lack novelty. The reviewer states that prior research has already used word-level adaptive policies and that the idea of going from subword level to word level is not new. The suggested papers seem to provide evidence for this claim, with Wilken et al. (2020) using alignment-based chunking on the word level and Papi et al. (2022) potentially discussing related ideas in simultaneous speech translation. By citing these papers, the authors could address the reviewer's concerns and provide a more accurate representation of the state of the art in simultaneous machine translation.",1,"2022, 2020, 2020, 2022",Does Simultaneous Speech Translation need Simultaneous Models 
l1wmwaRRVf,The paper proposes to use policies like wait-k for simultaneous MT on word level rather than on subword level. The authors additionally suggest to use a language model (possibly with a different subword segmentation than the translation model) to further improve the quality of the translation; operating on word level makes it possible to apply the LM despite the differences in subword segmentation.,"One of the main claims of the paper that so far, the average lagging, one of the main ways of evaluating simultaneous MT systems, has happened on subword level. This claim is not true, and the idea to go from subword level to word level when computing the policy is not new at all either. The remaining part about using the LM is not enough for publication.

So overall, I see no reasons to accept the paper in its current form.","The main claim of the paper that so far the average lagging was computed on subword level and not on word level is simply not true. In fact, SimEval by default uses word level.  This is absolutely necessary to do to have fair comparison between submissions to competitive evaluations/shared tasks like Workshop on Simultaneous Translation, and also Simultaneous speech translation track at the IWSLT conference. 

Also, prior research definitely used word-level adaptive policies, like the phrases/chunks in one of the papers that I listed below - these were defined on the word level. 

Your use of word-level ""chunk attention"" is misleading - you seem to just attend to multiple subwords of the same word - this is not a chunk in any of the established meaning of this word. 

Overall, it is clear that the output of words, not subwords is expected in simultaneous MT, as they have to be presented to a user in form of words. How you read the input, in subwords, words, or directly from speech, or in characters - this is up to the architecture of the system, so there is nothing novel about going from subwords to words there. 

So overall, I see no merit in the paper, as its main claims have no ground.",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","From Line 42: ""The performance analysis and implementation of most SiMT systems, to the best of our knowledge, have been carried out on encoded sequences of source and target subwords, rather than on the original source and target sentences. This has led to two critical issues that need to be addressed.""

This is simply not true! And because of this, the paper has no value! The authors did add ""to the best of our knowledge"", but actually, this is a known fact for everyone working on simultaneous MT. 

=== After the author's rebuttal, I acknowledge that some papers still use BPE-level evaluation of simultaneous MT systems. Still, I believe that it is clear that this is not the right thing to do - yet I don't have any ethical concerns anymore after the authors re-worded the paper.","@inproceedings{papi2022does, title={Does Simultaneous Speech Translation need Simultaneous Models?}, author={Papi, Sara and Gaido, Marco and Negri, Matteo and Turchi, Marco}, booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022}, pages={141--153}, year={2022} }

@article{wilken2020neural, title={Neural Simultaneous Speech Translation Using Alignment-Based Chunking}, author={Wilken, Patrick and Alkhouli, Tamer and Matusov, Evgeny and Golik, Pavel}, journal={IWSLT 2020}, pages={237}, year={2020} }","Line 493: ""all tested latency settings"": according to Fig. 6b, this is not true: in the lowest latency setting of 2, the system without ""chunk attention"" is better.",1691163444258,,,EMNLP/2023/Conference,0ii51brFyn,"['EMNLP/2023/Conference/Submission65/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461005701,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission65/Reviewer_5oLi']",0ii51brFyn,['EMNLP/2023/Conference/Submission65/Reviewer_5oLi'],1691163444258,1701461005701,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission65/Reviewer_5oLi']","Yes

The suggested papers or references not cited in the manuscript are:
1. Papi et al. (2022) - ""Does Simultaneous Speech Translation need Simultaneous Models?""
2. Wilken et al. (2020) - ""Neural Simultaneous Speech Translation Using Alignment-Based Chunking""

These citations might be necessary because the reviewer claims that the main claims of the paper are not true and lack novelty. The reviewer states that prior research has already used word-level adaptive policies and that the idea of going from subword level to word level is not new. The suggested papers seem to provide evidence for this claim, with Wilken et al. (2020) using alignment-based chunking on the word level and Papi et al. (2022) potentially discussing related ideas in simultaneous speech translation. By citing these papers, the authors could address the reviewer's concerns and provide a more accurate representation of the state of the art in simultaneous machine translation.",1,"2022, 2020, 2020, 2022",Neural Simultaneous Speech Translation Using Alignment-Based Chunking
Vx1614fGcC,"This paper shows that moving from token-based to word-level policies in Simultaneous Machine Translation (SiMT) brings about improvements in translation quality leveraging all subwords that are part of the word being read and not writing in the middle of a word. In addition, and more importantly, it eases the integration of external LMs in order to improve SiMT models, and this is shown applying LM-fused attention previously applied in NMT. Finally, word-level policies allows for a fair comparative evaluation independently of the subword algorithm being applied. The results show a consistent improvements up to approximately 15%.",The improvements in translation quality are consistent and significant across k values in wait-k policies.,"The presentation and discussion of results need to be improved to put into perspective what contributions are having a more significant impact than others and under which conditions. For example, Figure 5 shows too many plots and too many curves, I would discard Transformer base plots and I would select those curves that clearly show the main conclusions that can be extracted from the experiments: word-level policies improve the results and adding a LM does much more. Those conditions that do not help much can be mentioned in the discussion. However, it would be nice to know what the contribution is for the word-level policies and the LMs and under which conditions one may contribute more than the other, if this is the case.

Word-level chunk attention is not clear to me since little detail is provided about how chunks are defined.

To sum up, the results are good and extensive, but are not easy to read in order to draw the main conclusions. The authors should have devoted more time to prepare the presentation of the results to ease the task of the reader.","Question A: How are the chunk boundaries defined? what are the parameters governing the model that takes the splitting decision? Obviously, the chunk size affects the quality of the bidirectional encoding, have you explored this point in your experiments? I guess bidirectional encoding is applied to all chunks, except for the last one in which unidirectional encoding would be applied, am I right?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"When discussing the encoding of the input sequence in lines 227-232, I think it is worth mentioning two additional references:

@inproceedings{iranzo-sanchez-etal-2022-simultaneous,
    title = ""From Simultaneous to Streaming Machine Translation by Leveraging Streaming History"",
    author = ""Iranzo Sanchez, Javier  and
      Civera, Jorge  and
      Juan-C{\'\i}scar, Alfons"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.480"",
    doi = ""10.18653/v1/2022.acl-long.480"",
    pages = ""6972--6985"",
    abstract = ""Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task"",
}

@inproceedings{kahardipraja-etal-2021-towards,
    title = ""Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental {NLU}"",
    author = ""Kahardipraja, Patrick  and
      Madureira, Brielen  and
      Schlangen, David"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.90"",
    doi = ""10.18653/v1/2021.emnlp-main.90"",
    pages = ""1178--1189"",
    abstract = ""Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs."",
}","Typos:

L. 142: attetnion -> attention
L. 216: , The goal -> , the goal
L. 232: unidirectionally**.** (Elbayad et al., 2020)
L. 147: missing words in ""Zhang and Feng (2022a) model to predict the alignment...""
L.422: BLEU calculation**.** (Post, 2018)
Table 2: An Eexample case -> An example case


Presentation improvements:

Tables 4-12 in the appendix could be more compacted and elaborated to ease the comparison across k values (since explored k values are different), relative improvements over the baseline would help to clearly see the impact of the different factors.

Figure 6 (a) and its description in the text need to be improved to make the description more consistent, you are using WW, TW, WT, but Read-k-Write-k to refer to TT policy.

I would discard Figure 6 (b), since it can be described with a sentence. In addition, this contribution is not clear to me since little detail is provided about how chunks are defined.

Figure 8 (a) contains too many curves that cannot be read properly. I would reduce the number of curves discarding those less significant in terms of impact in the results and describe these minor effects in the text. In addition, from an aesthetic viewpoint, I would try to use same font size across figures in page8 and shorthen legend description to not occlude the curves (see Figure 8 (a)).",1691172907356,,,EMNLP/2023/Conference,0ii51brFyn,"['EMNLP/2023/Conference/Submission65/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461005599,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission65/Reviewer_CvhJ']",0ii51brFyn,['EMNLP/2023/Conference/Submission65/Reviewer_CvhJ'],1691172907356,1701461005599,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission65/Reviewer_CvhJ']","Yes

The suggested papers or references not cited in the manuscript are:
1. Irzano-Sanchez et al. (2022) - ""From Simultaneous to Streaming Machine Translation by Leveraging Streaming History""
2. Kahardipraja et al. (2021) - ""Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU""

These citations might be necessary because the reviewer thinks they are relevant to the discussion of encoding input sequences in the manuscript, specifically in lines 227-232. The reviewer believes that mentioning these additional references would provide more context and support to the authors' discussion of simultaneous machine translation and incremental processing.",1,"2022, 2021",From Simultaneous to Streaming Machine Translation by Leveraging Streaming History 
Vx1614fGcC,"This paper shows that moving from token-based to word-level policies in Simultaneous Machine Translation (SiMT) brings about improvements in translation quality leveraging all subwords that are part of the word being read and not writing in the middle of a word. In addition, and more importantly, it eases the integration of external LMs in order to improve SiMT models, and this is shown applying LM-fused attention previously applied in NMT. Finally, word-level policies allows for a fair comparative evaluation independently of the subword algorithm being applied. The results show a consistent improvements up to approximately 15%.",The improvements in translation quality are consistent and significant across k values in wait-k policies.,"The presentation and discussion of results need to be improved to put into perspective what contributions are having a more significant impact than others and under which conditions. For example, Figure 5 shows too many plots and too many curves, I would discard Transformer base plots and I would select those curves that clearly show the main conclusions that can be extracted from the experiments: word-level policies improve the results and adding a LM does much more. Those conditions that do not help much can be mentioned in the discussion. However, it would be nice to know what the contribution is for the word-level policies and the LMs and under which conditions one may contribute more than the other, if this is the case.

Word-level chunk attention is not clear to me since little detail is provided about how chunks are defined.

To sum up, the results are good and extensive, but are not easy to read in order to draw the main conclusions. The authors should have devoted more time to prepare the presentation of the results to ease the task of the reader.","Question A: How are the chunk boundaries defined? what are the parameters governing the model that takes the splitting decision? Obviously, the chunk size affects the quality of the bidirectional encoding, have you explored this point in your experiments? I guess bidirectional encoding is applied to all chunks, except for the last one in which unidirectional encoding would be applied, am I right?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"When discussing the encoding of the input sequence in lines 227-232, I think it is worth mentioning two additional references:

@inproceedings{iranzo-sanchez-etal-2022-simultaneous,
    title = ""From Simultaneous to Streaming Machine Translation by Leveraging Streaming History"",
    author = ""Iranzo Sanchez, Javier  and
      Civera, Jorge  and
      Juan-C{\'\i}scar, Alfons"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.480"",
    doi = ""10.18653/v1/2022.acl-long.480"",
    pages = ""6972--6985"",
    abstract = ""Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task"",
}

@inproceedings{kahardipraja-etal-2021-towards,
    title = ""Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental {NLU}"",
    author = ""Kahardipraja, Patrick  and
      Madureira, Brielen  and
      Schlangen, David"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.90"",
    doi = ""10.18653/v1/2021.emnlp-main.90"",
    pages = ""1178--1189"",
    abstract = ""Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs."",
}","Typos:

L. 142: attetnion -> attention
L. 216: , The goal -> , the goal
L. 232: unidirectionally**.** (Elbayad et al., 2020)
L. 147: missing words in ""Zhang and Feng (2022a) model to predict the alignment...""
L.422: BLEU calculation**.** (Post, 2018)
Table 2: An Eexample case -> An example case


Presentation improvements:

Tables 4-12 in the appendix could be more compacted and elaborated to ease the comparison across k values (since explored k values are different), relative improvements over the baseline would help to clearly see the impact of the different factors.

Figure 6 (a) and its description in the text need to be improved to make the description more consistent, you are using WW, TW, WT, but Read-k-Write-k to refer to TT policy.

I would discard Figure 6 (b), since it can be described with a sentence. In addition, this contribution is not clear to me since little detail is provided about how chunks are defined.

Figure 8 (a) contains too many curves that cannot be read properly. I would reduce the number of curves discarding those less significant in terms of impact in the results and describe these minor effects in the text. In addition, from an aesthetic viewpoint, I would try to use same font size across figures in page8 and shorthen legend description to not occlude the curves (see Figure 8 (a)).",1691172907356,,,EMNLP/2023/Conference,0ii51brFyn,"['EMNLP/2023/Conference/Submission65/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461005599,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission65/Reviewer_CvhJ']",0ii51brFyn,['EMNLP/2023/Conference/Submission65/Reviewer_CvhJ'],1691172907356,1701461005599,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission65/Reviewer_CvhJ']","Yes

The suggested papers or references not cited in the manuscript are:
1. Irzano-Sanchez et al. (2022) - ""From Simultaneous to Streaming Machine Translation by Leveraging Streaming History""
2. Kahardipraja et al. (2021) - ""Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU""

These citations might be necessary because the reviewer thinks they are relevant to the discussion of encoding input sequences in the manuscript, specifically in lines 227-232. The reviewer believes that mentioning these additional references would provide more context and support to the authors' discussion of simultaneous machine translation and incremental processing.",1,"2022, 2021",Towards Incremental Transformers An Empirical Analysis of Transformer Models for Incremental NLU
28b5UnYZee,"This paper uses context-based causal intervention and prototype-based causal intervention to address the overfitting problem in the few-shot NER task. For 1-shot, the author uses prototype-based intervention to reduce the spurious correlation between the current prototype and the label. For 5-shot, the author introduces context-based intervention to replace the context to prevent overfitting. The experiments on various benchmarks demonstrate that this approach achieves new state-of-the-art results.","1. This paper uses causal intervention to solve the overfitting problem caused by data selection bias in the few-shot scenario, the research motivation is reasonable.
 
2. The author designs two causal intervention approach, named context-based causal intervention for 5-shot scenario and prototype-based causal intervention for 1-shot scenario. 
 
3. The author compared many novel few-shot NER methods in the experimental section and achieved SOTA results on the Few_NERD and SNIPS datasets, which have certain application values.","Insufficient in the experimental part:
1. Lack of case study analysis for more intuitive display.
2. The author only used GPT for data augmentation on the 1-shot setting, so what is the effect of using GPT for data augmentation on the 5-shot setting?
3. In Section 4.6, the author did not compare the effectiveness of using the sample reweighting method under the 5-shot setting.
4. In Section 4.7, the author just describes the word “German”, it is best to use visualization to demonstrate this case.

The paper needs further polishing and revision:
There are many formatting errors and inconsistent correspondence before and after the paper.","A. In Section 1, the author says “However, these existing methods tend to overlook the issue of overfitting caused by spurious correlation in few-shot tasks.” What elements are involved in this spurious association?
 
B. In Section 4, The author only used GPT for data augmentation on the 1-shot setting, so what is the effect of using GPT for data augmentation on the 5-shot setting?
 
C. In Section 4.2, the author describes utilizing the Bert-base-uncased model as the base model. If the author replaces Bert with Roberta and others pre-trained language models, will better results be achieved?

D. In Section 4.7, the author just describes the word “German”, it is best to use visualization to demonstrate this case. And need give more description to analyze the reason.

E. Besides the pre-trained method, will it effectively improve the accuracy of other models if the papers’ method is applied to other few-shot NER methods?
","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"There are some causal intervention approaches that need to be referenced.[1-3]
[1] Wu Y X, Wang X, Zhang A, et al. Discovering invariant rationales for graph neural networks[J]. arXiv preprint arXiv:2201.12872, 2022.
[2] Wu Q, Zhang H, Yan J, et al. Handling distribution shifts on graphs: An invariance perspective[J]. arXiv preprint arXiv:2202.02466, 2022.
[3] Fan S, Wang X, Mo Y, et al. Debiasing graph neural networks via learning disentangled causal substructure[J]. Advances in Neural Information Processing Systems, 2022, 35: 24934-24946.","1. In line 195, Reference “Pearl (2009)” has not been properly cited.

2. In line 239, the author uses “Equation1”, while in line 243, the author uses “Eq 2”, this method of referencing formulas is not uniform.
 
3. In line 420 the author uses “Few-NERD”, but in line 426, the author uses “FewNERD”, there exists the Inconsistent name of the dataset ""Few-NERD"".

4. In line 055-059, the case of ""animal"" and ""square"" is very farfetched, it would be great if you could provide some stronger examples.",1690235174640,,,EMNLP/2023/Conference,0iRgUfkwp3,"['EMNLP/2023/Conference/Submission3714/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461248833,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3714/Reviewer_c6Vr']",0iRgUfkwp3,['EMNLP/2023/Conference/Submission3714/Reviewer_c6Vr'],1690235174640,1701461248833,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3714/Reviewer_c6Vr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wu Y X, Wang X, Zhang A, et al. Discovering invariant rationales for graph neural networks[J]. arXiv preprint arXiv:2201.12872, 2022.
2. Wu Q, Zhang H, Yan J, et al. Handling distribution shifts on graphs: An invariance perspective[J]. arXiv preprint arXiv:2202.02466, 2022.
3. Fan S, Wang X, Mo Y, et al. Debiasing graph neural networks via learning disentangled causal substructure[J]. Advances in Neural Information Processing Systems, 2022, 35: 24934-24946.

These citations might be necessary because the reviewer mentions that there are some causal intervention approaches that need to be referenced, implying that the authors' work could be informed by or compared to existing research in this area, such as the papers listed. The reviewer specifically states ""Missing_References: There are some causal intervention approaches that need to be referenced"" and provides the list of papers, suggesting that including these references could strengthen the manuscript by situating it within the broader context of causal intervention research.",1,"2022, 2022, 2022, 2022, 2022",Discovering invariant rationales for graph neural networks
28b5UnYZee,"This paper uses context-based causal intervention and prototype-based causal intervention to address the overfitting problem in the few-shot NER task. For 1-shot, the author uses prototype-based intervention to reduce the spurious correlation between the current prototype and the label. For 5-shot, the author introduces context-based intervention to replace the context to prevent overfitting. The experiments on various benchmarks demonstrate that this approach achieves new state-of-the-art results.","1. This paper uses causal intervention to solve the overfitting problem caused by data selection bias in the few-shot scenario, the research motivation is reasonable.
 
2. The author designs two causal intervention approach, named context-based causal intervention for 5-shot scenario and prototype-based causal intervention for 1-shot scenario. 
 
3. The author compared many novel few-shot NER methods in the experimental section and achieved SOTA results on the Few_NERD and SNIPS datasets, which have certain application values.","Insufficient in the experimental part:
1. Lack of case study analysis for more intuitive display.
2. The author only used GPT for data augmentation on the 1-shot setting, so what is the effect of using GPT for data augmentation on the 5-shot setting?
3. In Section 4.6, the author did not compare the effectiveness of using the sample reweighting method under the 5-shot setting.
4. In Section 4.7, the author just describes the word “German”, it is best to use visualization to demonstrate this case.

The paper needs further polishing and revision:
There are many formatting errors and inconsistent correspondence before and after the paper.","A. In Section 1, the author says “However, these existing methods tend to overlook the issue of overfitting caused by spurious correlation in few-shot tasks.” What elements are involved in this spurious association?
 
B. In Section 4, The author only used GPT for data augmentation on the 1-shot setting, so what is the effect of using GPT for data augmentation on the 5-shot setting?
 
C. In Section 4.2, the author describes utilizing the Bert-base-uncased model as the base model. If the author replaces Bert with Roberta and others pre-trained language models, will better results be achieved?

D. In Section 4.7, the author just describes the word “German”, it is best to use visualization to demonstrate this case. And need give more description to analyze the reason.

E. Besides the pre-trained method, will it effectively improve the accuracy of other models if the papers’ method is applied to other few-shot NER methods?
","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"There are some causal intervention approaches that need to be referenced.[1-3]
[1] Wu Y X, Wang X, Zhang A, et al. Discovering invariant rationales for graph neural networks[J]. arXiv preprint arXiv:2201.12872, 2022.
[2] Wu Q, Zhang H, Yan J, et al. Handling distribution shifts on graphs: An invariance perspective[J]. arXiv preprint arXiv:2202.02466, 2022.
[3] Fan S, Wang X, Mo Y, et al. Debiasing graph neural networks via learning disentangled causal substructure[J]. Advances in Neural Information Processing Systems, 2022, 35: 24934-24946.","1. In line 195, Reference “Pearl (2009)” has not been properly cited.

2. In line 239, the author uses “Equation1”, while in line 243, the author uses “Eq 2”, this method of referencing formulas is not uniform.
 
3. In line 420 the author uses “Few-NERD”, but in line 426, the author uses “FewNERD”, there exists the Inconsistent name of the dataset ""Few-NERD"".

4. In line 055-059, the case of ""animal"" and ""square"" is very farfetched, it would be great if you could provide some stronger examples.",1690235174640,,,EMNLP/2023/Conference,0iRgUfkwp3,"['EMNLP/2023/Conference/Submission3714/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461248833,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3714/Reviewer_c6Vr']",0iRgUfkwp3,['EMNLP/2023/Conference/Submission3714/Reviewer_c6Vr'],1690235174640,1701461248833,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3714/Reviewer_c6Vr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wu Y X, Wang X, Zhang A, et al. Discovering invariant rationales for graph neural networks[J]. arXiv preprint arXiv:2201.12872, 2022.
2. Wu Q, Zhang H, Yan J, et al. Handling distribution shifts on graphs: An invariance perspective[J]. arXiv preprint arXiv:2202.02466, 2022.
3. Fan S, Wang X, Mo Y, et al. Debiasing graph neural networks via learning disentangled causal substructure[J]. Advances in Neural Information Processing Systems, 2022, 35: 24934-24946.

These citations might be necessary because the reviewer mentions that there are some causal intervention approaches that need to be referenced, implying that the authors' work could be informed by or compared to existing research in this area, such as the papers listed. The reviewer specifically states ""Missing_References: There are some causal intervention approaches that need to be referenced"" and provides the list of papers, suggesting that including these references could strengthen the manuscript by situating it within the broader context of causal intervention research.",1,"2022, 2022, 2022, 2022, 2022",Handling distribution shifts on graphs An invariance perspective
28b5UnYZee,"This paper uses context-based causal intervention and prototype-based causal intervention to address the overfitting problem in the few-shot NER task. For 1-shot, the author uses prototype-based intervention to reduce the spurious correlation between the current prototype and the label. For 5-shot, the author introduces context-based intervention to replace the context to prevent overfitting. The experiments on various benchmarks demonstrate that this approach achieves new state-of-the-art results.","1. This paper uses causal intervention to solve the overfitting problem caused by data selection bias in the few-shot scenario, the research motivation is reasonable.
 
2. The author designs two causal intervention approach, named context-based causal intervention for 5-shot scenario and prototype-based causal intervention for 1-shot scenario. 
 
3. The author compared many novel few-shot NER methods in the experimental section and achieved SOTA results on the Few_NERD and SNIPS datasets, which have certain application values.","Insufficient in the experimental part:
1. Lack of case study analysis for more intuitive display.
2. The author only used GPT for data augmentation on the 1-shot setting, so what is the effect of using GPT for data augmentation on the 5-shot setting?
3. In Section 4.6, the author did not compare the effectiveness of using the sample reweighting method under the 5-shot setting.
4. In Section 4.7, the author just describes the word “German”, it is best to use visualization to demonstrate this case.

The paper needs further polishing and revision:
There are many formatting errors and inconsistent correspondence before and after the paper.","A. In Section 1, the author says “However, these existing methods tend to overlook the issue of overfitting caused by spurious correlation in few-shot tasks.” What elements are involved in this spurious association?
 
B. In Section 4, The author only used GPT for data augmentation on the 1-shot setting, so what is the effect of using GPT for data augmentation on the 5-shot setting?
 
C. In Section 4.2, the author describes utilizing the Bert-base-uncased model as the base model. If the author replaces Bert with Roberta and others pre-trained language models, will better results be achieved?

D. In Section 4.7, the author just describes the word “German”, it is best to use visualization to demonstrate this case. And need give more description to analyze the reason.

E. Besides the pre-trained method, will it effectively improve the accuracy of other models if the papers’ method is applied to other few-shot NER methods?
","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"There are some causal intervention approaches that need to be referenced.[1-3]
[1] Wu Y X, Wang X, Zhang A, et al. Discovering invariant rationales for graph neural networks[J]. arXiv preprint arXiv:2201.12872, 2022.
[2] Wu Q, Zhang H, Yan J, et al. Handling distribution shifts on graphs: An invariance perspective[J]. arXiv preprint arXiv:2202.02466, 2022.
[3] Fan S, Wang X, Mo Y, et al. Debiasing graph neural networks via learning disentangled causal substructure[J]. Advances in Neural Information Processing Systems, 2022, 35: 24934-24946.","1. In line 195, Reference “Pearl (2009)” has not been properly cited.

2. In line 239, the author uses “Equation1”, while in line 243, the author uses “Eq 2”, this method of referencing formulas is not uniform.
 
3. In line 420 the author uses “Few-NERD”, but in line 426, the author uses “FewNERD”, there exists the Inconsistent name of the dataset ""Few-NERD"".

4. In line 055-059, the case of ""animal"" and ""square"" is very farfetched, it would be great if you could provide some stronger examples.",1690235174640,,,EMNLP/2023/Conference,0iRgUfkwp3,"['EMNLP/2023/Conference/Submission3714/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461248833,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission3714/Reviewer_c6Vr']",0iRgUfkwp3,['EMNLP/2023/Conference/Submission3714/Reviewer_c6Vr'],1690235174640,1701461248833,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3714/Reviewer_c6Vr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wu Y X, Wang X, Zhang A, et al. Discovering invariant rationales for graph neural networks[J]. arXiv preprint arXiv:2201.12872, 2022.
2. Wu Q, Zhang H, Yan J, et al. Handling distribution shifts on graphs: An invariance perspective[J]. arXiv preprint arXiv:2202.02466, 2022.
3. Fan S, Wang X, Mo Y, et al. Debiasing graph neural networks via learning disentangled causal substructure[J]. Advances in Neural Information Processing Systems, 2022, 35: 24934-24946.

These citations might be necessary because the reviewer mentions that there are some causal intervention approaches that need to be referenced, implying that the authors' work could be informed by or compared to existing research in this area, such as the papers listed. The reviewer specifically states ""Missing_References: There are some causal intervention approaches that need to be referenced"" and provides the list of papers, suggesting that including these references could strengthen the manuscript by situating it within the broader context of causal intervention research.",1,"2022, 2022, 2022, 2022, 2022",Debiasing graph neural networks via learning disentangled causal substructure
6Pfs7uwiBa,"This paper studies news recommendation. The authors propose a model TADI that can obtain good performance via dual interaction and the interaction efficiency is high. The core contribution is the proposed topic-aware attention and dual-encoder interaction module, which can achieve effective embedding learning and interaction between news and user sides and keep good efficiency in the inference stage.","1. The writing is clear and easy to understand.

2. The proposed model is modularized making it easily adapted to larger content-based encoders, for example, recent LLM models.

3. The motivation is clear and the model design is rational.
","1. The authors seem to overlook a crucial problem: the training and inference ranking objective is discrepant. Specifically, the training objective of is $a\mathcal{\hat{L}}+b\mathcal{\hat{L}'}+(1-a-b)\mathcal{\hat{L}''}$, whereas the inference target is only related to $\mathcal{\hat{L}}$. This training/inference discrepancy is not beneficial to model performance (even harmful from my empirical experience on score-oriented recommendation, but I keep neutral to this point). The major reason could be multi-task gradient conflict [1, 2]. That is, the multi-task objectives tend to be disentangled and conflicted, one objective cannot well represent the other. In this paper, $\mathcal{\hat{L}}$, $\mathcal{\hat{L}'}$ and $\mathcal{\hat{L}''}$ are jointly optimized in training, solely evaluating based on $\mathcal{\hat{L}}$ may be inferior. The author may consider gradient surgery [2] to alleviate this problem. Frustratingly, previous studies and industry practice in single/dual recommender encoders show that the interaction performance and efficiency are always in a trade-off relation.

2. Because this paper claims an efficient interaction module, efficiency comparisons with baselines in inference are very important and should be done, but we cannot see it in the experiment tables. The authors claim the overhead of dot product on embeddings is ""1/300 in contrast to HieRec"" in L500. I think this is not rigorous because the authors neither give detailed inference time comparisons nor give more comprehensive explanations, given the fact that dot-product on embeddings is just a very lightweight computation compared to previous FCN and Transformer computation.

3. Experiments three times on MIND-small and once on MIND-large is not statistically supportive. This is because MIND-small is literally small and always leads to high experiment variance. This may be the reason why the curve in Figure 5 fluctuates. I think more experiment trials like previous papers [3, 4] can better support the paper.

4. Topic-aware attention was widely studied in previous papers [3, 4, 5], which is somehow incremental.


[1] Gradient Surgery for Multi-Task Learning. Neurips 2020.

[2] Conflict-Averse Gradient Descent for Multi-task Learning. Neurips 2021.

[3] HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation. ACL 2021.

[4] MINER: Multi-Interest Matching Network for News Recommendation. ACL 2022.

[5] Neural news recommendation with topic-aware news representation. ACL 2019.",Could you provide inference time compared to the baselines to justify the efficiency of DI?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"L86: ""To response the"" -> ""To response to the""

L380 Equation (5): ""s.t."" abbreviates ""subject to"" meaning the constraint term in an optimization problem, here ""where"" is more appropriate.

L506: ""encode title"" -> ""encode titles""

L527: ""further decline"" -> ""further declines""

A suggestion: the word ""aux"" should be ""auxiliary"" in formal paper writing.",1691057593151,,,EMNLP/2023/Conference,0hyn6MJmnP,"['EMNLP/2023/Conference/Submission3041/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206144,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']",0hyn6MJmnP,['EMNLP/2023/Conference/Submission3041/Reviewer_Gomy'],1691057593151,1701461206144,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Gradient Surgery for Multi-Task Learning"" (Neurips 2020)
2. ""Conflict-Averse Gradient Descent for Multi-task Learning"" (Neurips 2021)
3. ""HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation"" (ACL 2021)
4. ""MINER: Multi-Interest Matching Network for News Recommendation"" (ACL 2022)
5. ""Neural news recommendation with topic-aware news representation"" (ACL 2019)

These citations might be necessary because the reviewer mentions that the authors overlook a crucial problem of training and inference ranking objective discrepancy, which is discussed in [1] and [2]. The reviewer also suggests comparing the efficiency of the proposed model with baselines, referencing [3]. Additionally, the reviewer notes that topic-aware attention was widely studied in previous papers [3, 4, 5], implying that the authors should be aware of and cite these relevant works to contextualize their contributions.",1,"2020, 2021, 2021, 2022, 2019",Gradient Surgery for Multi-Task Learning
6Pfs7uwiBa,"This paper studies news recommendation. The authors propose a model TADI that can obtain good performance via dual interaction and the interaction efficiency is high. The core contribution is the proposed topic-aware attention and dual-encoder interaction module, which can achieve effective embedding learning and interaction between news and user sides and keep good efficiency in the inference stage.","1. The writing is clear and easy to understand.

2. The proposed model is modularized making it easily adapted to larger content-based encoders, for example, recent LLM models.

3. The motivation is clear and the model design is rational.
","1. The authors seem to overlook a crucial problem: the training and inference ranking objective is discrepant. Specifically, the training objective of is $a\mathcal{\hat{L}}+b\mathcal{\hat{L}'}+(1-a-b)\mathcal{\hat{L}''}$, whereas the inference target is only related to $\mathcal{\hat{L}}$. This training/inference discrepancy is not beneficial to model performance (even harmful from my empirical experience on score-oriented recommendation, but I keep neutral to this point). The major reason could be multi-task gradient conflict [1, 2]. That is, the multi-task objectives tend to be disentangled and conflicted, one objective cannot well represent the other. In this paper, $\mathcal{\hat{L}}$, $\mathcal{\hat{L}'}$ and $\mathcal{\hat{L}''}$ are jointly optimized in training, solely evaluating based on $\mathcal{\hat{L}}$ may be inferior. The author may consider gradient surgery [2] to alleviate this problem. Frustratingly, previous studies and industry practice in single/dual recommender encoders show that the interaction performance and efficiency are always in a trade-off relation.

2. Because this paper claims an efficient interaction module, efficiency comparisons with baselines in inference are very important and should be done, but we cannot see it in the experiment tables. The authors claim the overhead of dot product on embeddings is ""1/300 in contrast to HieRec"" in L500. I think this is not rigorous because the authors neither give detailed inference time comparisons nor give more comprehensive explanations, given the fact that dot-product on embeddings is just a very lightweight computation compared to previous FCN and Transformer computation.

3. Experiments three times on MIND-small and once on MIND-large is not statistically supportive. This is because MIND-small is literally small and always leads to high experiment variance. This may be the reason why the curve in Figure 5 fluctuates. I think more experiment trials like previous papers [3, 4] can better support the paper.

4. Topic-aware attention was widely studied in previous papers [3, 4, 5], which is somehow incremental.


[1] Gradient Surgery for Multi-Task Learning. Neurips 2020.

[2] Conflict-Averse Gradient Descent for Multi-task Learning. Neurips 2021.

[3] HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation. ACL 2021.

[4] MINER: Multi-Interest Matching Network for News Recommendation. ACL 2022.

[5] Neural news recommendation with topic-aware news representation. ACL 2019.",Could you provide inference time compared to the baselines to justify the efficiency of DI?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"L86: ""To response the"" -> ""To response to the""

L380 Equation (5): ""s.t."" abbreviates ""subject to"" meaning the constraint term in an optimization problem, here ""where"" is more appropriate.

L506: ""encode title"" -> ""encode titles""

L527: ""further decline"" -> ""further declines""

A suggestion: the word ""aux"" should be ""auxiliary"" in formal paper writing.",1691057593151,,,EMNLP/2023/Conference,0hyn6MJmnP,"['EMNLP/2023/Conference/Submission3041/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206144,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']",0hyn6MJmnP,['EMNLP/2023/Conference/Submission3041/Reviewer_Gomy'],1691057593151,1701461206144,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Gradient Surgery for Multi-Task Learning"" (Neurips 2020)
2. ""Conflict-Averse Gradient Descent for Multi-task Learning"" (Neurips 2021)
3. ""HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation"" (ACL 2021)
4. ""MINER: Multi-Interest Matching Network for News Recommendation"" (ACL 2022)
5. ""Neural news recommendation with topic-aware news representation"" (ACL 2019)

These citations might be necessary because the reviewer mentions that the authors overlook a crucial problem of training and inference ranking objective discrepancy, which is discussed in [1] and [2]. The reviewer also suggests comparing the efficiency of the proposed model with baselines, referencing [3]. Additionally, the reviewer notes that topic-aware attention was widely studied in previous papers [3, 4, 5], implying that the authors should be aware of and cite these relevant works to contextualize their contributions.",1,"2020, 2021, 2021, 2022, 2019",Conflict-Averse Gradient Descent for Multi-task Learning
6Pfs7uwiBa,"This paper studies news recommendation. The authors propose a model TADI that can obtain good performance via dual interaction and the interaction efficiency is high. The core contribution is the proposed topic-aware attention and dual-encoder interaction module, which can achieve effective embedding learning and interaction between news and user sides and keep good efficiency in the inference stage.","1. The writing is clear and easy to understand.

2. The proposed model is modularized making it easily adapted to larger content-based encoders, for example, recent LLM models.

3. The motivation is clear and the model design is rational.
","1. The authors seem to overlook a crucial problem: the training and inference ranking objective is discrepant. Specifically, the training objective of is $a\mathcal{\hat{L}}+b\mathcal{\hat{L}'}+(1-a-b)\mathcal{\hat{L}''}$, whereas the inference target is only related to $\mathcal{\hat{L}}$. This training/inference discrepancy is not beneficial to model performance (even harmful from my empirical experience on score-oriented recommendation, but I keep neutral to this point). The major reason could be multi-task gradient conflict [1, 2]. That is, the multi-task objectives tend to be disentangled and conflicted, one objective cannot well represent the other. In this paper, $\mathcal{\hat{L}}$, $\mathcal{\hat{L}'}$ and $\mathcal{\hat{L}''}$ are jointly optimized in training, solely evaluating based on $\mathcal{\hat{L}}$ may be inferior. The author may consider gradient surgery [2] to alleviate this problem. Frustratingly, previous studies and industry practice in single/dual recommender encoders show that the interaction performance and efficiency are always in a trade-off relation.

2. Because this paper claims an efficient interaction module, efficiency comparisons with baselines in inference are very important and should be done, but we cannot see it in the experiment tables. The authors claim the overhead of dot product on embeddings is ""1/300 in contrast to HieRec"" in L500. I think this is not rigorous because the authors neither give detailed inference time comparisons nor give more comprehensive explanations, given the fact that dot-product on embeddings is just a very lightweight computation compared to previous FCN and Transformer computation.

3. Experiments three times on MIND-small and once on MIND-large is not statistically supportive. This is because MIND-small is literally small and always leads to high experiment variance. This may be the reason why the curve in Figure 5 fluctuates. I think more experiment trials like previous papers [3, 4] can better support the paper.

4. Topic-aware attention was widely studied in previous papers [3, 4, 5], which is somehow incremental.


[1] Gradient Surgery for Multi-Task Learning. Neurips 2020.

[2] Conflict-Averse Gradient Descent for Multi-task Learning. Neurips 2021.

[3] HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation. ACL 2021.

[4] MINER: Multi-Interest Matching Network for News Recommendation. ACL 2022.

[5] Neural news recommendation with topic-aware news representation. ACL 2019.",Could you provide inference time compared to the baselines to justify the efficiency of DI?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"L86: ""To response the"" -> ""To response to the""

L380 Equation (5): ""s.t."" abbreviates ""subject to"" meaning the constraint term in an optimization problem, here ""where"" is more appropriate.

L506: ""encode title"" -> ""encode titles""

L527: ""further decline"" -> ""further declines""

A suggestion: the word ""aux"" should be ""auxiliary"" in formal paper writing.",1691057593151,,,EMNLP/2023/Conference,0hyn6MJmnP,"['EMNLP/2023/Conference/Submission3041/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206144,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']",0hyn6MJmnP,['EMNLP/2023/Conference/Submission3041/Reviewer_Gomy'],1691057593151,1701461206144,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Gradient Surgery for Multi-Task Learning"" (Neurips 2020)
2. ""Conflict-Averse Gradient Descent for Multi-task Learning"" (Neurips 2021)
3. ""HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation"" (ACL 2021)
4. ""MINER: Multi-Interest Matching Network for News Recommendation"" (ACL 2022)
5. ""Neural news recommendation with topic-aware news representation"" (ACL 2019)

These citations might be necessary because the reviewer mentions that the authors overlook a crucial problem of training and inference ranking objective discrepancy, which is discussed in [1] and [2]. The reviewer also suggests comparing the efficiency of the proposed model with baselines, referencing [3]. Additionally, the reviewer notes that topic-aware attention was widely studied in previous papers [3, 4, 5], implying that the authors should be aware of and cite these relevant works to contextualize their contributions.",1,"2020, 2021, 2021, 2022, 2019",HieRec Hierarchical User Interest Modeling for Personalized News Recommendation
6Pfs7uwiBa,"This paper studies news recommendation. The authors propose a model TADI that can obtain good performance via dual interaction and the interaction efficiency is high. The core contribution is the proposed topic-aware attention and dual-encoder interaction module, which can achieve effective embedding learning and interaction between news and user sides and keep good efficiency in the inference stage.","1. The writing is clear and easy to understand.

2. The proposed model is modularized making it easily adapted to larger content-based encoders, for example, recent LLM models.

3. The motivation is clear and the model design is rational.
","1. The authors seem to overlook a crucial problem: the training and inference ranking objective is discrepant. Specifically, the training objective of is $a\mathcal{\hat{L}}+b\mathcal{\hat{L}'}+(1-a-b)\mathcal{\hat{L}''}$, whereas the inference target is only related to $\mathcal{\hat{L}}$. This training/inference discrepancy is not beneficial to model performance (even harmful from my empirical experience on score-oriented recommendation, but I keep neutral to this point). The major reason could be multi-task gradient conflict [1, 2]. That is, the multi-task objectives tend to be disentangled and conflicted, one objective cannot well represent the other. In this paper, $\mathcal{\hat{L}}$, $\mathcal{\hat{L}'}$ and $\mathcal{\hat{L}''}$ are jointly optimized in training, solely evaluating based on $\mathcal{\hat{L}}$ may be inferior. The author may consider gradient surgery [2] to alleviate this problem. Frustratingly, previous studies and industry practice in single/dual recommender encoders show that the interaction performance and efficiency are always in a trade-off relation.

2. Because this paper claims an efficient interaction module, efficiency comparisons with baselines in inference are very important and should be done, but we cannot see it in the experiment tables. The authors claim the overhead of dot product on embeddings is ""1/300 in contrast to HieRec"" in L500. I think this is not rigorous because the authors neither give detailed inference time comparisons nor give more comprehensive explanations, given the fact that dot-product on embeddings is just a very lightweight computation compared to previous FCN and Transformer computation.

3. Experiments three times on MIND-small and once on MIND-large is not statistically supportive. This is because MIND-small is literally small and always leads to high experiment variance. This may be the reason why the curve in Figure 5 fluctuates. I think more experiment trials like previous papers [3, 4] can better support the paper.

4. Topic-aware attention was widely studied in previous papers [3, 4, 5], which is somehow incremental.


[1] Gradient Surgery for Multi-Task Learning. Neurips 2020.

[2] Conflict-Averse Gradient Descent for Multi-task Learning. Neurips 2021.

[3] HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation. ACL 2021.

[4] MINER: Multi-Interest Matching Network for News Recommendation. ACL 2022.

[5] Neural news recommendation with topic-aware news representation. ACL 2019.",Could you provide inference time compared to the baselines to justify the efficiency of DI?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"L86: ""To response the"" -> ""To response to the""

L380 Equation (5): ""s.t."" abbreviates ""subject to"" meaning the constraint term in an optimization problem, here ""where"" is more appropriate.

L506: ""encode title"" -> ""encode titles""

L527: ""further decline"" -> ""further declines""

A suggestion: the word ""aux"" should be ""auxiliary"" in formal paper writing.",1691057593151,,,EMNLP/2023/Conference,0hyn6MJmnP,"['EMNLP/2023/Conference/Submission3041/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206144,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']",0hyn6MJmnP,['EMNLP/2023/Conference/Submission3041/Reviewer_Gomy'],1691057593151,1701461206144,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Gradient Surgery for Multi-Task Learning"" (Neurips 2020)
2. ""Conflict-Averse Gradient Descent for Multi-task Learning"" (Neurips 2021)
3. ""HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation"" (ACL 2021)
4. ""MINER: Multi-Interest Matching Network for News Recommendation"" (ACL 2022)
5. ""Neural news recommendation with topic-aware news representation"" (ACL 2019)

These citations might be necessary because the reviewer mentions that the authors overlook a crucial problem of training and inference ranking objective discrepancy, which is discussed in [1] and [2]. The reviewer also suggests comparing the efficiency of the proposed model with baselines, referencing [3]. Additionally, the reviewer notes that topic-aware attention was widely studied in previous papers [3, 4, 5], implying that the authors should be aware of and cite these relevant works to contextualize their contributions.",1,"2020, 2021, 2021, 2022, 2019",MINER Multi-Interest Matching Network for News Recommendation
6Pfs7uwiBa,"This paper studies news recommendation. The authors propose a model TADI that can obtain good performance via dual interaction and the interaction efficiency is high. The core contribution is the proposed topic-aware attention and dual-encoder interaction module, which can achieve effective embedding learning and interaction between news and user sides and keep good efficiency in the inference stage.","1. The writing is clear and easy to understand.

2. The proposed model is modularized making it easily adapted to larger content-based encoders, for example, recent LLM models.

3. The motivation is clear and the model design is rational.
","1. The authors seem to overlook a crucial problem: the training and inference ranking objective is discrepant. Specifically, the training objective of is $a\mathcal{\hat{L}}+b\mathcal{\hat{L}'}+(1-a-b)\mathcal{\hat{L}''}$, whereas the inference target is only related to $\mathcal{\hat{L}}$. This training/inference discrepancy is not beneficial to model performance (even harmful from my empirical experience on score-oriented recommendation, but I keep neutral to this point). The major reason could be multi-task gradient conflict [1, 2]. That is, the multi-task objectives tend to be disentangled and conflicted, one objective cannot well represent the other. In this paper, $\mathcal{\hat{L}}$, $\mathcal{\hat{L}'}$ and $\mathcal{\hat{L}''}$ are jointly optimized in training, solely evaluating based on $\mathcal{\hat{L}}$ may be inferior. The author may consider gradient surgery [2] to alleviate this problem. Frustratingly, previous studies and industry practice in single/dual recommender encoders show that the interaction performance and efficiency are always in a trade-off relation.

2. Because this paper claims an efficient interaction module, efficiency comparisons with baselines in inference are very important and should be done, but we cannot see it in the experiment tables. The authors claim the overhead of dot product on embeddings is ""1/300 in contrast to HieRec"" in L500. I think this is not rigorous because the authors neither give detailed inference time comparisons nor give more comprehensive explanations, given the fact that dot-product on embeddings is just a very lightweight computation compared to previous FCN and Transformer computation.

3. Experiments three times on MIND-small and once on MIND-large is not statistically supportive. This is because MIND-small is literally small and always leads to high experiment variance. This may be the reason why the curve in Figure 5 fluctuates. I think more experiment trials like previous papers [3, 4] can better support the paper.

4. Topic-aware attention was widely studied in previous papers [3, 4, 5], which is somehow incremental.


[1] Gradient Surgery for Multi-Task Learning. Neurips 2020.

[2] Conflict-Averse Gradient Descent for Multi-task Learning. Neurips 2021.

[3] HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation. ACL 2021.

[4] MINER: Multi-Interest Matching Network for News Recommendation. ACL 2022.

[5] Neural news recommendation with topic-aware news representation. ACL 2019.",Could you provide inference time compared to the baselines to justify the efficiency of DI?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"L86: ""To response the"" -> ""To response to the""

L380 Equation (5): ""s.t."" abbreviates ""subject to"" meaning the constraint term in an optimization problem, here ""where"" is more appropriate.

L506: ""encode title"" -> ""encode titles""

L527: ""further decline"" -> ""further declines""

A suggestion: the word ""aux"" should be ""auxiliary"" in formal paper writing.",1691057593151,,,EMNLP/2023/Conference,0hyn6MJmnP,"['EMNLP/2023/Conference/Submission3041/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206144,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']",0hyn6MJmnP,['EMNLP/2023/Conference/Submission3041/Reviewer_Gomy'],1691057593151,1701461206144,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3041/Reviewer_Gomy']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""Gradient Surgery for Multi-Task Learning"" (Neurips 2020)
2. ""Conflict-Averse Gradient Descent for Multi-task Learning"" (Neurips 2021)
3. ""HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation"" (ACL 2021)
4. ""MINER: Multi-Interest Matching Network for News Recommendation"" (ACL 2022)
5. ""Neural news recommendation with topic-aware news representation"" (ACL 2019)

These citations might be necessary because the reviewer mentions that the authors overlook a crucial problem of training and inference ranking objective discrepancy, which is discussed in [1] and [2]. The reviewer also suggests comparing the efficiency of the proposed model with baselines, referencing [3]. Additionally, the reviewer notes that topic-aware attention was widely studied in previous papers [3, 4, 5], implying that the authors should be aware of and cite these relevant works to contextualize their contributions.",1,"2020, 2021, 2021, 2022, 2019",Neural news recommendation with topic-aware news representation
mImbqnWmAa,"This work proposes a news recommender system by designing topic-award attention and dual-encoder interaction modules. The proposed model was evaluated on the MIND datasets, showing the highest performance compared to baseline models.","1. The proposed deep learning architecture achieves the highest performance compared to baseline models.
2. The paper is generally well written, and thus readers will be easy to fully understand the idea in the paper.","1. The paper does not include an important related work [1] while the performance of [1] on MIND-large dataset is higher than the numbers reported in this paper. Therefore, the paper should compare the proposed approach with the recent related work [1].

2. ~~Analysis on topic-aware attention was missing. The goal of topic-aware attention is to avoid irrelevant word distraction. However, in the experiment, the paper does not show whether the goal is indeed achieved or not.~~  

[1] Yang Yu, Fangzhao Wu, Chuhan Wu, Jingwei Yi, and Qi Liu. 2022. Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation. EMNLP'22.",Why do you use the term 'recall' in news recommendation? I'm confused by the term as the task conducted in the experiment is simply the news recommendation (ranking). Is there any reference mentioning the term in news recommendation field?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Yang Yu, Fangzhao Wu, Chuhan Wu, Jingwei Yi, and Qi Liu. 2022. Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5478–5489, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",Change 'new recall' to 'news recall',1691156713821,,,EMNLP/2023/Conference,0hyn6MJmnP,"['EMNLP/2023/Conference/Submission3041/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206052,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3041/Reviewer_o5wg']",0hyn6MJmnP,['EMNLP/2023/Conference/Submission3041/Reviewer_o5wg'],1691156713821,1701461206052,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3041/Reviewer_o5wg']","Yes

List of suggested papers or references not cited in the manuscript:
1. Yang Yu, Fangzhao Wu, Chuhan Wu, Jingwei Yi, and Qi Liu. 2022. Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation. EMNLP'22.

These citations might be necessary because the reviewer mentions that the paper does not include an important related work [1], specifically Tiny-NewsRec, which has higher performance on the MIND-large dataset compared to the numbers reported in the paper. The reviewer suggests that the authors should compare their proposed approach with this recent related work to provide a more comprehensive evaluation.",1,"2022, 2022",Tiny-NewsRec Effective and Efficient PLM-based News Recommendation
zXeAOjYkER,"This paper presents a dataset for evaluating knowledge editing in LMs, focusing on evaluating multi-hop questions whose answers should change as the result of and edit to the answer to one of its sub questions. The authors construct two versions of this dataset from WikiData, where multi-hop questions are constructed from knowledge-base tuples using ChatGPT. Their first dataset is larger, consisting of chains of 37 different relation types up to a length of 4 connecting the 20% of most popular entities in WikiData. Relations are further filtered based on whether GPT-J is able to recall each individual fact. To construct edits, the authors replace the object of a subject-relation-object tuple with another object sharing the same relation. The authors also construct a smaller dataset containing real factual edits that have occurred over time (2021 to 2023), focusing on changes in 6 different relation types.

The authors experiment with editing in a single or multiple (up to 3K) facts into a single LM and experiment with several baselines. They find that existing methods, while they're able to recall individual edited facts, they are unable to successfully use these updated facts to answer multi-hop questions using them, failing catastrophically. When allowed to perform chain-of-thought reasoning for multi-hop questions, however, all methods are able improve and recover much of their base performance. This observation holds true for both versions of their dataset.

The authors finally propose their own method for knowledge editing, that is based around (1) decomposing multi-hop questions into their subquestions and (2) performing retrieval for each subquestion from a non-parametric corpus of edited facts. The LM then makes a decision whether the answer should be affected by this retrieved fact, then updates its answer based on that decision. The authors demonstrate that this pipeline outperforms ","The paper is well-motivated, highlighting a overlooked issue knowledge editing work.

The dataset and its construction process may support future work in this area.

The paper is well-written and includes a variety of knowledge-editing baselines and base LMs in experiments.","While this dataset provides a method for evaluating whether facts are successfully edited in multi-hop questions, it fails to also evaluate the specificity/locality of edited facts and whether edited facts are having unintended effects (e.g., catastrophic forgetting) elsewhere. Evaluating the specificity of edits is a consistent concern in other works proposing datasets for knowledge editing. For instance, this dataset does not consider questions such as whether editing (WALL-E, creator, Andrew Stanton → James Watt) affects other facts, for instance (Finding Nemo, creator, Andrew Stanton → James Watt).

The baselines and comparisons for the proposed method in Section 5 make it unclear where improvements in the proposed method are coming from. In particular, some of the cited work has demonstrated that, for multi-hop QA, CoT prompting  by decomposing inputs into QA pairs outperforms standard paragraph-style CoT prompting. Baselines where edited models receive the same prompt, with the retrieval and counterfactual reasoning steps removed, would've been well-placed here.

Adding onto the point above, some of the cited work (SERAC) has proposed a method that similarly (1) performs retrieval over edited facts, (2) determines whether the edited facts should affect the answer to the given question and (3) use the edited fact to produce the correct output. The primary difference here is the usage of a single LM to do all these inferences jointly rather than training separate components. Comparisons between these two methods of performing steps 1-3 would also add context for the proposed method.

",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691689102442,,,EMNLP/2023/Conference,0hTPJBnncc,"['EMNLP/2023/Conference/Submission2205/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461150863,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2205/Reviewer_ojrC']",0hTPJBnncc,['EMNLP/2023/Conference/Submission2205/Reviewer_ojrC'],1691689102442,1701461150863,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2205/Reviewer_ojrC']","Yes

List of suggested papers or references not cited in the manuscript:
1. SERAC (a method that performs retrieval over edited facts, determines whether the edited facts should affect the answer, and uses the edited fact to produce the correct output)
2. Cited work on CoT prompting for multi-hop QA (demonstrating that decomposing inputs into QA pairs outperforms standard paragraph-style CoT prompting)

These citations might be necessary because the reviewer points out that the proposed method in the manuscript bears similarities to existing work, such as SERAC, and that comparisons between these methods would add context to the proposed method. Additionally, the reviewer mentions that some cited work has demonstrated the effectiveness of CoT prompting for multi-hop QA, suggesting that the authors should consider these findings when evaluating their own method. By citing these papers, the authors can provide a more comprehensive discussion of related work and strengthen their contributions.",1,,SERAC 
zXeAOjYkER,"This paper presents a dataset for evaluating knowledge editing in LMs, focusing on evaluating multi-hop questions whose answers should change as the result of and edit to the answer to one of its sub questions. The authors construct two versions of this dataset from WikiData, where multi-hop questions are constructed from knowledge-base tuples using ChatGPT. Their first dataset is larger, consisting of chains of 37 different relation types up to a length of 4 connecting the 20% of most popular entities in WikiData. Relations are further filtered based on whether GPT-J is able to recall each individual fact. To construct edits, the authors replace the object of a subject-relation-object tuple with another object sharing the same relation. The authors also construct a smaller dataset containing real factual edits that have occurred over time (2021 to 2023), focusing on changes in 6 different relation types.

The authors experiment with editing in a single or multiple (up to 3K) facts into a single LM and experiment with several baselines. They find that existing methods, while they're able to recall individual edited facts, they are unable to successfully use these updated facts to answer multi-hop questions using them, failing catastrophically. When allowed to perform chain-of-thought reasoning for multi-hop questions, however, all methods are able improve and recover much of their base performance. This observation holds true for both versions of their dataset.

The authors finally propose their own method for knowledge editing, that is based around (1) decomposing multi-hop questions into their subquestions and (2) performing retrieval for each subquestion from a non-parametric corpus of edited facts. The LM then makes a decision whether the answer should be affected by this retrieved fact, then updates its answer based on that decision. The authors demonstrate that this pipeline outperforms ","The paper is well-motivated, highlighting a overlooked issue knowledge editing work.

The dataset and its construction process may support future work in this area.

The paper is well-written and includes a variety of knowledge-editing baselines and base LMs in experiments.","While this dataset provides a method for evaluating whether facts are successfully edited in multi-hop questions, it fails to also evaluate the specificity/locality of edited facts and whether edited facts are having unintended effects (e.g., catastrophic forgetting) elsewhere. Evaluating the specificity of edits is a consistent concern in other works proposing datasets for knowledge editing. For instance, this dataset does not consider questions such as whether editing (WALL-E, creator, Andrew Stanton → James Watt) affects other facts, for instance (Finding Nemo, creator, Andrew Stanton → James Watt).

The baselines and comparisons for the proposed method in Section 5 make it unclear where improvements in the proposed method are coming from. In particular, some of the cited work has demonstrated that, for multi-hop QA, CoT prompting  by decomposing inputs into QA pairs outperforms standard paragraph-style CoT prompting. Baselines where edited models receive the same prompt, with the retrieval and counterfactual reasoning steps removed, would've been well-placed here.

Adding onto the point above, some of the cited work (SERAC) has proposed a method that similarly (1) performs retrieval over edited facts, (2) determines whether the edited facts should affect the answer to the given question and (3) use the edited fact to produce the correct output. The primary difference here is the usage of a single LM to do all these inferences jointly rather than training separate components. Comparisons between these two methods of performing steps 1-3 would also add context for the proposed method.

",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691689102442,,,EMNLP/2023/Conference,0hTPJBnncc,"['EMNLP/2023/Conference/Submission2205/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461150863,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2205/Reviewer_ojrC']",0hTPJBnncc,['EMNLP/2023/Conference/Submission2205/Reviewer_ojrC'],1691689102442,1701461150863,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2205/Reviewer_ojrC']","Yes

List of suggested papers or references not cited in the manuscript:
1. SERAC (a method that performs retrieval over edited facts, determines whether the edited facts should affect the answer, and uses the edited fact to produce the correct output)
2. Cited work on CoT prompting for multi-hop QA (demonstrating that decomposing inputs into QA pairs outperforms standard paragraph-style CoT prompting)

These citations might be necessary because the reviewer points out that the proposed method in the manuscript bears similarities to existing work, such as SERAC, and that comparisons between these methods would add context to the proposed method. Additionally, the reviewer mentions that some cited work has demonstrated the effectiveness of CoT prompting for multi-hop QA, suggesting that the authors should consider these findings when evaluating their own method. By citing these papers, the authors can provide a more comprehensive discussion of related work and strengthen their contributions.",1,,Cited work on CoT prompting for multi-hop QA
48HbQg99jr,"The paper studies the impact of argument pattern on persuasiveness in online discussion. The authors claim that existing research mostly focuses on individual comments and ignore the interactive relationship between arguments. They propose to use ADU (argumentative discourse unit) patterns to capture the structure of discussions. They fine-tune a pre-trained LLM to classify ADU type and propose two approaches, Edits and embeddings based, to cluster ADUs. The authors analyze the data they collected to understand the ADU type distribution and cluster statistics. To verify the effectiveness of using argument patterns for predicting persuasiveness, the authors incorporate the argument structure features into an LSTM model and compare the model with length based and BERT based baselines.

","- The paper studies an interesting problem. 
- The proposed method of capturing argument structure makes sense","The interactive relationship between arguments have been studied before. The paper does not provide comparison with existing approaches. 

The paper leverages LLM for ADU identification. However, the quality verification is rather ad-hoc. The authors verify their model using a different CMV data and claim the two datasets are similar. (1) CMV contains topics from a large variety of domains. (2) If the two datasets are similar, what is the benefit of creating a new dataset?

","What is the benefit of creating a new CMV dataset?

","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Ji et.al., 2018. Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model. COLING.
Guo et.al., 2020. In Opinion Holders’ Shoes: Modeling Cumulative Influence for View Change in Online Argumentation. 2388-2399. WWW.

",,1691015704738,,,EMNLP/2023/Conference,0eWQVWvPgu,"['EMNLP/2023/Conference/Submission5366/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461345095,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission5366/Reviewer_HDqE']",0eWQVWvPgu,['EMNLP/2023/Conference/Submission5366/Reviewer_HDqE'],1691015704738,1701461345095,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5366/Reviewer_HDqE']","Yes

The suggested papers or references not cited in the manuscript are:
1. Ji et.al., 2018. Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model. COLING.
2. Guo et.al., 2020. In Opinion Holders’ Shoes: Modeling Cumulative Influence for View Change in Online Argumentation. 2388-2399. WWW.

These citations might be necessary because the reviewer mentions that the interactive relationship between arguments has been studied before, implying that the authors' claim of novelty may be overstated. The reviewer explicitly lists these two papers as ""Missing_References"", suggesting that the authors should be aware of and engage with this existing research to strengthen their contribution and situate their work within the broader literature on argumentation and persuasion.",1,"2018, 2020",Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model 
48HbQg99jr,"The paper studies the impact of argument pattern on persuasiveness in online discussion. The authors claim that existing research mostly focuses on individual comments and ignore the interactive relationship between arguments. They propose to use ADU (argumentative discourse unit) patterns to capture the structure of discussions. They fine-tune a pre-trained LLM to classify ADU type and propose two approaches, Edits and embeddings based, to cluster ADUs. The authors analyze the data they collected to understand the ADU type distribution and cluster statistics. To verify the effectiveness of using argument patterns for predicting persuasiveness, the authors incorporate the argument structure features into an LSTM model and compare the model with length based and BERT based baselines.

","- The paper studies an interesting problem. 
- The proposed method of capturing argument structure makes sense","The interactive relationship between arguments have been studied before. The paper does not provide comparison with existing approaches. 

The paper leverages LLM for ADU identification. However, the quality verification is rather ad-hoc. The authors verify their model using a different CMV data and claim the two datasets are similar. (1) CMV contains topics from a large variety of domains. (2) If the two datasets are similar, what is the benefit of creating a new dataset?

","What is the benefit of creating a new CMV dataset?

","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Ji et.al., 2018. Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model. COLING.
Guo et.al., 2020. In Opinion Holders’ Shoes: Modeling Cumulative Influence for View Change in Online Argumentation. 2388-2399. WWW.

",,1691015704738,,,EMNLP/2023/Conference,0eWQVWvPgu,"['EMNLP/2023/Conference/Submission5366/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461345095,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission5366/Reviewer_HDqE']",0eWQVWvPgu,['EMNLP/2023/Conference/Submission5366/Reviewer_HDqE'],1691015704738,1701461345095,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5366/Reviewer_HDqE']","Yes

The suggested papers or references not cited in the manuscript are:
1. Ji et.al., 2018. Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model. COLING.
2. Guo et.al., 2020. In Opinion Holders’ Shoes: Modeling Cumulative Influence for View Change in Online Argumentation. 2388-2399. WWW.

These citations might be necessary because the reviewer mentions that the interactive relationship between arguments has been studied before, implying that the authors' claim of novelty may be overstated. The reviewer explicitly lists these two papers as ""Missing_References"", suggesting that the authors should be aware of and engage with this existing research to strengthen their contribution and situate their work within the broader literature on argumentation and persuasion.",1,"2018, 2020",In Opinion Holders’ Shoes: Modeling Cumulative Influence for View Change in Online Argumentation
VVw10MdDRM,"Topic: The paper studies the topic of human evaluation of dialogue systems.

Research problem: Given the poor reliability of automatic metrics, human evaluation remains the gold standard in assessing dialogue systems. Yet, existing human evaluation setups, such as multi-dimensional scoring based on instruction manuals, pairwise comparisons of dialogue systems, are time-consuming. 

Contributions: The authors propose a Free-For-All ranking approach to interactively evaluate the dialogue systems. Specifically, a human user will simultaneously chat with multiple dialogue models and select the best response among all the candidates generated by the models. The conversation continues with the selected response.","1. The clarity of the paper is good.

2. The proposed method is technical sound and the analyses are comprehensive","The design of FFAEVAL is not new. Refer to [1] in the missing reference, which contains two similar interactive evaluation setups as FFAEVAL: (1) multi-model Select-One-Best-from-All and (2) multi-model Select-All That-Apply.","1. How does FFAEVAL deal with the case when none of the generated responses are appropriate [1]? 

2. Line 302, does comparison with reference-based metrics make sense? My understanding is that FFAEVAL is designed for the interactive evaluation setting. As the conversation evolves, the trajectory will diverge from the original human-human conversations.

3. In Figure 1 and Table 1, considering that you are ranking just five dialogue models, how reliable or statistically significant is your findings? Additionally, if the number of models increases, will the reliability of the human user decrease given that they have to choose among a large pool of response candidates?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] Liu, Sijia, et al. ""Towards credible human evaluation of open-domain dialog systems using interactive setup."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.

[2] Smith, Eric, et al. ""Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents."" Proceedings of the 4th Workshop on NLP for Conversational AI. 2022.",Line 428 - “analysis” -> “analyze”,1691159622925,,,EMNLP/2023/Conference,0bderX6zwr,"['EMNLP/2023/Conference/Submission5673/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461358695,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5673/Reviewer_nRyd']",0bderX6zwr,['EMNLP/2023/Conference/Submission5673/Reviewer_nRyd'],1691159622925,1701461358695,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5673/Reviewer_nRyd']","Yes

The suggested papers or references not cited in the manuscript are:
1. Liu, Sijia, et al. ""Towards credible human evaluation of open-domain dialog systems using interactive setup."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.
2. Smith, Eric, et al. ""Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents."" Proceedings of the 4th Workshop on NLP for Conversational AI. 2022.

These citations might be necessary because the reviewer mentions that the design of FFAEVAL is not new and refers to similar interactive evaluation setups in the paper by Liu et al. [1]. The reviewer also lists Smith et al. [2] as a missing reference, implying that the authors should be aware of and potentially cite this work as well, as it relates to the open problem of human evaluation of conversations. This suggests that the reviewer believes the authors should be aware of and engage with existing research in the field to strengthen their contribution and provide a more comprehensive overview of the current state of human evaluation of dialogue systems.",1,"2023, 2022",Towards credible human evaluation of open-domain dialog systems using interactive setup
VVw10MdDRM,"Topic: The paper studies the topic of human evaluation of dialogue systems.

Research problem: Given the poor reliability of automatic metrics, human evaluation remains the gold standard in assessing dialogue systems. Yet, existing human evaluation setups, such as multi-dimensional scoring based on instruction manuals, pairwise comparisons of dialogue systems, are time-consuming. 

Contributions: The authors propose a Free-For-All ranking approach to interactively evaluate the dialogue systems. Specifically, a human user will simultaneously chat with multiple dialogue models and select the best response among all the candidates generated by the models. The conversation continues with the selected response.","1. The clarity of the paper is good.

2. The proposed method is technical sound and the analyses are comprehensive","The design of FFAEVAL is not new. Refer to [1] in the missing reference, which contains two similar interactive evaluation setups as FFAEVAL: (1) multi-model Select-One-Best-from-All and (2) multi-model Select-All That-Apply.","1. How does FFAEVAL deal with the case when none of the generated responses are appropriate [1]? 

2. Line 302, does comparison with reference-based metrics make sense? My understanding is that FFAEVAL is designed for the interactive evaluation setting. As the conversation evolves, the trajectory will diverge from the original human-human conversations.

3. In Figure 1 and Table 1, considering that you are ranking just five dialogue models, how reliable or statistically significant is your findings? Additionally, if the number of models increases, will the reliability of the human user decrease given that they have to choose among a large pool of response candidates?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,"2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] Liu, Sijia, et al. ""Towards credible human evaluation of open-domain dialog systems using interactive setup."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.

[2] Smith, Eric, et al. ""Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents."" Proceedings of the 4th Workshop on NLP for Conversational AI. 2022.",Line 428 - “analysis” -> “analyze”,1691159622925,,,EMNLP/2023/Conference,0bderX6zwr,"['EMNLP/2023/Conference/Submission5673/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461358695,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5673/Reviewer_nRyd']",0bderX6zwr,['EMNLP/2023/Conference/Submission5673/Reviewer_nRyd'],1691159622925,1701461358695,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5673/Reviewer_nRyd']","Yes

The suggested papers or references not cited in the manuscript are:
1. Liu, Sijia, et al. ""Towards credible human evaluation of open-domain dialog systems using interactive setup."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.
2. Smith, Eric, et al. ""Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents."" Proceedings of the 4th Workshop on NLP for Conversational AI. 2022.

These citations might be necessary because the reviewer mentions that the design of FFAEVAL is not new and refers to similar interactive evaluation setups in the paper by Liu et al. [1]. The reviewer also lists Smith et al. [2] as a missing reference, implying that the authors should be aware of and potentially cite this work as well, as it relates to the open problem of human evaluation of conversations. This suggests that the reviewer believes the authors should be aware of and engage with existing research in the field to strengthen their contribution and provide a more comprehensive overview of the current state of human evaluation of dialogue systems.",1,"2023, 2022",Human Evaluation of Conversations is an Open Problem comparing the sensitivity of various methods for evaluating dialogue agents
FdF1nDPq1y,"This short paper presents NormDial, a new dataset of English and Chinese dialogues annotated with relevant social norms along with labels on whether the norm is being followed or violated. Authors use a machine generation with human verification pipeline to create the dataset, which contains 4k dialogues and 30k conversational utterances: authors first generate a set of social norms with ChatGPT, then flesh out the social norms into dyadic social situations, from which a dialogue is then generated. The English and Chinese conversations are then verified for naturalness, interestingness, and coherence, as well as whether the speakers follow the norms of the respective cultures. Finally, authors perform a small investigation of whether ChatGPT can accurately predict whether a conversation follows/violates a social norm, finding that it struggles to detect norm-violating utterances.","- I appreciate the fact that the dataset is in a language other than English, and that a non-Western culture is being included.
- The machine-and-human creation of the dataset is reasonable and effective, and I particularly appreciated the careful manual verification of the dialogues.
- The experiments on ChatGPT (§4) are well-executed and yield some interesting insights.","This work is sound and reasonable. However, the paper missed some important related work that was not covered, making some claims of novelty not well supported. Specifically, Kim et al. (2022a) introduced the ProSocial Dialogues corpus which contains English conversations with social norms annotated at the utterance level. Additionally, the machine-generated-human-verified pipeline to generate social situations which are fleshed out into dialogues was introduced by Kim et al (2022b) and Chen et al. (2023). I suggest the authors edit their introduction to better scope their claims of novelty, and include these works into the related work section. That being said, the paper's main novelty lies in the cross-cultural nature of the dataset, which is a huge asset for the NLP community!",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"- Chen, Maximillian, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2023. “PLACES: Prompting Language Models for Social Conversation Synthesis.” In *Findings of EACL 2023*. http://arxiv.org/abs/2302.03269.
- Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap & Yejin Choi (2022) **SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization**. *arXiv*. https://arxiv.org/abs/2212.10465
- Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi & Maarten Sap (2022) **ProsocialDialog: A Prosocial Backbone for Conversational Agents**. *EMNLP*. https://arxiv.org/abs/2205.12688",,1691371913213,,,EMNLP/2023/Conference,0VQImEvjPJ,"['EMNLP/2023/Conference/Submission3912/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461260288,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3912/Reviewer_nowG']",0VQImEvjPJ,['EMNLP/2023/Conference/Submission3912/Reviewer_nowG'],1691371913213,1701461260288,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3912/Reviewer_nowG']","Yes

The suggested papers or references not cited in the manuscript are:
1. Kim et al. (2022a) - ""ProsocialDialog: A Prosocial Backbone for Conversational Agents""
2. Kim et al. (2022b) - ""SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization""
3. Chen et al. (2023) - ""PLACES: Prompting Language Models for Social Conversation Synthesis""

These citations might be necessary because the reviewer states that the paper missed some important related work, specifically the introduction of the ProSocial Dialogues corpus and the machine-generated-human-verified pipeline to generate social situations. The reviewer suggests that the authors edit their introduction to better scope their claims of novelty and include these works in the related work section to provide a more accurate representation of the existing research in the field.",1,2023,ProsocialDialog A Prosocial Backbone for Conversational Agents
FdF1nDPq1y,"This short paper presents NormDial, a new dataset of English and Chinese dialogues annotated with relevant social norms along with labels on whether the norm is being followed or violated. Authors use a machine generation with human verification pipeline to create the dataset, which contains 4k dialogues and 30k conversational utterances: authors first generate a set of social norms with ChatGPT, then flesh out the social norms into dyadic social situations, from which a dialogue is then generated. The English and Chinese conversations are then verified for naturalness, interestingness, and coherence, as well as whether the speakers follow the norms of the respective cultures. Finally, authors perform a small investigation of whether ChatGPT can accurately predict whether a conversation follows/violates a social norm, finding that it struggles to detect norm-violating utterances.","- I appreciate the fact that the dataset is in a language other than English, and that a non-Western culture is being included.
- The machine-and-human creation of the dataset is reasonable and effective, and I particularly appreciated the careful manual verification of the dialogues.
- The experiments on ChatGPT (§4) are well-executed and yield some interesting insights.","This work is sound and reasonable. However, the paper missed some important related work that was not covered, making some claims of novelty not well supported. Specifically, Kim et al. (2022a) introduced the ProSocial Dialogues corpus which contains English conversations with social norms annotated at the utterance level. Additionally, the machine-generated-human-verified pipeline to generate social situations which are fleshed out into dialogues was introduced by Kim et al (2022b) and Chen et al. (2023). I suggest the authors edit their introduction to better scope their claims of novelty, and include these works into the related work section. That being said, the paper's main novelty lies in the cross-cultural nature of the dataset, which is a huge asset for the NLP community!",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"- Chen, Maximillian, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2023. “PLACES: Prompting Language Models for Social Conversation Synthesis.” In *Findings of EACL 2023*. http://arxiv.org/abs/2302.03269.
- Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap & Yejin Choi (2022) **SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization**. *arXiv*. https://arxiv.org/abs/2212.10465
- Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi & Maarten Sap (2022) **ProsocialDialog: A Prosocial Backbone for Conversational Agents**. *EMNLP*. https://arxiv.org/abs/2205.12688",,1691371913213,,,EMNLP/2023/Conference,0VQImEvjPJ,"['EMNLP/2023/Conference/Submission3912/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461260288,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3912/Reviewer_nowG']",0VQImEvjPJ,['EMNLP/2023/Conference/Submission3912/Reviewer_nowG'],1691371913213,1701461260288,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3912/Reviewer_nowG']","Yes

The suggested papers or references not cited in the manuscript are:
1. Kim et al. (2022a) - ""ProsocialDialog: A Prosocial Backbone for Conversational Agents""
2. Kim et al. (2022b) - ""SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization""
3. Chen et al. (2023) - ""PLACES: Prompting Language Models for Social Conversation Synthesis""

These citations might be necessary because the reviewer states that the paper missed some important related work, specifically the introduction of the ProSocial Dialogues corpus and the machine-generated-human-verified pipeline to generate social situations. The reviewer suggests that the authors edit their introduction to better scope their claims of novelty and include these works in the related work section to provide a more accurate representation of the existing research in the field.",1,2023,SODA Million-scale Dialogue Distillation with Social Commonsense Contextualization
FdF1nDPq1y,"This short paper presents NormDial, a new dataset of English and Chinese dialogues annotated with relevant social norms along with labels on whether the norm is being followed or violated. Authors use a machine generation with human verification pipeline to create the dataset, which contains 4k dialogues and 30k conversational utterances: authors first generate a set of social norms with ChatGPT, then flesh out the social norms into dyadic social situations, from which a dialogue is then generated. The English and Chinese conversations are then verified for naturalness, interestingness, and coherence, as well as whether the speakers follow the norms of the respective cultures. Finally, authors perform a small investigation of whether ChatGPT can accurately predict whether a conversation follows/violates a social norm, finding that it struggles to detect norm-violating utterances.","- I appreciate the fact that the dataset is in a language other than English, and that a non-Western culture is being included.
- The machine-and-human creation of the dataset is reasonable and effective, and I particularly appreciated the careful manual verification of the dialogues.
- The experiments on ChatGPT (§4) are well-executed and yield some interesting insights.","This work is sound and reasonable. However, the paper missed some important related work that was not covered, making some claims of novelty not well supported. Specifically, Kim et al. (2022a) introduced the ProSocial Dialogues corpus which contains English conversations with social norms annotated at the utterance level. Additionally, the machine-generated-human-verified pipeline to generate social situations which are fleshed out into dialogues was introduced by Kim et al (2022b) and Chen et al. (2023). I suggest the authors edit their introduction to better scope their claims of novelty, and include these works into the related work section. That being said, the paper's main novelty lies in the cross-cultural nature of the dataset, which is a huge asset for the NLP community!",,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"- Chen, Maximillian, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2023. “PLACES: Prompting Language Models for Social Conversation Synthesis.” In *Findings of EACL 2023*. http://arxiv.org/abs/2302.03269.
- Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap & Yejin Choi (2022) **SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization**. *arXiv*. https://arxiv.org/abs/2212.10465
- Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi & Maarten Sap (2022) **ProsocialDialog: A Prosocial Backbone for Conversational Agents**. *EMNLP*. https://arxiv.org/abs/2205.12688",,1691371913213,,,EMNLP/2023/Conference,0VQImEvjPJ,"['EMNLP/2023/Conference/Submission3912/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461260288,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3912/Reviewer_nowG']",0VQImEvjPJ,['EMNLP/2023/Conference/Submission3912/Reviewer_nowG'],1691371913213,1701461260288,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3912/Reviewer_nowG']","Yes

The suggested papers or references not cited in the manuscript are:
1. Kim et al. (2022a) - ""ProsocialDialog: A Prosocial Backbone for Conversational Agents""
2. Kim et al. (2022b) - ""SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization""
3. Chen et al. (2023) - ""PLACES: Prompting Language Models for Social Conversation Synthesis""

These citations might be necessary because the reviewer states that the paper missed some important related work, specifically the introduction of the ProSocial Dialogues corpus and the machine-generated-human-verified pipeline to generate social situations. The reviewer suggests that the authors edit their introduction to better scope their claims of novelty and include these works in the related work section to provide a more accurate representation of the existing research in the field.",1,2023,PLACES Prompting Language Models for Social Conversation Synthesis
XM0nrfLQ9R,"The research article focuses on enhancing the existing Net Zero Tracker's capability to algorithmically assess the sustainability targets (reduction and net zero goals) of corporations and nations. To accomplish this, the authors present a labeled dataset, fine-tune a language model known as ClimateBERT-NetZero, and aim to make this trained model publicly available.

The research further introduces two unique use cases for the fine-tuned model (ClimateBERT-NetZero). First, it employs the extracted climate claims to ask supplementary questions and garner information such as the target year, base year, and percentage reduction. Second, it utilizes the trained model to extract climate goals from earnings call transcripts, illustrating that the percentage of companies communicating climate goals saw a significant rise in 2020 and 2021.","This paper presents a refined dataset for a critical task and provides a comparative analysis of different models, including zero-shot ChatGPT. It also outlines two practical use cases, contributing to its acceptability.","Despite careful examination of the main paper and its appendix, I found that vital information about the data and its annotation is notably absent. This omission complicates the assessment of the paper's quality, hence my low score for soundness. 

The paper's primary contribution lies in introducing/refining the dataset. Given the significance of this contribution, comprehensive details on the annotation process are necessary.

The paper needs a lot of writing work to address many questions I have listed in the section below.","Q1: What is the distribution of data between companies, countries, regions, and cities categories? Table 1 doesn’t provide detail on this. 
Q2: Appendix lines 451-455: How does performed preprocessing change the descriptive statistics of the data? Does it induce any bias? 
Q3: Appendix lines 456-463: If the data was already labeled by Net Zero Tracker then how can paper stat contribution as introducing new data? It is more appropriate to say refine the data. 
Q4: Appendix lines 463-464: What do you mean by “at least one author”? What was the average number of annotators used for each data point? How it was decided how many annotators to use for a particular data point. 
Q5: The paper claims that data was annotated by experts. What was the qualification of the annotators?
Q6: What was an inter-annotator agreement? 
Q7: Appendix lines 482-491: The entire process of human-in-the-loop is not clear. Which model was used to identify misclassification? Filtering sentences for readjusting labels based on the model’s output might induce some kind of bias. If the model is wrong on certain data points which can be also wrongly labeled in the first place as authors are refining them, it contaminates the labeled dataset. 
Q8: What is the accuracy of the model in detecting Net Zero and Reduction Targets in Earnings Call Transcripts? 
Q9: In Figure 1, does y-axis values are multiplied by 100 to show the % value or it is just a fraction?
Q10: In the second use case, does the sample of companies each year stays the same or it changes over time? If it changes over time, then one can’t say if the trend is changing because of the sample of earnings call transcript or some companies actually moving from not committing to climate goals to committing to them and vice versa. Also, it doesn’t list from which country these companies are located. The sharp increase since 2019 can be driven by just a change of policy in a few countries. 
Q11: As earnings call is a quarterly reporting not yearly, how and why data was aggregated to a yearly frequency in Figure 1? 
Q12: Why the guidelines listed in the appendix (lines 469-480) is not included in ChatGPT zero-shot prompt and rather a different prompt is used? 
Q13: How does model performance ranking change if hyperparameters different from the one listed in Table C.2 are used?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"1. RoBERTa model is used but not cited: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

    2. Anna Rogers article: Anna Rogers, Niranjan Balasubramanian, Leon Derczynski, Jesse Dodge, Alexander Koller, Sasha Luccioni, Maarten Sap, Roy Schwartz, Noah A. Smith, and Emma Strubell. 2023. Closed ai models make bad baselines.","In general, the paper can be greatly improved by providing more detail on everything. Below are some specific comments: 

C1: The abstract says corporates and nations while the “Data” section says companies, countries, regions, and cities. 

C2: It is more conventional to write 1,500 instead of 1.500 (line #165).

C3: It will be better to include the ChatGPT prompt in the main paper instead of the Appendix. Even in the 4-page limit Limitations can be moved to the 5th page and the prompt could be included. 

C5: More details can be added to the caption of Table 3 to improve the readability of the work. 

C6: Data description for two use cases is missing in the main paper. 

C7: Table H.4 can use better legends.",1690328465553,,,EMNLP/2023/Conference,0ULLuIRdcu,"['EMNLP/2023/Conference/Submission1027/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461079249,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1027/Reviewer_vRSM']",0ULLuIRdcu,['EMNLP/2023/Conference/Submission1027/Reviewer_vRSM'],1690328465553,1701461079249,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1027/Reviewer_vRSM']","Yes

The suggested papers or references not cited in the manuscript are:
1. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
2. Anna Rogers, Niranjan Balasubramanian, Leon Derczynski, Jesse Dodge, Alexander Koller, Sasha Luccioni, Maarten Sap, Roy Schwartz, Noah A. Smith, and Emma Strubell. 2023. Closed ai models make bad baselines.

These citations might be necessary because the reviewer points out that the RoBERTa model is used in the paper but not cited, and the article by Anna Rogers is relevant to the discussion of baselines in the paper. The reviewer suggests that including these citations would improve the completeness and accuracy of the manuscript.",1,"2019, 2019, 2023",Roberta A robustly optimized bert pretraining approach 
9mTFzVNrQM,"The major contribution of this paper is the proposed dataset.

Net-zero (""net zero"" GHG emissions)  claims by firms and countries have been on the rise recently, but are sometimes pointed to as greenwashing.
This paper proposes a task and dataset for detecting net-zero claims from text.
The authors provided a 3.5K text annotated corpus including three labels: NetZero, Reduction, and NoTarget.
The fine-tuned ClimateBERT outperformed baselines.
The authors also experimented the reduction target year extraction by using QA approach.
Finally, a case study on the earning call transcripts analysis suggests that the net-zero target discussion has experienced a sharp increase since 2019.","- This paper proposes a novel task and dataset on a net-zero perspective of climate change. The dataset is valuable to researchers because this field is nascent and lacks datasets and task definitions. The authors will release the dataset and models, which will be useful for researchers in this domain.
- They appear to be collaborating with domain experts to create the dataset","The major drawback is that the paper lacks detail in the discussion and dataset.
It is especially important to discuss motives because this paper takes on a new challenge.
See below comments.

### High-level comment

A1. Lack of discussion for the motivation

The authors begin their motivation with a reference to greenwashing. Indeed, claims of net zero have sometimes been accused of greenwashing, so I agree with the importance of this motivation.
While the study seems to focus on extracting net-zero targets, there is no insight of what can be done to suppress greenwashing as a result.

There are various typologies of greenwashing, e.g., net zero may be dependent on carbon offsetting, there may be no clear intermediate target, it may not follow IPCC guidance, etc.

It is not clear what typologies of greenwashing the authors have in mind or whether they have extracted the necessary information to do so. It might be good to see these discussions in a related work, introduction, or discussion section.


A2. Lack of discussion for the task design

Similar to above, I would like to know the motivation behind the task definition.
The net-zero claim is a complex matter in practice. For example,
- Is the scope of the net-zero claim related to the company or for a specific project? Is it for Scope 1, 2, or 3 emissions?
- The difference between intermediary and long-term goals.
- Differences in units and calculation methods

The Appendix presents some of the guidelines, but they are oversimplified and lack an explanation of why the task was defined in the way.


### Low-level comment

B1. Detailed description of the dataset is missing.

- The label distribution for both train and test data should be described in Table 1.
- Please specify the nature of ""expert annotator""
- The design of the NoTarget label is questionable. These samples may be different from the domains of the NetZero and Reduction labels. Thus, they may overestimate the classification performance of the model that can use such ""bias"". In fact, the models are extremely accurate. Furthermore, the reason for selecting the number of NoTarget labels is ambiguous. For example, there should be far more NoTarget labels in a sustainability report or other data source.
- The sample text of the dataset is not presented in the paper, making it difficult for readers to understand.
- It is not clear whether the annotation is in a paragraph or a sentence -level.","- Any thoughts about A1 above?
- Any thoughts about A2 above?
- What is the nature of ""expert annotator""?
- How did the authors decide the number of NoTarget samples. 
- The evaluation results show extremely high classification performance of the model. This suggests that the task is extremely easy. Is it possible that the model is making decisions due to specific biases in the dataset?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"I recommend citing and discussing some net-zero or greenwashing related papers.
Below is an example of papers I know of, but there must be others.

- An Integrated Framework to Assess Greenwashing. Sustainability. 2022.
- The meaning of net zero and how to get it right. Nature Climate Change. 2022.","- By removing the bold text (""Motivation"", ""Contribution"", ""Results"", and ""Implications"") in Introduction, more paper space can be used?
- Table 1 should include train and test statistics.",1690849052430,,,EMNLP/2023/Conference,0ULLuIRdcu,"['EMNLP/2023/Conference/Submission1027/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461079045,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1027/Reviewer_8w4H']",0ULLuIRdcu,['EMNLP/2023/Conference/Submission1027/Reviewer_8w4H'],1690849052430,1701461079045,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1027/Reviewer_8w4H']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""An Integrated Framework to Assess Greenwashing. Sustainability. 2022.""
2. ""The meaning of net zero and how to get it right. Nature Climate Change. 2022.""

These citations might be necessary because the reviewer believes that discussing and citing relevant papers on net-zero and greenwashing is essential to provide a more comprehensive understanding of the topic and to support the authors' claims and motivations. The reviewer mentions that the paper lacks discussion on the motivation behind the task definition and the typologies of greenwashing, and that citing these papers could help address these issues. Additionally, the reviewer suggests that the authors should discuss and cite other relevant papers on net-zero and greenwashing to provide a more thorough background and context for their research.",1,"2022, 2022",An Integrated Framework to Assess Greenwashing 
9mTFzVNrQM,"The major contribution of this paper is the proposed dataset.

Net-zero (""net zero"" GHG emissions)  claims by firms and countries have been on the rise recently, but are sometimes pointed to as greenwashing.
This paper proposes a task and dataset for detecting net-zero claims from text.
The authors provided a 3.5K text annotated corpus including three labels: NetZero, Reduction, and NoTarget.
The fine-tuned ClimateBERT outperformed baselines.
The authors also experimented the reduction target year extraction by using QA approach.
Finally, a case study on the earning call transcripts analysis suggests that the net-zero target discussion has experienced a sharp increase since 2019.","- This paper proposes a novel task and dataset on a net-zero perspective of climate change. The dataset is valuable to researchers because this field is nascent and lacks datasets and task definitions. The authors will release the dataset and models, which will be useful for researchers in this domain.
- They appear to be collaborating with domain experts to create the dataset","The major drawback is that the paper lacks detail in the discussion and dataset.
It is especially important to discuss motives because this paper takes on a new challenge.
See below comments.

### High-level comment

A1. Lack of discussion for the motivation

The authors begin their motivation with a reference to greenwashing. Indeed, claims of net zero have sometimes been accused of greenwashing, so I agree with the importance of this motivation.
While the study seems to focus on extracting net-zero targets, there is no insight of what can be done to suppress greenwashing as a result.

There are various typologies of greenwashing, e.g., net zero may be dependent on carbon offsetting, there may be no clear intermediate target, it may not follow IPCC guidance, etc.

It is not clear what typologies of greenwashing the authors have in mind or whether they have extracted the necessary information to do so. It might be good to see these discussions in a related work, introduction, or discussion section.


A2. Lack of discussion for the task design

Similar to above, I would like to know the motivation behind the task definition.
The net-zero claim is a complex matter in practice. For example,
- Is the scope of the net-zero claim related to the company or for a specific project? Is it for Scope 1, 2, or 3 emissions?
- The difference between intermediary and long-term goals.
- Differences in units and calculation methods

The Appendix presents some of the guidelines, but they are oversimplified and lack an explanation of why the task was defined in the way.


### Low-level comment

B1. Detailed description of the dataset is missing.

- The label distribution for both train and test data should be described in Table 1.
- Please specify the nature of ""expert annotator""
- The design of the NoTarget label is questionable. These samples may be different from the domains of the NetZero and Reduction labels. Thus, they may overestimate the classification performance of the model that can use such ""bias"". In fact, the models are extremely accurate. Furthermore, the reason for selecting the number of NoTarget labels is ambiguous. For example, there should be far more NoTarget labels in a sustainability report or other data source.
- The sample text of the dataset is not presented in the paper, making it difficult for readers to understand.
- It is not clear whether the annotation is in a paragraph or a sentence -level.","- Any thoughts about A1 above?
- Any thoughts about A2 above?
- What is the nature of ""expert annotator""?
- How did the authors decide the number of NoTarget samples. 
- The evaluation results show extremely high classification performance of the model. This suggests that the task is extremely easy. Is it possible that the model is making decisions due to specific biases in the dataset?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"I recommend citing and discussing some net-zero or greenwashing related papers.
Below is an example of papers I know of, but there must be others.

- An Integrated Framework to Assess Greenwashing. Sustainability. 2022.
- The meaning of net zero and how to get it right. Nature Climate Change. 2022.","- By removing the bold text (""Motivation"", ""Contribution"", ""Results"", and ""Implications"") in Introduction, more paper space can be used?
- Table 1 should include train and test statistics.",1690849052430,,,EMNLP/2023/Conference,0ULLuIRdcu,"['EMNLP/2023/Conference/Submission1027/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461079045,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission1027/Reviewer_8w4H']",0ULLuIRdcu,['EMNLP/2023/Conference/Submission1027/Reviewer_8w4H'],1690849052430,1701461079045,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1027/Reviewer_8w4H']","Yes

The suggested papers or references not cited in the manuscript are:
1. ""An Integrated Framework to Assess Greenwashing. Sustainability. 2022.""
2. ""The meaning of net zero and how to get it right. Nature Climate Change. 2022.""

These citations might be necessary because the reviewer believes that discussing and citing relevant papers on net-zero and greenwashing is essential to provide a more comprehensive understanding of the topic and to support the authors' claims and motivations. The reviewer mentions that the paper lacks discussion on the motivation behind the task definition and the typologies of greenwashing, and that citing these papers could help address these issues. Additionally, the reviewer suggests that the authors should discuss and cite other relevant papers on net-zero and greenwashing to provide a more thorough background and context for their research.",1,"2022, 2022",The meaning of net zero and how to get it right
7hdjUtyBDe,"Summary: The authors investigate the functional form of the surprisal / reading time relationship across languages, honing in on whether the link is linear or super linear, which has been a topic of some debate in recent literature. Previous work has addressed this question either through visual inspection of the surprisal / RT function or through linear vs. nonlinear model comparison. The authors employ the latter, albeit a slightly different method than has been used in previous studies. They find that about half the languages tested demonstrate superlinear surprisal / RT effects, and that these tend to be for languages with larger datasets. Overall, I think this paper contributes to the current discussion and that the community will benefit from its inclusion in EMNLP. However, I do have some concerns about the statistical tests used.","(a) The authors address a timely topic that has been the focus of recent discussion in the literature

(b) The authors go beyond previous monolingual analyses, which is a valuable contribution

(c) The paper is well written and clear

(d) Although the authors present their handling of spillover has a limitation (I see their point) their method allows for variable spillover between datasets. This is better than many recent papers which set a fixed sized spillover window regardless of dataset.
","(a) Previous studies have demonstrated an effect of unigram surprisal (i.e. frequency) on reading times. Frequency is absent from this paper and not used in the baseline model M0. I’m worried that the overall picture of results could change if frequency was added in as an additional predictor. (The authors do mention that word length with random by-participant intercepts was the maximal consistently random effects structure – does this mean that they tried random slopes, or that they tried random slopes plus also other main effects, such as frequency?)

(b) The likelihood ratio tests were computed on the log likelihoods of the training data, correct? I think the paper could be made much stronger if the tests were conducted on a held-out portion of the dataset. Especially as models grow more complex, testing on held-out data grows crucial to avoid charges of overfitting. (My apologies if you are already doing this – it was not clear to me from the text!)
","(a) (135) It would be very helpful to see which languages are first fixation and which are gaze durations. It’s a little difficult to tell from Footnote 1. Relatedly, was there a relationship between FF / GD in the linear / non-linearity of the results? How do the authors think the different metrics changed or impacted their results? I would appreciate more discussion about this.

(b) (147) Could the authors provide a justification for this? Is this something that has been done in the previous literature?

(c) (Fig 1) One concern I have about the figure is that plotting the raw surprisal vs. RT data does not take into account things like the effect of word length on reading times. In previous studies such as Smith and Levy (2013) and Hoover et al., (2022) figures were generated by first fitting a model with both surprisal and baseline predictors, and then sampling from that model to show the isolated effect of surprisal alone. I think doing something like this could help make the implications of the data clearer.

(d) (257) I don’t follow this logic. Shouldn’t this mean that the non-linear effects of reanalysis are more apparent in smaller datasets, where a single non-linearity can skew the overall pattern in the data even more?
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"(a) I’m sure the authors are aware of Wilcox et al., “Testing the Predictions of Surprisal Theory in 11 Languages” but just in case you are not, then this paper was arXived recently. It reaches many of the same conclusions, but some different ones. Discussion of the differences – both in terms of methods and conclusions – will be essential in the final version of this paper.

(b) De Varda, Andrea, and Marco Marelli. ""Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times."" Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.

(c) Andrea de Varda and Marco Marelli. 2022. The effects of surprisal across languages: Results from native and non-native reading. In Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pages 138–144, Online only. Association for Computational Linguistics.
","(a) (37) “Whereas previous empirical work has addressed this question in English” → I would soften this claim a bit given the recent multilingual work that’s been appearing in the literature. This paper does include new languages, though, so that is worth pointing out up top!

(b) (96) I would mention what a sampling-based model is, briefly, here.
",1691067521887,,,EMNLP/2023/Conference,0SIyWZEOmJ,"['EMNLP/2023/Conference/Submission2520/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170131,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2520/Reviewer_fm4s']",0SIyWZEOmJ,['EMNLP/2023/Conference/Submission2520/Reviewer_fm4s'],1691067521887,1701461170131,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2520/Reviewer_fm4s']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wilcox et al., ""Testing the Predictions of Surprisal Theory in 11 Languages""
2. De Varda, Andrea, and Marco Marelli. ""Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times.""
3. Andrea de Varda and Marco Marelli. 2022. ""The effects of surprisal across languages: Results from native and non-native reading.""

These citations might be necessary because the reviewer believes that discussing the differences in methods and conclusions between the submitted paper and these recent studies is essential for the final version of the paper. The reviewer thinks that the authors should be aware of these studies, which have reached similar or different conclusions, and that including them in the discussion will strengthen the paper. Specifically, the reviewer mentions that Wilcox et al. reaches many of the same conclusions, but also some different ones, and that discussion of these differences is crucial. Additionally, the reviewer suggests that the authors should be aware of the recent multilingual work in the literature, such as the papers by De Varda and Marelli, which could provide valuable context and comparisons for the submitted paper.",1,2022,Testing the Predictions of Surprisal Theory in 11 Languages
7hdjUtyBDe,"Summary: The authors investigate the functional form of the surprisal / reading time relationship across languages, honing in on whether the link is linear or super linear, which has been a topic of some debate in recent literature. Previous work has addressed this question either through visual inspection of the surprisal / RT function or through linear vs. nonlinear model comparison. The authors employ the latter, albeit a slightly different method than has been used in previous studies. They find that about half the languages tested demonstrate superlinear surprisal / RT effects, and that these tend to be for languages with larger datasets. Overall, I think this paper contributes to the current discussion and that the community will benefit from its inclusion in EMNLP. However, I do have some concerns about the statistical tests used.","(a) The authors address a timely topic that has been the focus of recent discussion in the literature

(b) The authors go beyond previous monolingual analyses, which is a valuable contribution

(c) The paper is well written and clear

(d) Although the authors present their handling of spillover has a limitation (I see their point) their method allows for variable spillover between datasets. This is better than many recent papers which set a fixed sized spillover window regardless of dataset.
","(a) Previous studies have demonstrated an effect of unigram surprisal (i.e. frequency) on reading times. Frequency is absent from this paper and not used in the baseline model M0. I’m worried that the overall picture of results could change if frequency was added in as an additional predictor. (The authors do mention that word length with random by-participant intercepts was the maximal consistently random effects structure – does this mean that they tried random slopes, or that they tried random slopes plus also other main effects, such as frequency?)

(b) The likelihood ratio tests were computed on the log likelihoods of the training data, correct? I think the paper could be made much stronger if the tests were conducted on a held-out portion of the dataset. Especially as models grow more complex, testing on held-out data grows crucial to avoid charges of overfitting. (My apologies if you are already doing this – it was not clear to me from the text!)
","(a) (135) It would be very helpful to see which languages are first fixation and which are gaze durations. It’s a little difficult to tell from Footnote 1. Relatedly, was there a relationship between FF / GD in the linear / non-linearity of the results? How do the authors think the different metrics changed or impacted their results? I would appreciate more discussion about this.

(b) (147) Could the authors provide a justification for this? Is this something that has been done in the previous literature?

(c) (Fig 1) One concern I have about the figure is that plotting the raw surprisal vs. RT data does not take into account things like the effect of word length on reading times. In previous studies such as Smith and Levy (2013) and Hoover et al., (2022) figures were generated by first fitting a model with both surprisal and baseline predictors, and then sampling from that model to show the isolated effect of surprisal alone. I think doing something like this could help make the implications of the data clearer.

(d) (257) I don’t follow this logic. Shouldn’t this mean that the non-linear effects of reanalysis are more apparent in smaller datasets, where a single non-linearity can skew the overall pattern in the data even more?
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"(a) I’m sure the authors are aware of Wilcox et al., “Testing the Predictions of Surprisal Theory in 11 Languages” but just in case you are not, then this paper was arXived recently. It reaches many of the same conclusions, but some different ones. Discussion of the differences – both in terms of methods and conclusions – will be essential in the final version of this paper.

(b) De Varda, Andrea, and Marco Marelli. ""Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times."" Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.

(c) Andrea de Varda and Marco Marelli. 2022. The effects of surprisal across languages: Results from native and non-native reading. In Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pages 138–144, Online only. Association for Computational Linguistics.
","(a) (37) “Whereas previous empirical work has addressed this question in English” → I would soften this claim a bit given the recent multilingual work that’s been appearing in the literature. This paper does include new languages, though, so that is worth pointing out up top!

(b) (96) I would mention what a sampling-based model is, briefly, here.
",1691067521887,,,EMNLP/2023/Conference,0SIyWZEOmJ,"['EMNLP/2023/Conference/Submission2520/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170131,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2520/Reviewer_fm4s']",0SIyWZEOmJ,['EMNLP/2023/Conference/Submission2520/Reviewer_fm4s'],1691067521887,1701461170131,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2520/Reviewer_fm4s']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wilcox et al., ""Testing the Predictions of Surprisal Theory in 11 Languages""
2. De Varda, Andrea, and Marco Marelli. ""Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times.""
3. Andrea de Varda and Marco Marelli. 2022. ""The effects of surprisal across languages: Results from native and non-native reading.""

These citations might be necessary because the reviewer believes that discussing the differences in methods and conclusions between the submitted paper and these recent studies is essential for the final version of the paper. The reviewer thinks that the authors should be aware of these studies, which have reached similar or different conclusions, and that including them in the discussion will strengthen the paper. Specifically, the reviewer mentions that Wilcox et al. reaches many of the same conclusions, but also some different ones, and that discussion of these differences is crucial. Additionally, the reviewer suggests that the authors should be aware of the recent multilingual work in the literature, such as the papers by De Varda and Marelli, which could provide valuable context and comparisons for the submitted paper.",1,2022,Scaling in Cognitive Modelling a Multilingual Approach to Human Reading Times
7hdjUtyBDe,"Summary: The authors investigate the functional form of the surprisal / reading time relationship across languages, honing in on whether the link is linear or super linear, which has been a topic of some debate in recent literature. Previous work has addressed this question either through visual inspection of the surprisal / RT function or through linear vs. nonlinear model comparison. The authors employ the latter, albeit a slightly different method than has been used in previous studies. They find that about half the languages tested demonstrate superlinear surprisal / RT effects, and that these tend to be for languages with larger datasets. Overall, I think this paper contributes to the current discussion and that the community will benefit from its inclusion in EMNLP. However, I do have some concerns about the statistical tests used.","(a) The authors address a timely topic that has been the focus of recent discussion in the literature

(b) The authors go beyond previous monolingual analyses, which is a valuable contribution

(c) The paper is well written and clear

(d) Although the authors present their handling of spillover has a limitation (I see their point) their method allows for variable spillover between datasets. This is better than many recent papers which set a fixed sized spillover window regardless of dataset.
","(a) Previous studies have demonstrated an effect of unigram surprisal (i.e. frequency) on reading times. Frequency is absent from this paper and not used in the baseline model M0. I’m worried that the overall picture of results could change if frequency was added in as an additional predictor. (The authors do mention that word length with random by-participant intercepts was the maximal consistently random effects structure – does this mean that they tried random slopes, or that they tried random slopes plus also other main effects, such as frequency?)

(b) The likelihood ratio tests were computed on the log likelihoods of the training data, correct? I think the paper could be made much stronger if the tests were conducted on a held-out portion of the dataset. Especially as models grow more complex, testing on held-out data grows crucial to avoid charges of overfitting. (My apologies if you are already doing this – it was not clear to me from the text!)
","(a) (135) It would be very helpful to see which languages are first fixation and which are gaze durations. It’s a little difficult to tell from Footnote 1. Relatedly, was there a relationship between FF / GD in the linear / non-linearity of the results? How do the authors think the different metrics changed or impacted their results? I would appreciate more discussion about this.

(b) (147) Could the authors provide a justification for this? Is this something that has been done in the previous literature?

(c) (Fig 1) One concern I have about the figure is that plotting the raw surprisal vs. RT data does not take into account things like the effect of word length on reading times. In previous studies such as Smith and Levy (2013) and Hoover et al., (2022) figures were generated by first fitting a model with both surprisal and baseline predictors, and then sampling from that model to show the isolated effect of surprisal alone. I think doing something like this could help make the implications of the data clearer.

(d) (257) I don’t follow this logic. Shouldn’t this mean that the non-linear effects of reanalysis are more apparent in smaller datasets, where a single non-linearity can skew the overall pattern in the data even more?
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"(a) I’m sure the authors are aware of Wilcox et al., “Testing the Predictions of Surprisal Theory in 11 Languages” but just in case you are not, then this paper was arXived recently. It reaches many of the same conclusions, but some different ones. Discussion of the differences – both in terms of methods and conclusions – will be essential in the final version of this paper.

(b) De Varda, Andrea, and Marco Marelli. ""Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times."" Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.

(c) Andrea de Varda and Marco Marelli. 2022. The effects of surprisal across languages: Results from native and non-native reading. In Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pages 138–144, Online only. Association for Computational Linguistics.
","(a) (37) “Whereas previous empirical work has addressed this question in English” → I would soften this claim a bit given the recent multilingual work that’s been appearing in the literature. This paper does include new languages, though, so that is worth pointing out up top!

(b) (96) I would mention what a sampling-based model is, briefly, here.
",1691067521887,,,EMNLP/2023/Conference,0SIyWZEOmJ,"['EMNLP/2023/Conference/Submission2520/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170131,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2520/Reviewer_fm4s']",0SIyWZEOmJ,['EMNLP/2023/Conference/Submission2520/Reviewer_fm4s'],1691067521887,1701461170131,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2520/Reviewer_fm4s']","Yes

The suggested papers or references not cited in the manuscript are:
1. Wilcox et al., ""Testing the Predictions of Surprisal Theory in 11 Languages""
2. De Varda, Andrea, and Marco Marelli. ""Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times.""
3. Andrea de Varda and Marco Marelli. 2022. ""The effects of surprisal across languages: Results from native and non-native reading.""

These citations might be necessary because the reviewer believes that discussing the differences in methods and conclusions between the submitted paper and these recent studies is essential for the final version of the paper. The reviewer thinks that the authors should be aware of these studies, which have reached similar or different conclusions, and that including them in the discussion will strengthen the paper. Specifically, the reviewer mentions that Wilcox et al. reaches many of the same conclusions, but also some different ones, and that discussion of these differences is crucial. Additionally, the reviewer suggests that the authors should be aware of the recent multilingual work in the literature, such as the papers by De Varda and Marelli, which could provide valuable context and comparisons for the submitted paper.",1,2022,The effects of surprisal across languages Results from native and non-native reading
fnIToUVGUJ,"This paper tests the relationship between surprisal and reading time on seven languages. It is well established that surprisal and reading time are correlated, but recent work has debated whether the linking function is sublinear, linear, or superlinear. Much of the debate has focused on English data; this study expands the investigation to several other languages. Results suggest a slightly superlinear relationship between surprisal and RT.",The paper contributes useful crosslinguistic data to an ongoing debate about the connection between surprisal and reading time. The methods seem sound and the results are clearly presented.,"The opening sections of the paper seem to slightly mischaracterize Surprisal Theory. As presented in Levy 2008, Surprisal Theory posits that comprehension difficulty (e.g. reading time) is proportional to surprisal -- meaning that the linking function is linear. This would mean that theories that posit e.g. a superlinear linking function do not fall under Surprisal Theory per se.","A) Do you think any of your results would be different if you tested off-the-shelf language models instead of training your own?

B) How did you decide on the stopping criteria in Section 3.2, and why is Mandarin treated differently from the other languages?",4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Andrea de Varda and Marco Marelli. 2022. The Effects of Surprisal across Languages: Results from Native and Non-native Reading. In Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pages 138–144, Online only. Association for Computational Linguistics.","A) Wilcox et al. have a very recently submitted preprint which addresses similar questions: https://arxiv.org/abs/2307.03667

B) The Section 2 citation to Hikaru Clark et al. 2023 should just be Clark et al. 2023",1691191371546,,,EMNLP/2023/Conference,0SIyWZEOmJ,"['EMNLP/2023/Conference/Submission2520/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170038,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2520/Reviewer_JsrJ']",0SIyWZEOmJ,['EMNLP/2023/Conference/Submission2520/Reviewer_JsrJ'],1691191371546,1701461170038,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2520/Reviewer_JsrJ']","Yes

The suggested papers or references not cited in the manuscript are:
1. Andrea de Varda and Marco Marelli. 2022. The Effects of Surprisal across Languages: Results from Native and Non-native Reading.
2. Wilcox et al. (preprint), https://arxiv.org/abs/2307.03667

These citations might be necessary because the reviewer mentions them as relevant to the topic of the paper, specifically the study of surprisal and reading time across languages. The reviewer seems to think that the authors could benefit from being aware of and engaging with this related work, potentially to strengthen their argument or provide additional context for their findings.",1,"2022, 2023",The Effects of Surprisal across Languages: Results from Native and Non-native Reading
X22NGT2iZV,"The paper is about Reinforcement Learning based models for the task of QFS. They use multiple rewards with Policy Gradient networks, using signals such as ROUGE, BLEU, and Semantic Similarity,  ","- The curated datasets (RPEDT for passage embedding models and RQFT for analyses of QFS models) could be useful as a testbed for future research.

- The interesting discovery that RL aids in the acquisition of QFS by the model, even while considering the ROUGE score. Also, the Cluster Hypothesis outperforms rival passage embedders in providing enhanced semantic feedback.","- The most common QFS evaluation datasets are QMSum and SQuALITY. I would like to see the comparison w/ SL and w/ RL there, and compare to those existing results reported, for example, BART-LS, BART-Large SegEnc, and BART-Large SegEnc + SOCRATIC Pretraining. 

- The authors might want to double-check whether the claim in line 84 is correct. For example, ""Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)"" and some of the papers in their related work might be a good starting point to check more details. I am not very familiar with that line of research.","- Line 15: Be specific on which datasets.
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"- QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
- SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
- Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
- Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
- Adapting pretrained text-to-text models for long text sequences. (Xiong et al., 2022)
",,1691472874241,,,EMNLP/2023/Conference,0OtGfwj8eB,"['EMNLP/2023/Conference/Submission5329/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461342845,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']",0OtGfwj8eB,['EMNLP/2023/Conference/Submission5329/Reviewer_G1x8'],1691472874241,1701461342845,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']","Yes

The suggested papers or references not cited in the manuscript are:
1. QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
2. SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
3. Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
4. Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
5. Adapting pretrained text-to-text models for long text sequences (Xiong et al., 2022)
6. Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)

These citations might be necessary because the reviewer wants to see comparisons with existing results on common QFS evaluation datasets like QMSum and SQuALITY, and the authors' claims verified against related work, such as the paper by Hao et al. (2022) on teacher forcing and reward functions. The reviewer also suggests that the authors should be aware of other relevant research, such as SOCRATIC Pretraining, Interactive Query-Assisted Summarization, and Exploring Neural Models for Query-Focused Summarization, to strengthen their work.",1,"2021, 2023, 2022, 2022, 2022, 2022, 2022",QMSum A New Benchmark for Query-based Multi-domain Meeting Summarization 
X22NGT2iZV,"The paper is about Reinforcement Learning based models for the task of QFS. They use multiple rewards with Policy Gradient networks, using signals such as ROUGE, BLEU, and Semantic Similarity,  ","- The curated datasets (RPEDT for passage embedding models and RQFT for analyses of QFS models) could be useful as a testbed for future research.

- The interesting discovery that RL aids in the acquisition of QFS by the model, even while considering the ROUGE score. Also, the Cluster Hypothesis outperforms rival passage embedders in providing enhanced semantic feedback.","- The most common QFS evaluation datasets are QMSum and SQuALITY. I would like to see the comparison w/ SL and w/ RL there, and compare to those existing results reported, for example, BART-LS, BART-Large SegEnc, and BART-Large SegEnc + SOCRATIC Pretraining. 

- The authors might want to double-check whether the claim in line 84 is correct. For example, ""Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)"" and some of the papers in their related work might be a good starting point to check more details. I am not very familiar with that line of research.","- Line 15: Be specific on which datasets.
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"- QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
- SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
- Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
- Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
- Adapting pretrained text-to-text models for long text sequences. (Xiong et al., 2022)
",,1691472874241,,,EMNLP/2023/Conference,0OtGfwj8eB,"['EMNLP/2023/Conference/Submission5329/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461342845,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']",0OtGfwj8eB,['EMNLP/2023/Conference/Submission5329/Reviewer_G1x8'],1691472874241,1701461342845,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']","Yes

The suggested papers or references not cited in the manuscript are:
1. QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
2. SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
3. Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
4. Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
5. Adapting pretrained text-to-text models for long text sequences (Xiong et al., 2022)
6. Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)

These citations might be necessary because the reviewer wants to see comparisons with existing results on common QFS evaluation datasets like QMSum and SQuALITY, and the authors' claims verified against related work, such as the paper by Hao et al. (2022) on teacher forcing and reward functions. The reviewer also suggests that the authors should be aware of other relevant research, such as SOCRATIC Pretraining, Interactive Query-Assisted Summarization, and Exploring Neural Models for Query-Focused Summarization, to strengthen their work.",1,"2021, 2023, 2022, 2022, 2022, 2022, 2022",SOCRATIC Pretraining Question-Driven Pretraining for Controllable Summarization 
X22NGT2iZV,"The paper is about Reinforcement Learning based models for the task of QFS. They use multiple rewards with Policy Gradient networks, using signals such as ROUGE, BLEU, and Semantic Similarity,  ","- The curated datasets (RPEDT for passage embedding models and RQFT for analyses of QFS models) could be useful as a testbed for future research.

- The interesting discovery that RL aids in the acquisition of QFS by the model, even while considering the ROUGE score. Also, the Cluster Hypothesis outperforms rival passage embedders in providing enhanced semantic feedback.","- The most common QFS evaluation datasets are QMSum and SQuALITY. I would like to see the comparison w/ SL and w/ RL there, and compare to those existing results reported, for example, BART-LS, BART-Large SegEnc, and BART-Large SegEnc + SOCRATIC Pretraining. 

- The authors might want to double-check whether the claim in line 84 is correct. For example, ""Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)"" and some of the papers in their related work might be a good starting point to check more details. I am not very familiar with that line of research.","- Line 15: Be specific on which datasets.
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"- QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
- SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
- Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
- Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
- Adapting pretrained text-to-text models for long text sequences. (Xiong et al., 2022)
",,1691472874241,,,EMNLP/2023/Conference,0OtGfwj8eB,"['EMNLP/2023/Conference/Submission5329/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461342845,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']",0OtGfwj8eB,['EMNLP/2023/Conference/Submission5329/Reviewer_G1x8'],1691472874241,1701461342845,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']","Yes

The suggested papers or references not cited in the manuscript are:
1. QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
2. SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
3. Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
4. Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
5. Adapting pretrained text-to-text models for long text sequences (Xiong et al., 2022)
6. Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)

These citations might be necessary because the reviewer wants to see comparisons with existing results on common QFS evaluation datasets like QMSum and SQuALITY, and the authors' claims verified against related work, such as the paper by Hao et al. (2022) on teacher forcing and reward functions. The reviewer also suggests that the authors should be aware of other relevant research, such as SOCRATIC Pretraining, Interactive Query-Assisted Summarization, and Exploring Neural Models for Query-Focused Summarization, to strengthen their work.",1,"2021, 2023, 2022, 2022, 2022, 2022, 2022",Interactive Query-Assisted Summarization via Deep Reinforcement Learning 
X22NGT2iZV,"The paper is about Reinforcement Learning based models for the task of QFS. They use multiple rewards with Policy Gradient networks, using signals such as ROUGE, BLEU, and Semantic Similarity,  ","- The curated datasets (RPEDT for passage embedding models and RQFT for analyses of QFS models) could be useful as a testbed for future research.

- The interesting discovery that RL aids in the acquisition of QFS by the model, even while considering the ROUGE score. Also, the Cluster Hypothesis outperforms rival passage embedders in providing enhanced semantic feedback.","- The most common QFS evaluation datasets are QMSum and SQuALITY. I would like to see the comparison w/ SL and w/ RL there, and compare to those existing results reported, for example, BART-LS, BART-Large SegEnc, and BART-Large SegEnc + SOCRATIC Pretraining. 

- The authors might want to double-check whether the claim in line 84 is correct. For example, ""Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)"" and some of the papers in their related work might be a good starting point to check more details. I am not very familiar with that line of research.","- Line 15: Be specific on which datasets.
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"- QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
- SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
- Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
- Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
- Adapting pretrained text-to-text models for long text sequences. (Xiong et al., 2022)
",,1691472874241,,,EMNLP/2023/Conference,0OtGfwj8eB,"['EMNLP/2023/Conference/Submission5329/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461342845,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']",0OtGfwj8eB,['EMNLP/2023/Conference/Submission5329/Reviewer_G1x8'],1691472874241,1701461342845,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']","Yes

The suggested papers or references not cited in the manuscript are:
1. QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
2. SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
3. Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
4. Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
5. Adapting pretrained text-to-text models for long text sequences (Xiong et al., 2022)
6. Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)

These citations might be necessary because the reviewer wants to see comparisons with existing results on common QFS evaluation datasets like QMSum and SQuALITY, and the authors' claims verified against related work, such as the paper by Hao et al. (2022) on teacher forcing and reward functions. The reviewer also suggests that the authors should be aware of other relevant research, such as SOCRATIC Pretraining, Interactive Query-Assisted Summarization, and Exploring Neural Models for Query-Focused Summarization, to strengthen their work.",1,"2021, 2023, 2022, 2022, 2022, 2022, 2022",Exploring Neural Models for Query-Focused Summarization 
X22NGT2iZV,"The paper is about Reinforcement Learning based models for the task of QFS. They use multiple rewards with Policy Gradient networks, using signals such as ROUGE, BLEU, and Semantic Similarity,  ","- The curated datasets (RPEDT for passage embedding models and RQFT for analyses of QFS models) could be useful as a testbed for future research.

- The interesting discovery that RL aids in the acquisition of QFS by the model, even while considering the ROUGE score. Also, the Cluster Hypothesis outperforms rival passage embedders in providing enhanced semantic feedback.","- The most common QFS evaluation datasets are QMSum and SQuALITY. I would like to see the comparison w/ SL and w/ RL there, and compare to those existing results reported, for example, BART-LS, BART-Large SegEnc, and BART-Large SegEnc + SOCRATIC Pretraining. 

- The authors might want to double-check whether the claim in line 84 is correct. For example, ""Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)"" and some of the papers in their related work might be a good starting point to check more details. I am not very familiar with that line of research.","- Line 15: Be specific on which datasets.
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"- QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
- SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
- Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
- Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
- Adapting pretrained text-to-text models for long text sequences. (Xiong et al., 2022)
",,1691472874241,,,EMNLP/2023/Conference,0OtGfwj8eB,"['EMNLP/2023/Conference/Submission5329/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461342845,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']",0OtGfwj8eB,['EMNLP/2023/Conference/Submission5329/Reviewer_G1x8'],1691472874241,1701461342845,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']","Yes

The suggested papers or references not cited in the manuscript are:
1. QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
2. SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
3. Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
4. Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
5. Adapting pretrained text-to-text models for long text sequences (Xiong et al., 2022)
6. Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)

These citations might be necessary because the reviewer wants to see comparisons with existing results on common QFS evaluation datasets like QMSum and SQuALITY, and the authors' claims verified against related work, such as the paper by Hao et al. (2022) on teacher forcing and reward functions. The reviewer also suggests that the authors should be aware of other relevant research, such as SOCRATIC Pretraining, Interactive Query-Assisted Summarization, and Exploring Neural Models for Query-Focused Summarization, to strengthen their work.",1,"2021, 2023, 2022, 2022, 2022, 2022, 2022",Adapting pretrained text-to-text models for long text sequences 
X22NGT2iZV,"The paper is about Reinforcement Learning based models for the task of QFS. They use multiple rewards with Policy Gradient networks, using signals such as ROUGE, BLEU, and Semantic Similarity,  ","- The curated datasets (RPEDT for passage embedding models and RQFT for analyses of QFS models) could be useful as a testbed for future research.

- The interesting discovery that RL aids in the acquisition of QFS by the model, even while considering the ROUGE score. Also, the Cluster Hypothesis outperforms rival passage embedders in providing enhanced semantic feedback.","- The most common QFS evaluation datasets are QMSum and SQuALITY. I would like to see the comparison w/ SL and w/ RL there, and compare to those existing results reported, for example, BART-LS, BART-Large SegEnc, and BART-Large SegEnc + SOCRATIC Pretraining. 

- The authors might want to double-check whether the claim in line 84 is correct. For example, ""Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)"" and some of the papers in their related work might be a good starting point to check more details. I am not very familiar with that line of research.","- Line 15: Be specific on which datasets.
","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"- QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
- SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
- Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
- Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
- Adapting pretrained text-to-text models for long text sequences. (Xiong et al., 2022)
",,1691472874241,,,EMNLP/2023/Conference,0OtGfwj8eB,"['EMNLP/2023/Conference/Submission5329/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461342845,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']",0OtGfwj8eB,['EMNLP/2023/Conference/Submission5329/Reviewer_G1x8'],1691472874241,1701461342845,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission5329/Reviewer_G1x8']","Yes

The suggested papers or references not cited in the manuscript are:
1. QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization (Zhong et al., 2021)
2. SOCRATIC Pretraining: Question-Driven Pretraining for Controllable Summarization (Pagnoni et al., 2023)
3. Interactive Query-Assisted Summarization via Deep Reinforcement Learning (Shapira et al, 2022)
4. Exploring Neural Models for Query-Focused Summarization (Vig et al., 2022)
5. Adapting pretrained text-to-text models for long text sequences (Xiong et al., 2022)
6. Teacher Forcing Recovers Reward Functions for Text Generation (Hao et al., 2022)

These citations might be necessary because the reviewer wants to see comparisons with existing results on common QFS evaluation datasets like QMSum and SQuALITY, and the authors' claims verified against related work, such as the paper by Hao et al. (2022) on teacher forcing and reward functions. The reviewer also suggests that the authors should be aware of other relevant research, such as SOCRATIC Pretraining, Interactive Query-Assisted Summarization, and Exploring Neural Models for Query-Focused Summarization, to strengthen their work.",1,"2021, 2023, 2022, 2022, 2022, 2022, 2022",Teacher Forcing Recovers Reward Functions for Text Generation
7KDBhaXSfv,The paper propose to improve fairness for text classification via minimizing the mutual information between the task representation and representation of the sensitive attribute. The mutual is estimated with the Wasserstein distance following previous works. Experiments show that the proposed objective is effective in improving fairness.,The paper is written clearly.,"Since the objective is to minimize the mutual information, why adopting a low-bound for mutual information estimation? In [1], they also aim at improving fairness via minimizing mutual information, but adopt an upper-bound for estimation which is more plausible. 

I think the novelty is limited, since [1] has proposed a similar idea and has a better methodology design (i.e., by using an upper-bound estimation). More importantly, [1] is neither compared or cited in the paper. The ""Wasserstein Dependency Measure"" in the paper follows previous works and is not appropriate to be used in this setting (as a lower-bound estimation).


[1] https://arxiv.org/pdf/2103.06413.pdf",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,,,1690919864651,,,EMNLP/2023/Conference,0M2m9GUTLN,"['EMNLP/2023/Conference/Submission2804/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461189182,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2804/Reviewer_6C1r']",0M2m9GUTLN,['EMNLP/2023/Conference/Submission2804/Reviewer_6C1r'],1690919864651,1701461189182,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2804/Reviewer_6C1r']","Yes

* [1] https://arxiv.org/pdf/2103.06413.pdf

The reviewer suggests that the authors should cite this paper because it proposes a similar idea of improving fairness via minimizing mutual information, but with a better methodology design (using an upper-bound estimation). The reviewer implies that the authors' choice of using a lower-bound estimation is less plausible and that citing and comparing their work to [1] would be necessary to demonstrate the novelty and validity of their approach. The reviewer also mentions that [1] is not cited in the paper, which suggests that the authors may not be aware of this relevant work or have not properly acknowledged its contribution to the field.",1,2021,FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders
tLDB8pIbab,"This paper proposed a frame work TacoPrompt, a prompt-based framework for self-supervised taxonomy completion. The taxonomy completion task involves attaching new concepts to the most suitable positions in an existing taxonomy.  

The paper highlight the limitations of simply attaching new concepts to hypernyms without considering hyponyms. Instead, they emphasize the completion task, where new concepts are added as parent-child pairs in existing taxonomies. 

The paper suggests incorporating two subtasks - hypernym and hyponym matching  to enhance concept representation learning. However, the existing methods lack a comprehensive understanding of how these subtasks contribute to the main task and ignore their interdependencies. Moreover, paper mentioned that difficulties arise in attaching non-leaf queries, especially in low-resource or imbalanced taxonomies. The paper aims to address these issues and provide insights into improving completion methods for taxonomies.

TacoPrompt utilizes two variants of result context, explicit answers and implicit hidden vectors, to facilitate task collaboration. Experimental results demonstrate that TacoPrompt outperforms state-of-the-art methods on benchmark taxonomy datasets. The paper also analyzes the effectiveness of different tasks, contexts, and backbone language models in the proposed framework. Overall, author claims that TacoPrompt offers a promising approach for efficient and accurate taxonomy completion in natural language processing tasks.","1. As claimed by author, The paper proposes a novel prompt-based framework, TacoPrompt, for the challenging task of taxonomy completion.  While the concept of attaching new concepts to hypernyms and considering hyponyms is not new and has been previously studied, the novelty of TacoPrompt lies in its application of prompt learning to tackle the taxonomy completion task. Prompt learning is a recent and innovative technique that converts the task into a masked language model (MLM) form, effectively transforming it into a fill-in-the-blank problem.

2. The authors conduct extensive experiments on three benchmark taxonomy completion datasets, comparing TacoPrompt with state-of-the-art methods and taxonomy expansion baselines. The comprehensive evaluation, along with ablation studies, demonstrates the effectiveness of the proposed approach in different scenarios and highlights its superior performance, especially for attaching non-leaf concepts.","1. The paper claims its novelty in utilizing prompt learning for taxonomy completion. However, it seems that the approach primarily revolves around applying a Masked Language Model (MLM) in a taxonomy sequence. This raises questions about the true novelty of the contribution.

2.  Moreover, The reported results do demonstrate improvements over baselines, including ""TaxoExpan"". However, considering that ""TaxoExpan"" already incorporates attention mechanisms and network information, it raises questions about the true significance of the proposed method's performance enhancement. It is worth noting that the current approach appears to lack the utilization of network information, which could be essential in the context of taxonomy expansion tasks. Additional clarification and analysis from the authors would be valuable to better comprehend the actual impact of their proposed approach, especially in comparison to methods that leverage network information.

3. The authors have introduced a relatively straightforward approach, but the presentation and explanation of the method tend to be  complex.

4. It seems that there are some inconsistencies and issues with the equations presented in the paper. For example... (1) What is the difference between PM ([MASK] and PM ([M]. (2). Equation 4 is difficult to understand.",1. I am curious about why the framework performs poorly on the WordNet dataset compared to the other datasets.?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"1.While there may not be any prior work specifically on prompt learning for taxonomy completion, I came across a relevant study titled ""TEAM: A multitask learning based Taxonomy Expansion approach for Attach and Merge"". This paper delves into multi-task scenarios in the domain of taxonomy expansion. Considering the similarities in the research context, I believe that the reference to this paper is missing in this paper.",,1691225713999,,,EMNLP/2023/Conference,0KYSlQdMu6,"['EMNLP/2023/Conference/Submission2874/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461194815,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2874/Reviewer_Z6sr']",0KYSlQdMu6,['EMNLP/2023/Conference/Submission2874/Reviewer_Z6sr'],1691225713999,1701461194815,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2874/Reviewer_Z6sr']","Yes

List of suggested papers or references not cited in the manuscript:
1. ""TEAM: A multitask learning based Taxonomy Expansion approach for Attach and Merge""

These citations might be necessary because the reviewer believes that the paper ""TEAM: A multitask learning based Taxonomy Expansion approach for Attach and Merge"" is relevant to the research context of the submitted manuscript, as it delves into multi-task scenarios in the domain of taxonomy expansion, which is similar to the topic of the manuscript. The reviewer thinks that including this reference would be beneficial, possibly to provide a more comprehensive overview of existing work in the field or to compare and contrast the approaches.",1,,TEAM: A multitask learning based Taxonomy Expansion approach for Attach and Merge
UTxweKfd4U,"The paper proposes TacoPrompt, which utilizes prompting and multi-task objectives to address the taxonomy completion task. It leverages the prompt-based template to compute the probability of the query concept being attached to the candidate position. And then the framework divides the learning objective into two subtasks to learn the relationships between query nodes and the candidate positions respectively. And the experiments show that the proposed method outperforms all compared baselines by a large margin.","1. The proposed method of using prompts in the taxonomy completion task is interesting and the motivation is clear.
2. The writing is well-structured and easy to follow overall.
3. The experimental results show that the proposed framework is very effective.
4. The framework was extended to TacoPrompt-Chain and Hidden. These two variants are well-motivated and shown to be effective.","1. The overall novelty and contribution of the paper are limited. The prompting template is similar to what is described GenTaxo (Zeng et al., 2021) and TaxoEnrich (Jiang et al., 2022) where these two papers used pseudo sentences to generate the embeddings for the concepts. But the prompting template used in this paper is definitely more natural and effective. The multi-task learning objective is almost the same as what was used in TMN (Zhang et al., 2021), which also considers the relationships between query node and parent, query node and child respectively.
2. See the questions below.","A. Why is the current prompting template chosen in the first place? Would be good to see some ablation studies or analysis on the prompting template.

B. In some of the performance of the baselines reported in the main table, it seems that many methods performed worse than the baselines before them, which do not follow the patterns studied in the previous papers. Could you explain why these methods (TMN is better than TaxoEnrich and QEN in Food dataset, TMN is worse than Arborist in MeSH dataset, etc.) became worse in the comparisons?

C. Can you report the running time and the efficiency of the proposed method compared to other baselines? It seems to me that the framework is complex when the number of nodes becomes very large.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Zeng, Qingkai, et al. ""Enhancing taxonomy completion with concept generation via fusing relational representations."" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.
Hope to see some discussions and comparisons maybe with this paper since it's also one major baseline in other previous papers.",,1691375202016,,,EMNLP/2023/Conference,0KYSlQdMu6,"['EMNLP/2023/Conference/Submission2874/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461194740,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2874/Reviewer_6PQe']",0KYSlQdMu6,['EMNLP/2023/Conference/Submission2874/Reviewer_6PQe'],1691375202016,1701461194740,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2874/Reviewer_6PQe']","Yes

The suggested papers or references not cited in the manuscript are:
1. Zeng, Qingkai, et al. ""Enhancing taxonomy completion with concept generation via fusing relational representations."" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.
2. Jiang et al., ""TaxoEnrich"" (2022)
3. Zhang et al., ""TMN"" (2021)

These citations might be necessary because the reviewer mentions that the prompting template used in the paper is similar to what is described in GenTaxo (Zeng et al., 2021) and TaxoEnrich (Jiang et al., 2022), and the multi-task learning objective is almost the same as what was used in TMN (Zhang et al., 2021). The reviewer suggests that the authors should discuss and compare their work with these papers, as they are major baselines in previous papers and can provide context for the novelty and contribution of the proposed method.",1,"2021, 2022, 2021, 2021, 2022, 2021",Enhancing taxonomy completion with concept generation via fusing relational representations 
UTxweKfd4U,"The paper proposes TacoPrompt, which utilizes prompting and multi-task objectives to address the taxonomy completion task. It leverages the prompt-based template to compute the probability of the query concept being attached to the candidate position. And then the framework divides the learning objective into two subtasks to learn the relationships between query nodes and the candidate positions respectively. And the experiments show that the proposed method outperforms all compared baselines by a large margin.","1. The proposed method of using prompts in the taxonomy completion task is interesting and the motivation is clear.
2. The writing is well-structured and easy to follow overall.
3. The experimental results show that the proposed framework is very effective.
4. The framework was extended to TacoPrompt-Chain and Hidden. These two variants are well-motivated and shown to be effective.","1. The overall novelty and contribution of the paper are limited. The prompting template is similar to what is described GenTaxo (Zeng et al., 2021) and TaxoEnrich (Jiang et al., 2022) where these two papers used pseudo sentences to generate the embeddings for the concepts. But the prompting template used in this paper is definitely more natural and effective. The multi-task learning objective is almost the same as what was used in TMN (Zhang et al., 2021), which also considers the relationships between query node and parent, query node and child respectively.
2. See the questions below.","A. Why is the current prompting template chosen in the first place? Would be good to see some ablation studies or analysis on the prompting template.

B. In some of the performance of the baselines reported in the main table, it seems that many methods performed worse than the baselines before them, which do not follow the patterns studied in the previous papers. Could you explain why these methods (TMN is better than TaxoEnrich and QEN in Food dataset, TMN is worse than Arborist in MeSH dataset, etc.) became worse in the comparisons?

C. Can you report the running time and the efficiency of the proposed method compared to other baselines? It seems to me that the framework is complex when the number of nodes becomes very large.","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Zeng, Qingkai, et al. ""Enhancing taxonomy completion with concept generation via fusing relational representations."" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.
Hope to see some discussions and comparisons maybe with this paper since it's also one major baseline in other previous papers.",,1691375202016,,,EMNLP/2023/Conference,0KYSlQdMu6,"['EMNLP/2023/Conference/Submission2874/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461194740,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2874/Reviewer_6PQe']",0KYSlQdMu6,['EMNLP/2023/Conference/Submission2874/Reviewer_6PQe'],1691375202016,1701461194740,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2874/Reviewer_6PQe']","Yes

The suggested papers or references not cited in the manuscript are:
1. Zeng, Qingkai, et al. ""Enhancing taxonomy completion with concept generation via fusing relational representations."" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.
2. Jiang et al., ""TaxoEnrich"" (2022)
3. Zhang et al., ""TMN"" (2021)

These citations might be necessary because the reviewer mentions that the prompting template used in the paper is similar to what is described in GenTaxo (Zeng et al., 2021) and TaxoEnrich (Jiang et al., 2022), and the multi-task learning objective is almost the same as what was used in TMN (Zhang et al., 2021). The reviewer suggests that the authors should discuss and compare their work with these papers, as they are major baselines in previous papers and can provide context for the novelty and contribution of the proposed method.",1,"2021, 2022, 2021, 2021, 2022, 2021",TaxoEnrich: Self-Supervised Taxonomy Completion via Structure-Semantic Representations
i4bODoyAaN,"This paper proposes a new method called TacoPrompt for taxonomy completion. Taxonomy completion aims to attach new concepts to appropriate hypernym-hyponym pairs in an existing taxonomy. The key ideas are: Formulate taxonomy completion as a masked language modeling task, which allows efficiently learning to attach non-leaf concepts with few labeled examples. Use a multi-task learning approach with two subtasks: attaching the concept to the hypernym, and attaching it to the hyponym. Introduce a contextual approach to integrate the subtask results, showing them as contexts to enable collaboration between tasks. Propose two variants: TacoPrompt-Chain uses explicit subtask answers as context, while TacoPrompt-Hidden uses implicit vector representations. Experiments on 3 datasets show significant improvements over prior state-of-the-art methods, especially for attaching non-leaf concepts.","- Contributions are clearly articulated and convincingly demonstrated.
- Addresses limitations of prior work in non-leaf concept attachment.
- Strong empirical results, outperforming prior methods substantially.

---

Overall, the paper has solid experimentation, the results are relevant to the claims, and it demonstrates that using this method is reasonable. Therefore, I recommend a **Soundness=4.5**","- While the author claims to be the first to propose the MLM format for solving the taxonomy completion (TC) task, utilizing prompts to help BERT solve the TC task is not novel [1][2]. The author attributes the inspiration of TacoPrompt-Chain to Chain of Thought, but the author did not choose causal language models like GPT/OPT/Bloom/Llama to perform the TC task (to my knowledge, placing ""Yes""/""No"" at the end would allow a similar approach with TacoPrompt). We also do not know how well Large Language Models would perform.

---

Therefore, I still have doubts regarding Excitement, and if the author can adequately address my concerns, I am willing to reconsider the rating.",A. The baseline methods used are not the most recent state-of-the-art. Could you compare against methods from [1] and [2] to better situate the improvements?,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] Liu Z, Xu H, Wen Y, et al. TEMP: taxonomy expansion with dynamic margin loss through taxonomy-paths[C]//Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 3854-3863.

[2] Xia F, Weng Y, He S, et al. Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model[C]//Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 2023: 1032-1042.",,1691691416608,,,EMNLP/2023/Conference,0KYSlQdMu6,"['EMNLP/2023/Conference/Submission2874/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461194652,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2874/Reviewer_tsdR']",0KYSlQdMu6,['EMNLP/2023/Conference/Submission2874/Reviewer_tsdR'],1691691416608,1701461194652,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2874/Reviewer_tsdR']","Yes

The suggested papers or references not cited in the manuscript are:
1. Liu Z, Xu H, Wen Y, et al. TEMP: taxonomy expansion with dynamic margin loss through taxonomy-paths (2021)
2. Xia F, Weng Y, He S, et al. Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model (2023)

These citations might be necessary because the reviewer mentions that the baseline methods used in the paper are not the most recent state-of-the-art. The reviewer suggests comparing the proposed method against methods from [1] and [2] to better situate the improvements, implying that these papers represent recent advancements in the field that are relevant to the manuscript's topic. By citing these papers, the authors can provide a more comprehensive comparison and demonstrate the novelty and significance of their contribution.",1,"2021, 2023",TEMP: taxonomy expansion with dynamic margin loss through taxonomy-paths 
i4bODoyAaN,"This paper proposes a new method called TacoPrompt for taxonomy completion. Taxonomy completion aims to attach new concepts to appropriate hypernym-hyponym pairs in an existing taxonomy. The key ideas are: Formulate taxonomy completion as a masked language modeling task, which allows efficiently learning to attach non-leaf concepts with few labeled examples. Use a multi-task learning approach with two subtasks: attaching the concept to the hypernym, and attaching it to the hyponym. Introduce a contextual approach to integrate the subtask results, showing them as contexts to enable collaboration between tasks. Propose two variants: TacoPrompt-Chain uses explicit subtask answers as context, while TacoPrompt-Hidden uses implicit vector representations. Experiments on 3 datasets show significant improvements over prior state-of-the-art methods, especially for attaching non-leaf concepts.","- Contributions are clearly articulated and convincingly demonstrated.
- Addresses limitations of prior work in non-leaf concept attachment.
- Strong empirical results, outperforming prior methods substantially.

---

Overall, the paper has solid experimentation, the results are relevant to the claims, and it demonstrates that using this method is reasonable. Therefore, I recommend a **Soundness=4.5**","- While the author claims to be the first to propose the MLM format for solving the taxonomy completion (TC) task, utilizing prompts to help BERT solve the TC task is not novel [1][2]. The author attributes the inspiration of TacoPrompt-Chain to Chain of Thought, but the author did not choose causal language models like GPT/OPT/Bloom/Llama to perform the TC task (to my knowledge, placing ""Yes""/""No"" at the end would allow a similar approach with TacoPrompt). We also do not know how well Large Language Models would perform.

---

Therefore, I still have doubts regarding Excitement, and if the author can adequately address my concerns, I am willing to reconsider the rating.",A. The baseline methods used are not the most recent state-of-the-art. Could you compare against methods from [1] and [2] to better situate the improvements?,4: Strong: This study provides sufficient support for all of its claims/arguments. ,4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,"4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"[1] Liu Z, Xu H, Wen Y, et al. TEMP: taxonomy expansion with dynamic margin loss through taxonomy-paths[C]//Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 3854-3863.

[2] Xia F, Weng Y, He S, et al. Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model[C]//Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 2023: 1032-1042.",,1691691416608,,,EMNLP/2023/Conference,0KYSlQdMu6,"['EMNLP/2023/Conference/Submission2874/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461194652,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2874/Reviewer_tsdR']",0KYSlQdMu6,['EMNLP/2023/Conference/Submission2874/Reviewer_tsdR'],1691691416608,1701461194652,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2874/Reviewer_tsdR']","Yes

The suggested papers or references not cited in the manuscript are:
1. Liu Z, Xu H, Wen Y, et al. TEMP: taxonomy expansion with dynamic margin loss through taxonomy-paths (2021)
2. Xia F, Weng Y, He S, et al. Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model (2023)

These citations might be necessary because the reviewer mentions that the baseline methods used in the paper are not the most recent state-of-the-art. The reviewer suggests comparing the proposed method against methods from [1] and [2] to better situate the improvements, implying that these papers represent recent advancements in the field that are relevant to the manuscript's topic. By citing these papers, the authors can provide a more comprehensive comparison and demonstrate the novelty and significance of their contribution.",1,"2021, 2023",Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model
NNIdpFd5Xg,"The paper introduces a multi-task prompt learning method for tackling the problem of automatic taxonomy completion. This approach is characterized by its collaboration between two subtasks, specifically attaching concepts to the hypernym and hyponym within existing taxonomies. The proposed method formulates the completion task in a masked language model form, enabling the system to learn how to attach non-leaf concepts with limited labeled data. By integrating the results of subtasks in a contextual manner, the system allows collaborative completion. The paper also provides experimental evidence to demonstrate the superiority of TacoPrompt over existing methods on three benchmark datasets.","(1) Interesting Method: The proposed approach formulates the task as a multi-task prompt learning problem, which seems a interesting perspective for taxonomy completion.

(2) Robust Experimental Validation: The paper conducts comprehensive experiments to study diffierent aspects of the targeted task and the proposed method based on three benchmark datasets.

(3) Clear Writing: The paper is well-written and concise, allowing for easy understanding of the main concepts and contributions of the paper.","(1) Missing Important Reference: This paper misses the most related reference from IJCAI22, titled “TaxoPrompt: A Prompt-based Generation Method with Taxonomic Context for Self-Supervised Taxonomy Expansion”. This missed reference is the first to propose the prompt based method to solve the similar task (although the transcript under the review claims itself is the first one). 

(2) Unconvincing Results: According to Table 2, the proposed model greatly exceeds the baseline in different evaluation indicators in the three datasets. For example, the strongest baseline, QEN, 25% is exceeded on Hits@10 on the SemEval-Food dataset. But from the design of the method, I don't see enough advantages to support such an obvious improvement. Additionally, according to the ablation experiments in Table 3, the simplest base models in this paper, FT-ST, and the strongest baseline, QEN, use the same backbone LM (Bert). But there is a significant gap between them in the experimental results. Specifically, FT-ST is 19% higher than QEN on R@10 on the Food dataset. I think this result needs more explanation to make it convincing.","According to Table 2, it’s interesting that two versions of the proposed method are good at the different subtasks. Specifically, TacoPrompt-Hidden is better at leaf attachment while TacoPrompt-Hidden is more suitable for Non-leaf attachment. Is there any analysis on this question?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.",,"@inproceedings{xu2022taxoprompt,

  title={TaxoPrompt: A Prompt-based Generation Method with Taxonomic Context for Self-Supervised Taxonomy Expansion.},

  author={Xu, Hongyuan and Chen, Yunong and Liu, Zichen and Wen, Yanlong and Yuan, Xiaojie},

  booktitle={IJCAI},

  pages={4432--4438},

  year={2022}
}",,1691734406208,,,EMNLP/2023/Conference,0KYSlQdMu6,"['EMNLP/2023/Conference/Submission2874/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461194545,[],4,,,"['everyone', 'EMNLP/2023/Conference/Submission2874/Reviewer_j9Kp']",0KYSlQdMu6,['EMNLP/2023/Conference/Submission2874/Reviewer_j9Kp'],1691734406208,1701461194545,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2874/Reviewer_j9Kp']","Yes

The suggested paper is:
@inproceedings{xu2022taxoprompt,
  title={TaxoPrompt: A Prompt-based Generation Method with Taxonomic Context for Self-Supervised Taxonomy Expansion.},
  author={Xu, Hongyuan and Chen, Yunong and Liu, Zichen and Wen, Yanlong and Yuan, Xiaojie},
  booktitle={IJCAI},
  pages={4432--4438},
  year={2022}
}

This citation might be necessary because the reviewer claims that the paper ""TaxoPrompt: A Prompt-based Generation Method with Taxonomic Context for Self-Supervised Taxonomy Expansion"" from IJCAI22 is the first to propose a prompt-based method for a similar task, and the submitted manuscript misses this important reference. The reviewer implies that the authors' claim of being the first to propose a multi-task prompt learning approach for taxonomy completion may be incorrect, and citing this paper would provide a more accurate representation of the state of the art in the field.",1,2022,TaxoPrompt A Prompt-based Generation Method with Taxonomic Context for Self-Supervised Taxonomy Expansion
4xMr00T0te,"This paper considers the setting in which a the input to a task is reformulated as a masked language modeling task e.g., in the sentiment classification of a movie review converting the input and label (""This was a great movie"", Positive) to a masked language modeling task ""This was a great movie. The movie was [MASK]."" The authors discuss that, although this task conversion is flexible, it often suffers from poorly calibrated logits being used to decide the replacement of the [MASK]. The authors then offer a calibration method which they test on both unilingua multilingual language modeling encoders and which essentially ""shifts"" the thresholding based on the uncalibrated logit score. The authors show that this simple calibration method allows unilingual and multilingual MLMs to perform better than uncalibrated ones.","I would like to take this opportunity to congratulate the authors on a nice piece of work.
- The work tackles an important, under-explored problem in the are of calibrating MLMs which has implications on model reuse and the use of task specific fintetuning.
- The method proposed is simple and intuitive and does better than no calibration in most cases and sometimes outperforms SOTA calibration methods. (e.g. Amazon-P for the unilingual case and AGNews and XNLI for the multilingual case)
- The experiments are thorough and surpass my expectations for a short paper.","I believe this paper is worth accepting but only if the primary concern discussed below is addressed.
- Primary concern: The results, analysis and conclusions of the paper do not line up with the claimed contributions. In particular, paragraph 73-82 claims that ""its [the calibration method] effectiveness in achieving performance enhancements comparable to other existing calibration techniques"" which is amplified in the conclusion where a claim is made that ""the calibrated probabilities yield significant enhancements"". I don't believe this to be entirely true as, in several cases, the ""Penalty"" method performs much worse than current methods. This point is made not to devalue the author's contributions nor to use a ""Not SOTA so Reject"" argument, but rather because several, if not the majority, of results seem to dispute the authors' claim. I would consider slightly reframing the paper to be more exploratory/case-study-like. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.,5: Could easily reproduce the results.,No,5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.,,"Although you discuss them in your limitations section, I believe calibration work in autoregressive LMs should be at least discussed briefly. One reference which I was surprised was not included is the following:
1. Min, Sewon, et al. ""Noisy channel language model prompting for few-shot text classification."" arXiv preprint arXiv:2108.04106 (2021).","- Line 001: Pretraiend
- Line 181: ""after of""",1691793429900,,,EMNLP/2023/Conference,0GO8Dtl8lJ,"['EMNLP/2023/Conference/Submission4118/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461273652,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission4118/Reviewer_3ZFj']",0GO8Dtl8lJ,['EMNLP/2023/Conference/Submission4118/Reviewer_3ZFj'],1691793429900,1701461273652,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission4118/Reviewer_3ZFj']","Yes

List of suggested papers or references not cited in the manuscript:
1. Min, Sewon, et al. ""Noisy channel language model prompting for few-shot text classification."" arXiv preprint arXiv:2108.04106 (2021)

The reviewer suggests that this citation is necessary because it discusses calibration work in autoregressive LMs, which is relevant to the topic of the paper. The reviewer mentions that although the authors discuss their limitations, they should at least briefly discuss this related work to provide a more comprehensive understanding of the field. This citation might be necessary to provide a more complete overview of existing research on calibration techniques and to contextualize the authors' contributions within the broader field.",1,"2021, 2021",Noisy channel language model prompting for few-shot text classification
i7Xo6tzGkb,"The objective of this paper is to extend WEAT, the classic test for bias in word embeddings, to non-Western/European languages and locales. The paper has several contributions to advance this goal : first, a dataset of target and attribute pairs (like the original WEAT) are created for a subset of previously identified bias dimensions + 5 new dimensions. This dataset is created by semi-automatically translating existing English data to these languages. Second, the WEAT is adapted for multilingual settings by capturing multi-word translations of lexical items in English as well as by identifying a new metric to measure bias. An extensive evaluation of various pre-trained masked language models is carried out on this dataset and several observations about the models and languages studied are presented.","* Extensive evaluation of WEAT to 24 languages, including several languages not commonly studied in the fairness literature. The extension to human-centered contemporary biases is particularly noteworthy. Most of the evaluation is very sounds and backed by a large set of empirical results using the mechanisms and data developed in this work.","* The 24 languages mentioned in the paper are not explicitly spelled out anywhere, except for figures in the appendix (and that too using language codes). This is quite a glaring omission.
* It is difficult to judge the quality of the contribution of the dataset, since the paper does not include a single example, and the anonymous link does not work.
* The bias sensitivity metric is a little difficult to interpret, since it is not contextualized with respect to previous metrics (e.g. those by Kurita et al.) in the results. While I understand what the metric aims to do, the justification of creating a new metric is unclear. 
* Some observations in Section 4.3 are not backed up very rigorously. For instance, the conclusion in line 406 is driven by a web-search + conversation with a single annotator. Similarly, the conclusion in line 426 is also based on a single annotator. It is unclear to what extent this is influenced by selection bias.",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"* Not penalizing the paper for this, since it is contemporaneous, but [1] and several papers cited in [1] are relevant for future iterations of this work. 

[1] - https://arxiv.org/abs/2305.11242",,1691353335799,,,EMNLP/2023/Conference,0EQ4z8n5rp,"['EMNLP/2023/Conference/Submission2145/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461146188,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission2145/Reviewer_L4qs']",0EQ4z8n5rp,['EMNLP/2023/Conference/Submission2145/Reviewer_L4qs'],1691353335799,1701461146188,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2145/Reviewer_L4qs']","Yes

* [1] and several papers cited in [1], available at https://arxiv.org/abs/2305.11242

The reviewer mentions that these citations are relevant for future iterations of the work, implying that they might be necessary to further support or contextualize the research presented in the manuscript. Specifically, the reviewer notes that the paper ""[1]"" and the papers cited within it are contemporaneous and relevant, suggesting that they could provide additional insights or background information that would enhance the manuscript. The reviewer does not penalize the authors for not including these citations, but rather suggests them as a potential area for future improvement.",1,2023,Comparing Biases and the Impact of Multilingual Training across Multiple Languages
kB1RwfWAnA,"Since traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting perform better than dense retrieval (DR) model, this paper first presents a thorough examination of the zero-shot capability of DR models. This paper can help us to understand the zero-shot capacity of the DR model.",The experimental evaluation is adequate and draws some useful conclusions.,"1. RocketQAv2 looks a bit dated. It would be better to adopt a newer model such as AR2 and ColBERTv2. It is not clear if these conclusions are applicable to other PLMs.
2. Some of the conclusions weren't very exciting. For example, the model trained on MSMARCO dataset achieves better performance than NQ.",1. Is there some way to use the conclusions drawn from this paper to improve performance in zero-shot setting?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"1: Not my area, or paper was hard for me to understand. My evaluation is just an educated guess.",,"1. ADVERSARIAL RETRIEVER-RANKER FOR DENSE TEXT RETRIEVAL, ICLR 2022
2. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction, NAACL 2022","1. In line 204, Furthermore, The ... .",1690558604648,,,EMNLP/2023/Conference,0DyJbE93XO,"['EMNLP/2023/Conference/Submission1753/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461121315,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1753/Reviewer_19Ji']",0DyJbE93XO,['EMNLP/2023/Conference/Submission1753/Reviewer_19Ji'],1690558604648,1701461121315,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1753/Reviewer_19Ji']","Yes

The suggested papers or references not cited in the manuscript are:
1. ADVERSARIAL RETRIEVER-RANKER FOR DENSE TEXT RETRIEVAL, ICLR 2022
2. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction, NAACL 2022
3. AR2 (no specific publication details provided, but mentioned as a newer model)

These citations might be necessary because the reviewer mentions that the model used in the paper, RocketQAv2, ""looks a bit dated"" and suggests that adopting newer models like AR2 and ColBERTv2 would be better. The reviewer also explicitly lists the two papers as ""Missing_References"", implying that they are relevant to the topic and should be considered by the authors. The inclusion of these references could help to strengthen the paper's conclusions and make them more applicable to current research in the field.",1,"2022, 2022",ADVERSARIAL RETRIEVER-RANKER FOR DENSE TEXT RETRIEVAL 
kB1RwfWAnA,"Since traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting perform better than dense retrieval (DR) model, this paper first presents a thorough examination of the zero-shot capability of DR models. This paper can help us to understand the zero-shot capacity of the DR model.",The experimental evaluation is adequate and draws some useful conclusions.,"1. RocketQAv2 looks a bit dated. It would be better to adopt a newer model such as AR2 and ColBERTv2. It is not clear if these conclusions are applicable to other PLMs.
2. Some of the conclusions weren't very exciting. For example, the model trained on MSMARCO dataset achieves better performance than NQ.",1. Is there some way to use the conclusions drawn from this paper to improve performance in zero-shot setting?,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.,No,"1: Not my area, or paper was hard for me to understand. My evaluation is just an educated guess.",,"1. ADVERSARIAL RETRIEVER-RANKER FOR DENSE TEXT RETRIEVAL, ICLR 2022
2. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction, NAACL 2022","1. In line 204, Furthermore, The ... .",1690558604648,,,EMNLP/2023/Conference,0DyJbE93XO,"['EMNLP/2023/Conference/Submission1753/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461121315,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission1753/Reviewer_19Ji']",0DyJbE93XO,['EMNLP/2023/Conference/Submission1753/Reviewer_19Ji'],1690558604648,1701461121315,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission1753/Reviewer_19Ji']","Yes

The suggested papers or references not cited in the manuscript are:
1. ADVERSARIAL RETRIEVER-RANKER FOR DENSE TEXT RETRIEVAL, ICLR 2022
2. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction, NAACL 2022
3. AR2 (no specific publication details provided, but mentioned as a newer model)

These citations might be necessary because the reviewer mentions that the model used in the paper, RocketQAv2, ""looks a bit dated"" and suggests that adopting newer models like AR2 and ColBERTv2 would be better. The reviewer also explicitly lists the two papers as ""Missing_References"", implying that they are relevant to the topic and should be considered by the authors. The inclusion of these references could help to strengthen the paper's conclusions and make them more applicable to current research in the field.",1,"2022, 2022",ColBERTv2 Effective and Efficient Retrieval via Lightweight Late Interaction
aRzR6yJvE2,"Since existing PLMs are usually  pre-trained on general corpus, directly fine-tuning the existing PLMs with the Expert Finding task may not be the most optimal approach for Community Question Answering (CQA).  Furthermore, such architectures fail to account for the personalized characteristics of experts.
To solve the aforementioned problems, the paper designs a novel expert finding framework. The main contributions include: 
1) designing a novel question title-body contrastive pre-training task, which could improve question modeling effectively,
2) adopting a personalized tuning network to learn personalized representations of experts.
","1) The proposed  title-body contrastive pre-training task  and the personalized tuning network sound reasonable and innovative
2) The performance comparison is well-executed as it involves multiple baselines compared across six datasets, making the results highly convincing.","1) Inadequate method details: The ""expert ID embedding"" mentioned in Section 4.3 is somewhat confusing as it lacks specific clarification.  It remains unclear whether this ID refers to the registered name of the expert or some other form of identification.  If it simply represents the expert's registered name, its ability to capture the personalized characteristics of the expert is questionable.
2) Insufficient experiments: The ""Further Analysis"" section of the paper and the experiments of ""ExpertPLM: Pre-training Expert Representation for Expert Finding (https://aclanthology.org/2022.findings-emnlp.74.pdf)"" share significant similarities. However, there is a lack of experimental analysis specific to certain parameters within the paper itself, such as the length of the question body and title and the number of negative samples (K value).
3) Lack of fair comparison: In Figure 3, the authors compared CPEF with PMEF to demonstrate the advantages of the pre-trained question representation model under data scarcity conditions (line 529-lin534). However, emphasizing the advantages of CPEF through this comparison is unjust since PMEF lacks a pre-training module. To ensure fairness, it is recommended to compare CPEF with another pre-trained model, such as ExpertBert, to showcase the advantage of the innovative pre-training module design of CPEF.
","A. What exactly does ""expert ID"" refer to? Is it the username or something else? Furthermore, can the ID information truly represent the personal characteristics of the expert?
B. Why are the lengths of the question body and question title set to 30 and 10 respectively, which are far from the maximum length of Bert(512 
characters)? Is it possible to lose information if the length is too short? Is the length setting the same for different datasets?
C. The experimental dataset is different from that mentioned in the comparison method paper, why can't choose the same batch of experimental data?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,ExpertPLM: Pre-training Expert Representation for Expert Finding (https://aclanthology.org/2022.findings-emnlp.74.pdf),"1) It is suggested to replace ""comparative learning"" on line 487 with ""contrastive learning"", which is more in line with the academic expression.
2) There are  inconsistent format settings for ""log"" in line 295 and line 307, it is suggested to unify after checking.
",1691044304549,,,EMNLP/2023/Conference,0DkaimvWs0,"['EMNLP/2023/Conference/Submission793/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461054440,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission793/Reviewer_L924']",0DkaimvWs0,['EMNLP/2023/Conference/Submission793/Reviewer_L924'],1691044304549,1701461054440,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission793/Reviewer_L924']","Yes

List of suggested papers or references not cited in the manuscript:
1. ExpertPLM: Pre-training Expert Representation for Expert Finding (https://aclanthology.org/2022.findings-emnlp.74.pdf)
2. Possibly ExpertBert (mentioned as a comparison model)

These citations might be necessary because the reviewer mentions that the ""Further Analysis"" section of the paper shares significant similarities with the experiments in ExpertPLM, suggesting that the authors should be aware of and compare their work to this existing research. Additionally, the reviewer recommends comparing the proposed model (CPEF) with another pre-trained model like ExpertBert to ensure a fair comparison, implying that the authors should be familiar with and cite relevant work in the field of expert finding and pre-trained language models.",1,2022,ExpertPLM: Pre-training Expert Representation for Expert Finding 
aRzR6yJvE2,"Since existing PLMs are usually  pre-trained on general corpus, directly fine-tuning the existing PLMs with the Expert Finding task may not be the most optimal approach for Community Question Answering (CQA).  Furthermore, such architectures fail to account for the personalized characteristics of experts.
To solve the aforementioned problems, the paper designs a novel expert finding framework. The main contributions include: 
1) designing a novel question title-body contrastive pre-training task, which could improve question modeling effectively,
2) adopting a personalized tuning network to learn personalized representations of experts.
","1) The proposed  title-body contrastive pre-training task  and the personalized tuning network sound reasonable and innovative
2) The performance comparison is well-executed as it involves multiple baselines compared across six datasets, making the results highly convincing.","1) Inadequate method details: The ""expert ID embedding"" mentioned in Section 4.3 is somewhat confusing as it lacks specific clarification.  It remains unclear whether this ID refers to the registered name of the expert or some other form of identification.  If it simply represents the expert's registered name, its ability to capture the personalized characteristics of the expert is questionable.
2) Insufficient experiments: The ""Further Analysis"" section of the paper and the experiments of ""ExpertPLM: Pre-training Expert Representation for Expert Finding (https://aclanthology.org/2022.findings-emnlp.74.pdf)"" share significant similarities. However, there is a lack of experimental analysis specific to certain parameters within the paper itself, such as the length of the question body and title and the number of negative samples (K value).
3) Lack of fair comparison: In Figure 3, the authors compared CPEF with PMEF to demonstrate the advantages of the pre-trained question representation model under data scarcity conditions (line 529-lin534). However, emphasizing the advantages of CPEF through this comparison is unjust since PMEF lacks a pre-training module. To ensure fairness, it is recommended to compare CPEF with another pre-trained model, such as ExpertBert, to showcase the advantage of the innovative pre-training module design of CPEF.
","A. What exactly does ""expert ID"" refer to? Is it the username or something else? Furthermore, can the ID information truly represent the personal characteristics of the expert?
B. Why are the lengths of the question body and question title set to 30 and 10 respectively, which are far from the maximum length of Bert(512 
characters)? Is it possible to lose information if the length is too short? Is the length setting the same for different datasets?
C. The experimental dataset is different from that mentioned in the comparison method paper, why can't choose the same batch of experimental data?","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,ExpertPLM: Pre-training Expert Representation for Expert Finding (https://aclanthology.org/2022.findings-emnlp.74.pdf),"1) It is suggested to replace ""comparative learning"" on line 487 with ""contrastive learning"", which is more in line with the academic expression.
2) There are  inconsistent format settings for ""log"" in line 295 and line 307, it is suggested to unify after checking.
",1691044304549,,,EMNLP/2023/Conference,0DkaimvWs0,"['EMNLP/2023/Conference/Submission793/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461054440,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission793/Reviewer_L924']",0DkaimvWs0,['EMNLP/2023/Conference/Submission793/Reviewer_L924'],1691044304549,1701461054440,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission793/Reviewer_L924']","Yes

List of suggested papers or references not cited in the manuscript:
1. ExpertPLM: Pre-training Expert Representation for Expert Finding (https://aclanthology.org/2022.findings-emnlp.74.pdf)
2. Possibly ExpertBert (mentioned as a comparison model)

These citations might be necessary because the reviewer mentions that the ""Further Analysis"" section of the paper shares significant similarities with the experiments in ExpertPLM, suggesting that the authors should be aware of and compare their work to this existing research. Additionally, the reviewer recommends comparing the proposed model (CPEF) with another pre-trained model like ExpertBert to ensure a fair comparison, implying that the authors should be familiar with and cite relevant work in the field of expert finding and pre-trained language models.",1,2022,ExpertBert
vtcMb3QzKi,"This paper proposes techniques for improving question representation in the expert finding task through contrastive learning, and for finding experts that match personalized preferences by learning personalized expert representations.

In particular, it suggests viewing the question title and body as a positive pair, offering a solution to overcome the limitations of data augmentation methods, which could potentially be laborious work.
For personalized tuning, they proposed a mechanism grounded in attention-based processing of historically answered questions.

The authors assert the superiority and effectiveness of the proposed method based on the experimental results.","1. When comparing the experimental results of the proposed method with other baseline methods, the proposed method exhibits superior performance.
2. The authors effectively performed question representation without manual tasks like labeling by introducing contrastive learning techniques to unlabeled data.","1. It appears that the explanation for the necessity of personalized fine-tuning is insufficient.
2. Shouldn't there be a comparison with existing data augmentation methods, even if it's limited to a small amount of data?
3. It seems necessary to compare the proposed positive pair method with other unsupervised contrastive learning methods, such as SimCSE: Simple Contrastive Learning of Sentence Embeddings (EMNLP 2021).



","1. It appears that the explanation for the necessity of personalized fine-tuning is insufficient.
2. Shouldn't there be a comparison with existing data augmentation methods, even if it's limited to a small amount of data?
3. It seems necessary to compare the proposed positive pair method with other unsupervised contrastive learning methods, such as SimCSE: Simple Contrastive Learning of Sentence Embeddings (EMNLP 2021).



","3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Tianyu Gao, Xingcheng Yao, and  Danqi Chen, SimCSE: Simple Contrastive Learning of Sentence Embeddings, EMNLP 2021
",,1691130211222,,,EMNLP/2023/Conference,0DkaimvWs0,"['EMNLP/2023/Conference/Submission793/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461054324,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission793/Reviewer_hASg']",0DkaimvWs0,['EMNLP/2023/Conference/Submission793/Reviewer_hASg'],1691130211222,1701461054324,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission793/Reviewer_hASg']","Yes

The suggested papers or references not cited in the manuscript are:
1. Tianyu Gao, Xingcheng Yao, and Danqi Chen, SimCSE: Simple Contrastive Learning of Sentence Embeddings, EMNLP 2021

These citations might be necessary because the reviewer suggests comparing the proposed positive pair method with other unsupervised contrastive learning methods, such as SimCSE. This comparison could provide a more comprehensive evaluation of the proposed method's effectiveness and demonstrate its superiority over existing approaches. Additionally, the reviewer questions the lack of comparison with existing data augmentation methods, implying that citing relevant papers on data augmentation could strengthen the manuscript by addressing potential limitations and alternatives to the proposed technique.",1,2021,SimCSE: Simple Contrastive Learning of Sentence Embeddings
JhcEHBYXgq,"This paper analyzes the strength and limitations of graph and sequence based representations of knowledge for knowledge grounded dialogue. They use 3 datasets and conduct studies to test effect of different representations on response quality, factual consistency, generalizability and response scores in few shot settings. The paper tries to compare graphs and sequences head to head and answer which for may be better under various circumstances. 

","The paper suggests some good findings as follows:
- the knowledge graph outperforms generation quality and exhibits stronger generalizability, while the knowledge sequence outperforms factual consistency in generations.
- Performance can be effectively improved by denoising the knowledge, for example, by selecting the succinct sequence or extracting a structured knowledge graph.
- Performance could be universally improved further by advanced Dual-Encoders structure or by employing domain adaption pre-training; however, impact of model size and knowledge size is understudied
- (and more) 
The findings should be reported in a tabular form for quick access to the results and findings. 
- Exhaustive set of experiments are reported in the appendix to study the 3 major questions authors are trying to study in this work.","- This seems like a limited study that uses 3 datasets for studying knowledge augmented response generation. The goal of the study is to compare graphs and text sequences. How one represents the text and graphs structures for knowledge augmentation including the reasoning methods utilized with the graphs is understudied (to make general claims about which paradigm is better)   
- Dialog evaluation metrics are very dated and do not correlate well with human judgment.  
- Response quality results seem to be inconclusive and dataset dependent 
- As the authors note, factual consistency results may be biased due to the text based metrics used in the study 
- Various graph embedding approaches exist, how would these interplay with the response generation system and compare to the sequence based representations.
- Role of prompting that is very common presently in response generation is not studied at all in the paper. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Search suggests several recent surveys exist on the topic; these are not mentioned in the paper. 

Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.” ACM Computing Surveys, vol. 54, no. 11s, Jan. 2022, pp. 1–38. Crossref, https://doi.org/10.1145/3512467.

Mialon, Grégoire, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière et al. ""Augmented language models: a survey."" arXiv preprint arXiv:2302.07842 (2023).

Chowdhury, T., Ling, C., Zhang, X., Zhao, X., Bai, G., Pei, J., Chen, H. and Zhao, L., 2023. Knowledge-enhanced Neural Machine Reasoning: A Review. arXiv preprint arXiv:2302.02093.

Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J. and Wu, X., 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. arXiv preprint arXiv:2306.08302.

Groth, P., Simperl, E., van Erp, M. and Vrandečić, D., 2023. Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372). In Dagstuhl Reports (Vol. 12, No. 9). Schloss Dagstuhl-Leibniz-Zentrum für Informatik.
",,1691367398364,,,EMNLP/2023/Conference,06oozRd4jU,"['EMNLP/2023/Conference/Submission3047/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206212,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']",06oozRd4jU,['EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J'],1691367398364,1701461206212,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']","Yes

Here are the suggested papers or references not cited in the manuscript:
1. Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.”
2. Mialon, Grégoire, et al. ""Augmented language models: a survey.""
3. Chowdhury, T., et al. ""Knowledge-enhanced Neural Machine Reasoning: A Review.""
4. Pan, S., et al. ""Unifying Large Language Models and Knowledge Graphs: A Roadmap.""
5. Groth, P., et al. ""Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372).""

These citations might be necessary because the reviewer mentions that ""Search suggests several recent surveys exist on the topic; these are not mentioned in the paper."" This implies that the reviewer believes the authors should be aware of and reference recent surveys and research related to knowledge-enhanced text generation, augmented language models, and knowledge graphs. The suggested papers appear to be recent surveys and reviews in the field, which could provide context and background information for the authors' research, as well as help to identify gaps and areas for further study.",1,,A Survey of Knowledge-Enhanced Text Generation
JhcEHBYXgq,"This paper analyzes the strength and limitations of graph and sequence based representations of knowledge for knowledge grounded dialogue. They use 3 datasets and conduct studies to test effect of different representations on response quality, factual consistency, generalizability and response scores in few shot settings. The paper tries to compare graphs and sequences head to head and answer which for may be better under various circumstances. 

","The paper suggests some good findings as follows:
- the knowledge graph outperforms generation quality and exhibits stronger generalizability, while the knowledge sequence outperforms factual consistency in generations.
- Performance can be effectively improved by denoising the knowledge, for example, by selecting the succinct sequence or extracting a structured knowledge graph.
- Performance could be universally improved further by advanced Dual-Encoders structure or by employing domain adaption pre-training; however, impact of model size and knowledge size is understudied
- (and more) 
The findings should be reported in a tabular form for quick access to the results and findings. 
- Exhaustive set of experiments are reported in the appendix to study the 3 major questions authors are trying to study in this work.","- This seems like a limited study that uses 3 datasets for studying knowledge augmented response generation. The goal of the study is to compare graphs and text sequences. How one represents the text and graphs structures for knowledge augmentation including the reasoning methods utilized with the graphs is understudied (to make general claims about which paradigm is better)   
- Dialog evaluation metrics are very dated and do not correlate well with human judgment.  
- Response quality results seem to be inconclusive and dataset dependent 
- As the authors note, factual consistency results may be biased due to the text based metrics used in the study 
- Various graph embedding approaches exist, how would these interplay with the response generation system and compare to the sequence based representations.
- Role of prompting that is very common presently in response generation is not studied at all in the paper. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Search suggests several recent surveys exist on the topic; these are not mentioned in the paper. 

Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.” ACM Computing Surveys, vol. 54, no. 11s, Jan. 2022, pp. 1–38. Crossref, https://doi.org/10.1145/3512467.

Mialon, Grégoire, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière et al. ""Augmented language models: a survey."" arXiv preprint arXiv:2302.07842 (2023).

Chowdhury, T., Ling, C., Zhang, X., Zhao, X., Bai, G., Pei, J., Chen, H. and Zhao, L., 2023. Knowledge-enhanced Neural Machine Reasoning: A Review. arXiv preprint arXiv:2302.02093.

Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J. and Wu, X., 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. arXiv preprint arXiv:2306.08302.

Groth, P., Simperl, E., van Erp, M. and Vrandečić, D., 2023. Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372). In Dagstuhl Reports (Vol. 12, No. 9). Schloss Dagstuhl-Leibniz-Zentrum für Informatik.
",,1691367398364,,,EMNLP/2023/Conference,06oozRd4jU,"['EMNLP/2023/Conference/Submission3047/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206212,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']",06oozRd4jU,['EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J'],1691367398364,1701461206212,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']","Yes

Here are the suggested papers or references not cited in the manuscript:
1. Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.”
2. Mialon, Grégoire, et al. ""Augmented language models: a survey.""
3. Chowdhury, T., et al. ""Knowledge-enhanced Neural Machine Reasoning: A Review.""
4. Pan, S., et al. ""Unifying Large Language Models and Knowledge Graphs: A Roadmap.""
5. Groth, P., et al. ""Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372).""

These citations might be necessary because the reviewer mentions that ""Search suggests several recent surveys exist on the topic; these are not mentioned in the paper."" This implies that the reviewer believes the authors should be aware of and reference recent surveys and research related to knowledge-enhanced text generation, augmented language models, and knowledge graphs. The suggested papers appear to be recent surveys and reviews in the field, which could provide context and background information for the authors' research, as well as help to identify gaps and areas for further study.",1,,Augmented language models a survey
JhcEHBYXgq,"This paper analyzes the strength and limitations of graph and sequence based representations of knowledge for knowledge grounded dialogue. They use 3 datasets and conduct studies to test effect of different representations on response quality, factual consistency, generalizability and response scores in few shot settings. The paper tries to compare graphs and sequences head to head and answer which for may be better under various circumstances. 

","The paper suggests some good findings as follows:
- the knowledge graph outperforms generation quality and exhibits stronger generalizability, while the knowledge sequence outperforms factual consistency in generations.
- Performance can be effectively improved by denoising the knowledge, for example, by selecting the succinct sequence or extracting a structured knowledge graph.
- Performance could be universally improved further by advanced Dual-Encoders structure or by employing domain adaption pre-training; however, impact of model size and knowledge size is understudied
- (and more) 
The findings should be reported in a tabular form for quick access to the results and findings. 
- Exhaustive set of experiments are reported in the appendix to study the 3 major questions authors are trying to study in this work.","- This seems like a limited study that uses 3 datasets for studying knowledge augmented response generation. The goal of the study is to compare graphs and text sequences. How one represents the text and graphs structures for knowledge augmentation including the reasoning methods utilized with the graphs is understudied (to make general claims about which paradigm is better)   
- Dialog evaluation metrics are very dated and do not correlate well with human judgment.  
- Response quality results seem to be inconclusive and dataset dependent 
- As the authors note, factual consistency results may be biased due to the text based metrics used in the study 
- Various graph embedding approaches exist, how would these interplay with the response generation system and compare to the sequence based representations.
- Role of prompting that is very common presently in response generation is not studied at all in the paper. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Search suggests several recent surveys exist on the topic; these are not mentioned in the paper. 

Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.” ACM Computing Surveys, vol. 54, no. 11s, Jan. 2022, pp. 1–38. Crossref, https://doi.org/10.1145/3512467.

Mialon, Grégoire, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière et al. ""Augmented language models: a survey."" arXiv preprint arXiv:2302.07842 (2023).

Chowdhury, T., Ling, C., Zhang, X., Zhao, X., Bai, G., Pei, J., Chen, H. and Zhao, L., 2023. Knowledge-enhanced Neural Machine Reasoning: A Review. arXiv preprint arXiv:2302.02093.

Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J. and Wu, X., 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. arXiv preprint arXiv:2306.08302.

Groth, P., Simperl, E., van Erp, M. and Vrandečić, D., 2023. Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372). In Dagstuhl Reports (Vol. 12, No. 9). Schloss Dagstuhl-Leibniz-Zentrum für Informatik.
",,1691367398364,,,EMNLP/2023/Conference,06oozRd4jU,"['EMNLP/2023/Conference/Submission3047/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206212,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']",06oozRd4jU,['EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J'],1691367398364,1701461206212,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']","Yes

Here are the suggested papers or references not cited in the manuscript:
1. Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.”
2. Mialon, Grégoire, et al. ""Augmented language models: a survey.""
3. Chowdhury, T., et al. ""Knowledge-enhanced Neural Machine Reasoning: A Review.""
4. Pan, S., et al. ""Unifying Large Language Models and Knowledge Graphs: A Roadmap.""
5. Groth, P., et al. ""Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372).""

These citations might be necessary because the reviewer mentions that ""Search suggests several recent surveys exist on the topic; these are not mentioned in the paper."" This implies that the reviewer believes the authors should be aware of and reference recent surveys and research related to knowledge-enhanced text generation, augmented language models, and knowledge graphs. The suggested papers appear to be recent surveys and reviews in the field, which could provide context and background information for the authors' research, as well as help to identify gaps and areas for further study.",1,,Knowledge-enhanced Neural Machine Reasoning A Review
JhcEHBYXgq,"This paper analyzes the strength and limitations of graph and sequence based representations of knowledge for knowledge grounded dialogue. They use 3 datasets and conduct studies to test effect of different representations on response quality, factual consistency, generalizability and response scores in few shot settings. The paper tries to compare graphs and sequences head to head and answer which for may be better under various circumstances. 

","The paper suggests some good findings as follows:
- the knowledge graph outperforms generation quality and exhibits stronger generalizability, while the knowledge sequence outperforms factual consistency in generations.
- Performance can be effectively improved by denoising the knowledge, for example, by selecting the succinct sequence or extracting a structured knowledge graph.
- Performance could be universally improved further by advanced Dual-Encoders structure or by employing domain adaption pre-training; however, impact of model size and knowledge size is understudied
- (and more) 
The findings should be reported in a tabular form for quick access to the results and findings. 
- Exhaustive set of experiments are reported in the appendix to study the 3 major questions authors are trying to study in this work.","- This seems like a limited study that uses 3 datasets for studying knowledge augmented response generation. The goal of the study is to compare graphs and text sequences. How one represents the text and graphs structures for knowledge augmentation including the reasoning methods utilized with the graphs is understudied (to make general claims about which paradigm is better)   
- Dialog evaluation metrics are very dated and do not correlate well with human judgment.  
- Response quality results seem to be inconclusive and dataset dependent 
- As the authors note, factual consistency results may be biased due to the text based metrics used in the study 
- Various graph embedding approaches exist, how would these interplay with the response generation system and compare to the sequence based representations.
- Role of prompting that is very common presently in response generation is not studied at all in the paper. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Search suggests several recent surveys exist on the topic; these are not mentioned in the paper. 

Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.” ACM Computing Surveys, vol. 54, no. 11s, Jan. 2022, pp. 1–38. Crossref, https://doi.org/10.1145/3512467.

Mialon, Grégoire, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière et al. ""Augmented language models: a survey."" arXiv preprint arXiv:2302.07842 (2023).

Chowdhury, T., Ling, C., Zhang, X., Zhao, X., Bai, G., Pei, J., Chen, H. and Zhao, L., 2023. Knowledge-enhanced Neural Machine Reasoning: A Review. arXiv preprint arXiv:2302.02093.

Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J. and Wu, X., 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. arXiv preprint arXiv:2306.08302.

Groth, P., Simperl, E., van Erp, M. and Vrandečić, D., 2023. Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372). In Dagstuhl Reports (Vol. 12, No. 9). Schloss Dagstuhl-Leibniz-Zentrum für Informatik.
",,1691367398364,,,EMNLP/2023/Conference,06oozRd4jU,"['EMNLP/2023/Conference/Submission3047/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206212,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']",06oozRd4jU,['EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J'],1691367398364,1701461206212,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']","Yes

Here are the suggested papers or references not cited in the manuscript:
1. Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.”
2. Mialon, Grégoire, et al. ""Augmented language models: a survey.""
3. Chowdhury, T., et al. ""Knowledge-enhanced Neural Machine Reasoning: A Review.""
4. Pan, S., et al. ""Unifying Large Language Models and Knowledge Graphs: A Roadmap.""
5. Groth, P., et al. ""Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372).""

These citations might be necessary because the reviewer mentions that ""Search suggests several recent surveys exist on the topic; these are not mentioned in the paper."" This implies that the reviewer believes the authors should be aware of and reference recent surveys and research related to knowledge-enhanced text generation, augmented language models, and knowledge graphs. The suggested papers appear to be recent surveys and reviews in the field, which could provide context and background information for the authors' research, as well as help to identify gaps and areas for further study.",1,,Unifying Large Language Models and Knowledge Graphs A Roadmap
JhcEHBYXgq,"This paper analyzes the strength and limitations of graph and sequence based representations of knowledge for knowledge grounded dialogue. They use 3 datasets and conduct studies to test effect of different representations on response quality, factual consistency, generalizability and response scores in few shot settings. The paper tries to compare graphs and sequences head to head and answer which for may be better under various circumstances. 

","The paper suggests some good findings as follows:
- the knowledge graph outperforms generation quality and exhibits stronger generalizability, while the knowledge sequence outperforms factual consistency in generations.
- Performance can be effectively improved by denoising the knowledge, for example, by selecting the succinct sequence or extracting a structured knowledge graph.
- Performance could be universally improved further by advanced Dual-Encoders structure or by employing domain adaption pre-training; however, impact of model size and knowledge size is understudied
- (and more) 
The findings should be reported in a tabular form for quick access to the results and findings. 
- Exhaustive set of experiments are reported in the appendix to study the 3 major questions authors are trying to study in this work.","- This seems like a limited study that uses 3 datasets for studying knowledge augmented response generation. The goal of the study is to compare graphs and text sequences. How one represents the text and graphs structures for knowledge augmentation including the reasoning methods utilized with the graphs is understudied (to make general claims about which paradigm is better)   
- Dialog evaluation metrics are very dated and do not correlate well with human judgment.  
- Response quality results seem to be inconclusive and dataset dependent 
- As the authors note, factual consistency results may be biased due to the text based metrics used in the study 
- Various graph embedding approaches exist, how would these interplay with the response generation system and compare to the sequence based representations.
- Role of prompting that is very common presently in response generation is not studied at all in the paper. ",,"3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.","3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"Search suggests several recent surveys exist on the topic; these are not mentioned in the paper. 

Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.” ACM Computing Surveys, vol. 54, no. 11s, Jan. 2022, pp. 1–38. Crossref, https://doi.org/10.1145/3512467.

Mialon, Grégoire, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière et al. ""Augmented language models: a survey."" arXiv preprint arXiv:2302.07842 (2023).

Chowdhury, T., Ling, C., Zhang, X., Zhao, X., Bai, G., Pei, J., Chen, H. and Zhao, L., 2023. Knowledge-enhanced Neural Machine Reasoning: A Review. arXiv preprint arXiv:2302.02093.

Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J. and Wu, X., 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. arXiv preprint arXiv:2306.08302.

Groth, P., Simperl, E., van Erp, M. and Vrandečić, D., 2023. Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372). In Dagstuhl Reports (Vol. 12, No. 9). Schloss Dagstuhl-Leibniz-Zentrum für Informatik.
",,1691367398364,,,EMNLP/2023/Conference,06oozRd4jU,"['EMNLP/2023/Conference/Submission3047/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461206212,[],3,,,"['everyone', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']",06oozRd4jU,['EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J'],1691367398364,1701461206212,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission3047/Reviewer_Ez2J']","Yes

Here are the suggested papers or references not cited in the manuscript:
1. Yu, Wenhao, et al. “A Survey of Knowledge-Enhanced Text Generation.”
2. Mialon, Grégoire, et al. ""Augmented language models: a survey.""
3. Chowdhury, T., et al. ""Knowledge-enhanced Neural Machine Reasoning: A Review.""
4. Pan, S., et al. ""Unifying Large Language Models and Knowledge Graphs: A Roadmap.""
5. Groth, P., et al. ""Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372).""

These citations might be necessary because the reviewer mentions that ""Search suggests several recent surveys exist on the topic; these are not mentioned in the paper."" This implies that the reviewer believes the authors should be aware of and reference recent surveys and research related to knowledge-enhanced text generation, augmented language models, and knowledge graphs. The suggested papers appear to be recent surveys and reviews in the field, which could provide context and background information for the authors' research, as well as help to identify gaps and areas for further study.",1,,Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century
AfzTBT3ugY,"This paper starts by making the observation that randomly selecting in-context examples leads to high variance in performance, similar to previous observations that shuffling example order leads to performance fluctuation. The authors then propose to select in-context learning examples from unlabeled data with low output entropy (or equivalently high information gain) and use the LLM predictions on these data as in-context examples. The authors further make the observation that the template sometimes induces bias, and they propose calibrating output logits with the CC technique from Zhao et al, 2021 before computing the entropy. They then evaluate several classification tasks on various GPT-style LLMs to show the promise of the proposed method.","- ICL has become one of the dominant learning paradigms for NLP, and this paper thus addresses an important problem.
- The methodology design is convincing. Both components of the suggested approach (calibration and ICL selection) are reasonable and intuitive, and the method can be applied in a black-box setting 1) without the need to access model parameters except for the output logits, 2) is capable of working in a zero-shot setup without accessing labels. These are other pluses of the proposed method.
- Experimental validation is largely thorough, and it is nice that the authors have also added several analyses and experiments combining their methods with existing methods to show further enhancements. I have some concerns, though, as listed in the section below.","1. The paper makes three main contributions, namely 1) that ICL is sensitive to random demo selection, 2) ICL selection based on entropy and 3) calibration before sampling. On Point 1, I feel that this is largely expected in light of existing findings that models can be sensitive to even the orders of the examples (which the authors cite) -- I'd tend to think that using *different demos* is a larger source of uncertainty compared to *demos of different order*, so it's unsurprising that randomly sampling demos lead to large variance.


   On 2) and 3), while combining ICL selection and calibration may be new, each of the components is not, and similar ideas have been previously explored. For the first component, the authors are essentially proposing to leverage the low-entropy / high-confidence predictions from the models, and similar ideas have been explored in a number of previous works like [1], where the authors fine-tune the LLMs on the confident outputs based on self-consistency to self-improve. [2] is more directly related, where the authors use confident outputs as in-context examples. [3] is a concurrent work, and the use of USP in the CLS case is almost identical to the proposed method in this paper, but USP also handles additional generative tasks and can work even without accessing the logits in some cases. On the calibration part, the authors essentially lift the CC technique for selection -- while the authors try to distinguish their usage from the one proposed in Zhao et al., 2021, I feel that CC is still essentially used in the same way except that the outputs are now used to compute entropy rather than generating outputs directly.  

   Note that I recognize that [2] and [3] are considered contemporaneous to this work, and I have *not* penalized the authors for not discussing them. However, I'd still encourage the authors to discuss the relation with respect to these works when they have a chance to revise their paper.

2. In terms of experiments, I have some suggestions for further improvement: 

- In addition to random sampling, there are other suggested approaches for ICL selection, including but not limited to nearest neighbour retrieval (Liu et al., 2021 -- cited in the paper) and diversity based on clustering [4]. I think these are nice to have, even though Liu et al., 2021 and [4] work in slightly different setups ( Liu et al., 2021 assume the availability of golden labels, and [4] is originally proposed for chain-of-thought tasks, like [2], but I think it's easy to adapt the idea of [4] into general tasks) -- note that given the difference in setup, I do not deem the lack of comparison to these works as major weaknesses against the authors. 
- It might be beneficial to consider a wider range of tasks outside general classification -- there are ample examples in the GPT-3 paper, for example, where the authors consider broader NLI and commonsense reasoning tasks.",Please address my concerns in *Reasons to Reject*.,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., & Han, J. (2022). Large language models can self-improve. arXiv preprint arXiv:2210.11610.

[2] Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.

[3] Wan, X., Sun, R., Nakhost, H., Dai, H., Eisenschlos, J. M., Arik, S. O., & Pfister, T. (2023). Universal Self-adaptive Prompting. arXiv preprint arXiv:2305.14926.

[4] Zhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting in large language models. ICLR.

I acknowledge that [2] and [3] are contemporaneous and I have not penalized the authors for not comparing against these works.
",,1690033842504,,,EMNLP/2023/Conference,05vb8rwGct,"['EMNLP/2023/Conference/Submission2521/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170579,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2521/Reviewer_CxLY']",05vb8rwGct,['EMNLP/2023/Conference/Submission2521/Reviewer_CxLY'],1690033842504,1701461170579,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2521/Reviewer_CxLY']","Yes

The suggested papers or references not cited in the manuscript are:
1. Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., & Han, J. (2022). Large language models can self-improve. arXiv preprint arXiv:2210.11610.
2. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.
3. Wan, X., Sun, R., Nakhost, H., Dai, H., Eisenschlos, J. M., Arik, S. O., & Pfister, T. (2023). Universal Self-adaptive Prompting. arXiv preprint arXiv:2305.14926.
4. Zhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting in large language models. ICLR.

These citations might be necessary because the reviewer believes that the authors' proposed method of selecting in-context learning examples based on low output entropy and calibrating output logits has similarities to existing works. The reviewer suggests that the authors discuss the relation of their work to these contemporaneous and prior studies to provide a more comprehensive understanding of the research landscape and to distinguish their contributions from existing ideas. Additionally, the reviewer recommends comparing the authors' approach to other methods for ICL selection, such as nearest neighbor retrieval and diversity-based selection, which are discussed in the suggested papers.",1,"2022, 2022, 2023, 2023, 2023, 2023, 2023",Large language models can self-improve
AfzTBT3ugY,"This paper starts by making the observation that randomly selecting in-context examples leads to high variance in performance, similar to previous observations that shuffling example order leads to performance fluctuation. The authors then propose to select in-context learning examples from unlabeled data with low output entropy (or equivalently high information gain) and use the LLM predictions on these data as in-context examples. The authors further make the observation that the template sometimes induces bias, and they propose calibrating output logits with the CC technique from Zhao et al, 2021 before computing the entropy. They then evaluate several classification tasks on various GPT-style LLMs to show the promise of the proposed method.","- ICL has become one of the dominant learning paradigms for NLP, and this paper thus addresses an important problem.
- The methodology design is convincing. Both components of the suggested approach (calibration and ICL selection) are reasonable and intuitive, and the method can be applied in a black-box setting 1) without the need to access model parameters except for the output logits, 2) is capable of working in a zero-shot setup without accessing labels. These are other pluses of the proposed method.
- Experimental validation is largely thorough, and it is nice that the authors have also added several analyses and experiments combining their methods with existing methods to show further enhancements. I have some concerns, though, as listed in the section below.","1. The paper makes three main contributions, namely 1) that ICL is sensitive to random demo selection, 2) ICL selection based on entropy and 3) calibration before sampling. On Point 1, I feel that this is largely expected in light of existing findings that models can be sensitive to even the orders of the examples (which the authors cite) -- I'd tend to think that using *different demos* is a larger source of uncertainty compared to *demos of different order*, so it's unsurprising that randomly sampling demos lead to large variance.


   On 2) and 3), while combining ICL selection and calibration may be new, each of the components is not, and similar ideas have been previously explored. For the first component, the authors are essentially proposing to leverage the low-entropy / high-confidence predictions from the models, and similar ideas have been explored in a number of previous works like [1], where the authors fine-tune the LLMs on the confident outputs based on self-consistency to self-improve. [2] is more directly related, where the authors use confident outputs as in-context examples. [3] is a concurrent work, and the use of USP in the CLS case is almost identical to the proposed method in this paper, but USP also handles additional generative tasks and can work even without accessing the logits in some cases. On the calibration part, the authors essentially lift the CC technique for selection -- while the authors try to distinguish their usage from the one proposed in Zhao et al., 2021, I feel that CC is still essentially used in the same way except that the outputs are now used to compute entropy rather than generating outputs directly.  

   Note that I recognize that [2] and [3] are considered contemporaneous to this work, and I have *not* penalized the authors for not discussing them. However, I'd still encourage the authors to discuss the relation with respect to these works when they have a chance to revise their paper.

2. In terms of experiments, I have some suggestions for further improvement: 

- In addition to random sampling, there are other suggested approaches for ICL selection, including but not limited to nearest neighbour retrieval (Liu et al., 2021 -- cited in the paper) and diversity based on clustering [4]. I think these are nice to have, even though Liu et al., 2021 and [4] work in slightly different setups ( Liu et al., 2021 assume the availability of golden labels, and [4] is originally proposed for chain-of-thought tasks, like [2], but I think it's easy to adapt the idea of [4] into general tasks) -- note that given the difference in setup, I do not deem the lack of comparison to these works as major weaknesses against the authors. 
- It might be beneficial to consider a wider range of tasks outside general classification -- there are ample examples in the GPT-3 paper, for example, where the authors consider broader NLI and commonsense reasoning tasks.",Please address my concerns in *Reasons to Reject*.,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., & Han, J. (2022). Large language models can self-improve. arXiv preprint arXiv:2210.11610.

[2] Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.

[3] Wan, X., Sun, R., Nakhost, H., Dai, H., Eisenschlos, J. M., Arik, S. O., & Pfister, T. (2023). Universal Self-adaptive Prompting. arXiv preprint arXiv:2305.14926.

[4] Zhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting in large language models. ICLR.

I acknowledge that [2] and [3] are contemporaneous and I have not penalized the authors for not comparing against these works.
",,1690033842504,,,EMNLP/2023/Conference,05vb8rwGct,"['EMNLP/2023/Conference/Submission2521/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170579,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2521/Reviewer_CxLY']",05vb8rwGct,['EMNLP/2023/Conference/Submission2521/Reviewer_CxLY'],1690033842504,1701461170579,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2521/Reviewer_CxLY']","Yes

The suggested papers or references not cited in the manuscript are:
1. Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., & Han, J. (2022). Large language models can self-improve. arXiv preprint arXiv:2210.11610.
2. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.
3. Wan, X., Sun, R., Nakhost, H., Dai, H., Eisenschlos, J. M., Arik, S. O., & Pfister, T. (2023). Universal Self-adaptive Prompting. arXiv preprint arXiv:2305.14926.
4. Zhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting in large language models. ICLR.

These citations might be necessary because the reviewer believes that the authors' proposed method of selecting in-context learning examples based on low output entropy and calibrating output logits has similarities to existing works. The reviewer suggests that the authors discuss the relation of their work to these contemporaneous and prior studies to provide a more comprehensive understanding of the research landscape and to distinguish their contributions from existing ideas. Additionally, the reviewer recommends comparing the authors' approach to other methods for ICL selection, such as nearest neighbor retrieval and diversity-based selection, which are discussed in the suggested papers.",1,"2022, 2022, 2023, 2023, 2023, 2023, 2023",Better Zero-Shot Reasoning with Self-Adaptive Prompting
AfzTBT3ugY,"This paper starts by making the observation that randomly selecting in-context examples leads to high variance in performance, similar to previous observations that shuffling example order leads to performance fluctuation. The authors then propose to select in-context learning examples from unlabeled data with low output entropy (or equivalently high information gain) and use the LLM predictions on these data as in-context examples. The authors further make the observation that the template sometimes induces bias, and they propose calibrating output logits with the CC technique from Zhao et al, 2021 before computing the entropy. They then evaluate several classification tasks on various GPT-style LLMs to show the promise of the proposed method.","- ICL has become one of the dominant learning paradigms for NLP, and this paper thus addresses an important problem.
- The methodology design is convincing. Both components of the suggested approach (calibration and ICL selection) are reasonable and intuitive, and the method can be applied in a black-box setting 1) without the need to access model parameters except for the output logits, 2) is capable of working in a zero-shot setup without accessing labels. These are other pluses of the proposed method.
- Experimental validation is largely thorough, and it is nice that the authors have also added several analyses and experiments combining their methods with existing methods to show further enhancements. I have some concerns, though, as listed in the section below.","1. The paper makes three main contributions, namely 1) that ICL is sensitive to random demo selection, 2) ICL selection based on entropy and 3) calibration before sampling. On Point 1, I feel that this is largely expected in light of existing findings that models can be sensitive to even the orders of the examples (which the authors cite) -- I'd tend to think that using *different demos* is a larger source of uncertainty compared to *demos of different order*, so it's unsurprising that randomly sampling demos lead to large variance.


   On 2) and 3), while combining ICL selection and calibration may be new, each of the components is not, and similar ideas have been previously explored. For the first component, the authors are essentially proposing to leverage the low-entropy / high-confidence predictions from the models, and similar ideas have been explored in a number of previous works like [1], where the authors fine-tune the LLMs on the confident outputs based on self-consistency to self-improve. [2] is more directly related, where the authors use confident outputs as in-context examples. [3] is a concurrent work, and the use of USP in the CLS case is almost identical to the proposed method in this paper, but USP also handles additional generative tasks and can work even without accessing the logits in some cases. On the calibration part, the authors essentially lift the CC technique for selection -- while the authors try to distinguish their usage from the one proposed in Zhao et al., 2021, I feel that CC is still essentially used in the same way except that the outputs are now used to compute entropy rather than generating outputs directly.  

   Note that I recognize that [2] and [3] are considered contemporaneous to this work, and I have *not* penalized the authors for not discussing them. However, I'd still encourage the authors to discuss the relation with respect to these works when they have a chance to revise their paper.

2. In terms of experiments, I have some suggestions for further improvement: 

- In addition to random sampling, there are other suggested approaches for ICL selection, including but not limited to nearest neighbour retrieval (Liu et al., 2021 -- cited in the paper) and diversity based on clustering [4]. I think these are nice to have, even though Liu et al., 2021 and [4] work in slightly different setups ( Liu et al., 2021 assume the availability of golden labels, and [4] is originally proposed for chain-of-thought tasks, like [2], but I think it's easy to adapt the idea of [4] into general tasks) -- note that given the difference in setup, I do not deem the lack of comparison to these works as major weaknesses against the authors. 
- It might be beneficial to consider a wider range of tasks outside general classification -- there are ample examples in the GPT-3 paper, for example, where the authors consider broader NLI and commonsense reasoning tasks.",Please address my concerns in *Reasons to Reject*.,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., & Han, J. (2022). Large language models can self-improve. arXiv preprint arXiv:2210.11610.

[2] Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.

[3] Wan, X., Sun, R., Nakhost, H., Dai, H., Eisenschlos, J. M., Arik, S. O., & Pfister, T. (2023). Universal Self-adaptive Prompting. arXiv preprint arXiv:2305.14926.

[4] Zhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting in large language models. ICLR.

I acknowledge that [2] and [3] are contemporaneous and I have not penalized the authors for not comparing against these works.
",,1690033842504,,,EMNLP/2023/Conference,05vb8rwGct,"['EMNLP/2023/Conference/Submission2521/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170579,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2521/Reviewer_CxLY']",05vb8rwGct,['EMNLP/2023/Conference/Submission2521/Reviewer_CxLY'],1690033842504,1701461170579,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2521/Reviewer_CxLY']","Yes

The suggested papers or references not cited in the manuscript are:
1. Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., & Han, J. (2022). Large language models can self-improve. arXiv preprint arXiv:2210.11610.
2. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.
3. Wan, X., Sun, R., Nakhost, H., Dai, H., Eisenschlos, J. M., Arik, S. O., & Pfister, T. (2023). Universal Self-adaptive Prompting. arXiv preprint arXiv:2305.14926.
4. Zhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting in large language models. ICLR.

These citations might be necessary because the reviewer believes that the authors' proposed method of selecting in-context learning examples based on low output entropy and calibrating output logits has similarities to existing works. The reviewer suggests that the authors discuss the relation of their work to these contemporaneous and prior studies to provide a more comprehensive understanding of the research landscape and to distinguish their contributions from existing ideas. Additionally, the reviewer recommends comparing the authors' approach to other methods for ICL selection, such as nearest neighbor retrieval and diversity-based selection, which are discussed in the suggested papers.",1,"2022, 2022, 2023, 2023, 2023, 2023, 2023",Universal Self-adaptive Prompting
AfzTBT3ugY,"This paper starts by making the observation that randomly selecting in-context examples leads to high variance in performance, similar to previous observations that shuffling example order leads to performance fluctuation. The authors then propose to select in-context learning examples from unlabeled data with low output entropy (or equivalently high information gain) and use the LLM predictions on these data as in-context examples. The authors further make the observation that the template sometimes induces bias, and they propose calibrating output logits with the CC technique from Zhao et al, 2021 before computing the entropy. They then evaluate several classification tasks on various GPT-style LLMs to show the promise of the proposed method.","- ICL has become one of the dominant learning paradigms for NLP, and this paper thus addresses an important problem.
- The methodology design is convincing. Both components of the suggested approach (calibration and ICL selection) are reasonable and intuitive, and the method can be applied in a black-box setting 1) without the need to access model parameters except for the output logits, 2) is capable of working in a zero-shot setup without accessing labels. These are other pluses of the proposed method.
- Experimental validation is largely thorough, and it is nice that the authors have also added several analyses and experiments combining their methods with existing methods to show further enhancements. I have some concerns, though, as listed in the section below.","1. The paper makes three main contributions, namely 1) that ICL is sensitive to random demo selection, 2) ICL selection based on entropy and 3) calibration before sampling. On Point 1, I feel that this is largely expected in light of existing findings that models can be sensitive to even the orders of the examples (which the authors cite) -- I'd tend to think that using *different demos* is a larger source of uncertainty compared to *demos of different order*, so it's unsurprising that randomly sampling demos lead to large variance.


   On 2) and 3), while combining ICL selection and calibration may be new, each of the components is not, and similar ideas have been previously explored. For the first component, the authors are essentially proposing to leverage the low-entropy / high-confidence predictions from the models, and similar ideas have been explored in a number of previous works like [1], where the authors fine-tune the LLMs on the confident outputs based on self-consistency to self-improve. [2] is more directly related, where the authors use confident outputs as in-context examples. [3] is a concurrent work, and the use of USP in the CLS case is almost identical to the proposed method in this paper, but USP also handles additional generative tasks and can work even without accessing the logits in some cases. On the calibration part, the authors essentially lift the CC technique for selection -- while the authors try to distinguish their usage from the one proposed in Zhao et al., 2021, I feel that CC is still essentially used in the same way except that the outputs are now used to compute entropy rather than generating outputs directly.  

   Note that I recognize that [2] and [3] are considered contemporaneous to this work, and I have *not* penalized the authors for not discussing them. However, I'd still encourage the authors to discuss the relation with respect to these works when they have a chance to revise their paper.

2. In terms of experiments, I have some suggestions for further improvement: 

- In addition to random sampling, there are other suggested approaches for ICL selection, including but not limited to nearest neighbour retrieval (Liu et al., 2021 -- cited in the paper) and diversity based on clustering [4]. I think these are nice to have, even though Liu et al., 2021 and [4] work in slightly different setups ( Liu et al., 2021 assume the availability of golden labels, and [4] is originally proposed for chain-of-thought tasks, like [2], but I think it's easy to adapt the idea of [4] into general tasks) -- note that given the difference in setup, I do not deem the lack of comparison to these works as major weaknesses against the authors. 
- It might be beneficial to consider a wider range of tasks outside general classification -- there are ample examples in the GPT-3 paper, for example, where the authors consider broader NLI and commonsense reasoning tasks.",Please address my concerns in *Reasons to Reject*.,4: Strong: This study provides sufficient support for all of its claims/arguments. ,"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.","4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"[1] Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., & Han, J. (2022). Large language models can self-improve. arXiv preprint arXiv:2210.11610.

[2] Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.

[3] Wan, X., Sun, R., Nakhost, H., Dai, H., Eisenschlos, J. M., Arik, S. O., & Pfister, T. (2023). Universal Self-adaptive Prompting. arXiv preprint arXiv:2305.14926.

[4] Zhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting in large language models. ICLR.

I acknowledge that [2] and [3] are contemporaneous and I have not penalized the authors for not comparing against these works.
",,1690033842504,,,EMNLP/2023/Conference,05vb8rwGct,"['EMNLP/2023/Conference/Submission2521/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170579,[],1,,,"['everyone', 'EMNLP/2023/Conference/Submission2521/Reviewer_CxLY']",05vb8rwGct,['EMNLP/2023/Conference/Submission2521/Reviewer_CxLY'],1690033842504,1701461170579,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2521/Reviewer_CxLY']","Yes

The suggested papers or references not cited in the manuscript are:
1. Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., & Han, J. (2022). Large language models can self-improve. arXiv preprint arXiv:2210.11610.
2. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. 2023. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada. Association for Computational Linguistics.
3. Wan, X., Sun, R., Nakhost, H., Dai, H., Eisenschlos, J. M., Arik, S. O., & Pfister, T. (2023). Universal Self-adaptive Prompting. arXiv preprint arXiv:2305.14926.
4. Zhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting in large language models. ICLR.

These citations might be necessary because the reviewer believes that the authors' proposed method of selecting in-context learning examples based on low output entropy and calibrating output logits has similarities to existing works. The reviewer suggests that the authors discuss the relation of their work to these contemporaneous and prior studies to provide a more comprehensive understanding of the research landscape and to distinguish their contributions from existing ideas. Additionally, the reviewer recommends comparing the authors' approach to other methods for ICL selection, such as nearest neighbor retrieval and diversity-based selection, which are discussed in the suggested papers.",1,"2022, 2022, 2023, 2023, 2023, 2023, 2023",Automatic chain of thought prompting in large language models
fOSfasQI7r,This study addresses the challenge of demonstration selection in in-context learning. It introduces a novel approach that employs information gain (IG) as a measure to quantify the informativeness of potential examples. The most informative candidates are then selected as demonstration examples within the in-context learning paradigm. Findings indicate that the proposed method typically outperforms baseline approaches in terms of performance.,"1. The proposed method boasts cost-efficiency as a key advantage. The selection of demonstration examples is determined solely by the task and template, circumventing the need for selecting new demonstration examples for each incoming test sample. Additionally, there is no need for a large annotated dataset as annotations are required only for the examples once they have been chosen. 

2. The idea of using information gain to measure the value of demonstration examples sounds interesting. ","1. The proposed method, as described in the methods section, lacks sufficient detail and clarity. See Questions 1 below for detail.

2. This work appears to be missing a comparative analysis with contemporary methods in the field. Even though the authors have considered the MaxEntropy method, there is an absence of referenced literature that could verify its effectiveness in in-context learning scenarios. Additionally, from an active learning perspective, choosing samples with maximum entropy in low-resource conditions (where only a limited number of samples can be selected) may not be the most optimal approach [1]. The absence of comparisons with effective, proven methods makes it difficult to accurately assess the relative efficiency, effectiveness, and innovation of your proposed method.

3. There is a potential issue in the experimental setup that can cause bias in the evaluation. In one-shot in-context learning, the model output can be influenced by the majority label bias [2], where the majority label bias leads the model to predict training answers that frequently appear in the prompt (i.e., the label of the selected one-shot example). The absence of a countermeasure for this bias, especially given that the majority of experiments in this work are conducted under a one-shot learning scenario, raises questions about the accuracy of the results.

[1] Hacohen, G., Dekel, A., & Weinshall, D. (2022). Active learning on a budget: Opposite strategies suit high and low budgets. arXiv preprint arXiv:2202.02794.
[2] Zhao, Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021, July). Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning (pp. 12697-12706). PMLR.","1. Could you provide some clarification regarding the definition of Y in Sec. 3.2? As per Eq.2, Y appears to represent the prediction distribution for $x_{ob}$, rather than the label distribution in the test dataset. Assuming this is accurate, could you elaborate on how this information gain (i.e., reducing uncertainty on its own prediction given certain LLM) could indicate the informativeness of $x_{ob}$ in enhancing the predictive performance on samples from the testing dataset? If my interpretation is incorrect, could you please explain the correct understanding of this concept?

2. The findings presented in Figure 5 suggest that the proposed method, even without calibration, can yield results comparable to those achieved with calibration. The calibration technique [1] is specifically intended to counteract various biases that may arise during in-context learning in language models. Nevertheless, it seems that no measures were implemented in the proposed method to address these biases. Could you clarify what factors might be contributing to the proposed method's apparent capacity to mitigate these biases?","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691307357157,,,EMNLP/2023/Conference,05vb8rwGct,"['EMNLP/2023/Conference/Submission2521/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170476,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2521/Reviewer_iLKr']",05vb8rwGct,['EMNLP/2023/Conference/Submission2521/Reviewer_iLKr'],1691307357157,1701461170476,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2521/Reviewer_iLKr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Hacohen, G., Dekel, A., & Weinshall, D. (2022). Active learning on a budget: Opposite strategies suit high and low budgets. arXiv preprint arXiv:2202.02794.
2. Zhao, Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021, July). Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning (pp. 12697-12706). PMLR.

These citations might be necessary because the reviewer points out that the manuscript lacks a comparative analysis with contemporary methods in the field. The reviewer specifically mentions that the proposed method's effectiveness in in-context learning scenarios is not verified through referenced literature. The suggested papers seem to provide relevant information on active learning and calibration techniques, which could help to assess the relative efficiency, effectiveness, and innovation of the proposed method. Additionally, the reviewer questions the experimental setup and the potential issue of majority label bias, which is addressed in one of the suggested papers (Zhao et al., 2021).",1,"2022, 2022, 2021, 2021",Active learning on a budget Opposite strategies suit high and low budgets 
fOSfasQI7r,This study addresses the challenge of demonstration selection in in-context learning. It introduces a novel approach that employs information gain (IG) as a measure to quantify the informativeness of potential examples. The most informative candidates are then selected as demonstration examples within the in-context learning paradigm. Findings indicate that the proposed method typically outperforms baseline approaches in terms of performance.,"1. The proposed method boasts cost-efficiency as a key advantage. The selection of demonstration examples is determined solely by the task and template, circumventing the need for selecting new demonstration examples for each incoming test sample. Additionally, there is no need for a large annotated dataset as annotations are required only for the examples once they have been chosen. 

2. The idea of using information gain to measure the value of demonstration examples sounds interesting. ","1. The proposed method, as described in the methods section, lacks sufficient detail and clarity. See Questions 1 below for detail.

2. This work appears to be missing a comparative analysis with contemporary methods in the field. Even though the authors have considered the MaxEntropy method, there is an absence of referenced literature that could verify its effectiveness in in-context learning scenarios. Additionally, from an active learning perspective, choosing samples with maximum entropy in low-resource conditions (where only a limited number of samples can be selected) may not be the most optimal approach [1]. The absence of comparisons with effective, proven methods makes it difficult to accurately assess the relative efficiency, effectiveness, and innovation of your proposed method.

3. There is a potential issue in the experimental setup that can cause bias in the evaluation. In one-shot in-context learning, the model output can be influenced by the majority label bias [2], where the majority label bias leads the model to predict training answers that frequently appear in the prompt (i.e., the label of the selected one-shot example). The absence of a countermeasure for this bias, especially given that the majority of experiments in this work are conducted under a one-shot learning scenario, raises questions about the accuracy of the results.

[1] Hacohen, G., Dekel, A., & Weinshall, D. (2022). Active learning on a budget: Opposite strategies suit high and low budgets. arXiv preprint arXiv:2202.02794.
[2] Zhao, Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021, July). Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning (pp. 12697-12706). PMLR.","1. Could you provide some clarification regarding the definition of Y in Sec. 3.2? As per Eq.2, Y appears to represent the prediction distribution for $x_{ob}$, rather than the label distribution in the test dataset. Assuming this is accurate, could you elaborate on how this information gain (i.e., reducing uncertainty on its own prediction given certain LLM) could indicate the informativeness of $x_{ob}$ in enhancing the predictive performance on samples from the testing dataset? If my interpretation is incorrect, could you please explain the correct understanding of this concept?

2. The findings presented in Figure 5 suggest that the proposed method, even without calibration, can yield results comparable to those achieved with calibration. The calibration technique [1] is specifically intended to counteract various biases that may arise during in-context learning in language models. Nevertheless, it seems that no measures were implemented in the proposed method to address these biases. Could you clarify what factors might be contributing to the proposed method's apparent capacity to mitigate these biases?","2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems","2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.,No,"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,,1691307357157,,,EMNLP/2023/Conference,05vb8rwGct,"['EMNLP/2023/Conference/Submission2521/-/Official_Review', 'EMNLP/2023/Conference/-/Edit']",CC BY 4.0,1701461170476,[],2,,,"['everyone', 'EMNLP/2023/Conference/Submission2521/Reviewer_iLKr']",05vb8rwGct,['EMNLP/2023/Conference/Submission2521/Reviewer_iLKr'],1691307357157,1701461170476,"['EMNLP/2023/Conference', 'EMNLP/2023/Conference/Submission2521/Reviewer_iLKr']","Yes

The suggested papers or references not cited in the manuscript are:
1. Hacohen, G., Dekel, A., & Weinshall, D. (2022). Active learning on a budget: Opposite strategies suit high and low budgets. arXiv preprint arXiv:2202.02794.
2. Zhao, Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021, July). Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning (pp. 12697-12706). PMLR.

These citations might be necessary because the reviewer points out that the manuscript lacks a comparative analysis with contemporary methods in the field. The reviewer specifically mentions that the proposed method's effectiveness in in-context learning scenarios is not verified through referenced literature. The suggested papers seem to provide relevant information on active learning and calibration techniques, which could help to assess the relative efficiency, effectiveness, and innovation of the proposed method. Additionally, the reviewer questions the experimental setup and the potential issue of majority label bias, which is addressed in one of the suggested papers (Zhao et al., 2021).",1,"2022, 2022, 2021, 2021",Calibrate before use Improving few-shot performance of language models
