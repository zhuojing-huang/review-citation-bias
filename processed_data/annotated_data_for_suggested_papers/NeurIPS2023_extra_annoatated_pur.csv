id,summary,strengths,weaknesses,questions,limitations,flag_for_ethics_review,rating,confidence,Reviewer_Confidence,code_of_conduct,contribution,presentation,soundness,cdate,ddate,details,domain,forum,invitations,license,mdate,nonreaders,number,odate,pdate,readers,replyto,signatures,tcdate,tmdate,writers,response,binary_label,suggested_years,paper_info
2cOLIshRgd,The authors consider tackling the optimization fo finite sum of piecewise linear quadratic functions (PLQ) arising from e.g. robust empirical risk minimization by means of a reformulation of PLQ functions as sums of ReLU and smoothed ReLU functions combined with a stochastic dual ascent algorithm. The authors start by presenting how any PLQ function can be formulated as sums of of ReLU and smoothed ReLU functions with a table summarizing some common ones. Then the proposed algorithm is presented whose convergence is ensured by previous work. Experimental results advocate for the superiority of the approach compared to old all-purpose solvers in various classification tasks. ,"- The generality of the approach is well defended by the flexibility of the parameterization with ReLU and smoothed ReLU functions.
- The proposed algorithm captures efficiently the underlying structure of the algorithm. In particular, compared to primal-based approaches (which could easily be implemented using the proximal operators of the losses), the approach can handle constraints. 
- The proposed algorithm is hyper-parameter free. 
- The experiments could be a strength, although they are simply misleading in their current state.","- The paper lacks some important related work. In particular, the proposed algorithm appears to simply be a form of stochastic coordinate dual ascent. Such a note would help gain perspective on the contributions of the authors.
- A 
- Numerous algorithms have been developed to tackle empirical risk minimization problems. For example, s-SVM or SVM^2 losses are smooth and could be tackled by fast incremental solvers such as SAGA, SVRG or SDCA or even their accelerated versions such as [1]. see also [2]. Non-smooth losses can also generally easily be smoothed, see e.g. [3], to be amenable theoretically and in practice to fast resolution by fast incremental algorithms. True, most of these algorithms require stepsizes, which make the proposed approach friendlier at first. However, SDCA for example does not require any and could a priori be formulated like the proposed algorithm (using that the proximal operator of the conjugate of the losses is available here). A thorough discussion and comparison to all of the aforementioned algorithms is necessary to held this paper to the claims the authors make. Additional 
- Non-negativity, box and monotonicity constraints can be handled by simple projections and primal algorithms could also be considered in those cases.
- The current approach cannot handle non-strongly convex penalty such as sparsity inducing penalties. 
- The experimental results and claims are misleading: no one uses all-purpose solvers such as MOSEK. In particular no one uses interior point methods for empirical risk minimization. Claiming 1000 times improvement is overselling the method which in turns is detrimental to the contributions of the paper. Usual comparisons consist in claiming improvements over state of the art method, which appear to hold but at much smaller scale. The proposed algorithm may provide quantitative benefits, but a comprehensive comparison would help the paper.

Minor details:
- E appears not have been defined in Theorem 2. Similarly recalling what a mode-3 unfolding of a tensor is, would improve the readability of the paper. 

[1] A. Defazio. A Simple Practical Accelerated Method for Finite Sums. NIPS 2016
[2] S Shalev-Shwartz, T Zhang. Accelerated Mini-Batch Stochastic Dual Coordinate Ascent. NIPS 2013
[3] A Beck. Smoothing and First Order Methods: A Unified Framework. SIOPT 2012","- Can the authors do comparisons with state of the art algorithms for empirical risk minimization for each of the proposed problems rather than using all-purpose algorithms? A list of potential candidates has been given in the weakness section. 
- In particular, can the authors discuss and compare empirically their algorithm against SDCA in the absence of constraints?
- How can the proposed algorithms handle nonlinear models such as kernel methods?","- No support for sparsity inducing norms
",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1690282503897,,,NeurIPS.cc/2023/Conference,3pEBW2UPAD,"['NeurIPS.cc/2023/Conference/Submission11891/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411352072,[],5,,,['everyone'],3pEBW2UPAD,['NeurIPS.cc/2023/Conference/Submission11891/Reviewer_kbfD'],1690282503897,1702411352072,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission11891/Reviewer_kbfD']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. A. Defazio. A Simple Practical Accelerated Method for Finite Sums. NIPS 2016
2. S Shalev-Shwartz, T Zhang. Accelerated Mini-Batch Stochastic Dual Coordinate Ascent. NIPS 2013
3. A Beck. Smoothing and First Order Methods: A Unified Framework. SIOPT 2012

These citations are mentioned in the weaknesses section of the review, where the reviewer points out that numerous algorithms have been developed to tackle empirical risk minimization problems and suggests that the authors should discuss and compare their algorithm with these existing methods.",1,"2016, 2013, 2012",A Simple Practical Accelerated Method for Finite Sums 
2cOLIshRgd,The authors consider tackling the optimization fo finite sum of piecewise linear quadratic functions (PLQ) arising from e.g. robust empirical risk minimization by means of a reformulation of PLQ functions as sums of ReLU and smoothed ReLU functions combined with a stochastic dual ascent algorithm. The authors start by presenting how any PLQ function can be formulated as sums of of ReLU and smoothed ReLU functions with a table summarizing some common ones. Then the proposed algorithm is presented whose convergence is ensured by previous work. Experimental results advocate for the superiority of the approach compared to old all-purpose solvers in various classification tasks. ,"- The generality of the approach is well defended by the flexibility of the parameterization with ReLU and smoothed ReLU functions.
- The proposed algorithm captures efficiently the underlying structure of the algorithm. In particular, compared to primal-based approaches (which could easily be implemented using the proximal operators of the losses), the approach can handle constraints. 
- The proposed algorithm is hyper-parameter free. 
- The experiments could be a strength, although they are simply misleading in their current state.","- The paper lacks some important related work. In particular, the proposed algorithm appears to simply be a form of stochastic coordinate dual ascent. Such a note would help gain perspective on the contributions of the authors.
- A 
- Numerous algorithms have been developed to tackle empirical risk minimization problems. For example, s-SVM or SVM^2 losses are smooth and could be tackled by fast incremental solvers such as SAGA, SVRG or SDCA or even their accelerated versions such as [1]. see also [2]. Non-smooth losses can also generally easily be smoothed, see e.g. [3], to be amenable theoretically and in practice to fast resolution by fast incremental algorithms. True, most of these algorithms require stepsizes, which make the proposed approach friendlier at first. However, SDCA for example does not require any and could a priori be formulated like the proposed algorithm (using that the proximal operator of the conjugate of the losses is available here). A thorough discussion and comparison to all of the aforementioned algorithms is necessary to held this paper to the claims the authors make. Additional 
- Non-negativity, box and monotonicity constraints can be handled by simple projections and primal algorithms could also be considered in those cases.
- The current approach cannot handle non-strongly convex penalty such as sparsity inducing penalties. 
- The experimental results and claims are misleading: no one uses all-purpose solvers such as MOSEK. In particular no one uses interior point methods for empirical risk minimization. Claiming 1000 times improvement is overselling the method which in turns is detrimental to the contributions of the paper. Usual comparisons consist in claiming improvements over state of the art method, which appear to hold but at much smaller scale. The proposed algorithm may provide quantitative benefits, but a comprehensive comparison would help the paper.

Minor details:
- E appears not have been defined in Theorem 2. Similarly recalling what a mode-3 unfolding of a tensor is, would improve the readability of the paper. 

[1] A. Defazio. A Simple Practical Accelerated Method for Finite Sums. NIPS 2016
[2] S Shalev-Shwartz, T Zhang. Accelerated Mini-Batch Stochastic Dual Coordinate Ascent. NIPS 2013
[3] A Beck. Smoothing and First Order Methods: A Unified Framework. SIOPT 2012","- Can the authors do comparisons with state of the art algorithms for empirical risk minimization for each of the proposed problems rather than using all-purpose algorithms? A list of potential candidates has been given in the weakness section. 
- In particular, can the authors discuss and compare empirically their algorithm against SDCA in the absence of constraints?
- How can the proposed algorithms handle nonlinear models such as kernel methods?","- No support for sparsity inducing norms
",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1690282503897,,,NeurIPS.cc/2023/Conference,3pEBW2UPAD,"['NeurIPS.cc/2023/Conference/Submission11891/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411352072,[],5,,,['everyone'],3pEBW2UPAD,['NeurIPS.cc/2023/Conference/Submission11891/Reviewer_kbfD'],1690282503897,1702411352072,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission11891/Reviewer_kbfD']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. A. Defazio. A Simple Practical Accelerated Method for Finite Sums. NIPS 2016
2. S Shalev-Shwartz, T Zhang. Accelerated Mini-Batch Stochastic Dual Coordinate Ascent. NIPS 2013
3. A Beck. Smoothing and First Order Methods: A Unified Framework. SIOPT 2012

These citations are mentioned in the weaknesses section of the review, where the reviewer points out that numerous algorithms have been developed to tackle empirical risk minimization problems and suggests that the authors should discuss and compare their algorithm with these existing methods.",1,"2016, 2013, 2012",Accelerated Mini-Batch Stochastic Dual Coordinate Ascent 
2cOLIshRgd,The authors consider tackling the optimization fo finite sum of piecewise linear quadratic functions (PLQ) arising from e.g. robust empirical risk minimization by means of a reformulation of PLQ functions as sums of ReLU and smoothed ReLU functions combined with a stochastic dual ascent algorithm. The authors start by presenting how any PLQ function can be formulated as sums of of ReLU and smoothed ReLU functions with a table summarizing some common ones. Then the proposed algorithm is presented whose convergence is ensured by previous work. Experimental results advocate for the superiority of the approach compared to old all-purpose solvers in various classification tasks. ,"- The generality of the approach is well defended by the flexibility of the parameterization with ReLU and smoothed ReLU functions.
- The proposed algorithm captures efficiently the underlying structure of the algorithm. In particular, compared to primal-based approaches (which could easily be implemented using the proximal operators of the losses), the approach can handle constraints. 
- The proposed algorithm is hyper-parameter free. 
- The experiments could be a strength, although they are simply misleading in their current state.","- The paper lacks some important related work. In particular, the proposed algorithm appears to simply be a form of stochastic coordinate dual ascent. Such a note would help gain perspective on the contributions of the authors.
- A 
- Numerous algorithms have been developed to tackle empirical risk minimization problems. For example, s-SVM or SVM^2 losses are smooth and could be tackled by fast incremental solvers such as SAGA, SVRG or SDCA or even their accelerated versions such as [1]. see also [2]. Non-smooth losses can also generally easily be smoothed, see e.g. [3], to be amenable theoretically and in practice to fast resolution by fast incremental algorithms. True, most of these algorithms require stepsizes, which make the proposed approach friendlier at first. However, SDCA for example does not require any and could a priori be formulated like the proposed algorithm (using that the proximal operator of the conjugate of the losses is available here). A thorough discussion and comparison to all of the aforementioned algorithms is necessary to held this paper to the claims the authors make. Additional 
- Non-negativity, box and monotonicity constraints can be handled by simple projections and primal algorithms could also be considered in those cases.
- The current approach cannot handle non-strongly convex penalty such as sparsity inducing penalties. 
- The experimental results and claims are misleading: no one uses all-purpose solvers such as MOSEK. In particular no one uses interior point methods for empirical risk minimization. Claiming 1000 times improvement is overselling the method which in turns is detrimental to the contributions of the paper. Usual comparisons consist in claiming improvements over state of the art method, which appear to hold but at much smaller scale. The proposed algorithm may provide quantitative benefits, but a comprehensive comparison would help the paper.

Minor details:
- E appears not have been defined in Theorem 2. Similarly recalling what a mode-3 unfolding of a tensor is, would improve the readability of the paper. 

[1] A. Defazio. A Simple Practical Accelerated Method for Finite Sums. NIPS 2016
[2] S Shalev-Shwartz, T Zhang. Accelerated Mini-Batch Stochastic Dual Coordinate Ascent. NIPS 2013
[3] A Beck. Smoothing and First Order Methods: A Unified Framework. SIOPT 2012","- Can the authors do comparisons with state of the art algorithms for empirical risk minimization for each of the proposed problems rather than using all-purpose algorithms? A list of potential candidates has been given in the weakness section. 
- In particular, can the authors discuss and compare empirically their algorithm against SDCA in the absence of constraints?
- How can the proposed algorithms handle nonlinear models such as kernel methods?","- No support for sparsity inducing norms
",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1690282503897,,,NeurIPS.cc/2023/Conference,3pEBW2UPAD,"['NeurIPS.cc/2023/Conference/Submission11891/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411352072,[],5,,,['everyone'],3pEBW2UPAD,['NeurIPS.cc/2023/Conference/Submission11891/Reviewer_kbfD'],1690282503897,1702411352072,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission11891/Reviewer_kbfD']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. A. Defazio. A Simple Practical Accelerated Method for Finite Sums. NIPS 2016
2. S Shalev-Shwartz, T Zhang. Accelerated Mini-Batch Stochastic Dual Coordinate Ascent. NIPS 2013
3. A Beck. Smoothing and First Order Methods: A Unified Framework. SIOPT 2012

These citations are mentioned in the weaknesses section of the review, where the reviewer points out that numerous algorithms have been developed to tackle empirical risk minimization problems and suggests that the authors should discuss and compare their algorithm with these existing methods.",1,"2016, 2013, 2012",Smoothing and First Order Methods A Unified Framework
Kd1L5z4p1q,"This paper proposes a new task of unsupervised disentanglement of diffusion probabilistic models (DPMs) and presents an approach named DisDiff to achieve disentangled representation learning in the framework of DPMs. The authors connect disentangled representation learning to DPMs to take advantage of the remarkable modeling ability of DPMs. DisDiff learns a disentangled representation of the input image in the diffusion process and for each factor, DisDiff learns a disentangled gradient field, which brings new properties for disentanglement literature. The proposed method is evaluated on several real-world datasets, and the results show that DisDiff outperforms state-of-the-art methods in terms of disentanglement quality and interpretability. The paper concludes by discussing potential future directions for applying DisDiff to more general conditioned DPMs and pre-trained conditional DPMs.","Strengths:
- Performance: The proposed method is evaluated on several real-world datasets, and the results show that DisDiff outperforms state-of-the-art methods in terms of disentanglement quality and interpretability.

- Clarity: The writing is good overall and the proposed idea in this paper is easy to follow. The authors provide good explanations of the method and the experiments.

- Significance: The proposed method has potential applications in various fields, such as image editing, controllable generation, etc.

- References: The paper provides comprehensive references.","Weaknesses:
- Overclaim: this paper claims that they are the very first work introducing disentanglement tasks for diffusion probabilistic models (DPM). However, a very recent work [1] in ICML 2023 has introduced and studied this problem. I understand that there might be some timeline issue that makes the authors not aware of this work. But it would be better if the authors could properly cite and discuss this work and revise their claims in the corresponding paragraphs.
- Abuse of notations: there are some wrong notations (might be typos) and abuse of notations, especially in Sec. 4.3. For example, in line 179, it should be $E_\phi^k(\hat{x}_0^c)$ and $E_\phi^k(\hat{x}_0)$. In lines 180 - 181, the conditioned representation should be $E_\phi^c(\hat{x}_0^c)$ and the claim should be that the unconditioned one is closer to $E_\phi^c(\hat{x}_0)$ than the conditioned one. I suggest the authors carefully examine their statements instead of letting the reviewers guess the meaning.
- More baselines: I appreciate that the authors include some of the most important baselines of VAEs and GANs along the line of disentanglement research. However, some more recent diffusion baselines could also be included, e.g., PADE [2] and DiffAE [3] (I understand [1] would be too new to have the open-source codebase to evaluate).
- More metrics for CelebA: there is a tailored quantitative metric [4] for evaluating disentanglement performance on CelebA. It would be better to have the numbers in the tables.
- In line 176, fulfil -> fulfill

[1] Wang, Yingheng, et al. ""InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models."" arXiv preprint arXiv:2306.08757 (2023).

[2] Zhang, Zijian, Zhou Zhao, and Zhijie Lin. ""Unsupervised representation learning from pre-trained diffusion probabilistic models."" Advances in Neural Information Processing Systems 35 (2022): 22117-22130.

[3] Preechakul, Konpat, et al. ""Diffusion autoencoders: Toward a meaningful and decodable representation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

[4] Yeats, Eric, et al. ""Nashae: Disentangling representations through adversarial covariance minimization."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.","- Distortion in generated images: although the quantitative metrics (e.g. DCI) are pretty promising, the qualitative results on CelebA still seem a bit distorted with disentangled latent variables. Would there be some training tricks that can be applied to alleviate this issue? Are there any other explanations?
- In line 159, the authors claim that Eq. (7) is derived from Eq. (6) by using Tweedie's formula, however, in my opinion, this is just a straightforward application of reparameterization trick. Can the authors provide the full derivation of the methodology part?",There is a section of discussion for limitations.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,2 fair,3 good,1689222143022,,,NeurIPS.cc/2023/Conference,3ofe0lpwQP,"['NeurIPS.cc/2023/Conference/Submission860/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410760499,[],3,,,['everyone'],3ofe0lpwQP,['NeurIPS.cc/2023/Conference/Submission860/Reviewer_6mFa'],1689222143022,1702410760499,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission860/Reviewer_6mFa']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, specifically:

1. Wang, Yingheng, et al. ""InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models."" arXiv preprint arXiv:2306.08757 (2023) to address the overclaim issue.
2. PADE [2] and DiffAE [3] as additional baselines for comparison.
3. Yeats, Eric, et al. ""Nashae: Disentangling representations through adversarial covariance minimization."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022, for evaluating disentanglement performance on CelebA using a tailored quantitative metric.",1,"2023, 2023, 2022",InfoDiffusion Representation Learning Using Information Maximizing Diffusion Models
Kd1L5z4p1q,"This paper proposes a new task of unsupervised disentanglement of diffusion probabilistic models (DPMs) and presents an approach named DisDiff to achieve disentangled representation learning in the framework of DPMs. The authors connect disentangled representation learning to DPMs to take advantage of the remarkable modeling ability of DPMs. DisDiff learns a disentangled representation of the input image in the diffusion process and for each factor, DisDiff learns a disentangled gradient field, which brings new properties for disentanglement literature. The proposed method is evaluated on several real-world datasets, and the results show that DisDiff outperforms state-of-the-art methods in terms of disentanglement quality and interpretability. The paper concludes by discussing potential future directions for applying DisDiff to more general conditioned DPMs and pre-trained conditional DPMs.","Strengths:
- Performance: The proposed method is evaluated on several real-world datasets, and the results show that DisDiff outperforms state-of-the-art methods in terms of disentanglement quality and interpretability.

- Clarity: The writing is good overall and the proposed idea in this paper is easy to follow. The authors provide good explanations of the method and the experiments.

- Significance: The proposed method has potential applications in various fields, such as image editing, controllable generation, etc.

- References: The paper provides comprehensive references.","Weaknesses:
- Overclaim: this paper claims that they are the very first work introducing disentanglement tasks for diffusion probabilistic models (DPM). However, a very recent work [1] in ICML 2023 has introduced and studied this problem. I understand that there might be some timeline issue that makes the authors not aware of this work. But it would be better if the authors could properly cite and discuss this work and revise their claims in the corresponding paragraphs.
- Abuse of notations: there are some wrong notations (might be typos) and abuse of notations, especially in Sec. 4.3. For example, in line 179, it should be $E_\phi^k(\hat{x}_0^c)$ and $E_\phi^k(\hat{x}_0)$. In lines 180 - 181, the conditioned representation should be $E_\phi^c(\hat{x}_0^c)$ and the claim should be that the unconditioned one is closer to $E_\phi^c(\hat{x}_0)$ than the conditioned one. I suggest the authors carefully examine their statements instead of letting the reviewers guess the meaning.
- More baselines: I appreciate that the authors include some of the most important baselines of VAEs and GANs along the line of disentanglement research. However, some more recent diffusion baselines could also be included, e.g., PADE [2] and DiffAE [3] (I understand [1] would be too new to have the open-source codebase to evaluate).
- More metrics for CelebA: there is a tailored quantitative metric [4] for evaluating disentanglement performance on CelebA. It would be better to have the numbers in the tables.
- In line 176, fulfil -> fulfill

[1] Wang, Yingheng, et al. ""InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models."" arXiv preprint arXiv:2306.08757 (2023).

[2] Zhang, Zijian, Zhou Zhao, and Zhijie Lin. ""Unsupervised representation learning from pre-trained diffusion probabilistic models."" Advances in Neural Information Processing Systems 35 (2022): 22117-22130.

[3] Preechakul, Konpat, et al. ""Diffusion autoencoders: Toward a meaningful and decodable representation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

[4] Yeats, Eric, et al. ""Nashae: Disentangling representations through adversarial covariance minimization."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.","- Distortion in generated images: although the quantitative metrics (e.g. DCI) are pretty promising, the qualitative results on CelebA still seem a bit distorted with disentangled latent variables. Would there be some training tricks that can be applied to alleviate this issue? Are there any other explanations?
- In line 159, the authors claim that Eq. (7) is derived from Eq. (6) by using Tweedie's formula, however, in my opinion, this is just a straightforward application of reparameterization trick. Can the authors provide the full derivation of the methodology part?",There is a section of discussion for limitations.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,2 fair,3 good,1689222143022,,,NeurIPS.cc/2023/Conference,3ofe0lpwQP,"['NeurIPS.cc/2023/Conference/Submission860/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410760499,[],3,,,['everyone'],3ofe0lpwQP,['NeurIPS.cc/2023/Conference/Submission860/Reviewer_6mFa'],1689222143022,1702410760499,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission860/Reviewer_6mFa']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, specifically:

1. Wang, Yingheng, et al. ""InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models."" arXiv preprint arXiv:2306.08757 (2023) to address the overclaim issue.
2. PADE [2] and DiffAE [3] as additional baselines for comparison.
3. Yeats, Eric, et al. ""Nashae: Disentangling representations through adversarial covariance minimization."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022, for evaluating disentanglement performance on CelebA using a tailored quantitative metric.",1,"2023, 2023, 2022",Nashae Disentangling representations through adversarial covariance minimization
fpPkCFLBEA,"This paper proposes a new defense against model poisoning attacks in FL. The idea is that the server sends some perturbed aggregation gradient to clients in some selected training rounds, and based on the responses, the server adjusts trust scores for the clients. Experimental results show the effectiveness of the proposed defense against state of the art attacks and outperforms state-of-the-art defenses. ","+ Important and relevant research problem. 

+ Well written paper. 

+ Interesting and novel approach. ","I didn't see severe weaknesses. I think the paper is above the bar. 

I have three suggestions:

1. Since the paper talks about detection. I would suggest also comparing with detection methods, e.g., the following:

FLDetector: Defending federated learning against model poisoning attacks via detecting malicious clients. In KDD, 2022. 

This detection method may also be adapted to assign trust scores for clients. 

2. All the evaluated attacks are for compromised genuine clients. Recent attacks use fake clients, e.g., the following:

Mpaf: Model poisoning attacks to federated learning based on fake clients. In CVPR Workshop, 2022. 

The paper may want to evaluate such attacks. 

3. The paper seems to assume that the server sends aggregated gradient to the clients. This is different from standard FL. The paper can make this more clear. Algorithmically, it is equivalent to send the new global model to clients, but it's good to make this clear. 
",See above. ,See above. ,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688406740179,,,NeurIPS.cc/2023/Conference,3n8PNUdvSg,"['NeurIPS.cc/2023/Conference/Submission7493/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411116760,[],2,,,['everyone'],3n8PNUdvSg,['NeurIPS.cc/2023/Conference/Submission7493/Reviewer_tdiM'],1688406740179,1702411116760,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7493/Reviewer_tdiM']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature:

1. FLDetector: Defending federated learning against model poisoning attacks via detecting malicious clients. In KDD, 2022.
2. Mpaf: Model poisoning attacks to federated learning based on fake clients. In CVPR Workshop, 2022.

The reviewer suggests comparing the proposed defense with the detection method FLDetector and evaluating the defense against the Mpaf attack, which uses fake clients.",1,"2022, 2022",FLDetector Defending federated learning against model poisoning attacks via detecting malicious clients 
fpPkCFLBEA,"This paper proposes a new defense against model poisoning attacks in FL. The idea is that the server sends some perturbed aggregation gradient to clients in some selected training rounds, and based on the responses, the server adjusts trust scores for the clients. Experimental results show the effectiveness of the proposed defense against state of the art attacks and outperforms state-of-the-art defenses. ","+ Important and relevant research problem. 

+ Well written paper. 

+ Interesting and novel approach. ","I didn't see severe weaknesses. I think the paper is above the bar. 

I have three suggestions:

1. Since the paper talks about detection. I would suggest also comparing with detection methods, e.g., the following:

FLDetector: Defending federated learning against model poisoning attacks via detecting malicious clients. In KDD, 2022. 

This detection method may also be adapted to assign trust scores for clients. 

2. All the evaluated attacks are for compromised genuine clients. Recent attacks use fake clients, e.g., the following:

Mpaf: Model poisoning attacks to federated learning based on fake clients. In CVPR Workshop, 2022. 

The paper may want to evaluate such attacks. 

3. The paper seems to assume that the server sends aggregated gradient to the clients. This is different from standard FL. The paper can make this more clear. Algorithmically, it is equivalent to send the new global model to clients, but it's good to make this clear. 
",See above. ,See above. ,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688406740179,,,NeurIPS.cc/2023/Conference,3n8PNUdvSg,"['NeurIPS.cc/2023/Conference/Submission7493/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411116760,[],2,,,['everyone'],3n8PNUdvSg,['NeurIPS.cc/2023/Conference/Submission7493/Reviewer_tdiM'],1688406740179,1702411116760,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7493/Reviewer_tdiM']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature:

1. FLDetector: Defending federated learning against model poisoning attacks via detecting malicious clients. In KDD, 2022.
2. Mpaf: Model poisoning attacks to federated learning based on fake clients. In CVPR Workshop, 2022.

The reviewer suggests comparing the proposed defense with the detection method FLDetector and evaluating the defense against the Mpaf attack, which uses fake clients.",1,"2022, 2022",Mpaf Model poisoning attacks to federated learning based on fake clients
w8eZ3YzJoF,"To defend the untargeted poisoning attack on the federated learning, the authors proposed a new defense called as RECESS, which exploits the outlier detection to analyze the gradients returned from clients. Once the gradient from one client is judged as outlier, this client will be considered as malicious client. The outlier detection is based on the intuition, i.e., for the benign samples even including the outliers, their updated gradients always point to the direction leading to a (local) minimum loss value, while the gradients of poisoned data will point to the reversed direction.","Similar as previous refs [11, 12] as shown in the paper, this work aims to alarm a malicious gradient from a group of gradients updated from clients in the federated learning. The paper is well-written and can be easily understood.","The main weakness of this work is due to its trivial contribution compared with the STOA [11, 12]. The main intuition for outlier detection, as mentioned in ‘Summary’, has been considered. Compared with STOA, the new metric ‘trust scoring’ does not improve too much as shown in Table2.","For the trust score, is it possible for attackers to first pretend as benign client to increase the trust score, and then provide the poisoning gradient? How should the defender limit this kind of attack? Could the attacker perform the adaptive attack to poison the global model?","One limitation of this work is that it only considers the non-target attack, which is less stealthy than target backdoor attack. The main reason is that the non-target attack’s goal is to affect the normal performance of the model, which can be easily recognized by the trainer. I recommend the authors mentioned this limitation in the paper. For more details on the difference between target and non-target attack, please check this survey. 

E. Cinà, K. Grosse, A. Demontis, S. Vascon, W. Zellinger, B. A. Moser, A. Oprea, B. Biggio, M. Pelillo, and F. Roli. Wild patterns reloaded: A survey of machine learning security against training data poisoning. ACM Comp. Surveys, 2023. 

Guo, Wei, Benedetta Tondi, and Mauro Barni. ""An overview of backdoor attacks against deep neural networks and possible defences."" IEEE Open Journal of Signal Processing (2022).
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1690207049079,,,NeurIPS.cc/2023/Conference,3n8PNUdvSg,"['NeurIPS.cc/2023/Conference/Submission7493/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411116594,[],4,,,['everyone'],3n8PNUdvSg,['NeurIPS.cc/2023/Conference/Submission7493/Reviewer_St5y'],1690207049079,1702411116594,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7493/Reviewer_St5y']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. A survey by E. Cinà et al. (2023) titled ""Wild patterns reloaded: A survey of machine learning security against training data poisoning"" published in ACM Comp. Surveys.
2. A paper by Guo, W., Tondi, B., and Barni, M. (2022) titled ""An overview of backdoor attacks against deep neural networks and possible defences"" published in IEEE Open Journal of Signal Processing.

The reviewer recommends citing these references to provide more details on the difference between target and non-target attacks.",1,"2023, 2022",Wild patterns reloaded A survey of machine learning security against training data poisoning 
w8eZ3YzJoF,"To defend the untargeted poisoning attack on the federated learning, the authors proposed a new defense called as RECESS, which exploits the outlier detection to analyze the gradients returned from clients. Once the gradient from one client is judged as outlier, this client will be considered as malicious client. The outlier detection is based on the intuition, i.e., for the benign samples even including the outliers, their updated gradients always point to the direction leading to a (local) minimum loss value, while the gradients of poisoned data will point to the reversed direction.","Similar as previous refs [11, 12] as shown in the paper, this work aims to alarm a malicious gradient from a group of gradients updated from clients in the federated learning. The paper is well-written and can be easily understood.","The main weakness of this work is due to its trivial contribution compared with the STOA [11, 12]. The main intuition for outlier detection, as mentioned in ‘Summary’, has been considered. Compared with STOA, the new metric ‘trust scoring’ does not improve too much as shown in Table2.","For the trust score, is it possible for attackers to first pretend as benign client to increase the trust score, and then provide the poisoning gradient? How should the defender limit this kind of attack? Could the attacker perform the adaptive attack to poison the global model?","One limitation of this work is that it only considers the non-target attack, which is less stealthy than target backdoor attack. The main reason is that the non-target attack’s goal is to affect the normal performance of the model, which can be easily recognized by the trainer. I recommend the authors mentioned this limitation in the paper. For more details on the difference between target and non-target attack, please check this survey. 

E. Cinà, K. Grosse, A. Demontis, S. Vascon, W. Zellinger, B. A. Moser, A. Oprea, B. Biggio, M. Pelillo, and F. Roli. Wild patterns reloaded: A survey of machine learning security against training data poisoning. ACM Comp. Surveys, 2023. 

Guo, Wei, Benedetta Tondi, and Mauro Barni. ""An overview of backdoor attacks against deep neural networks and possible defences."" IEEE Open Journal of Signal Processing (2022).
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1690207049079,,,NeurIPS.cc/2023/Conference,3n8PNUdvSg,"['NeurIPS.cc/2023/Conference/Submission7493/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411116594,[],4,,,['everyone'],3n8PNUdvSg,['NeurIPS.cc/2023/Conference/Submission7493/Reviewer_St5y'],1690207049079,1702411116594,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7493/Reviewer_St5y']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. A survey by E. Cinà et al. (2023) titled ""Wild patterns reloaded: A survey of machine learning security against training data poisoning"" published in ACM Comp. Surveys.
2. A paper by Guo, W., Tondi, B., and Barni, M. (2022) titled ""An overview of backdoor attacks against deep neural networks and possible defences"" published in IEEE Open Journal of Signal Processing.

The reviewer recommends citing these references to provide more details on the difference between target and non-target attacks.",1,"2023, 2022",An overview of backdoor attacks against deep neural networks and possible defences
ShZ93jBn0R,"This work focuses on improving calibration (e.g., measured with Expected Calibration Error) using partitions of the feature space. This contrasts with most well-established recalibration techniques (isotonic regression, histogram binning, temperature scaling...) that solely use the estimated probabilities of the classifier. Partitions are learned during training with an extra linear layer + softmax on the features extracted by the deep model. Each part of the partition is recalibrated by the accuracy-preserving existing calibration techniques such as temperature scaling or ensembling temperature scaling. This ensures that the proposed method is also accuracy-preserving. The method is benchmarked on 3 datasets (CIFAR10, CIFAR100, ImageNet) and 3 networks on each dataset. The method is compared to 9 existing calibration techniques, such as temperature scaling, isotonic regression, and histogram binning.",* Learning the partitions using an extra linear layer + softmax is valuable since it is differentiable and enables end-to-end learning jointly with the network. This contrasts with existing works that use hard partitions by thresholding quantiles on a proximity metric [4-5] or by using decision trees [2-3].,"* Positioning in literature: It misses related works that share strong conceptual links with the proposed work. Those works could have been compared either in related work or in experiments.
  * It is not compared to multicalibration [1], which proposes an algorithm for learning a multicalibrated predictor with respect to any subpopulation class.
  * The idea of partitioning the network feature space to find local miscalibration has been used before [3-4]. [3] shows that strong local miscalibration arises in modern neural networks and proposes an estimator based on feature space partitions to evaluate them. [4] links this local miscalibration to atypicality.
  * [5] also links local miscalibration to proximity and proposes an algorithm to recalibrate those subgroups. NB: [5] was released after the NeurIPS submission deadline. I put it for information.
* Framing:
  * The valuable part of finding local miscalibration and correcting them, as it is done in the proposed work, is not to improve calibration but rather to improve estimated individual posterior probabilities (i.e., reducing the epistemic loss) as pointed out in [3], or improving fairness metrics as done in [1]. The problem is that the proposed work is entirely focused on improving calibration, which is blind to local miscalibration.
  * In addition to the missing related work, the discrepancy between the current framing and the potential of the proposed method can be felt on several levels. In the introduction: ""A perfectly calibrated model should be calibrated across any data space partition"" (L61). This is false since a perfectly calibrated model in the standard definition of calibration (e.g., in [6] eq. (1)) just needs to be calibrated on level sets of the same predicted confidence (i.e., satisfy $\mathbb{P}(Y=\hat{Y}|\hat{\mathbb{P}} = p) = p$). However, a perfect probabilistic classifier, that is, a classifier that outputs the true individual posterior probabilities $\mathbb{P}(Y=\hat{Y}|X)$, should be calibrated on any data space partition. The exposition of the results focuses on improving ECE (Table 1 & 2).
* The improvement in calibration is marginal.
  * For example, in Table 1, the proposed method GC+TS improves on Temperature Scaling (TS) by absolute differences ranging from 1e-4 to 1e-3 in Expected Calibration Error (ECE), which is an extremely small scale for ECE. Similarly, the proposed method GC+ETS improves on Ensembling Temperature Scaling (ETS) by absolute differences ranging from 1e-4 to 1e-3.


In summary, the proposed method has the potential to improve individual posterior probabilities and fairness metrics. Unfortunately, this potential is not exploited in the current framing since it focuses on improving calibration instead. On improving calibration, the proposed method has a too marginal effect.

## References

[1] Hebert-Johnson, U., Kim, M., Reingold, O., & Rothblum, G. (2018). Multicalibration: Calibration for the (Computationally-Identifiable) Masses. In Proceedings of the 35th International Conference on Machine Learning (pp. 1939–1948). PMLR.

[2] David Durfee, Aman Gupta, & Kinjal Basu. (2022). Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization.

[3] Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux. (2023). Beyond calibration: estimating the grouping loss of modern neural networks. ICLR.  
(First released on 8 Oct 2022).

[4] Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin. (2023). Beyond Confidence: Reliable Models Should Also Consider Atypicality.  
(First released on 04 Mar 2023, https://openreview.net/forum?id=nPOKJCCvlLF)

[5] Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi. (2023). Proximity-Informed Calibration for Deep Neural Networks.  
(First released on 7 Jun 2023).

[6] Chuan Guo, Geoff Pleiss, Yu Sun, & Kilian Q. Weinberger. (2017). On Calibration of Modern Neural Networks.","* Could you confirm that ECE is the metric plotted in Tables 1 & 2? It is not indicated.
* Could you develop on the statistical test used for assessing the significance of the results? What are the p-values obtained with the test? Do you correct for multiple comparisons?","The authors highlight the following limitations:
* The method is restricted to deep networks since it works in the extracted feature space. It thus cannot be applied to tree models, for example.
* Increasing the number of partitions and using more complex grouping models increased the computational complexity of the method.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,1 poor,1688301190589,,,NeurIPS.cc/2023/Conference,3kitbpEZZO,"['NeurIPS.cc/2023/Conference/Submission2840/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410866712,[],2,,,['everyone'],3kitbpEZZO,['NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v'],1688301190589,1702410866712,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Hebert-Johnson et al. (2018) - ""Multicalibration: Calibration for the (Computationally-Identifiable) Masses""
2. David Durfee, Aman Gupta, & Kinjal Basu (2022) - ""Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization""
3. Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux (2023) - ""Beyond calibration: estimating the grouping loss of modern neural networks""
4. Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin (2023) - ""Beyond Confidence: Reliable Models Should Also Consider Atypicality""
5. Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi (2023) - ""Proximity-Informed Calibration for Deep Neural Networks""

These citations are suggested to improve the positioning of the paper in the literature, as the reviewer believes that the proposed work has strong conceptual links with these existing works.",1,"2018, 2022, 2023, 2023, 2023",Multicalibration Calibration for the Computationally-Identifiable Masses
ShZ93jBn0R,"This work focuses on improving calibration (e.g., measured with Expected Calibration Error) using partitions of the feature space. This contrasts with most well-established recalibration techniques (isotonic regression, histogram binning, temperature scaling...) that solely use the estimated probabilities of the classifier. Partitions are learned during training with an extra linear layer + softmax on the features extracted by the deep model. Each part of the partition is recalibrated by the accuracy-preserving existing calibration techniques such as temperature scaling or ensembling temperature scaling. This ensures that the proposed method is also accuracy-preserving. The method is benchmarked on 3 datasets (CIFAR10, CIFAR100, ImageNet) and 3 networks on each dataset. The method is compared to 9 existing calibration techniques, such as temperature scaling, isotonic regression, and histogram binning.",* Learning the partitions using an extra linear layer + softmax is valuable since it is differentiable and enables end-to-end learning jointly with the network. This contrasts with existing works that use hard partitions by thresholding quantiles on a proximity metric [4-5] or by using decision trees [2-3].,"* Positioning in literature: It misses related works that share strong conceptual links with the proposed work. Those works could have been compared either in related work or in experiments.
  * It is not compared to multicalibration [1], which proposes an algorithm for learning a multicalibrated predictor with respect to any subpopulation class.
  * The idea of partitioning the network feature space to find local miscalibration has been used before [3-4]. [3] shows that strong local miscalibration arises in modern neural networks and proposes an estimator based on feature space partitions to evaluate them. [4] links this local miscalibration to atypicality.
  * [5] also links local miscalibration to proximity and proposes an algorithm to recalibrate those subgroups. NB: [5] was released after the NeurIPS submission deadline. I put it for information.
* Framing:
  * The valuable part of finding local miscalibration and correcting them, as it is done in the proposed work, is not to improve calibration but rather to improve estimated individual posterior probabilities (i.e., reducing the epistemic loss) as pointed out in [3], or improving fairness metrics as done in [1]. The problem is that the proposed work is entirely focused on improving calibration, which is blind to local miscalibration.
  * In addition to the missing related work, the discrepancy between the current framing and the potential of the proposed method can be felt on several levels. In the introduction: ""A perfectly calibrated model should be calibrated across any data space partition"" (L61). This is false since a perfectly calibrated model in the standard definition of calibration (e.g., in [6] eq. (1)) just needs to be calibrated on level sets of the same predicted confidence (i.e., satisfy $\mathbb{P}(Y=\hat{Y}|\hat{\mathbb{P}} = p) = p$). However, a perfect probabilistic classifier, that is, a classifier that outputs the true individual posterior probabilities $\mathbb{P}(Y=\hat{Y}|X)$, should be calibrated on any data space partition. The exposition of the results focuses on improving ECE (Table 1 & 2).
* The improvement in calibration is marginal.
  * For example, in Table 1, the proposed method GC+TS improves on Temperature Scaling (TS) by absolute differences ranging from 1e-4 to 1e-3 in Expected Calibration Error (ECE), which is an extremely small scale for ECE. Similarly, the proposed method GC+ETS improves on Ensembling Temperature Scaling (ETS) by absolute differences ranging from 1e-4 to 1e-3.


In summary, the proposed method has the potential to improve individual posterior probabilities and fairness metrics. Unfortunately, this potential is not exploited in the current framing since it focuses on improving calibration instead. On improving calibration, the proposed method has a too marginal effect.

## References

[1] Hebert-Johnson, U., Kim, M., Reingold, O., & Rothblum, G. (2018). Multicalibration: Calibration for the (Computationally-Identifiable) Masses. In Proceedings of the 35th International Conference on Machine Learning (pp. 1939–1948). PMLR.

[2] David Durfee, Aman Gupta, & Kinjal Basu. (2022). Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization.

[3] Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux. (2023). Beyond calibration: estimating the grouping loss of modern neural networks. ICLR.  
(First released on 8 Oct 2022).

[4] Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin. (2023). Beyond Confidence: Reliable Models Should Also Consider Atypicality.  
(First released on 04 Mar 2023, https://openreview.net/forum?id=nPOKJCCvlLF)

[5] Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi. (2023). Proximity-Informed Calibration for Deep Neural Networks.  
(First released on 7 Jun 2023).

[6] Chuan Guo, Geoff Pleiss, Yu Sun, & Kilian Q. Weinberger. (2017). On Calibration of Modern Neural Networks.","* Could you confirm that ECE is the metric plotted in Tables 1 & 2? It is not indicated.
* Could you develop on the statistical test used for assessing the significance of the results? What are the p-values obtained with the test? Do you correct for multiple comparisons?","The authors highlight the following limitations:
* The method is restricted to deep networks since it works in the extracted feature space. It thus cannot be applied to tree models, for example.
* Increasing the number of partitions and using more complex grouping models increased the computational complexity of the method.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,1 poor,1688301190589,,,NeurIPS.cc/2023/Conference,3kitbpEZZO,"['NeurIPS.cc/2023/Conference/Submission2840/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410866712,[],2,,,['everyone'],3kitbpEZZO,['NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v'],1688301190589,1702410866712,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Hebert-Johnson et al. (2018) - ""Multicalibration: Calibration for the (Computationally-Identifiable) Masses""
2. David Durfee, Aman Gupta, & Kinjal Basu (2022) - ""Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization""
3. Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux (2023) - ""Beyond calibration: estimating the grouping loss of modern neural networks""
4. Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin (2023) - ""Beyond Confidence: Reliable Models Should Also Consider Atypicality""
5. Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi (2023) - ""Proximity-Informed Calibration for Deep Neural Networks""

These citations are suggested to improve the positioning of the paper in the literature, as the reviewer believes that the proposed work has strong conceptual links with these existing works.",1,"2018, 2022, 2023, 2023, 2023",Heterogeneous Calibration A post-hoc model-agnostic framework for improved generalization
ShZ93jBn0R,"This work focuses on improving calibration (e.g., measured with Expected Calibration Error) using partitions of the feature space. This contrasts with most well-established recalibration techniques (isotonic regression, histogram binning, temperature scaling...) that solely use the estimated probabilities of the classifier. Partitions are learned during training with an extra linear layer + softmax on the features extracted by the deep model. Each part of the partition is recalibrated by the accuracy-preserving existing calibration techniques such as temperature scaling or ensembling temperature scaling. This ensures that the proposed method is also accuracy-preserving. The method is benchmarked on 3 datasets (CIFAR10, CIFAR100, ImageNet) and 3 networks on each dataset. The method is compared to 9 existing calibration techniques, such as temperature scaling, isotonic regression, and histogram binning.",* Learning the partitions using an extra linear layer + softmax is valuable since it is differentiable and enables end-to-end learning jointly with the network. This contrasts with existing works that use hard partitions by thresholding quantiles on a proximity metric [4-5] or by using decision trees [2-3].,"* Positioning in literature: It misses related works that share strong conceptual links with the proposed work. Those works could have been compared either in related work or in experiments.
  * It is not compared to multicalibration [1], which proposes an algorithm for learning a multicalibrated predictor with respect to any subpopulation class.
  * The idea of partitioning the network feature space to find local miscalibration has been used before [3-4]. [3] shows that strong local miscalibration arises in modern neural networks and proposes an estimator based on feature space partitions to evaluate them. [4] links this local miscalibration to atypicality.
  * [5] also links local miscalibration to proximity and proposes an algorithm to recalibrate those subgroups. NB: [5] was released after the NeurIPS submission deadline. I put it for information.
* Framing:
  * The valuable part of finding local miscalibration and correcting them, as it is done in the proposed work, is not to improve calibration but rather to improve estimated individual posterior probabilities (i.e., reducing the epistemic loss) as pointed out in [3], or improving fairness metrics as done in [1]. The problem is that the proposed work is entirely focused on improving calibration, which is blind to local miscalibration.
  * In addition to the missing related work, the discrepancy between the current framing and the potential of the proposed method can be felt on several levels. In the introduction: ""A perfectly calibrated model should be calibrated across any data space partition"" (L61). This is false since a perfectly calibrated model in the standard definition of calibration (e.g., in [6] eq. (1)) just needs to be calibrated on level sets of the same predicted confidence (i.e., satisfy $\mathbb{P}(Y=\hat{Y}|\hat{\mathbb{P}} = p) = p$). However, a perfect probabilistic classifier, that is, a classifier that outputs the true individual posterior probabilities $\mathbb{P}(Y=\hat{Y}|X)$, should be calibrated on any data space partition. The exposition of the results focuses on improving ECE (Table 1 & 2).
* The improvement in calibration is marginal.
  * For example, in Table 1, the proposed method GC+TS improves on Temperature Scaling (TS) by absolute differences ranging from 1e-4 to 1e-3 in Expected Calibration Error (ECE), which is an extremely small scale for ECE. Similarly, the proposed method GC+ETS improves on Ensembling Temperature Scaling (ETS) by absolute differences ranging from 1e-4 to 1e-3.


In summary, the proposed method has the potential to improve individual posterior probabilities and fairness metrics. Unfortunately, this potential is not exploited in the current framing since it focuses on improving calibration instead. On improving calibration, the proposed method has a too marginal effect.

## References

[1] Hebert-Johnson, U., Kim, M., Reingold, O., & Rothblum, G. (2018). Multicalibration: Calibration for the (Computationally-Identifiable) Masses. In Proceedings of the 35th International Conference on Machine Learning (pp. 1939–1948). PMLR.

[2] David Durfee, Aman Gupta, & Kinjal Basu. (2022). Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization.

[3] Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux. (2023). Beyond calibration: estimating the grouping loss of modern neural networks. ICLR.  
(First released on 8 Oct 2022).

[4] Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin. (2023). Beyond Confidence: Reliable Models Should Also Consider Atypicality.  
(First released on 04 Mar 2023, https://openreview.net/forum?id=nPOKJCCvlLF)

[5] Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi. (2023). Proximity-Informed Calibration for Deep Neural Networks.  
(First released on 7 Jun 2023).

[6] Chuan Guo, Geoff Pleiss, Yu Sun, & Kilian Q. Weinberger. (2017). On Calibration of Modern Neural Networks.","* Could you confirm that ECE is the metric plotted in Tables 1 & 2? It is not indicated.
* Could you develop on the statistical test used for assessing the significance of the results? What are the p-values obtained with the test? Do you correct for multiple comparisons?","The authors highlight the following limitations:
* The method is restricted to deep networks since it works in the extracted feature space. It thus cannot be applied to tree models, for example.
* Increasing the number of partitions and using more complex grouping models increased the computational complexity of the method.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,1 poor,1688301190589,,,NeurIPS.cc/2023/Conference,3kitbpEZZO,"['NeurIPS.cc/2023/Conference/Submission2840/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410866712,[],2,,,['everyone'],3kitbpEZZO,['NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v'],1688301190589,1702410866712,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Hebert-Johnson et al. (2018) - ""Multicalibration: Calibration for the (Computationally-Identifiable) Masses""
2. David Durfee, Aman Gupta, & Kinjal Basu (2022) - ""Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization""
3. Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux (2023) - ""Beyond calibration: estimating the grouping loss of modern neural networks""
4. Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin (2023) - ""Beyond Confidence: Reliable Models Should Also Consider Atypicality""
5. Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi (2023) - ""Proximity-Informed Calibration for Deep Neural Networks""

These citations are suggested to improve the positioning of the paper in the literature, as the reviewer believes that the proposed work has strong conceptual links with these existing works.",1,"2018, 2022, 2023, 2023, 2023",Beyond calibration estimating the grouping loss of modern neural networks
ShZ93jBn0R,"This work focuses on improving calibration (e.g., measured with Expected Calibration Error) using partitions of the feature space. This contrasts with most well-established recalibration techniques (isotonic regression, histogram binning, temperature scaling...) that solely use the estimated probabilities of the classifier. Partitions are learned during training with an extra linear layer + softmax on the features extracted by the deep model. Each part of the partition is recalibrated by the accuracy-preserving existing calibration techniques such as temperature scaling or ensembling temperature scaling. This ensures that the proposed method is also accuracy-preserving. The method is benchmarked on 3 datasets (CIFAR10, CIFAR100, ImageNet) and 3 networks on each dataset. The method is compared to 9 existing calibration techniques, such as temperature scaling, isotonic regression, and histogram binning.",* Learning the partitions using an extra linear layer + softmax is valuable since it is differentiable and enables end-to-end learning jointly with the network. This contrasts with existing works that use hard partitions by thresholding quantiles on a proximity metric [4-5] or by using decision trees [2-3].,"* Positioning in literature: It misses related works that share strong conceptual links with the proposed work. Those works could have been compared either in related work or in experiments.
  * It is not compared to multicalibration [1], which proposes an algorithm for learning a multicalibrated predictor with respect to any subpopulation class.
  * The idea of partitioning the network feature space to find local miscalibration has been used before [3-4]. [3] shows that strong local miscalibration arises in modern neural networks and proposes an estimator based on feature space partitions to evaluate them. [4] links this local miscalibration to atypicality.
  * [5] also links local miscalibration to proximity and proposes an algorithm to recalibrate those subgroups. NB: [5] was released after the NeurIPS submission deadline. I put it for information.
* Framing:
  * The valuable part of finding local miscalibration and correcting them, as it is done in the proposed work, is not to improve calibration but rather to improve estimated individual posterior probabilities (i.e., reducing the epistemic loss) as pointed out in [3], or improving fairness metrics as done in [1]. The problem is that the proposed work is entirely focused on improving calibration, which is blind to local miscalibration.
  * In addition to the missing related work, the discrepancy between the current framing and the potential of the proposed method can be felt on several levels. In the introduction: ""A perfectly calibrated model should be calibrated across any data space partition"" (L61). This is false since a perfectly calibrated model in the standard definition of calibration (e.g., in [6] eq. (1)) just needs to be calibrated on level sets of the same predicted confidence (i.e., satisfy $\mathbb{P}(Y=\hat{Y}|\hat{\mathbb{P}} = p) = p$). However, a perfect probabilistic classifier, that is, a classifier that outputs the true individual posterior probabilities $\mathbb{P}(Y=\hat{Y}|X)$, should be calibrated on any data space partition. The exposition of the results focuses on improving ECE (Table 1 & 2).
* The improvement in calibration is marginal.
  * For example, in Table 1, the proposed method GC+TS improves on Temperature Scaling (TS) by absolute differences ranging from 1e-4 to 1e-3 in Expected Calibration Error (ECE), which is an extremely small scale for ECE. Similarly, the proposed method GC+ETS improves on Ensembling Temperature Scaling (ETS) by absolute differences ranging from 1e-4 to 1e-3.


In summary, the proposed method has the potential to improve individual posterior probabilities and fairness metrics. Unfortunately, this potential is not exploited in the current framing since it focuses on improving calibration instead. On improving calibration, the proposed method has a too marginal effect.

## References

[1] Hebert-Johnson, U., Kim, M., Reingold, O., & Rothblum, G. (2018). Multicalibration: Calibration for the (Computationally-Identifiable) Masses. In Proceedings of the 35th International Conference on Machine Learning (pp. 1939–1948). PMLR.

[2] David Durfee, Aman Gupta, & Kinjal Basu. (2022). Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization.

[3] Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux. (2023). Beyond calibration: estimating the grouping loss of modern neural networks. ICLR.  
(First released on 8 Oct 2022).

[4] Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin. (2023). Beyond Confidence: Reliable Models Should Also Consider Atypicality.  
(First released on 04 Mar 2023, https://openreview.net/forum?id=nPOKJCCvlLF)

[5] Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi. (2023). Proximity-Informed Calibration for Deep Neural Networks.  
(First released on 7 Jun 2023).

[6] Chuan Guo, Geoff Pleiss, Yu Sun, & Kilian Q. Weinberger. (2017). On Calibration of Modern Neural Networks.","* Could you confirm that ECE is the metric plotted in Tables 1 & 2? It is not indicated.
* Could you develop on the statistical test used for assessing the significance of the results? What are the p-values obtained with the test? Do you correct for multiple comparisons?","The authors highlight the following limitations:
* The method is restricted to deep networks since it works in the extracted feature space. It thus cannot be applied to tree models, for example.
* Increasing the number of partitions and using more complex grouping models increased the computational complexity of the method.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,1 poor,1688301190589,,,NeurIPS.cc/2023/Conference,3kitbpEZZO,"['NeurIPS.cc/2023/Conference/Submission2840/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410866712,[],2,,,['everyone'],3kitbpEZZO,['NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v'],1688301190589,1702410866712,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Hebert-Johnson et al. (2018) - ""Multicalibration: Calibration for the (Computationally-Identifiable) Masses""
2. David Durfee, Aman Gupta, & Kinjal Basu (2022) - ""Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization""
3. Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux (2023) - ""Beyond calibration: estimating the grouping loss of modern neural networks""
4. Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin (2023) - ""Beyond Confidence: Reliable Models Should Also Consider Atypicality""
5. Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi (2023) - ""Proximity-Informed Calibration for Deep Neural Networks""

These citations are suggested to improve the positioning of the paper in the literature, as the reviewer believes that the proposed work has strong conceptual links with these existing works.",1,"2018, 2022, 2023, 2023, 2023",Beyond Confidence Reliable Models Should Also Consider Atypicality
ShZ93jBn0R,"This work focuses on improving calibration (e.g., measured with Expected Calibration Error) using partitions of the feature space. This contrasts with most well-established recalibration techniques (isotonic regression, histogram binning, temperature scaling...) that solely use the estimated probabilities of the classifier. Partitions are learned during training with an extra linear layer + softmax on the features extracted by the deep model. Each part of the partition is recalibrated by the accuracy-preserving existing calibration techniques such as temperature scaling or ensembling temperature scaling. This ensures that the proposed method is also accuracy-preserving. The method is benchmarked on 3 datasets (CIFAR10, CIFAR100, ImageNet) and 3 networks on each dataset. The method is compared to 9 existing calibration techniques, such as temperature scaling, isotonic regression, and histogram binning.",* Learning the partitions using an extra linear layer + softmax is valuable since it is differentiable and enables end-to-end learning jointly with the network. This contrasts with existing works that use hard partitions by thresholding quantiles on a proximity metric [4-5] or by using decision trees [2-3].,"* Positioning in literature: It misses related works that share strong conceptual links with the proposed work. Those works could have been compared either in related work or in experiments.
  * It is not compared to multicalibration [1], which proposes an algorithm for learning a multicalibrated predictor with respect to any subpopulation class.
  * The idea of partitioning the network feature space to find local miscalibration has been used before [3-4]. [3] shows that strong local miscalibration arises in modern neural networks and proposes an estimator based on feature space partitions to evaluate them. [4] links this local miscalibration to atypicality.
  * [5] also links local miscalibration to proximity and proposes an algorithm to recalibrate those subgroups. NB: [5] was released after the NeurIPS submission deadline. I put it for information.
* Framing:
  * The valuable part of finding local miscalibration and correcting them, as it is done in the proposed work, is not to improve calibration but rather to improve estimated individual posterior probabilities (i.e., reducing the epistemic loss) as pointed out in [3], or improving fairness metrics as done in [1]. The problem is that the proposed work is entirely focused on improving calibration, which is blind to local miscalibration.
  * In addition to the missing related work, the discrepancy between the current framing and the potential of the proposed method can be felt on several levels. In the introduction: ""A perfectly calibrated model should be calibrated across any data space partition"" (L61). This is false since a perfectly calibrated model in the standard definition of calibration (e.g., in [6] eq. (1)) just needs to be calibrated on level sets of the same predicted confidence (i.e., satisfy $\mathbb{P}(Y=\hat{Y}|\hat{\mathbb{P}} = p) = p$). However, a perfect probabilistic classifier, that is, a classifier that outputs the true individual posterior probabilities $\mathbb{P}(Y=\hat{Y}|X)$, should be calibrated on any data space partition. The exposition of the results focuses on improving ECE (Table 1 & 2).
* The improvement in calibration is marginal.
  * For example, in Table 1, the proposed method GC+TS improves on Temperature Scaling (TS) by absolute differences ranging from 1e-4 to 1e-3 in Expected Calibration Error (ECE), which is an extremely small scale for ECE. Similarly, the proposed method GC+ETS improves on Ensembling Temperature Scaling (ETS) by absolute differences ranging from 1e-4 to 1e-3.


In summary, the proposed method has the potential to improve individual posterior probabilities and fairness metrics. Unfortunately, this potential is not exploited in the current framing since it focuses on improving calibration instead. On improving calibration, the proposed method has a too marginal effect.

## References

[1] Hebert-Johnson, U., Kim, M., Reingold, O., & Rothblum, G. (2018). Multicalibration: Calibration for the (Computationally-Identifiable) Masses. In Proceedings of the 35th International Conference on Machine Learning (pp. 1939–1948). PMLR.

[2] David Durfee, Aman Gupta, & Kinjal Basu. (2022). Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization.

[3] Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux. (2023). Beyond calibration: estimating the grouping loss of modern neural networks. ICLR.  
(First released on 8 Oct 2022).

[4] Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin. (2023). Beyond Confidence: Reliable Models Should Also Consider Atypicality.  
(First released on 04 Mar 2023, https://openreview.net/forum?id=nPOKJCCvlLF)

[5] Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi. (2023). Proximity-Informed Calibration for Deep Neural Networks.  
(First released on 7 Jun 2023).

[6] Chuan Guo, Geoff Pleiss, Yu Sun, & Kilian Q. Weinberger. (2017). On Calibration of Modern Neural Networks.","* Could you confirm that ECE is the metric plotted in Tables 1 & 2? It is not indicated.
* Could you develop on the statistical test used for assessing the significance of the results? What are the p-values obtained with the test? Do you correct for multiple comparisons?","The authors highlight the following limitations:
* The method is restricted to deep networks since it works in the extracted feature space. It thus cannot be applied to tree models, for example.
* Increasing the number of partitions and using more complex grouping models increased the computational complexity of the method.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,1 poor,1688301190589,,,NeurIPS.cc/2023/Conference,3kitbpEZZO,"['NeurIPS.cc/2023/Conference/Submission2840/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410866712,[],2,,,['everyone'],3kitbpEZZO,['NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v'],1688301190589,1702410866712,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2840/Reviewer_TM5v']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Hebert-Johnson et al. (2018) - ""Multicalibration: Calibration for the (Computationally-Identifiable) Masses""
2. David Durfee, Aman Gupta, & Kinjal Basu (2022) - ""Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization""
3. Alexandre Perez-Lebel, Marine Le Morvan, & Gaël Varoquaux (2023) - ""Beyond calibration: estimating the grouping loss of modern neural networks""
4. Mert Yuksekgonul, Linjun Zhang, James Zou, & Carlos Guestrin (2023) - ""Beyond Confidence: Reliable Models Should Also Consider Atypicality""
5. Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, & Bryan Hooi (2023) - ""Proximity-Informed Calibration for Deep Neural Networks""

These citations are suggested to improve the positioning of the paper in the literature, as the reviewer believes that the proposed work has strong conceptual links with these existing works.",1,"2018, 2022, 2023, 2023, 2023",Proximity-Informed Calibration for Deep Neural Networks
B57kdkjmyt,"This paper proposes a symmetry equivariance constraints on the transition dynamics of latent representations for time-indexed data. The claim is that imposing these certain symmetries create interpretable model representations that correspond to popular domain-specific representations in the audio and video domains.

Experiments on audio demonstrate how enforcing a translational symmetry recovers an interpretable one-dimensional latent factor corresponding to pitch. Experiments on video show how enforcing translational and rotational symmetries recovers interpretable three-dimensional latent factors corresponding to spatial coordinates. Ablations also show some robustness of learning to mis-specification of symmetries.","This is an inspiring paper! The paper is well-motivated and well-written. The methodology is clearly described and easy to follow (although I do think there might be an even better probabilistic formulation/presentation of the training objective; see the Questions section). The methods make sense and I feel confident that I could implement these ideas myself, based on the description in the paper.

The experiments are well-executed and support the hypothesis that physical symmetries can provide a powerful inductive bias, at a higher level of abstraction than more domain-specific approaches.","My only major criticism is that the experiments are in somewhat ""toy"" settings. It would be interesting to see an application of these ideas to a more significant problem, with stronger baselines.

A very minor criticism of the (otherwise excellent!) introduction: ""Such an approach is very different from human learning; even without formal music training, one can at least perceive pitch, a fundamental music concept, from the experience of listening to music."" I think this claim is too strong. It is not clear how much of human perception is learned from experience, vs. baked in to our genetics and brain structure. This claim isn't important to your central argument, so I suggest moderating it a bit.","Is it possible to frame the training objective (Equation 1) as a proper probabilistic loss? See [1] (the static case) and [2] (the time-indexed case) for the probabilistic formulations of the Gaussian prior. If this is possible, it might help to clarify why/whether it is possible to drop the KL regularization term.

Does this work relate to previous work on imposing symmetries on intermediate layers of neural networks? E.g. [3] and [4]. I am not deeply familiar with that line of work, but I'm bringing it up because it might be relevant (and at the very least, you might find it interesting if you aren't already aware).

[1] Auto-Encoding Variational Bayes. Diederik P. Kingma, Max Welling.

[2] A Recurrent Latent Variable Model for Sequential Data. Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio.

[3] Deep Symmetry Networks. Robert Gens, Pedro Domingos.

[4] Group Equivariant Convolutional Networks. Taco S. Cohen, Max Welling.","The experiments presented in this work use low-dimensional latent spaces: 1 dimension for audio and 3 dimensions for video. It is not clear how to adapt these methods to the high-dimensional latent spaces commonly used (and often required) for expressive models where model performance and capacity is prioritized more highly than interpretability. Furthermore, it is not clear what symmetries we ought to impose upon high-dimensional latent spaces. That said, these questions are clearly beyond the scope of the present work: I see this limitation more as an interesting avenues for future investigation rather than a weakness of the present work.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,4 excellent,1688653391116,,,NeurIPS.cc/2023/Conference,3iSj4l8ZGT,"['NeurIPS.cc/2023/Conference/Submission11097/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411315303,[],4,,,['everyone'],3iSj4l8ZGT,['NeurIPS.cc/2023/Conference/Submission11097/Reviewer_5wLb'],1688653391116,1702411315303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission11097/Reviewer_5wLb']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. [1] Auto-Encoding Variational Bayes by Diederik P. Kingma and Max Welling.
2. [2] A Recurrent Latent Variable Model for Sequential Data by Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua Bengio.
3. [3] Deep Symmetry Networks by Robert Gens and Pedro Domingos.
4. [4] Group Equivariant Convolutional Networks by Taco S. Cohen and Max Welling.

The reviewer mentions these papers as potentially relevant to the work presented and suggests that the authors might find them interesting. However, it's not a strict requirement, but rather a suggestion for the authors to consider.",1,,Auto-Encoding Variational Bayes 
B57kdkjmyt,"This paper proposes a symmetry equivariance constraints on the transition dynamics of latent representations for time-indexed data. The claim is that imposing these certain symmetries create interpretable model representations that correspond to popular domain-specific representations in the audio and video domains.

Experiments on audio demonstrate how enforcing a translational symmetry recovers an interpretable one-dimensional latent factor corresponding to pitch. Experiments on video show how enforcing translational and rotational symmetries recovers interpretable three-dimensional latent factors corresponding to spatial coordinates. Ablations also show some robustness of learning to mis-specification of symmetries.","This is an inspiring paper! The paper is well-motivated and well-written. The methodology is clearly described and easy to follow (although I do think there might be an even better probabilistic formulation/presentation of the training objective; see the Questions section). The methods make sense and I feel confident that I could implement these ideas myself, based on the description in the paper.

The experiments are well-executed and support the hypothesis that physical symmetries can provide a powerful inductive bias, at a higher level of abstraction than more domain-specific approaches.","My only major criticism is that the experiments are in somewhat ""toy"" settings. It would be interesting to see an application of these ideas to a more significant problem, with stronger baselines.

A very minor criticism of the (otherwise excellent!) introduction: ""Such an approach is very different from human learning; even without formal music training, one can at least perceive pitch, a fundamental music concept, from the experience of listening to music."" I think this claim is too strong. It is not clear how much of human perception is learned from experience, vs. baked in to our genetics and brain structure. This claim isn't important to your central argument, so I suggest moderating it a bit.","Is it possible to frame the training objective (Equation 1) as a proper probabilistic loss? See [1] (the static case) and [2] (the time-indexed case) for the probabilistic formulations of the Gaussian prior. If this is possible, it might help to clarify why/whether it is possible to drop the KL regularization term.

Does this work relate to previous work on imposing symmetries on intermediate layers of neural networks? E.g. [3] and [4]. I am not deeply familiar with that line of work, but I'm bringing it up because it might be relevant (and at the very least, you might find it interesting if you aren't already aware).

[1] Auto-Encoding Variational Bayes. Diederik P. Kingma, Max Welling.

[2] A Recurrent Latent Variable Model for Sequential Data. Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio.

[3] Deep Symmetry Networks. Robert Gens, Pedro Domingos.

[4] Group Equivariant Convolutional Networks. Taco S. Cohen, Max Welling.","The experiments presented in this work use low-dimensional latent spaces: 1 dimension for audio and 3 dimensions for video. It is not clear how to adapt these methods to the high-dimensional latent spaces commonly used (and often required) for expressive models where model performance and capacity is prioritized more highly than interpretability. Furthermore, it is not clear what symmetries we ought to impose upon high-dimensional latent spaces. That said, these questions are clearly beyond the scope of the present work: I see this limitation more as an interesting avenues for future investigation rather than a weakness of the present work.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,4 excellent,1688653391116,,,NeurIPS.cc/2023/Conference,3iSj4l8ZGT,"['NeurIPS.cc/2023/Conference/Submission11097/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411315303,[],4,,,['everyone'],3iSj4l8ZGT,['NeurIPS.cc/2023/Conference/Submission11097/Reviewer_5wLb'],1688653391116,1702411315303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission11097/Reviewer_5wLb']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. [1] Auto-Encoding Variational Bayes by Diederik P. Kingma and Max Welling.
2. [2] A Recurrent Latent Variable Model for Sequential Data by Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua Bengio.
3. [3] Deep Symmetry Networks by Robert Gens and Pedro Domingos.
4. [4] Group Equivariant Convolutional Networks by Taco S. Cohen and Max Welling.

The reviewer mentions these papers as potentially relevant to the work presented and suggests that the authors might find them interesting. However, it's not a strict requirement, but rather a suggestion for the authors to consider.",1,,A Recurrent Latent Variable Model for Sequential Data 
B57kdkjmyt,"This paper proposes a symmetry equivariance constraints on the transition dynamics of latent representations for time-indexed data. The claim is that imposing these certain symmetries create interpretable model representations that correspond to popular domain-specific representations in the audio and video domains.

Experiments on audio demonstrate how enforcing a translational symmetry recovers an interpretable one-dimensional latent factor corresponding to pitch. Experiments on video show how enforcing translational and rotational symmetries recovers interpretable three-dimensional latent factors corresponding to spatial coordinates. Ablations also show some robustness of learning to mis-specification of symmetries.","This is an inspiring paper! The paper is well-motivated and well-written. The methodology is clearly described and easy to follow (although I do think there might be an even better probabilistic formulation/presentation of the training objective; see the Questions section). The methods make sense and I feel confident that I could implement these ideas myself, based on the description in the paper.

The experiments are well-executed and support the hypothesis that physical symmetries can provide a powerful inductive bias, at a higher level of abstraction than more domain-specific approaches.","My only major criticism is that the experiments are in somewhat ""toy"" settings. It would be interesting to see an application of these ideas to a more significant problem, with stronger baselines.

A very minor criticism of the (otherwise excellent!) introduction: ""Such an approach is very different from human learning; even without formal music training, one can at least perceive pitch, a fundamental music concept, from the experience of listening to music."" I think this claim is too strong. It is not clear how much of human perception is learned from experience, vs. baked in to our genetics and brain structure. This claim isn't important to your central argument, so I suggest moderating it a bit.","Is it possible to frame the training objective (Equation 1) as a proper probabilistic loss? See [1] (the static case) and [2] (the time-indexed case) for the probabilistic formulations of the Gaussian prior. If this is possible, it might help to clarify why/whether it is possible to drop the KL regularization term.

Does this work relate to previous work on imposing symmetries on intermediate layers of neural networks? E.g. [3] and [4]. I am not deeply familiar with that line of work, but I'm bringing it up because it might be relevant (and at the very least, you might find it interesting if you aren't already aware).

[1] Auto-Encoding Variational Bayes. Diederik P. Kingma, Max Welling.

[2] A Recurrent Latent Variable Model for Sequential Data. Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio.

[3] Deep Symmetry Networks. Robert Gens, Pedro Domingos.

[4] Group Equivariant Convolutional Networks. Taco S. Cohen, Max Welling.","The experiments presented in this work use low-dimensional latent spaces: 1 dimension for audio and 3 dimensions for video. It is not clear how to adapt these methods to the high-dimensional latent spaces commonly used (and often required) for expressive models where model performance and capacity is prioritized more highly than interpretability. Furthermore, it is not clear what symmetries we ought to impose upon high-dimensional latent spaces. That said, these questions are clearly beyond the scope of the present work: I see this limitation more as an interesting avenues for future investigation rather than a weakness of the present work.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,4 excellent,1688653391116,,,NeurIPS.cc/2023/Conference,3iSj4l8ZGT,"['NeurIPS.cc/2023/Conference/Submission11097/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411315303,[],4,,,['everyone'],3iSj4l8ZGT,['NeurIPS.cc/2023/Conference/Submission11097/Reviewer_5wLb'],1688653391116,1702411315303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission11097/Reviewer_5wLb']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. [1] Auto-Encoding Variational Bayes by Diederik P. Kingma and Max Welling.
2. [2] A Recurrent Latent Variable Model for Sequential Data by Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua Bengio.
3. [3] Deep Symmetry Networks by Robert Gens and Pedro Domingos.
4. [4] Group Equivariant Convolutional Networks by Taco S. Cohen and Max Welling.

The reviewer mentions these papers as potentially relevant to the work presented and suggests that the authors might find them interesting. However, it's not a strict requirement, but rather a suggestion for the authors to consider.",1,,Deep Symmetry Networks 
B57kdkjmyt,"This paper proposes a symmetry equivariance constraints on the transition dynamics of latent representations for time-indexed data. The claim is that imposing these certain symmetries create interpretable model representations that correspond to popular domain-specific representations in the audio and video domains.

Experiments on audio demonstrate how enforcing a translational symmetry recovers an interpretable one-dimensional latent factor corresponding to pitch. Experiments on video show how enforcing translational and rotational symmetries recovers interpretable three-dimensional latent factors corresponding to spatial coordinates. Ablations also show some robustness of learning to mis-specification of symmetries.","This is an inspiring paper! The paper is well-motivated and well-written. The methodology is clearly described and easy to follow (although I do think there might be an even better probabilistic formulation/presentation of the training objective; see the Questions section). The methods make sense and I feel confident that I could implement these ideas myself, based on the description in the paper.

The experiments are well-executed and support the hypothesis that physical symmetries can provide a powerful inductive bias, at a higher level of abstraction than more domain-specific approaches.","My only major criticism is that the experiments are in somewhat ""toy"" settings. It would be interesting to see an application of these ideas to a more significant problem, with stronger baselines.

A very minor criticism of the (otherwise excellent!) introduction: ""Such an approach is very different from human learning; even without formal music training, one can at least perceive pitch, a fundamental music concept, from the experience of listening to music."" I think this claim is too strong. It is not clear how much of human perception is learned from experience, vs. baked in to our genetics and brain structure. This claim isn't important to your central argument, so I suggest moderating it a bit.","Is it possible to frame the training objective (Equation 1) as a proper probabilistic loss? See [1] (the static case) and [2] (the time-indexed case) for the probabilistic formulations of the Gaussian prior. If this is possible, it might help to clarify why/whether it is possible to drop the KL regularization term.

Does this work relate to previous work on imposing symmetries on intermediate layers of neural networks? E.g. [3] and [4]. I am not deeply familiar with that line of work, but I'm bringing it up because it might be relevant (and at the very least, you might find it interesting if you aren't already aware).

[1] Auto-Encoding Variational Bayes. Diederik P. Kingma, Max Welling.

[2] A Recurrent Latent Variable Model for Sequential Data. Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio.

[3] Deep Symmetry Networks. Robert Gens, Pedro Domingos.

[4] Group Equivariant Convolutional Networks. Taco S. Cohen, Max Welling.","The experiments presented in this work use low-dimensional latent spaces: 1 dimension for audio and 3 dimensions for video. It is not clear how to adapt these methods to the high-dimensional latent spaces commonly used (and often required) for expressive models where model performance and capacity is prioritized more highly than interpretability. Furthermore, it is not clear what symmetries we ought to impose upon high-dimensional latent spaces. That said, these questions are clearly beyond the scope of the present work: I see this limitation more as an interesting avenues for future investigation rather than a weakness of the present work.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,4 excellent,1688653391116,,,NeurIPS.cc/2023/Conference,3iSj4l8ZGT,"['NeurIPS.cc/2023/Conference/Submission11097/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411315303,[],4,,,['everyone'],3iSj4l8ZGT,['NeurIPS.cc/2023/Conference/Submission11097/Reviewer_5wLb'],1688653391116,1702411315303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission11097/Reviewer_5wLb']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. [1] Auto-Encoding Variational Bayes by Diederik P. Kingma and Max Welling.
2. [2] A Recurrent Latent Variable Model for Sequential Data by Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua Bengio.
3. [3] Deep Symmetry Networks by Robert Gens and Pedro Domingos.
4. [4] Group Equivariant Convolutional Networks by Taco S. Cohen and Max Welling.

The reviewer mentions these papers as potentially relevant to the work presented and suggests that the authors might find them interesting. However, it's not a strict requirement, but rather a suggestion for the authors to consider.",1,,Group Equivariant Convolutional Networks
Xr0a7GVMdk,"This paper proposes a novel quantization method for SR models, including two new technologies to improve the unit representation and architectural potential of quantized SR models and push low-bit SR models to full-precision performance. The results on CNN and Transformer models on SR tasks show that QuantSR achieves SOTA performance and exceeds existing SR quantization methods. The most significant point of this paper is that the dynamic quantized architecture of QuantSR pushes up the performance ceiling of quantized SR from a block-stacking perspective and allows flexible reasoning according to resources in actual deployment, leading to a new promising way to lossless quantized SR.","In this paper, the authors propose novel RLQ and DQA technologies to jointly improve quantized SR models from two orthogonal perspectives of the unit representation and architectural potential:

a) RLQ is an effective technique. By introducing learnable parameters and redistribution functions in the forward and backward propagation of quantized computing units, the amount of information in the quantization training propagation process is significantly improved and more accurate, and the representation ability of the quantization unit is improved.

b) DQA is more attractive to me. In addition to being effective, this is a novel attempt to improve the quantized SR model from an architectural perspective. QuantSR's dynamic quantization architecture pushes up the performance ceiling of quantized SR from a block-stacking perspective, thus providing a new promising way towards lossless quantized SR models. In addition, it also allows the quantized SR model to perform flexible reasoning according to the resources in the actual deployment, which means that the practicality of the quantized SR model is greatly enhanced.

The experimental results are superior. QuantSR shows significant performance improvement under all bit widths and surpasses the existing quantized SR methods. This enables QuantSR to achieve a SOTA structure while being novel. The visualization results clearly show that the effect of the proposed technologies follows their motivation.
","a) One notable issue is there are some places in the paper that should be revised and clarified, including but not limited to the following:
- In the box of Redistribution in Fig.2, should the y-axis coordinate be PDF(a), that is, the probability density function of a
- In the box of Learnable Quantizer in Fig.2, according to the formula Eq.5 in the paper, s should be corrected to vˆb
- The gamma in Stage 2 in Fig.2 is undefined
- Symbol a is defined repeatedly as activation or range in Eq.4
- According to the implementation in the article, the options of var may be actually 200% (32 blocks), 100% (16 blocks), and 50% (8 blocks)

b) In RLQ, the ϕ function seems to be used as an estimator to replace STE, so why it needs to be used in the forward pass and not just backward? In addition, the authors also need to explain why the ϕ function adopts this shape instead of the existing soft quantization functions such as DSQ [1] and Bireal [2].

c) The author should compare more quantized transformer SR models, and existing experiments only show the QuantSR performance on this type of network, other quantized methods are also needed. And it is necessary to show the comparison of the parameters and calculation amount of SwinIR and SRResNet, including full precision and quantized versions, to make the comparison more fair and clear.

[1] Gong R, Liu X, Jiang S, et al. Differentiable soft quantization: Bridging full-precision and low-bit neural networks, ICCV, 2019.
[2] Liu Z, Wu B, Luo W, et al. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm, ECCV, 2018.
",See weaknesses.,"The author proposes an effective quantification method for SR tasks in this paper, and I suggest the author discuss the potential of QuantSR in more low-level vision tasks in more detail, which will make the contributions of the proposed method more significant and wide.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,4 excellent,3 good,1687922526214,,,NeurIPS.cc/2023/Conference,3gamyee9Yh,"['NeurIPS.cc/2023/Conference/Submission422/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410744092,[],2,,,['everyone'],3gamyee9Yh,['NeurIPS.cc/2023/Conference/Submission422/Reviewer_j1Cc'],1687922526214,1702410744092,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission422/Reviewer_j1Cc']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Gong R, Liu X, Jiang S, et al. Differentiable soft quantization: Bridging full-precision and low-bit neural networks, ICCV, 2019. (referred to as [1])
2. Liu Z, Wu B, Luo W, et al. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm, ECCV, 2018. (referred to as [2])

The reviewer mentions these papers as existing soft quantization functions (DSQ and Bireal) that the authors should consider and compare with their proposed ϕ function.",1,"2019, 2018",Differentiable soft quantization Bridging full-precision and low-bit neural networks 
Xr0a7GVMdk,"This paper proposes a novel quantization method for SR models, including two new technologies to improve the unit representation and architectural potential of quantized SR models and push low-bit SR models to full-precision performance. The results on CNN and Transformer models on SR tasks show that QuantSR achieves SOTA performance and exceeds existing SR quantization methods. The most significant point of this paper is that the dynamic quantized architecture of QuantSR pushes up the performance ceiling of quantized SR from a block-stacking perspective and allows flexible reasoning according to resources in actual deployment, leading to a new promising way to lossless quantized SR.","In this paper, the authors propose novel RLQ and DQA technologies to jointly improve quantized SR models from two orthogonal perspectives of the unit representation and architectural potential:

a) RLQ is an effective technique. By introducing learnable parameters and redistribution functions in the forward and backward propagation of quantized computing units, the amount of information in the quantization training propagation process is significantly improved and more accurate, and the representation ability of the quantization unit is improved.

b) DQA is more attractive to me. In addition to being effective, this is a novel attempt to improve the quantized SR model from an architectural perspective. QuantSR's dynamic quantization architecture pushes up the performance ceiling of quantized SR from a block-stacking perspective, thus providing a new promising way towards lossless quantized SR models. In addition, it also allows the quantized SR model to perform flexible reasoning according to the resources in the actual deployment, which means that the practicality of the quantized SR model is greatly enhanced.

The experimental results are superior. QuantSR shows significant performance improvement under all bit widths and surpasses the existing quantized SR methods. This enables QuantSR to achieve a SOTA structure while being novel. The visualization results clearly show that the effect of the proposed technologies follows their motivation.
","a) One notable issue is there are some places in the paper that should be revised and clarified, including but not limited to the following:
- In the box of Redistribution in Fig.2, should the y-axis coordinate be PDF(a), that is, the probability density function of a
- In the box of Learnable Quantizer in Fig.2, according to the formula Eq.5 in the paper, s should be corrected to vˆb
- The gamma in Stage 2 in Fig.2 is undefined
- Symbol a is defined repeatedly as activation or range in Eq.4
- According to the implementation in the article, the options of var may be actually 200% (32 blocks), 100% (16 blocks), and 50% (8 blocks)

b) In RLQ, the ϕ function seems to be used as an estimator to replace STE, so why it needs to be used in the forward pass and not just backward? In addition, the authors also need to explain why the ϕ function adopts this shape instead of the existing soft quantization functions such as DSQ [1] and Bireal [2].

c) The author should compare more quantized transformer SR models, and existing experiments only show the QuantSR performance on this type of network, other quantized methods are also needed. And it is necessary to show the comparison of the parameters and calculation amount of SwinIR and SRResNet, including full precision and quantized versions, to make the comparison more fair and clear.

[1] Gong R, Liu X, Jiang S, et al. Differentiable soft quantization: Bridging full-precision and low-bit neural networks, ICCV, 2019.
[2] Liu Z, Wu B, Luo W, et al. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm, ECCV, 2018.
",See weaknesses.,"The author proposes an effective quantification method for SR tasks in this paper, and I suggest the author discuss the potential of QuantSR in more low-level vision tasks in more detail, which will make the contributions of the proposed method more significant and wide.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,4 excellent,3 good,1687922526214,,,NeurIPS.cc/2023/Conference,3gamyee9Yh,"['NeurIPS.cc/2023/Conference/Submission422/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410744092,[],2,,,['everyone'],3gamyee9Yh,['NeurIPS.cc/2023/Conference/Submission422/Reviewer_j1Cc'],1687922526214,1702410744092,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission422/Reviewer_j1Cc']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Gong R, Liu X, Jiang S, et al. Differentiable soft quantization: Bridging full-precision and low-bit neural networks, ICCV, 2019. (referred to as [1])
2. Liu Z, Wu B, Luo W, et al. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm, ECCV, 2018. (referred to as [2])

The reviewer mentions these papers as existing soft quantization functions (DSQ and Bireal) that the authors should consider and compare with their proposed ϕ function.",1,"2019, 2018",Bi-real net Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm
2XEeDtIbzh,"This work proposes a new approach called formal feature attribution (FFA), inspired by successful FXAI methods, to compute feature attribution scores. FFA is defined as the proportion of explanations where a feature occurs. Experiments try to demonstrate the effectiveness of FFA compared to SHAP and LIME under several public datasets. ","1. The authors proposed a new perspective on providing feature attributions. They did point out some limitations of existing popular methods.
2. The proposed FFA is straightforward and with clear motivations. 
","1. Important questions regarding the reasonableness of the proposed method are unanswered. For instance, the three advantage properties of FFA claimed in this paper are not well reasoned. 
2. The experiments provided in this work are hard to support FFA is better than other existing XAI scoring methods. 
3. Several definitions and annotations throughout the text lack proper illustration or clarification, which can hinder the reader's understanding. Without clear explanations, it becomes challenging to grasp the intended meanings and implications of these terms and annotations.
","1. The authors claim that FFA gains three nice properties. However, the descriptions of those properties are too vague to understand. The authors are highly encouraged to explain more details about those points. Some example concerns are illustrated as follows. The first advantage is that FFA has a strict and formal definition. However, the author fails to explain why the Shapley Value lacks a strict and formal definition, despite it being commonly regarded as the ground truth value in feature attributions. I am unable to discern the distinction between the Shapley Value and FFA in terms of a strict and formal definition, as well as why the Shapley Value cannot provide formal feature attributions. Additionally, claiming that percentage-wise feature attributions are more user-friendly without conducting user experiment studies or related experiments is unsubstantiated. Insufficient case studies on image data are inadequate to provide support.
2. There are a lot of advancements discussing the issues of out-of-distribution sampling. It would be better to see the comparison between these series of methods, making the proposed approach more convincing, as FFA is claimed to address the limitations of out-of-distribution sampling (see line 140 to 148).
3. LIME and SHAP are two classical baselines to provide a model explanation but obtain inferior performance on image datasets. It is highly encouraged the authors compare with some SOTA Shapley-based methods on image data, such as DeepSHAP[1], FastSHAP[2], or CoRTX[3]. 
4. As for the experimental results in Figure 3, it is very limited for authors to use only one example to reveal that FFA is better than LIME and SHAP. Furthermore, Table 1 only demonstrates LIME and SHAP fail to get close enough to FFA, which is expected as neither of them is designed to approximate FFA. However, the conclusion in Table 1 does not mean that FFA is a better indicator than LIME, SHAP, or Shapley Values in providing a formal explanation. I did not see any direct evidence to support this point. 

[1] Delivering Trustworthy AI through Formal XAI. Marques-Silva et al. AAAI 2022.
[2] A unified approach to interpreting model predictions. Lundberg et al. NeurIPS 2017.
[3] FastSHAP: Real-Time Shapley Value Estimation. Neil et al. ICLR 2022.
[4] CoRTX: Contrastive Framework for Real-time Explanation. Chuang et al. ICLR 2023.
","1. The proposed FFA only focuses on the classification task, which makes it limited to be applied in several other common tasks, such as the regression task. 
2. The paper lacks clear illustrations of experiment settings and does not compare with relevant advancements, leading to unconvincing results for verifying the proposed hypothesis. More detailed descriptions of the experiment settings and comparative analyses with related advancements are necessary to strengthen the study's validity.
3. Several annotations and definitions are not clearly illustrated. For example, the “right arrow” in Equation 1 and the definition of “formal explanation” is not well-illustrated in this work. This makes the work hard to follow. 
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688707148676,,,NeurIPS.cc/2023/Conference,3b5e2AFs7f,"['NeurIPS.cc/2023/Conference/Submission12057/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411360301,[],4,,,['everyone'],3b5e2AFs7f,['NeurIPS.cc/2023/Conference/Submission12057/Reviewer_aL7B'],1688707148676,1702411360301,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12057/Reviewer_aL7B']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that the authors should be aware of or compare their work with, including:

1. ""Delivering Trustworthy AI through Formal XAI"" by Marques-Silva et al. (AAAI 2022) [1]
2. ""A unified approach to interpreting model predictions"" by Lundberg et al. (NeurIPS 2017) [2]
3. ""FastSHAP: Real-Time Shapley Value Estimation"" by Neil et al. (ICLR 2022) [3]
4. ""CoRTX: Contrastive Framework for Real-time Explanation"" by Chuang et al. (ICLR 2023) [4]

The reviewer suggests that the authors should compare their proposed approach (FFA) with these state-of-the-art (SOTA) Shapley-based methods, particularly on image data, to make their approach more convincing.",1,"2022, 2017, 2022, 2023",Delivering Trustworthy AI through Formal XAI 
2XEeDtIbzh,"This work proposes a new approach called formal feature attribution (FFA), inspired by successful FXAI methods, to compute feature attribution scores. FFA is defined as the proportion of explanations where a feature occurs. Experiments try to demonstrate the effectiveness of FFA compared to SHAP and LIME under several public datasets. ","1. The authors proposed a new perspective on providing feature attributions. They did point out some limitations of existing popular methods.
2. The proposed FFA is straightforward and with clear motivations. 
","1. Important questions regarding the reasonableness of the proposed method are unanswered. For instance, the three advantage properties of FFA claimed in this paper are not well reasoned. 
2. The experiments provided in this work are hard to support FFA is better than other existing XAI scoring methods. 
3. Several definitions and annotations throughout the text lack proper illustration or clarification, which can hinder the reader's understanding. Without clear explanations, it becomes challenging to grasp the intended meanings and implications of these terms and annotations.
","1. The authors claim that FFA gains three nice properties. However, the descriptions of those properties are too vague to understand. The authors are highly encouraged to explain more details about those points. Some example concerns are illustrated as follows. The first advantage is that FFA has a strict and formal definition. However, the author fails to explain why the Shapley Value lacks a strict and formal definition, despite it being commonly regarded as the ground truth value in feature attributions. I am unable to discern the distinction between the Shapley Value and FFA in terms of a strict and formal definition, as well as why the Shapley Value cannot provide formal feature attributions. Additionally, claiming that percentage-wise feature attributions are more user-friendly without conducting user experiment studies or related experiments is unsubstantiated. Insufficient case studies on image data are inadequate to provide support.
2. There are a lot of advancements discussing the issues of out-of-distribution sampling. It would be better to see the comparison between these series of methods, making the proposed approach more convincing, as FFA is claimed to address the limitations of out-of-distribution sampling (see line 140 to 148).
3. LIME and SHAP are two classical baselines to provide a model explanation but obtain inferior performance on image datasets. It is highly encouraged the authors compare with some SOTA Shapley-based methods on image data, such as DeepSHAP[1], FastSHAP[2], or CoRTX[3]. 
4. As for the experimental results in Figure 3, it is very limited for authors to use only one example to reveal that FFA is better than LIME and SHAP. Furthermore, Table 1 only demonstrates LIME and SHAP fail to get close enough to FFA, which is expected as neither of them is designed to approximate FFA. However, the conclusion in Table 1 does not mean that FFA is a better indicator than LIME, SHAP, or Shapley Values in providing a formal explanation. I did not see any direct evidence to support this point. 

[1] Delivering Trustworthy AI through Formal XAI. Marques-Silva et al. AAAI 2022.
[2] A unified approach to interpreting model predictions. Lundberg et al. NeurIPS 2017.
[3] FastSHAP: Real-Time Shapley Value Estimation. Neil et al. ICLR 2022.
[4] CoRTX: Contrastive Framework for Real-time Explanation. Chuang et al. ICLR 2023.
","1. The proposed FFA only focuses on the classification task, which makes it limited to be applied in several other common tasks, such as the regression task. 
2. The paper lacks clear illustrations of experiment settings and does not compare with relevant advancements, leading to unconvincing results for verifying the proposed hypothesis. More detailed descriptions of the experiment settings and comparative analyses with related advancements are necessary to strengthen the study's validity.
3. Several annotations and definitions are not clearly illustrated. For example, the “right arrow” in Equation 1 and the definition of “formal explanation” is not well-illustrated in this work. This makes the work hard to follow. 
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688707148676,,,NeurIPS.cc/2023/Conference,3b5e2AFs7f,"['NeurIPS.cc/2023/Conference/Submission12057/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411360301,[],4,,,['everyone'],3b5e2AFs7f,['NeurIPS.cc/2023/Conference/Submission12057/Reviewer_aL7B'],1688707148676,1702411360301,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12057/Reviewer_aL7B']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that the authors should be aware of or compare their work with, including:

1. ""Delivering Trustworthy AI through Formal XAI"" by Marques-Silva et al. (AAAI 2022) [1]
2. ""A unified approach to interpreting model predictions"" by Lundberg et al. (NeurIPS 2017) [2]
3. ""FastSHAP: Real-Time Shapley Value Estimation"" by Neil et al. (ICLR 2022) [3]
4. ""CoRTX: Contrastive Framework for Real-time Explanation"" by Chuang et al. (ICLR 2023) [4]

The reviewer suggests that the authors should compare their proposed approach (FFA) with these state-of-the-art (SOTA) Shapley-based methods, particularly on image data, to make their approach more convincing.",1,"2022, 2017, 2022, 2023",A unified approach to interpreting model predictions 
2XEeDtIbzh,"This work proposes a new approach called formal feature attribution (FFA), inspired by successful FXAI methods, to compute feature attribution scores. FFA is defined as the proportion of explanations where a feature occurs. Experiments try to demonstrate the effectiveness of FFA compared to SHAP and LIME under several public datasets. ","1. The authors proposed a new perspective on providing feature attributions. They did point out some limitations of existing popular methods.
2. The proposed FFA is straightforward and with clear motivations. 
","1. Important questions regarding the reasonableness of the proposed method are unanswered. For instance, the three advantage properties of FFA claimed in this paper are not well reasoned. 
2. The experiments provided in this work are hard to support FFA is better than other existing XAI scoring methods. 
3. Several definitions and annotations throughout the text lack proper illustration or clarification, which can hinder the reader's understanding. Without clear explanations, it becomes challenging to grasp the intended meanings and implications of these terms and annotations.
","1. The authors claim that FFA gains three nice properties. However, the descriptions of those properties are too vague to understand. The authors are highly encouraged to explain more details about those points. Some example concerns are illustrated as follows. The first advantage is that FFA has a strict and formal definition. However, the author fails to explain why the Shapley Value lacks a strict and formal definition, despite it being commonly regarded as the ground truth value in feature attributions. I am unable to discern the distinction between the Shapley Value and FFA in terms of a strict and formal definition, as well as why the Shapley Value cannot provide formal feature attributions. Additionally, claiming that percentage-wise feature attributions are more user-friendly without conducting user experiment studies or related experiments is unsubstantiated. Insufficient case studies on image data are inadequate to provide support.
2. There are a lot of advancements discussing the issues of out-of-distribution sampling. It would be better to see the comparison between these series of methods, making the proposed approach more convincing, as FFA is claimed to address the limitations of out-of-distribution sampling (see line 140 to 148).
3. LIME and SHAP are two classical baselines to provide a model explanation but obtain inferior performance on image datasets. It is highly encouraged the authors compare with some SOTA Shapley-based methods on image data, such as DeepSHAP[1], FastSHAP[2], or CoRTX[3]. 
4. As for the experimental results in Figure 3, it is very limited for authors to use only one example to reveal that FFA is better than LIME and SHAP. Furthermore, Table 1 only demonstrates LIME and SHAP fail to get close enough to FFA, which is expected as neither of them is designed to approximate FFA. However, the conclusion in Table 1 does not mean that FFA is a better indicator than LIME, SHAP, or Shapley Values in providing a formal explanation. I did not see any direct evidence to support this point. 

[1] Delivering Trustworthy AI through Formal XAI. Marques-Silva et al. AAAI 2022.
[2] A unified approach to interpreting model predictions. Lundberg et al. NeurIPS 2017.
[3] FastSHAP: Real-Time Shapley Value Estimation. Neil et al. ICLR 2022.
[4] CoRTX: Contrastive Framework for Real-time Explanation. Chuang et al. ICLR 2023.
","1. The proposed FFA only focuses on the classification task, which makes it limited to be applied in several other common tasks, such as the regression task. 
2. The paper lacks clear illustrations of experiment settings and does not compare with relevant advancements, leading to unconvincing results for verifying the proposed hypothesis. More detailed descriptions of the experiment settings and comparative analyses with related advancements are necessary to strengthen the study's validity.
3. Several annotations and definitions are not clearly illustrated. For example, the “right arrow” in Equation 1 and the definition of “formal explanation” is not well-illustrated in this work. This makes the work hard to follow. 
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688707148676,,,NeurIPS.cc/2023/Conference,3b5e2AFs7f,"['NeurIPS.cc/2023/Conference/Submission12057/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411360301,[],4,,,['everyone'],3b5e2AFs7f,['NeurIPS.cc/2023/Conference/Submission12057/Reviewer_aL7B'],1688707148676,1702411360301,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12057/Reviewer_aL7B']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that the authors should be aware of or compare their work with, including:

1. ""Delivering Trustworthy AI through Formal XAI"" by Marques-Silva et al. (AAAI 2022) [1]
2. ""A unified approach to interpreting model predictions"" by Lundberg et al. (NeurIPS 2017) [2]
3. ""FastSHAP: Real-Time Shapley Value Estimation"" by Neil et al. (ICLR 2022) [3]
4. ""CoRTX: Contrastive Framework for Real-time Explanation"" by Chuang et al. (ICLR 2023) [4]

The reviewer suggests that the authors should compare their proposed approach (FFA) with these state-of-the-art (SOTA) Shapley-based methods, particularly on image data, to make their approach more convincing.",1,"2022, 2017, 2022, 2023",FastSHAP Real-Time Shapley Value Estimation 
2XEeDtIbzh,"This work proposes a new approach called formal feature attribution (FFA), inspired by successful FXAI methods, to compute feature attribution scores. FFA is defined as the proportion of explanations where a feature occurs. Experiments try to demonstrate the effectiveness of FFA compared to SHAP and LIME under several public datasets. ","1. The authors proposed a new perspective on providing feature attributions. They did point out some limitations of existing popular methods.
2. The proposed FFA is straightforward and with clear motivations. 
","1. Important questions regarding the reasonableness of the proposed method are unanswered. For instance, the three advantage properties of FFA claimed in this paper are not well reasoned. 
2. The experiments provided in this work are hard to support FFA is better than other existing XAI scoring methods. 
3. Several definitions and annotations throughout the text lack proper illustration or clarification, which can hinder the reader's understanding. Without clear explanations, it becomes challenging to grasp the intended meanings and implications of these terms and annotations.
","1. The authors claim that FFA gains three nice properties. However, the descriptions of those properties are too vague to understand. The authors are highly encouraged to explain more details about those points. Some example concerns are illustrated as follows. The first advantage is that FFA has a strict and formal definition. However, the author fails to explain why the Shapley Value lacks a strict and formal definition, despite it being commonly regarded as the ground truth value in feature attributions. I am unable to discern the distinction between the Shapley Value and FFA in terms of a strict and formal definition, as well as why the Shapley Value cannot provide formal feature attributions. Additionally, claiming that percentage-wise feature attributions are more user-friendly without conducting user experiment studies or related experiments is unsubstantiated. Insufficient case studies on image data are inadequate to provide support.
2. There are a lot of advancements discussing the issues of out-of-distribution sampling. It would be better to see the comparison between these series of methods, making the proposed approach more convincing, as FFA is claimed to address the limitations of out-of-distribution sampling (see line 140 to 148).
3. LIME and SHAP are two classical baselines to provide a model explanation but obtain inferior performance on image datasets. It is highly encouraged the authors compare with some SOTA Shapley-based methods on image data, such as DeepSHAP[1], FastSHAP[2], or CoRTX[3]. 
4. As for the experimental results in Figure 3, it is very limited for authors to use only one example to reveal that FFA is better than LIME and SHAP. Furthermore, Table 1 only demonstrates LIME and SHAP fail to get close enough to FFA, which is expected as neither of them is designed to approximate FFA. However, the conclusion in Table 1 does not mean that FFA is a better indicator than LIME, SHAP, or Shapley Values in providing a formal explanation. I did not see any direct evidence to support this point. 

[1] Delivering Trustworthy AI through Formal XAI. Marques-Silva et al. AAAI 2022.
[2] A unified approach to interpreting model predictions. Lundberg et al. NeurIPS 2017.
[3] FastSHAP: Real-Time Shapley Value Estimation. Neil et al. ICLR 2022.
[4] CoRTX: Contrastive Framework for Real-time Explanation. Chuang et al. ICLR 2023.
","1. The proposed FFA only focuses on the classification task, which makes it limited to be applied in several other common tasks, such as the regression task. 
2. The paper lacks clear illustrations of experiment settings and does not compare with relevant advancements, leading to unconvincing results for verifying the proposed hypothesis. More detailed descriptions of the experiment settings and comparative analyses with related advancements are necessary to strengthen the study's validity.
3. Several annotations and definitions are not clearly illustrated. For example, the “right arrow” in Equation 1 and the definition of “formal explanation” is not well-illustrated in this work. This makes the work hard to follow. 
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688707148676,,,NeurIPS.cc/2023/Conference,3b5e2AFs7f,"['NeurIPS.cc/2023/Conference/Submission12057/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411360301,[],4,,,['everyone'],3b5e2AFs7f,['NeurIPS.cc/2023/Conference/Submission12057/Reviewer_aL7B'],1688707148676,1702411360301,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12057/Reviewer_aL7B']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that the authors should be aware of or compare their work with, including:

1. ""Delivering Trustworthy AI through Formal XAI"" by Marques-Silva et al. (AAAI 2022) [1]
2. ""A unified approach to interpreting model predictions"" by Lundberg et al. (NeurIPS 2017) [2]
3. ""FastSHAP: Real-Time Shapley Value Estimation"" by Neil et al. (ICLR 2022) [3]
4. ""CoRTX: Contrastive Framework for Real-time Explanation"" by Chuang et al. (ICLR 2023) [4]

The reviewer suggests that the authors should compare their proposed approach (FFA) with these state-of-the-art (SOTA) Shapley-based methods, particularly on image data, to make their approach more convincing.",1,"2022, 2017, 2022, 2023",CoRTX Contrastive Framework for Real-time Explanation
1RHgGhZKt6,"In the paper, a novel method for view synthesis is proposed. More specifically, for given posed RGB input images, multi-resolution hash grid features are trained which are rendered to the image plane via volume rendering. The rendered feature is that passed through an MLP, processed with Spherical Harmonics (SH) feature encoding schemes, and then the final color is predicted. The main difference of the proposed system to previous NeRF models is that in 3D space, only feature grids without any MLP layers are optimized, and the view-dependent color prediction is only performed on the rendered features, i.e. ""deferred shading"" is applied. ","- Results: I believe the main strengths of the paper is that the proposed system leads to good results, i.e. the proposed system outperforms SOTA methods such as Zip-NeRF or Instant-NGP on the two dataset types in both, the reported training times as well as the reported view synthesis quality.

- Ablation Study: A rather extensive ablation study is performed which I believe makes the manuscript stronger. It highlights the importance of the various components, and helps the reader to understand where the speed / quality improvements are coming from.

- Limitation Section: The manuscript contains a rather extensive limitation section which is beneficial as it portrays a more complete impression of the method. 

- Manuscript organization: The manuscript is organized coherently and the information flow is rather easy to understand. ","- Time comparison (L. 216, L. 229): The time comparison is, if I understood this correctly, not reported with running the methods on the same hardware, e.g. different GPUs (RTX 3090 vs V100) are used, and also times are simply adopted from other papers. As the time complexity is the main selling point of this work, I believe this should be reported more accurately by running the methods on the exact same hardware / setup. 

- Qualitative Results - Animations: While the reported method achieves high-quality view synthesis performance measured by per-image metrics, the ""smoothness"" / ""3d consistency"" between frames is not shown. As the proposed system uses features instead of directly color predictions, the consistency between frames might be lower than for previous methods while not having a significant influence on the per-image metrics. It would there be very helpful to also show animations, e.g. in the form of fly-through videos, to compare methods with this focus.

- Unclear how the “pilot network” is used: Is simply the MLP size changed, or is this operating as the “default NeRF” models, i.e. per-point MLP calls are performed, and then color/density is aggregated via volume rendering? It would be helpful if the description of the pilot network would be made more precise. 

- Figure 3: Unclear. What exactly is happening in the individual blocks? Is a per SH feature predicted, and then they are summed together? What is the “bottleneck arrow” indicating? Why is this skip connection actually necessary? The exact math formulations is also not contained in the respective Section 3.4. It would be helpful if the description of the deferred rendering part is more precise and clear formulas are used so that the reader can better understand the exact technique.

- Density prediction: How is the density obtained? Is this stored, similarly to the features, in a 3D hash grid?

- Qualitative Comparison - Figure 5: I think in Figure 5, also results from Zip-NeRF should be shown.

Typos / Unclear passages: 

- L. 5: either in occupancy grids -> either in the form of occupancy grids
- L. 8: after neural network evaluation -> after neural network evaluations
- L. 19: View synthesis is a task that synthesizes -> For the task of view synthesis, unobserved views are synthesized …
- L. 36: near surface -> near the surface
- L. 39: standard volume rendering technique -> standard volume rendering techniques
- L. 58: NN runnings -> NN calls
- L. 61: to enable one sample on the surface -> to enable one sample-based rendering
- L. 69: modeling the specular effect -> modeling specular effects
- L. 96: represent F by use of MLPs. -> represent F by using MLPs
- L. 97: as N times MLP evaluations are required -> as N MLP evaluations are required
- L. 97/98: Alternatively, recent research shows that -> please add respective citation
- L. 100: organized in the forms of -> organized in the form of
- L. 110: the many times NN running is … -> the many NN calls are …
- L. 111: the importance of a large NN in realistic rendering -> the importance of a large NNs for realistic rendering
- L. 112 - 114: “Considering the fact that valuable samples on the surface are close in terms of spatial position, the queried feature vectors are also very similar as they are interpolated according to their position” -> unclear meaning
- L. 213: reports on the rendering -> reports the rendering
- L. 223: Using learning features organized in hash grids -> Using learned features organized in hash grids
- L. 249: Larger MLP (B, C) -> B does not indicate a “larger MLP” if I understood these symbols correctly. I would also suggest to structure the names and ablation study slightly different, e.g. with the typical “Baseline”, “+ GeLU”, “+larger MLP”, etc naming convention; currently, it is very cryptic and hard to understand.

Missing Citations:
- PlenOctrees For Real-time Rendering of Neural Radiance Fields, Yu et al., ICCV 2021
- Plenoxels Radiance Fields without Neural Networks, Yu et al., CVPR 2022","- Are the time comparisons performed with respect to different hardware, e.g. RTX 3090 vs V100 GPUs?
- Could the authors expand on the ""3D consistency"" / ""smoothness"" of in-between frames when rendering a fly-through, compared to non-feature-based methods such as ZipNeRF?
- How exactly is the pilot network used? Is this predicting density and color per 3D sample point, or is this a smaller network but also operating on the rendered feature?
- Could the authors explain the SH-based deferred rendering technique more precise?
- Is the density predicted in 3D and stored in 3D hash grids, similar to the features?","- The authors discuss limitations of their work in the main manuscript. They mainly discuss the limitation of per-scene optimization and comparably slow / not yet real-time rendering times. I believe the limitation discussion is OK like it is; if some interesting failure mode could be illustrated with a figure, this could be interesting to add to the manuscript. ",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688644192353,,,NeurIPS.cc/2023/Conference,3aVZhMfsyz,"['NeurIPS.cc/2023/Conference/Submission6742/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411076462,[],4,,,['everyone'],3aVZhMfsyz,['NeurIPS.cc/2023/Conference/Submission6742/Reviewer_4LZz'],1688644192353,1702411076462,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission6742/Reviewer_4LZz']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. ""PlenOctrees For Real-time Rendering of Neural Radiance Fields"" by Yu et al., ICCV 2021
2. ""Plenoxels Radiance Fields without Neural Networks"" by Yu et al., CVPR 2022

Additionally, the reviewer also mentions that a citation is needed for the statement ""Alternatively, recent research shows that"" on lines 97/98, but does not specify a particular paper.",1,"2021, 2022",PlenOctrees For Real-time Rendering of Neural Radiance Fields 
1RHgGhZKt6,"In the paper, a novel method for view synthesis is proposed. More specifically, for given posed RGB input images, multi-resolution hash grid features are trained which are rendered to the image plane via volume rendering. The rendered feature is that passed through an MLP, processed with Spherical Harmonics (SH) feature encoding schemes, and then the final color is predicted. The main difference of the proposed system to previous NeRF models is that in 3D space, only feature grids without any MLP layers are optimized, and the view-dependent color prediction is only performed on the rendered features, i.e. ""deferred shading"" is applied. ","- Results: I believe the main strengths of the paper is that the proposed system leads to good results, i.e. the proposed system outperforms SOTA methods such as Zip-NeRF or Instant-NGP on the two dataset types in both, the reported training times as well as the reported view synthesis quality.

- Ablation Study: A rather extensive ablation study is performed which I believe makes the manuscript stronger. It highlights the importance of the various components, and helps the reader to understand where the speed / quality improvements are coming from.

- Limitation Section: The manuscript contains a rather extensive limitation section which is beneficial as it portrays a more complete impression of the method. 

- Manuscript organization: The manuscript is organized coherently and the information flow is rather easy to understand. ","- Time comparison (L. 216, L. 229): The time comparison is, if I understood this correctly, not reported with running the methods on the same hardware, e.g. different GPUs (RTX 3090 vs V100) are used, and also times are simply adopted from other papers. As the time complexity is the main selling point of this work, I believe this should be reported more accurately by running the methods on the exact same hardware / setup. 

- Qualitative Results - Animations: While the reported method achieves high-quality view synthesis performance measured by per-image metrics, the ""smoothness"" / ""3d consistency"" between frames is not shown. As the proposed system uses features instead of directly color predictions, the consistency between frames might be lower than for previous methods while not having a significant influence on the per-image metrics. It would there be very helpful to also show animations, e.g. in the form of fly-through videos, to compare methods with this focus.

- Unclear how the “pilot network” is used: Is simply the MLP size changed, or is this operating as the “default NeRF” models, i.e. per-point MLP calls are performed, and then color/density is aggregated via volume rendering? It would be helpful if the description of the pilot network would be made more precise. 

- Figure 3: Unclear. What exactly is happening in the individual blocks? Is a per SH feature predicted, and then they are summed together? What is the “bottleneck arrow” indicating? Why is this skip connection actually necessary? The exact math formulations is also not contained in the respective Section 3.4. It would be helpful if the description of the deferred rendering part is more precise and clear formulas are used so that the reader can better understand the exact technique.

- Density prediction: How is the density obtained? Is this stored, similarly to the features, in a 3D hash grid?

- Qualitative Comparison - Figure 5: I think in Figure 5, also results from Zip-NeRF should be shown.

Typos / Unclear passages: 

- L. 5: either in occupancy grids -> either in the form of occupancy grids
- L. 8: after neural network evaluation -> after neural network evaluations
- L. 19: View synthesis is a task that synthesizes -> For the task of view synthesis, unobserved views are synthesized …
- L. 36: near surface -> near the surface
- L. 39: standard volume rendering technique -> standard volume rendering techniques
- L. 58: NN runnings -> NN calls
- L. 61: to enable one sample on the surface -> to enable one sample-based rendering
- L. 69: modeling the specular effect -> modeling specular effects
- L. 96: represent F by use of MLPs. -> represent F by using MLPs
- L. 97: as N times MLP evaluations are required -> as N MLP evaluations are required
- L. 97/98: Alternatively, recent research shows that -> please add respective citation
- L. 100: organized in the forms of -> organized in the form of
- L. 110: the many times NN running is … -> the many NN calls are …
- L. 111: the importance of a large NN in realistic rendering -> the importance of a large NNs for realistic rendering
- L. 112 - 114: “Considering the fact that valuable samples on the surface are close in terms of spatial position, the queried feature vectors are also very similar as they are interpolated according to their position” -> unclear meaning
- L. 213: reports on the rendering -> reports the rendering
- L. 223: Using learning features organized in hash grids -> Using learned features organized in hash grids
- L. 249: Larger MLP (B, C) -> B does not indicate a “larger MLP” if I understood these symbols correctly. I would also suggest to structure the names and ablation study slightly different, e.g. with the typical “Baseline”, “+ GeLU”, “+larger MLP”, etc naming convention; currently, it is very cryptic and hard to understand.

Missing Citations:
- PlenOctrees For Real-time Rendering of Neural Radiance Fields, Yu et al., ICCV 2021
- Plenoxels Radiance Fields without Neural Networks, Yu et al., CVPR 2022","- Are the time comparisons performed with respect to different hardware, e.g. RTX 3090 vs V100 GPUs?
- Could the authors expand on the ""3D consistency"" / ""smoothness"" of in-between frames when rendering a fly-through, compared to non-feature-based methods such as ZipNeRF?
- How exactly is the pilot network used? Is this predicting density and color per 3D sample point, or is this a smaller network but also operating on the rendered feature?
- Could the authors explain the SH-based deferred rendering technique more precise?
- Is the density predicted in 3D and stored in 3D hash grids, similar to the features?","- The authors discuss limitations of their work in the main manuscript. They mainly discuss the limitation of per-scene optimization and comparably slow / not yet real-time rendering times. I believe the limitation discussion is OK like it is; if some interesting failure mode could be illustrated with a figure, this could be interesting to add to the manuscript. ",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688644192353,,,NeurIPS.cc/2023/Conference,3aVZhMfsyz,"['NeurIPS.cc/2023/Conference/Submission6742/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411076462,[],4,,,['everyone'],3aVZhMfsyz,['NeurIPS.cc/2023/Conference/Submission6742/Reviewer_4LZz'],1688644192353,1702411076462,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission6742/Reviewer_4LZz']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. ""PlenOctrees For Real-time Rendering of Neural Radiance Fields"" by Yu et al., ICCV 2021
2. ""Plenoxels Radiance Fields without Neural Networks"" by Yu et al., CVPR 2022

Additionally, the reviewer also mentions that a citation is needed for the statement ""Alternatively, recent research shows that"" on lines 97/98, but does not specify a particular paper.",1,"2021, 2022",Plenoxels Radiance Fields without Neural Networks
EzS13n9gKO,The paper presents a learning-based framework on the well-studied neural surface reconstruction problem. The key contribution of this paper is to take the complex photon-particle interaction into account and present a more generalized pipeline rather than relying on volume rendering. The proposed framework released the power of the transformer to achieve enhanced feature representation of sampled points along the ray. Experiments on several popular benchmarks have shown the effectiveness of the proposed approach. ,"(1) The idea of modeling the complex light transport and releasing the flexibility from regular volume rendering is novel and interesting.
(2) Overall, the paper is well presented and easy to follow.
(3) The paper has achieved state-of-the-art generalizable neural surface reconstruction performance across different datasets.
","(1) (Major) The idea of using 3D feature volumes with hybrid resolution is not new and has been proposed in NeuralRecon (https://arxiv.org/pdf/2104.00681.pdf). Besides, since there are two major differences (the elimination of FPN and the construction of multi-level projected feature maps) between the proposed hybrid extraction and the original one, it is better to make separate ablation studies to further verify the effect of the two variations.
(2) (Major) For the ablation study of the occlusion transformer, compared with directly removing this module, a better ablation way is to attend every point’s feature as the input of the key embedding in the self-attention computation. This way ensures a fair setting (roughly same architecture and complexity) and the only difference is whether the later points contribute to the former ones.
(3) (Minor) Visual comparison on view synthesis with other methods: since one of the main motivations of this work is to model the photon-particle interaction, I assume a major outcome is more robust rendering against the variations (blur, specular..) from input views. Thus, it is better to show some visual comparison with other baselines (SparseNeus, VolRecon, NeRF…) to verify this point.
(4) (Minor)The main diagram (Figure 2) can be displayed in a clearer and more elegant way. Basically the part of transformer details would belong to either occlusion transformer or render transformer, rather than ‘feature fusion’. Besides, there is ambiguity on what the patches with different colors stand for.
","Overall I think the idea of the paper is novel and technically sound, presenting another perspective of the mainstream methods based on volume rendering. Despite some technical concerns listed in weaknesses, now I lean toward accepting this paper. I expect authors to address my concerns by providing more comprehensive experiments to further show the effectiveness of this work.",Authors have discussed their limitations on rendering speed. I think another limitation is that the SOTA neural surface reconstruction method is still only comparable with a classic MVS-based baseline (MVSNet) at this time. But MVSNet and its extensions must be much faster to get a depth map than the rendering-based ones. So there is a long way to go for this area to further release the power of implicit representation.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,2 fair,3 good,1687456940218,,,NeurIPS.cc/2023/Conference,3ZICE99e6n,"['NeurIPS.cc/2023/Conference/Submission3051/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410879673,[],1,,,['everyone'],3ZICE99e6n,['NeurIPS.cc/2023/Conference/Submission3051/Reviewer_1Qeo'],1687456940218,1702410879673,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3051/Reviewer_1Qeo']","Yes, the peer review explicitly suggests the authors of the paper to be aware of a specific literature, which is NeuralRecon (https://arxiv.org/pdf/2104.00681.pdf), as the idea of using 3D feature volumes with hybrid resolution has been proposed in this work. However, it does not explicitly state that the authors should cite this literature, but rather implies that they should be aware of it and possibly compare or discuss their work in relation to it, especially in the context of the hybrid extraction method.",1,2021,NeuralRecon
6na8q2WvEF,"This paper deals with large graph learning tasks via graph segmentations. More specifically, in each training step, the author sample nodes from graph segmentations and only update parameters related to the selected nodes. To optimize memory consumption, the author further introduced a historical embedding table. To bridge the training and prediction gap, the author also designed a prediction head fine-tuning scheme. Besides, the author provided some theoretical analysis of the proposed method. Empirical results show the proposed method remains good memory efficiency and test accuracy.","1. The paper targets an important problem with high application value, which makes property predictions on Large graphs. The proposed segment training idea looks reasonable and works well practically. 

2. Based on the graph segmentation idea, the author has made comprehensive and detailed consideration of the memory usage, the training-testing gap, and the theoretical analysis of the historical approximation bias, which makes the proposed method more technically sound.

3. The paper is well-organized and easy to follow.","1. The proposed method didn't discuss how to deal with the inter-segmentation training, for example, how to learn a model with link prediction between two nodes from different graph segmentations.

2. About Baselines: The experimental part does not compare with other large-graph learning baselines such as GraphSage[1]. More baselines should be compared to evaluate the effectiveness of proposed GST.

Reference:
[1] GraphSage: Representation Learning on Large Graphs, NIPS 2017.",1. How to adapt the proposed method with graph properties learning that are between segmentations?,"The proposed method can process large-scale graphs, which might be used for user information mining on large social networks. There are potential risks to user privacy when applying such methods.",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688705893468,,,NeurIPS.cc/2023/Conference,3YDukx2cpr,"['NeurIPS.cc/2023/Conference/Submission1145/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410771446,[],4,,,['everyone'],3YDukx2cpr,['NeurIPS.cc/2023/Conference/Submission1145/Reviewer_rf6m'],1688705893468,1702410771446,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1145/Reviewer_rf6m']","Yes, the peer review explicitly suggests the authors of the paper to cite the literature ""GraphSage: Representation Learning on Large Graphs, NIPS 2017"" as a baseline for comparison. This is mentioned in the weaknesses section, point 2, where the reviewer states ""More baselines should be compared to evaluate the effectiveness of proposed GST"" and specifically references GraphSage.",1,2017,GraphSage: Representation Learning on Large Graphs
yM8lbgiT7l,"The paper proposes an innovative approach for non-linear tensor decomposition. It utilizes the deep energy-based model (EBM) to model the joint energy function of tensor observations and latent factors. This design enables a more flexible decomposition without the need for explicitly defining the interaction between the latent factor and the commonly used Gaussian prior. Furthermore, the expressive nature of the energy function allows for the modeling of side information, such as timestamps.","The paper presents a clear motivation and introduces innovative and sophisticated solutions. To my knowledge, this is the first work to adopt Evidence-Based Medicine (EBM) for tensor decomposition without the need for an explicit design of the decomposition form.

Furthermore, the adaptability of the proposed method for dynamic tensors is impressive. The experimental setup is robust and comprehensive, demonstrating the thoroughness of the research.","1. Instead of using NON-FAT or BCTT for experiments on dynamic tensors' decomposition and visualization, it would be more appropriate to choose THIS-ODE[1] as the baseline. While NON-FAT and BCTT are designed for modeling temporal dynamics of factors in a tensor, THIS-ODE aligns with the proposed model by having an entry-wise dynamic design.
 
2. It appears that the objective function involves entry-wise importance sampling, and completing the posterior will require another round of entry-wise sampling. I am unsure if this will lead to training and inference costs becoming problematic, especially in scenarios with a large number of entries. It would be beneficial to have a more in-depth discussion about the scalability aspect.


[1] Decomposing Temporal High-Order Interactions via Latent ODEs, Shibo Li, Robert M. Kirby, and Shandian Zhe, The 39th International Conference on Machine Learning, 2022",See weakness,See weakness,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1688599182982,,,NeurIPS.cc/2023/Conference,3NWWgB2SuF,"['NeurIPS.cc/2023/Conference/Submission10889/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411305886,[],4,,,['everyone'],3NWWgB2SuF,['NeurIPS.cc/2023/Conference/Submission10889/Reviewer_buha'],1688599182982,1702411305886,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10889/Reviewer_buha']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, namely ""Decomposing Temporal High-Order Interactions via Latent ODEs"" by Shibo Li, Robert M. Kirby, and Shandian Zhe, published in The 39th International Conference on Machine Learning, 2022, referred to as ""THIS-ODE"" in the review. The reviewer recommends using THIS-ODE as a baseline for experiments on dynamic tensors' decomposition and visualization instead of NON-FAT or BCTT.",1,2022,Decomposing Temporal High-Order Interactions via Latent ODEs
WzZuYoUbAN,"This paper demonstrates how post hoc explanations through a small LM could assist the performance of LLMs. The process is divided into 4 steps: Proxy Model Selection, Few-shot sample selection, compute explanations, and formatting prompts for LLMs. This technique automatically generates few-shot demonstrations, reducing human-annotation for few-shot in-context learning.","- The method is sound and novel. 
- The paper contains extensive ablation experiments.
- The improvement for some tasks is dramatic. (e.g. snarks)
","- This paper does not compare the performance with other baselines such as Auto-CoT that automatically generate demonstrations for in-context learning. 
Zhang et al (2022) Automatic Chain of Thought Prompting in Large Language Models 

- Although it is stated that the experimentation is specifically carried out on datasets aimed at evaluating complex linguistic understanding concepts, it omits datasets such as DisambiguationQA, Hyperbaton, and Word Soriting of Big-Bench-Hard benchmark. 

- When GPT-2 is not fine-tuned (E=0) in Table 2, AMPLIFY outperforms both AO and CoT on 3 out of 7 datasets for both GPT-3 and GPT-3.5, indicating that fine-tuning process is crucial. Also, the performance gets worse compared to AO for datasets such as formal fallacies, commonsenseQA, and ruin names for GPT-3.5. 

","- When fine-tuning GPT-2, what training datasets are used? As far as I know, tasks of Big-Bench-Hard do not contain training instances.

- Do you expect that AMPLIFY would be still effective for models with less than 100B parameters? Also, do you think the effect of AMPLIFY could generalize to open-source LLMs?

- What is the default s value (number of shots) for GPT-3, GPT-3.5 result of Table 1?",The authors adequately addressed the limitations and potential negative societal impact of their work.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688300494531,,,NeurIPS.cc/2023/Conference,3H37XciUEv,"['NeurIPS.cc/2023/Conference/Submission2928/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410872566,[],2,,,['everyone'],3H37XciUEv,['NeurIPS.cc/2023/Conference/Submission2928/Reviewer_4Npw'],1688300494531,1702410872566,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2928/Reviewer_4Npw']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely ""Zhang et al (2022) Automatic Chain of Thought Prompting in Large Language Models"", also referred to as Auto-CoT, as a baseline for comparison. The reviewer mentions that the paper does not compare its performance with other baselines such as Auto-CoT, implying that the authors should consider including this comparison and citing the relevant literature.",1,2022,Automatic Chain of Thought Prompting in Large Language Models
svYyoqf7Q5,"The paper presents AMPLIFY, an approach that uses post-hoc explanations from a proxy model to improve the prompting performance of large language models. For a given dataset, the approach assumes access to a set of labeled validation data that is used for crafting a prompt. First, the approach selects k examples that are misclassified by LLMs and exhibit high misclassification confidence scores. Next, post-hoc explanation techniques are used to find the most important input features of these selected examples, which are later used to construct template-based rationales. The rationales are used in the final prompts following the chain-of-thought paradigm.

The paper conducts experiments on 7 datasets from Big-Bench-Hard. Experimental results suggest good improvements over the few-shot answer-only prompting (AO) baseline and the few-shot CoT baseline. The paper also includes analyses suggesting the importance of example selection strategies as well as the impacts of using different proxy models and explanation techniques.
","The paper studies an interesting problem that focuses on using post-hoc explanations for improving LLMs’ performance, which is relatively new.

The experiments cover 7 datasets and show performance improvements compared to few-shot baselines.

The paper provides some useful analyses, especially the analysis of the example selection strategies.

The paper is well-written and easy to follow.
","1: The major comparison is unfair in some way

The proposed approach uses a validation set (the size of the validation set is also not mentioned in the paper, nor in the appendix) and selects examples from the validation set. By contrast, standard prompting and chain-of-thought prompting typically only use few-shot examples. So part of the improvements might also be attributed to the use of more data in addition to the prompting method proposed in this paper (see weakness 2). In addition, the paper does not compare against approaches that can also use the validation set instead of just a few-shot examples.

2: However, the experiments do not clearly suggest the effectiveness of using post-hoc explanations

The proposed approach uses the LLM itself + smaller proxy models to find more informative examples to be used in the prompts (the selection strategy). This active selection strategy does not rely on using post-hoc explanations and can be applied to answer-only prompting or chain-of-thought prompting as well. It would be good to provide AO and CoT performance using the selected examples as well to give a better understanding of the effectiveness of using better examples versus the effectiveness of using post-hoc explanations in prompts.

The paper does provide analysis of the impacts of using different selection strategies. As shown in Table 3, using random examples, the performance of AMPLIFY is only 59.3 on GPT3 and 62.0 on GPT-3.5; at the same time, according to Table 1, CoT performance is 58.0 on GPT3 and 62.9 on GPT-3.5. IIUC, this suggests CoT and AMPLIFY is comparable; and selecting better examples contribute the majority of the performance improvements.

While it seems that most of the improvements come from the selection strategy, the paper does not compare against other approaches that involve active annotating examples for in-context learning (e.g., Su et al. (2022); Diao et al. (2023)). Also, this proposed selective annotation strategy does not distinguish itself from existing work, which uses similar confidence-based ways to actively annotate examples (Su et al., 2022; Diao et al., 2023).

Selective Annotation Makes Language Models Better Few-Shot Learners (Su et al., 2022)
Active Prompting with Chain-of-Thought for Large Language Models (Diao et al., 2023)

3: The paper also misses some important details on the approach and the baseline
Regarding the proposed approach:
* When getting the post-hoc explanations. Does it explain the ground truth label or the predicted label? (it is not clearly explained in line 172)
* How does the method get the initial predictions, in order to determine the misclassified examples? Is it obtained using few-shot AO or few-shot CoT?
* It seems no information regarding the size of the validation set is provided.

Regarding the baselines:
* It seems the CoT baselines and AO baselines use few-shot prompting. How is the shot selected? Are they randomly selected? How many shots are used?
* How are the CoTs for the CoT baseline written? Based on line 129, the CoTs are taken from Wei et al. (2022), but the original CoT paper does not include prompts for these datasets. Are they from Suzgun et al. (2022)?

In addition, it would be also helpful if the appendix can include some prompt examples.

4: The paper uses a subset from big-bench-hard that mainly focuses on understanding linguistic concepts.

On these datasets, CoT typically leads to no improvements or minor improvements, compared to AO. While it is understandable that the paper chooses to focus on understanding linguistics concepts, it would still be beneficial to benchmark the effectiveness of this approach on a broader range of multi-step reasoning tasks (e.g., GSM) where CoT shows substantial improvements over AO. And on these tasks, using rationales consisting of just top-k words may be less effective. 


",See weakness,The paper discusses the limitations in appendix.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688448219277,,,NeurIPS.cc/2023/Conference,3H37XciUEv,"['NeurIPS.cc/2023/Conference/Submission2928/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410872485,[],3,,,['everyone'],3H37XciUEv,['NeurIPS.cc/2023/Conference/Submission2928/Reviewer_LsYX'],1688448219277,1702410872485,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2928/Reviewer_LsYX']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Su et al. (2022) - ""Selective Annotation Makes Language Models Better Few-Shot Learners""
2. Diao et al. (2023) - ""Active Prompting with Chain-of-Thought for Large Language Models""
3. Wei et al. (2022) - (mentioned as the source of CoT prompts, but the reviewer notes that the original CoT paper does not include prompts for these datasets)
4. Suzgun et al. (2022) - (mentioned as a possible source of CoT prompts, but the reviewer is unsure)",1,"2022, 2023, 2022, 2022",Selective Annotation Makes Language Models Better Few-Shot Learners
svYyoqf7Q5,"The paper presents AMPLIFY, an approach that uses post-hoc explanations from a proxy model to improve the prompting performance of large language models. For a given dataset, the approach assumes access to a set of labeled validation data that is used for crafting a prompt. First, the approach selects k examples that are misclassified by LLMs and exhibit high misclassification confidence scores. Next, post-hoc explanation techniques are used to find the most important input features of these selected examples, which are later used to construct template-based rationales. The rationales are used in the final prompts following the chain-of-thought paradigm.

The paper conducts experiments on 7 datasets from Big-Bench-Hard. Experimental results suggest good improvements over the few-shot answer-only prompting (AO) baseline and the few-shot CoT baseline. The paper also includes analyses suggesting the importance of example selection strategies as well as the impacts of using different proxy models and explanation techniques.
","The paper studies an interesting problem that focuses on using post-hoc explanations for improving LLMs’ performance, which is relatively new.

The experiments cover 7 datasets and show performance improvements compared to few-shot baselines.

The paper provides some useful analyses, especially the analysis of the example selection strategies.

The paper is well-written and easy to follow.
","1: The major comparison is unfair in some way

The proposed approach uses a validation set (the size of the validation set is also not mentioned in the paper, nor in the appendix) and selects examples from the validation set. By contrast, standard prompting and chain-of-thought prompting typically only use few-shot examples. So part of the improvements might also be attributed to the use of more data in addition to the prompting method proposed in this paper (see weakness 2). In addition, the paper does not compare against approaches that can also use the validation set instead of just a few-shot examples.

2: However, the experiments do not clearly suggest the effectiveness of using post-hoc explanations

The proposed approach uses the LLM itself + smaller proxy models to find more informative examples to be used in the prompts (the selection strategy). This active selection strategy does not rely on using post-hoc explanations and can be applied to answer-only prompting or chain-of-thought prompting as well. It would be good to provide AO and CoT performance using the selected examples as well to give a better understanding of the effectiveness of using better examples versus the effectiveness of using post-hoc explanations in prompts.

The paper does provide analysis of the impacts of using different selection strategies. As shown in Table 3, using random examples, the performance of AMPLIFY is only 59.3 on GPT3 and 62.0 on GPT-3.5; at the same time, according to Table 1, CoT performance is 58.0 on GPT3 and 62.9 on GPT-3.5. IIUC, this suggests CoT and AMPLIFY is comparable; and selecting better examples contribute the majority of the performance improvements.

While it seems that most of the improvements come from the selection strategy, the paper does not compare against other approaches that involve active annotating examples for in-context learning (e.g., Su et al. (2022); Diao et al. (2023)). Also, this proposed selective annotation strategy does not distinguish itself from existing work, which uses similar confidence-based ways to actively annotate examples (Su et al., 2022; Diao et al., 2023).

Selective Annotation Makes Language Models Better Few-Shot Learners (Su et al., 2022)
Active Prompting with Chain-of-Thought for Large Language Models (Diao et al., 2023)

3: The paper also misses some important details on the approach and the baseline
Regarding the proposed approach:
* When getting the post-hoc explanations. Does it explain the ground truth label or the predicted label? (it is not clearly explained in line 172)
* How does the method get the initial predictions, in order to determine the misclassified examples? Is it obtained using few-shot AO or few-shot CoT?
* It seems no information regarding the size of the validation set is provided.

Regarding the baselines:
* It seems the CoT baselines and AO baselines use few-shot prompting. How is the shot selected? Are they randomly selected? How many shots are used?
* How are the CoTs for the CoT baseline written? Based on line 129, the CoTs are taken from Wei et al. (2022), but the original CoT paper does not include prompts for these datasets. Are they from Suzgun et al. (2022)?

In addition, it would be also helpful if the appendix can include some prompt examples.

4: The paper uses a subset from big-bench-hard that mainly focuses on understanding linguistic concepts.

On these datasets, CoT typically leads to no improvements or minor improvements, compared to AO. While it is understandable that the paper chooses to focus on understanding linguistics concepts, it would still be beneficial to benchmark the effectiveness of this approach on a broader range of multi-step reasoning tasks (e.g., GSM) where CoT shows substantial improvements over AO. And on these tasks, using rationales consisting of just top-k words may be less effective. 


",See weakness,The paper discusses the limitations in appendix.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688448219277,,,NeurIPS.cc/2023/Conference,3H37XciUEv,"['NeurIPS.cc/2023/Conference/Submission2928/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410872485,[],3,,,['everyone'],3H37XciUEv,['NeurIPS.cc/2023/Conference/Submission2928/Reviewer_LsYX'],1688448219277,1702410872485,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2928/Reviewer_LsYX']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Su et al. (2022) - ""Selective Annotation Makes Language Models Better Few-Shot Learners""
2. Diao et al. (2023) - ""Active Prompting with Chain-of-Thought for Large Language Models""
3. Wei et al. (2022) - (mentioned as the source of CoT prompts, but the reviewer notes that the original CoT paper does not include prompts for these datasets)
4. Suzgun et al. (2022) - (mentioned as a possible source of CoT prompts, but the reviewer is unsure)",1,"2022, 2023, 2022, 2022",Active Prompting with Chain-of-Thought for Large Language Models
6rDdWwWB8Z,"This paper analyzes diffusion training from an MTL perspective and observes the negative transfer in diffusion training. Several multi-task learning algorithms are employed to address the negative transfer problem, leading to improved performance.","1. The paper presents a detailed analysis of task affinity and negative transfer across different diffusion steps, which is interesting.
2. The incorporation of MTL methods has yielded a substantial improvement in the performance of the diffusion model.","1. The paper lacks a comparison of time and GPU memory usage between the vanilla and MTL approaches. Typically, MTL methods require more time and significantly higher GPU memory.
2. It would be beneficial to include random weights and linear scalarization baselines. Several studies have suggested that linear scalarization and random weights can serve as strong baselines [1][2].

[1] Xin et al., Do Current Multi-Task Optimization Methods in Deep Learning Even Help?, NeurIPS 2022

[2] Lin et al., A closer look at loss weighting in multi-task learning, arXiv 2022
","Have you considered utilizing the weights obtained by MTL approaches as the loss weights for training the model? For instance, using the weighted loss with the mean weight of each interval in Figure 3b. Would this approach yield comparable results?",See weakness.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,4 excellent,4 excellent,1688706194119,,,NeurIPS.cc/2023/Conference,3G2ec833mW,"['NeurIPS.cc/2023/Conference/Submission7067/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411092661,[],2,,,['everyone'],3G2ec833mW,['NeurIPS.cc/2023/Conference/Submission7067/Reviewer_8Ptq'],1688706194119,1702411092661,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7067/Reviewer_8Ptq']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. Xin et al., ""Do Current Multi-Task Optimization Methods in Deep Learning Even Help?"", NeurIPS 2022
2. Lin et al., ""A closer look at loss weighting in multi-task learning"", arXiv 2022

These references are mentioned as supporting the idea that linear scalarization and random weights can serve as strong baselines in multi-task learning. The reviewer is implying that the authors should consider including these baselines in their comparison and potentially citing these works to provide a more comprehensive evaluation of their approach.",1,"2022, 2020",Do Current Multi-Task Optimization Methods in Deep Learning Even Help 
6rDdWwWB8Z,"This paper analyzes diffusion training from an MTL perspective and observes the negative transfer in diffusion training. Several multi-task learning algorithms are employed to address the negative transfer problem, leading to improved performance.","1. The paper presents a detailed analysis of task affinity and negative transfer across different diffusion steps, which is interesting.
2. The incorporation of MTL methods has yielded a substantial improvement in the performance of the diffusion model.","1. The paper lacks a comparison of time and GPU memory usage between the vanilla and MTL approaches. Typically, MTL methods require more time and significantly higher GPU memory.
2. It would be beneficial to include random weights and linear scalarization baselines. Several studies have suggested that linear scalarization and random weights can serve as strong baselines [1][2].

[1] Xin et al., Do Current Multi-Task Optimization Methods in Deep Learning Even Help?, NeurIPS 2022

[2] Lin et al., A closer look at loss weighting in multi-task learning, arXiv 2022
","Have you considered utilizing the weights obtained by MTL approaches as the loss weights for training the model? For instance, using the weighted loss with the mean weight of each interval in Figure 3b. Would this approach yield comparable results?",See weakness.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,4 excellent,4 excellent,1688706194119,,,NeurIPS.cc/2023/Conference,3G2ec833mW,"['NeurIPS.cc/2023/Conference/Submission7067/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411092661,[],2,,,['everyone'],3G2ec833mW,['NeurIPS.cc/2023/Conference/Submission7067/Reviewer_8Ptq'],1688706194119,1702411092661,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7067/Reviewer_8Ptq']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. Xin et al., ""Do Current Multi-Task Optimization Methods in Deep Learning Even Help?"", NeurIPS 2022
2. Lin et al., ""A closer look at loss weighting in multi-task learning"", arXiv 2022

These references are mentioned as supporting the idea that linear scalarization and random weights can serve as strong baselines in multi-task learning. The reviewer is implying that the authors should consider including these baselines in their comparison and potentially citing these works to provide a more comprehensive evaluation of their approach.",1,"2022, 2020",A closer look at loss weighting in multi-task learning
zlhEAE7amr,"The paper introduces an efficient algorithm for learning halfspaces in the testable learning model in which the tester-learner first applies a test on the training data and if the test succeeds the algorithm produces a hypothesis which is guaranteed to be near-optimal. It is required that if the data comes from a target distribution, then the test must succeed with high probability.

The paper considers learning halfspaces in the case where the target distribution is Gaussian (or strongly log-concave) and where the labels are subjected to Massart noise or adversarial noise (i.e., agnostic setting). The paper builds on several ideas from previous papers by Diakonikolas et al.","Learning halfspaces is a fundamental problem in machine learning. Even though it is one of the simplest tasks, the problem becomes non-trivial in the presence of label noise. Recently, there has been a lot of interest in developing algorithms for learning halfspaces in several settings (e.g., Massart noise, ...). The submitted paper is the first work that presents an efficient algorithm for learning halfspaces in the testable learning model of Rubinfeld and Vasilyan.

I find the paper to be clear and generally well-written, and I find the results novel and interesting.","Minor comments/typos:
- Page 2 line 71: ""an important one being that the probability mass of any region close to the origin is proportional to its geometric measure"" -> ""an important one being that the probability mass of any region close to the origin is roughly proportional to its geometric measure"".
- Page 5, line 198: Shouldn't the title of the section be ""Testing properties of isotropic strongly log-concave distributions""?
- Page 5, line 214: ""and runs and in time poly(...)"" -> ""and runs in time poly(...)""
- Page 5, line 235: There should be a comma between \tau and \delta.
- Page 7, line 307: ""Each of the failure events will have probability at least $\delta'$ "" -> ""Each of the failure events will have probability at most $\delta'$ "".
- Page 8, line 322: ""under theempirical distribution"" -> ""under the empirical distribution"".

The following relevant references seem to be missing:
- [1] Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau, ""Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability"", NeurIPS 2020.
- [2] Rajai Nasser, Stefan Tiegel, ""Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise"", COLT 2022.","Minor questions:
- Page 5: In Algorithm 1, shouldn't T1 also run with $\delta'$ ?

Optional suggestion:
- Did the authors consider the Tsybakov noise model? It is harder than the Massart noise but easier than the agnostic setting, and hence it might be possible to get an efficient tester-learner that achieves the information-theoretically optimal error $\text{opt}+\epsilon$ for halfspaces with Tsybakov noise (of course, under structured marginal distributions such as Gaussian).",No concerns regarding potential societal impact of this work.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,3 good,3 good,1688591088633,,,NeurIPS.cc/2023/Conference,3Da0eESvN1,"['NeurIPS.cc/2023/Conference/Submission14083/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411460840,[],1,,,['everyone'],3Da0eESvN1,['NeurIPS.cc/2023/Conference/Submission14083/Reviewer_Vb7c'],1688591088633,1702411460840,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14083/Reviewer_Vb7c']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. [1] Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau, ""Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability"", NeurIPS 2020.
2. [2] Rajai Nasser, Stefan Tiegel, ""Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise"", COLT 2022.

These references are mentioned in the ""weaknesses"" section of the review as ""relevant references that seem to be missing"" from the paper.",1,"2020, 2022","Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability "
zlhEAE7amr,"The paper introduces an efficient algorithm for learning halfspaces in the testable learning model in which the tester-learner first applies a test on the training data and if the test succeeds the algorithm produces a hypothesis which is guaranteed to be near-optimal. It is required that if the data comes from a target distribution, then the test must succeed with high probability.

The paper considers learning halfspaces in the case where the target distribution is Gaussian (or strongly log-concave) and where the labels are subjected to Massart noise or adversarial noise (i.e., agnostic setting). The paper builds on several ideas from previous papers by Diakonikolas et al.","Learning halfspaces is a fundamental problem in machine learning. Even though it is one of the simplest tasks, the problem becomes non-trivial in the presence of label noise. Recently, there has been a lot of interest in developing algorithms for learning halfspaces in several settings (e.g., Massart noise, ...). The submitted paper is the first work that presents an efficient algorithm for learning halfspaces in the testable learning model of Rubinfeld and Vasilyan.

I find the paper to be clear and generally well-written, and I find the results novel and interesting.","Minor comments/typos:
- Page 2 line 71: ""an important one being that the probability mass of any region close to the origin is proportional to its geometric measure"" -> ""an important one being that the probability mass of any region close to the origin is roughly proportional to its geometric measure"".
- Page 5, line 198: Shouldn't the title of the section be ""Testing properties of isotropic strongly log-concave distributions""?
- Page 5, line 214: ""and runs and in time poly(...)"" -> ""and runs in time poly(...)""
- Page 5, line 235: There should be a comma between \tau and \delta.
- Page 7, line 307: ""Each of the failure events will have probability at least $\delta'$ "" -> ""Each of the failure events will have probability at most $\delta'$ "".
- Page 8, line 322: ""under theempirical distribution"" -> ""under the empirical distribution"".

The following relevant references seem to be missing:
- [1] Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau, ""Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability"", NeurIPS 2020.
- [2] Rajai Nasser, Stefan Tiegel, ""Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise"", COLT 2022.","Minor questions:
- Page 5: In Algorithm 1, shouldn't T1 also run with $\delta'$ ?

Optional suggestion:
- Did the authors consider the Tsybakov noise model? It is harder than the Massart noise but easier than the agnostic setting, and hence it might be possible to get an efficient tester-learner that achieves the information-theoretically optimal error $\text{opt}+\epsilon$ for halfspaces with Tsybakov noise (of course, under structured marginal distributions such as Gaussian).",No concerns regarding potential societal impact of this work.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,3 good,3 good,1688591088633,,,NeurIPS.cc/2023/Conference,3Da0eESvN1,"['NeurIPS.cc/2023/Conference/Submission14083/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411460840,[],1,,,['everyone'],3Da0eESvN1,['NeurIPS.cc/2023/Conference/Submission14083/Reviewer_Vb7c'],1688591088633,1702411460840,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14083/Reviewer_Vb7c']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. [1] Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau, ""Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability"", NeurIPS 2020.
2. [2] Rajai Nasser, Stefan Tiegel, ""Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise"", COLT 2022.

These references are mentioned in the ""weaknesses"" section of the review as ""relevant references that seem to be missing"" from the paper.",1,"2020, 2022",Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise
fbWbHHKfoF,"This paper presents HotBEV, a new model developed for 3D detection tasks. By prioritizing actual on-device latency and considering key hardware properties, HotBEV achieves impressive reductions in computational delay. This optimization allows for real-time decision-making in self-driving scenarios, making it a significant contribution to the field. The model's versatility, being compatible with both high-end and low-end GPUs, further underscores its practical value. Rigorous experimental validation showcases the model's superior performance in terms of speed and accuracy compared to existing solutions.","1. The model is compatible with both high-end and low-end GPUs, demonstrating a broad range of applicability.
2. The proposed method successfully achieve a delicate balance between model speed and detection precision.
3. They utilize a theoretical latency prediction model to guide their design, an innovative approach that differs from the typical focus on computational FLOPs.","1. The comparison is not fair in main experiments. The length of temporal fusion is critical to model performance. SoloFusion [1] suggests that a longer temporal sequence does not affect the model's FPS (Frames Per Second). This paper's methodology employs four frames for temporal fusion, while most comparative methods use only one to two frames.
2. The exploration of utilizing convolutional modulation, as opposed to self-attention, for relationship building is already documented in certain works [2], which have not been incorporated into this paper's analysis.


[1] Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection.                                                                                     
[2] You Only Segment Once: Towards Real-Time Panoptic Segmentation","1. To ensure an equitable comparison, the inclusion of SoloFusion with a four-frame input is indispensable.
2. Comprehensive relevant literature should be integrated into this paper for completeness.",This paper discuss the limitations.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1688828641270,,,NeurIPS.cc/2023/Conference,3Cj67k38st,"['NeurIPS.cc/2023/Conference/Submission3626/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410908558,[],4,,,['everyone'],3Cj67k38st,['NeurIPS.cc/2023/Conference/Submission3626/Reviewer_Z9GJ'],1688828641270,1702410908558,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3626/Reviewer_Z9GJ']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two papers, [1] ""Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection"" and [2] ""You Only Segment Once: Towards Real-Time Panoptic Segmentation"", and states that they ""have not been incorporated into this paper's analysis"", implying that the authors should include them in their discussion. Additionally, the reviewer asks a question about integrating ""Comprehensive relevant literature"" into the paper, which further reinforces the suggestion to cite additional relevant works.",1,,Time Will Tell New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection
fbWbHHKfoF,"This paper presents HotBEV, a new model developed for 3D detection tasks. By prioritizing actual on-device latency and considering key hardware properties, HotBEV achieves impressive reductions in computational delay. This optimization allows for real-time decision-making in self-driving scenarios, making it a significant contribution to the field. The model's versatility, being compatible with both high-end and low-end GPUs, further underscores its practical value. Rigorous experimental validation showcases the model's superior performance in terms of speed and accuracy compared to existing solutions.","1. The model is compatible with both high-end and low-end GPUs, demonstrating a broad range of applicability.
2. The proposed method successfully achieve a delicate balance between model speed and detection precision.
3. They utilize a theoretical latency prediction model to guide their design, an innovative approach that differs from the typical focus on computational FLOPs.","1. The comparison is not fair in main experiments. The length of temporal fusion is critical to model performance. SoloFusion [1] suggests that a longer temporal sequence does not affect the model's FPS (Frames Per Second). This paper's methodology employs four frames for temporal fusion, while most comparative methods use only one to two frames.
2. The exploration of utilizing convolutional modulation, as opposed to self-attention, for relationship building is already documented in certain works [2], which have not been incorporated into this paper's analysis.


[1] Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection.                                                                                     
[2] You Only Segment Once: Towards Real-Time Panoptic Segmentation","1. To ensure an equitable comparison, the inclusion of SoloFusion with a four-frame input is indispensable.
2. Comprehensive relevant literature should be integrated into this paper for completeness.",This paper discuss the limitations.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1688828641270,,,NeurIPS.cc/2023/Conference,3Cj67k38st,"['NeurIPS.cc/2023/Conference/Submission3626/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410908558,[],4,,,['everyone'],3Cj67k38st,['NeurIPS.cc/2023/Conference/Submission3626/Reviewer_Z9GJ'],1688828641270,1702410908558,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3626/Reviewer_Z9GJ']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two papers, [1] ""Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection"" and [2] ""You Only Segment Once: Towards Real-Time Panoptic Segmentation"", and states that they ""have not been incorporated into this paper's analysis"", implying that the authors should include them in their discussion. Additionally, the reviewer asks a question about integrating ""Comprehensive relevant literature"" into the paper, which further reinforces the suggestion to cite additional relevant works.",1,,You Only Segment Once Towards Real-Time Panoptic Segmentation
UlVGLAJj3l,"This work investigates the problem of last-iterate convergence in time-varying zero-sum games. Specifically, the authors study the last-iterate convergence of three kinds of algorithms (OGDA, EG, and negative momentum method) considering two kinds of time-varying games with specific structures, i.e., periodic games and convergent perturbed games. The authors obtained a convergence rate of EG in periodic games while providing a counterexample showing that OGDA and negative momentum method will diverge in this kind of game. In convergent perturbed games, the authors showed that all three kinds of algorithm will converge with a rate dependent on the perturbation $B_t$. Finally, experiments validate the statements proposed by the authors.","In general, I think there is no last-iterate convergence rate in time-varying games due to the changing nature of the games. However, with specific problem structures, I believe that last-iterate convergence results can be established, and this work provides a piece of clear evidence. Overall, although the solutions are relatively simple by modeling the learning algorithms as dynamical systems and leveraging some existing results in the control theory, I am satisfied with the motivation and the final results of this work.","There are no significant weaknesses that affect my rating of the whole paper. In the following, I only list some minor issues.

1. The notion of $T$ in periodic games. The notion of $T$ is commonly used to refer to the total number of rounds in online learning. In this work, the authors use $T$ to represent the number of rounds inside each period, which may lead to unnecessary understandings. I suggest the authors could choose a different notion to denote the period length.
2. The notion of $\text{ker}(\cdot)$. This notion, which appears in Line 98 for the first time, seems not to be defined or given a clear description before.
3. In Line 120, the authors mention that 'Recently, there are also works analyzing the regret behaviors of OGDA under a time-varying setting [1]'. The reference to [1] is not accurate enough. The work of [33], which is published at ICML2022, firstly gives a comprehensive study of the optimistic methods in time-varying zero-sum games by optimizing multiple performance measures simultaneously. I suggest the authors could refer to [33] in the aforementioned statement to make the credits more accurate.
4. The negative momentum method requires the $x$-player to evolve to the next round first to give its gradient $A_{t+1} x_{t+1}$ to the $y$-player. In some cases where the learning procedure is strictly in an online style, this algorithm is NOT applicable since both players must act first to make the game evolve. I suggest the authors could give a discussion on this point in the revised version.

Typos:
1. Line 89: 'Section 4' not 'section 4'.
2. Line 188: 'Section 3.3' not 'Section3.3'.
3. The reference to theorems are not unified, e.g., in Line 237, 'theorem 3.2', and in Line 255, 'Theorem (3.1)'.","I only have a major question on Theorem 3.1. What is the range of $t$? If $t$ is only chosen in $[1,T]$, choosing $t=T$ yields a convergence rate of $O(\lambda_* \text{poly}(T))$, which can be seen as a constant in terms of $T$. As mentioned in the 'Weaknesses' part, a bound linear in $T$ is pretty bad in the standard online learning convention. Theorem 3.1 exhibits an exponential rate only when $t$ can be significantly larger than the period length. Can the authors give some further explanations on this point?

In the end, I am very curious about whether the method in this paper, which models the learning algorithms as dynamical systems, can be applied to the constrained case, where the projection operations may bring some unique challenges to the modeling issue. Can the authors give some further explanations on this point?",Please mainly refer to the 'Weaknesses' part.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1690018897491,,,NeurIPS.cc/2023/Conference,3CJOaJugMG,"['NeurIPS.cc/2023/Conference/Submission11797/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411348108,[],5,,,['everyone'],3CJOaJugMG,['NeurIPS.cc/2023/Conference/Submission11797/Reviewer_abUR'],1690018897491,1702411348108,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission11797/Reviewer_abUR']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, [33], which is a work published at ICML2022 that provides a comprehensive study of optimistic methods in time-varying zero-sum games. The reviewer suggests that the authors should refer to [33] in the statement about recent works analyzing the regret behaviors of OGDA under a time-varying setting, to make the credits more accurate.",1,,A comprehensive study of optimistic methods in time-varying zero-sum games
wjhslBG1Vm,"The paper investigates whether analogical reasoning can enable in-context compositional generalization over visual entities. The authors construct three benchmarks to test the compositional generalization on visual analogy-making, including 3D shapes, BitMoji Faces, and CLEVR. The authors also present a visual analogy-making algorithm called Im-Promptu. The authors test the proposed method on the constructed benchmarks with various visual representations, including vector representations, patch representations, and object slots. The experiments demonstrate the tradeoffs between extrapolation abilities and the compositionality degree of the visual representations.","1. The paper constructs three benchmarks to test the compositional generalization on visual analogy-making, including 3D shapes, BitMoji Faces, and CLEVR.

2. The paper conducts experiments using various visual representations, including vector representations, patch representations, and object slots, and provides insights into the impact of the compositionality degree of the visual representations on compositional generalization. The results demonstrate the effectiveness of the object-centric representation for the compositional visual analogy-making.","1. The technical novelty of the proposed framework (Im-Promptu) is limited. The formulation in Section 4 is similar to visual analogy-making [1,2]. It is inappropriate to rename it as in-context learning or Im-Promptu learning, if there is no fundamental difference. Section 5 lists several model variants for visual analogy-making by composing existing modules. It is unclear what is the paper's contribution to the methodology.

2. The constructed benchmarks are all synthetic domains with little variance. This is a weakness considering that previous works [2,3] have already studied similar tasks using natural images.

3. The experimental results are inadequate. The authors only evaluated the proposed framework on the self-constructed benchmarks, while previous works [1,2,3] have introduced several benchmarks for visual analogy-making. It is unclear whether the proposed framework generalizes to other realistic benchmarks.

4. The paper lacks reference to important related works [2,3].

[1] Reed, Scott E., et al. ""Deep visual analogy-making."" Advances in neural information processing systems 28 (2015).

[2] Sadeghi, Fereshteh, C. Lawrence Zitnick, and Ali Farhadi. ""Visalogy: Answering visual analogy questions."" Advances in Neural Information Processing Systems 28 (2015).

[3] Bar, Amir, et al. ""Visual prompting via image inpainting."" Advances in Neural Information Processing Systems 35 (2022): 25005-25017.
","1. What is the fundamental difference between the proposed Im-Promptu learning in Section 4 and visual analogy-making?

2. What is the main technical contribution of this paper?

3. The method of this paper works better on spatially consistent datasets, so how to reflect the statement ""produces a more generalized composition beyond spatial relations"" in the paper?

4. Why are there some results with lower MSE and higher FID?",The authors do not discuss the limitations of the proposed method.,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,1 poor,2 fair,2 fair,1690356674069,,,NeurIPS.cc/2023/Conference,38o372YoYt,"['NeurIPS.cc/2023/Conference/Submission8705/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411185101,[],5,,,['everyone'],38o372YoYt,['NeurIPS.cc/2023/Conference/Submission8705/Reviewer_4dpY'],1690356674069,1702411185101,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8705/Reviewer_4dpY']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions that ""The paper lacks reference to important related works [2,3]"" and lists the references:

[2] Sadeghi, Fereshteh, C. Lawrence Zitnick, and Ali Farhadi. ""Visalogy: Answering visual analogy questions."" Advances in Neural Information Processing Systems 28 (2015).

[3] Bar, Amir, et al. ""Visual prompting via image inpainting."" Advances in Neural Information Processing Systems 35 (2022): 25005-25017.

Additionally, the reviewer also mentions another related work:

[1] Reed, Scott E., et al. ""Deep visual analogy-making."" Advances in neural information processing systems 28 (2015).

This implies that the reviewer thinks the authors should be aware of and cite these related works to improve the paper.",1,"2015, 2022, 2015",Visalogy Answering visual analogy questions
wjhslBG1Vm,"The paper investigates whether analogical reasoning can enable in-context compositional generalization over visual entities. The authors construct three benchmarks to test the compositional generalization on visual analogy-making, including 3D shapes, BitMoji Faces, and CLEVR. The authors also present a visual analogy-making algorithm called Im-Promptu. The authors test the proposed method on the constructed benchmarks with various visual representations, including vector representations, patch representations, and object slots. The experiments demonstrate the tradeoffs between extrapolation abilities and the compositionality degree of the visual representations.","1. The paper constructs three benchmarks to test the compositional generalization on visual analogy-making, including 3D shapes, BitMoji Faces, and CLEVR.

2. The paper conducts experiments using various visual representations, including vector representations, patch representations, and object slots, and provides insights into the impact of the compositionality degree of the visual representations on compositional generalization. The results demonstrate the effectiveness of the object-centric representation for the compositional visual analogy-making.","1. The technical novelty of the proposed framework (Im-Promptu) is limited. The formulation in Section 4 is similar to visual analogy-making [1,2]. It is inappropriate to rename it as in-context learning or Im-Promptu learning, if there is no fundamental difference. Section 5 lists several model variants for visual analogy-making by composing existing modules. It is unclear what is the paper's contribution to the methodology.

2. The constructed benchmarks are all synthetic domains with little variance. This is a weakness considering that previous works [2,3] have already studied similar tasks using natural images.

3. The experimental results are inadequate. The authors only evaluated the proposed framework on the self-constructed benchmarks, while previous works [1,2,3] have introduced several benchmarks for visual analogy-making. It is unclear whether the proposed framework generalizes to other realistic benchmarks.

4. The paper lacks reference to important related works [2,3].

[1] Reed, Scott E., et al. ""Deep visual analogy-making."" Advances in neural information processing systems 28 (2015).

[2] Sadeghi, Fereshteh, C. Lawrence Zitnick, and Ali Farhadi. ""Visalogy: Answering visual analogy questions."" Advances in Neural Information Processing Systems 28 (2015).

[3] Bar, Amir, et al. ""Visual prompting via image inpainting."" Advances in Neural Information Processing Systems 35 (2022): 25005-25017.
","1. What is the fundamental difference between the proposed Im-Promptu learning in Section 4 and visual analogy-making?

2. What is the main technical contribution of this paper?

3. The method of this paper works better on spatially consistent datasets, so how to reflect the statement ""produces a more generalized composition beyond spatial relations"" in the paper?

4. Why are there some results with lower MSE and higher FID?",The authors do not discuss the limitations of the proposed method.,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,1 poor,2 fair,2 fair,1690356674069,,,NeurIPS.cc/2023/Conference,38o372YoYt,"['NeurIPS.cc/2023/Conference/Submission8705/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411185101,[],5,,,['everyone'],38o372YoYt,['NeurIPS.cc/2023/Conference/Submission8705/Reviewer_4dpY'],1690356674069,1702411185101,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8705/Reviewer_4dpY']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions that ""The paper lacks reference to important related works [2,3]"" and lists the references:

[2] Sadeghi, Fereshteh, C. Lawrence Zitnick, and Ali Farhadi. ""Visalogy: Answering visual analogy questions."" Advances in Neural Information Processing Systems 28 (2015).

[3] Bar, Amir, et al. ""Visual prompting via image inpainting."" Advances in Neural Information Processing Systems 35 (2022): 25005-25017.

Additionally, the reviewer also mentions another related work:

[1] Reed, Scott E., et al. ""Deep visual analogy-making."" Advances in neural information processing systems 28 (2015).

This implies that the reviewer thinks the authors should be aware of and cite these related works to improve the paper.",1,"2015, 2022, 2015",Visual prompting via image inpainting
wjhslBG1Vm,"The paper investigates whether analogical reasoning can enable in-context compositional generalization over visual entities. The authors construct three benchmarks to test the compositional generalization on visual analogy-making, including 3D shapes, BitMoji Faces, and CLEVR. The authors also present a visual analogy-making algorithm called Im-Promptu. The authors test the proposed method on the constructed benchmarks with various visual representations, including vector representations, patch representations, and object slots. The experiments demonstrate the tradeoffs between extrapolation abilities and the compositionality degree of the visual representations.","1. The paper constructs three benchmarks to test the compositional generalization on visual analogy-making, including 3D shapes, BitMoji Faces, and CLEVR.

2. The paper conducts experiments using various visual representations, including vector representations, patch representations, and object slots, and provides insights into the impact of the compositionality degree of the visual representations on compositional generalization. The results demonstrate the effectiveness of the object-centric representation for the compositional visual analogy-making.","1. The technical novelty of the proposed framework (Im-Promptu) is limited. The formulation in Section 4 is similar to visual analogy-making [1,2]. It is inappropriate to rename it as in-context learning or Im-Promptu learning, if there is no fundamental difference. Section 5 lists several model variants for visual analogy-making by composing existing modules. It is unclear what is the paper's contribution to the methodology.

2. The constructed benchmarks are all synthetic domains with little variance. This is a weakness considering that previous works [2,3] have already studied similar tasks using natural images.

3. The experimental results are inadequate. The authors only evaluated the proposed framework on the self-constructed benchmarks, while previous works [1,2,3] have introduced several benchmarks for visual analogy-making. It is unclear whether the proposed framework generalizes to other realistic benchmarks.

4. The paper lacks reference to important related works [2,3].

[1] Reed, Scott E., et al. ""Deep visual analogy-making."" Advances in neural information processing systems 28 (2015).

[2] Sadeghi, Fereshteh, C. Lawrence Zitnick, and Ali Farhadi. ""Visalogy: Answering visual analogy questions."" Advances in Neural Information Processing Systems 28 (2015).

[3] Bar, Amir, et al. ""Visual prompting via image inpainting."" Advances in Neural Information Processing Systems 35 (2022): 25005-25017.
","1. What is the fundamental difference between the proposed Im-Promptu learning in Section 4 and visual analogy-making?

2. What is the main technical contribution of this paper?

3. The method of this paper works better on spatially consistent datasets, so how to reflect the statement ""produces a more generalized composition beyond spatial relations"" in the paper?

4. Why are there some results with lower MSE and higher FID?",The authors do not discuss the limitations of the proposed method.,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,1 poor,2 fair,2 fair,1690356674069,,,NeurIPS.cc/2023/Conference,38o372YoYt,"['NeurIPS.cc/2023/Conference/Submission8705/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411185101,[],5,,,['everyone'],38o372YoYt,['NeurIPS.cc/2023/Conference/Submission8705/Reviewer_4dpY'],1690356674069,1702411185101,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8705/Reviewer_4dpY']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions that ""The paper lacks reference to important related works [2,3]"" and lists the references:

[2] Sadeghi, Fereshteh, C. Lawrence Zitnick, and Ali Farhadi. ""Visalogy: Answering visual analogy questions."" Advances in Neural Information Processing Systems 28 (2015).

[3] Bar, Amir, et al. ""Visual prompting via image inpainting."" Advances in Neural Information Processing Systems 35 (2022): 25005-25017.

Additionally, the reviewer also mentions another related work:

[1] Reed, Scott E., et al. ""Deep visual analogy-making."" Advances in neural information processing systems 28 (2015).

This implies that the reviewer thinks the authors should be aware of and cite these related works to improve the paper.",1,"2015, 2022, 2015",Deep visual analogy-making
xjXQlCt37L,"The authors identify various aspects of fairness that require consideration when assessing recourse bias between subgroups. They propose FACTS (Fairness Aware Counterfactuals for Subgroups), in an attempt to lay foundations for a framework that can be used to audit subgroup fairness through counterfactual explanations (CEs). Experimental evaluation is relatively well thought out and thorough.","1. The paper is easy to read, with a logical structure and natural flow. In most cases, relevant examples are provided where necessary.

2. I believe the issue being addressed is complex, and hard conclusions regarding the effectiveness of fairness tools should be made sparingly. However, the paper does an excellent job of analysing the problem at a deeper level than that which currently exists in the literature [1]. Explicit separation between the micro and macro viewpoint is well positioned.

3. While there are still gaps to be filled, this paper lays a strong foundation for future research into the use of counterfactuals in assessing recourse bias/unfairness. It is worth noting that [2] conducts similar analyses of the potential pitfalls of bias assessment via subgroup comparison, arriving at a similar conclusion to this paper that recourses should be compared 1 to 1 for more reliable results.

[1] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. ICML 2018.

[2] Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. ICML 2023.
","I appreciate the thoroughness of the authors in tackling an issue as complex as this. There are a few important gaps that I believe may have evaded their attention, which I will detail below.

1. The assumption that while cost is not exactly quantifiable, a given action's cost is uniform across the input space, and can thus be compared between instances, does not always hold. One simple example would be changing salary by amount X. Individuals commanding higher initial salaries are likely to find this action easier. I would thus modify the ""oblivious to the cost function"" claim appropriately (though it is still a more than reasonable starting point for now). 

2. Evaluation of fairness appears quite sensitive to the set of actions A chosen (specifically the fixed cost used/described in lines 290-302). Such a budget can often handle poorly the asymmetries between subgroup cost distributions for a given action. That said, multiple cost budgets are evaluated in the experiments, to provide a more complete overview. It's also worth considering the idea of scaling the cost of certain action directions, as in [2], and evaluating fairness accordingly.

3. In a similar vein, frequent itemsets are prone to resolution issues with numerical features. Moving beyond the apriori approach in [3] is wise, though frequent itemsets often do not uncover the minimal cost directions to flip predictions. Additionally, if the motivation behind fp-growth is primarily efficiency, it would be useful to have some analysis of this.

4. Discussion of when and how the fairness metrics can be manipulated (as above) is needed. Additionally, in practical settings, individuals are each in control of diverse actions, often unique to them. The effect of such actions (not included in but confounding with the feature space) is worth pointing out.

5. A large quantity of metrics are introduced. Table 1 caption could do with more clarification so readers don't have to study the text to understand the highlighted values.

[3] Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses. NeurIPS 2020.","The bins chosen for numerical features are rather large, and so may not effectively capture the minimum cost recourses between subgroups if the numerical features are influential, which might commonly be the case. This is one of the issues with frequent itemset mining, and something that [3] originally suffered with- do the authors have any proposed solutions? From my experience in the area, this is a fairly serious shortcoming, and something that prevented me from awarding this paper an even higher score.","Described above. Limitations associated with choosing parameters for fairness metrics and determining appropriate action spaces should be discussed in the revision.

The research contributes valuable insights and provides a robust foundation for future work, though I would suggest to keep in mind the inherent complexity of the fairness problem, which is indeed emphasised in the paper. Overconfidence in the reliability of the proposed framework may not encourage an appropriately critical approach to this multifaceted issue i.e. conclusions drawn should be cautiously optimistic and mindful of the above limitations.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,3 good,3 good,1688493809830,,,NeurIPS.cc/2023/Conference,38dQv3OwN3,"['NeurIPS.cc/2023/Conference/Submission3430/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410898577,[],1,,,['everyone'],38dQv3OwN3,['NeurIPS.cc/2023/Conference/Submission3430/Reviewer_ZFdX'],1688493809830,1702410898577,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3430/Reviewer_ZFdX']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. ICML 2018. (referenced as [1])
2. Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. ICML 2023. (referenced as [2])
3. Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses. NeurIPS 2020. (referenced as [3])

The reviewer mentions these papers as relevant to the topic and suggests that the authors consider the ideas and conclusions presented in these papers when revising their work.",1,"2018, 2023, 2020",Preventing fairness gerrymandering: Auditing and learning for subgroup fairness 
xjXQlCt37L,"The authors identify various aspects of fairness that require consideration when assessing recourse bias between subgroups. They propose FACTS (Fairness Aware Counterfactuals for Subgroups), in an attempt to lay foundations for a framework that can be used to audit subgroup fairness through counterfactual explanations (CEs). Experimental evaluation is relatively well thought out and thorough.","1. The paper is easy to read, with a logical structure and natural flow. In most cases, relevant examples are provided where necessary.

2. I believe the issue being addressed is complex, and hard conclusions regarding the effectiveness of fairness tools should be made sparingly. However, the paper does an excellent job of analysing the problem at a deeper level than that which currently exists in the literature [1]. Explicit separation between the micro and macro viewpoint is well positioned.

3. While there are still gaps to be filled, this paper lays a strong foundation for future research into the use of counterfactuals in assessing recourse bias/unfairness. It is worth noting that [2] conducts similar analyses of the potential pitfalls of bias assessment via subgroup comparison, arriving at a similar conclusion to this paper that recourses should be compared 1 to 1 for more reliable results.

[1] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. ICML 2018.

[2] Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. ICML 2023.
","I appreciate the thoroughness of the authors in tackling an issue as complex as this. There are a few important gaps that I believe may have evaded their attention, which I will detail below.

1. The assumption that while cost is not exactly quantifiable, a given action's cost is uniform across the input space, and can thus be compared between instances, does not always hold. One simple example would be changing salary by amount X. Individuals commanding higher initial salaries are likely to find this action easier. I would thus modify the ""oblivious to the cost function"" claim appropriately (though it is still a more than reasonable starting point for now). 

2. Evaluation of fairness appears quite sensitive to the set of actions A chosen (specifically the fixed cost used/described in lines 290-302). Such a budget can often handle poorly the asymmetries between subgroup cost distributions for a given action. That said, multiple cost budgets are evaluated in the experiments, to provide a more complete overview. It's also worth considering the idea of scaling the cost of certain action directions, as in [2], and evaluating fairness accordingly.

3. In a similar vein, frequent itemsets are prone to resolution issues with numerical features. Moving beyond the apriori approach in [3] is wise, though frequent itemsets often do not uncover the minimal cost directions to flip predictions. Additionally, if the motivation behind fp-growth is primarily efficiency, it would be useful to have some analysis of this.

4. Discussion of when and how the fairness metrics can be manipulated (as above) is needed. Additionally, in practical settings, individuals are each in control of diverse actions, often unique to them. The effect of such actions (not included in but confounding with the feature space) is worth pointing out.

5. A large quantity of metrics are introduced. Table 1 caption could do with more clarification so readers don't have to study the text to understand the highlighted values.

[3] Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses. NeurIPS 2020.","The bins chosen for numerical features are rather large, and so may not effectively capture the minimum cost recourses between subgroups if the numerical features are influential, which might commonly be the case. This is one of the issues with frequent itemset mining, and something that [3] originally suffered with- do the authors have any proposed solutions? From my experience in the area, this is a fairly serious shortcoming, and something that prevented me from awarding this paper an even higher score.","Described above. Limitations associated with choosing parameters for fairness metrics and determining appropriate action spaces should be discussed in the revision.

The research contributes valuable insights and provides a robust foundation for future work, though I would suggest to keep in mind the inherent complexity of the fairness problem, which is indeed emphasised in the paper. Overconfidence in the reliability of the proposed framework may not encourage an appropriately critical approach to this multifaceted issue i.e. conclusions drawn should be cautiously optimistic and mindful of the above limitations.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,3 good,3 good,1688493809830,,,NeurIPS.cc/2023/Conference,38dQv3OwN3,"['NeurIPS.cc/2023/Conference/Submission3430/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410898577,[],1,,,['everyone'],38dQv3OwN3,['NeurIPS.cc/2023/Conference/Submission3430/Reviewer_ZFdX'],1688493809830,1702410898577,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3430/Reviewer_ZFdX']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. ICML 2018. (referenced as [1])
2. Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. ICML 2023. (referenced as [2])
3. Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses. NeurIPS 2020. (referenced as [3])

The reviewer mentions these papers as relevant to the topic and suggests that the authors consider the ideas and conclusions presented in these papers when revising their work.",1,"2018, 2023, 2020",GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations 
xjXQlCt37L,"The authors identify various aspects of fairness that require consideration when assessing recourse bias between subgroups. They propose FACTS (Fairness Aware Counterfactuals for Subgroups), in an attempt to lay foundations for a framework that can be used to audit subgroup fairness through counterfactual explanations (CEs). Experimental evaluation is relatively well thought out and thorough.","1. The paper is easy to read, with a logical structure and natural flow. In most cases, relevant examples are provided where necessary.

2. I believe the issue being addressed is complex, and hard conclusions regarding the effectiveness of fairness tools should be made sparingly. However, the paper does an excellent job of analysing the problem at a deeper level than that which currently exists in the literature [1]. Explicit separation between the micro and macro viewpoint is well positioned.

3. While there are still gaps to be filled, this paper lays a strong foundation for future research into the use of counterfactuals in assessing recourse bias/unfairness. It is worth noting that [2] conducts similar analyses of the potential pitfalls of bias assessment via subgroup comparison, arriving at a similar conclusion to this paper that recourses should be compared 1 to 1 for more reliable results.

[1] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. ICML 2018.

[2] Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. ICML 2023.
","I appreciate the thoroughness of the authors in tackling an issue as complex as this. There are a few important gaps that I believe may have evaded their attention, which I will detail below.

1. The assumption that while cost is not exactly quantifiable, a given action's cost is uniform across the input space, and can thus be compared between instances, does not always hold. One simple example would be changing salary by amount X. Individuals commanding higher initial salaries are likely to find this action easier. I would thus modify the ""oblivious to the cost function"" claim appropriately (though it is still a more than reasonable starting point for now). 

2. Evaluation of fairness appears quite sensitive to the set of actions A chosen (specifically the fixed cost used/described in lines 290-302). Such a budget can often handle poorly the asymmetries between subgroup cost distributions for a given action. That said, multiple cost budgets are evaluated in the experiments, to provide a more complete overview. It's also worth considering the idea of scaling the cost of certain action directions, as in [2], and evaluating fairness accordingly.

3. In a similar vein, frequent itemsets are prone to resolution issues with numerical features. Moving beyond the apriori approach in [3] is wise, though frequent itemsets often do not uncover the minimal cost directions to flip predictions. Additionally, if the motivation behind fp-growth is primarily efficiency, it would be useful to have some analysis of this.

4. Discussion of when and how the fairness metrics can be manipulated (as above) is needed. Additionally, in practical settings, individuals are each in control of diverse actions, often unique to them. The effect of such actions (not included in but confounding with the feature space) is worth pointing out.

5. A large quantity of metrics are introduced. Table 1 caption could do with more clarification so readers don't have to study the text to understand the highlighted values.

[3] Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses. NeurIPS 2020.","The bins chosen for numerical features are rather large, and so may not effectively capture the minimum cost recourses between subgroups if the numerical features are influential, which might commonly be the case. This is one of the issues with frequent itemset mining, and something that [3] originally suffered with- do the authors have any proposed solutions? From my experience in the area, this is a fairly serious shortcoming, and something that prevented me from awarding this paper an even higher score.","Described above. Limitations associated with choosing parameters for fairness metrics and determining appropriate action spaces should be discussed in the revision.

The research contributes valuable insights and provides a robust foundation for future work, though I would suggest to keep in mind the inherent complexity of the fairness problem, which is indeed emphasised in the paper. Overconfidence in the reliability of the proposed framework may not encourage an appropriately critical approach to this multifaceted issue i.e. conclusions drawn should be cautiously optimistic and mindful of the above limitations.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,3 good,3 good,1688493809830,,,NeurIPS.cc/2023/Conference,38dQv3OwN3,"['NeurIPS.cc/2023/Conference/Submission3430/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410898577,[],1,,,['everyone'],38dQv3OwN3,['NeurIPS.cc/2023/Conference/Submission3430/Reviewer_ZFdX'],1688493809830,1702410898577,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3430/Reviewer_ZFdX']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. ICML 2018. (referenced as [1])
2. Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. ICML 2023. (referenced as [2])
3. Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses. NeurIPS 2020. (referenced as [3])

The reviewer mentions these papers as relevant to the topic and suggests that the authors consider the ideas and conclusions presented in these papers when revising their work.",1,"2018, 2023, 2020",Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses
rAvNgcaXXd,"The paper proposes a framework (termed as FACTS) for analysing the recourse fairness of a machine learning model. The work introduces multiple metrics for quantifying recourse fairness - both at micro level (individuals considered separate) and macro level (individuals considered together). Some proposed recourse fairness metrics are argued to be unaffected by the underlying cost metric. Finally, the work demonstrates the proposed framework for a logistic regression model by training it on several datasets. All except one dataset results are present in the supplementary material.","1. Analysing model fairness from the perspective of recourse explanations is an important research problem. There exists much work in analysing the fairness of model predictions, but less on analysing if the generated recourses are fair.
2. The paper is very well-written. The motivation, definitions, and metrics are all clearly explained. The authors cover the current metrics clearly, highlight their weaknesses, and then introduce their proposed metrics.
3. Bibliography is extensive and covers most of the recent papers on the related topics.","1. Experiments section seems insufficient. Please see below for some examples
   
   a) The experiments section and the supplementary material presents outputs of the FACTS framework for different datasets, however, it is unclear how to evaluate the efficacy of the proposed framework. It would have been great if the authors had included experiments using toy datasets to create a biased model and then demonstrated that FACTS can uncover the recourse bias while the existing methods don’t/do it with poor coverage.

  b) I think the current experiments could have been more detailed for example by covering other model classes (e.g., tree-based, neural networks) to understand if the proposed metrics generalise to non-linear and non-differentiable models.

  c) FACTS is argued to be scalable, interpretable and highly parametrizable (Line 113). However, there are no experiments to support this. It would be great to understand interpretability through a user study for example. Similarly, analysing how FACTS scales to continuous datasets would be very helpful because as it uses an itemset miming algorithms which generally don't scale well with continuous datasets as feature binning creates a large search space [1]. 

2. Line 119 - FACTS can explore systematically the feature space. I wonder if there are any guarantees about this? If the feature dimension is high and the dataset has continuous features then the search space becomes too large to scale well.

[1] Dan Ley, Saumitra Mishra, Daniele Magazzeni. Global Counterfactual Explanations: Investigations, Implementations and Improvements. ICLR Workshop 2022.

","1. It seems to create counterfactual summaries, FACTS mines conjunctions of predicates for all positive samples and then uses them to find counterfactuals. Hwoever, doesn't this approach limit the coverage (effectiveness) because if the positive samples are not diverse (i.e., the feature space is not well covered), we may not find counterfactuals for many individuals, resulting in poor coverage.

2. I think FACTS have many similarities in terms of metrics with [2]. For example, micro and macro vs finding a global direction and scaling it locally. Similarly, ECD vs coverage-cost profiles. I would like to hear authors comment on how their work differs from GLOBE-CE.

3. Some points needs clarification/correction
    1. What is ""horizontal intervention"". lines 342 and line 70
    2. I think the term ""effectiveness"" (Line 193) is same as the term ""coverage"" used in the context of GCEs? See Ref[3]
    3. Further, set of actions A can be considered as diverse CFs. ECD profiles same as in GCEs? Micro vs Macro too in G~LOBE-CE
    4. Line 154: I don’t think this claim is correct. AReS[3], GLOBE-CE[2] and Gupta et al. have tried to address this problem before using global (group-level) counterfactuals.
    6. Line 161: It would be helpful to understand authors comments on how FACTS compares to other global explanations framework in terms of efficiency.

[2] A Translation-Based Approach for Global Counterfactual Explanations. Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: CoRR abs/2305.17021 (2023)

[3] Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses. Kaivalya Rawal and Himabindu Lakkaraju. NeurIPS 2020","The authors do not discuss any limitations of their work. However, it is important to note, given that FACTS is proposed as a tool to investigate recourse fairness, detailed experimentation and analysis would be needed before using the tool in the real-world applications.",['Ethics review needed: Discrimination / Bias / Fairness Concerns'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,4 excellent,2 fair,1689031319327,,,NeurIPS.cc/2023/Conference,38dQv3OwN3,"['NeurIPS.cc/2023/Conference/Submission3430/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410898193,[],5,,,['everyone'],38dQv3OwN3,['NeurIPS.cc/2023/Conference/Submission3430/Reviewer_m3rQ'],1689031319327,1702410898193,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3430/Reviewer_m3rQ']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers and asks the authors to comment on how their work differs from these papers, implying that the authors should be aware of and cite these papers. The mentioned papers include:

1. Dan Ley, Saumitra Mishra, Daniele Magazzeni. Global Counterfactual Explanations: Investigations, Implementations and Improvements. ICLR Workshop 2022.
2. A Translation-Based Approach for Global Counterfactual Explanations. Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: CoRR abs/2305.17021 (2023)
3. Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses. Kaivalya Rawal and Himabindu Lakkaraju. NeurIPS 2020

The reviewer also mentions Gupta et al., but does not provide a specific reference.",1,"2022, 2023, 2023, 2020","Global Counterfactual Explanations: Investigations, Implementations and Improvements"
rAvNgcaXXd,"The paper proposes a framework (termed as FACTS) for analysing the recourse fairness of a machine learning model. The work introduces multiple metrics for quantifying recourse fairness - both at micro level (individuals considered separate) and macro level (individuals considered together). Some proposed recourse fairness metrics are argued to be unaffected by the underlying cost metric. Finally, the work demonstrates the proposed framework for a logistic regression model by training it on several datasets. All except one dataset results are present in the supplementary material.","1. Analysing model fairness from the perspective of recourse explanations is an important research problem. There exists much work in analysing the fairness of model predictions, but less on analysing if the generated recourses are fair.
2. The paper is very well-written. The motivation, definitions, and metrics are all clearly explained. The authors cover the current metrics clearly, highlight their weaknesses, and then introduce their proposed metrics.
3. Bibliography is extensive and covers most of the recent papers on the related topics.","1. Experiments section seems insufficient. Please see below for some examples
   
   a) The experiments section and the supplementary material presents outputs of the FACTS framework for different datasets, however, it is unclear how to evaluate the efficacy of the proposed framework. It would have been great if the authors had included experiments using toy datasets to create a biased model and then demonstrated that FACTS can uncover the recourse bias while the existing methods don’t/do it with poor coverage.

  b) I think the current experiments could have been more detailed for example by covering other model classes (e.g., tree-based, neural networks) to understand if the proposed metrics generalise to non-linear and non-differentiable models.

  c) FACTS is argued to be scalable, interpretable and highly parametrizable (Line 113). However, there are no experiments to support this. It would be great to understand interpretability through a user study for example. Similarly, analysing how FACTS scales to continuous datasets would be very helpful because as it uses an itemset miming algorithms which generally don't scale well with continuous datasets as feature binning creates a large search space [1]. 

2. Line 119 - FACTS can explore systematically the feature space. I wonder if there are any guarantees about this? If the feature dimension is high and the dataset has continuous features then the search space becomes too large to scale well.

[1] Dan Ley, Saumitra Mishra, Daniele Magazzeni. Global Counterfactual Explanations: Investigations, Implementations and Improvements. ICLR Workshop 2022.

","1. It seems to create counterfactual summaries, FACTS mines conjunctions of predicates for all positive samples and then uses them to find counterfactuals. Hwoever, doesn't this approach limit the coverage (effectiveness) because if the positive samples are not diverse (i.e., the feature space is not well covered), we may not find counterfactuals for many individuals, resulting in poor coverage.

2. I think FACTS have many similarities in terms of metrics with [2]. For example, micro and macro vs finding a global direction and scaling it locally. Similarly, ECD vs coverage-cost profiles. I would like to hear authors comment on how their work differs from GLOBE-CE.

3. Some points needs clarification/correction
    1. What is ""horizontal intervention"". lines 342 and line 70
    2. I think the term ""effectiveness"" (Line 193) is same as the term ""coverage"" used in the context of GCEs? See Ref[3]
    3. Further, set of actions A can be considered as diverse CFs. ECD profiles same as in GCEs? Micro vs Macro too in G~LOBE-CE
    4. Line 154: I don’t think this claim is correct. AReS[3], GLOBE-CE[2] and Gupta et al. have tried to address this problem before using global (group-level) counterfactuals.
    6. Line 161: It would be helpful to understand authors comments on how FACTS compares to other global explanations framework in terms of efficiency.

[2] A Translation-Based Approach for Global Counterfactual Explanations. Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: CoRR abs/2305.17021 (2023)

[3] Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses. Kaivalya Rawal and Himabindu Lakkaraju. NeurIPS 2020","The authors do not discuss any limitations of their work. However, it is important to note, given that FACTS is proposed as a tool to investigate recourse fairness, detailed experimentation and analysis would be needed before using the tool in the real-world applications.",['Ethics review needed: Discrimination / Bias / Fairness Concerns'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,4 excellent,2 fair,1689031319327,,,NeurIPS.cc/2023/Conference,38dQv3OwN3,"['NeurIPS.cc/2023/Conference/Submission3430/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410898193,[],5,,,['everyone'],38dQv3OwN3,['NeurIPS.cc/2023/Conference/Submission3430/Reviewer_m3rQ'],1689031319327,1702410898193,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3430/Reviewer_m3rQ']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers and asks the authors to comment on how their work differs from these papers, implying that the authors should be aware of and cite these papers. The mentioned papers include:

1. Dan Ley, Saumitra Mishra, Daniele Magazzeni. Global Counterfactual Explanations: Investigations, Implementations and Improvements. ICLR Workshop 2022.
2. A Translation-Based Approach for Global Counterfactual Explanations. Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: CoRR abs/2305.17021 (2023)
3. Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses. Kaivalya Rawal and Himabindu Lakkaraju. NeurIPS 2020

The reviewer also mentions Gupta et al., but does not provide a specific reference.",1,"2022, 2023, 2023, 2020",A Translation-Based Approach for Global Counterfactual Explanations
rAvNgcaXXd,"The paper proposes a framework (termed as FACTS) for analysing the recourse fairness of a machine learning model. The work introduces multiple metrics for quantifying recourse fairness - both at micro level (individuals considered separate) and macro level (individuals considered together). Some proposed recourse fairness metrics are argued to be unaffected by the underlying cost metric. Finally, the work demonstrates the proposed framework for a logistic regression model by training it on several datasets. All except one dataset results are present in the supplementary material.","1. Analysing model fairness from the perspective of recourse explanations is an important research problem. There exists much work in analysing the fairness of model predictions, but less on analysing if the generated recourses are fair.
2. The paper is very well-written. The motivation, definitions, and metrics are all clearly explained. The authors cover the current metrics clearly, highlight their weaknesses, and then introduce their proposed metrics.
3. Bibliography is extensive and covers most of the recent papers on the related topics.","1. Experiments section seems insufficient. Please see below for some examples
   
   a) The experiments section and the supplementary material presents outputs of the FACTS framework for different datasets, however, it is unclear how to evaluate the efficacy of the proposed framework. It would have been great if the authors had included experiments using toy datasets to create a biased model and then demonstrated that FACTS can uncover the recourse bias while the existing methods don’t/do it with poor coverage.

  b) I think the current experiments could have been more detailed for example by covering other model classes (e.g., tree-based, neural networks) to understand if the proposed metrics generalise to non-linear and non-differentiable models.

  c) FACTS is argued to be scalable, interpretable and highly parametrizable (Line 113). However, there are no experiments to support this. It would be great to understand interpretability through a user study for example. Similarly, analysing how FACTS scales to continuous datasets would be very helpful because as it uses an itemset miming algorithms which generally don't scale well with continuous datasets as feature binning creates a large search space [1]. 

2. Line 119 - FACTS can explore systematically the feature space. I wonder if there are any guarantees about this? If the feature dimension is high and the dataset has continuous features then the search space becomes too large to scale well.

[1] Dan Ley, Saumitra Mishra, Daniele Magazzeni. Global Counterfactual Explanations: Investigations, Implementations and Improvements. ICLR Workshop 2022.

","1. It seems to create counterfactual summaries, FACTS mines conjunctions of predicates for all positive samples and then uses them to find counterfactuals. Hwoever, doesn't this approach limit the coverage (effectiveness) because if the positive samples are not diverse (i.e., the feature space is not well covered), we may not find counterfactuals for many individuals, resulting in poor coverage.

2. I think FACTS have many similarities in terms of metrics with [2]. For example, micro and macro vs finding a global direction and scaling it locally. Similarly, ECD vs coverage-cost profiles. I would like to hear authors comment on how their work differs from GLOBE-CE.

3. Some points needs clarification/correction
    1. What is ""horizontal intervention"". lines 342 and line 70
    2. I think the term ""effectiveness"" (Line 193) is same as the term ""coverage"" used in the context of GCEs? See Ref[3]
    3. Further, set of actions A can be considered as diverse CFs. ECD profiles same as in GCEs? Micro vs Macro too in G~LOBE-CE
    4. Line 154: I don’t think this claim is correct. AReS[3], GLOBE-CE[2] and Gupta et al. have tried to address this problem before using global (group-level) counterfactuals.
    6. Line 161: It would be helpful to understand authors comments on how FACTS compares to other global explanations framework in terms of efficiency.

[2] A Translation-Based Approach for Global Counterfactual Explanations. Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: CoRR abs/2305.17021 (2023)

[3] Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses. Kaivalya Rawal and Himabindu Lakkaraju. NeurIPS 2020","The authors do not discuss any limitations of their work. However, it is important to note, given that FACTS is proposed as a tool to investigate recourse fairness, detailed experimentation and analysis would be needed before using the tool in the real-world applications.",['Ethics review needed: Discrimination / Bias / Fairness Concerns'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,4 excellent,2 fair,1689031319327,,,NeurIPS.cc/2023/Conference,38dQv3OwN3,"['NeurIPS.cc/2023/Conference/Submission3430/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410898193,[],5,,,['everyone'],38dQv3OwN3,['NeurIPS.cc/2023/Conference/Submission3430/Reviewer_m3rQ'],1689031319327,1702410898193,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3430/Reviewer_m3rQ']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers and asks the authors to comment on how their work differs from these papers, implying that the authors should be aware of and cite these papers. The mentioned papers include:

1. Dan Ley, Saumitra Mishra, Daniele Magazzeni. Global Counterfactual Explanations: Investigations, Implementations and Improvements. ICLR Workshop 2022.
2. A Translation-Based Approach for Global Counterfactual Explanations. Dan Ley, Saumitra Mishra, Daniele Magazzeni. GLOBE-CE: CoRR abs/2305.17021 (2023)
3. Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses. Kaivalya Rawal and Himabindu Lakkaraju. NeurIPS 2020

The reviewer also mentions Gupta et al., but does not provide a specific reference.",1,"2022, 2023, 2023, 2020",Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses
BwW3NXOyXx,"This paper introduces an E(3)-invariant temporal attention scheme, calculated with the help of discrete Fourier transform, within the E(3)-equivariant GNN framework. The overall idea of considering higher-order temporal effects in physics is sound, and the formulation appears to be correct.

There are a few typos that do not affect the overall scoring, I would recommend the authors do a full proofreading.

There may be missing references and potentially a missing benchmark to compare with. I recommend adding these (in **Weaknesses**).

Overall, this work is solid, and I recommend accepting it for Neural IPS 2023.","The idea presented in the paper is novel, although not groundbreaking. It fills a gap in the existing framework and the direction is practical and meaningful.

There are empirical improvements.

The illustrations are very easy to follow.","There are some typos in the paper, such as the missing year in reference [16]. I recommend thorough proofreading.

—Lack of previous SOTA for comparison—

[1] Chen, Runfa and Han, Jiaqi and Sun, Fuchun and Huang, Wenbing. ""Subequivariant Graph Reinforcement Learning in 3D Environments"". Link: https://arxiv.org/abs/2305.18951

—Lack of reference for future improvements—

One future direction I have, which has already been used in [1], is combining equivariance with multi-scale (MS) GNN, as most industrial-level applications involve huge graphs. Therefore, the following papers should be cited as future works. Note that [2] also combines equivariance with MS, similar to [1]. However, since there are significant differences in graph type and application needs between this paper and [2], it is not suggested to compare them directly (but they should still be cited).

[2] Lino, Mario and Fotiadis, Stathi and Bharath, Anil A and Cantwell, Chris D. “Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics”. Link: https://pubs.aip.org/aip/pof/article/34/8/087110/2847850

[3] Cao, Yadi, Menglei Chai, Minchen Li, and Chenfanfu Jiang. ""Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network."". Link: https://openreview.net/forum?id=2Mbo7IEtZW

[4] Meire Fortunato, Tobias Pfaff, Peter Wirnsberger, Alexander Pritzel, Peter Battaglia. “MultiScale MeshGraphNets”. Link: https://arxiv.org/abs/2210.00612",Please refer to the **Weaknesses** section.,"Please see the comments regarding ""combining equivariance with multi-scale"" in the **Weaknesses** section. The attention module for higher-order temporal relationships will significantly increase complexity, which may limit the application in industrial scenarios with huge graphs. The author should either acknowledge this limit, and/or analyze potential remedies for this overhead.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,3 good,4 excellent,1688522960585,,,NeurIPS.cc/2023/Conference,35nFSbEBks,"['NeurIPS.cc/2023/Conference/Submission2734/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410860554,[],1,,,['everyone'],35nFSbEBks,['NeurIPS.cc/2023/Conference/Submission2734/Reviewer_TWyG'],1688522960585,1702410860554,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2734/Reviewer_TWyG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Chen et al. (2023) - ""Subequivariant Graph Reinforcement Learning in 3D Environments"" (reference [1])
2. Lino et al. - ""Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics"" (reference [2])
3. Cao et al. - ""Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network"" (reference [3])
4. Fortunato et al. - ""MultiScale MeshGraphNets"" (reference [4])

These citations are recommended as references for future improvements and to acknowledge related work in the field of combining equivariance with multi-scale graph neural networks.",1,2023,Subequivariant Graph Reinforcement Learning in 3D Environments 
BwW3NXOyXx,"This paper introduces an E(3)-invariant temporal attention scheme, calculated with the help of discrete Fourier transform, within the E(3)-equivariant GNN framework. The overall idea of considering higher-order temporal effects in physics is sound, and the formulation appears to be correct.

There are a few typos that do not affect the overall scoring, I would recommend the authors do a full proofreading.

There may be missing references and potentially a missing benchmark to compare with. I recommend adding these (in **Weaknesses**).

Overall, this work is solid, and I recommend accepting it for Neural IPS 2023.","The idea presented in the paper is novel, although not groundbreaking. It fills a gap in the existing framework and the direction is practical and meaningful.

There are empirical improvements.

The illustrations are very easy to follow.","There are some typos in the paper, such as the missing year in reference [16]. I recommend thorough proofreading.

—Lack of previous SOTA for comparison—

[1] Chen, Runfa and Han, Jiaqi and Sun, Fuchun and Huang, Wenbing. ""Subequivariant Graph Reinforcement Learning in 3D Environments"". Link: https://arxiv.org/abs/2305.18951

—Lack of reference for future improvements—

One future direction I have, which has already been used in [1], is combining equivariance with multi-scale (MS) GNN, as most industrial-level applications involve huge graphs. Therefore, the following papers should be cited as future works. Note that [2] also combines equivariance with MS, similar to [1]. However, since there are significant differences in graph type and application needs between this paper and [2], it is not suggested to compare them directly (but they should still be cited).

[2] Lino, Mario and Fotiadis, Stathi and Bharath, Anil A and Cantwell, Chris D. “Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics”. Link: https://pubs.aip.org/aip/pof/article/34/8/087110/2847850

[3] Cao, Yadi, Menglei Chai, Minchen Li, and Chenfanfu Jiang. ""Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network."". Link: https://openreview.net/forum?id=2Mbo7IEtZW

[4] Meire Fortunato, Tobias Pfaff, Peter Wirnsberger, Alexander Pritzel, Peter Battaglia. “MultiScale MeshGraphNets”. Link: https://arxiv.org/abs/2210.00612",Please refer to the **Weaknesses** section.,"Please see the comments regarding ""combining equivariance with multi-scale"" in the **Weaknesses** section. The attention module for higher-order temporal relationships will significantly increase complexity, which may limit the application in industrial scenarios with huge graphs. The author should either acknowledge this limit, and/or analyze potential remedies for this overhead.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,3 good,4 excellent,1688522960585,,,NeurIPS.cc/2023/Conference,35nFSbEBks,"['NeurIPS.cc/2023/Conference/Submission2734/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410860554,[],1,,,['everyone'],35nFSbEBks,['NeurIPS.cc/2023/Conference/Submission2734/Reviewer_TWyG'],1688522960585,1702410860554,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2734/Reviewer_TWyG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Chen et al. (2023) - ""Subequivariant Graph Reinforcement Learning in 3D Environments"" (reference [1])
2. Lino et al. - ""Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics"" (reference [2])
3. Cao et al. - ""Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network"" (reference [3])
4. Fortunato et al. - ""MultiScale MeshGraphNets"" (reference [4])

These citations are recommended as references for future improvements and to acknowledge related work in the field of combining equivariance with multi-scale graph neural networks.",1,2023,Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics 
BwW3NXOyXx,"This paper introduces an E(3)-invariant temporal attention scheme, calculated with the help of discrete Fourier transform, within the E(3)-equivariant GNN framework. The overall idea of considering higher-order temporal effects in physics is sound, and the formulation appears to be correct.

There are a few typos that do not affect the overall scoring, I would recommend the authors do a full proofreading.

There may be missing references and potentially a missing benchmark to compare with. I recommend adding these (in **Weaknesses**).

Overall, this work is solid, and I recommend accepting it for Neural IPS 2023.","The idea presented in the paper is novel, although not groundbreaking. It fills a gap in the existing framework and the direction is practical and meaningful.

There are empirical improvements.

The illustrations are very easy to follow.","There are some typos in the paper, such as the missing year in reference [16]. I recommend thorough proofreading.

—Lack of previous SOTA for comparison—

[1] Chen, Runfa and Han, Jiaqi and Sun, Fuchun and Huang, Wenbing. ""Subequivariant Graph Reinforcement Learning in 3D Environments"". Link: https://arxiv.org/abs/2305.18951

—Lack of reference for future improvements—

One future direction I have, which has already been used in [1], is combining equivariance with multi-scale (MS) GNN, as most industrial-level applications involve huge graphs. Therefore, the following papers should be cited as future works. Note that [2] also combines equivariance with MS, similar to [1]. However, since there are significant differences in graph type and application needs between this paper and [2], it is not suggested to compare them directly (but they should still be cited).

[2] Lino, Mario and Fotiadis, Stathi and Bharath, Anil A and Cantwell, Chris D. “Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics”. Link: https://pubs.aip.org/aip/pof/article/34/8/087110/2847850

[3] Cao, Yadi, Menglei Chai, Minchen Li, and Chenfanfu Jiang. ""Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network."". Link: https://openreview.net/forum?id=2Mbo7IEtZW

[4] Meire Fortunato, Tobias Pfaff, Peter Wirnsberger, Alexander Pritzel, Peter Battaglia. “MultiScale MeshGraphNets”. Link: https://arxiv.org/abs/2210.00612",Please refer to the **Weaknesses** section.,"Please see the comments regarding ""combining equivariance with multi-scale"" in the **Weaknesses** section. The attention module for higher-order temporal relationships will significantly increase complexity, which may limit the application in industrial scenarios with huge graphs. The author should either acknowledge this limit, and/or analyze potential remedies for this overhead.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,3 good,4 excellent,1688522960585,,,NeurIPS.cc/2023/Conference,35nFSbEBks,"['NeurIPS.cc/2023/Conference/Submission2734/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410860554,[],1,,,['everyone'],35nFSbEBks,['NeurIPS.cc/2023/Conference/Submission2734/Reviewer_TWyG'],1688522960585,1702410860554,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2734/Reviewer_TWyG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Chen et al. (2023) - ""Subequivariant Graph Reinforcement Learning in 3D Environments"" (reference [1])
2. Lino et al. - ""Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics"" (reference [2])
3. Cao et al. - ""Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network"" (reference [3])
4. Fortunato et al. - ""MultiScale MeshGraphNets"" (reference [4])

These citations are recommended as references for future improvements and to acknowledge related work in the field of combining equivariance with multi-scale graph neural networks.",1,2023,Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network 
BwW3NXOyXx,"This paper introduces an E(3)-invariant temporal attention scheme, calculated with the help of discrete Fourier transform, within the E(3)-equivariant GNN framework. The overall idea of considering higher-order temporal effects in physics is sound, and the formulation appears to be correct.

There are a few typos that do not affect the overall scoring, I would recommend the authors do a full proofreading.

There may be missing references and potentially a missing benchmark to compare with. I recommend adding these (in **Weaknesses**).

Overall, this work is solid, and I recommend accepting it for Neural IPS 2023.","The idea presented in the paper is novel, although not groundbreaking. It fills a gap in the existing framework and the direction is practical and meaningful.

There are empirical improvements.

The illustrations are very easy to follow.","There are some typos in the paper, such as the missing year in reference [16]. I recommend thorough proofreading.

—Lack of previous SOTA for comparison—

[1] Chen, Runfa and Han, Jiaqi and Sun, Fuchun and Huang, Wenbing. ""Subequivariant Graph Reinforcement Learning in 3D Environments"". Link: https://arxiv.org/abs/2305.18951

—Lack of reference for future improvements—

One future direction I have, which has already been used in [1], is combining equivariance with multi-scale (MS) GNN, as most industrial-level applications involve huge graphs. Therefore, the following papers should be cited as future works. Note that [2] also combines equivariance with MS, similar to [1]. However, since there are significant differences in graph type and application needs between this paper and [2], it is not suggested to compare them directly (but they should still be cited).

[2] Lino, Mario and Fotiadis, Stathi and Bharath, Anil A and Cantwell, Chris D. “Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics”. Link: https://pubs.aip.org/aip/pof/article/34/8/087110/2847850

[3] Cao, Yadi, Menglei Chai, Minchen Li, and Chenfanfu Jiang. ""Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network."". Link: https://openreview.net/forum?id=2Mbo7IEtZW

[4] Meire Fortunato, Tobias Pfaff, Peter Wirnsberger, Alexander Pritzel, Peter Battaglia. “MultiScale MeshGraphNets”. Link: https://arxiv.org/abs/2210.00612",Please refer to the **Weaknesses** section.,"Please see the comments regarding ""combining equivariance with multi-scale"" in the **Weaknesses** section. The attention module for higher-order temporal relationships will significantly increase complexity, which may limit the application in industrial scenarios with huge graphs. The author should either acknowledge this limit, and/or analyze potential remedies for this overhead.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,3 good,4 excellent,1688522960585,,,NeurIPS.cc/2023/Conference,35nFSbEBks,"['NeurIPS.cc/2023/Conference/Submission2734/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410860554,[],1,,,['everyone'],35nFSbEBks,['NeurIPS.cc/2023/Conference/Submission2734/Reviewer_TWyG'],1688522960585,1702410860554,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2734/Reviewer_TWyG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Chen et al. (2023) - ""Subequivariant Graph Reinforcement Learning in 3D Environments"" (reference [1])
2. Lino et al. - ""Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics"" (reference [2])
3. Cao et al. - ""Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network"" (reference [3])
4. Fortunato et al. - ""MultiScale MeshGraphNets"" (reference [4])

These citations are recommended as references for future improvements and to acknowledge related work in the field of combining equivariance with multi-scale graph neural networks.",1,2023,MultiScale MeshGraphNets
XoKOeWB3Fx,"This paper studies the non-Markovian dynamics that often appear in physical systems and proposes a spatio-temporal E(3) equivariant graph network that moves beyond the simple frame-to-frame prediction task.
The authors introduce an equivariant feature extraction method based on Fourier Transform, as well as separable equivariant spatial and temporal modules to process spatio-temporal information.
They evaluate the proposed method on different benchmarks and vastly outperform frame-to-frame equivariant methods and non-equivariant spatio-temporal GNNs.","The paper studies the non-Markovian dynamics in physical systems, an often overlooked yet very important property.
It is well-written and easy to follow. The novelties are clear, and the ablation studies support their usefulness.
The quantitative results show a massive performance gain from incorporating equivariance and sequence dynamics.
","The paper claims that ""we are the first to use equivariant spatio-temporal graph models for physical dynamics simulation"", yet it is missing out on a few related works, namely LoCS [1], and more recently, EqMotion [2]. Both works propose equivariant graph networks and focus on sequence-to-sequence prediction for physical systems. Hence, the authors should do a more thorough literature review and adjust their claims.

Many of the neural network modules used in the proposed method are not adequately described in the manuscript, and their significance is not tested with an ablation study. For example, the learnable parameters $w_k$ are only briefly described, and their exact form, as well as their usefulness, are unclear.

#### References
[1] Kofinas, Miltiadis et al. Roto-translated Local Coordinate Frames for Interacting Dynamical Systems. NeurIPS 2021.

[2] Xu, Chenxin et al. EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning. CVPR 2023.","1. Following the weaknesses above, a comparison with other spatio-temporal equivariant graph networks would further enhance the credibility of the proposed method.

2. How important is the use of $w_k$, given that these features are further processed during message passing?

3. Since this work focuses on non-Markovian dynamics, an ablation study on the optimal number of past timesteps would be beneficial and insightful.",The authors have adequately addressed the limitations.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688648044895,,,NeurIPS.cc/2023/Conference,35nFSbEBks,"['NeurIPS.cc/2023/Conference/Submission2734/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410860352,[],3,,,['everyone'],35nFSbEBks,['NeurIPS.cc/2023/Conference/Submission2734/Reviewer_Gpx3'],1688648044895,1702410860352,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2734/Reviewer_Gpx3']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. LoCS [1] (Kofinas, Miltiadis et al. Roto-translated Local Coordinate Frames for Interacting Dynamical Systems. NeurIPS 2021)
2. EqMotion [2] (Xu, Chenxin et al. EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning. CVPR 2023)

The reviewer mentions that the authors should do a more thorough literature review and adjust their claims, implying that they should cite these related works.",1,"2021, 2023",Roto-translated Local Coordinate Frames for Interacting Dynamical Systems 
XoKOeWB3Fx,"This paper studies the non-Markovian dynamics that often appear in physical systems and proposes a spatio-temporal E(3) equivariant graph network that moves beyond the simple frame-to-frame prediction task.
The authors introduce an equivariant feature extraction method based on Fourier Transform, as well as separable equivariant spatial and temporal modules to process spatio-temporal information.
They evaluate the proposed method on different benchmarks and vastly outperform frame-to-frame equivariant methods and non-equivariant spatio-temporal GNNs.","The paper studies the non-Markovian dynamics in physical systems, an often overlooked yet very important property.
It is well-written and easy to follow. The novelties are clear, and the ablation studies support their usefulness.
The quantitative results show a massive performance gain from incorporating equivariance and sequence dynamics.
","The paper claims that ""we are the first to use equivariant spatio-temporal graph models for physical dynamics simulation"", yet it is missing out on a few related works, namely LoCS [1], and more recently, EqMotion [2]. Both works propose equivariant graph networks and focus on sequence-to-sequence prediction for physical systems. Hence, the authors should do a more thorough literature review and adjust their claims.

Many of the neural network modules used in the proposed method are not adequately described in the manuscript, and their significance is not tested with an ablation study. For example, the learnable parameters $w_k$ are only briefly described, and their exact form, as well as their usefulness, are unclear.

#### References
[1] Kofinas, Miltiadis et al. Roto-translated Local Coordinate Frames for Interacting Dynamical Systems. NeurIPS 2021.

[2] Xu, Chenxin et al. EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning. CVPR 2023.","1. Following the weaknesses above, a comparison with other spatio-temporal equivariant graph networks would further enhance the credibility of the proposed method.

2. How important is the use of $w_k$, given that these features are further processed during message passing?

3. Since this work focuses on non-Markovian dynamics, an ablation study on the optimal number of past timesteps would be beneficial and insightful.",The authors have adequately addressed the limitations.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688648044895,,,NeurIPS.cc/2023/Conference,35nFSbEBks,"['NeurIPS.cc/2023/Conference/Submission2734/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410860352,[],3,,,['everyone'],35nFSbEBks,['NeurIPS.cc/2023/Conference/Submission2734/Reviewer_Gpx3'],1688648044895,1702410860352,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2734/Reviewer_Gpx3']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. LoCS [1] (Kofinas, Miltiadis et al. Roto-translated Local Coordinate Frames for Interacting Dynamical Systems. NeurIPS 2021)
2. EqMotion [2] (Xu, Chenxin et al. EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning. CVPR 2023)

The reviewer mentions that the authors should do a more thorough literature review and adjust their claims, implying that they should cite these related works.",1,"2021, 2023",EqMotion Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning
XOU37vdq8I,"The paper studies unsupervised OOD detection (i.e., training data contains no labels) using deep generative models. DGMs model the probability distribution of the inputs, and can be an ideal candidate for unsupervised OOD detection. The authors study one specific class of DGMs, namely VAEs. They show that VAEs suffer from overestimation problem ($P(x_{ood}) > P(x_{id})$) due to two main reasons — dataset’s inherent entropy and improper design of prior distribution. The paper then proceeds to theoretically suggest ways to mitigate this issue, and shows experimental results that do so.","1. The theory of the paper is simple but inspiring, and matches neatly with the designed algorithm.
2. The experiments are well-designed and executed.
3. The ablation studies are well-done.","1. Prior work such as [1] that discusses causes of deep generating models’ (specifically, normalizing flows) reason for failure to perform OOD detection was not cited/discussed in the paper. Similarly, [2] is also an important paper for using DGMs for OOD detection that wasn’t cited.
2. The paper is not self-contained and the organization could be improved — for example, one could put the limitations in the main paper instead of in the appendix.
3. Notation of the paper. For example, $p(x) = N(x | 0, \Sigma_x)$ can be more readable as $x \sim N(0, \Sigma_x)$, following more commonly used convention.

[1] Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson. Why Normalizing Flows Fail to Detect Out-of-Distribution Data, https://arxiv.org/abs/2006.08545, 2020

[2] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-distribution inputs to deep generative models using a test for typicality. arXiv preprint arXiv:1906.02994, 2019.","1. What is the architecture used for the supervised OOD detection methods in Table 1? Is it a 3 layer MLP?
2. Intuitively, supervised OOD detection methods should do better than unsupervised ones, since having access to more information should never hurt the performance. However, that is not the case in Table 1. Do the authors have an explanation for this? 
3. Line 278, why is the max epochs set to such a high value (1000)? Is there some sort of early stopping that is used here?
4. I am curious on how the reasons noted by [1], namely normalizing flows learning latent representations based on local pixel correlations and not semantic content, are relevant for VAEs. Does VAEs focus on semantic content and not local pixel correlations?


[1] Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson. Why Normalizing Flows Fail to Detect Out-of-Distribution Data, https://arxiv.org/abs/2006.08545, 2020",,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688656074653,,,NeurIPS.cc/2023/Conference,31zVEkOGYU,"['NeurIPS.cc/2023/Conference/Submission3187/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410885926,[],3,,,['everyone'],31zVEkOGYU,['NeurIPS.cc/2023/Conference/Submission3187/Reviewer_Bzqi'],1688656074653,1702410885926,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3187/Reviewer_Bzqi']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature sources:

1. [1] Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson. Why Normalizing Flows Fail to Detect Out-of-Distribution Data, https://arxiv.org/abs/2006.08545, 2020
2. [2] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-distribution inputs to deep generative models using a test for typicality. arXiv preprint arXiv:1906.02994, 2019.

The reviewer mentions that these papers were not cited or discussed in the paper, and that they are important for understanding the causes of deep generating models' failure to perform OOD detection and for using DGMs for OOD detection, respectively.",1,"2020, 2020, 2019, 2019",Why Normalizing Flows Fail to Detect Out-of-Distribution Data 
XOU37vdq8I,"The paper studies unsupervised OOD detection (i.e., training data contains no labels) using deep generative models. DGMs model the probability distribution of the inputs, and can be an ideal candidate for unsupervised OOD detection. The authors study one specific class of DGMs, namely VAEs. They show that VAEs suffer from overestimation problem ($P(x_{ood}) > P(x_{id})$) due to two main reasons — dataset’s inherent entropy and improper design of prior distribution. The paper then proceeds to theoretically suggest ways to mitigate this issue, and shows experimental results that do so.","1. The theory of the paper is simple but inspiring, and matches neatly with the designed algorithm.
2. The experiments are well-designed and executed.
3. The ablation studies are well-done.","1. Prior work such as [1] that discusses causes of deep generating models’ (specifically, normalizing flows) reason for failure to perform OOD detection was not cited/discussed in the paper. Similarly, [2] is also an important paper for using DGMs for OOD detection that wasn’t cited.
2. The paper is not self-contained and the organization could be improved — for example, one could put the limitations in the main paper instead of in the appendix.
3. Notation of the paper. For example, $p(x) = N(x | 0, \Sigma_x)$ can be more readable as $x \sim N(0, \Sigma_x)$, following more commonly used convention.

[1] Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson. Why Normalizing Flows Fail to Detect Out-of-Distribution Data, https://arxiv.org/abs/2006.08545, 2020

[2] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-distribution inputs to deep generative models using a test for typicality. arXiv preprint arXiv:1906.02994, 2019.","1. What is the architecture used for the supervised OOD detection methods in Table 1? Is it a 3 layer MLP?
2. Intuitively, supervised OOD detection methods should do better than unsupervised ones, since having access to more information should never hurt the performance. However, that is not the case in Table 1. Do the authors have an explanation for this? 
3. Line 278, why is the max epochs set to such a high value (1000)? Is there some sort of early stopping that is used here?
4. I am curious on how the reasons noted by [1], namely normalizing flows learning latent representations based on local pixel correlations and not semantic content, are relevant for VAEs. Does VAEs focus on semantic content and not local pixel correlations?


[1] Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson. Why Normalizing Flows Fail to Detect Out-of-Distribution Data, https://arxiv.org/abs/2006.08545, 2020",,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688656074653,,,NeurIPS.cc/2023/Conference,31zVEkOGYU,"['NeurIPS.cc/2023/Conference/Submission3187/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410885926,[],3,,,['everyone'],31zVEkOGYU,['NeurIPS.cc/2023/Conference/Submission3187/Reviewer_Bzqi'],1688656074653,1702410885926,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3187/Reviewer_Bzqi']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature sources:

1. [1] Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson. Why Normalizing Flows Fail to Detect Out-of-Distribution Data, https://arxiv.org/abs/2006.08545, 2020
2. [2] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-distribution inputs to deep generative models using a test for typicality. arXiv preprint arXiv:1906.02994, 2019.

The reviewer mentions that these papers were not cited or discussed in the paper, and that they are important for understanding the causes of deep generating models' failure to perform OOD detection and for using DGMs for OOD detection, respectively.",1,"2020, 2020, 2019, 2019",Detecting out-of-distribution inputs to deep generative models using a test for typicality
XcQfldGBFP,"This paper proposes a data augmentation technique named R-Blur to improve the robustness of vision classifiers against adversarial perturbations and other non-adversarial image corruptions. The method is inspired by human visual systems where the perceived scene consists of varying levels of fidelity. As such, the training images are modified in a way that adaptive Gaussian filtering is applied centered around the fixation point in the image. Results on CIFAR-10, Ecoset, and Imagenet demonstrate that R-Blur improves robustness to adversarial perturbations and common corruptions compared to standard trained models. ","Originality: There are only a few biologically inspired adversarial defense techniques. The proposed method mimics the peripheral vision in human visual systems and modifies the training images with an adaptive Gaussian filter. The approach is very unique and interesting.

Quality: The paper is well-written. 

Clarity: The motivation behind the proposed method as well as the overall structure of the paper is clear. The technical details are explained clearly.

Significance: The presence of adversarial examples presents a security concern for deep neural networks utilized in various applications. This paper introduces a novel approach to bolster network robustness.","This paper has two main weaknesses.
1. The choice of baseline methods in evaluation. The R-Blur method, in its essence, is a Gaussian data augmentation technique, while the only non-adversarial training technique in the baseline is RandAffine. To concretely verify that the adaptive filtering from R-Blur is indeed improving the robustness of the model beyond simple Gaussian data augmentation, other baseline methods such as Gaussian augmentation (with difference variances) and l2 regularization are necessary. 

2. The paper position itself as an approach to improve the adversarial robustness of deep neural networks. However, results in Sec. 3 show that the improvement in adversarial robustness towards the APGD attack is significantly lower than adversarial training. Also, the choice of $\epsilon$ in the adversarial robustness is much smaller than the standard values used in other works. ","Question/clarification:
Is $W_{V}$ in (1) the width of the image? 
Are (2), (3), and (4) from previous work ([26]?), or are they part of the novel contributions from this paper? Why are Laplace and Cauchy distributions used? Perhaps some additional discussion following the definitions can be helpful to further improve the clarity.

In the evaluation of adversarial robustness, why is only APGD in Autoattack used, rather than the complete version of AutoAttack?

In other adversarial training methods, we can explicitly control the trade-off between standard accuracy and robust accuracy. For instance, $\beta$ in TRADES, $\epsilon$ of the perturbation in standard adversarial training. Does such a concept exist for R-Blur? I think understanding such a mechanism can further improve the adversarial robustness of models trained with R-Blur.

Suggestion:
In Figure 4, it seems that the sequence of fixation points does not converge at all. Also, from Ln182, it seems that the results are based on randomly selected fixation points. One suggestion is to identify the fixation point as the pixel location with the highest saliency. Saliency-based data augmentation (i.e., Ma et al and Uddin et al) can be a good starting point.

Be consistent with the use of gray or grey (grayscale or greyscale).


Ma, Avery, et al. ""SAGE: Saliency-Guided Mixup with Optimal Rearrangements."" arXiv preprint arXiv:2211.00113 (2022).
Uddin, A. F. M., et al. ""Saliencymix: A saliency guided data augmentation strategy for better regularization."" arXiv preprint arXiv:2006.01791 (2020).",Limitations including the trade-off between the robust accuracy and the standard accuracy are discussed.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688792632279,,,NeurIPS.cc/2023/Conference,2tfG9QaFA7,"['NeurIPS.cc/2023/Conference/Submission13820/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411449166,[],4,,,['everyone'],2tfG9QaFA7,['NeurIPS.cc/2023/Conference/Submission13820/Reviewer_6sqj'],1688792632279,1702411449166,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13820/Reviewer_6sqj']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Ma, Avery, et al. ""SAGE: Saliency-Guided Mixup with Optimal Rearrangements."" arXiv preprint arXiv:2211.00113 (2022).
2. Uddin, A. F. M., et al. ""Saliencymix: A saliency guided data augmentation strategy for better regularization."" arXiv preprint arXiv:2006.01791 (2020).

These references are provided as potential starting points for the authors to explore saliency-based data augmentation techniques, which could be used to improve the R-Blur method by identifying fixation points based on image saliency.",1,"2022, 2022, 2020, 2020",SAGE Saliency-Guided Mixup with Optimal Rearrangements 
XcQfldGBFP,"This paper proposes a data augmentation technique named R-Blur to improve the robustness of vision classifiers against adversarial perturbations and other non-adversarial image corruptions. The method is inspired by human visual systems where the perceived scene consists of varying levels of fidelity. As such, the training images are modified in a way that adaptive Gaussian filtering is applied centered around the fixation point in the image. Results on CIFAR-10, Ecoset, and Imagenet demonstrate that R-Blur improves robustness to adversarial perturbations and common corruptions compared to standard trained models. ","Originality: There are only a few biologically inspired adversarial defense techniques. The proposed method mimics the peripheral vision in human visual systems and modifies the training images with an adaptive Gaussian filter. The approach is very unique and interesting.

Quality: The paper is well-written. 

Clarity: The motivation behind the proposed method as well as the overall structure of the paper is clear. The technical details are explained clearly.

Significance: The presence of adversarial examples presents a security concern for deep neural networks utilized in various applications. This paper introduces a novel approach to bolster network robustness.","This paper has two main weaknesses.
1. The choice of baseline methods in evaluation. The R-Blur method, in its essence, is a Gaussian data augmentation technique, while the only non-adversarial training technique in the baseline is RandAffine. To concretely verify that the adaptive filtering from R-Blur is indeed improving the robustness of the model beyond simple Gaussian data augmentation, other baseline methods such as Gaussian augmentation (with difference variances) and l2 regularization are necessary. 

2. The paper position itself as an approach to improve the adversarial robustness of deep neural networks. However, results in Sec. 3 show that the improvement in adversarial robustness towards the APGD attack is significantly lower than adversarial training. Also, the choice of $\epsilon$ in the adversarial robustness is much smaller than the standard values used in other works. ","Question/clarification:
Is $W_{V}$ in (1) the width of the image? 
Are (2), (3), and (4) from previous work ([26]?), or are they part of the novel contributions from this paper? Why are Laplace and Cauchy distributions used? Perhaps some additional discussion following the definitions can be helpful to further improve the clarity.

In the evaluation of adversarial robustness, why is only APGD in Autoattack used, rather than the complete version of AutoAttack?

In other adversarial training methods, we can explicitly control the trade-off between standard accuracy and robust accuracy. For instance, $\beta$ in TRADES, $\epsilon$ of the perturbation in standard adversarial training. Does such a concept exist for R-Blur? I think understanding such a mechanism can further improve the adversarial robustness of models trained with R-Blur.

Suggestion:
In Figure 4, it seems that the sequence of fixation points does not converge at all. Also, from Ln182, it seems that the results are based on randomly selected fixation points. One suggestion is to identify the fixation point as the pixel location with the highest saliency. Saliency-based data augmentation (i.e., Ma et al and Uddin et al) can be a good starting point.

Be consistent with the use of gray or grey (grayscale or greyscale).


Ma, Avery, et al. ""SAGE: Saliency-Guided Mixup with Optimal Rearrangements."" arXiv preprint arXiv:2211.00113 (2022).
Uddin, A. F. M., et al. ""Saliencymix: A saliency guided data augmentation strategy for better regularization."" arXiv preprint arXiv:2006.01791 (2020).",Limitations including the trade-off between the robust accuracy and the standard accuracy are discussed.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688792632279,,,NeurIPS.cc/2023/Conference,2tfG9QaFA7,"['NeurIPS.cc/2023/Conference/Submission13820/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411449166,[],4,,,['everyone'],2tfG9QaFA7,['NeurIPS.cc/2023/Conference/Submission13820/Reviewer_6sqj'],1688792632279,1702411449166,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13820/Reviewer_6sqj']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Ma, Avery, et al. ""SAGE: Saliency-Guided Mixup with Optimal Rearrangements."" arXiv preprint arXiv:2211.00113 (2022).
2. Uddin, A. F. M., et al. ""Saliencymix: A saliency guided data augmentation strategy for better regularization."" arXiv preprint arXiv:2006.01791 (2020).

These references are provided as potential starting points for the authors to explore saliency-based data augmentation techniques, which could be used to improve the R-Blur method by identifying fixation points based on image saliency.",1,"2022, 2022, 2020, 2020",Saliencymix A saliency guided data augmentation strategy for better regularization
CtfWuv6K6Z,"This paper attempts to decompose the learned representation of a data-driven saliency model into a constituent set of bases that are mapped onto semantic concepts, thereby providing insight into what is driving the model's representation of saliency. This method is applied to three different saliency models of varying formulation over several datasets. Some discussion is then provided of the results, as well as some qualitative discussion of examples of model failures.","The paper takes on a challenging and open-ended problem in saliency modelling, namely the difficulty of teasing apart the different contributions to attentional capture. Similarly, the work provides an example of continued work in general explainability within deep learning methods, which is an important issue within the field.

The paper is clearly written and relatively easy to follow, although there are a few details that are missing (such as the choice of N).","There are some references that seem pertinent that were not discussed. In particular, significant work on failure modes of modern saliency models was poorly represented.
- Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
  -- This paper digs into some of the failure modes common to deep learning models, including object vs. background and semantic vs. feature contrast elements (e.g. see Figure 7), providing pertinent insights to the discussion in the submission.
- Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
  -- This paper explicitly explores constituent elements of saliency representation in deep networks from the perspective of high-level vs. low-level features. Given the way the paper attempts to tease apart the representation of saliency between different feature classes, it is conceptually highly relevant background for the current submission.
- Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
  -- This paper provides a psychophysical (P^3) and natural image (O^3) dataset with targets explicitly defined by low-level salient features (e.g. colour, orientation, shape, or size singletons), and finds that saliency models (including deep learning-based models) largely perform quite poorly. For exploring failure rates the O^3 dataset would be a potentially useful (albeit ground truth was defined by semantic object annotation and not fixation data), but even if the dataset is not used the examination of model failures in the submission should include the context of this prior exploration.
- Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
  -- This paper explores the evolution of fixations through time, including aspects such as central vs. peripheral distribution and inter-subject consistency of fixation location. Given that this is one area that the submission claims novelty, it would be good to put it in context with prior explorations in this area of the temporal evolution of low-level human attention.

Overall, while the paper is interesting and tackles an exceedingly challenging problem, I think there are a number of conceptual issues that it needs to overcome. While some specific issues are given in the questions below, the primary issue is that while the submission encodes the positive/negative importance of the various bases extracted, it is well established within psychophysics that the relative importance of elements to saliency is contextual (e.g. see Nothdurft (1993) ""Saliency effects across dimensions in visual search""; Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions"" for some low level examples), and so these relative attributes are likely to change from image to image. Within the current submission, these attributes change from model to model and dataset to dataset; what conclusions are to be drawn from this? Is the technique shedding light on dataset composition, model bias, or some deeper aspect of relative aspects of saliency? Much of the analysis is presented without clear connection to either human behaviour (with the exception of Section 4.5, which cleverly makes use of the technique to explore the representations learned from data from different human subject populations or conditions) or model performance in a traditional sense, making it difficult to put into context or derive deeper insight.","- Although drawing on the architectures of SALICON, DINet, and TranSalNet, the model configurations in this paper are distinct from the instantiations of the original publications in order to accommodate the need for trainable bases. How does that change the ultimate behaviour of the models with respect to standard measures of saliency performance? Alternatively, even a quantified value for the change in saliency maps when compared within model (e.g. the paper's version of SALICON correlated against the standard instantiation of SALICON) would help put the paper's results in context with the existing literature.

- What is the value of N (the number of bases)? How stable are the results with respect to N (i.e. does the mapped semantic content change substantially with even small changes in N)?

- Each basis is mapped onto a top-5 semantic mix. Why not onto a single concept? How was 5 selected? Similar to the previous question, how does this choice affect the subsequent analysis?

- How is ""action"" defined in a static image? The primary example given, ""having meeting"", seems like a social activity, which was a separate category. I get that it is challenging to relate the messy details of semantic categorization in a short paper, but given that this is central to the topic of the paper, I think it needs a clearer explanation.

- How do the insights provided in this paper relate to model performance? When the models show a markedly different breakdown of salient factors (e.g. Figure 4, which shows SALICON emphasizing vehicles much more strongly than TranSalNet, while TranSalNet emphasizes clothing more than any other model), does this correlate with predictive accuracy?

- Related to the previous question, could you use the IoU Measurement process used to assign labels to the bases to provide an approximate breakdown of the factors leading to human fixations directly? This might provide another point of comparison to better put the results of this paper in context.

- Given the range of behaviours across the models shown in Figure 4, why does Figure 5 average the semantic weights across models (also, this should be noted in the caption; when I first read the paper I was quite confused which model was being shown in Figure 5)? What is the justification for this? Do the models tend to converge after fine-tuning?

- Figure 6 (b.) and (c.): are these results for subjects without autism? If so, this should be more clearly noted.

- What was meant by line 304: ""Whether models... open question.""? What would lead to a model behaving ""even better than humans"", given that humans are the system trying to be modelled?",The limitations discussed seem clear.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688619991190,,,NeurIPS.cc/2023/Conference,2rq4LwwjfE,"['NeurIPS.cc/2023/Conference/Submission503/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410746654,[],3,,,['everyone'],2rq4LwwjfE,['NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD'],1688619991190,1702410746654,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
2. Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
3. Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
4. Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
5. Nothdurft (1993) ""Saliency effects across dimensions in visual search""
6. Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions""

These references are mentioned as relevant background or prior work that the authors should consider discussing or citing in their paper to provide a more comprehensive understanding of the topic and to put their results into context.",1,"2016, 2017, 2019, 2005, 1993, 2000","A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond "
CtfWuv6K6Z,"This paper attempts to decompose the learned representation of a data-driven saliency model into a constituent set of bases that are mapped onto semantic concepts, thereby providing insight into what is driving the model's representation of saliency. This method is applied to three different saliency models of varying formulation over several datasets. Some discussion is then provided of the results, as well as some qualitative discussion of examples of model failures.","The paper takes on a challenging and open-ended problem in saliency modelling, namely the difficulty of teasing apart the different contributions to attentional capture. Similarly, the work provides an example of continued work in general explainability within deep learning methods, which is an important issue within the field.

The paper is clearly written and relatively easy to follow, although there are a few details that are missing (such as the choice of N).","There are some references that seem pertinent that were not discussed. In particular, significant work on failure modes of modern saliency models was poorly represented.
- Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
  -- This paper digs into some of the failure modes common to deep learning models, including object vs. background and semantic vs. feature contrast elements (e.g. see Figure 7), providing pertinent insights to the discussion in the submission.
- Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
  -- This paper explicitly explores constituent elements of saliency representation in deep networks from the perspective of high-level vs. low-level features. Given the way the paper attempts to tease apart the representation of saliency between different feature classes, it is conceptually highly relevant background for the current submission.
- Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
  -- This paper provides a psychophysical (P^3) and natural image (O^3) dataset with targets explicitly defined by low-level salient features (e.g. colour, orientation, shape, or size singletons), and finds that saliency models (including deep learning-based models) largely perform quite poorly. For exploring failure rates the O^3 dataset would be a potentially useful (albeit ground truth was defined by semantic object annotation and not fixation data), but even if the dataset is not used the examination of model failures in the submission should include the context of this prior exploration.
- Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
  -- This paper explores the evolution of fixations through time, including aspects such as central vs. peripheral distribution and inter-subject consistency of fixation location. Given that this is one area that the submission claims novelty, it would be good to put it in context with prior explorations in this area of the temporal evolution of low-level human attention.

Overall, while the paper is interesting and tackles an exceedingly challenging problem, I think there are a number of conceptual issues that it needs to overcome. While some specific issues are given in the questions below, the primary issue is that while the submission encodes the positive/negative importance of the various bases extracted, it is well established within psychophysics that the relative importance of elements to saliency is contextual (e.g. see Nothdurft (1993) ""Saliency effects across dimensions in visual search""; Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions"" for some low level examples), and so these relative attributes are likely to change from image to image. Within the current submission, these attributes change from model to model and dataset to dataset; what conclusions are to be drawn from this? Is the technique shedding light on dataset composition, model bias, or some deeper aspect of relative aspects of saliency? Much of the analysis is presented without clear connection to either human behaviour (with the exception of Section 4.5, which cleverly makes use of the technique to explore the representations learned from data from different human subject populations or conditions) or model performance in a traditional sense, making it difficult to put into context or derive deeper insight.","- Although drawing on the architectures of SALICON, DINet, and TranSalNet, the model configurations in this paper are distinct from the instantiations of the original publications in order to accommodate the need for trainable bases. How does that change the ultimate behaviour of the models with respect to standard measures of saliency performance? Alternatively, even a quantified value for the change in saliency maps when compared within model (e.g. the paper's version of SALICON correlated against the standard instantiation of SALICON) would help put the paper's results in context with the existing literature.

- What is the value of N (the number of bases)? How stable are the results with respect to N (i.e. does the mapped semantic content change substantially with even small changes in N)?

- Each basis is mapped onto a top-5 semantic mix. Why not onto a single concept? How was 5 selected? Similar to the previous question, how does this choice affect the subsequent analysis?

- How is ""action"" defined in a static image? The primary example given, ""having meeting"", seems like a social activity, which was a separate category. I get that it is challenging to relate the messy details of semantic categorization in a short paper, but given that this is central to the topic of the paper, I think it needs a clearer explanation.

- How do the insights provided in this paper relate to model performance? When the models show a markedly different breakdown of salient factors (e.g. Figure 4, which shows SALICON emphasizing vehicles much more strongly than TranSalNet, while TranSalNet emphasizes clothing more than any other model), does this correlate with predictive accuracy?

- Related to the previous question, could you use the IoU Measurement process used to assign labels to the bases to provide an approximate breakdown of the factors leading to human fixations directly? This might provide another point of comparison to better put the results of this paper in context.

- Given the range of behaviours across the models shown in Figure 4, why does Figure 5 average the semantic weights across models (also, this should be noted in the caption; when I first read the paper I was quite confused which model was being shown in Figure 5)? What is the justification for this? Do the models tend to converge after fine-tuning?

- Figure 6 (b.) and (c.): are these results for subjects without autism? If so, this should be more clearly noted.

- What was meant by line 304: ""Whether models... open question.""? What would lead to a model behaving ""even better than humans"", given that humans are the system trying to be modelled?",The limitations discussed seem clear.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688619991190,,,NeurIPS.cc/2023/Conference,2rq4LwwjfE,"['NeurIPS.cc/2023/Conference/Submission503/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410746654,[],3,,,['everyone'],2rq4LwwjfE,['NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD'],1688619991190,1702410746654,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
2. Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
3. Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
4. Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
5. Nothdurft (1993) ""Saliency effects across dimensions in visual search""
6. Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions""

These references are mentioned as relevant background or prior work that the authors should consider discussing or citing in their paper to provide a more comprehensive understanding of the topic and to put their results into context.",1,"2016, 2017, 2019, 2005, 1993, 2000",Understanding Low- and High-Level Contributions to Fixation Prediction 
CtfWuv6K6Z,"This paper attempts to decompose the learned representation of a data-driven saliency model into a constituent set of bases that are mapped onto semantic concepts, thereby providing insight into what is driving the model's representation of saliency. This method is applied to three different saliency models of varying formulation over several datasets. Some discussion is then provided of the results, as well as some qualitative discussion of examples of model failures.","The paper takes on a challenging and open-ended problem in saliency modelling, namely the difficulty of teasing apart the different contributions to attentional capture. Similarly, the work provides an example of continued work in general explainability within deep learning methods, which is an important issue within the field.

The paper is clearly written and relatively easy to follow, although there are a few details that are missing (such as the choice of N).","There are some references that seem pertinent that were not discussed. In particular, significant work on failure modes of modern saliency models was poorly represented.
- Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
  -- This paper digs into some of the failure modes common to deep learning models, including object vs. background and semantic vs. feature contrast elements (e.g. see Figure 7), providing pertinent insights to the discussion in the submission.
- Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
  -- This paper explicitly explores constituent elements of saliency representation in deep networks from the perspective of high-level vs. low-level features. Given the way the paper attempts to tease apart the representation of saliency between different feature classes, it is conceptually highly relevant background for the current submission.
- Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
  -- This paper provides a psychophysical (P^3) and natural image (O^3) dataset with targets explicitly defined by low-level salient features (e.g. colour, orientation, shape, or size singletons), and finds that saliency models (including deep learning-based models) largely perform quite poorly. For exploring failure rates the O^3 dataset would be a potentially useful (albeit ground truth was defined by semantic object annotation and not fixation data), but even if the dataset is not used the examination of model failures in the submission should include the context of this prior exploration.
- Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
  -- This paper explores the evolution of fixations through time, including aspects such as central vs. peripheral distribution and inter-subject consistency of fixation location. Given that this is one area that the submission claims novelty, it would be good to put it in context with prior explorations in this area of the temporal evolution of low-level human attention.

Overall, while the paper is interesting and tackles an exceedingly challenging problem, I think there are a number of conceptual issues that it needs to overcome. While some specific issues are given in the questions below, the primary issue is that while the submission encodes the positive/negative importance of the various bases extracted, it is well established within psychophysics that the relative importance of elements to saliency is contextual (e.g. see Nothdurft (1993) ""Saliency effects across dimensions in visual search""; Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions"" for some low level examples), and so these relative attributes are likely to change from image to image. Within the current submission, these attributes change from model to model and dataset to dataset; what conclusions are to be drawn from this? Is the technique shedding light on dataset composition, model bias, or some deeper aspect of relative aspects of saliency? Much of the analysis is presented without clear connection to either human behaviour (with the exception of Section 4.5, which cleverly makes use of the technique to explore the representations learned from data from different human subject populations or conditions) or model performance in a traditional sense, making it difficult to put into context or derive deeper insight.","- Although drawing on the architectures of SALICON, DINet, and TranSalNet, the model configurations in this paper are distinct from the instantiations of the original publications in order to accommodate the need for trainable bases. How does that change the ultimate behaviour of the models with respect to standard measures of saliency performance? Alternatively, even a quantified value for the change in saliency maps when compared within model (e.g. the paper's version of SALICON correlated against the standard instantiation of SALICON) would help put the paper's results in context with the existing literature.

- What is the value of N (the number of bases)? How stable are the results with respect to N (i.e. does the mapped semantic content change substantially with even small changes in N)?

- Each basis is mapped onto a top-5 semantic mix. Why not onto a single concept? How was 5 selected? Similar to the previous question, how does this choice affect the subsequent analysis?

- How is ""action"" defined in a static image? The primary example given, ""having meeting"", seems like a social activity, which was a separate category. I get that it is challenging to relate the messy details of semantic categorization in a short paper, but given that this is central to the topic of the paper, I think it needs a clearer explanation.

- How do the insights provided in this paper relate to model performance? When the models show a markedly different breakdown of salient factors (e.g. Figure 4, which shows SALICON emphasizing vehicles much more strongly than TranSalNet, while TranSalNet emphasizes clothing more than any other model), does this correlate with predictive accuracy?

- Related to the previous question, could you use the IoU Measurement process used to assign labels to the bases to provide an approximate breakdown of the factors leading to human fixations directly? This might provide another point of comparison to better put the results of this paper in context.

- Given the range of behaviours across the models shown in Figure 4, why does Figure 5 average the semantic weights across models (also, this should be noted in the caption; when I first read the paper I was quite confused which model was being shown in Figure 5)? What is the justification for this? Do the models tend to converge after fine-tuning?

- Figure 6 (b.) and (c.): are these results for subjects without autism? If so, this should be more clearly noted.

- What was meant by line 304: ""Whether models... open question.""? What would lead to a model behaving ""even better than humans"", given that humans are the system trying to be modelled?",The limitations discussed seem clear.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688619991190,,,NeurIPS.cc/2023/Conference,2rq4LwwjfE,"['NeurIPS.cc/2023/Conference/Submission503/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410746654,[],3,,,['everyone'],2rq4LwwjfE,['NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD'],1688619991190,1702410746654,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
2. Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
3. Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
4. Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
5. Nothdurft (1993) ""Saliency effects across dimensions in visual search""
6. Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions""

These references are mentioned as relevant background or prior work that the authors should consider discussing or citing in their paper to provide a more comprehensive understanding of the topic and to put their results into context.",1,"2016, 2017, 2019, 2005, 1993, 2000",Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations 
CtfWuv6K6Z,"This paper attempts to decompose the learned representation of a data-driven saliency model into a constituent set of bases that are mapped onto semantic concepts, thereby providing insight into what is driving the model's representation of saliency. This method is applied to three different saliency models of varying formulation over several datasets. Some discussion is then provided of the results, as well as some qualitative discussion of examples of model failures.","The paper takes on a challenging and open-ended problem in saliency modelling, namely the difficulty of teasing apart the different contributions to attentional capture. Similarly, the work provides an example of continued work in general explainability within deep learning methods, which is an important issue within the field.

The paper is clearly written and relatively easy to follow, although there are a few details that are missing (such as the choice of N).","There are some references that seem pertinent that were not discussed. In particular, significant work on failure modes of modern saliency models was poorly represented.
- Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
  -- This paper digs into some of the failure modes common to deep learning models, including object vs. background and semantic vs. feature contrast elements (e.g. see Figure 7), providing pertinent insights to the discussion in the submission.
- Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
  -- This paper explicitly explores constituent elements of saliency representation in deep networks from the perspective of high-level vs. low-level features. Given the way the paper attempts to tease apart the representation of saliency between different feature classes, it is conceptually highly relevant background for the current submission.
- Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
  -- This paper provides a psychophysical (P^3) and natural image (O^3) dataset with targets explicitly defined by low-level salient features (e.g. colour, orientation, shape, or size singletons), and finds that saliency models (including deep learning-based models) largely perform quite poorly. For exploring failure rates the O^3 dataset would be a potentially useful (albeit ground truth was defined by semantic object annotation and not fixation data), but even if the dataset is not used the examination of model failures in the submission should include the context of this prior exploration.
- Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
  -- This paper explores the evolution of fixations through time, including aspects such as central vs. peripheral distribution and inter-subject consistency of fixation location. Given that this is one area that the submission claims novelty, it would be good to put it in context with prior explorations in this area of the temporal evolution of low-level human attention.

Overall, while the paper is interesting and tackles an exceedingly challenging problem, I think there are a number of conceptual issues that it needs to overcome. While some specific issues are given in the questions below, the primary issue is that while the submission encodes the positive/negative importance of the various bases extracted, it is well established within psychophysics that the relative importance of elements to saliency is contextual (e.g. see Nothdurft (1993) ""Saliency effects across dimensions in visual search""; Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions"" for some low level examples), and so these relative attributes are likely to change from image to image. Within the current submission, these attributes change from model to model and dataset to dataset; what conclusions are to be drawn from this? Is the technique shedding light on dataset composition, model bias, or some deeper aspect of relative aspects of saliency? Much of the analysis is presented without clear connection to either human behaviour (with the exception of Section 4.5, which cleverly makes use of the technique to explore the representations learned from data from different human subject populations or conditions) or model performance in a traditional sense, making it difficult to put into context or derive deeper insight.","- Although drawing on the architectures of SALICON, DINet, and TranSalNet, the model configurations in this paper are distinct from the instantiations of the original publications in order to accommodate the need for trainable bases. How does that change the ultimate behaviour of the models with respect to standard measures of saliency performance? Alternatively, even a quantified value for the change in saliency maps when compared within model (e.g. the paper's version of SALICON correlated against the standard instantiation of SALICON) would help put the paper's results in context with the existing literature.

- What is the value of N (the number of bases)? How stable are the results with respect to N (i.e. does the mapped semantic content change substantially with even small changes in N)?

- Each basis is mapped onto a top-5 semantic mix. Why not onto a single concept? How was 5 selected? Similar to the previous question, how does this choice affect the subsequent analysis?

- How is ""action"" defined in a static image? The primary example given, ""having meeting"", seems like a social activity, which was a separate category. I get that it is challenging to relate the messy details of semantic categorization in a short paper, but given that this is central to the topic of the paper, I think it needs a clearer explanation.

- How do the insights provided in this paper relate to model performance? When the models show a markedly different breakdown of salient factors (e.g. Figure 4, which shows SALICON emphasizing vehicles much more strongly than TranSalNet, while TranSalNet emphasizes clothing more than any other model), does this correlate with predictive accuracy?

- Related to the previous question, could you use the IoU Measurement process used to assign labels to the bases to provide an approximate breakdown of the factors leading to human fixations directly? This might provide another point of comparison to better put the results of this paper in context.

- Given the range of behaviours across the models shown in Figure 4, why does Figure 5 average the semantic weights across models (also, this should be noted in the caption; when I first read the paper I was quite confused which model was being shown in Figure 5)? What is the justification for this? Do the models tend to converge after fine-tuning?

- Figure 6 (b.) and (c.): are these results for subjects without autism? If so, this should be more clearly noted.

- What was meant by line 304: ""Whether models... open question.""? What would lead to a model behaving ""even better than humans"", given that humans are the system trying to be modelled?",The limitations discussed seem clear.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688619991190,,,NeurIPS.cc/2023/Conference,2rq4LwwjfE,"['NeurIPS.cc/2023/Conference/Submission503/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410746654,[],3,,,['everyone'],2rq4LwwjfE,['NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD'],1688619991190,1702410746654,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
2. Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
3. Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
4. Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
5. Nothdurft (1993) ""Saliency effects across dimensions in visual search""
6. Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions""

These references are mentioned as relevant background or prior work that the authors should consider discussing or citing in their paper to provide a more comprehensive understanding of the topic and to put their results into context.",1,"2016, 2017, 2019, 2005, 1993, 2000",Visual correlates of fixation selection: effects of scale and time 
CtfWuv6K6Z,"This paper attempts to decompose the learned representation of a data-driven saliency model into a constituent set of bases that are mapped onto semantic concepts, thereby providing insight into what is driving the model's representation of saliency. This method is applied to three different saliency models of varying formulation over several datasets. Some discussion is then provided of the results, as well as some qualitative discussion of examples of model failures.","The paper takes on a challenging and open-ended problem in saliency modelling, namely the difficulty of teasing apart the different contributions to attentional capture. Similarly, the work provides an example of continued work in general explainability within deep learning methods, which is an important issue within the field.

The paper is clearly written and relatively easy to follow, although there are a few details that are missing (such as the choice of N).","There are some references that seem pertinent that were not discussed. In particular, significant work on failure modes of modern saliency models was poorly represented.
- Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
  -- This paper digs into some of the failure modes common to deep learning models, including object vs. background and semantic vs. feature contrast elements (e.g. see Figure 7), providing pertinent insights to the discussion in the submission.
- Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
  -- This paper explicitly explores constituent elements of saliency representation in deep networks from the perspective of high-level vs. low-level features. Given the way the paper attempts to tease apart the representation of saliency between different feature classes, it is conceptually highly relevant background for the current submission.
- Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
  -- This paper provides a psychophysical (P^3) and natural image (O^3) dataset with targets explicitly defined by low-level salient features (e.g. colour, orientation, shape, or size singletons), and finds that saliency models (including deep learning-based models) largely perform quite poorly. For exploring failure rates the O^3 dataset would be a potentially useful (albeit ground truth was defined by semantic object annotation and not fixation data), but even if the dataset is not used the examination of model failures in the submission should include the context of this prior exploration.
- Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
  -- This paper explores the evolution of fixations through time, including aspects such as central vs. peripheral distribution and inter-subject consistency of fixation location. Given that this is one area that the submission claims novelty, it would be good to put it in context with prior explorations in this area of the temporal evolution of low-level human attention.

Overall, while the paper is interesting and tackles an exceedingly challenging problem, I think there are a number of conceptual issues that it needs to overcome. While some specific issues are given in the questions below, the primary issue is that while the submission encodes the positive/negative importance of the various bases extracted, it is well established within psychophysics that the relative importance of elements to saliency is contextual (e.g. see Nothdurft (1993) ""Saliency effects across dimensions in visual search""; Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions"" for some low level examples), and so these relative attributes are likely to change from image to image. Within the current submission, these attributes change from model to model and dataset to dataset; what conclusions are to be drawn from this? Is the technique shedding light on dataset composition, model bias, or some deeper aspect of relative aspects of saliency? Much of the analysis is presented without clear connection to either human behaviour (with the exception of Section 4.5, which cleverly makes use of the technique to explore the representations learned from data from different human subject populations or conditions) or model performance in a traditional sense, making it difficult to put into context or derive deeper insight.","- Although drawing on the architectures of SALICON, DINet, and TranSalNet, the model configurations in this paper are distinct from the instantiations of the original publications in order to accommodate the need for trainable bases. How does that change the ultimate behaviour of the models with respect to standard measures of saliency performance? Alternatively, even a quantified value for the change in saliency maps when compared within model (e.g. the paper's version of SALICON correlated against the standard instantiation of SALICON) would help put the paper's results in context with the existing literature.

- What is the value of N (the number of bases)? How stable are the results with respect to N (i.e. does the mapped semantic content change substantially with even small changes in N)?

- Each basis is mapped onto a top-5 semantic mix. Why not onto a single concept? How was 5 selected? Similar to the previous question, how does this choice affect the subsequent analysis?

- How is ""action"" defined in a static image? The primary example given, ""having meeting"", seems like a social activity, which was a separate category. I get that it is challenging to relate the messy details of semantic categorization in a short paper, but given that this is central to the topic of the paper, I think it needs a clearer explanation.

- How do the insights provided in this paper relate to model performance? When the models show a markedly different breakdown of salient factors (e.g. Figure 4, which shows SALICON emphasizing vehicles much more strongly than TranSalNet, while TranSalNet emphasizes clothing more than any other model), does this correlate with predictive accuracy?

- Related to the previous question, could you use the IoU Measurement process used to assign labels to the bases to provide an approximate breakdown of the factors leading to human fixations directly? This might provide another point of comparison to better put the results of this paper in context.

- Given the range of behaviours across the models shown in Figure 4, why does Figure 5 average the semantic weights across models (also, this should be noted in the caption; when I first read the paper I was quite confused which model was being shown in Figure 5)? What is the justification for this? Do the models tend to converge after fine-tuning?

- Figure 6 (b.) and (c.): are these results for subjects without autism? If so, this should be more clearly noted.

- What was meant by line 304: ""Whether models... open question.""? What would lead to a model behaving ""even better than humans"", given that humans are the system trying to be modelled?",The limitations discussed seem clear.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688619991190,,,NeurIPS.cc/2023/Conference,2rq4LwwjfE,"['NeurIPS.cc/2023/Conference/Submission503/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410746654,[],3,,,['everyone'],2rq4LwwjfE,['NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD'],1688619991190,1702410746654,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
2. Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
3. Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
4. Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
5. Nothdurft (1993) ""Saliency effects across dimensions in visual search""
6. Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions""

These references are mentioned as relevant background or prior work that the authors should consider discussing or citing in their paper to provide a more comprehensive understanding of the topic and to put their results into context.",1,"2016, 2017, 2019, 2005, 1993, 2000",Saliency effects across dimensions in visual search 
CtfWuv6K6Z,"This paper attempts to decompose the learned representation of a data-driven saliency model into a constituent set of bases that are mapped onto semantic concepts, thereby providing insight into what is driving the model's representation of saliency. This method is applied to three different saliency models of varying formulation over several datasets. Some discussion is then provided of the results, as well as some qualitative discussion of examples of model failures.","The paper takes on a challenging and open-ended problem in saliency modelling, namely the difficulty of teasing apart the different contributions to attentional capture. Similarly, the work provides an example of continued work in general explainability within deep learning methods, which is an important issue within the field.

The paper is clearly written and relatively easy to follow, although there are a few details that are missing (such as the choice of N).","There are some references that seem pertinent that were not discussed. In particular, significant work on failure modes of modern saliency models was poorly represented.
- Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
  -- This paper digs into some of the failure modes common to deep learning models, including object vs. background and semantic vs. feature contrast elements (e.g. see Figure 7), providing pertinent insights to the discussion in the submission.
- Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
  -- This paper explicitly explores constituent elements of saliency representation in deep networks from the perspective of high-level vs. low-level features. Given the way the paper attempts to tease apart the representation of saliency between different feature classes, it is conceptually highly relevant background for the current submission.
- Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
  -- This paper provides a psychophysical (P^3) and natural image (O^3) dataset with targets explicitly defined by low-level salient features (e.g. colour, orientation, shape, or size singletons), and finds that saliency models (including deep learning-based models) largely perform quite poorly. For exploring failure rates the O^3 dataset would be a potentially useful (albeit ground truth was defined by semantic object annotation and not fixation data), but even if the dataset is not used the examination of model failures in the submission should include the context of this prior exploration.
- Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
  -- This paper explores the evolution of fixations through time, including aspects such as central vs. peripheral distribution and inter-subject consistency of fixation location. Given that this is one area that the submission claims novelty, it would be good to put it in context with prior explorations in this area of the temporal evolution of low-level human attention.

Overall, while the paper is interesting and tackles an exceedingly challenging problem, I think there are a number of conceptual issues that it needs to overcome. While some specific issues are given in the questions below, the primary issue is that while the submission encodes the positive/negative importance of the various bases extracted, it is well established within psychophysics that the relative importance of elements to saliency is contextual (e.g. see Nothdurft (1993) ""Saliency effects across dimensions in visual search""; Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions"" for some low level examples), and so these relative attributes are likely to change from image to image. Within the current submission, these attributes change from model to model and dataset to dataset; what conclusions are to be drawn from this? Is the technique shedding light on dataset composition, model bias, or some deeper aspect of relative aspects of saliency? Much of the analysis is presented without clear connection to either human behaviour (with the exception of Section 4.5, which cleverly makes use of the technique to explore the representations learned from data from different human subject populations or conditions) or model performance in a traditional sense, making it difficult to put into context or derive deeper insight.","- Although drawing on the architectures of SALICON, DINet, and TranSalNet, the model configurations in this paper are distinct from the instantiations of the original publications in order to accommodate the need for trainable bases. How does that change the ultimate behaviour of the models with respect to standard measures of saliency performance? Alternatively, even a quantified value for the change in saliency maps when compared within model (e.g. the paper's version of SALICON correlated against the standard instantiation of SALICON) would help put the paper's results in context with the existing literature.

- What is the value of N (the number of bases)? How stable are the results with respect to N (i.e. does the mapped semantic content change substantially with even small changes in N)?

- Each basis is mapped onto a top-5 semantic mix. Why not onto a single concept? How was 5 selected? Similar to the previous question, how does this choice affect the subsequent analysis?

- How is ""action"" defined in a static image? The primary example given, ""having meeting"", seems like a social activity, which was a separate category. I get that it is challenging to relate the messy details of semantic categorization in a short paper, but given that this is central to the topic of the paper, I think it needs a clearer explanation.

- How do the insights provided in this paper relate to model performance? When the models show a markedly different breakdown of salient factors (e.g. Figure 4, which shows SALICON emphasizing vehicles much more strongly than TranSalNet, while TranSalNet emphasizes clothing more than any other model), does this correlate with predictive accuracy?

- Related to the previous question, could you use the IoU Measurement process used to assign labels to the bases to provide an approximate breakdown of the factors leading to human fixations directly? This might provide another point of comparison to better put the results of this paper in context.

- Given the range of behaviours across the models shown in Figure 4, why does Figure 5 average the semantic weights across models (also, this should be noted in the caption; when I first read the paper I was quite confused which model was being shown in Figure 5)? What is the justification for this? Do the models tend to converge after fine-tuning?

- Figure 6 (b.) and (c.): are these results for subjects without autism? If so, this should be more clearly noted.

- What was meant by line 304: ""Whether models... open question.""? What would lead to a model behaving ""even better than humans"", given that humans are the system trying to be modelled?",The limitations discussed seem clear.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688619991190,,,NeurIPS.cc/2023/Conference,2rq4LwwjfE,"['NeurIPS.cc/2023/Conference/Submission503/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410746654,[],3,,,['everyone'],2rq4LwwjfE,['NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD'],1688619991190,1702410746654,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission503/Reviewer_ZMfD']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Bruce et al., ""A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond"", CVPR 2016
2. Kümmerer et al., ""Understanding Low- and High-Level Contributions to Fixation Prediction"", ICCV 2017
3. Kotseruba et al., ""Do Saliency Models Detect Odd-One-Out Targets? New Datasets and Evaluations"", BMVC 2019
4. Tatler et al., ""Visual correlates of fixation selection: effects of scale and time"", Vision Research, 2005
5. Nothdurft (1993) ""Saliency effects across dimensions in visual search""
6. Nothdurft (2000) ""Salience from feature contrast: additivity across dimensions""

These references are mentioned as relevant background or prior work that the authors should consider discussing or citing in their paper to provide a more comprehensive understanding of the topic and to put their results into context.",1,"2016, 2017, 2019, 2005, 1993, 2000",Salience from feature contrast: additivity across dimensions
8HmoU2VC0C,"This paper introduces PromptCoT, an enhancer that automatically refines text prompts for diffusion-based generative models, improving their capability to produce high-quality visual content. The system is based on the idea that prompts that resemble high-quality image descriptions from the training set lead to better generation performance. Pre-trained Large Language Models (LLM) are fine-tuned using a dataset of such high-quality descriptions, allowing them to generate improved prompts. To mitigate the tendency of LLMs to generate irrelevant information, contamination, transfer, and even a Chain-of-Thought (CoT) mechanism are used to improve alignment between original and refined prompts. Additionally, to maintain computational efficiency, the system employs adapters for dataset-specific adaptations, leveraging a shared pre-trained LLM. When tested on popular latent diffusion models for image and video generation, PromptCoT showed significant performance improvements.
","1) This paper shows an insight from the observation that prompts resembling high-quality image descriptions from the training set lead to better generation performance.  It makes sense from my perspective.  Current text-guided image generation is still with limited generalization ability.  The points near the training support samples are transferred better. 
2) The idea to utilize LLM to adapt the original prompt to the one that is more aligned with training samples makes sense. It leverages the ability of LLMs to align the distributions. 
3) Three training methods are proposed to implement the ideas including the continuation, revision, and CoT.  
4) This paper builds the corresponding datasets to help the fine-tuning, which can benefit the following research.
5) The method using the GPT-3 to build datasets is smart.","1) The finetuning of LLaMa is time-consuming.  Could it be replaced by prompt learning or LORA? 
2) Please add a discussion (e.g. in related work) with ''Visual Chain-of-Thought Diffusion Models'', though these two methods are clearly different. 
3) If the datasets with neural captions can be built.  How about adding these sentences to the training set of the diffusion model?
",Please refer to the weaknesses part.,It can be found in the F part of the appendix.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,4 excellent,1686575765455,,,NeurIPS.cc/2023/Conference,2pVogxJyDA,"['NeurIPS.cc/2023/Conference/Submission4616/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410962150,[],1,,,['everyone'],2pVogxJyDA,['NeurIPS.cc/2023/Conference/Submission4616/Reviewer_3t2z'],1686575765455,1702410962150,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4616/Reviewer_3t2z']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, namely ""Visual Chain-of-Thought Diffusion Models"", in the discussion or related work section. This is mentioned in the ""weaknesses"" part of the review, where the reviewer asks the authors to add a discussion about this related work, even though the two methods are clearly different.",1,,Visual Chain-of-Thought Diffusion Models
u2kSTBa8zC,"The paper presents a novel approach named Parameter-Efficient and Quantization-aware Adaptation (PEQA) to address the challenges of efficiently fine-tuning and deploying large language models (LLMs). The paper demonstrates PEQA's effectiveness and scalability through extensive experiments, comparing it with competitive baselines across a range of tasks from natural language understanding to generation benchmarks.","* The paper centers on the topic of efficient model compression and fine-tuning, framing its approach through a unified method referred to as PEQA.

* The authors' proposed method offers benefits, most noticeably in substantially reducing the memory usage associated with optimizer state saving and the final model size.

* The authors have undertaken a comprehensive evaluation setup which encompasses several benchmark tasks and a variety of model sizes.","* The overall presentation of the paper could be improved for clarity. Specifically, the meaning of statements such as ""Notice that s0 and z0 have nothing to do with a choice of downstream task"" on Line 148, and the term ""integer quantization indices"" on Line 149, are not clear.

* The calculation of memory usage during fine-tuning is not sufficiently explained. While it's understood that the proposed method lowers optimizer state storage by only updating scaling factors, the paper does not address memory usage during gradient calculation, a crucial aspect that often dictates peak memory usage during optimization.

* It's unclear why the choice of only updating scaling factors is advantageous. The paper only compares the proposed method with LoRA, limiting the assessment of its effectiveness. For instance, the BitFit method (Ben Zaken et al., 2022) demonstrates that fine-tuning the LLM can be achieved by only updating biases. Moreover, the paper does not discuss the potential outcomes of optimizing zero-points only or optimizing both zero-points and the scaling factor.","* The methodology section of the paper leaves some key details unaddressed. Specifically, it's unclear how the gradient of the scaling factors is computed in the proposed model. The paper PACT by Choi, Jungwook, et al. [1] presents two different ways to compute the gradient for the scaling factor/clamp_range for the quantization function. It would be beneficial for the authors to clarify which method was adopted in their work.

[1] Choi, Jungwook, et al. \Pact: Parameterized clipping activation for quantized neural networks.\ arXiv preprint arXiv:1805.06085 (2018).",see above.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,2 fair,1688771814664,,,NeurIPS.cc/2023/Conference,2jUKhUrBxP,"['NeurIPS.cc/2023/Conference/Submission4353/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410949482,[],3,,,['everyone'],2jUKhUrBxP,['NeurIPS.cc/2023/Conference/Submission4353/Reviewer_rGqm'],1688771814664,1702410949482,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4353/Reviewer_rGqm']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely the paper ""Pact: Parameterized clipping activation for quantized neural networks"" by Choi, Jungwook, et al. (2018), which is referenced as [1] in the review. The reviewer mentions this paper as a relevant work that presents two different ways to compute the gradient for the scaling factor, and suggests that the authors clarify which method was adopted in their work. Additionally, the reviewer also mentions another paper by Ben Zaken et al. (2022), which presents the BitFit method, but this is not explicitly suggested as a citation, rather as a comparison to assess the effectiveness of the proposed method.",1,"2018, 2022",Pact: Parameterized clipping activation for quantized neural networks
gyeh2VGXSv,"This work provides a neural framework that combines network training, symbol grounding, and logical constraint synthesis. 
It utilizes the cardinality constraints to express the logical constraint learning and a DC penalty for constraint relaxation.
The evaluation demonstrates that this method outperforms state-of-the-art models including both SATNET and L1R32H4 by a large margin. 
","Originality: 4/5
Pros: This methodology introduces two novel important components, DC relaxation loss, and cardinality constraints, into the numerical learning framework. 
Cons: One interesting component in this paper, mapping the learned numerical rules into the symbolic space is not quite clearly explained. By studying the related work section, I think the ""Softened Symbol Grounding for Neuro-symbolic Systems""[1] seems quite related to the rule extraction part. I feel this work is probably not emphasized enough in the paper's related work section. 

Quality: 4/5 
Pro: The theory and properties come with all the proofs explained in the appendix, which is quite convincing. This work also reaches better than the SOTA performance for the two experiments.
Cons: The SATNET* uses Distilled LeNet as its underlying perceptual model, while you have used a recurrent transformer model. This seems an unfair comparison. 


Clarity: 3/5
Pros: The math component is nicely defined and with well-explained definitions. 
Cons:   It is hard to understand the process of extracting a constraint from its numerical form ($\mathbf{w}$ and $\mathbf{b}$). 

Significance: 4/5 
Pros: This work improves from the existing work by learning constraints along with ground truth perception, which is an important task in neural symbolic learning.
Cons: The learned constraints are in boolean form and thus not high level enough to generalize across different task variants. 

[1] Li, Zenan, et al. ""Softened Symbol Grounding for Neuro-symbolic Systems."" The Eleventh International Conference on Learning Representations. 2022.","See Strength. 

Minor comments: 
1. Line 91: There are two papers coming from different groups, whose first author's names are both Li. It seems that the two works come from the same person in the article. 
2. I would recommend putting the limitation section into the main paper. ","1. What is the limitation of using cardinality constraint? Is there a fundamental limitation other than the tricky usage where introducing auxiliary variables is required? 
2. How are the 324 cardinality constraints calculated? 
3. Suppose the number of the constraint is unknown for a new task, what should a user do? How will that impact the performance? 
4. How to interpret a boolean-based constraint to an average programmer? 

I am happy to raise my score if these questions are addressed. ",Suggestions included in the Question section. ,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1688514805446,,,NeurIPS.cc/2023/Conference,2ioRi2uwLR,"['NeurIPS.cc/2023/Conference/Submission13013/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411409722,[],2,,,['everyone'],2ioRi2uwLR,['NeurIPS.cc/2023/Conference/Submission13013/Reviewer_MGh8'],1688514805446,1702411409722,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13013/Reviewer_MGh8']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, namely ""Softened Symbol Grounding for Neuro-symbolic Systems"" by Li et al. (2022), as mentioned in the ""Cons"" section under ""Strengths"": ""By studying the related work section, I think the 'Softened Symbol Grounding for Neuro-symbolic Systems'[1] seems quite related to the rule extraction part. I feel this work is probably not emphasized enough in the paper's related work section.""",1,2022,Softened Symbol Grounding for Neuro-symbolic Systems
uTVZbXDyoH,This paper proposes a propensity estimation model for causality-based recommendation without accessing the ground-truth propensity score or exposure data. Prior knowledge about item popularity is utilized to estimate the propensity score. A theoretical analysis is provided to understand the proposed model.,"- This paper investigates an interesting problem in causality-based recommendation, propensity score estimation, and proposes a model to get rid of the requirements for ground-truth exposure data.
- A theoretical analysis is provided to understand the critical factors of the proposed model.
- Ablation study is conducted.","- I have serious concerns about the evaluation framework in this paper. All the recommendation experiments are based on one single DLCE model, and the adopted baselines for comparison are rather weak. I suggest the authors to include more advanced baselines, and more importantly, to compare with a wider range of backbone models. Just to list a few widely acknowledged causal recommendation approaches [1-3] which all do not require exposure data.
- The technical contributions of the paper is limited. The proposed pairwise relationship between item popularity and propensity score is similar to the design in [1,2], which also leverage popularity to serve as a *soft* proxy for propensity score.
- The adopted three datasets are very small, considering modern recommendation platforms.
- In Table 3, the proposed PropCare model does not achieve the best performance with respect to Tau and F1 score, outperformed by both POP and CJBPR.
- The case study is hard to follow and not convincing to me.  

[1] Bonner, Stephen, and Flavian Vasile. ""Causal embeddings for recommendation."" Proceedings of the 12th ACM conference on recommender systems. 2018.
[2] Zheng, Yu, et al. ""Disentangling user interest and conformity for recommendation with causal embedding."" Proceedings of the Web Conference 2021. 2021.
[3] Zhang, Yang, et al. ""Causal intervention for leveraging popularity bias in recommendation."" Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021.","1. The authors only experiment with one backbone model, namely DLCE. I suggest the authors to compare with more state-of-the-art causal recommendation approaches.
2. What are the main contributions of the proposed model? What are the differences between the proposed pairwise relationship and existing works such as [1] and [2].
3. I suggest the authors to conduct experiments on large-scale datasets.
4. Please provide more explanations on the case study.",,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,2 fair,1688295313608,,,NeurIPS.cc/2023/Conference,2hhIDEHhkk,"['NeurIPS.cc/2023/Conference/Submission10776/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411301222,[],2,,,['everyone'],2hhIDEHhkk,['NeurIPS.cc/2023/Conference/Submission10776/Reviewer_Pdiw'],1688295313608,1702411301222,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10776/Reviewer_Pdiw']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, namely:

[1] Bonner, Stephen, and Flavian Vasile. ""Causal embeddings for recommendation."" Proceedings of the 12th ACM conference on recommender systems. 2018.
[2] Zheng, Yu, et al. ""Disentangling user interest and conformity for recommendation with causal embedding."" Proceedings of the Web Conference 2021. 2021.
[3] Zhang, Yang, et al. ""Causal intervention for leveraging popularity bias in recommendation."" Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021.

The reviewer mentions these papers as examples of widely acknowledged causal recommendation approaches that do not require exposure data and suggests that the authors compare their work with these approaches.",1,"2018, 2021, 2021, 2021",Causal embeddings for recommendation 
uTVZbXDyoH,This paper proposes a propensity estimation model for causality-based recommendation without accessing the ground-truth propensity score or exposure data. Prior knowledge about item popularity is utilized to estimate the propensity score. A theoretical analysis is provided to understand the proposed model.,"- This paper investigates an interesting problem in causality-based recommendation, propensity score estimation, and proposes a model to get rid of the requirements for ground-truth exposure data.
- A theoretical analysis is provided to understand the critical factors of the proposed model.
- Ablation study is conducted.","- I have serious concerns about the evaluation framework in this paper. All the recommendation experiments are based on one single DLCE model, and the adopted baselines for comparison are rather weak. I suggest the authors to include more advanced baselines, and more importantly, to compare with a wider range of backbone models. Just to list a few widely acknowledged causal recommendation approaches [1-3] which all do not require exposure data.
- The technical contributions of the paper is limited. The proposed pairwise relationship between item popularity and propensity score is similar to the design in [1,2], which also leverage popularity to serve as a *soft* proxy for propensity score.
- The adopted three datasets are very small, considering modern recommendation platforms.
- In Table 3, the proposed PropCare model does not achieve the best performance with respect to Tau and F1 score, outperformed by both POP and CJBPR.
- The case study is hard to follow and not convincing to me.  

[1] Bonner, Stephen, and Flavian Vasile. ""Causal embeddings for recommendation."" Proceedings of the 12th ACM conference on recommender systems. 2018.
[2] Zheng, Yu, et al. ""Disentangling user interest and conformity for recommendation with causal embedding."" Proceedings of the Web Conference 2021. 2021.
[3] Zhang, Yang, et al. ""Causal intervention for leveraging popularity bias in recommendation."" Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021.","1. The authors only experiment with one backbone model, namely DLCE. I suggest the authors to compare with more state-of-the-art causal recommendation approaches.
2. What are the main contributions of the proposed model? What are the differences between the proposed pairwise relationship and existing works such as [1] and [2].
3. I suggest the authors to conduct experiments on large-scale datasets.
4. Please provide more explanations on the case study.",,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,2 fair,1688295313608,,,NeurIPS.cc/2023/Conference,2hhIDEHhkk,"['NeurIPS.cc/2023/Conference/Submission10776/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411301222,[],2,,,['everyone'],2hhIDEHhkk,['NeurIPS.cc/2023/Conference/Submission10776/Reviewer_Pdiw'],1688295313608,1702411301222,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10776/Reviewer_Pdiw']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, namely:

[1] Bonner, Stephen, and Flavian Vasile. ""Causal embeddings for recommendation."" Proceedings of the 12th ACM conference on recommender systems. 2018.
[2] Zheng, Yu, et al. ""Disentangling user interest and conformity for recommendation with causal embedding."" Proceedings of the Web Conference 2021. 2021.
[3] Zhang, Yang, et al. ""Causal intervention for leveraging popularity bias in recommendation."" Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021.

The reviewer mentions these papers as examples of widely acknowledged causal recommendation approaches that do not require exposure data and suggests that the authors compare their work with these approaches.",1,"2018, 2021, 2021, 2021",Disentangling user interest and conformity for recommendation with causal embedding 
uTVZbXDyoH,This paper proposes a propensity estimation model for causality-based recommendation without accessing the ground-truth propensity score or exposure data. Prior knowledge about item popularity is utilized to estimate the propensity score. A theoretical analysis is provided to understand the proposed model.,"- This paper investigates an interesting problem in causality-based recommendation, propensity score estimation, and proposes a model to get rid of the requirements for ground-truth exposure data.
- A theoretical analysis is provided to understand the critical factors of the proposed model.
- Ablation study is conducted.","- I have serious concerns about the evaluation framework in this paper. All the recommendation experiments are based on one single DLCE model, and the adopted baselines for comparison are rather weak. I suggest the authors to include more advanced baselines, and more importantly, to compare with a wider range of backbone models. Just to list a few widely acknowledged causal recommendation approaches [1-3] which all do not require exposure data.
- The technical contributions of the paper is limited. The proposed pairwise relationship between item popularity and propensity score is similar to the design in [1,2], which also leverage popularity to serve as a *soft* proxy for propensity score.
- The adopted three datasets are very small, considering modern recommendation platforms.
- In Table 3, the proposed PropCare model does not achieve the best performance with respect to Tau and F1 score, outperformed by both POP and CJBPR.
- The case study is hard to follow and not convincing to me.  

[1] Bonner, Stephen, and Flavian Vasile. ""Causal embeddings for recommendation."" Proceedings of the 12th ACM conference on recommender systems. 2018.
[2] Zheng, Yu, et al. ""Disentangling user interest and conformity for recommendation with causal embedding."" Proceedings of the Web Conference 2021. 2021.
[3] Zhang, Yang, et al. ""Causal intervention for leveraging popularity bias in recommendation."" Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021.","1. The authors only experiment with one backbone model, namely DLCE. I suggest the authors to compare with more state-of-the-art causal recommendation approaches.
2. What are the main contributions of the proposed model? What are the differences between the proposed pairwise relationship and existing works such as [1] and [2].
3. I suggest the authors to conduct experiments on large-scale datasets.
4. Please provide more explanations on the case study.",,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,2 fair,1688295313608,,,NeurIPS.cc/2023/Conference,2hhIDEHhkk,"['NeurIPS.cc/2023/Conference/Submission10776/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411301222,[],2,,,['everyone'],2hhIDEHhkk,['NeurIPS.cc/2023/Conference/Submission10776/Reviewer_Pdiw'],1688295313608,1702411301222,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10776/Reviewer_Pdiw']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, namely:

[1] Bonner, Stephen, and Flavian Vasile. ""Causal embeddings for recommendation."" Proceedings of the 12th ACM conference on recommender systems. 2018.
[2] Zheng, Yu, et al. ""Disentangling user interest and conformity for recommendation with causal embedding."" Proceedings of the Web Conference 2021. 2021.
[3] Zhang, Yang, et al. ""Causal intervention for leveraging popularity bias in recommendation."" Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021.

The reviewer mentions these papers as examples of widely acknowledged causal recommendation approaches that do not require exposure data and suggests that the authors compare their work with these approaches.",1,"2018, 2021, 2021, 2021",Causal intervention for leveraging popularity bias in recommendation
CpszPRJ7E2,"The starting point for the paper is recent work in the area of “differentiable economics”, in which high-revenue strategyproof auction mechanisms are found by optimizing parameters using machine-learning-inspired gradient descent techniques.

The authors consider the problem of selling multiple goods to a single buyer. In such setting, strategyproof mechanisms can always be identified with a “menu” of (allocation, price) pairs — the bidder chooses their best choice from the menu, and carefully optimizing the menu can increase revenue. The neural architecture that represents these menu items is called “RochetNet”. They also consider multi-buyer versions of this problem, in particular searching through the space of affine maximizer auctions (AMA), which are structurally similar (there is a “menu” of possible outcomes, and a “boost” which roughly plays the role of the price, although actual per-bidder payments are calculated according to a VCG-style rule).

In these classes of auctions, the performance goal (revenue) is a very non-convex function of the auction parameters. Nevertheless, first-order optimization seems to work well in finding good or even known-optimal mechanisms. Also, empirically, allowing the auctions to learn over thousands of menu items helped performance, even though at the end of training, only a handful of these menu items were actually used (and in some cases known-optimal mechanisms might only have 3-4 menu items). The authors of this paper aim to explain these phenomena.

They build on existing work in deep learning for more standard tasks, where things work similarly: even though loss landscapes are not convex, it is possible to optimize over them using first-order methods, and overparameterizing the neural networks seems to help with this. One recent direction of theoretical work aims to explain these phenomena using the concept of “mode connectivity” — there are various results showing that if it is possible to remove lots of neurons from a trained network and get almost the same performance, then any two such solutions must be connected by a continuous path where at any point along the path, the loss function is within some constant of the two solutions. Presumably, loss landscapes with such a property should be easier to optimize over.

The authors establish similar properties for the two types of auction architectures. In particular, between any two menus where there is a small subset of options that would be chosen by a bidder with probability 1-\epsilon, embedded in a much larger unused or redundant menu, they show that (epsilon) mode connectivity holds. Additionally, any pair of menus with sufficiently large (as a function of epsilon and the number of items) menus is also epsilon-mode connected. Analogous results hold for AMAs.This provides a nice theoretical explanation of why these auctions are unexpectedly easy to optimize successfully, and why using large menus may help optimization.","Two separate papers in this area observed an interesting and useful empirical phenomenon but gave no good explanation for it. This paper gives a very solid explanation, and is extremely interesting for that reason. It also will hopefully motivate better designs and learning techniques for strategyproof auction architectures.",Currently I don’t see any significant weaknesses.,"I recommend further discussion of Shen et al. “Automated mechanism design via neural networks”, which presents an architecture that for certain cases is equivalent to RochetNet. It’s cited at one point but the actual neural architecture merits discussion (it also encodes menu items).

Is there an explicit explanation of exactly why mode connectivity helps optimization? Maybe it’s out there in the literature already. If so I think it’s worth devoting a few sentences to this.","Limitations are adequately addressed — in particular, that these techniques probably won’t extend to even more complicated/flexible auction architectures — they rely on the relatively simple “menu based” or “list of allocations + boost” structure of the particular architectures in question.",['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,4 excellent,4 excellent,1688741742588,,,NeurIPS.cc/2023/Conference,2gn9WFlqJ4,"['NeurIPS.cc/2023/Conference/Submission6806/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411079720,[],3,,,['everyone'],2gn9WFlqJ4,['NeurIPS.cc/2023/Conference/Submission6806/Reviewer_hKdM'],1688741742588,1702411079720,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission6806/Reviewer_hKdM']","Yes, the peer review explicitly suggests the authors of the paper to discuss or cite the work of Shen et al., ""Automated mechanism design via neural networks"", as it presents an architecture that is equivalent to RochetNet for certain cases. Additionally, the reviewer asks for a few sentences to be devoted to explaining why mode connectivity helps optimization, possibly referencing existing literature on the topic.",1,,Automated mechanism design via neural networks
foHF43V9pj,"This paper focuses on justifying the empirical success of this differentiable economics, particularly in the context of menu-based methods like RochetNet and Differentiable AMA auctions.

The authors introduce the concept of the $\epsilon$-mode connectivity property, which establishes that two locally optimal menus are connected by a simple path where the revenue loss along the path is at most $\epsilon$. The paper demonstrates that this property holds true under two conditions:
- If the valuations are normalized, and the menus are $\epsilon$-reducible, meaning that there exists a small subset of menus that are active for a buyer with a probability of $1-\epsilon$.
- If the number of menu options is sufficiently large.
","**Significance:** Although menu-based approaches for differentiable economics show promising empirical performance, there is a lack of understanding regarding their theoretical properties. This paper takes the first stride towards exploring these theoretical aspects. 

**Originality:**  While mode-connectivity has primarily been studied in the context of prediction problems involving convex losses with linear transformations in the final layer, RochetNet and Differentiable AMA minimize negated revenue loss, which involves more complex calculations. This fundamental difference sets them apart in their analysis.

**Clarity and Quality:** The paper is well written and easy to follow. "," - This paper would benefit a lot from a discussion on how exactly mode connectivity can be used to justify empirical performance. While I agree that this is a cool theoretical property, it is very unclear to me how this exactly would help
- There is also no discussion/comments what practitioners should make of this property at all - (are there any insights on how to initialize these networks better to break permutation symmetry of menus etc or anything that would help improve training)
- The optimal allocations are not necessarily finite (this assumption isn't state clearly wherever used). See Example 3 in [*]

[*] Daskalakis, C., Deckelbaum, A., and Tzamos, C. (2017). Strong duality for a multiple-good monopolist. Econometrica, 85:735–767.",Please see Weaknesses.,N/A ,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,4 excellent,1690289958699,,,NeurIPS.cc/2023/Conference,2gn9WFlqJ4,"['NeurIPS.cc/2023/Conference/Submission6806/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411079550,[],5,,,['everyone'],2gn9WFlqJ4,['NeurIPS.cc/2023/Conference/Submission6806/Reviewer_Cnd8'],1690289958699,1702411079550,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission6806/Reviewer_Cnd8']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, which is:

Daskalakis, C., Deckelbaum, A., and Tzamos, C. (2017). Strong duality for a multiple-good monopolist. Econometrica, 85:735–767.

This is mentioned in the ""weaknesses"" section, where the reviewer references ""Example 3 in [*]"" and provides the citation.",1,2017,Strong duality for a multiple-good monopolist
LsOJkUZWbH,"The authors apply a recently proposed method for similarity-based prediction to the problem of invariant learning. This paper builds off of the Nadaraya-Watson architecture where predictions for a test input are derived via nearest proximity according to a learned kernel. The proposed method uses a NW head for each training environment, plus some additional regularization to encourage invariance over an assumed causal graph.
","Regularization-based approaches to invariant learning are known to entail optimization difficulties [https://proceedings.mlr.press/v162/zhang22u.html]. The non-parametric prediction approach taken by the authors provides a unique and interesting alternative.

The proposed method shows promise on relevant datasets. I was especially interested in the probe variant of NW-training, which indicates a more invariant internal representation than IRM.
","While the proposed approach is novel and interesting, after reading the paper I was still left with questions as to how the method is implemented, and why it works (see questions below). Also, I think the paper could benefit from some simple theoretical analysis that demonstrates (even in a simplified setting) when we expect NW-based training will discover invariant features. I also feel that the proposed approach, which uses causal graphs to motivate independances on subsets of the training data, is very related to recently proposed MMD-based approaches [https://proceedings.mlr.press/v151/makar22a, https://arxiv.org/abs/2209.09423]. A discussion on how the proposed method (and its use of non-parameteric prediction) differs from these papers would be useful.

The fact that so many different variants of the NW method are tried makes me a bit wary of claims like ""NW^B does X percent better than ERM"" [line 256]. Not all the NW^B flavors outperform ERM.
","* What is the role of the support set in OOD generalization? Are training samples always used to make predictions at test time? Empirical evidence suggests OOD generalization is possible, but since the method relies on domain-labeled training examples to make predictions, is there some implicit assumption about how domains are related? For example in the IRM paper they discuss the relationship between domains geometrically in terms of “linear general position”.
* How is the kernel bandwidth (called ""temperature"" in the NW head paper) chosen?  This seems like a critical hyperparameter but I didn't see any discussion of it.
* Why is it more effective to regularize a non-parametric predictor (Eqn 7) than to directly regularize a standard classification loss?
* With most invariant learning methods there is a tradeoff between in-distribution and out-of-distribution generalization. Do you see that pattern with your method? Table 2 only reports OOD generalization if I understand correctly

Misc comments:
* The proposed method “leverages the NW head as a conditional estimator for Y conditioned on Z” [143–144]. The workshop paper “Towards Environment-Invariant Transfer..."" by Eyre et al (https://openreview.net/forum?id=c4l4HoM2AFf) may be of interest because they also use kernels to estimate similar statistics of the learned representations.
","Limitations are addressed [Sec 6].
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688659061909,,,NeurIPS.cc/2023/Conference,2ePf1sBgLU,"['NeurIPS.cc/2023/Conference/Submission2570/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410840468,[],3,,,['everyone'],2ePf1sBgLU,['NeurIPS.cc/2023/Conference/Submission2570/Reviewer_48ug'],1688659061909,1702410840468,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2570/Reviewer_48ug']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers and provides links to them, implying that the authors should consider citing these papers in their work. The mentioned papers are:

1. https://proceedings.mlr.press/v162/zhang22u.html
2. https://proceedings.mlr.press/v151/makar22a
3. https://arxiv.org/abs/2209.09423
4. The IRM paper (no specific link provided, but it is likely referring to the paper ""Invariance, Causality and Robustness"" by Arjovsky et al.)
5. The workshop paper ""Towards Environment-Invariant Transfer..."" by Eyre et al (https://openreview.net/forum?id=c4l4HoM2AFf)

The reviewer suggests that discussing the relationship between the proposed method and these papers would be useful, particularly in terms of how the non-parametric prediction approach differs from the MMD-based approaches.",1,2022,"Invariance, Causality and Robustness "
LsOJkUZWbH,"The authors apply a recently proposed method for similarity-based prediction to the problem of invariant learning. This paper builds off of the Nadaraya-Watson architecture where predictions for a test input are derived via nearest proximity according to a learned kernel. The proposed method uses a NW head for each training environment, plus some additional regularization to encourage invariance over an assumed causal graph.
","Regularization-based approaches to invariant learning are known to entail optimization difficulties [https://proceedings.mlr.press/v162/zhang22u.html]. The non-parametric prediction approach taken by the authors provides a unique and interesting alternative.

The proposed method shows promise on relevant datasets. I was especially interested in the probe variant of NW-training, which indicates a more invariant internal representation than IRM.
","While the proposed approach is novel and interesting, after reading the paper I was still left with questions as to how the method is implemented, and why it works (see questions below). Also, I think the paper could benefit from some simple theoretical analysis that demonstrates (even in a simplified setting) when we expect NW-based training will discover invariant features. I also feel that the proposed approach, which uses causal graphs to motivate independances on subsets of the training data, is very related to recently proposed MMD-based approaches [https://proceedings.mlr.press/v151/makar22a, https://arxiv.org/abs/2209.09423]. A discussion on how the proposed method (and its use of non-parameteric prediction) differs from these papers would be useful.

The fact that so many different variants of the NW method are tried makes me a bit wary of claims like ""NW^B does X percent better than ERM"" [line 256]. Not all the NW^B flavors outperform ERM.
","* What is the role of the support set in OOD generalization? Are training samples always used to make predictions at test time? Empirical evidence suggests OOD generalization is possible, but since the method relies on domain-labeled training examples to make predictions, is there some implicit assumption about how domains are related? For example in the IRM paper they discuss the relationship between domains geometrically in terms of “linear general position”.
* How is the kernel bandwidth (called ""temperature"" in the NW head paper) chosen?  This seems like a critical hyperparameter but I didn't see any discussion of it.
* Why is it more effective to regularize a non-parametric predictor (Eqn 7) than to directly regularize a standard classification loss?
* With most invariant learning methods there is a tradeoff between in-distribution and out-of-distribution generalization. Do you see that pattern with your method? Table 2 only reports OOD generalization if I understand correctly

Misc comments:
* The proposed method “leverages the NW head as a conditional estimator for Y conditioned on Z” [143–144]. The workshop paper “Towards Environment-Invariant Transfer..."" by Eyre et al (https://openreview.net/forum?id=c4l4HoM2AFf) may be of interest because they also use kernels to estimate similar statistics of the learned representations.
","Limitations are addressed [Sec 6].
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688659061909,,,NeurIPS.cc/2023/Conference,2ePf1sBgLU,"['NeurIPS.cc/2023/Conference/Submission2570/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410840468,[],3,,,['everyone'],2ePf1sBgLU,['NeurIPS.cc/2023/Conference/Submission2570/Reviewer_48ug'],1688659061909,1702410840468,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2570/Reviewer_48ug']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers and provides links to them, implying that the authors should consider citing these papers in their work. The mentioned papers are:

1. https://proceedings.mlr.press/v162/zhang22u.html
2. https://proceedings.mlr.press/v151/makar22a
3. https://arxiv.org/abs/2209.09423
4. The IRM paper (no specific link provided, but it is likely referring to the paper ""Invariance, Causality and Robustness"" by Arjovsky et al.)
5. The workshop paper ""Towards Environment-Invariant Transfer..."" by Eyre et al (https://openreview.net/forum?id=c4l4HoM2AFf)

The reviewer suggests that discussing the relationship between the proposed method and these papers would be useful, particularly in terms of how the non-parametric prediction approach differs from the MMD-based approaches.",1,2022,Towards Environment-Invariant Transfer
8VQg41qwDZ,"Past works have limitations in terms of scalability, formulation approximation, or performance. This paper offers an efficient coreset selection problem with provable theoretical guarantees. That is, the authors solve a bilevel optimization on a probability distribution over the dataset with loss minimization on the selected dataset that is guided into a low-dimensional manifold via a smoothed top-K loss as a regularizer on the probability distribution. ","The approach is simple, yet well-motivated by the limitations of previous works.
The writing addresses the limitations well and is easy to follow. 
","(1) Further analysis into the top-K loss and its causal effects would aid in understanding the mechanics of probability distribution being regularized into a low-dimensional manifold. 

(2) comparisons with state-of-the-art non-coreset replay methods (e.g., using stability plasticity scores [1], contrastive representation based selection [2]), as it would be meaningful to see the prospect of this direction. 

(3) [minor] main illustration figure 1 could include more technical information (embedding some important eqns for example) or include an additional figure to guide the reader better.



[1] Sun et al, Exploring Example Influence in Continual Learning, NeurIPS 2022
[2] Kim et al, Continual Learning on Noisy Data Streams via Self-Purified Replay, ICCV 2021","if Weaknessses (1) could be further analyzed and shown, as well as comparison with (2) other state of the art rehearsal baselines comparison would be informative.",Not mentioned in the paper.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,2 fair,3 good,1688109917707,,,NeurIPS.cc/2023/Conference,2dtU9ZbgSN,"['NeurIPS.cc/2023/Conference/Submission5787/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411028529,[],1,,,['everyone'],2dtU9ZbgSN,['NeurIPS.cc/2023/Conference/Submission5787/Reviewer_b44u'],1688109917707,1702411028529,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5787/Reviewer_b44u']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. Sun et al, ""Exploring Example Influence in Continual Learning"", NeurIPS 2022
2. Kim et al, ""Continual Learning on Noisy Data Streams via Self-Purified Replay"", ICCV 2021

These citations are mentioned in the weaknesses section (point 2) as state-of-the-art non-coreset replay methods that the authors should compare their work with.",1,"2022, 2021",Exploring Example Influence in Continual Learning 
8VQg41qwDZ,"Past works have limitations in terms of scalability, formulation approximation, or performance. This paper offers an efficient coreset selection problem with provable theoretical guarantees. That is, the authors solve a bilevel optimization on a probability distribution over the dataset with loss minimization on the selected dataset that is guided into a low-dimensional manifold via a smoothed top-K loss as a regularizer on the probability distribution. ","The approach is simple, yet well-motivated by the limitations of previous works.
The writing addresses the limitations well and is easy to follow. 
","(1) Further analysis into the top-K loss and its causal effects would aid in understanding the mechanics of probability distribution being regularized into a low-dimensional manifold. 

(2) comparisons with state-of-the-art non-coreset replay methods (e.g., using stability plasticity scores [1], contrastive representation based selection [2]), as it would be meaningful to see the prospect of this direction. 

(3) [minor] main illustration figure 1 could include more technical information (embedding some important eqns for example) or include an additional figure to guide the reader better.



[1] Sun et al, Exploring Example Influence in Continual Learning, NeurIPS 2022
[2] Kim et al, Continual Learning on Noisy Data Streams via Self-Purified Replay, ICCV 2021","if Weaknessses (1) could be further analyzed and shown, as well as comparison with (2) other state of the art rehearsal baselines comparison would be informative.",Not mentioned in the paper.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,2 fair,3 good,1688109917707,,,NeurIPS.cc/2023/Conference,2dtU9ZbgSN,"['NeurIPS.cc/2023/Conference/Submission5787/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411028529,[],1,,,['everyone'],2dtU9ZbgSN,['NeurIPS.cc/2023/Conference/Submission5787/Reviewer_b44u'],1688109917707,1702411028529,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5787/Reviewer_b44u']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. Sun et al, ""Exploring Example Influence in Continual Learning"", NeurIPS 2022
2. Kim et al, ""Continual Learning on Noisy Data Streams via Self-Purified Replay"", ICCV 2021

These citations are mentioned in the weaknesses section (point 2) as state-of-the-art non-coreset replay methods that the authors should compare their work with.",1,"2022, 2021",Continual Learning on Noisy Data Streams via Self-Purified Replay
5653gIus5K,"This work addresses the coreset selection problem in rehearsal-based continual learning, focusing specifically on the application of bilevel optimization. The authors identify limitations in existing bilevel optimization-based coreset selection methods for continual learning, including high computational costs resulting from greedy search and the loss of bilevel optimization nature due to single-level equivalence. To overcome these drawbacks, the authors propose a new formulation that incorporates the probability simplex and a smoothed top-K regularizer. The latter enforces the K most important elements have larger weights. The authors develop a new stochastic bilevel method for this continuous and smooth loss. The effectiveness of the proposed method is demonstrated through comprehensive experiments, which also include various ablation studies. The authors establish the properties of the new loss function and provide guarantees on convergence.","1. Both continual learning and bilevel optimization are timely topics.The exploration of the benefits of bilevel optimization in continual learning is an under-explored area, making the studied topic in this work both interesting and important.
2. The paper is well written and easy to follow. The inclusion of Figure 1, illustrating the process of training bilevel algorithms in continual learning, along with PyTorch-style pseudocode, greatly aids in understanding the entire process.
3. The introduction of the new loss function and bilevel algorithms is well-motivated. The replacement of the nuclear norm with a smoothed top-K regularizer, as proposed in the paper, is a good idea that leads to faster and improved bilevel optimization algorithms. The empirical evidence provided in Figure 3 serves as a strong justification for the efficacy of the new loss function and algorithms.
4. The experiments conducted in the paper are comprehensive. The evaluation encompasses various datasets, including multiple datasets, split Cifar100, and large-scale datasets like Tiny-ImageNet and Food-101. Additionally, various ablation studies are also included.
5. The paper establishes guarantees on convergence rate and explores the properties of the new loss function. ","The proposed algorithm needs to compute the Hessian-vector products, which may be computation expensive in large-scale cases. This computational expense may pose challenges and limit the applicability of the algorithm. Is it possible to improve the efficiency of the proposed method by utilizing Hessian-free bilevel algorithms based on recent advancements (e.g., Liu et al. [1], Sow et al. [2])？

[1] Liu, Bo, et al. ""Bome! bilevel optimization made easy: A simple first-order approach."" Advances in Neural Information Processing Systems 35 (2022): 17248-17262.

[2] Sow, Daouda, Kaiyi Ji, and Yingbin Liang. ""On the convergence theory for hessian-free bilevel algorithms."" Advances in Neural Information Processing Systems 35 (2022): 4136-4149.

","1. How to select the parameters such as N, Q and $\delta$ in Algorithm 3 for continual learning? It would be beneficial to provide suggestions or guidelines for choosing these parameters and to discuss the sensitivity of the algorithm's performance to such parameters.

2. What are the main challenges of the bilevel optimization analysis in the continual learning setting? ",yes,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1688368643128,,,NeurIPS.cc/2023/Conference,2dtU9ZbgSN,"['NeurIPS.cc/2023/Conference/Submission5787/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411028361,[],3,,,['everyone'],2dtU9ZbgSN,['NeurIPS.cc/2023/Conference/Submission5787/Reviewer_AXy9'],1688368643128,1702411028361,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5787/Reviewer_AXy9']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. Liu, Bo, et al. ""Bome! bilevel optimization made easy: A simple first-order approach."" Advances in Neural Information Processing Systems 35 (2022): 17248-17262.
2. Sow, Daouda, Kaiyi Ji, and Yingbin Liang. ""On the convergence theory for hessian-free bilevel algorithms."" Advances in Neural Information Processing Systems 35 (2022): 4136-4149.

The reviewer mentions these references as potential solutions to improve the efficiency of the proposed method by utilizing Hessian-free bilevel algorithms.",1,"2022, 2022",Bome bilevel optimization made easy A simple first-order approach 
5653gIus5K,"This work addresses the coreset selection problem in rehearsal-based continual learning, focusing specifically on the application of bilevel optimization. The authors identify limitations in existing bilevel optimization-based coreset selection methods for continual learning, including high computational costs resulting from greedy search and the loss of bilevel optimization nature due to single-level equivalence. To overcome these drawbacks, the authors propose a new formulation that incorporates the probability simplex and a smoothed top-K regularizer. The latter enforces the K most important elements have larger weights. The authors develop a new stochastic bilevel method for this continuous and smooth loss. The effectiveness of the proposed method is demonstrated through comprehensive experiments, which also include various ablation studies. The authors establish the properties of the new loss function and provide guarantees on convergence.","1. Both continual learning and bilevel optimization are timely topics.The exploration of the benefits of bilevel optimization in continual learning is an under-explored area, making the studied topic in this work both interesting and important.
2. The paper is well written and easy to follow. The inclusion of Figure 1, illustrating the process of training bilevel algorithms in continual learning, along with PyTorch-style pseudocode, greatly aids in understanding the entire process.
3. The introduction of the new loss function and bilevel algorithms is well-motivated. The replacement of the nuclear norm with a smoothed top-K regularizer, as proposed in the paper, is a good idea that leads to faster and improved bilevel optimization algorithms. The empirical evidence provided in Figure 3 serves as a strong justification for the efficacy of the new loss function and algorithms.
4. The experiments conducted in the paper are comprehensive. The evaluation encompasses various datasets, including multiple datasets, split Cifar100, and large-scale datasets like Tiny-ImageNet and Food-101. Additionally, various ablation studies are also included.
5. The paper establishes guarantees on convergence rate and explores the properties of the new loss function. ","The proposed algorithm needs to compute the Hessian-vector products, which may be computation expensive in large-scale cases. This computational expense may pose challenges and limit the applicability of the algorithm. Is it possible to improve the efficiency of the proposed method by utilizing Hessian-free bilevel algorithms based on recent advancements (e.g., Liu et al. [1], Sow et al. [2])？

[1] Liu, Bo, et al. ""Bome! bilevel optimization made easy: A simple first-order approach."" Advances in Neural Information Processing Systems 35 (2022): 17248-17262.

[2] Sow, Daouda, Kaiyi Ji, and Yingbin Liang. ""On the convergence theory for hessian-free bilevel algorithms."" Advances in Neural Information Processing Systems 35 (2022): 4136-4149.

","1. How to select the parameters such as N, Q and $\delta$ in Algorithm 3 for continual learning? It would be beneficial to provide suggestions or guidelines for choosing these parameters and to discuss the sensitivity of the algorithm's performance to such parameters.

2. What are the main challenges of the bilevel optimization analysis in the continual learning setting? ",yes,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1688368643128,,,NeurIPS.cc/2023/Conference,2dtU9ZbgSN,"['NeurIPS.cc/2023/Conference/Submission5787/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411028361,[],3,,,['everyone'],2dtU9ZbgSN,['NeurIPS.cc/2023/Conference/Submission5787/Reviewer_AXy9'],1688368643128,1702411028361,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5787/Reviewer_AXy9']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. Liu, Bo, et al. ""Bome! bilevel optimization made easy: A simple first-order approach."" Advances in Neural Information Processing Systems 35 (2022): 17248-17262.
2. Sow, Daouda, Kaiyi Ji, and Yingbin Liang. ""On the convergence theory for hessian-free bilevel algorithms."" Advances in Neural Information Processing Systems 35 (2022): 4136-4149.

The reviewer mentions these references as potential solutions to improve the efficiency of the proposed method by utilizing Hessian-free bilevel algorithms.",1,"2022, 2022",On the convergence theory for hessian-free bilevel algorithms
re1jZBcrW9,"In this work, the authors proposed a new algorithm to find approximate second-order stationary points for stochastic optimization problems under the strong contamination model. The general algorithm is applied to the robust matrix sensing problem and the convergence results are proved for the robust matrix sensing problem.",The results in this work are novel and should be interesting to audiences in optimization and machine learning fields.,"It is unclear how the results of this work differ with those in literature; see my comment (10) in the next section. The presentation of the results and the sketch of proofs can be improved. Currently, many important technical details are omitted in the main manuscript. For example, Algorithms A.1-2 and the construction of distributions for the SQ lower bound.","(1) Line 48: ""newdiscussed"" is a typo.

(2) Line 69: it would be better to (briefly) discuss the counterexample in the appendix.

(3) Line 85: it seems that both ""second-order stationary point"" and the abbreviation ""SOSP"" are used throughout the paper. It would be better to be consistent in using the abbreviation.

(4) Line 93: I think B_D_g and B_D_H should be the bound on the norm of the gradient and the Hessian matrix, respectively?

(5) Theorem 1.5: maybe the authors can briefly discuss the reason why the sample complexity is inversely proportional to the corruption rate \epsilon?

(6) Line 148: it may be better to use D to denote the dimension.

(7) Line 157: I wonder if the region is the same as the region B in Assumption 1.4? In addition, is the information about region B provided to the algorithm as an input parameter? It will be helpful if the authors can clarify it in the paper.

(8) It seems that the statements on Lines 163 and 166 are the same.

(9) Line 169: it would be better if the authors can be more specific on the circular dependence. Does it refer to the rotational invariance of the objective function?

(10) Line 176: I think the results in the following paper also concerns the noiseless and Gaussian measurement case. It will be better if the results in the following paper can be discussed and compared.

Li, Xiao, Zhihui Zhu, Anthony Man-Cho So, and Rene Vidal. ""Nonconvex robust low-rank matrix recovery."" SIAM Journal on Optimization 30, no. 1 (2020): 660-686.

(11) In Equation (1), the computational complexity is proportional to \epsilon_g^2. This is a little counter-intuitive, since the upper bound of the running time is not changed is we shrink \epsilon_g and \epsilon_H together (at different rates). It would be better if the authors can include an explanation to this relation.

(12) Line 231: ""exists""

(13) Theorem 3.5: It would be better to mention the condition \epsilon = O(1/k^2/r^2).

(14) Line 320: ""Theorem 3.1 applies""

(15) Line 367: it would be better to explain why the results are more significant or important for the rank-1 case.

(16) Line 378: it will be helpful if the authors can briefly explain how to generate the SQ oracle.",See my comments in the previous section.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,2 fair,3 good,1688524662607,,,NeurIPS.cc/2023/Conference,2ccH4zjKVs,"['NeurIPS.cc/2023/Conference/Submission10140/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411270828,[],3,,,['everyone'],2ccH4zjKVs,['NeurIPS.cc/2023/Conference/Submission10140/Reviewer_emkc'],1688524662607,1702411270828,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10140/Reviewer_emkc']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature. In comment (10), the reviewer mentions a paper by Li, Xiao, Zhihui Zhu, Anthony Man-Cho So, and Rene Vidal, titled ""Nonconvex robust low-rank matrix recovery"", published in the SIAM Journal on Optimization in 2020, and suggests that the authors discuss and compare their results with those in this paper.",1,2020,Nonconvex robust low-rank matrix recovery
uzymDpv380,"The paper presents a new defense against backdoor attacks on pre-trained language models (PLMs). By leveraging the observation that the loss of poisoned samples drops faster in early layers of PLMs compared to clean samples, it dynamically reduces the weight of suspicious samples in fine-tuning. Empirical results show the effectiveness of the proposed defense against 4 attacks. Overall, the work represents yet another defense that exploits the learning dynamics difference of clean and poisoned samples. ","- Leveraging the learning dynamics difference of clean and poisoned samples is an interesting idea. Even though it has been exploited in prior work (e.g., [26]), this work proposes to dynamically adjust the weight of suspicious samples, which seems new. 

- Empirical results show the effectiveness against both word-based and style-based backdoor attacks. 

- The paper is well-structured and easy to follow. ","- The threat model needs better motivation. It assumes a clean PLM, which is fine-tuned using potentially poisoned data. Typically, the PLM is provided by external parties (e.g., downloaded from the Web) while the fine-tuning dataset is managed by the user. It seems a more practical setting that the PLM contains backdoors while the fine-tuning data is clean. 

- The proposed defense bears a lot of similarity to existing defenses that also exploit the learning dynamics difference of clean and poisoned samples (e.g., [26] and Li et al. ""Anti-Backdoor Learning: Training Clean Models on Poisoned Data"", NeurIPS 2021). However, there is no empirical comparison with prior work, which makes it difficult to assess its superiority. 

- While the work targets PLMs, the proposed method seems agnostic to the underlying models. It is suggested to tailor the method to the unique characteristics of NLP models.","- Please clarify the threat model. 

- Compare the defense with other defenses based on the learning dynamics difference of clean and poisoned samples.

- Optimize the defense for NLP models.",The limitations are adequately addressed. ,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688194698570,,,NeurIPS.cc/2023/Conference,2cYxNWNzk3,"['NeurIPS.cc/2023/Conference/Submission2066/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410817796,[],2,,,['everyone'],2cYxNWNzk3,['NeurIPS.cc/2023/Conference/Submission2066/Reviewer_Fa4H'],1688194698570,1702410817796,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2066/Reviewer_Fa4H']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The review mentions two specific references: 

1. [26] (no title or authors provided, but presumably a prior work that exploits the learning dynamics difference of clean and poisoned samples)
2. Li et al. ""Anti-Backdoor Learning: Training Clean Models on Poisoned Data"", NeurIPS 2021 (as an example of existing defenses that also exploit the learning dynamics difference of clean and poisoned samples)

The reviewer suggests that the authors compare their defense with these prior works, implying that they should be cited in the paper.",1,2021,Anti-Backdoor Learning Training Clean Models on Poisoned Data
UWkfNOz0yu,"This paper first makes an observation that in a backdoor poisoning attack against a pretrained language model, the lower layers learn the backdoor feature quickly and easily. Based on this observation, the authors then design a honeypot-based defense that catches the training samples that could be learned with low loss by the lower layers, hoping that these would be poisoned. On the other hand, the loss from the training samples that cannot be learned easily by lower layers is upweighted, which defuses the backdoor. The authors evaluate their defense on multiple NLP classification tasks on multiple architectures and attacks, including adaptive attacks.","+ The proposed defense is well reasoned based on empirical observations about how backdoor attacks are learned.
+ Detailed evaluation, considers stronger adaptive attacks as well and shows success.
+ The idea of probing hidden layers to make observations about the learning dynamics is interesting as most work focuses on looking at loss dynamics during training.","- The main idea the defense relies on is not a new one and there are already successful defenses that exploit this idea. The authors have not considered these defenses as a baseline.
- Some additional experiments regarding the impact of the defense in low-ratio poisoning regimes would be informative to have.

The defense essentially observes that backdoor poison samples are learned easily by the lower layers, and, as a result, a defense that isolates away the samples learned in lower layers can prevent the attack. This is a good observation but a very similar one has been made before toward a defense. For example, Anti-Backdoor Learning (ABL) by Li et al. has also made this observation and proposed a similar defense based on isolating easy-to-learn samples. A difference in the work under review is using the lower layers instead of loss dynamics during training as a measure of sample difficulty. However, it is known that these notions are very correlated (e.g., Deep Learning Through the Lens of Example Difficulty, by Baldock et al.), so this difference might not matter after all.

This brings us to my first bullet point, the authors should've evaluated their work against ABL (or a more recent follow-up if that exists) as the proposed defense and ABL share their starting point. I'm not convinced that the proposed defense can improve upon ABL significantly without offering a different insight.

That being said, the observation that backdoor poison samples are easy to learn is an artifact of the attack and its parameters. There are recent backdoor attacks that break defenses like ABL by crafting more difficult-to-learn poisons. For example, Narcissus: A Practical Clean-Label Backdoor... by Zeng et al. Lowering the poison percentage is a way to craft such poisons but the experiment provided in Section 5.3 is not enough to be convincing. 

Ideally, I would like to see a plot when you apply the DPR-AST attacks in Section 5.3 but you vary the poison percentage, starting from a percentage that achieves very low ASR. Essentially, the x-axis is the poison ratio and the y-axis is the ASR, and there are two curves, one when the honeypot defense is applied and the other for an undefended model. In particular, I would like to see if there is a regime where the undefended model achieves lower ASR than the defended model because the proposed honeypot defense starts boosting difficult-to-learn poison samples (which could be poison samples depending on the poison ratio). This way, we can have a better idea when the honeypot defense is viable and effective and when it is necessary to deploy another type of defense that makes different assumptions.
","- How does the observation in Section 3 change if the poison ratio is varied (let's say between 0.1% to 10%)? Do you think the observation would persist in low poison percentages or the poison samples would not be learned easily in lower layers anymore?

- Why do you think, in Figure 1, the CE Loss for poison samples increases over layers? This seems to conflict with the claim in Line 200 (easier samples, the majority of which are poisoned samples). If poison samples are easier, why do they seemingly become more difficult at the deeper layers?

- What type of model did you use to train the probing classifiers in Section 3? Is it a simple linear model?",-,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688657929173,,,NeurIPS.cc/2023/Conference,2cYxNWNzk3,"['NeurIPS.cc/2023/Conference/Submission2066/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410817713,[],3,,,['everyone'],2cYxNWNzk3,['NeurIPS.cc/2023/Conference/Submission2066/Reviewer_hYyY'],1688657929173,1702410817713,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2066/Reviewer_hYyY']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, such as:

1. ""Anti-Backdoor Learning (ABL)"" by Li et al.
2. ""Deep Learning Through the Lens of Example Difficulty"" by Baldock et al.
3. ""Narcissus: A Practical Clean-Label Backdoor..."" by Zeng et al.

The reviewer mentions that the authors should have evaluated their work against ABL and suggests that the proposed defense may not improve upon ABL significantly without offering a different insight. The reviewer also references other works to support their points and to suggest potential improvements to the paper.",1,,Anti-Backdoor Learning 
UWkfNOz0yu,"This paper first makes an observation that in a backdoor poisoning attack against a pretrained language model, the lower layers learn the backdoor feature quickly and easily. Based on this observation, the authors then design a honeypot-based defense that catches the training samples that could be learned with low loss by the lower layers, hoping that these would be poisoned. On the other hand, the loss from the training samples that cannot be learned easily by lower layers is upweighted, which defuses the backdoor. The authors evaluate their defense on multiple NLP classification tasks on multiple architectures and attacks, including adaptive attacks.","+ The proposed defense is well reasoned based on empirical observations about how backdoor attacks are learned.
+ Detailed evaluation, considers stronger adaptive attacks as well and shows success.
+ The idea of probing hidden layers to make observations about the learning dynamics is interesting as most work focuses on looking at loss dynamics during training.","- The main idea the defense relies on is not a new one and there are already successful defenses that exploit this idea. The authors have not considered these defenses as a baseline.
- Some additional experiments regarding the impact of the defense in low-ratio poisoning regimes would be informative to have.

The defense essentially observes that backdoor poison samples are learned easily by the lower layers, and, as a result, a defense that isolates away the samples learned in lower layers can prevent the attack. This is a good observation but a very similar one has been made before toward a defense. For example, Anti-Backdoor Learning (ABL) by Li et al. has also made this observation and proposed a similar defense based on isolating easy-to-learn samples. A difference in the work under review is using the lower layers instead of loss dynamics during training as a measure of sample difficulty. However, it is known that these notions are very correlated (e.g., Deep Learning Through the Lens of Example Difficulty, by Baldock et al.), so this difference might not matter after all.

This brings us to my first bullet point, the authors should've evaluated their work against ABL (or a more recent follow-up if that exists) as the proposed defense and ABL share their starting point. I'm not convinced that the proposed defense can improve upon ABL significantly without offering a different insight.

That being said, the observation that backdoor poison samples are easy to learn is an artifact of the attack and its parameters. There are recent backdoor attacks that break defenses like ABL by crafting more difficult-to-learn poisons. For example, Narcissus: A Practical Clean-Label Backdoor... by Zeng et al. Lowering the poison percentage is a way to craft such poisons but the experiment provided in Section 5.3 is not enough to be convincing. 

Ideally, I would like to see a plot when you apply the DPR-AST attacks in Section 5.3 but you vary the poison percentage, starting from a percentage that achieves very low ASR. Essentially, the x-axis is the poison ratio and the y-axis is the ASR, and there are two curves, one when the honeypot defense is applied and the other for an undefended model. In particular, I would like to see if there is a regime where the undefended model achieves lower ASR than the defended model because the proposed honeypot defense starts boosting difficult-to-learn poison samples (which could be poison samples depending on the poison ratio). This way, we can have a better idea when the honeypot defense is viable and effective and when it is necessary to deploy another type of defense that makes different assumptions.
","- How does the observation in Section 3 change if the poison ratio is varied (let's say between 0.1% to 10%)? Do you think the observation would persist in low poison percentages or the poison samples would not be learned easily in lower layers anymore?

- Why do you think, in Figure 1, the CE Loss for poison samples increases over layers? This seems to conflict with the claim in Line 200 (easier samples, the majority of which are poisoned samples). If poison samples are easier, why do they seemingly become more difficult at the deeper layers?

- What type of model did you use to train the probing classifiers in Section 3? Is it a simple linear model?",-,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688657929173,,,NeurIPS.cc/2023/Conference,2cYxNWNzk3,"['NeurIPS.cc/2023/Conference/Submission2066/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410817713,[],3,,,['everyone'],2cYxNWNzk3,['NeurIPS.cc/2023/Conference/Submission2066/Reviewer_hYyY'],1688657929173,1702410817713,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2066/Reviewer_hYyY']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, such as:

1. ""Anti-Backdoor Learning (ABL)"" by Li et al.
2. ""Deep Learning Through the Lens of Example Difficulty"" by Baldock et al.
3. ""Narcissus: A Practical Clean-Label Backdoor..."" by Zeng et al.

The reviewer mentions that the authors should have evaluated their work against ABL and suggests that the proposed defense may not improve upon ABL significantly without offering a different insight. The reviewer also references other works to support their points and to suggest potential improvements to the paper.",1,,Deep Learning Through the Lens of Example Difficulty 
UWkfNOz0yu,"This paper first makes an observation that in a backdoor poisoning attack against a pretrained language model, the lower layers learn the backdoor feature quickly and easily. Based on this observation, the authors then design a honeypot-based defense that catches the training samples that could be learned with low loss by the lower layers, hoping that these would be poisoned. On the other hand, the loss from the training samples that cannot be learned easily by lower layers is upweighted, which defuses the backdoor. The authors evaluate their defense on multiple NLP classification tasks on multiple architectures and attacks, including adaptive attacks.","+ The proposed defense is well reasoned based on empirical observations about how backdoor attacks are learned.
+ Detailed evaluation, considers stronger adaptive attacks as well and shows success.
+ The idea of probing hidden layers to make observations about the learning dynamics is interesting as most work focuses on looking at loss dynamics during training.","- The main idea the defense relies on is not a new one and there are already successful defenses that exploit this idea. The authors have not considered these defenses as a baseline.
- Some additional experiments regarding the impact of the defense in low-ratio poisoning regimes would be informative to have.

The defense essentially observes that backdoor poison samples are learned easily by the lower layers, and, as a result, a defense that isolates away the samples learned in lower layers can prevent the attack. This is a good observation but a very similar one has been made before toward a defense. For example, Anti-Backdoor Learning (ABL) by Li et al. has also made this observation and proposed a similar defense based on isolating easy-to-learn samples. A difference in the work under review is using the lower layers instead of loss dynamics during training as a measure of sample difficulty. However, it is known that these notions are very correlated (e.g., Deep Learning Through the Lens of Example Difficulty, by Baldock et al.), so this difference might not matter after all.

This brings us to my first bullet point, the authors should've evaluated their work against ABL (or a more recent follow-up if that exists) as the proposed defense and ABL share their starting point. I'm not convinced that the proposed defense can improve upon ABL significantly without offering a different insight.

That being said, the observation that backdoor poison samples are easy to learn is an artifact of the attack and its parameters. There are recent backdoor attacks that break defenses like ABL by crafting more difficult-to-learn poisons. For example, Narcissus: A Practical Clean-Label Backdoor... by Zeng et al. Lowering the poison percentage is a way to craft such poisons but the experiment provided in Section 5.3 is not enough to be convincing. 

Ideally, I would like to see a plot when you apply the DPR-AST attacks in Section 5.3 but you vary the poison percentage, starting from a percentage that achieves very low ASR. Essentially, the x-axis is the poison ratio and the y-axis is the ASR, and there are two curves, one when the honeypot defense is applied and the other for an undefended model. In particular, I would like to see if there is a regime where the undefended model achieves lower ASR than the defended model because the proposed honeypot defense starts boosting difficult-to-learn poison samples (which could be poison samples depending on the poison ratio). This way, we can have a better idea when the honeypot defense is viable and effective and when it is necessary to deploy another type of defense that makes different assumptions.
","- How does the observation in Section 3 change if the poison ratio is varied (let's say between 0.1% to 10%)? Do you think the observation would persist in low poison percentages or the poison samples would not be learned easily in lower layers anymore?

- Why do you think, in Figure 1, the CE Loss for poison samples increases over layers? This seems to conflict with the claim in Line 200 (easier samples, the majority of which are poisoned samples). If poison samples are easier, why do they seemingly become more difficult at the deeper layers?

- What type of model did you use to train the probing classifiers in Section 3? Is it a simple linear model?",-,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688657929173,,,NeurIPS.cc/2023/Conference,2cYxNWNzk3,"['NeurIPS.cc/2023/Conference/Submission2066/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410817713,[],3,,,['everyone'],2cYxNWNzk3,['NeurIPS.cc/2023/Conference/Submission2066/Reviewer_hYyY'],1688657929173,1702410817713,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2066/Reviewer_hYyY']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, such as:

1. ""Anti-Backdoor Learning (ABL)"" by Li et al.
2. ""Deep Learning Through the Lens of Example Difficulty"" by Baldock et al.
3. ""Narcissus: A Practical Clean-Label Backdoor..."" by Zeng et al.

The reviewer mentions that the authors should have evaluated their work against ABL and suggests that the proposed defense may not improve upon ABL significantly without offering a different insight. The reviewer also references other works to support their points and to suggest potential improvements to the paper.",1,,Narcissus A Practical Clean-Label Backdoor
lL06iM0tTE,"This paper proposes a method to defend against NLP backdoors during training.
The proposed method works by using an honeypot module to absorb backdoor
information, and prevent the backdoor behaviors to be learned by the stem
network. Experiments on SST-2, IMDB, and OLID demonstrate the effectiveness of
the propsoed method.
","* The investigated problem is interesting.

* The motivation of this paper is good.","* The proposed method is based on the observation that
learning the backdoor task is generally easier than learning
the main task. However, this observation may not always hold
true. In the case of label-specific backdoor attacks (also
called as all-to-all attak in BadNets [1]), where samples
with different original labels have different target labels,
the backdoor task becomes even more complex compared to the
main task. To achieve the desired backdoor behavior in such
attacks, the model must first identify the correct label of
the backdoor samples before making backdoor predictions
based on that recognized label. Unfortunately, this paper
lacks a discussion and empirical results concerning
label-specific attacks. As a result, the generalizability of
the proposed method to different types of attacks remains
unclear.

* Comparison to related work CUBE [2] is missing. While this
paper claims that the proposed method surpasses existing
defenses, it fails to include a comparison to CUBE, a
training-time textual backdoor defense method. It is
recommended to incorporate a comparison with CUBE to provide
a more comprehensive evaluation of the proposed method's
performance.


* The proposed approach essentially considers samples with
W(x) values below the threshold value c as identified
poisoning samples. To gain a deeper understanding of the
proposed method's effectiveness, it is recommended to
discuss the measures of detection precision and recall in
detail.

[1] Gu et al., BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. arXiv 2017.

[2] Cui et al., A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. NeurIPS 2022 Datasets & Benchmarks.",See Weaknesses.,The limitations is discussed in the Appendix.,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688787333808,,,NeurIPS.cc/2023/Conference,2cYxNWNzk3,"['NeurIPS.cc/2023/Conference/Submission2066/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410817618,[],4,,,['everyone'],2cYxNWNzk3,['NeurIPS.cc/2023/Conference/Submission2066/Reviewer_5e6e'],1688787333808,1702410817618,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2066/Reviewer_5e6e']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. Gu et al., BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. arXiv 2017.
2. Cui et al., A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. NeurIPS 2022 Datasets & Benchmarks.

The reviewer mentions these references to point out the lack of discussion on label-specific attacks (referencing [1]) and the missing comparison to related work CUBE (referencing [2]).",1,"2020, 2022",BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain 
lL06iM0tTE,"This paper proposes a method to defend against NLP backdoors during training.
The proposed method works by using an honeypot module to absorb backdoor
information, and prevent the backdoor behaviors to be learned by the stem
network. Experiments on SST-2, IMDB, and OLID demonstrate the effectiveness of
the propsoed method.
","* The investigated problem is interesting.

* The motivation of this paper is good.","* The proposed method is based on the observation that
learning the backdoor task is generally easier than learning
the main task. However, this observation may not always hold
true. In the case of label-specific backdoor attacks (also
called as all-to-all attak in BadNets [1]), where samples
with different original labels have different target labels,
the backdoor task becomes even more complex compared to the
main task. To achieve the desired backdoor behavior in such
attacks, the model must first identify the correct label of
the backdoor samples before making backdoor predictions
based on that recognized label. Unfortunately, this paper
lacks a discussion and empirical results concerning
label-specific attacks. As a result, the generalizability of
the proposed method to different types of attacks remains
unclear.

* Comparison to related work CUBE [2] is missing. While this
paper claims that the proposed method surpasses existing
defenses, it fails to include a comparison to CUBE, a
training-time textual backdoor defense method. It is
recommended to incorporate a comparison with CUBE to provide
a more comprehensive evaluation of the proposed method's
performance.


* The proposed approach essentially considers samples with
W(x) values below the threshold value c as identified
poisoning samples. To gain a deeper understanding of the
proposed method's effectiveness, it is recommended to
discuss the measures of detection precision and recall in
detail.

[1] Gu et al., BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. arXiv 2017.

[2] Cui et al., A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. NeurIPS 2022 Datasets & Benchmarks.",See Weaknesses.,The limitations is discussed in the Appendix.,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688787333808,,,NeurIPS.cc/2023/Conference,2cYxNWNzk3,"['NeurIPS.cc/2023/Conference/Submission2066/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410817618,[],4,,,['everyone'],2cYxNWNzk3,['NeurIPS.cc/2023/Conference/Submission2066/Reviewer_5e6e'],1688787333808,1702410817618,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2066/Reviewer_5e6e']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, namely:

1. Gu et al., BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. arXiv 2017.
2. Cui et al., A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. NeurIPS 2022 Datasets & Benchmarks.

The reviewer mentions these references to point out the lack of discussion on label-specific attacks (referencing [1]) and the missing comparison to related work CUBE (referencing [2]).",1,"2020, 2022",A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks
eHiuWp7EY7,"The authors provide compelling evidence that underlines the power of a repetitive prompt-training approach, which leverages CLIP-based pseudo labels. Regardless of the learning model (SSL, TZSL, UL) or the type of prompt (text, visual), this strategy significantly enhances the image classification capabilities of CLIP across multiple settings. Moreover, by using the Top-K pseudo-labeling approach, they ensure a balanced distribution of pseudo-labeled training samples for each class, thereby effectively addressing the inherent bias of CLIP pseudo labels.","- Various learning paradigms, including semi-supervised, transductive zero-shot, and unsupervised learning, can all be viewed as unique instances of a single objective function when pseudolabels are used as a form of guidance.
-  As evidenced by Table 1and Table 2, the experimental results demonstrate a significant boost in performance, indicating that the proposed pseudo-labeling strategy holds considerable merit.","The overall structure or layout of the manuscript needs to be further refined.
- Table 5 (""There is a trade-off between quality and quantity of pseudolabels"") is mentioned in L290, but Table 5 does not exist in the main manuscript. 
- Table 2 was not mentioned at all in the body text. 
- Since Fig 3 and Fig 5 are too small to be seen properly, efforts are needed to improve readability.
- The formulas of L157-L158 should be specified as an equation in the form of (1), but it was not.

Methodologically, the proposed approach is too naive. 
- Setting the trade-off parameters in unified objective function (L157-L158), e.g., gamma, and lambda, is too heuristic, and there is lack of the analysis. 
- Selecting the Top-K most confident samples by class is too naive. It would be nice to select a confident sample by improving class diversity through the application of techniques such as information maximization loss [1]. And it would be good to present comparison results with other attempts for class diversity.

[1] Liang, Jian, Dapeng Hu, and Jiashi Feng. ""Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation."" ICML 2020","- In Table 1 and Table 2, there's a noticeable drop in some experimental results under the UL setting. I'm curious as to the cause of this decrease.
- Within Table 2, the outcomes for the TRZSL learning setting in RESICS45 and DTD, under the ""Multimodal prompts"" category, show a decline. I'm interested in understanding why this is so.
- In the experimental results presented in Table 3, where ""GRIP benefits adaptation even for larger image encoders,"" there seems to be a reduced rate of improvement when using a larger image encoder. I wonder the reason or analysis behind this observation.",I already pointed out in the weakness part.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688691193391,,,NeurIPS.cc/2023/Conference,2b9aY2NgXE,"['NeurIPS.cc/2023/Conference/Submission14336/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411473676,[],2,,,['everyone'],2b9aY2NgXE,['NeurIPS.cc/2023/Conference/Submission14336/Reviewer_sr7c'],1688691193391,1702411473676,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14336/Reviewer_sr7c']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature. The reviewer mentions:

""And it would be good to present comparison results with other attempts for class diversity.""

and also references a specific paper:

""[1] Liang, Jian, Dapeng Hu, and Jiashi Feng. ""Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation."" ICML 2020""

This indicates that the reviewer is suggesting the authors to cite this paper and potentially compare their work with the techniques presented in it, such as information maximization loss.",1,2020,Do we really need to access the source data source hypothesis transfer for unsupervised domain adaptation
KpCsb8Qjpg,"This work considers the setting of steerable networks, in which a particular kind of group-equivariant (alternatively, G-steerable) kernel is translationally convolved with an input vector (or matrix) field. The group-specific design of such a network focuses on the derivation of the group-equivariant kernel. Although prior work laid out a procedure for deriving an analytic, orthogonal basis of functions whose span encompasses all such G-steerable kernels, this paper presents a conceptually simpler method: parametrizing the kernel with an equivariant map. This method is flexible and allows conditioning the kernel on other invariant or equivariant attributes. They test their method on datasets of point cloud classification, synthetic N-body simulations, and molecules (QM9).","Originality: Using an equivariant network to parametrize a group-steerable kernel is a natural and elegant idea. Although implicit kernels have been used in other settings, their application to steerable kernels is novel and well-founded. Conceptually, it is simpler than previous analytical approaches to deriving explicit, orthogonal bases. 

Clarity: The proposal is clearly described, and the paper provides a helpful consolidation of related work overall.

Quality: The proposal is indeed equivariant, and experiments show good results over a sufficiently wide range of settings.

Significance: Group-steerable networks are widely used, and this formulation seems to apply to all compact groups of interest.","1. It seems to me that the vast majority of application problems involving subgroups of $E(3)$ acting on $\mathbb{R}^3$ simply involve $SO(3)$ or $O(3)$. In that sense, the scope of this paper may be limited. Nonetheless, this does not preclude other groups of interest from arising in future applications, but the paper could be strengthened by providing additional motivating examples.
2. The tradeoffs between this method and the analytical method of [7] are subtle, and indeed they require many of the same tools (for example, knowing the irrep structure of the group of interest, so that one can construct equivariant functions over the input space, even if they are not required to be orthogonal). As noted in the questions, I do not fully understand the benefits of this method as compared to [7], although it is clearly more general and simpler to articulate.","1. My biggest question is that I do not understand the second paragraph of Section 3.2, comparing this approach with that of [7]. The paragraph seems to claim that the basis produced by [7] might be too large, but I thought [7] would produce an orthogonal equivariant basis — and if it is orthogonal, how can the “size” of the basis be suboptimal (as it is determined the dimension of the vector space of steerable functions)?  
2. On a related note, both this paper and [7] seem to require knowledge of the irreps of the group of interest. Can the authors elaborate on which of the ingredients (a) through (d) listed at the bottom of page 5 of [7], are troublesome? Section 3.2 seems to imply not that the subgroup method for obtaining a steerable basis is computationally intensive, but that it produces a lower quality output — is that the case?
3. If already provided with a steerable basis in the style of [7], could one still “condition” on physical info (at least invariants) by just storing a bank of coefficients, and choosing different coefficients based on endpoint features? If so, this could make more sense as a baseline experiment. Of course, such a method may not choose coefficients smoothly as a function of the endpoint features, which is one potential benefit of the authors’ proposed conditioning method.
4. To clarify, if you had a G-equivariant MLP, is the primary barrier to using this G-MLP to derive a G-steerable orthogonal basis the orthogonalization? In other words, one could set random weights to the G-MLP, and then try to orthogonalized to get a basis of equivariant functions? (I think the problem is this: orthogonalization can be done via Gram-Schmidt, but only if one can compute inner products exactly — with $\mathbb{R}^3$ as the base space, it does not seem like one could do this exactly. However, I would be happy to hear the authors’ thoughts on this, as if there were an easy way to reduce a G-equivariant MLP directly to a steerable basis, it would perhaps weaken the argument that their proposed method is easier than computing an explicit G-steerable basis.)
5. In Section 5.2, why is the baseline an $O(3)$-equivariant net? This is an unfair comparison for a problem that is not $O(3)$-equivariant, since such a network can never express the ground truth solution. Can the authors report performance relative to a generic net instead (e.g. only translation invariant), or an approximately equivariant network?
6. What do the authors hypothesize causes the difference in performance shown in Figure 3? For example, is it conditioning on radial vectors? 

As a quick note, here is one other piece of slightly related work at the intersection of equivariance and implicit convolutional kernels: “Relaxing equivariance constraints with non-stationary continuous filters” by Ouderaa et al 2022.",There are not potential negative societal impacts.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,4 excellent,1688669818305,,,NeurIPS.cc/2023/Conference,2YtdxqvdjX,"['NeurIPS.cc/2023/Conference/Submission7886/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411135608,[],2,,,['everyone'],2YtdxqvdjX,['NeurIPS.cc/2023/Conference/Submission7886/Reviewer_GuaE'],1688669818305,1702411135608,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7886/Reviewer_GuaE']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature: ""Relaxing equivariance constraints with non-stationary continuous filters"" by Ouderaa et al 2022.",1,2022,Relaxing equivariance constraints with non-stationary continuous filters
F3GFFUDWAo,"This paper studies the problem of “Adaptive Neyman Allocation”, which involves designing an efficient, adaptive experimental design. Neyman allocation is an infeasible experimental design which would be optimal (minimum variance) if the planner knew all the exact potential outcomes under different treatments. However, this is infeasible, and so the goal considered in the paper is to build an adaptive experimental design which is nearly as efficient (in terms of variance) as the infeasible non-adaptive Neyman allocation asymptotically.
To measure the performance, the first contribution of the paper is to propose new measures of regret (and regret ratio) similar to the notion of regret in bandits/statistical learning. Second, the paper proposes an adaptive design based on the idea of adaptive gradience descent to adjust the treatment probabilities to minimize regret. The paper shows that the regret in this approach scales as O(sqrt(T)). Finally the paper constructs confidence intervals which guarantee asymptotic coverage of the average treatment effect.  
","– Novelty: The paper claims to be the first to introduce the notion of Neyman regret in the context of adaptive experimental designs. I am not fully aware of the related literature, but this seems to be an interesting contribution to analyze from this perspective. 

– Well-written: The paper is well organized overall and the concepts are introduced and explained crisply.  

– Theory: The paper presents results well-grounded in theory.

","1. Related Work: The related work section mainly talks about  Neyman allocation and about casual inference under adaptively collected data. However, it seems there is also a large body of work that studies adaptive experimentation/ adaptive design for randomized trials. All of these references seem to be missing (please see few examples below and references therein)? It would be good to distinguish this paper from this body of related work. 

-- Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289

-- Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180.

-- Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261.

-- Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678.

2. Unsurprising: To me it is a little unsurprising/ unimpressive that the approach can achieve the optimal data efficiency as T tend to infinity (after a really large number of samples). The authors also seem to acknowledge this in part in the final section of the paper. 

3. Empirical Evaluation: It may be useful to add more interesting baselines if available in comparison of the regret 
","– In section 4.1 why isn’t the variance of adaptive experimental design, V also a function of t?

– There seem to be a few other approaches that study adaptive randomziation in clinical trials. For example: Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569. 
How does the proposed approach compare against existing methods?

– Ethical concern: Please see additional question under Limitations section.
","Could there be fairness/social welfare concerns stemming from such optimal designs? Perhaps the allocation algorithm may assign a higher probability to a less effective or detrimental treatment simply because the variance in its outcomes is higher. As a result, in the quest for minimized variance, could the negative treatment be administered more often than advisable/necessary?
",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,3 good,1689133073122,,,NeurIPS.cc/2023/Conference,2Xqvk2KVAq,"['NeurIPS.cc/2023/Conference/Submission10160/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411271797,[],4,,,['everyone'],2Xqvk2KVAq,['NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr'],1689133073122,1702411271797,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that are related to adaptive experimentation and adaptive design for randomized trials, such as:

* Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289
* Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180
* Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261
* Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678
* Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569

The reviewer suggests that the authors should distinguish their paper from this body of related work and provide a more comprehensive review of the literature on adaptive experimentation and adaptive design for randomized trials.",1,"1923, 2016, 1994, 2003, 2006",Über die statistik verketteter vorgänge
F3GFFUDWAo,"This paper studies the problem of “Adaptive Neyman Allocation”, which involves designing an efficient, adaptive experimental design. Neyman allocation is an infeasible experimental design which would be optimal (minimum variance) if the planner knew all the exact potential outcomes under different treatments. However, this is infeasible, and so the goal considered in the paper is to build an adaptive experimental design which is nearly as efficient (in terms of variance) as the infeasible non-adaptive Neyman allocation asymptotically.
To measure the performance, the first contribution of the paper is to propose new measures of regret (and regret ratio) similar to the notion of regret in bandits/statistical learning. Second, the paper proposes an adaptive design based on the idea of adaptive gradience descent to adjust the treatment probabilities to minimize regret. The paper shows that the regret in this approach scales as O(sqrt(T)). Finally the paper constructs confidence intervals which guarantee asymptotic coverage of the average treatment effect.  
","– Novelty: The paper claims to be the first to introduce the notion of Neyman regret in the context of adaptive experimental designs. I am not fully aware of the related literature, but this seems to be an interesting contribution to analyze from this perspective. 

– Well-written: The paper is well organized overall and the concepts are introduced and explained crisply.  

– Theory: The paper presents results well-grounded in theory.

","1. Related Work: The related work section mainly talks about  Neyman allocation and about casual inference under adaptively collected data. However, it seems there is also a large body of work that studies adaptive experimentation/ adaptive design for randomized trials. All of these references seem to be missing (please see few examples below and references therein)? It would be good to distinguish this paper from this body of related work. 

-- Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289

-- Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180.

-- Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261.

-- Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678.

2. Unsurprising: To me it is a little unsurprising/ unimpressive that the approach can achieve the optimal data efficiency as T tend to infinity (after a really large number of samples). The authors also seem to acknowledge this in part in the final section of the paper. 

3. Empirical Evaluation: It may be useful to add more interesting baselines if available in comparison of the regret 
","– In section 4.1 why isn’t the variance of adaptive experimental design, V also a function of t?

– There seem to be a few other approaches that study adaptive randomziation in clinical trials. For example: Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569. 
How does the proposed approach compare against existing methods?

– Ethical concern: Please see additional question under Limitations section.
","Could there be fairness/social welfare concerns stemming from such optimal designs? Perhaps the allocation algorithm may assign a higher probability to a less effective or detrimental treatment simply because the variance in its outcomes is higher. As a result, in the quest for minimized variance, could the negative treatment be administered more often than advisable/necessary?
",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,3 good,1689133073122,,,NeurIPS.cc/2023/Conference,2Xqvk2KVAq,"['NeurIPS.cc/2023/Conference/Submission10160/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411271797,[],4,,,['everyone'],2Xqvk2KVAq,['NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr'],1689133073122,1702411271797,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that are related to adaptive experimentation and adaptive design for randomized trials, such as:

* Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289
* Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180
* Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261
* Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678
* Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569

The reviewer suggests that the authors should distinguish their paper from this body of related work and provide a more comprehensive review of the literature on adaptive experimentation and adaptive design for randomized trials.",1,"1923, 2016, 1994, 2003, 2006",Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials
F3GFFUDWAo,"This paper studies the problem of “Adaptive Neyman Allocation”, which involves designing an efficient, adaptive experimental design. Neyman allocation is an infeasible experimental design which would be optimal (minimum variance) if the planner knew all the exact potential outcomes under different treatments. However, this is infeasible, and so the goal considered in the paper is to build an adaptive experimental design which is nearly as efficient (in terms of variance) as the infeasible non-adaptive Neyman allocation asymptotically.
To measure the performance, the first contribution of the paper is to propose new measures of regret (and regret ratio) similar to the notion of regret in bandits/statistical learning. Second, the paper proposes an adaptive design based on the idea of adaptive gradience descent to adjust the treatment probabilities to minimize regret. The paper shows that the regret in this approach scales as O(sqrt(T)). Finally the paper constructs confidence intervals which guarantee asymptotic coverage of the average treatment effect.  
","– Novelty: The paper claims to be the first to introduce the notion of Neyman regret in the context of adaptive experimental designs. I am not fully aware of the related literature, but this seems to be an interesting contribution to analyze from this perspective. 

– Well-written: The paper is well organized overall and the concepts are introduced and explained crisply.  

– Theory: The paper presents results well-grounded in theory.

","1. Related Work: The related work section mainly talks about  Neyman allocation and about casual inference under adaptively collected data. However, it seems there is also a large body of work that studies adaptive experimentation/ adaptive design for randomized trials. All of these references seem to be missing (please see few examples below and references therein)? It would be good to distinguish this paper from this body of related work. 

-- Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289

-- Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180.

-- Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261.

-- Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678.

2. Unsurprising: To me it is a little unsurprising/ unimpressive that the approach can achieve the optimal data efficiency as T tend to infinity (after a really large number of samples). The authors also seem to acknowledge this in part in the final section of the paper. 

3. Empirical Evaluation: It may be useful to add more interesting baselines if available in comparison of the regret 
","– In section 4.1 why isn’t the variance of adaptive experimental design, V also a function of t?

– There seem to be a few other approaches that study adaptive randomziation in clinical trials. For example: Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569. 
How does the proposed approach compare against existing methods?

– Ethical concern: Please see additional question under Limitations section.
","Could there be fairness/social welfare concerns stemming from such optimal designs? Perhaps the allocation algorithm may assign a higher probability to a less effective or detrimental treatment simply because the variance in its outcomes is higher. As a result, in the quest for minimized variance, could the negative treatment be administered more often than advisable/necessary?
",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,3 good,1689133073122,,,NeurIPS.cc/2023/Conference,2Xqvk2KVAq,"['NeurIPS.cc/2023/Conference/Submission10160/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411271797,[],4,,,['everyone'],2Xqvk2KVAq,['NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr'],1689133073122,1702411271797,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that are related to adaptive experimentation and adaptive design for randomized trials, such as:

* Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289
* Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180
* Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261
* Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678
* Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569

The reviewer suggests that the authors should distinguish their paper from this body of related work and provide a more comprehensive review of the literature on adaptive experimentation and adaptive design for randomized trials.",1,"1923, 2016, 1994, 2003, 2006",The doubly adaptive biased coin design for sequential clinical trials
F3GFFUDWAo,"This paper studies the problem of “Adaptive Neyman Allocation”, which involves designing an efficient, adaptive experimental design. Neyman allocation is an infeasible experimental design which would be optimal (minimum variance) if the planner knew all the exact potential outcomes under different treatments. However, this is infeasible, and so the goal considered in the paper is to build an adaptive experimental design which is nearly as efficient (in terms of variance) as the infeasible non-adaptive Neyman allocation asymptotically.
To measure the performance, the first contribution of the paper is to propose new measures of regret (and regret ratio) similar to the notion of regret in bandits/statistical learning. Second, the paper proposes an adaptive design based on the idea of adaptive gradience descent to adjust the treatment probabilities to minimize regret. The paper shows that the regret in this approach scales as O(sqrt(T)). Finally the paper constructs confidence intervals which guarantee asymptotic coverage of the average treatment effect.  
","– Novelty: The paper claims to be the first to introduce the notion of Neyman regret in the context of adaptive experimental designs. I am not fully aware of the related literature, but this seems to be an interesting contribution to analyze from this perspective. 

– Well-written: The paper is well organized overall and the concepts are introduced and explained crisply.  

– Theory: The paper presents results well-grounded in theory.

","1. Related Work: The related work section mainly talks about  Neyman allocation and about casual inference under adaptively collected data. However, it seems there is also a large body of work that studies adaptive experimentation/ adaptive design for randomized trials. All of these references seem to be missing (please see few examples below and references therein)? It would be good to distinguish this paper from this body of related work. 

-- Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289

-- Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180.

-- Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261.

-- Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678.

2. Unsurprising: To me it is a little unsurprising/ unimpressive that the approach can achieve the optimal data efficiency as T tend to infinity (after a really large number of samples). The authors also seem to acknowledge this in part in the final section of the paper. 

3. Empirical Evaluation: It may be useful to add more interesting baselines if available in comparison of the regret 
","– In section 4.1 why isn’t the variance of adaptive experimental design, V also a function of t?

– There seem to be a few other approaches that study adaptive randomziation in clinical trials. For example: Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569. 
How does the proposed approach compare against existing methods?

– Ethical concern: Please see additional question under Limitations section.
","Could there be fairness/social welfare concerns stemming from such optimal designs? Perhaps the allocation algorithm may assign a higher probability to a less effective or detrimental treatment simply because the variance in its outcomes is higher. As a result, in the quest for minimized variance, could the negative treatment be administered more often than advisable/necessary?
",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,3 good,1689133073122,,,NeurIPS.cc/2023/Conference,2Xqvk2KVAq,"['NeurIPS.cc/2023/Conference/Submission10160/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411271797,[],4,,,['everyone'],2Xqvk2KVAq,['NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr'],1689133073122,1702411271797,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that are related to adaptive experimentation and adaptive design for randomized trials, such as:

* Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289
* Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180
* Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261
* Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678
* Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569

The reviewer suggests that the authors should distinguish their paper from this body of related work and provide a more comprehensive review of the literature on adaptive experimentation and adaptive design for randomized trials.",1,"1923, 2016, 1994, 2003, 2006","Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons"
F3GFFUDWAo,"This paper studies the problem of “Adaptive Neyman Allocation”, which involves designing an efficient, adaptive experimental design. Neyman allocation is an infeasible experimental design which would be optimal (minimum variance) if the planner knew all the exact potential outcomes under different treatments. However, this is infeasible, and so the goal considered in the paper is to build an adaptive experimental design which is nearly as efficient (in terms of variance) as the infeasible non-adaptive Neyman allocation asymptotically.
To measure the performance, the first contribution of the paper is to propose new measures of regret (and regret ratio) similar to the notion of regret in bandits/statistical learning. Second, the paper proposes an adaptive design based on the idea of adaptive gradience descent to adjust the treatment probabilities to minimize regret. The paper shows that the regret in this approach scales as O(sqrt(T)). Finally the paper constructs confidence intervals which guarantee asymptotic coverage of the average treatment effect.  
","– Novelty: The paper claims to be the first to introduce the notion of Neyman regret in the context of adaptive experimental designs. I am not fully aware of the related literature, but this seems to be an interesting contribution to analyze from this perspective. 

– Well-written: The paper is well organized overall and the concepts are introduced and explained crisply.  

– Theory: The paper presents results well-grounded in theory.

","1. Related Work: The related work section mainly talks about  Neyman allocation and about casual inference under adaptively collected data. However, it seems there is also a large body of work that studies adaptive experimentation/ adaptive design for randomized trials. All of these references seem to be missing (please see few examples below and references therein)? It would be good to distinguish this paper from this body of related work. 

-- Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289

-- Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180.

-- Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261.

-- Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678.

2. Unsurprising: To me it is a little unsurprising/ unimpressive that the approach can achieve the optimal data efficiency as T tend to infinity (after a really large number of samples). The authors also seem to acknowledge this in part in the final section of the paper. 

3. Empirical Evaluation: It may be useful to add more interesting baselines if available in comparison of the regret 
","– In section 4.1 why isn’t the variance of adaptive experimental design, V also a function of t?

– There seem to be a few other approaches that study adaptive randomziation in clinical trials. For example: Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569. 
How does the proposed approach compare against existing methods?

– Ethical concern: Please see additional question under Limitations section.
","Could there be fairness/social welfare concerns stemming from such optimal designs? Perhaps the allocation algorithm may assign a higher probability to a less effective or detrimental treatment simply because the variance in its outcomes is higher. As a result, in the quest for minimized variance, could the negative treatment be administered more often than advisable/necessary?
",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,3 good,1689133073122,,,NeurIPS.cc/2023/Conference,2Xqvk2KVAq,"['NeurIPS.cc/2023/Conference/Submission10160/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411271797,[],4,,,['everyone'],2Xqvk2KVAq,['NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr'],1689133073122,1702411271797,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10160/Reviewer_NsFr']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that are related to adaptive experimentation and adaptive design for randomized trials, such as:

* Eggenberger, Florian, and George Pólya. ""Über die statistik verketteter vorgänge."" ZAMM‐Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik 3, no. 4 (1923): 279-289
* Xu, Yanxun, Lorenzo Trippa, Peter Müller, and Yuan Ji. ""Subgroup-based adaptive (SUBA) designs for multi-arm biomarker trials."" Statistics in Biosciences 8 (2016): 159-180
* Eisele, Jeffrey R. ""The doubly adaptive biased coin design for sequential clinical trials."" Journal of Statistical Planning and Inference 38, no. 2 (1994): 249-261
* Hu, Feifang, and William F. Rosenberger. ""Optimality, variability, power: evaluating response-adaptive randomization procedures for treatment comparisons."" Journal of the American Statistical Association 98, no. 463 (2003): 671-678
* Zhang, Lanju, and William F. Rosenberger. ""Response‐adaptive randomization for clinical trials with continuous outcomes."" Biometrics 62.2 (2006): 562-569

The reviewer suggests that the authors should distinguish their paper from this body of related work and provide a more comprehensive review of the literature on adaptive experimentation and adaptive design for randomized trials.",1,"1923, 2016, 1994, 2003, 2006",Response‐adaptive randomization for clinical trials with continuous outcomes
0ffzAsLeiz,"Differentiating through optimal control problems to learn various components, such as the dynamics model or cost function, is a promising method for inverse reinforcement learning (IRL) or incorporating more structure in learned control policies. The central component of these approaches is to differentiate through optimality conditions, such as the KKT conditions or Pontryagin's maximum principle (PMP). Methods which differentiate through the PMP scale linearly with the planning horizon by constructing an auxiliary control system solved in the backward pass. Prior work has argued that this is not the case for methods which differentiate through the KKT conditions due to a large matrix inversion. Instead, this paper shows that this is not true if one properly accounts for the block structure and sparsity patterns of the matrices. The authors show that their approach also scales linearly with horizon and has more opportunity for parallelism to enable faster gradient computation. They compare to Pontryagin Differentiable Programming (PDP), a method derived from PMP, on a number of standard benchmark problems in the case of inverse reinforcement learning. Both methods perform similarly most of the time, with PDP sometimes failing, potentially due to issues with numerical stability. They also show this gap widens when inequality constraints are introduced, with their method yielding significantly better gradients and imitation loss.","- Improving the scalability of implicit differentiation for learning parameters of optimal control problems is an important problem with applications in IRL, system identification, and structured feedback policy classes for reinforcement and imitation learning.
- The paper is well organized and clearly written. It does a good job explaining the novelty and results and provides enough information to support its claims.
- This paper shows that computing gradients through the KKT conditions for general optimal control problems with inequality and equality constraints can also scale linearly with horizon when we properly account for the matrix structure. Additionally, they show that methods which differentiate through the PMP conditions are equivalent to their approach, only differing in the use of a recursive rule for gradient computation.
- Parts of the gradient computation in their method is parallel across time steps, unlike prior work which is entirely sequential. This allows them to compute gradients and vector-Jacobian products much more quickly.
- Gradients appear to be more stable, especially in the case of inequality constraints, compared to PDP and its extensions. The computation time for the backwards pass is also significantly faster for the proposed method over PDP. This may enable scalability to longer horizon problems.
- Unlike prior methods which efficiently differentiate through the KKT conditions, such as [2], they do not rely on differentiating through an approximation of the non-convex problem. Instead, they compute gradients through the original problem, which may have benefits in terms of gradient quality.","- The central goal of the paper is to improve the scalability and numerical stability of gradient computation for long horizons. However, there is no evaluation of runtime or gradient stability across different horizon lengths. Instead, the benchmarks use a fixed horizon, which is of moderate length. It would strengthen the paper to see a breakdown of how the improvements over PDP scale with the horizon length of the problem, and if these trends carry over to even longer horizons than currently considered in the paper. If PDP truly scales worse due to numerical stability issues, then its performance should get worse with longer horizons while the proposed method does not.
- There is no discussion of what solver is used in the forward pass for the experiments and how the Lagrange multipliers are found for gradient computation. Even if the paper reuses the methods from the PDP paper, the paper should still be stand-alone in that it contains these details in the appendix.
- The paper says that differentiable MPC [2] is limited to affine-quadratic systems, which is not true. By using iLQR, it is able to handle nonlinear dynamics and non-convex cost functions. The paper also shows how to incorporate box constraints on controls. However, the proposed approach is more general in that it can handle arbitrary constraints. This should be fixed in the final paper.
- This method is not the first to exploit structure in the KKT conditions to scale linearly with horizon. Despite the arguments in [18, 20], differentiable MPC [2] does not involve a large matrix inversion. Instead, it scales linearly with horizon by solving an auxiliary LQR in the backwards pass, similar in spirit to PDP in [18, 20]. While the proposed method is more general, there should be some discussion of this relationship. It would also strengthen the paper to include it as a benchmark given that it is also derived from the KKT conditions. It would especially be interesting to see how the quality of gradients and runtime of the backwards passes compare.","- How does the gradient stability of the proposed method compare to PDP and its related methods as horizon increases? 
- How does the proposed method fair with much longer horizons than those considered in the experiments?
- What is the solver used in the forward pass and how are the Lagrange multipliers found?","There is some discussion of how the bottleneck on speed is now in the forward pass, given that the backward pass has been significantly sped up. The speed of the forward pass will also depend on the choice of solver for the constrained optimization problem. And there is some discussion of how the opportunities for parallelism in the backward pass are limited due to the availability of solvers for general block tridiagonal linear systems.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1689656243759,,,NeurIPS.cc/2023/Conference,2URr3mkagy,"['NeurIPS.cc/2023/Conference/Submission3511/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410902838,[],5,,,['everyone'],2URr3mkagy,['NeurIPS.cc/2023/Conference/Submission3511/Reviewer_kJe4'],1689656243759,1702410902838,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3511/Reviewer_kJe4']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, particularly:

1. Differentiable MPC [2] - The reviewer mentions that this method is not limited to affine-quadratic systems and can handle nonlinear dynamics and non-convex cost functions. The reviewer suggests that the authors should correct this statement in the paper and also consider including differentiable MPC as a benchmark for comparison.

The reviewer also implies that the authors should be aware of the relationship between their proposed method and differentiable MPC, as both methods exploit structure in the KKT conditions to scale linearly with horizon. However, the reviewer does not explicitly state that the authors need to cite a specific paper, but rather to acknowledge and discuss this relationship in the paper.",1,,Differentiable MPC
XC84ucwF56,"The paper introduces an empirical study for visual pre-training. By focusing on the pre-training, the authors conduct wide-range experiments through (i), data quantity, (ii) label granularity, (iii) label semantics, (iv) image diversity, (v) data sources. The empirical study considers how to use pre-training datasets (both real and synthetic datasets), fine-tuning (data distribution), and test tasks (image classification, out-of-distribution detection). In the experimental section, the study disappears several findings for visual pre-training.","- The novelty of the project is experimental findings with a wide-range of visual pre-training. The paper reveals five items in effect of data quantity (Section 4.1), label granularity (Section 4.2), label semantics (Section 4.3), image diversity (Section 4.4), and pre-training with different data sources (Section 4.5).","- How about implementing vision transformer (ViT) architecture? According to the experimental setup in Section 3, almost all of the network architectures are based on CNN architecture. Although some additional results are reported CLIP, one of the current visual architectures is undoubtedly ViT architecture. Since many architectures are implemented in the experiments, it doesn't seem that there is a lack of computing resources for the experimental conduction.

- The reviewer would touch the results shown in Figure 4. Does the graph show the highest score in the upper right corner? The claimed point with 25k pre-training samples (green line) is better than the baseline with 129k iWildCam images, but still wouldn't it be better to have more data such as 100k and 150k in terms of robustness? With this results, can we conclude tha ""we do not need a lot of pre-training data to see significant robustness gains"" (l.46-47)? 

- Related to Section 4.5.1 and Figure 10, the PixMix framework [Hendrycks+, CVPR22] with the combination of real and synthetic images improves a robustness score. Therefore, for robustness performance, the present paper should implement PixMix framework rather than that of single usage of fractal images.

[Hendrycks+, CVPR22] Dan Hendrycks et al. ""PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures"" in CVPR, 2022.",- See above-mentioned questions,There are no negative limitations and societal impacts.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,2 fair,2 fair,1688392973154,,,NeurIPS.cc/2023/Conference,2SScUiWUbn,"['NeurIPS.cc/2023/Conference/Submission10811/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411303033,[],1,,,['everyone'],2SScUiWUbn,['NeurIPS.cc/2023/Conference/Submission10811/Reviewer_t8Yd'],1688392973154,1702411303033,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10811/Reviewer_t8Yd']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

[Hendrycks+, CVPR22] Dan Hendrycks et al. ""PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures"" in CVPR, 2022.

The reviewer mentions this paper as a relevant work that the authors should consider implementing (PixMix framework) to improve robustness performance, rather than just using fractal images.",1,2022,PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures
MBi0sS7N2e,"The paper presents a modification to the recently proposed DoG algorithm to obtain a adaptive algorithm for the deterministic, convex and [Lipschitz or Smooth] setting that does not need any hyperparameter to achieve a convergence rate competitive with algorithms that know the problem-specific constants.

The submission discusses the benefits of normalization, linking its behavior to the edge of stability phenomenon and providing an analysis that normalized gradient descent obtains the convergence of GD on smooth function with step-size 1/L without needing to know the smoothness constant.","The submission is well presented and I could understand the main objective of the paper at first read. The DoWG algorithm, and especially its proof, is simpler than the one found the prior work of Ivgi et al. (2023) on DoG, and would be an easier starting point for readers trying to understand algorithms that can adapt to the initial distance to the optimum.","Beyond the DoWG algorithm, the contributions are also already known in the literature. To the credit of the submission, this is clearly mentioned for the observation that normalized gradient descent on smooth functions leads to a similar behavior to the edge of stability phenomenon (Arora et al., 2022). However, the result that normalized gradient descent obtains the smooth rate can be found in the work of Levy (2017), which seems to have been missed. 

The submission also suffers from a lack of formalism on some of its claims. Some concepts such as weak and strong adaptivity, “adapting to geometry” and what is meant by the edge of stability are left vague, and would benefit from a formal definition and nuanced discussion. 

If those issues, detailed in the question section, are addressed in a revision, I will increase my score to 6.","**Prior work**

I found the discussion of the adaptivity to smoothness of normalized GD and the key inequalities (5—7) useful to understand how DoWG achieves a similar result. However, the paragraph prior to Theorem 4 should cite Levy (2017). Their analysis of what they call the AdaNGDk algorithm (an overly general form that recovers normalized gradient descent with a decreasing step-size of the order $1/\sqrt{t}$
for k=1), in Theorem 2.1 is the equivalent of theorem 4 in the current submission, except with a decreasing step-size of $1/\sqrt{t}$ rather than the constant but horizon-dependent step-size $1/\sqrt{T}$ used here.

Kfir Levy, “Online to Offline Conversions, Universality and Adaptive Minibatch Sizes”, NeurIPS 2017 

---

**Formal definition for some concepts**

**Strong vs. Weak Adaptivity**

The working definitions of “weak” and “strong” adaptivity are not clear from the current writing, or seems to lead to undesirable conclusions. I find the separation useful, and being able to formalize how sub-optimal the “adaptivity” of AdaGrad-Norm is in the smooth setting would be helpful. 

However, the definition of weakly adaptive requires that “[weak-adaptivity] just seeks non-divergence given stepsize misspecification. […] the algorithm’s objective is to ensure that the learning process does not result in divergence, even if the chosen stepsize is not optimal” (L145). This definition is problematic because an algorithm that does not move, or gradient descent with a step-size of 0, satisfies the above definition. 

Similarly, for strongly adaptive, the text requires that “an algorithm [is strongly adaptive if] it preserves the convergence rate of optimally tuned gradient descent without any manual tuning.” The proposed DoWG algorithm does not fit this description as the rate worse, if only by a log-factor. 

A formalization of the above that could work would be to say that an algorithm is strongly adaptive in a problem setting if it achieves the same convergence rate, up to polylogarithmic terms, as an algorithm that knows problem specific constants. Weakly-adaptive could similarly be defined by allowing for multiplicative polynomial factors. 

**Adapting to geometry**

The term “adapting to geometry” is used in multiple places, but it isn’t clear what is meant by that statement. This term is often used in the literature as a way to convey the intuition of why a method is good, but with limited formalism. For example, Newton adapts to the geometry of the problem by using the Hessian, or AdaGrad adapts to the geometry of the problem by finding a preconditioner that is competitive with the optimal one in hindsight. On each usage of “adapting to geometry”, I do not understand what is meant,. I strongly suggest avoiding it and using a more direct description instead.

- L32: “We say an algorithm is universal if it adapts to many different problem geometries or regularity conditions on the function f”
(should be removed and only mention regularity condition)
- L56: “In particular, such adaptive methods empirically show the ability to adapt to the local geometry of the problem (Cohen et al., 2022), especially compared to plain (stochastic) gradient descent (Pan and Li, 2022).”
L107: “There are other justifications for why adaptive methods work outside of adapting to geometry.”
- L246 “Therefore, we may may expect this to aid the method in adapting to the geometry of the problem once far away from the initialization x0.”

---

**A mention of the difficulties of obtaining similar results in online learning would be helpful to the reader.** 

While the online-learning algorithms of Streeter and McMahan (2012), Orabona et Pal (2016) and Orabona and Cutkosky (2020) aim for a similar goal of not having to know the diameter of the set and the Lipschitz constant, the rates presented for the current algorithm or those in the prior works of of Ivgi et al. (2023) and Carmon and Hinder (2022) are not achievable in the adversarial that is common in online learning, see Cutkosky and Boahen (2016, 2017).

Ashok Cutkosky, Kwabena Boahen, “Online Convex Optimization with Unconstrained Domains and Losses”, NeurIPS 2016
Ashok Cutkosky, Kwabena Boahen, “Online Learning Without Prior Information”, COLT 2017

---

**Question on Universality and the edge of stability**

The convergence of normalized GD or DoWG are not discussed under strong-convexity, which I think should be mentioned given the focus on universal algorithms. Especially as some alternative algorithms such as the Polyak step-size do benefit from strong-convexity, whether in the smooth or Lipschitz+bounded set case.

Given that the empirical results shown in Figure 2 are on a strongly convex problem and the effective step-size oscillates around a constant 2/L, I am interpreting it as the algorithm *not* achieving a linear rate (as the gradient norm should also go down as a linear rate, which does not seem to be the case if $\eta/\Vert\nabla f(x_t)\Vert$ is constant)?


---

**typo?**
- L80: ""Orvieto et al. (2022) show that a variant of the Polyak stepsize with decreasing stepsizes can recover the convergence rate of gradient descent in the deterministic setting, provided the stepsize is initialized properly"" -- this should be stochastic?",Yes,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1689105735430,,,NeurIPS.cc/2023/Conference,2RQhgx1WLA,"['NeurIPS.cc/2023/Conference/Submission5458/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411009672,[],2,,,['everyone'],2RQhgx1WLA,['NeurIPS.cc/2023/Conference/Submission5458/Reviewer_oAov'],1689105735430,1702411009672,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5458/Reviewer_oAov']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Levy (2017) - ""Online to Offline Conversions, Universality and Adaptive Minibatch Sizes"", NeurIPS 2017, specifically Theorem 2.1.
2. Cutkosky and Boahen (2016, 2017) - ""Online Convex Optimization with Unconstrained Domains and Losses"", NeurIPS 2016, and ""Online Learning Without Prior Information"", COLT 2017, to discuss the difficulties of obtaining similar results in online learning.

Additionally, the reviewer mentions Arora et al. (2022) as a reference that is already cited in the paper, but notes that Levy (2017) seems to have been missed.",1,"2017, 2017, 2016, 2017, 2016, 2017, 2022, 2017","Online to Offline Conversions, Universality and Adaptive Minibatch Sizes "
MBi0sS7N2e,"The paper presents a modification to the recently proposed DoG algorithm to obtain a adaptive algorithm for the deterministic, convex and [Lipschitz or Smooth] setting that does not need any hyperparameter to achieve a convergence rate competitive with algorithms that know the problem-specific constants.

The submission discusses the benefits of normalization, linking its behavior to the edge of stability phenomenon and providing an analysis that normalized gradient descent obtains the convergence of GD on smooth function with step-size 1/L without needing to know the smoothness constant.","The submission is well presented and I could understand the main objective of the paper at first read. The DoWG algorithm, and especially its proof, is simpler than the one found the prior work of Ivgi et al. (2023) on DoG, and would be an easier starting point for readers trying to understand algorithms that can adapt to the initial distance to the optimum.","Beyond the DoWG algorithm, the contributions are also already known in the literature. To the credit of the submission, this is clearly mentioned for the observation that normalized gradient descent on smooth functions leads to a similar behavior to the edge of stability phenomenon (Arora et al., 2022). However, the result that normalized gradient descent obtains the smooth rate can be found in the work of Levy (2017), which seems to have been missed. 

The submission also suffers from a lack of formalism on some of its claims. Some concepts such as weak and strong adaptivity, “adapting to geometry” and what is meant by the edge of stability are left vague, and would benefit from a formal definition and nuanced discussion. 

If those issues, detailed in the question section, are addressed in a revision, I will increase my score to 6.","**Prior work**

I found the discussion of the adaptivity to smoothness of normalized GD and the key inequalities (5—7) useful to understand how DoWG achieves a similar result. However, the paragraph prior to Theorem 4 should cite Levy (2017). Their analysis of what they call the AdaNGDk algorithm (an overly general form that recovers normalized gradient descent with a decreasing step-size of the order $1/\sqrt{t}$
for k=1), in Theorem 2.1 is the equivalent of theorem 4 in the current submission, except with a decreasing step-size of $1/\sqrt{t}$ rather than the constant but horizon-dependent step-size $1/\sqrt{T}$ used here.

Kfir Levy, “Online to Offline Conversions, Universality and Adaptive Minibatch Sizes”, NeurIPS 2017 

---

**Formal definition for some concepts**

**Strong vs. Weak Adaptivity**

The working definitions of “weak” and “strong” adaptivity are not clear from the current writing, or seems to lead to undesirable conclusions. I find the separation useful, and being able to formalize how sub-optimal the “adaptivity” of AdaGrad-Norm is in the smooth setting would be helpful. 

However, the definition of weakly adaptive requires that “[weak-adaptivity] just seeks non-divergence given stepsize misspecification. […] the algorithm’s objective is to ensure that the learning process does not result in divergence, even if the chosen stepsize is not optimal” (L145). This definition is problematic because an algorithm that does not move, or gradient descent with a step-size of 0, satisfies the above definition. 

Similarly, for strongly adaptive, the text requires that “an algorithm [is strongly adaptive if] it preserves the convergence rate of optimally tuned gradient descent without any manual tuning.” The proposed DoWG algorithm does not fit this description as the rate worse, if only by a log-factor. 

A formalization of the above that could work would be to say that an algorithm is strongly adaptive in a problem setting if it achieves the same convergence rate, up to polylogarithmic terms, as an algorithm that knows problem specific constants. Weakly-adaptive could similarly be defined by allowing for multiplicative polynomial factors. 

**Adapting to geometry**

The term “adapting to geometry” is used in multiple places, but it isn’t clear what is meant by that statement. This term is often used in the literature as a way to convey the intuition of why a method is good, but with limited formalism. For example, Newton adapts to the geometry of the problem by using the Hessian, or AdaGrad adapts to the geometry of the problem by finding a preconditioner that is competitive with the optimal one in hindsight. On each usage of “adapting to geometry”, I do not understand what is meant,. I strongly suggest avoiding it and using a more direct description instead.

- L32: “We say an algorithm is universal if it adapts to many different problem geometries or regularity conditions on the function f”
(should be removed and only mention regularity condition)
- L56: “In particular, such adaptive methods empirically show the ability to adapt to the local geometry of the problem (Cohen et al., 2022), especially compared to plain (stochastic) gradient descent (Pan and Li, 2022).”
L107: “There are other justifications for why adaptive methods work outside of adapting to geometry.”
- L246 “Therefore, we may may expect this to aid the method in adapting to the geometry of the problem once far away from the initialization x0.”

---

**A mention of the difficulties of obtaining similar results in online learning would be helpful to the reader.** 

While the online-learning algorithms of Streeter and McMahan (2012), Orabona et Pal (2016) and Orabona and Cutkosky (2020) aim for a similar goal of not having to know the diameter of the set and the Lipschitz constant, the rates presented for the current algorithm or those in the prior works of of Ivgi et al. (2023) and Carmon and Hinder (2022) are not achievable in the adversarial that is common in online learning, see Cutkosky and Boahen (2016, 2017).

Ashok Cutkosky, Kwabena Boahen, “Online Convex Optimization with Unconstrained Domains and Losses”, NeurIPS 2016
Ashok Cutkosky, Kwabena Boahen, “Online Learning Without Prior Information”, COLT 2017

---

**Question on Universality and the edge of stability**

The convergence of normalized GD or DoWG are not discussed under strong-convexity, which I think should be mentioned given the focus on universal algorithms. Especially as some alternative algorithms such as the Polyak step-size do benefit from strong-convexity, whether in the smooth or Lipschitz+bounded set case.

Given that the empirical results shown in Figure 2 are on a strongly convex problem and the effective step-size oscillates around a constant 2/L, I am interpreting it as the algorithm *not* achieving a linear rate (as the gradient norm should also go down as a linear rate, which does not seem to be the case if $\eta/\Vert\nabla f(x_t)\Vert$ is constant)?


---

**typo?**
- L80: ""Orvieto et al. (2022) show that a variant of the Polyak stepsize with decreasing stepsizes can recover the convergence rate of gradient descent in the deterministic setting, provided the stepsize is initialized properly"" -- this should be stochastic?",Yes,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1689105735430,,,NeurIPS.cc/2023/Conference,2RQhgx1WLA,"['NeurIPS.cc/2023/Conference/Submission5458/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411009672,[],2,,,['everyone'],2RQhgx1WLA,['NeurIPS.cc/2023/Conference/Submission5458/Reviewer_oAov'],1689105735430,1702411009672,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5458/Reviewer_oAov']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Levy (2017) - ""Online to Offline Conversions, Universality and Adaptive Minibatch Sizes"", NeurIPS 2017, specifically Theorem 2.1.
2. Cutkosky and Boahen (2016, 2017) - ""Online Convex Optimization with Unconstrained Domains and Losses"", NeurIPS 2016, and ""Online Learning Without Prior Information"", COLT 2017, to discuss the difficulties of obtaining similar results in online learning.

Additionally, the reviewer mentions Arora et al. (2022) as a reference that is already cited in the paper, but notes that Levy (2017) seems to have been missed.",1,"2017, 2017, 2016, 2017, 2016, 2017, 2022, 2017",Online Convex Optimization with Unconstrained Domains and Losses 
MBi0sS7N2e,"The paper presents a modification to the recently proposed DoG algorithm to obtain a adaptive algorithm for the deterministic, convex and [Lipschitz or Smooth] setting that does not need any hyperparameter to achieve a convergence rate competitive with algorithms that know the problem-specific constants.

The submission discusses the benefits of normalization, linking its behavior to the edge of stability phenomenon and providing an analysis that normalized gradient descent obtains the convergence of GD on smooth function with step-size 1/L without needing to know the smoothness constant.","The submission is well presented and I could understand the main objective of the paper at first read. The DoWG algorithm, and especially its proof, is simpler than the one found the prior work of Ivgi et al. (2023) on DoG, and would be an easier starting point for readers trying to understand algorithms that can adapt to the initial distance to the optimum.","Beyond the DoWG algorithm, the contributions are also already known in the literature. To the credit of the submission, this is clearly mentioned for the observation that normalized gradient descent on smooth functions leads to a similar behavior to the edge of stability phenomenon (Arora et al., 2022). However, the result that normalized gradient descent obtains the smooth rate can be found in the work of Levy (2017), which seems to have been missed. 

The submission also suffers from a lack of formalism on some of its claims. Some concepts such as weak and strong adaptivity, “adapting to geometry” and what is meant by the edge of stability are left vague, and would benefit from a formal definition and nuanced discussion. 

If those issues, detailed in the question section, are addressed in a revision, I will increase my score to 6.","**Prior work**

I found the discussion of the adaptivity to smoothness of normalized GD and the key inequalities (5—7) useful to understand how DoWG achieves a similar result. However, the paragraph prior to Theorem 4 should cite Levy (2017). Their analysis of what they call the AdaNGDk algorithm (an overly general form that recovers normalized gradient descent with a decreasing step-size of the order $1/\sqrt{t}$
for k=1), in Theorem 2.1 is the equivalent of theorem 4 in the current submission, except with a decreasing step-size of $1/\sqrt{t}$ rather than the constant but horizon-dependent step-size $1/\sqrt{T}$ used here.

Kfir Levy, “Online to Offline Conversions, Universality and Adaptive Minibatch Sizes”, NeurIPS 2017 

---

**Formal definition for some concepts**

**Strong vs. Weak Adaptivity**

The working definitions of “weak” and “strong” adaptivity are not clear from the current writing, or seems to lead to undesirable conclusions. I find the separation useful, and being able to formalize how sub-optimal the “adaptivity” of AdaGrad-Norm is in the smooth setting would be helpful. 

However, the definition of weakly adaptive requires that “[weak-adaptivity] just seeks non-divergence given stepsize misspecification. […] the algorithm’s objective is to ensure that the learning process does not result in divergence, even if the chosen stepsize is not optimal” (L145). This definition is problematic because an algorithm that does not move, or gradient descent with a step-size of 0, satisfies the above definition. 

Similarly, for strongly adaptive, the text requires that “an algorithm [is strongly adaptive if] it preserves the convergence rate of optimally tuned gradient descent without any manual tuning.” The proposed DoWG algorithm does not fit this description as the rate worse, if only by a log-factor. 

A formalization of the above that could work would be to say that an algorithm is strongly adaptive in a problem setting if it achieves the same convergence rate, up to polylogarithmic terms, as an algorithm that knows problem specific constants. Weakly-adaptive could similarly be defined by allowing for multiplicative polynomial factors. 

**Adapting to geometry**

The term “adapting to geometry” is used in multiple places, but it isn’t clear what is meant by that statement. This term is often used in the literature as a way to convey the intuition of why a method is good, but with limited formalism. For example, Newton adapts to the geometry of the problem by using the Hessian, or AdaGrad adapts to the geometry of the problem by finding a preconditioner that is competitive with the optimal one in hindsight. On each usage of “adapting to geometry”, I do not understand what is meant,. I strongly suggest avoiding it and using a more direct description instead.

- L32: “We say an algorithm is universal if it adapts to many different problem geometries or regularity conditions on the function f”
(should be removed and only mention regularity condition)
- L56: “In particular, such adaptive methods empirically show the ability to adapt to the local geometry of the problem (Cohen et al., 2022), especially compared to plain (stochastic) gradient descent (Pan and Li, 2022).”
L107: “There are other justifications for why adaptive methods work outside of adapting to geometry.”
- L246 “Therefore, we may may expect this to aid the method in adapting to the geometry of the problem once far away from the initialization x0.”

---

**A mention of the difficulties of obtaining similar results in online learning would be helpful to the reader.** 

While the online-learning algorithms of Streeter and McMahan (2012), Orabona et Pal (2016) and Orabona and Cutkosky (2020) aim for a similar goal of not having to know the diameter of the set and the Lipschitz constant, the rates presented for the current algorithm or those in the prior works of of Ivgi et al. (2023) and Carmon and Hinder (2022) are not achievable in the adversarial that is common in online learning, see Cutkosky and Boahen (2016, 2017).

Ashok Cutkosky, Kwabena Boahen, “Online Convex Optimization with Unconstrained Domains and Losses”, NeurIPS 2016
Ashok Cutkosky, Kwabena Boahen, “Online Learning Without Prior Information”, COLT 2017

---

**Question on Universality and the edge of stability**

The convergence of normalized GD or DoWG are not discussed under strong-convexity, which I think should be mentioned given the focus on universal algorithms. Especially as some alternative algorithms such as the Polyak step-size do benefit from strong-convexity, whether in the smooth or Lipschitz+bounded set case.

Given that the empirical results shown in Figure 2 are on a strongly convex problem and the effective step-size oscillates around a constant 2/L, I am interpreting it as the algorithm *not* achieving a linear rate (as the gradient norm should also go down as a linear rate, which does not seem to be the case if $\eta/\Vert\nabla f(x_t)\Vert$ is constant)?


---

**typo?**
- L80: ""Orvieto et al. (2022) show that a variant of the Polyak stepsize with decreasing stepsizes can recover the convergence rate of gradient descent in the deterministic setting, provided the stepsize is initialized properly"" -- this should be stochastic?",Yes,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1689105735430,,,NeurIPS.cc/2023/Conference,2RQhgx1WLA,"['NeurIPS.cc/2023/Conference/Submission5458/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411009672,[],2,,,['everyone'],2RQhgx1WLA,['NeurIPS.cc/2023/Conference/Submission5458/Reviewer_oAov'],1689105735430,1702411009672,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5458/Reviewer_oAov']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Levy (2017) - ""Online to Offline Conversions, Universality and Adaptive Minibatch Sizes"", NeurIPS 2017, specifically Theorem 2.1.
2. Cutkosky and Boahen (2016, 2017) - ""Online Convex Optimization with Unconstrained Domains and Losses"", NeurIPS 2016, and ""Online Learning Without Prior Information"", COLT 2017, to discuss the difficulties of obtaining similar results in online learning.

Additionally, the reviewer mentions Arora et al. (2022) as a reference that is already cited in the paper, but notes that Levy (2017) seems to have been missed.",1,"2017, 2017, 2016, 2017, 2016, 2017, 2022, 2017",Online Learning Without Prior Information
wxHqnml4o6,"The paper works on mitigating the impacts of spurious correlations during the risk minimization. The authors firstly introduce a systematic generalization task and illustrates existing methods implicitly made the assumption that all group combinations are represented within the training procedure. And then reveal that the importance of bias labels in model selection. Hence, the authors leverage pre-trained self-supervised models for bias information extraction, during the training and validating the debiased models. ","The proposed method uLA trains a linear classifier on top of an SSL pre-trained model. The predictions are hence biased, and could be made use of model debiasing. The key advantage is that uLA does not need any group labels, neither in the training nor in the validation.","* (1) The presentation could be better , i.e., (a) in figure 2, it would be much better if authors could give more introduction about MPI3D. datasets; (b) Eqn (5) is a core component of uLA, it would be better if authors could explain more about the the insights of the estimate for $\hat{p}_{\text{data}}(Y, Z)$, for example, why this is a reliable estimate? Is it possible for authors to provide some empirical verification on the effectiveness of this estimation?

* (2) Personally speaking, the proposed method is somehow related to a family of distributional robustness approach, that holds the viewpoint that ""fine-tune last layer"" [R1] or ""post-hoc adjust the model prediction"" [R2] can improve the distributional robustness substantially. Such methods also rely on a well-trained model, uLA can also be categorized in this setting since uLA leverages SSL pre-trained models. Thus, it would be better to either compare or discuss the connections between uLA and this family of robust solutions.

* (3) Efficiency of uLA: SSL algorithms usually require longer training time and more computer resources, compared with classical training algorithms. 

References:

[R1] Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. [ICLR'23]

[R2] Distributionally Robust Post-hoc Classifiers under Prior Shifts. [ICLR'23]","* (1) The choice of ssl algorithm: I am not asking authors to run additional experiments, just curious about how uLA behaves when different SSL algorithms are utilized, if the authors already have the result.

* (2) Analysis of $\eta$: are there any analysis on the role of $\eta$ in eqn. (6), either theoretical or empirical analysis?

* (3) How is the performance of uLA on waterbirds when a SSL pre-trained model is utilized? ","As mentioned in weakness 3, one limitation is **Efficiency of uLA:** SSL algorithms usually require longer training time and more computer resources, compared with classical training algorithms. ",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,4 excellent,2 fair,3 good,1688770199020,,,NeurIPS.cc/2023/Conference,2OcNWFHFpk,"['NeurIPS.cc/2023/Conference/Submission14819/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411494771,[],3,,,['everyone'],2OcNWFHFpk,['NeurIPS.cc/2023/Conference/Submission14819/Reviewer_knxG'],1688770199020,1702411494771,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14819/Reviewer_knxG']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. [R1] Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. [ICLR'23]
2. [R2] Distributionally Robust Post-hoc Classifiers under Prior Shifts. [ICLR'23]

The reviewer mentions that the proposed method uLA can be categorized in the same setting as these references, which hold the viewpoint that ""fine-tune last layer"" or ""post-hoc adjust the model prediction"" can improve the distributional robustness substantially. The reviewer suggests that it would be better to either compare or discuss the connections between uLA and this family of robust solutions.",1,"2023, 2023",Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations 
wxHqnml4o6,"The paper works on mitigating the impacts of spurious correlations during the risk minimization. The authors firstly introduce a systematic generalization task and illustrates existing methods implicitly made the assumption that all group combinations are represented within the training procedure. And then reveal that the importance of bias labels in model selection. Hence, the authors leverage pre-trained self-supervised models for bias information extraction, during the training and validating the debiased models. ","The proposed method uLA trains a linear classifier on top of an SSL pre-trained model. The predictions are hence biased, and could be made use of model debiasing. The key advantage is that uLA does not need any group labels, neither in the training nor in the validation.","* (1) The presentation could be better , i.e., (a) in figure 2, it would be much better if authors could give more introduction about MPI3D. datasets; (b) Eqn (5) is a core component of uLA, it would be better if authors could explain more about the the insights of the estimate for $\hat{p}_{\text{data}}(Y, Z)$, for example, why this is a reliable estimate? Is it possible for authors to provide some empirical verification on the effectiveness of this estimation?

* (2) Personally speaking, the proposed method is somehow related to a family of distributional robustness approach, that holds the viewpoint that ""fine-tune last layer"" [R1] or ""post-hoc adjust the model prediction"" [R2] can improve the distributional robustness substantially. Such methods also rely on a well-trained model, uLA can also be categorized in this setting since uLA leverages SSL pre-trained models. Thus, it would be better to either compare or discuss the connections between uLA and this family of robust solutions.

* (3) Efficiency of uLA: SSL algorithms usually require longer training time and more computer resources, compared with classical training algorithms. 

References:

[R1] Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. [ICLR'23]

[R2] Distributionally Robust Post-hoc Classifiers under Prior Shifts. [ICLR'23]","* (1) The choice of ssl algorithm: I am not asking authors to run additional experiments, just curious about how uLA behaves when different SSL algorithms are utilized, if the authors already have the result.

* (2) Analysis of $\eta$: are there any analysis on the role of $\eta$ in eqn. (6), either theoretical or empirical analysis?

* (3) How is the performance of uLA on waterbirds when a SSL pre-trained model is utilized? ","As mentioned in weakness 3, one limitation is **Efficiency of uLA:** SSL algorithms usually require longer training time and more computer resources, compared with classical training algorithms. ",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,4 excellent,2 fair,3 good,1688770199020,,,NeurIPS.cc/2023/Conference,2OcNWFHFpk,"['NeurIPS.cc/2023/Conference/Submission14819/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411494771,[],3,,,['everyone'],2OcNWFHFpk,['NeurIPS.cc/2023/Conference/Submission14819/Reviewer_knxG'],1688770199020,1702411494771,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14819/Reviewer_knxG']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. [R1] Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. [ICLR'23]
2. [R2] Distributionally Robust Post-hoc Classifiers under Prior Shifts. [ICLR'23]

The reviewer mentions that the proposed method uLA can be categorized in the same setting as these references, which hold the viewpoint that ""fine-tune last layer"" or ""post-hoc adjust the model prediction"" can improve the distributional robustness substantially. The reviewer suggests that it would be better to either compare or discuss the connections between uLA and this family of robust solutions.",1,"2023, 2023",Distributionally Robust Post-hoc Classifiers under Prior Shifts
1s3TdoeS9D,"This paper introduces a new bias-unsupervised method for addressing spurious correlations, encompassing a debiasing training algorithm and a model selection paradigm. Specifically, the method employs a Self-Supervised Learning (SSL) pre-trained model as a bias proxy. The SSL model's fixed predictions are used to adjust the logit during the predictor's training phase. These predictions are also utilized in the model selection stage to adjust the weight of the sample, aiming to approximate group-balanced accuracy. Empirical evidence supports the method's efficacy, as demonstrated on several popular benchmarks, as well as a systematic generalization benchmark.","- How to design a proper bias proxy is important for the bias-unsupervised methods.
- The presentation of the method is clear and generally easy to follow.
- A variety of benchmarks are considered in the experiments and an ablation study part is included.","- Inconsistency. In the introduction part, one of the motivation of this work is to solve the failure of existing methods in systematic generalization. However, though empirically shown, it is not clearly introduced how the proposed method could achieve that improvement.
- Some important related works are missing. 
   - There are some other existing bias-unsupervised methods, including EIIL[1], PGI[2], and SCILL[3]. Similar to the method proposed in this work, all the 3 methods utilize a frozen reference model as a bias proxy. Especially, PGI is proposed for the systematic generalization task. 
   - The logit adjustment strategy is similar to the strategies in some debiasing training methods, e.g. PoE[4] and DRiFt[5]. 
   - A bias-unsupervised validation paradigm is also proposed in [2].

Therefore, this paper should discuss these related works and reposition its contribution.

References:

[1] Creager et al. ""Environment inference for invariant learning."" ICML, 2021.     
[2] Ahmed et al. ""Systematic generalisation with group invariant predictions."" ICML, 2021.     
[3] Chen et al. ""When does group invariant learning survive spurious correlations?"" NeurIPS, 2022.
[4] Clark et al. ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases."" EMNLP, 2019.
[5] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. DeepLo workshop, 2019.","- Why the proposed method can benefit systematic generalization, theoretically or intuitively?",Some theoretical justifications for the proposed method are lacking.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688833695366,,,NeurIPS.cc/2023/Conference,2OcNWFHFpk,"['NeurIPS.cc/2023/Conference/Submission14819/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411494670,[],4,,,['everyone'],2OcNWFHFpk,['NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n'],1688833695366,1702411494670,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. EIIL [1] - ""Environment inference for invariant learning"" by Creager et al. (ICML, 2021)
2. PGI [2] - ""Systematic generalisation with group invariant predictions"" by Ahmed et al. (ICML, 2021)
3. SCILL [3] - ""When does group invariant learning survive spurious correlations?"" by Chen et al. (NeurIPS, 2022)
4. PoE [4] - ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases"" by Clark et al. (EMNLP, 2019)
5. DRiFt [5] - ""Unlearn dataset bias in natural language inference by fitting the residual"" by He He, Sheng Zha, and Haohan Wang (DeepLo workshop, 2019)

The reviewer suggests that the authors discuss these related works and reposition their contribution to provide a clearer understanding of the proposed method's novelty and significance.",1,"2021, 2021, 2022, 2019, 2019",Environment inference for invariant learning 
1s3TdoeS9D,"This paper introduces a new bias-unsupervised method for addressing spurious correlations, encompassing a debiasing training algorithm and a model selection paradigm. Specifically, the method employs a Self-Supervised Learning (SSL) pre-trained model as a bias proxy. The SSL model's fixed predictions are used to adjust the logit during the predictor's training phase. These predictions are also utilized in the model selection stage to adjust the weight of the sample, aiming to approximate group-balanced accuracy. Empirical evidence supports the method's efficacy, as demonstrated on several popular benchmarks, as well as a systematic generalization benchmark.","- How to design a proper bias proxy is important for the bias-unsupervised methods.
- The presentation of the method is clear and generally easy to follow.
- A variety of benchmarks are considered in the experiments and an ablation study part is included.","- Inconsistency. In the introduction part, one of the motivation of this work is to solve the failure of existing methods in systematic generalization. However, though empirically shown, it is not clearly introduced how the proposed method could achieve that improvement.
- Some important related works are missing. 
   - There are some other existing bias-unsupervised methods, including EIIL[1], PGI[2], and SCILL[3]. Similar to the method proposed in this work, all the 3 methods utilize a frozen reference model as a bias proxy. Especially, PGI is proposed for the systematic generalization task. 
   - The logit adjustment strategy is similar to the strategies in some debiasing training methods, e.g. PoE[4] and DRiFt[5]. 
   - A bias-unsupervised validation paradigm is also proposed in [2].

Therefore, this paper should discuss these related works and reposition its contribution.

References:

[1] Creager et al. ""Environment inference for invariant learning."" ICML, 2021.     
[2] Ahmed et al. ""Systematic generalisation with group invariant predictions."" ICML, 2021.     
[3] Chen et al. ""When does group invariant learning survive spurious correlations?"" NeurIPS, 2022.
[4] Clark et al. ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases."" EMNLP, 2019.
[5] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. DeepLo workshop, 2019.","- Why the proposed method can benefit systematic generalization, theoretically or intuitively?",Some theoretical justifications for the proposed method are lacking.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688833695366,,,NeurIPS.cc/2023/Conference,2OcNWFHFpk,"['NeurIPS.cc/2023/Conference/Submission14819/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411494670,[],4,,,['everyone'],2OcNWFHFpk,['NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n'],1688833695366,1702411494670,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. EIIL [1] - ""Environment inference for invariant learning"" by Creager et al. (ICML, 2021)
2. PGI [2] - ""Systematic generalisation with group invariant predictions"" by Ahmed et al. (ICML, 2021)
3. SCILL [3] - ""When does group invariant learning survive spurious correlations?"" by Chen et al. (NeurIPS, 2022)
4. PoE [4] - ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases"" by Clark et al. (EMNLP, 2019)
5. DRiFt [5] - ""Unlearn dataset bias in natural language inference by fitting the residual"" by He He, Sheng Zha, and Haohan Wang (DeepLo workshop, 2019)

The reviewer suggests that the authors discuss these related works and reposition their contribution to provide a clearer understanding of the proposed method's novelty and significance.",1,"2021, 2021, 2022, 2019, 2019",Systematic generalisation with group invariant predictions 
1s3TdoeS9D,"This paper introduces a new bias-unsupervised method for addressing spurious correlations, encompassing a debiasing training algorithm and a model selection paradigm. Specifically, the method employs a Self-Supervised Learning (SSL) pre-trained model as a bias proxy. The SSL model's fixed predictions are used to adjust the logit during the predictor's training phase. These predictions are also utilized in the model selection stage to adjust the weight of the sample, aiming to approximate group-balanced accuracy. Empirical evidence supports the method's efficacy, as demonstrated on several popular benchmarks, as well as a systematic generalization benchmark.","- How to design a proper bias proxy is important for the bias-unsupervised methods.
- The presentation of the method is clear and generally easy to follow.
- A variety of benchmarks are considered in the experiments and an ablation study part is included.","- Inconsistency. In the introduction part, one of the motivation of this work is to solve the failure of existing methods in systematic generalization. However, though empirically shown, it is not clearly introduced how the proposed method could achieve that improvement.
- Some important related works are missing. 
   - There are some other existing bias-unsupervised methods, including EIIL[1], PGI[2], and SCILL[3]. Similar to the method proposed in this work, all the 3 methods utilize a frozen reference model as a bias proxy. Especially, PGI is proposed for the systematic generalization task. 
   - The logit adjustment strategy is similar to the strategies in some debiasing training methods, e.g. PoE[4] and DRiFt[5]. 
   - A bias-unsupervised validation paradigm is also proposed in [2].

Therefore, this paper should discuss these related works and reposition its contribution.

References:

[1] Creager et al. ""Environment inference for invariant learning."" ICML, 2021.     
[2] Ahmed et al. ""Systematic generalisation with group invariant predictions."" ICML, 2021.     
[3] Chen et al. ""When does group invariant learning survive spurious correlations?"" NeurIPS, 2022.
[4] Clark et al. ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases."" EMNLP, 2019.
[5] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. DeepLo workshop, 2019.","- Why the proposed method can benefit systematic generalization, theoretically or intuitively?",Some theoretical justifications for the proposed method are lacking.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688833695366,,,NeurIPS.cc/2023/Conference,2OcNWFHFpk,"['NeurIPS.cc/2023/Conference/Submission14819/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411494670,[],4,,,['everyone'],2OcNWFHFpk,['NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n'],1688833695366,1702411494670,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. EIIL [1] - ""Environment inference for invariant learning"" by Creager et al. (ICML, 2021)
2. PGI [2] - ""Systematic generalisation with group invariant predictions"" by Ahmed et al. (ICML, 2021)
3. SCILL [3] - ""When does group invariant learning survive spurious correlations?"" by Chen et al. (NeurIPS, 2022)
4. PoE [4] - ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases"" by Clark et al. (EMNLP, 2019)
5. DRiFt [5] - ""Unlearn dataset bias in natural language inference by fitting the residual"" by He He, Sheng Zha, and Haohan Wang (DeepLo workshop, 2019)

The reviewer suggests that the authors discuss these related works and reposition their contribution to provide a clearer understanding of the proposed method's novelty and significance.",1,"2021, 2021, 2022, 2019, 2019",When does group invariant learning survive spurious correlations 
1s3TdoeS9D,"This paper introduces a new bias-unsupervised method for addressing spurious correlations, encompassing a debiasing training algorithm and a model selection paradigm. Specifically, the method employs a Self-Supervised Learning (SSL) pre-trained model as a bias proxy. The SSL model's fixed predictions are used to adjust the logit during the predictor's training phase. These predictions are also utilized in the model selection stage to adjust the weight of the sample, aiming to approximate group-balanced accuracy. Empirical evidence supports the method's efficacy, as demonstrated on several popular benchmarks, as well as a systematic generalization benchmark.","- How to design a proper bias proxy is important for the bias-unsupervised methods.
- The presentation of the method is clear and generally easy to follow.
- A variety of benchmarks are considered in the experiments and an ablation study part is included.","- Inconsistency. In the introduction part, one of the motivation of this work is to solve the failure of existing methods in systematic generalization. However, though empirically shown, it is not clearly introduced how the proposed method could achieve that improvement.
- Some important related works are missing. 
   - There are some other existing bias-unsupervised methods, including EIIL[1], PGI[2], and SCILL[3]. Similar to the method proposed in this work, all the 3 methods utilize a frozen reference model as a bias proxy. Especially, PGI is proposed for the systematic generalization task. 
   - The logit adjustment strategy is similar to the strategies in some debiasing training methods, e.g. PoE[4] and DRiFt[5]. 
   - A bias-unsupervised validation paradigm is also proposed in [2].

Therefore, this paper should discuss these related works and reposition its contribution.

References:

[1] Creager et al. ""Environment inference for invariant learning."" ICML, 2021.     
[2] Ahmed et al. ""Systematic generalisation with group invariant predictions."" ICML, 2021.     
[3] Chen et al. ""When does group invariant learning survive spurious correlations?"" NeurIPS, 2022.
[4] Clark et al. ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases."" EMNLP, 2019.
[5] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. DeepLo workshop, 2019.","- Why the proposed method can benefit systematic generalization, theoretically or intuitively?",Some theoretical justifications for the proposed method are lacking.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688833695366,,,NeurIPS.cc/2023/Conference,2OcNWFHFpk,"['NeurIPS.cc/2023/Conference/Submission14819/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411494670,[],4,,,['everyone'],2OcNWFHFpk,['NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n'],1688833695366,1702411494670,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. EIIL [1] - ""Environment inference for invariant learning"" by Creager et al. (ICML, 2021)
2. PGI [2] - ""Systematic generalisation with group invariant predictions"" by Ahmed et al. (ICML, 2021)
3. SCILL [3] - ""When does group invariant learning survive spurious correlations?"" by Chen et al. (NeurIPS, 2022)
4. PoE [4] - ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases"" by Clark et al. (EMNLP, 2019)
5. DRiFt [5] - ""Unlearn dataset bias in natural language inference by fitting the residual"" by He He, Sheng Zha, and Haohan Wang (DeepLo workshop, 2019)

The reviewer suggests that the authors discuss these related works and reposition their contribution to provide a clearer understanding of the proposed method's novelty and significance.",1,"2021, 2021, 2022, 2019, 2019",Don’t take the easy way out Ensemble based methods for avoiding known dataset biases 
1s3TdoeS9D,"This paper introduces a new bias-unsupervised method for addressing spurious correlations, encompassing a debiasing training algorithm and a model selection paradigm. Specifically, the method employs a Self-Supervised Learning (SSL) pre-trained model as a bias proxy. The SSL model's fixed predictions are used to adjust the logit during the predictor's training phase. These predictions are also utilized in the model selection stage to adjust the weight of the sample, aiming to approximate group-balanced accuracy. Empirical evidence supports the method's efficacy, as demonstrated on several popular benchmarks, as well as a systematic generalization benchmark.","- How to design a proper bias proxy is important for the bias-unsupervised methods.
- The presentation of the method is clear and generally easy to follow.
- A variety of benchmarks are considered in the experiments and an ablation study part is included.","- Inconsistency. In the introduction part, one of the motivation of this work is to solve the failure of existing methods in systematic generalization. However, though empirically shown, it is not clearly introduced how the proposed method could achieve that improvement.
- Some important related works are missing. 
   - There are some other existing bias-unsupervised methods, including EIIL[1], PGI[2], and SCILL[3]. Similar to the method proposed in this work, all the 3 methods utilize a frozen reference model as a bias proxy. Especially, PGI is proposed for the systematic generalization task. 
   - The logit adjustment strategy is similar to the strategies in some debiasing training methods, e.g. PoE[4] and DRiFt[5]. 
   - A bias-unsupervised validation paradigm is also proposed in [2].

Therefore, this paper should discuss these related works and reposition its contribution.

References:

[1] Creager et al. ""Environment inference for invariant learning."" ICML, 2021.     
[2] Ahmed et al. ""Systematic generalisation with group invariant predictions."" ICML, 2021.     
[3] Chen et al. ""When does group invariant learning survive spurious correlations?"" NeurIPS, 2022.
[4] Clark et al. ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases."" EMNLP, 2019.
[5] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. DeepLo workshop, 2019.","- Why the proposed method can benefit systematic generalization, theoretically or intuitively?",Some theoretical justifications for the proposed method are lacking.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688833695366,,,NeurIPS.cc/2023/Conference,2OcNWFHFpk,"['NeurIPS.cc/2023/Conference/Submission14819/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411494670,[],4,,,['everyone'],2OcNWFHFpk,['NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n'],1688833695366,1702411494670,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14819/Reviewer_Y39n']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. EIIL [1] - ""Environment inference for invariant learning"" by Creager et al. (ICML, 2021)
2. PGI [2] - ""Systematic generalisation with group invariant predictions"" by Ahmed et al. (ICML, 2021)
3. SCILL [3] - ""When does group invariant learning survive spurious correlations?"" by Chen et al. (NeurIPS, 2022)
4. PoE [4] - ""Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases"" by Clark et al. (EMNLP, 2019)
5. DRiFt [5] - ""Unlearn dataset bias in natural language inference by fitting the residual"" by He He, Sheng Zha, and Haohan Wang (DeepLo workshop, 2019)

The reviewer suggests that the authors discuss these related works and reposition their contribution to provide a clearer understanding of the proposed method's novelty and significance.",1,"2021, 2021, 2022, 2019, 2019",Unlearn dataset bias in natural language inference by fitting the residual
1CkWom0IMn,"The authors propose a novel ways to train audio-text model by adding a token interaction module followed by the same frozen text encoder from the text modality after audio encoder to learn audio representation. And they show that proposed approach yield state-of-the-art results in couple audio tagging and classification tasks, cross-modal retrieval, and qualitative analysis on audio guided image generation.","- The proposed approach of appending token interaction module followed by text encoder is a very interesting and novel idea to leverage language model.
- Downstream evaluation are provided with both zero-shot and linear probe settings, this provides more thorough understanding of the behavior for proposed systems.","- Proposed architecture is really interesting, however, the training with AudioSet labels in constructing prompts as described in section 4.1 line 248-250 seem arbitrary. The claim that ""such templates have been shown to be more effective than simple concatenation"" needs more in depth evidence or reference. Also given the works in increasing audio captioning data and applied to audio-text model pre-training (e.g. [1], [2]), this work can benefit from pre-training utilizing these audio and natural language descriptions besides pre-training with AudioSet with arbitrary templating on labels.
- On the downstream tasks, other than audio tagging/classification tasks, language-based audio retrieval is another highly relevant task which provide different perspective to the audio-text pre-training, consider adding those audio-text benchmarks [2] to the evaluation to expand the coverage in this work.

[1] Elizalde, Benjamin, et al. ""Clap learning audio concepts from natural language supervision."" ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.

[2] Wu, Yusong, et al. ""Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation."" ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.","- For the audio and text encoder selections, have you tried other options, for example ResNet/CNN based for audio and other text encoders such as BERT or RoBERTa? For the other systems to compare as shown in Table 2, they all utilize different encoder architectures, such ablation study can help improve and disentangle whether the improvement comes from proposed architecture versus encoder choices.","- This work is slightly limited in then evaluation with only audio tagging/classification tasks, it can benefit from adding more audio-text tasks involving longer form natural languages to provide more holistic views.",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688313975087,,,NeurIPS.cc/2023/Conference,2NncD8AaFK,"['NeurIPS.cc/2023/Conference/Submission13079/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411413293,[],1,,,['everyone'],2NncD8AaFK,['NeurIPS.cc/2023/Conference/Submission13079/Reviewer_GZR1'],1688313975087,1702411413293,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13079/Reviewer_GZR1']","Yes, the peer review explicitly suggests the authors to cite specific literature, namely:

1. Elizalde, Benjamin, et al. ""Clap learning audio concepts from natural language supervision."" (ICASSP 2023)
2. Wu, Yusong, et al. ""Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation."" (ICASSP 2023)

These citations are mentioned in the ""weaknesses"" section of the review, where the reviewer suggests that the authors' work could benefit from pre-training utilizing audio and natural language descriptions, and that the authors should consider adding audio-text benchmarks from these references to the evaluation.",1,"2023, 2023",Clap learning audio concepts from natural language supervision 
1CkWom0IMn,"The authors propose a novel ways to train audio-text model by adding a token interaction module followed by the same frozen text encoder from the text modality after audio encoder to learn audio representation. And they show that proposed approach yield state-of-the-art results in couple audio tagging and classification tasks, cross-modal retrieval, and qualitative analysis on audio guided image generation.","- The proposed approach of appending token interaction module followed by text encoder is a very interesting and novel idea to leverage language model.
- Downstream evaluation are provided with both zero-shot and linear probe settings, this provides more thorough understanding of the behavior for proposed systems.","- Proposed architecture is really interesting, however, the training with AudioSet labels in constructing prompts as described in section 4.1 line 248-250 seem arbitrary. The claim that ""such templates have been shown to be more effective than simple concatenation"" needs more in depth evidence or reference. Also given the works in increasing audio captioning data and applied to audio-text model pre-training (e.g. [1], [2]), this work can benefit from pre-training utilizing these audio and natural language descriptions besides pre-training with AudioSet with arbitrary templating on labels.
- On the downstream tasks, other than audio tagging/classification tasks, language-based audio retrieval is another highly relevant task which provide different perspective to the audio-text pre-training, consider adding those audio-text benchmarks [2] to the evaluation to expand the coverage in this work.

[1] Elizalde, Benjamin, et al. ""Clap learning audio concepts from natural language supervision."" ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.

[2] Wu, Yusong, et al. ""Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation."" ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.","- For the audio and text encoder selections, have you tried other options, for example ResNet/CNN based for audio and other text encoders such as BERT or RoBERTa? For the other systems to compare as shown in Table 2, they all utilize different encoder architectures, such ablation study can help improve and disentangle whether the improvement comes from proposed architecture versus encoder choices.","- This work is slightly limited in then evaluation with only audio tagging/classification tasks, it can benefit from adding more audio-text tasks involving longer form natural languages to provide more holistic views.",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688313975087,,,NeurIPS.cc/2023/Conference,2NncD8AaFK,"['NeurIPS.cc/2023/Conference/Submission13079/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411413293,[],1,,,['everyone'],2NncD8AaFK,['NeurIPS.cc/2023/Conference/Submission13079/Reviewer_GZR1'],1688313975087,1702411413293,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13079/Reviewer_GZR1']","Yes, the peer review explicitly suggests the authors to cite specific literature, namely:

1. Elizalde, Benjamin, et al. ""Clap learning audio concepts from natural language supervision."" (ICASSP 2023)
2. Wu, Yusong, et al. ""Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation."" (ICASSP 2023)

These citations are mentioned in the ""weaknesses"" section of the review, where the reviewer suggests that the authors' work could benefit from pre-training utilizing audio and natural language descriptions, and that the authors should consider adding audio-text benchmarks from these references to the evaluation.",1,"2023, 2023",Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation
cf6cugvVfI,"The paper introduces CoLLAT, an audio-language framework that makes use of a pre-trained language model. The framework is trained with a contrastive objective which encourages learning of fine-grained audio-text grounding. The paper presents very strong results for diverse downstream tasks, such as zero-shot audio classification, audio-image retrieval, and audio-guided image generation.","The proposed framework leverages a locked pre-trained language model (CLIP text encoder) as audio and text encoder. It learns to map the audio input to input tokens for the frozen text encoder, which retains the strong capabilities of the language model. The model is trained to encourage very fine-grained understanding of audio. It achieves very convincing results on various downstream tasks.","Given that this paper claims to yield fine-grained audio-to-text grounding, I believe that it would be good to show results for audio-text retrieval on CLOTHO, AudioCaps, and SoundDescs [A,B]. 

One of the main model contributions, the token interaction module, should be explained better and in more detail. For instance, it is not mentioned how many blocks are used (l.192: “multiple cross-attention-based blocks”). In addition, it would be good to add a detailed explanation on the denoising pipeline in this module. Furthermore, different components of this block should be ablated in the model ablation study.
Similarly the paper does currently not contain a model ablation for 1) the use of the same encoder for audio and text instead of using a different audio model, 2) the effect of using pre-trained encoders. These should be added to validate the architectural choices.

Overall, I think the paper is interesting, and I would be happy to consider raising my score if the missing evaluations and ablations are added and convincing.



[A]: Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021

[B] Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022","* Is CLAP shown in Figure 5 referring to [6]? If not, different naming should be used. 

* I believe that some relevant prior works should be referenced, e.g. [C,D,E].

* What is the computational cost to train the framework and what kind of hardware was used for training?

[C]: Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022

​​[D]: Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022

[E]: Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023



* Typos:
l.62: lost -> lose

l.136 alignemnt -> alignment

l.181: space missing after “CLIP.”

l.232: We adopt AudioSet dataset -> We adopt the AudioSet dataset

Figure 4 is a table and should be labelled accordingly.

What does t in $128x100 t$ in l.240 refer to?",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1688672979293,,,NeurIPS.cc/2023/Conference,2NncD8AaFK,"['NeurIPS.cc/2023/Conference/Submission13079/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411412997,[],4,,,['everyone'],2NncD8AaFK,['NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz'],1688672979293,1702411412997,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

* Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021 (referenced as [A])
* Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022 (referenced as [B])
* Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022 (referenced as [C])
* Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022 (referenced as [D])
* Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023 (referenced as [E])

The reviewer believes that these prior works are relevant and should be referenced in the paper.",1,"2021, 2022, 2022, 2022, 2023",Audio Retrieval with Natural Language Queries 
cf6cugvVfI,"The paper introduces CoLLAT, an audio-language framework that makes use of a pre-trained language model. The framework is trained with a contrastive objective which encourages learning of fine-grained audio-text grounding. The paper presents very strong results for diverse downstream tasks, such as zero-shot audio classification, audio-image retrieval, and audio-guided image generation.","The proposed framework leverages a locked pre-trained language model (CLIP text encoder) as audio and text encoder. It learns to map the audio input to input tokens for the frozen text encoder, which retains the strong capabilities of the language model. The model is trained to encourage very fine-grained understanding of audio. It achieves very convincing results on various downstream tasks.","Given that this paper claims to yield fine-grained audio-to-text grounding, I believe that it would be good to show results for audio-text retrieval on CLOTHO, AudioCaps, and SoundDescs [A,B]. 

One of the main model contributions, the token interaction module, should be explained better and in more detail. For instance, it is not mentioned how many blocks are used (l.192: “multiple cross-attention-based blocks”). In addition, it would be good to add a detailed explanation on the denoising pipeline in this module. Furthermore, different components of this block should be ablated in the model ablation study.
Similarly the paper does currently not contain a model ablation for 1) the use of the same encoder for audio and text instead of using a different audio model, 2) the effect of using pre-trained encoders. These should be added to validate the architectural choices.

Overall, I think the paper is interesting, and I would be happy to consider raising my score if the missing evaluations and ablations are added and convincing.



[A]: Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021

[B] Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022","* Is CLAP shown in Figure 5 referring to [6]? If not, different naming should be used. 

* I believe that some relevant prior works should be referenced, e.g. [C,D,E].

* What is the computational cost to train the framework and what kind of hardware was used for training?

[C]: Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022

​​[D]: Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022

[E]: Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023



* Typos:
l.62: lost -> lose

l.136 alignemnt -> alignment

l.181: space missing after “CLIP.”

l.232: We adopt AudioSet dataset -> We adopt the AudioSet dataset

Figure 4 is a table and should be labelled accordingly.

What does t in $128x100 t$ in l.240 refer to?",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1688672979293,,,NeurIPS.cc/2023/Conference,2NncD8AaFK,"['NeurIPS.cc/2023/Conference/Submission13079/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411412997,[],4,,,['everyone'],2NncD8AaFK,['NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz'],1688672979293,1702411412997,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

* Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021 (referenced as [A])
* Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022 (referenced as [B])
* Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022 (referenced as [C])
* Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022 (referenced as [D])
* Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023 (referenced as [E])

The reviewer believes that these prior works are relevant and should be referenced in the paper.",1,"2021, 2022, 2022, 2022, 2023",Audio Retrieval with Natural Language Queries A benchmark study 
cf6cugvVfI,"The paper introduces CoLLAT, an audio-language framework that makes use of a pre-trained language model. The framework is trained with a contrastive objective which encourages learning of fine-grained audio-text grounding. The paper presents very strong results for diverse downstream tasks, such as zero-shot audio classification, audio-image retrieval, and audio-guided image generation.","The proposed framework leverages a locked pre-trained language model (CLIP text encoder) as audio and text encoder. It learns to map the audio input to input tokens for the frozen text encoder, which retains the strong capabilities of the language model. The model is trained to encourage very fine-grained understanding of audio. It achieves very convincing results on various downstream tasks.","Given that this paper claims to yield fine-grained audio-to-text grounding, I believe that it would be good to show results for audio-text retrieval on CLOTHO, AudioCaps, and SoundDescs [A,B]. 

One of the main model contributions, the token interaction module, should be explained better and in more detail. For instance, it is not mentioned how many blocks are used (l.192: “multiple cross-attention-based blocks”). In addition, it would be good to add a detailed explanation on the denoising pipeline in this module. Furthermore, different components of this block should be ablated in the model ablation study.
Similarly the paper does currently not contain a model ablation for 1) the use of the same encoder for audio and text instead of using a different audio model, 2) the effect of using pre-trained encoders. These should be added to validate the architectural choices.

Overall, I think the paper is interesting, and I would be happy to consider raising my score if the missing evaluations and ablations are added and convincing.



[A]: Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021

[B] Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022","* Is CLAP shown in Figure 5 referring to [6]? If not, different naming should be used. 

* I believe that some relevant prior works should be referenced, e.g. [C,D,E].

* What is the computational cost to train the framework and what kind of hardware was used for training?

[C]: Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022

​​[D]: Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022

[E]: Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023



* Typos:
l.62: lost -> lose

l.136 alignemnt -> alignment

l.181: space missing after “CLIP.”

l.232: We adopt AudioSet dataset -> We adopt the AudioSet dataset

Figure 4 is a table and should be labelled accordingly.

What does t in $128x100 t$ in l.240 refer to?",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1688672979293,,,NeurIPS.cc/2023/Conference,2NncD8AaFK,"['NeurIPS.cc/2023/Conference/Submission13079/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411412997,[],4,,,['everyone'],2NncD8AaFK,['NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz'],1688672979293,1702411412997,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

* Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021 (referenced as [A])
* Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022 (referenced as [B])
* Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022 (referenced as [C])
* Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022 (referenced as [D])
* Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023 (referenced as [E])

The reviewer believes that these prior works are relevant and should be referenced in the paper.",1,"2021, 2022, 2022, 2022, 2023",LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION 
cf6cugvVfI,"The paper introduces CoLLAT, an audio-language framework that makes use of a pre-trained language model. The framework is trained with a contrastive objective which encourages learning of fine-grained audio-text grounding. The paper presents very strong results for diverse downstream tasks, such as zero-shot audio classification, audio-image retrieval, and audio-guided image generation.","The proposed framework leverages a locked pre-trained language model (CLIP text encoder) as audio and text encoder. It learns to map the audio input to input tokens for the frozen text encoder, which retains the strong capabilities of the language model. The model is trained to encourage very fine-grained understanding of audio. It achieves very convincing results on various downstream tasks.","Given that this paper claims to yield fine-grained audio-to-text grounding, I believe that it would be good to show results for audio-text retrieval on CLOTHO, AudioCaps, and SoundDescs [A,B]. 

One of the main model contributions, the token interaction module, should be explained better and in more detail. For instance, it is not mentioned how many blocks are used (l.192: “multiple cross-attention-based blocks”). In addition, it would be good to add a detailed explanation on the denoising pipeline in this module. Furthermore, different components of this block should be ablated in the model ablation study.
Similarly the paper does currently not contain a model ablation for 1) the use of the same encoder for audio and text instead of using a different audio model, 2) the effect of using pre-trained encoders. These should be added to validate the architectural choices.

Overall, I think the paper is interesting, and I would be happy to consider raising my score if the missing evaluations and ablations are added and convincing.



[A]: Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021

[B] Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022","* Is CLAP shown in Figure 5 referring to [6]? If not, different naming should be used. 

* I believe that some relevant prior works should be referenced, e.g. [C,D,E].

* What is the computational cost to train the framework and what kind of hardware was used for training?

[C]: Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022

​​[D]: Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022

[E]: Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023



* Typos:
l.62: lost -> lose

l.136 alignemnt -> alignment

l.181: space missing after “CLIP.”

l.232: We adopt AudioSet dataset -> We adopt the AudioSet dataset

Figure 4 is a table and should be labelled accordingly.

What does t in $128x100 t$ in l.240 refer to?",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1688672979293,,,NeurIPS.cc/2023/Conference,2NncD8AaFK,"['NeurIPS.cc/2023/Conference/Submission13079/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411412997,[],4,,,['everyone'],2NncD8AaFK,['NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz'],1688672979293,1702411412997,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

* Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021 (referenced as [A])
* Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022 (referenced as [B])
* Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022 (referenced as [C])
* Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022 (referenced as [D])
* Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023 (referenced as [E])

The reviewer believes that these prior works are relevant and should be referenced in the paper.",1,"2021, 2022, 2022, 2022, 2023",Audio retrieval with wavtext5k and CLAP training 
cf6cugvVfI,"The paper introduces CoLLAT, an audio-language framework that makes use of a pre-trained language model. The framework is trained with a contrastive objective which encourages learning of fine-grained audio-text grounding. The paper presents very strong results for diverse downstream tasks, such as zero-shot audio classification, audio-image retrieval, and audio-guided image generation.","The proposed framework leverages a locked pre-trained language model (CLIP text encoder) as audio and text encoder. It learns to map the audio input to input tokens for the frozen text encoder, which retains the strong capabilities of the language model. The model is trained to encourage very fine-grained understanding of audio. It achieves very convincing results on various downstream tasks.","Given that this paper claims to yield fine-grained audio-to-text grounding, I believe that it would be good to show results for audio-text retrieval on CLOTHO, AudioCaps, and SoundDescs [A,B]. 

One of the main model contributions, the token interaction module, should be explained better and in more detail. For instance, it is not mentioned how many blocks are used (l.192: “multiple cross-attention-based blocks”). In addition, it would be good to add a detailed explanation on the denoising pipeline in this module. Furthermore, different components of this block should be ablated in the model ablation study.
Similarly the paper does currently not contain a model ablation for 1) the use of the same encoder for audio and text instead of using a different audio model, 2) the effect of using pre-trained encoders. These should be added to validate the architectural choices.

Overall, I think the paper is interesting, and I would be happy to consider raising my score if the missing evaluations and ablations are added and convincing.



[A]: Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021

[B] Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022","* Is CLAP shown in Figure 5 referring to [6]? If not, different naming should be used. 

* I believe that some relevant prior works should be referenced, e.g. [C,D,E].

* What is the computational cost to train the framework and what kind of hardware was used for training?

[C]: Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022

​​[D]: Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022

[E]: Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023



* Typos:
l.62: lost -> lose

l.136 alignemnt -> alignment

l.181: space missing after “CLIP.”

l.232: We adopt AudioSet dataset -> We adopt the AudioSet dataset

Figure 4 is a table and should be labelled accordingly.

What does t in $128x100 t$ in l.240 refer to?",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1688672979293,,,NeurIPS.cc/2023/Conference,2NncD8AaFK,"['NeurIPS.cc/2023/Conference/Submission13079/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411412997,[],4,,,['everyone'],2NncD8AaFK,['NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz'],1688672979293,1702411412997,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13079/Reviewer_swwz']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature, including:

* Oncescu et al.: Audio Retrieval with Natural Language Queries, INTERSPEECH 2021 (referenced as [A])
* Koepke et al.: Audio Retrieval with Natural Language Queries: A benchmark study, Transactions on Multimedia 2022 (referenced as [B])
* Wu et al: LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION, ICASSP 2022 (referenced as [C])
* Deshmukh et al.: Audio retrieval with wavtext5k and CLAP training, 2022 (referenced as [D])
* Wu et a.: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, ICASSP 2023 (referenced as [E])

The reviewer believes that these prior works are relevant and should be referenced in the paper.",1,"2021, 2022, 2022, 2022, 2023",Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation
0lS2hKb0OJ,"The authors introduce a new privacy notion called f-membership inference privacy (f-MIP), which relaxes strict Differential Privacy (DP) assumptions thereby promising better model utility. The paper proposes a theoretical analysis of membership inference attacks on DP-SGD based on trade-off curves (similar to f-DP) and introduces a family of f-MIP guarantees called µ-Gaussian Membership Inference Privacy (µ-GMIP)(similar to GDP).  The analysis follows a similar approach to the original DP-SGD analysis: first the privacy budget is derived for single step with subsampling which is then  composed over the training run.

The paper then verifies the theoretical analysis by introducing gradient attacks based on likelihood ratio tests.
The attack requires to know the underlying gradient distribution parameters. They present results for a single DP-SGD step with known parameters and the privacy guarantees seem tight. They also present results for estimated distribution parameters where the guarantees look loser.","- The derivations seem sound and follow the well regarded hypothesis testing interpretation in DP
- Investigating the tightness and potential relaxation of DP-SGD is an important problems and the authors make a solid contribution.
- The paper is clearly structured and easy to follow.","- It would be nice to see an investigation on the validity of the initial assumptions. The paper was motivated that DP is overly conservative since it also holds for pathological datasets such as empty datasets and singletons with an adversarial sample. While this is true, it has been recently shown that a simple canary insertion in an otherwise natural dataset may be sufficient to produce tight lower bounds for DP accountants [Nasr et al 2023].
- It would be great to extend the FPR and TPR ranges in the plots in figure 4 to smaller values. Ideally all the way to the first data point. These ranges capture very relevant adversary objectives where an adversary only cares about identifying one sample but that with high confidence.
- Minor:
  - Only asymptotic guarantees for composition. Recently, there has been significant progress in numerical composition of DP guarantees.


**References**

Nasr, Milad, et al. ""Tight Auditing of Differentially Private Machine Learning."" arXiv preprint arXiv:2302.07956 (2023).
","- Is it assumed that $\tau$ in the noise parameter includes the batch size? Algorithm 1 does not scale the noise by the batch size which is different to typical DP-SGD [Abadi et al 16].
- Figure 4a. Purchase and CIFAR10 seem to exceed the analytical bound for low FPRs. Is this because of the earlier mentioned assumption that the challenge points are sampled from the distribution which for low FPRs may be already distributional outliers?
- Are the techniques for composing DP guarantees numerically also applicable in this setting?","As discussed in weaknesses. I believe the discussion about the validity of the initial assumptions is limited.
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688682445610,,,NeurIPS.cc/2023/Conference,2NUFe4TZMS,"['NeurIPS.cc/2023/Conference/Submission7101/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411094065,[],2,,,['everyone'],2NUFe4TZMS,['NeurIPS.cc/2023/Conference/Submission7101/Reviewer_Uu8E'],1688682445610,1702411094065,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7101/Reviewer_Uu8E']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

Nasr, Milad, et al. ""Tight Auditing of Differentially Private Machine Learning."" arXiv preprint arXiv:2302.07956 (2023).

This is mentioned in the ""weaknesses"" section of the review, where the reviewer comments that the paper's motivation for relaxing Differential Privacy assumptions has been recently addressed in the referenced work by Nasr et al. (2023).",1,"2023, 2023, 2023",Tight Auditing of Differentially Private Machine Learning
60O4TUasRz,Authors came up with a privacy definition that is more relaxed than DP. It is called Gaussian Membership Inference Privacy (GMIP) which consists of a hypothesis testing which is supposed to decide whether a single instance is present in the training data. DP implies GMIP. The proposed privacy framework is applied with SGD and in experiments found to be much better than DP in terms of utility.,"* There is a need to come up with some privacy definition that is more practical than DP.
* Research direction is promising.","* I believe that standard notions from statistical hypothesis testing are reinvented, and the results seems not so surprising taken into account results already published in testing. I would suggest to reuse those results (if it is possible). Nevertheless, please address my questions!","* Definition 4.1 is basically the most powerfull test. $\mathcal{E}$ is the significance and $\beta$ is the power function, or more concretely 1 - power function?
* Regarding Theorem 4.1: according to the Neyman-Pearson fundamental lemma the risk function is always convex when the null and alternative consist of a single distribution. And the testing problem presented in (3) is like that. See Lehamnn-Romano: Testing Statistical Hypotheses, Sec 3.2. 
* Testing are applied in many rounds and i guess only union bound is used over the individual SGD steps. Some sequential hypthesis testing might be worth to consider?
","I believe that this research direction is interesting and promising. However, convexity of risk function seems not novel observation which is the key of having most uniformly powerful test. And this is what the paper relies on. I do not understand why to stick to Gaussion distribution, since any distribution from the exponential family can be used in a very similar way. Furthermore, FNR are controlled in each SGD step independently from each other however sequential testing approaches might applied here which would make this paper much more interesting. So technical contribution is somewhat limited.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1688997399306,,,NeurIPS.cc/2023/Conference,2NUFe4TZMS,"['NeurIPS.cc/2023/Conference/Submission7101/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411093893,[],4,,,['everyone'],2NUFe4TZMS,['NeurIPS.cc/2023/Conference/Submission7101/Reviewer_qYbt'],1688997399306,1702411093893,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7101/Reviewer_qYbt']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions ""Lehamnn-Romano: Testing Statistical Hypotheses, Sec 3.2"" as a reference that the authors could use to support their results, particularly with regards to the convexity of the risk function.",1,,Testing Statistical Hypotheses
e2mNhRqm5E,"The authors present a unified framework for solving deterministic and stochastic physical dynamical systems on high-dimensional mesh space. Instead of updating values at each discretized mesh, the paper introduces an approach that evolves states in a low dimensional latent space through encoding states and physical parameters. This encoding is done by a message passing graph neural network and multi-head attention (MHA) model, which encodes physics parameters into a conditional vector. The encoded state is then evolved using a conditional normalizing flow, dependent on the encoded conditional vector. The effectiveness of this method is shown through experiments, in which it outperforms other mesh-based ML models in terms of accumulation error in deterministic problems, with applications in stochastic problems also being demonstrated.","The proposed method is simple and applicable to wide range of PDE-simulation problems with discretized domain. Wide variety of experiments are conducted, and both quantitative and qualitative evaluations are provided.","About the novelty, it is still unclear that what components of the proposed model enables solving both deterministic and stochastic  systems efficiently. Are there any limitations that prevent existing models from being applied to both systems?

It is concerning that the proposed method take all the previous (latent) vectors as input to predict the next state, as opposed to fixed small number of input vectors of MeshGraphNets reported in Section 5, that may result in unfair advantages over other methods. Also, although in Table 1 the reconstruction error of PbGMR-GMUS are compared against GMR-GMUS, it is still unclear how the proposed model would compare against an ablation model whose encoder is GMR-GMUS.

The proposed model is compared against baselines which evolve states in original space. There is a possibly missing reference [1], and the motivation of using latent evolution is similar to the presented one. How does this work compare against it? Is the proposed method also applicable to grid space?

[1] Tailin Wu, Takashi Maruyama, and Jure Leskovec. Learning to accelerate partial differential equations via latent global evolution (NeurIPS 2022)


**Minor comments**

In Appendix A.10, Figure 11 is cited, but seemingly wrong. ","Please have a look at the weakness mentioned in the strength and weakness section and address these. Overall the idea seems interesting, the authors need to substantiate their claims in light of existing literature and possibly more experiments.
","Yes, the limitations are discussed in the appendix. ",['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,4 excellent,1688524312034,,,NeurIPS.cc/2023/Conference,2JtwuJtoa0,"['NeurIPS.cc/2023/Conference/Submission9760/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411250755,[],3,,,['everyone'],2JtwuJtoa0,['NeurIPS.cc/2023/Conference/Submission9760/Reviewer_YdeK'],1688524312034,1702411250755,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9760/Reviewer_YdeK']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, which is:

Tailin Wu, Takashi Maruyama, and Jure Leskovec. Learning to accelerate partial differential equations via latent global evolution (NeurIPS 2022)

The reviewer mentions that the motivation of using latent evolution is similar to the presented one and asks how the proposed method compares against this work, indicating that the authors should consider citing and comparing their work to this reference.",1,2022,Learning to accelerate partial differential equations via latent global evolution
yhCx7w1rbz,The authors proposed a Non-isotropic Gaussian Diffusion Model for the task of image-image translation and image editing. The NGDM is achieved by adding different noise variances to different image pixels so as to control the regions to edit. Experimental results have demonstrated the state-of-the-art quality of the proposed paper.,"- The paper proposed a practical method for image-image translation/editing by utilizing the off-the-shelf diffusion model. It seems like the proposed method is easy to re-implement and has demonstrated satisfactory results.

- The presentation of the paper is good.","- Why the proposed method would choose to implement the non-isotropic diffusion process by controlling each pixel's denoising steps? I can come up with one alternative quickly: at each iteration, we can add different levels of noise to different pixels (by also using the input-dependent weight matrix), and denoise as usual. It is a bit strange to set different denoise time steps for different pixels.

- I would like to see comparisons with the recent paper ""Delta Denoising Score"", which also targets the task of controllable image editing.
",See above,See above,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1690614965620,,,NeurIPS.cc/2023/Conference,2Ibp83esmb,"['NeurIPS.cc/2023/Conference/Submission6113/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411046025,[],6,,,['everyone'],2Ibp83esmb,['NeurIPS.cc/2023/Conference/Submission6113/Reviewer_xHHG'],1690614965620,1702411046025,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission6113/Reviewer_xHHG']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely the recent paper ""Delta Denoising Score"", which also targets the task of controllable image editing, for comparison purposes.",1,,Delta Denoising Score
eJQXxvPVMP,"This work addresses the situation in which data and model equivariance does not match exactly. It extends previous work from Wang et al., by proposing a pointwise version of their definition of correct, incorrect and extrinsic equivariance. The usefulness of these new concepts are demonstrated in three new performance lower-bounds derived from classification and regression problems. A series of examples and experiments are presented to illustrate the new proposed notions, confirm the lower-bounds derived and show that they usually seem tight in practice.","### Originality
Although it builds heavily on previous work from Wang [52], the paper presents novel theoretical concepts and results, as well as new experiments supporting them.

### Clarity
Overall, the paper is well written and structured.

### Quality
The paper presents new theoretical results with their corresponding proofs, which look sound to be (disclaimer: I have some familiarity with group theory, but I am not an expert). It also presents interesting results in a large range of experiments. The latter are all rather small/toy, but are still quite convincing IMO.
","### Clarity

Minor:

- I think there is a mistake in the xlabel of fig 7b. Shouldn't it be ""incorrect - correct""? Same for figure 7a: should be ""correct - extrinsic"" I guess, since when x=1 you have the highest INV model performance, which should correspond to c=1 and e=0 according to the text.
- L.363: I think you wanted to reference fig.9, not 8.

### Clarity and Quality

1. I think the definition of $p$ could be clarified. It is introduced in line 92 as the “probability density function of the domain”, so, at first reading, I thought p(x) was the “true” underlying population distribution from which both training and test examples are samples. This seems to be confirmed by line 387 in the conclusion: “our theoretical lower bounds require domain knowledge like the density function over the domain”.  But in section 6, it seems that you define extrinsic equivariance with respect to the actual examples in the training set, which is not the same thing: “F_E corresponds to an extrinsically equivariant class for **this data**” l.272

2. I also have other few questions regarding the example presented in section 6. First of all, does the data from figure 5 represent training or test data? Is the data $S$ the whole support of $p(x)$? What is the true labeling function $f$ in this example?
These elements seem important to conclude. The reason I ask is that if $f$ is indeed the “exclusive or” on (x,y) coordinates (i.e. it is indeed C2-invariant) and if the examples from figure 5a are just the training data but the test data can go outside (e.g. their symmetric elements in fig 5b could be the test set), then despite the 0% error rate of the unconstrained linear model on the training data, it would learn an incorrect labeling function and its test performance would be 50%, while the invariant model would still have a 25% error rate, which would be better.

3. If $p$ really denotes the population distribution, I wonder whether we can talk about $p$ independently of $f$ and vice-versa. From your figures 1 and 2, it seems that $f$ can be defined outside of the support of $p(x)$, and I wonder whether this makes sense. For example, if $f$ is the labeling function in a digit classification problem, what should be its output for an image of the digit “9” rotated by 90 degrees? Should it be 9 or 6?

### Originality:

Minor:

4. In the related work, some references to class-specific and instance-specific automatic data augmentation works are missing ([1, 2, 3] for example), while they are strongly related to the idea of pointwise invariance proposed in the paper.

[1] https://arxiv.org/abs/2106.13695

[2] https://arxiv.org/abs/1510.02795

[3] https://arxiv.org/abs/2206.00051",See 3 questions in the weaknesses section above.,"Yes, the authors mention one limitation of their work, which is the need to know $p$, which is never the case in practice.  ",['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1690303255556,,,NeurIPS.cc/2023/Conference,2FMJtNDLeE,"['NeurIPS.cc/2023/Conference/Submission3933/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410926584,[],6,,,['everyone'],2FMJtNDLeE,['NeurIPS.cc/2023/Conference/Submission3933/Reviewer_FMJP'],1690303255556,1702410926584,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3933/Reviewer_FMJP']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature related to class-specific and instance-specific automatic data augmentation works, specifically:

[1] https://arxiv.org/abs/2106.13695
[2] https://arxiv.org/abs/1510.02795
[3] https://arxiv.org/abs/2206.00051

These references are mentioned in the ""Originality"" section of the weaknesses, indicating that the reviewer believes they are relevant to the idea of pointwise invariance proposed in the paper and should be included in the related work section.",1,"2021, 2015, 2022",Pointwise Invariant Transformations for Point Clouds 
eJQXxvPVMP,"This work addresses the situation in which data and model equivariance does not match exactly. It extends previous work from Wang et al., by proposing a pointwise version of their definition of correct, incorrect and extrinsic equivariance. The usefulness of these new concepts are demonstrated in three new performance lower-bounds derived from classification and regression problems. A series of examples and experiments are presented to illustrate the new proposed notions, confirm the lower-bounds derived and show that they usually seem tight in practice.","### Originality
Although it builds heavily on previous work from Wang [52], the paper presents novel theoretical concepts and results, as well as new experiments supporting them.

### Clarity
Overall, the paper is well written and structured.

### Quality
The paper presents new theoretical results with their corresponding proofs, which look sound to be (disclaimer: I have some familiarity with group theory, but I am not an expert). It also presents interesting results in a large range of experiments. The latter are all rather small/toy, but are still quite convincing IMO.
","### Clarity

Minor:

- I think there is a mistake in the xlabel of fig 7b. Shouldn't it be ""incorrect - correct""? Same for figure 7a: should be ""correct - extrinsic"" I guess, since when x=1 you have the highest INV model performance, which should correspond to c=1 and e=0 according to the text.
- L.363: I think you wanted to reference fig.9, not 8.

### Clarity and Quality

1. I think the definition of $p$ could be clarified. It is introduced in line 92 as the “probability density function of the domain”, so, at first reading, I thought p(x) was the “true” underlying population distribution from which both training and test examples are samples. This seems to be confirmed by line 387 in the conclusion: “our theoretical lower bounds require domain knowledge like the density function over the domain”.  But in section 6, it seems that you define extrinsic equivariance with respect to the actual examples in the training set, which is not the same thing: “F_E corresponds to an extrinsically equivariant class for **this data**” l.272

2. I also have other few questions regarding the example presented in section 6. First of all, does the data from figure 5 represent training or test data? Is the data $S$ the whole support of $p(x)$? What is the true labeling function $f$ in this example?
These elements seem important to conclude. The reason I ask is that if $f$ is indeed the “exclusive or” on (x,y) coordinates (i.e. it is indeed C2-invariant) and if the examples from figure 5a are just the training data but the test data can go outside (e.g. their symmetric elements in fig 5b could be the test set), then despite the 0% error rate of the unconstrained linear model on the training data, it would learn an incorrect labeling function and its test performance would be 50%, while the invariant model would still have a 25% error rate, which would be better.

3. If $p$ really denotes the population distribution, I wonder whether we can talk about $p$ independently of $f$ and vice-versa. From your figures 1 and 2, it seems that $f$ can be defined outside of the support of $p(x)$, and I wonder whether this makes sense. For example, if $f$ is the labeling function in a digit classification problem, what should be its output for an image of the digit “9” rotated by 90 degrees? Should it be 9 or 6?

### Originality:

Minor:

4. In the related work, some references to class-specific and instance-specific automatic data augmentation works are missing ([1, 2, 3] for example), while they are strongly related to the idea of pointwise invariance proposed in the paper.

[1] https://arxiv.org/abs/2106.13695

[2] https://arxiv.org/abs/1510.02795

[3] https://arxiv.org/abs/2206.00051",See 3 questions in the weaknesses section above.,"Yes, the authors mention one limitation of their work, which is the need to know $p$, which is never the case in practice.  ",['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1690303255556,,,NeurIPS.cc/2023/Conference,2FMJtNDLeE,"['NeurIPS.cc/2023/Conference/Submission3933/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410926584,[],6,,,['everyone'],2FMJtNDLeE,['NeurIPS.cc/2023/Conference/Submission3933/Reviewer_FMJP'],1690303255556,1702410926584,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3933/Reviewer_FMJP']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature related to class-specific and instance-specific automatic data augmentation works, specifically:

[1] https://arxiv.org/abs/2106.13695
[2] https://arxiv.org/abs/1510.02795
[3] https://arxiv.org/abs/2206.00051

These references are mentioned in the ""Originality"" section of the weaknesses, indicating that the reviewer believes they are relevant to the idea of pointwise invariance proposed in the paper and should be included in the related work section.",1,"2021, 2015, 2022",Learning to Compose Domain-Specific Transformations for Data Augmentation 
eJQXxvPVMP,"This work addresses the situation in which data and model equivariance does not match exactly. It extends previous work from Wang et al., by proposing a pointwise version of their definition of correct, incorrect and extrinsic equivariance. The usefulness of these new concepts are demonstrated in three new performance lower-bounds derived from classification and regression problems. A series of examples and experiments are presented to illustrate the new proposed notions, confirm the lower-bounds derived and show that they usually seem tight in practice.","### Originality
Although it builds heavily on previous work from Wang [52], the paper presents novel theoretical concepts and results, as well as new experiments supporting them.

### Clarity
Overall, the paper is well written and structured.

### Quality
The paper presents new theoretical results with their corresponding proofs, which look sound to be (disclaimer: I have some familiarity with group theory, but I am not an expert). It also presents interesting results in a large range of experiments. The latter are all rather small/toy, but are still quite convincing IMO.
","### Clarity

Minor:

- I think there is a mistake in the xlabel of fig 7b. Shouldn't it be ""incorrect - correct""? Same for figure 7a: should be ""correct - extrinsic"" I guess, since when x=1 you have the highest INV model performance, which should correspond to c=1 and e=0 according to the text.
- L.363: I think you wanted to reference fig.9, not 8.

### Clarity and Quality

1. I think the definition of $p$ could be clarified. It is introduced in line 92 as the “probability density function of the domain”, so, at first reading, I thought p(x) was the “true” underlying population distribution from which both training and test examples are samples. This seems to be confirmed by line 387 in the conclusion: “our theoretical lower bounds require domain knowledge like the density function over the domain”.  But in section 6, it seems that you define extrinsic equivariance with respect to the actual examples in the training set, which is not the same thing: “F_E corresponds to an extrinsically equivariant class for **this data**” l.272

2. I also have other few questions regarding the example presented in section 6. First of all, does the data from figure 5 represent training or test data? Is the data $S$ the whole support of $p(x)$? What is the true labeling function $f$ in this example?
These elements seem important to conclude. The reason I ask is that if $f$ is indeed the “exclusive or” on (x,y) coordinates (i.e. it is indeed C2-invariant) and if the examples from figure 5a are just the training data but the test data can go outside (e.g. their symmetric elements in fig 5b could be the test set), then despite the 0% error rate of the unconstrained linear model on the training data, it would learn an incorrect labeling function and its test performance would be 50%, while the invariant model would still have a 25% error rate, which would be better.

3. If $p$ really denotes the population distribution, I wonder whether we can talk about $p$ independently of $f$ and vice-versa. From your figures 1 and 2, it seems that $f$ can be defined outside of the support of $p(x)$, and I wonder whether this makes sense. For example, if $f$ is the labeling function in a digit classification problem, what should be its output for an image of the digit “9” rotated by 90 degrees? Should it be 9 or 6?

### Originality:

Minor:

4. In the related work, some references to class-specific and instance-specific automatic data augmentation works are missing ([1, 2, 3] for example), while they are strongly related to the idea of pointwise invariance proposed in the paper.

[1] https://arxiv.org/abs/2106.13695

[2] https://arxiv.org/abs/1510.02795

[3] https://arxiv.org/abs/2206.00051",See 3 questions in the weaknesses section above.,"Yes, the authors mention one limitation of their work, which is the need to know $p$, which is never the case in practice.  ",['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1690303255556,,,NeurIPS.cc/2023/Conference,2FMJtNDLeE,"['NeurIPS.cc/2023/Conference/Submission3933/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410926584,[],6,,,['everyone'],2FMJtNDLeE,['NeurIPS.cc/2023/Conference/Submission3933/Reviewer_FMJP'],1690303255556,1702410926584,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3933/Reviewer_FMJP']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature related to class-specific and instance-specific automatic data augmentation works, specifically:

[1] https://arxiv.org/abs/2106.13695
[2] https://arxiv.org/abs/1510.02795
[3] https://arxiv.org/abs/2206.00051

These references are mentioned in the ""Originality"" section of the weaknesses, indicating that the reviewer believes they are relevant to the idea of pointwise invariance proposed in the paper and should be included in the related work section.",1,"2021, 2015, 2022",Instance-Dependent Augmentations for Robust Point Cloud Learning
6n2PtMbqIU,This paper proposes a 3D CNN based video stylization method which explicitly disentangles motion and appearance and adopts multi-phrase training. Experiments show that the method achieves high quality results.,"The proposed 3D CNN based framework and multi-phrase training is novel and effective, the paper also expand the AdaIN in time dimension to AdaIN3D. The quality of stylization results is also good.","I think the main weakness lies in the experiments.
1. In ablation study, the authors only showed and described the results but didn't not analyze the reason
2. In the qualitative results, this paper compares with only a few previous methods, while some related works should also be discussed. for example, 
Kotovenko D, Sanakoyeu A, Lang S, et al. Content and style disentanglement for artistic style transfer[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 4422-4431.
Kotovenko D, Sanakoyeu A, Ma P, et al. A content transformation block for image style transfer[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 10032-10041. 
Both papers showed high-quality video stylization results
3. For tasks where the quality is largely depended on subjective assessment, user study is a common criterion. For this paper, I feel the comparison is not sufficient and a user study is needed
4. In quantitative comparison, the proposed method doesn‘t obviously outperform previous method. 
5. Lack a comparison to single image stylization with optical flow motion compensation

","1. Why are only three previous methods included in the qualitative and quantitative comparison? There have been quantities of style transfer papers every year and many of them are about video stylization.
2. Why is user-study not included in the experiments?
3. What is the advantage compared to adding optical flow motion compensation to single frame style transfer method?
","This paper lacks several important parts:
1. The discussion and analysis of each part of the method are not included in the paper
2. User study is missing in experiments
3. More previous methods should be included in comparison
4. It would be better if more results of diverse scenarios could be shown. ",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1687792754191,,,NeurIPS.cc/2023/Conference,2EiqizElGO,"['NeurIPS.cc/2023/Conference/Submission7842/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411133303,[],1,,,['everyone'],2EiqizElGO,['NeurIPS.cc/2023/Conference/Submission7842/Reviewer_qZ5E'],1687792754191,1702411133303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7842/Reviewer_qZ5E']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Kotovenko D, Sanakoyeu A, Lang S, et al. Content and style disentanglement for artistic style transfer[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 4422-4431.
2. Kotovenko D, Sanakoyeu A, Ma P, et al. A content transformation block for image style transfer[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 10032-10041.

The reviewer mentions that these papers showed high-quality video stylization results and should be discussed in the comparison.",1,"2019, 2019",Content and style disentanglement for artistic style transfer 
6n2PtMbqIU,This paper proposes a 3D CNN based video stylization method which explicitly disentangles motion and appearance and adopts multi-phrase training. Experiments show that the method achieves high quality results.,"The proposed 3D CNN based framework and multi-phrase training is novel and effective, the paper also expand the AdaIN in time dimension to AdaIN3D. The quality of stylization results is also good.","I think the main weakness lies in the experiments.
1. In ablation study, the authors only showed and described the results but didn't not analyze the reason
2. In the qualitative results, this paper compares with only a few previous methods, while some related works should also be discussed. for example, 
Kotovenko D, Sanakoyeu A, Lang S, et al. Content and style disentanglement for artistic style transfer[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 4422-4431.
Kotovenko D, Sanakoyeu A, Ma P, et al. A content transformation block for image style transfer[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 10032-10041. 
Both papers showed high-quality video stylization results
3. For tasks where the quality is largely depended on subjective assessment, user study is a common criterion. For this paper, I feel the comparison is not sufficient and a user study is needed
4. In quantitative comparison, the proposed method doesn‘t obviously outperform previous method. 
5. Lack a comparison to single image stylization with optical flow motion compensation

","1. Why are only three previous methods included in the qualitative and quantitative comparison? There have been quantities of style transfer papers every year and many of them are about video stylization.
2. Why is user-study not included in the experiments?
3. What is the advantage compared to adding optical flow motion compensation to single frame style transfer method?
","This paper lacks several important parts:
1. The discussion and analysis of each part of the method are not included in the paper
2. User study is missing in experiments
3. More previous methods should be included in comparison
4. It would be better if more results of diverse scenarios could be shown. ",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1687792754191,,,NeurIPS.cc/2023/Conference,2EiqizElGO,"['NeurIPS.cc/2023/Conference/Submission7842/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411133303,[],1,,,['everyone'],2EiqizElGO,['NeurIPS.cc/2023/Conference/Submission7842/Reviewer_qZ5E'],1687792754191,1702411133303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7842/Reviewer_qZ5E']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Kotovenko D, Sanakoyeu A, Lang S, et al. Content and style disentanglement for artistic style transfer[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 4422-4431.
2. Kotovenko D, Sanakoyeu A, Ma P, et al. A content transformation block for image style transfer[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 10032-10041.

The reviewer mentions that these papers showed high-quality video stylization results and should be discussed in the comparison.",1,"2019, 2019",A content transformation block for image style transfer
rFu4RAkY66,"The paper presents uniform Bayesian regret bounds for Thompson Sampling by utilizing a uniform bound of information ratio and specific bounds of the Kolmogorov dimension in different settings. 
","1. The paper presents a uniform Bayesian regret bound for Thompson Sampling which yields results in a variety of settings, improving upon previous approaches in some scenarios.
2. The authors incorporate a comprehensive discussion with previous works which helps to understand the contribution of the proposed bounds.
","1. Potential overclaim: in the introduction, the authors state that they first define Bayesian RL with time inhomogeneous settings, which might be an overclaim since there are also previous works discussing this setting like [1].

[1]Variational Bayesian Reinforcement Learning with Regret Bounds. Brendan O'Donoghue

2. The presentation of the paper would benefit from a table that includes all the results discussed in the paper for a comprehensive comparison.","Why is time inhomogeneous specially highlighted in this work? If we consider time also as a part of the state observation, it would be homogeneous (i.e., share the same model across all timesteps). With that being considered, is it possible just to extend the time inhomogeneous bounds to this setting, and is the proposed bound also better than those?
",,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",,Yes,2 fair,3 good,3 good,1688678734481,,,NeurIPS.cc/2023/Conference,2EVTB1idyR,"['NeurIPS.cc/2023/Conference/Submission10911/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411307385,[],3,,,['everyone'],2EVTB1idyR,['NeurIPS.cc/2023/Conference/Submission10911/Reviewer_4ohk'],1688678734481,1702411307385,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10911/Reviewer_4ohk']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

[1] ""Variational Bayesian Reinforcement Learning with Regret Bounds"" by Brendan O'Donoghue.

This is mentioned in the ""weaknesses"" section, where the reviewer points out a potential overclaim by the authors and suggests that they should acknowledge previous work on Bayesian RL with time-inhomogeneous settings.",1,,Variational Bayesian Reinforcement Learning with Regret Bounds
Now9UjJOmV,"The authors propose the novel Bayesian regret analysis for posterior sampling for reinforcement learning algorithm. The proposed regret bounds are applicable in a large variety of different RL settings, such as tabular, linear and finite mixture MDPs.","- The novel analysis for posterior sampling algorithm in the setting of Bayesian regret;
- The presented result holds not only in the setting of tabular MDPs but also in linear and finite mixture MDPs.","- The weak notion of Bayesian regret is the main weakness of the presented result. Currently, there exists near-optimal results in posterior-sampling based algorithms in the frequentist setting (see section Questions for precise references).
- The computational side of the presented algorithm was not discusses.
- The upper bound in linear setup seems to be a contradiction with established lower bound in the setup of linear contextual bandits (see reference below for example). This effect requires additional explanations why is it possible in the presented setting.
- Lattimore, Tor, and Csaba Szepesvári. *Bandit algorithms*. Cambridge University Press, 2020.","- Is there any results in literature where this type of dimension were called “Kolmogorov”? Seems that this definition is a just usual covering dimension (at least in the presented setup of $\ell_1$ distance).
- Is there examples where $\lambda$ is much smaller than $H$?
- What is the limitation to show this analysis for a more general class of MDPs such as MDPs with a finite Eluder dimension?
- Where is topological structure of $S$ and $A$ were used during the proofs? What is the topological structure of them?
- The missed references of frequentist regret bounds for TS-based exploration in tabular and linear MDPs:
    - Zanette, Andrea, et al. ""Frequentist regret bounds for randomized least-squares value iteration."" *International Conference on Artificial Intelligence and Statistics*. PMLR, 2020.
    - Agrawal, Shipra, and Randy Jia. ""Optimistic posterior sampling for reinforcement learning: worst-case regret bounds."" *Advances in Neural Information Processing Systems* 30 (2017).
    - Tiapkin, Daniil, et al. ""Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees."" *Advances in Neural Information Processing Systems* 35 (2022): 10737-10751.
    - Agrawal, Priyank, Jinglin Chen, and Nan Jiang. ""Improved worst-case regret bounds for randomized least-squares value iteration."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 35. No. 8. 2021.
- Line 83: In [32] they consider not episodic setup. The state-of-the-art result in the episodic setup were presented in [4].","The paper presented the theoretical research on Bayesian regret for posterior-sampling algorithms for reinforcement learning, thus it does not require discussion of ethical limitations.",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,2 fair,1689240226028,,,NeurIPS.cc/2023/Conference,2EVTB1idyR,"['NeurIPS.cc/2023/Conference/Submission10911/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411307290,[],4,,,['everyone'],2EVTB1idyR,['NeurIPS.cc/2023/Conference/Submission10911/Reviewer_pUAk'],1689240226028,1702411307290,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10911/Reviewer_pUAk']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that are relevant to the topic of frequentist regret bounds for TS-based exploration in tabular and linear MDPs, including:

* Zanette, Andrea, et al. ""Frequentist regret bounds for randomized least-squares value iteration."" 
* Agrawal, Shipra, and Randy Jia. ""Optimistic posterior sampling for reinforcement learning: worst-case regret bounds."" 
* Tiapkin, Daniil, et al. ""Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees."" 
* Agrawal, Priyank, Jinglin Chen, and Nan Jiang. ""Improved worst-case regret bounds for randomized least-squares value iteration.""

The reviewer also mentions a book by Lattimore, Tor, and Csaba Szepesvári, ""Bandit algorithms"", which is relevant to the topic of linear contextual bandits. Additionally, the reviewer mentions that the state-of-the-art result in the episodic setup was presented in [4], implying that the authors should be aware of this reference.",1,,Frequentist regret bounds for randomized least-squares value iteration
Now9UjJOmV,"The authors propose the novel Bayesian regret analysis for posterior sampling for reinforcement learning algorithm. The proposed regret bounds are applicable in a large variety of different RL settings, such as tabular, linear and finite mixture MDPs.","- The novel analysis for posterior sampling algorithm in the setting of Bayesian regret;
- The presented result holds not only in the setting of tabular MDPs but also in linear and finite mixture MDPs.","- The weak notion of Bayesian regret is the main weakness of the presented result. Currently, there exists near-optimal results in posterior-sampling based algorithms in the frequentist setting (see section Questions for precise references).
- The computational side of the presented algorithm was not discusses.
- The upper bound in linear setup seems to be a contradiction with established lower bound in the setup of linear contextual bandits (see reference below for example). This effect requires additional explanations why is it possible in the presented setting.
- Lattimore, Tor, and Csaba Szepesvári. *Bandit algorithms*. Cambridge University Press, 2020.","- Is there any results in literature where this type of dimension were called “Kolmogorov”? Seems that this definition is a just usual covering dimension (at least in the presented setup of $\ell_1$ distance).
- Is there examples where $\lambda$ is much smaller than $H$?
- What is the limitation to show this analysis for a more general class of MDPs such as MDPs with a finite Eluder dimension?
- Where is topological structure of $S$ and $A$ were used during the proofs? What is the topological structure of them?
- The missed references of frequentist regret bounds for TS-based exploration in tabular and linear MDPs:
    - Zanette, Andrea, et al. ""Frequentist regret bounds for randomized least-squares value iteration."" *International Conference on Artificial Intelligence and Statistics*. PMLR, 2020.
    - Agrawal, Shipra, and Randy Jia. ""Optimistic posterior sampling for reinforcement learning: worst-case regret bounds."" *Advances in Neural Information Processing Systems* 30 (2017).
    - Tiapkin, Daniil, et al. ""Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees."" *Advances in Neural Information Processing Systems* 35 (2022): 10737-10751.
    - Agrawal, Priyank, Jinglin Chen, and Nan Jiang. ""Improved worst-case regret bounds for randomized least-squares value iteration."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 35. No. 8. 2021.
- Line 83: In [32] they consider not episodic setup. The state-of-the-art result in the episodic setup were presented in [4].","The paper presented the theoretical research on Bayesian regret for posterior-sampling algorithms for reinforcement learning, thus it does not require discussion of ethical limitations.",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,2 fair,1689240226028,,,NeurIPS.cc/2023/Conference,2EVTB1idyR,"['NeurIPS.cc/2023/Conference/Submission10911/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411307290,[],4,,,['everyone'],2EVTB1idyR,['NeurIPS.cc/2023/Conference/Submission10911/Reviewer_pUAk'],1689240226028,1702411307290,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10911/Reviewer_pUAk']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that are relevant to the topic of frequentist regret bounds for TS-based exploration in tabular and linear MDPs, including:

* Zanette, Andrea, et al. ""Frequentist regret bounds for randomized least-squares value iteration."" 
* Agrawal, Shipra, and Randy Jia. ""Optimistic posterior sampling for reinforcement learning: worst-case regret bounds."" 
* Tiapkin, Daniil, et al. ""Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees."" 
* Agrawal, Priyank, Jinglin Chen, and Nan Jiang. ""Improved worst-case regret bounds for randomized least-squares value iteration.""

The reviewer also mentions a book by Lattimore, Tor, and Csaba Szepesvári, ""Bandit algorithms"", which is relevant to the topic of linear contextual bandits. Additionally, the reviewer mentions that the state-of-the-art result in the episodic setup was presented in [4], implying that the authors should be aware of this reference.",1,,Optimistic posterior sampling for reinforcement learning worst-case regret bounds
Now9UjJOmV,"The authors propose the novel Bayesian regret analysis for posterior sampling for reinforcement learning algorithm. The proposed regret bounds are applicable in a large variety of different RL settings, such as tabular, linear and finite mixture MDPs.","- The novel analysis for posterior sampling algorithm in the setting of Bayesian regret;
- The presented result holds not only in the setting of tabular MDPs but also in linear and finite mixture MDPs.","- The weak notion of Bayesian regret is the main weakness of the presented result. Currently, there exists near-optimal results in posterior-sampling based algorithms in the frequentist setting (see section Questions for precise references).
- The computational side of the presented algorithm was not discusses.
- The upper bound in linear setup seems to be a contradiction with established lower bound in the setup of linear contextual bandits (see reference below for example). This effect requires additional explanations why is it possible in the presented setting.
- Lattimore, Tor, and Csaba Szepesvári. *Bandit algorithms*. Cambridge University Press, 2020.","- Is there any results in literature where this type of dimension were called “Kolmogorov”? Seems that this definition is a just usual covering dimension (at least in the presented setup of $\ell_1$ distance).
- Is there examples where $\lambda$ is much smaller than $H$?
- What is the limitation to show this analysis for a more general class of MDPs such as MDPs with a finite Eluder dimension?
- Where is topological structure of $S$ and $A$ were used during the proofs? What is the topological structure of them?
- The missed references of frequentist regret bounds for TS-based exploration in tabular and linear MDPs:
    - Zanette, Andrea, et al. ""Frequentist regret bounds for randomized least-squares value iteration."" *International Conference on Artificial Intelligence and Statistics*. PMLR, 2020.
    - Agrawal, Shipra, and Randy Jia. ""Optimistic posterior sampling for reinforcement learning: worst-case regret bounds."" *Advances in Neural Information Processing Systems* 30 (2017).
    - Tiapkin, Daniil, et al. ""Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees."" *Advances in Neural Information Processing Systems* 35 (2022): 10737-10751.
    - Agrawal, Priyank, Jinglin Chen, and Nan Jiang. ""Improved worst-case regret bounds for randomized least-squares value iteration."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 35. No. 8. 2021.
- Line 83: In [32] they consider not episodic setup. The state-of-the-art result in the episodic setup were presented in [4].","The paper presented the theoretical research on Bayesian regret for posterior-sampling algorithms for reinforcement learning, thus it does not require discussion of ethical limitations.",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,2 fair,1689240226028,,,NeurIPS.cc/2023/Conference,2EVTB1idyR,"['NeurIPS.cc/2023/Conference/Submission10911/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411307290,[],4,,,['everyone'],2EVTB1idyR,['NeurIPS.cc/2023/Conference/Submission10911/Reviewer_pUAk'],1689240226028,1702411307290,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10911/Reviewer_pUAk']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that are relevant to the topic of frequentist regret bounds for TS-based exploration in tabular and linear MDPs, including:

* Zanette, Andrea, et al. ""Frequentist regret bounds for randomized least-squares value iteration."" 
* Agrawal, Shipra, and Randy Jia. ""Optimistic posterior sampling for reinforcement learning: worst-case regret bounds."" 
* Tiapkin, Daniil, et al. ""Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees."" 
* Agrawal, Priyank, Jinglin Chen, and Nan Jiang. ""Improved worst-case regret bounds for randomized least-squares value iteration.""

The reviewer also mentions a book by Lattimore, Tor, and Csaba Szepesvári, ""Bandit algorithms"", which is relevant to the topic of linear contextual bandits. Additionally, the reviewer mentions that the state-of-the-art result in the episodic setup was presented in [4], implying that the authors should be aware of this reference.",1,,Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees
Now9UjJOmV,"The authors propose the novel Bayesian regret analysis for posterior sampling for reinforcement learning algorithm. The proposed regret bounds are applicable in a large variety of different RL settings, such as tabular, linear and finite mixture MDPs.","- The novel analysis for posterior sampling algorithm in the setting of Bayesian regret;
- The presented result holds not only in the setting of tabular MDPs but also in linear and finite mixture MDPs.","- The weak notion of Bayesian regret is the main weakness of the presented result. Currently, there exists near-optimal results in posterior-sampling based algorithms in the frequentist setting (see section Questions for precise references).
- The computational side of the presented algorithm was not discusses.
- The upper bound in linear setup seems to be a contradiction with established lower bound in the setup of linear contextual bandits (see reference below for example). This effect requires additional explanations why is it possible in the presented setting.
- Lattimore, Tor, and Csaba Szepesvári. *Bandit algorithms*. Cambridge University Press, 2020.","- Is there any results in literature where this type of dimension were called “Kolmogorov”? Seems that this definition is a just usual covering dimension (at least in the presented setup of $\ell_1$ distance).
- Is there examples where $\lambda$ is much smaller than $H$?
- What is the limitation to show this analysis for a more general class of MDPs such as MDPs with a finite Eluder dimension?
- Where is topological structure of $S$ and $A$ were used during the proofs? What is the topological structure of them?
- The missed references of frequentist regret bounds for TS-based exploration in tabular and linear MDPs:
    - Zanette, Andrea, et al. ""Frequentist regret bounds for randomized least-squares value iteration."" *International Conference on Artificial Intelligence and Statistics*. PMLR, 2020.
    - Agrawal, Shipra, and Randy Jia. ""Optimistic posterior sampling for reinforcement learning: worst-case regret bounds."" *Advances in Neural Information Processing Systems* 30 (2017).
    - Tiapkin, Daniil, et al. ""Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees."" *Advances in Neural Information Processing Systems* 35 (2022): 10737-10751.
    - Agrawal, Priyank, Jinglin Chen, and Nan Jiang. ""Improved worst-case regret bounds for randomized least-squares value iteration."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 35. No. 8. 2021.
- Line 83: In [32] they consider not episodic setup. The state-of-the-art result in the episodic setup were presented in [4].","The paper presented the theoretical research on Bayesian regret for posterior-sampling algorithms for reinforcement learning, thus it does not require discussion of ethical limitations.",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,2 fair,1689240226028,,,NeurIPS.cc/2023/Conference,2EVTB1idyR,"['NeurIPS.cc/2023/Conference/Submission10911/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411307290,[],4,,,['everyone'],2EVTB1idyR,['NeurIPS.cc/2023/Conference/Submission10911/Reviewer_pUAk'],1689240226028,1702411307290,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10911/Reviewer_pUAk']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that are relevant to the topic of frequentist regret bounds for TS-based exploration in tabular and linear MDPs, including:

* Zanette, Andrea, et al. ""Frequentist regret bounds for randomized least-squares value iteration."" 
* Agrawal, Shipra, and Randy Jia. ""Optimistic posterior sampling for reinforcement learning: worst-case regret bounds."" 
* Tiapkin, Daniil, et al. ""Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees."" 
* Agrawal, Priyank, Jinglin Chen, and Nan Jiang. ""Improved worst-case regret bounds for randomized least-squares value iteration.""

The reviewer also mentions a book by Lattimore, Tor, and Csaba Szepesvári, ""Bandit algorithms"", which is relevant to the topic of linear contextual bandits. Additionally, the reviewer mentions that the state-of-the-art result in the episodic setup was presented in [4], implying that the authors should be aware of this reference.",1,,Improved worst-case regret bounds for randomized least-squares value iteration
r03GM4wvl5,"This paper presents a classifier based approach to provide a measure of miscalibration in Bayesian computation, including for methods such as Approximate Bayesian Computation (ABC) and Simulation-Based Inference (SBI) methods like neural posterior estimation. The method enables the test statistic to be learned from data and  provides an interpretable divergence measure. The method is a form of two sample testing applied in the amortized and simulation based inference settings. Beyond standard approaches to using classifiers for two sample testing, the authors develop several classifier based approaches which use a form a ""label mapping"", and provide theoretical work on the validity and efficacy of these approaches. Experiments are provided to test the method for posterior inference in cases where the posterior is known, and provide empirical verification of the theoretical results. The paper also compares to the commonly used simulation based calibration method, and show clear improvements with the methods proposed in the paper. Experiments on cosmological data are also explored.","This is a good paper, it provides:
-  a clear description of the challenge they hope to address, specifically providing a better and more statistically interpretable measure of miscalibration over simulation-based calibration
- a clear description of the proposed methods, including how to use label mapping to develop calibration based diagnostics, and how this differs from other approaches
- theoretical grounding for the calibration based measures, including the expected behaviors in large sample limits
- useful discussions on implementation details
- useful discussion on the legitimacy and power of the tests.","While the idea of using classifiers for two sample testing is not new, the authors do develop new methods based on label mapping. While the new classification approaches are discussed and will likely be quite useful for simulation based inference, some attempts at using classifiers for testing posterior inference quality have been performed before (albeit not directly with some of the new classification methods discussed in the work). For instance, Vandegar et. al ""Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference"" AISTATS 2021, use the AUC of a classifier as a diagnostic in a simulation based setting.  

Further development of the experiments would be quite useful. A very simple experiment is shown, allowing an exploration of the methods, but some experiments more clearly showing the quality of the method in a controlled but more complex simulation based inference setting could provide useful insights to the readers. For instance, a common example is the Simple Likelihood, Complex Posterior (SLCP) problem from Papamakarios et. al, ""Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows"", AISTATS 2019. As it stands now, it is difficult to asses the quality of the method in the more complex cosmological data set experiment.

","Could you provide more details on the posterior model in the multi-variate Gaussian example?

Could you provide experiments on more complex but controlled examples, for instance the SLCP problem from Papamakarios et. al? This could provide more evidence on the utility of the method in SBI settings.

How do these methods scale with feature and parameter dimension? Does this affect the ability to attain a useful diagnostic?

You mention that the presence of nuisance parameters do not affect the quality of the diagnostics. However, the Neyman-Pearson lemma does not guarantee that the likelihood ratio is the uniform most powerful test for a given size in the presence of nuisance parameters. Perhaps I am not fully understanding the text, could you explain if this has any ramifications for the proposed tests?

Could you add references to other simulation based inference work using classifiers for diagnostics? In addition, recent work on using neural networks for two sample testing (e.g. Grosso et. al, https://arxiv.org/abs/2305.14137) may be relevant to discuss in the related work. ","The authors provide clear discussions of limitations. It would be interesting to also know if there are limitations dependent on the dimension of the features or parameters, and how this may impact the number of samples needed to attain a useful test.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,4 excellent,1688650140195,,,NeurIPS.cc/2023/Conference,2Cmdh5z6ph,"['NeurIPS.cc/2023/Conference/Submission14735/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411491337,[],1,,,['everyone'],2Cmdh5z6ph,['NeurIPS.cc/2023/Conference/Submission14735/Reviewer_aBBC'],1688650140195,1702411491337,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14735/Reviewer_aBBC']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Vandegar et al. ""Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference"" (AISTATS 2021)
2. Papamakarios et al. ""Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows"" (AISTATS 2019)
3. Grosso et al. (https://arxiv.org/abs/2305.14137) for recent work on using neural networks for two sample testing.

The reviewer suggests that citing these works would be relevant to discuss in the related work section and provide a more comprehensive overview of the existing literature on simulation-based inference using classifiers for diagnostics.",1,"2021, 2019, 2023",Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference 
r03GM4wvl5,"This paper presents a classifier based approach to provide a measure of miscalibration in Bayesian computation, including for methods such as Approximate Bayesian Computation (ABC) and Simulation-Based Inference (SBI) methods like neural posterior estimation. The method enables the test statistic to be learned from data and  provides an interpretable divergence measure. The method is a form of two sample testing applied in the amortized and simulation based inference settings. Beyond standard approaches to using classifiers for two sample testing, the authors develop several classifier based approaches which use a form a ""label mapping"", and provide theoretical work on the validity and efficacy of these approaches. Experiments are provided to test the method for posterior inference in cases where the posterior is known, and provide empirical verification of the theoretical results. The paper also compares to the commonly used simulation based calibration method, and show clear improvements with the methods proposed in the paper. Experiments on cosmological data are also explored.","This is a good paper, it provides:
-  a clear description of the challenge they hope to address, specifically providing a better and more statistically interpretable measure of miscalibration over simulation-based calibration
- a clear description of the proposed methods, including how to use label mapping to develop calibration based diagnostics, and how this differs from other approaches
- theoretical grounding for the calibration based measures, including the expected behaviors in large sample limits
- useful discussions on implementation details
- useful discussion on the legitimacy and power of the tests.","While the idea of using classifiers for two sample testing is not new, the authors do develop new methods based on label mapping. While the new classification approaches are discussed and will likely be quite useful for simulation based inference, some attempts at using classifiers for testing posterior inference quality have been performed before (albeit not directly with some of the new classification methods discussed in the work). For instance, Vandegar et. al ""Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference"" AISTATS 2021, use the AUC of a classifier as a diagnostic in a simulation based setting.  

Further development of the experiments would be quite useful. A very simple experiment is shown, allowing an exploration of the methods, but some experiments more clearly showing the quality of the method in a controlled but more complex simulation based inference setting could provide useful insights to the readers. For instance, a common example is the Simple Likelihood, Complex Posterior (SLCP) problem from Papamakarios et. al, ""Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows"", AISTATS 2019. As it stands now, it is difficult to asses the quality of the method in the more complex cosmological data set experiment.

","Could you provide more details on the posterior model in the multi-variate Gaussian example?

Could you provide experiments on more complex but controlled examples, for instance the SLCP problem from Papamakarios et. al? This could provide more evidence on the utility of the method in SBI settings.

How do these methods scale with feature and parameter dimension? Does this affect the ability to attain a useful diagnostic?

You mention that the presence of nuisance parameters do not affect the quality of the diagnostics. However, the Neyman-Pearson lemma does not guarantee that the likelihood ratio is the uniform most powerful test for a given size in the presence of nuisance parameters. Perhaps I am not fully understanding the text, could you explain if this has any ramifications for the proposed tests?

Could you add references to other simulation based inference work using classifiers for diagnostics? In addition, recent work on using neural networks for two sample testing (e.g. Grosso et. al, https://arxiv.org/abs/2305.14137) may be relevant to discuss in the related work. ","The authors provide clear discussions of limitations. It would be interesting to also know if there are limitations dependent on the dimension of the features or parameters, and how this may impact the number of samples needed to attain a useful test.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,4 excellent,1688650140195,,,NeurIPS.cc/2023/Conference,2Cmdh5z6ph,"['NeurIPS.cc/2023/Conference/Submission14735/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411491337,[],1,,,['everyone'],2Cmdh5z6ph,['NeurIPS.cc/2023/Conference/Submission14735/Reviewer_aBBC'],1688650140195,1702411491337,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14735/Reviewer_aBBC']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Vandegar et al. ""Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference"" (AISTATS 2021)
2. Papamakarios et al. ""Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows"" (AISTATS 2019)
3. Grosso et al. (https://arxiv.org/abs/2305.14137) for recent work on using neural networks for two sample testing.

The reviewer suggests that citing these works would be relevant to discuss in the related work section and provide a more comprehensive overview of the existing literature on simulation-based inference using classifiers for diagnostics.",1,"2021, 2019, 2023",Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows 
r03GM4wvl5,"This paper presents a classifier based approach to provide a measure of miscalibration in Bayesian computation, including for methods such as Approximate Bayesian Computation (ABC) and Simulation-Based Inference (SBI) methods like neural posterior estimation. The method enables the test statistic to be learned from data and  provides an interpretable divergence measure. The method is a form of two sample testing applied in the amortized and simulation based inference settings. Beyond standard approaches to using classifiers for two sample testing, the authors develop several classifier based approaches which use a form a ""label mapping"", and provide theoretical work on the validity and efficacy of these approaches. Experiments are provided to test the method for posterior inference in cases where the posterior is known, and provide empirical verification of the theoretical results. The paper also compares to the commonly used simulation based calibration method, and show clear improvements with the methods proposed in the paper. Experiments on cosmological data are also explored.","This is a good paper, it provides:
-  a clear description of the challenge they hope to address, specifically providing a better and more statistically interpretable measure of miscalibration over simulation-based calibration
- a clear description of the proposed methods, including how to use label mapping to develop calibration based diagnostics, and how this differs from other approaches
- theoretical grounding for the calibration based measures, including the expected behaviors in large sample limits
- useful discussions on implementation details
- useful discussion on the legitimacy and power of the tests.","While the idea of using classifiers for two sample testing is not new, the authors do develop new methods based on label mapping. While the new classification approaches are discussed and will likely be quite useful for simulation based inference, some attempts at using classifiers for testing posterior inference quality have been performed before (albeit not directly with some of the new classification methods discussed in the work). For instance, Vandegar et. al ""Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference"" AISTATS 2021, use the AUC of a classifier as a diagnostic in a simulation based setting.  

Further development of the experiments would be quite useful. A very simple experiment is shown, allowing an exploration of the methods, but some experiments more clearly showing the quality of the method in a controlled but more complex simulation based inference setting could provide useful insights to the readers. For instance, a common example is the Simple Likelihood, Complex Posterior (SLCP) problem from Papamakarios et. al, ""Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows"", AISTATS 2019. As it stands now, it is difficult to asses the quality of the method in the more complex cosmological data set experiment.

","Could you provide more details on the posterior model in the multi-variate Gaussian example?

Could you provide experiments on more complex but controlled examples, for instance the SLCP problem from Papamakarios et. al? This could provide more evidence on the utility of the method in SBI settings.

How do these methods scale with feature and parameter dimension? Does this affect the ability to attain a useful diagnostic?

You mention that the presence of nuisance parameters do not affect the quality of the diagnostics. However, the Neyman-Pearson lemma does not guarantee that the likelihood ratio is the uniform most powerful test for a given size in the presence of nuisance parameters. Perhaps I am not fully understanding the text, could you explain if this has any ramifications for the proposed tests?

Could you add references to other simulation based inference work using classifiers for diagnostics? In addition, recent work on using neural networks for two sample testing (e.g. Grosso et. al, https://arxiv.org/abs/2305.14137) may be relevant to discuss in the related work. ","The authors provide clear discussions of limitations. It would be interesting to also know if there are limitations dependent on the dimension of the features or parameters, and how this may impact the number of samples needed to attain a useful test.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,4 excellent,1688650140195,,,NeurIPS.cc/2023/Conference,2Cmdh5z6ph,"['NeurIPS.cc/2023/Conference/Submission14735/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411491337,[],1,,,['everyone'],2Cmdh5z6ph,['NeurIPS.cc/2023/Conference/Submission14735/Reviewer_aBBC'],1688650140195,1702411491337,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14735/Reviewer_aBBC']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Vandegar et al. ""Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference"" (AISTATS 2021)
2. Papamakarios et al. ""Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows"" (AISTATS 2019)
3. Grosso et al. (https://arxiv.org/abs/2305.14137) for recent work on using neural networks for two sample testing.

The reviewer suggests that citing these works would be relevant to discuss in the related work section and provide a more comprehensive overview of the existing literature on simulation-based inference using classifiers for diagnostics.",1,"2021, 2019, 2023",Goodness of fit by Neyman-Pearson testing
h1GTM9aicR,"In this paper the authors focus on the challenge of comparing two conditional distributions p, q from their samples. In particular, this is useful as a check for bayesian computations.
To achieve this, the authors propose the use of a probabilistic classification approach where they create a new dataset combining samples from p and q and labels related to the particular distribution. Here, the authors propose different approaches and make connections to previous work.
Given the combined dataset, a classifier tries to predict the label from the features. Failing to do so, indicates that the classifier could not discriminate between the two distributions.
The performance of the probabilistic classifier is then used to estimate the divergence between the distributions and for hypothesis testing. 
In the empirical section, the authors compare against SBC and show improvements in power given a reduced number of samples.","The paper analyses different approaches for the generation of samples to be used by discriminators. They then present theoretical developments and demonstrate their results empirically. The sampling approaches and theoretical treatment are novel, of high quality and well written.
The main advantage here is that even for a sub-optimal classifier, one gets improvements in data efficiency. ","The main weakness of the paper is that in the empirical evaluation, the authors do not explore the case where the samples present more challenging behaviors such as auto-correlations, different types of imbalances, etc.

Other minor suggestions:
- The colors of the thetas in the introduction are hard to see. 
- Describing why the bounds are tight could further help the reader in section 2.","I'm thinking of your paper as an approach to test whether the labels and the generated features are connected. This reminds me of [1] where they evaluate the mutual information via neural networks. I'm wondering both if it makes sense in your setting to estimate the divergence via the Donsker-Varadhan representation, and in their setting if your feature generation approach is beneficial. Do you have any opinions on that?

The same has applications for other approaches on independency testing via classifiers where your feature generation approach also has potential.

[1] Belghazi, Mohamed Ishmael, et al. ""Mutual information neural estimation."" International conference on machine learning. PMLR, 2018.",There are no potential negative societal impacts from this work. The authors mention limitations in a future work section. ,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,4 excellent,1688676090339,,,NeurIPS.cc/2023/Conference,2Cmdh5z6ph,"['NeurIPS.cc/2023/Conference/Submission14735/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411491198,[],2,,,['everyone'],2Cmdh5z6ph,['NeurIPS.cc/2023/Conference/Submission14735/Reviewer_gi2H'],1688676090339,1702411491198,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14735/Reviewer_gi2H']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, which is:

[1] Belghazi, Mohamed Ishmael, et al. ""Mutual information neural estimation."" International conference on machine learning. PMLR, 2018.

This suggestion is made in the context of exploring the connection between the proposed approach and estimating mutual information via neural networks, as well as potential applications to independency testing via classifiers.",1,2018,Mutual information neural estimation
0tSfbCHUar,The paper generalizes lower bounds on the adversarially robust error on a finite dataset from binary to multi-class classification.,"1. The paper is well presented and easy to read, which is no mean feat for the amount of theory that is introduced and developed.
2. The formalization and assumptions are clearly stated, well arranged, and do not contain unnecessary complications.
3. The developed theory is correct as far as I can tell.
4. The experimental evaluations make sense for the discussed topic, regard standard baselines and are easy to understand. They are shown even while they don't support the necessity of the multi-class theory, which is nice to see.
5. The fact that calculating the bounds based on binary l2-ball intersections makes no difference in practice doesn't mean that the theory is not helpful for estimations that are in principle more correct.
6. Limitations are discussed honestly.
7. The code looks nice, but I haven't tested it.","1. While they are no clear weaknesses, there are some points unclear to me which I list below in the Questions section and I would much appreciate to see answered and in some cases discussed in the paper.
2. A substantial part of the formalization, theory and experimental design are not completely newly developed, but carried over from the previous paper ""Lower Bounds on Cross-Entropy Loss in the Presence of Test-time Adversaries"" which deals with the same problem in the more special case of binary classification. This is why I'm rating this submission currently as a standard ""accept"".","1. While Equation (1) is true, I think it is not trivial, but a theorem that depends on some assumptions (which appear to be fulfilled by the considered hypothesis classes). It was proven in (Pydi,Jog 2022: ""The Many Faces of Adversarial Risk"")[https://arxiv.org/abs/2201.08956], but there might be earlier, more well known versions of the theorem. I think the intuition why (1) is true should be given.
2. l. 89 I think it should better read ""the vector of _robustly_ correct classification""
3. l. 97 Should ""feasible"" read ""achievable"" here to stay consistent?
4. l. 97 Do the nonnegative linear combinations need to have factors that sum to less than 1?
5. Is L without * correct in Eq. (2)? If so, it wasn't defined.
6. l. 122 missing word
7. l. 124 extra (
8. Is the construction of the full hypergraph indeed computationally expensive for an l2 threat model? Intutitively, the geometry might make incidence matrix very sparse and its computation quite straightforward. Or is it rather an issue with the LP solver?
9. Would it make sense to regard the reverse truncation where if we find a triple of pairs with overlap (as in Fig. 1 left), we assume that there is also a point that generates the degree 3 hyperedge (and so on for higher degrees)? this might maybe yield an upper bound on $L^*(K)$.
10. l. 296 whether and in which sense the hypothesis class is much smaller is not obvious, and it is not clear if insufficient fitting within the class might be responsible for a large part of the gap.
11. In Figure 3, an empirical model evaluation as in Figure 2 should be included. ($L_{CW}$ is a bit confusing at first look, since many papers use that for Carlini-Wagner l2 attacks.)
12. Why not use the full AutoAttack?
13. The statistics on number of hyperedges should be included for all datasets, and a few values of epsilon (including 2 and 2.5), since they quantify the importance of regarding multi-class overlaps.
14. Also statistics like the average distance of an image to the closest one from another class would be nice to understand the geometry of the neighborhood overlaps.
15. It would be great if the authors could find a type of dataset and threat model where multi-class neighborhood overlap plays a bigger role than with MNIST and CIFAR. Maybe even a toy example would be illustrative.
16. Since the paper talks about optimal errors for finite distributions, it would make sense to show evaluations both on training and test sets.
17. A more concrete comparison to Trillos et al.[21], if applicable with evaluation numbers, would be helpful.
18. A discussion of the limitation to distributions with finite support and whether one can expect this assumption to be softened in future works building on this one would be interesting.",The limitations are discussed in detail.,['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,4 excellent,1687267932253,,,NeurIPS.cc/2023/Conference,2CRaOpEKWh,"['NeurIPS.cc/2023/Conference/Submission4970/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410980293,[],1,,,['everyone'],2CRaOpEKWh,['NeurIPS.cc/2023/Conference/Submission4970/Reviewer_Q9vC'],1687267932253,1702410980293,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4970/Reviewer_Q9vC']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, namely ""The Many Faces of Adversarial Risk"" by Pydi and Jog (2022), which can be found at https://arxiv.org/abs/2201.08956. This is mentioned in question 1 of the review. The reviewer thinks that the intuition behind Equation (1) should be given and mentions that it was proven in the mentioned paper, but also notes that there might be earlier, more well-known versions of the theorem.",1,"2022, 2022",The Many Faces of Adversarial Risk
cQlePvdj4O,"This paper aims to analyze the optimal 0/1 loss under the most strongest test time attack. The study commences by formulating the problem of obtaining the optimal classifier (based on 0/1 loss) as a linear program on a graph. Subsequently, the authors address the high computational complexity of calculating the optimal 0/1 loss by proposing a reduction technique through graph truncation. This reduction enables the computation of a lower bound of the 0/1 loss in a feasible timeframe. Ultimately, the authors present empirical evidence comparing their bound with the empirical defense method on real-world data. Notably, they discover that the widely-used baseline (adversarial training) still offers significant potential for improvement.","- The paper is written clearly and easy to follow.
- This paper extents the analysis on the optimal classifier under test time attack to multi-class classification setting. It seems to be a decent extension/contribution to the theoretical side of the field of adversarial robustness.
- Usually for this kind of problem, the computation complexity is one of the main challenge. However, they are able to find a way to speed it up. The idea of reducing it to a graph and then truncate the complex edges to reduce the computation needed for getting the bounds is interesting. In addition, they also show that empirically, such relaxation does not lose much information.","- Although it is mentioned in the related work that this is different from verifying robustness. I think it would be a valuable information to include bounds for verified classifiers in the empirical section. It would be interesting to see how close the existing verified classifiers are to the optimal bound. 
- A related work titled ""Robustness for non-parametric classification: A generic attack and defense."" published in International Conference on Artificial Intelligence and Statistics, 2020 can be added. This work also utilize the idea of creating graph with the vertices being each example and edges being conflicting examples pairs. They tried to approach the optimal 0/1 by removing minimum number of edges. Although they did not compute specific bounds on the optimal 0/1 loss, I think it is still worth being discussed.
- This is a work with solid technical contribution. However, as mentioned in the limitation, the lack of implication on how to close the gap between the current robust classifiers and the optimal classifier limits the impact of this work to a moderate-to-high impact paper.
- Although a method for speeding up the algorithm through truncating the graph is proposed, the applicability of the proposed algorithm in practice seems to be still limited in practice due to heavy computational cost (in the experiment, only three class classification problems are run).
","Are there any of my review comments that misunderstood the paper? If so, please point them out. I am happy to adjust accordingly.
","The authors do properly addressed the limitation of this work, which includes the lack of scalability of their algorithm and the lack of implications on how to close the gap between the current robust classifiers and the optimal classifier.
",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1690303732514,,,NeurIPS.cc/2023/Conference,2CRaOpEKWh,"['NeurIPS.cc/2023/Conference/Submission4970/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410979971,[],4,,,['everyone'],2CRaOpEKWh,['NeurIPS.cc/2023/Conference/Submission4970/Reviewer_baps'],1690303732514,1702410979971,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4970/Reviewer_baps']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, which is:

""Robustness for non-parametric classification: A generic attack and defense."" published in International Conference on Artificial Intelligence and Statistics, 2020.

The reviewer mentions that this work also utilizes the idea of creating a graph with vertices being each example and edges being conflicting example pairs, and thinks it is still worth being discussed, implying that the authors should consider citing this related work.",1,2020,Robustness for non-parametric classification A generic attack and defense
pWSRPA0cXe,"The paper proposes using score matching to learn the backward process of a given forward SDE, notably coming from a physics application. The paper states that this can be used to simulate backward the distribution of the initial condition given the end state, by starting from the end state and drawing trajectories backward according to the backward SDE. This can be of interest in situations where the physics of a system is not invertible, as the authors point out is the case for most of the macroscopic state governing equations. 

The authors propose two inference procedures, a SDE (sampler) and an ODE (deterministic) and test in 4 different settings, with different levels of complexity.","*The paper exploits an interesting analogy between the current score matching framework used for generative modelling and SDEs coming from physics and the inverse problems associated to them. The idea is that the forward process do not need to be a diffusion and therefore we can learn the backward process for a given SDE to sample a trajectory backward. This is indeed an interesting analogy that can lead to several interesting applications in the future. 

*The numerical examples are clear and nicely illustrated. The numerical examples show that the current framework works better than simply running the simulator backwards or simply approaching it by a neural network.","* In line 70 the paper states that it's goal is to sample from the posterior distribution $p(x_0 | x_M^*)$ where $p$ corresponds to the forward physical process. But, during the remainder of the paper, one of the proposed algorithms suggested (SMDP-ODE) can not be considered as a sampler from $p(x_0 | x_M^*)$, since it is a function of $x_M^*$. As shown by [1], what holds is that the ODE pushes forward the distribution of $p(x_M)$ to a distribution that converges (weakly) to $p(x_0)$ so it is not clear what is the actual point of starting the ODE from a given (fixed) $x_M^*$. In the conclusion the authors do touch on the point that the ODE variant is not a sampler but I feel that the presentation generates a lot of confusion for the reader during most parts of the paper. 

* As I understand it, only the toy problem presents a multimodal posterior. One would expect this kind of technique to be particularly useful in settings where multimodality of the posterior is present, so I would expect the authors to focus more on those cases. This is also reflected by the metrics being used (comparisons to the ""true"" $x_0$, either $RMSE$ or $LPIPS$), which would arguably make less sense in the case of multimodality and even in the case where the mode of the posterior distribution does not match the $x_0$ that produced the fixed $x_M^*$.  

* The paper do not present any comparison with other inverse problems solvers, focusing on score matching based approaches only. Even though this in comprehensible to a degree, I feel there should be at least one of the non-score matching based approaches for solving inverse problems.


[1] Yang Song, et al. ""Score-Based Generative Modeling through Stochastic Differential Equations."" International Conference on Learning Representations. 2021.

Remarks:
* The posterior distribution is clearly defined, as being the distribution given by
$\int p(x_{0:M-1}, x_M^*) dx_{1:M-1}$, therefore I find it strange to say ""a"" posterior distribution in line 136.","* There is a practical difference between training the proposed model and the standard denoising diffusion model [2]. In [2] the losses are calculated by sampling from $p(x_t | x_0)$ which can be written as $\mu_t(x_0) + \epsilon$ where $\epsilon$ is some gaussian noise. In the context of the paper this is not always possible and one need to rely on a given set of paths. How does this impact the training? Is it possible to generate the same kind of training function that does not depend of unrolling the paths? 

* The values in Table 1 are counter-intuitive as far as I'm concerned. Why do the methods '1-step' and 'SSM-VR' seem to achieve the best posterior metric Q for smaller datasets in both ODE and SDE?

* Are the situations where using an inverse step solver $P^{-1}$ instead of $P$ motivated only by the numerical cost of running $P$ ?




[2] Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems (pp. 6840–6851). Curran Associates, Inc..",,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1688568353687,,,NeurIPS.cc/2023/Conference,2BpoGPSDCR,"['NeurIPS.cc/2023/Conference/Submission1882/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410810210,[],2,,,['everyone'],2BpoGPSDCR,['NeurIPS.cc/2023/Conference/Submission1882/Reviewer_mtJA'],1688568353687,1702410810210,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1882/Reviewer_mtJA']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Yang Song, et al. ""Score-Based Generative Modeling through Stochastic Differential Equations."" International Conference on Learning Representations. 2021. (referenced as [1])
2. Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems (pp. 6840–6851). Curran Associates, Inc.. (referenced as [2])",1,"2021, 2020",Score-Based Generative Modeling through Stochastic Differential Equations 
pWSRPA0cXe,"The paper proposes using score matching to learn the backward process of a given forward SDE, notably coming from a physics application. The paper states that this can be used to simulate backward the distribution of the initial condition given the end state, by starting from the end state and drawing trajectories backward according to the backward SDE. This can be of interest in situations where the physics of a system is not invertible, as the authors point out is the case for most of the macroscopic state governing equations. 

The authors propose two inference procedures, a SDE (sampler) and an ODE (deterministic) and test in 4 different settings, with different levels of complexity.","*The paper exploits an interesting analogy between the current score matching framework used for generative modelling and SDEs coming from physics and the inverse problems associated to them. The idea is that the forward process do not need to be a diffusion and therefore we can learn the backward process for a given SDE to sample a trajectory backward. This is indeed an interesting analogy that can lead to several interesting applications in the future. 

*The numerical examples are clear and nicely illustrated. The numerical examples show that the current framework works better than simply running the simulator backwards or simply approaching it by a neural network.","* In line 70 the paper states that it's goal is to sample from the posterior distribution $p(x_0 | x_M^*)$ where $p$ corresponds to the forward physical process. But, during the remainder of the paper, one of the proposed algorithms suggested (SMDP-ODE) can not be considered as a sampler from $p(x_0 | x_M^*)$, since it is a function of $x_M^*$. As shown by [1], what holds is that the ODE pushes forward the distribution of $p(x_M)$ to a distribution that converges (weakly) to $p(x_0)$ so it is not clear what is the actual point of starting the ODE from a given (fixed) $x_M^*$. In the conclusion the authors do touch on the point that the ODE variant is not a sampler but I feel that the presentation generates a lot of confusion for the reader during most parts of the paper. 

* As I understand it, only the toy problem presents a multimodal posterior. One would expect this kind of technique to be particularly useful in settings where multimodality of the posterior is present, so I would expect the authors to focus more on those cases. This is also reflected by the metrics being used (comparisons to the ""true"" $x_0$, either $RMSE$ or $LPIPS$), which would arguably make less sense in the case of multimodality and even in the case where the mode of the posterior distribution does not match the $x_0$ that produced the fixed $x_M^*$.  

* The paper do not present any comparison with other inverse problems solvers, focusing on score matching based approaches only. Even though this in comprehensible to a degree, I feel there should be at least one of the non-score matching based approaches for solving inverse problems.


[1] Yang Song, et al. ""Score-Based Generative Modeling through Stochastic Differential Equations."" International Conference on Learning Representations. 2021.

Remarks:
* The posterior distribution is clearly defined, as being the distribution given by
$\int p(x_{0:M-1}, x_M^*) dx_{1:M-1}$, therefore I find it strange to say ""a"" posterior distribution in line 136.","* There is a practical difference between training the proposed model and the standard denoising diffusion model [2]. In [2] the losses are calculated by sampling from $p(x_t | x_0)$ which can be written as $\mu_t(x_0) + \epsilon$ where $\epsilon$ is some gaussian noise. In the context of the paper this is not always possible and one need to rely on a given set of paths. How does this impact the training? Is it possible to generate the same kind of training function that does not depend of unrolling the paths? 

* The values in Table 1 are counter-intuitive as far as I'm concerned. Why do the methods '1-step' and 'SSM-VR' seem to achieve the best posterior metric Q for smaller datasets in both ODE and SDE?

* Are the situations where using an inverse step solver $P^{-1}$ instead of $P$ motivated only by the numerical cost of running $P$ ?




[2] Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems (pp. 6840–6851). Curran Associates, Inc..",,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1688568353687,,,NeurIPS.cc/2023/Conference,2BpoGPSDCR,"['NeurIPS.cc/2023/Conference/Submission1882/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410810210,[],2,,,['everyone'],2BpoGPSDCR,['NeurIPS.cc/2023/Conference/Submission1882/Reviewer_mtJA'],1688568353687,1702410810210,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1882/Reviewer_mtJA']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Yang Song, et al. ""Score-Based Generative Modeling through Stochastic Differential Equations."" International Conference on Learning Representations. 2021. (referenced as [1])
2. Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems (pp. 6840–6851). Curran Associates, Inc.. (referenced as [2])",1,"2021, 2020",Denoising Diffusion Probabilistic Models
YbYdC23PoT,"This paper proposes a diffusion-based unrolled strategy for learning solve ordinary differential equations (stochastic or not). After introducing the problem at stake and proposing two training strategies for solving it (namely a 1-step loss approach and a multi-step approach), the authors draw a theoretical parallel with denoising score matching and with probability flow ODEs. More precisely, the authors claim an equivalence between training an architecture with the proposed 1-step loss and training the same architecture with a score matching objective; and the equivalence between minimizing the multi-step loss and maximizing a variational lower bound. Eventually, the authors investigate the performance of the proposed approach in 4 setups, including deterministic and stochastic problems, ranging from a simple 1D experiment to a navier-stokes simulation.","- The paper investigates an original idea proposing to link traditional 1-step or multi-step losses in trajectory estimation for dynamical systems with diffusion and score matching. To the best of my knowledge, this idea is new in the literature and makes a link between traditional training approaches and diffusion processes. 
- The authors perform a large number of experiments to support their claims.","- Albeit well written, the article is difficult to follow due to not always well organised sections and very large amount of information.
- The chosen baselines do not seem very relevant, because not relying on standard losses for training denoising diffusion architectures for inverse problems.
- There is a potential problem with one of the theoretical results (Theorem 2.1)
- The literature review is not sufficient; while important and recent works are appropriately cited, there lacks references to physics informed diffusion approaches for inverse problems.","The following points list my concerns / questions / suggestions for the authors.

**1. Main concerns**

**1.1 Difficulties to follow the article** My main concern is that the paper is very difficult to follow. While overall well written, I tend to lose track of what the authors are aiming to do. For instance, in ""method overview"", the problem is not clearly stated. In my view, equation (6) of the main is the model that the authors aim at solving everywhere, but it comes in the middle of a section explaining score matching. In fact, the problem setting description from the appendix is rather clear while the first paragraph of 2 is not clear. There is a numerical update rule, but where does it come from? What is P? What is x? How are they related? Maybe if (6) had come in the introduction, the ""Problem formulation"" from section 2 would have been clearer.

**1.2 Comparison with baselines** While the theoretical explanations and intuitions link quite clearly denoising score matching with the problem of interest, the case where a network $s_{\theta}(x, t)$ is trained as a denoiser with a time embedding in a diffusion fashion does not seem to be present in the experiments (in ISM and SSM, $s_\theta(x, t)$ are not trained as denoisers if I understand correctly; please correct me if I'm wrong). Moreover, these architectures do not scale to higher dimensional problems (see line 225). As such, I wonder whether these baselines provide fair comparisons. Given the embedding of $s_\theta(x, t)$, I would suggest to use a method relying on denoising diffusion such as [HJA20]. If not applicable, maybe Diffusion Schrodinger Bridge would be a strong baseline to compare to [1]. Furthermore, given the similarities between the problem the authors want to tackle and the one from [1], I think adding a brief explanation on how the problems / approaches differ might be welcome.

**1.3 PDE litterature** I have difficulties with the relations to other works. In my opinion, section 4 comes too late and should be inserted way earlier first - probably before section 2. More precisely: in ""Learned corrections for numerical errors"", a more detailed review of learning-based methods for solving PDEs / stochastic PDEs would be helpful. 

**1.4 Lack of references to physics informed diffusion approaches** In general I believe that a large part of the imaging inverse problems literature is not mentioned. While this is not the core topic of the paper, it remains interesting in its own right since many papers have proposed methods for incorporating a measurement operator (or P(x) in the authors' words) within diffusion models, thus making the diffusion process ""physics aware"", which is precisely what the authors want to do here. A cornerstone reference, which the authors included, is [Chu+22]. However, the authors mention that this work performs uncertainty quantification, and state ""either focus on the denoising objective common for tasks involving natural images, or the synthesis process of solutions does no directly consider the underlying physics."": I disagree with this, see e.g. Figure 4 of the paper. If this reference does not convince the authors, here are other references where underlying physics / acquisition procedures are taken into account in a diffusion process: [2, 3, 4, 5]. Note also that an extensive literature in the inverse imaging literature has focused on a similar approach to your multistep loss, via architectures known as unfolded architectures incorporating the physics model inside the architecture [6].

**1.5 Proof of Theorem 2.1** I wonder whether the proof of $\Rightarrow$ is correct. My concern is with the particular sentence: ""let $\theta^*$ denote a minimizer such that $\mathcal{L}(\theta) \to 0$ as $\Delta t \to 0$. Note that at least one minimizer exists as we can choose $s_{\theta^*}(x, t) = \nabla_x \operatorname{log} p_t(x)$."" While I agree that a minimizer to this convex functional exists regardless of the nature of  $s_\theta(x, t)$, I am not sure you can assume that the minimum value of the functional tends to 0 without any further assumption on the very nature of $s_\theta(x, t)$: take for instance a simplistic model that is not powerful enough to approximate $\nabla_x \operatorname{log} p_t(x)$...

**2. Additional painpoints**

**2.1 Title** IMO, there is a mismatch between the title and the article: the authors do not propose a denoising score matching method, but a radically different approach that they claim to be equivalent to score matching. 

**2.2 Spectral loss** Why is a spectral loss necessary (line 241)? Aren't l2 (or l1, often more efficient) sufficient? Choosing a spectral loss seems unusual to me, maybe linking to some other papers using a similar loss would be welcome?

**2.3 Cumberstone notations** Notations are sometimes difficult to follow, maybe clarifying them would be useful. Some examples: around line 167, $p_1$ and $p_{-1}$ clash with $p_0$ and $p_T$. Around line 63, $\Delta t = t_j-t_k$, but then the authors use the convention $(t_m)_{0 \leq m \leq M}$. Why not replacing $j$ and $k$ with $m+1$ and $m$? etc...

**2.4** Shouldn't the term inside the norm in eq. (2) be updated $x_{m+1}-x_m - \Delta t \cdots$?

**2.5** line 110 of the supplementary, should (9) not be (8) instead?

**2.6** Eq. (38) in supplementary: is the brownian term not missing?

**2.7** Table 1 from supplementary: DilatedConv --> Dil-ResNet?

**2.8** Line 426 of supplementary: 100% of what?

**2.9** line 565 of supplementary: define the exact expression for $s_1$ and $s_2$.


**References:**

[1] De Bortoli, Valentin, James Thornton, Jeremy Heng, and Arnaud Doucet. ""Diffusion Schrödinger bridge with applications to score-based generative modeling."" Advances in Neural Information Processing Systems 34 (2021): 17695-17709.

[2] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. ""Diffusion posterior sampling for general noisy inverse problems."" arXiv preprint arXiv:2209.14687 (2022).

[3] Zhu, Yuanzhi, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. ""Denoising Diffusion Models for Plug-and-Play Image Restoration."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1219-1229. 2023.

[4] Kawar, Bahjat, Michael Elad, Stefano Ermon, and Jiaming Song. ""Denoising diffusion restoration models."" Advances in Neural Information Processing Systems 35 (2022): 23593-23606.

[5] Rout, Litu, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, and Sanjay Shakkottai. ""Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models."" arXiv preprint arXiv:2307.00619 (2023).

[6] Adler, Jonas, and Ozan Öktem. ""Learned primal-dual reconstruction."" IEEE transactions on medical imaging 37, no. 6 (2018): 1322-1332.",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1689246198218,,,NeurIPS.cc/2023/Conference,2BpoGPSDCR,"['NeurIPS.cc/2023/Conference/Submission1882/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410810079,[],4,,,['everyone'],2BpoGPSDCR,['NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs'],1689246198218,1702410810079,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that they believe are relevant to the paper and should be included, such as:

* [1] De Bortoli et al. (2021) - Diffusion Schrödinger bridge with applications to score-based generative modeling
* [2] Chung et al. (2022) - Diffusion posterior sampling for general noisy inverse problems
* [3] Zhu et al. (2023) - Denoising Diffusion Models for Plug-and-Play Image Restoration
* [4] Kawar et al. (2022) - Denoising diffusion restoration models
* [5] Rout et al. (2023) - Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models
* [6] Adler and Öktem (2018) - Learned primal-dual reconstruction

The reviewer also mentions that the authors should provide a more detailed review of learning-based methods for solving PDEs/stochastic PDEs, and that they should include references to physics-informed diffusion approaches for inverse problems.",1,"2021, 2022, 2023, 2022, 2023, 2018",Diffusion Schrödinger bridge with applications to score-based generative modeling
YbYdC23PoT,"This paper proposes a diffusion-based unrolled strategy for learning solve ordinary differential equations (stochastic or not). After introducing the problem at stake and proposing two training strategies for solving it (namely a 1-step loss approach and a multi-step approach), the authors draw a theoretical parallel with denoising score matching and with probability flow ODEs. More precisely, the authors claim an equivalence between training an architecture with the proposed 1-step loss and training the same architecture with a score matching objective; and the equivalence between minimizing the multi-step loss and maximizing a variational lower bound. Eventually, the authors investigate the performance of the proposed approach in 4 setups, including deterministic and stochastic problems, ranging from a simple 1D experiment to a navier-stokes simulation.","- The paper investigates an original idea proposing to link traditional 1-step or multi-step losses in trajectory estimation for dynamical systems with diffusion and score matching. To the best of my knowledge, this idea is new in the literature and makes a link between traditional training approaches and diffusion processes. 
- The authors perform a large number of experiments to support their claims.","- Albeit well written, the article is difficult to follow due to not always well organised sections and very large amount of information.
- The chosen baselines do not seem very relevant, because not relying on standard losses for training denoising diffusion architectures for inverse problems.
- There is a potential problem with one of the theoretical results (Theorem 2.1)
- The literature review is not sufficient; while important and recent works are appropriately cited, there lacks references to physics informed diffusion approaches for inverse problems.","The following points list my concerns / questions / suggestions for the authors.

**1. Main concerns**

**1.1 Difficulties to follow the article** My main concern is that the paper is very difficult to follow. While overall well written, I tend to lose track of what the authors are aiming to do. For instance, in ""method overview"", the problem is not clearly stated. In my view, equation (6) of the main is the model that the authors aim at solving everywhere, but it comes in the middle of a section explaining score matching. In fact, the problem setting description from the appendix is rather clear while the first paragraph of 2 is not clear. There is a numerical update rule, but where does it come from? What is P? What is x? How are they related? Maybe if (6) had come in the introduction, the ""Problem formulation"" from section 2 would have been clearer.

**1.2 Comparison with baselines** While the theoretical explanations and intuitions link quite clearly denoising score matching with the problem of interest, the case where a network $s_{\theta}(x, t)$ is trained as a denoiser with a time embedding in a diffusion fashion does not seem to be present in the experiments (in ISM and SSM, $s_\theta(x, t)$ are not trained as denoisers if I understand correctly; please correct me if I'm wrong). Moreover, these architectures do not scale to higher dimensional problems (see line 225). As such, I wonder whether these baselines provide fair comparisons. Given the embedding of $s_\theta(x, t)$, I would suggest to use a method relying on denoising diffusion such as [HJA20]. If not applicable, maybe Diffusion Schrodinger Bridge would be a strong baseline to compare to [1]. Furthermore, given the similarities between the problem the authors want to tackle and the one from [1], I think adding a brief explanation on how the problems / approaches differ might be welcome.

**1.3 PDE litterature** I have difficulties with the relations to other works. In my opinion, section 4 comes too late and should be inserted way earlier first - probably before section 2. More precisely: in ""Learned corrections for numerical errors"", a more detailed review of learning-based methods for solving PDEs / stochastic PDEs would be helpful. 

**1.4 Lack of references to physics informed diffusion approaches** In general I believe that a large part of the imaging inverse problems literature is not mentioned. While this is not the core topic of the paper, it remains interesting in its own right since many papers have proposed methods for incorporating a measurement operator (or P(x) in the authors' words) within diffusion models, thus making the diffusion process ""physics aware"", which is precisely what the authors want to do here. A cornerstone reference, which the authors included, is [Chu+22]. However, the authors mention that this work performs uncertainty quantification, and state ""either focus on the denoising objective common for tasks involving natural images, or the synthesis process of solutions does no directly consider the underlying physics."": I disagree with this, see e.g. Figure 4 of the paper. If this reference does not convince the authors, here are other references where underlying physics / acquisition procedures are taken into account in a diffusion process: [2, 3, 4, 5]. Note also that an extensive literature in the inverse imaging literature has focused on a similar approach to your multistep loss, via architectures known as unfolded architectures incorporating the physics model inside the architecture [6].

**1.5 Proof of Theorem 2.1** I wonder whether the proof of $\Rightarrow$ is correct. My concern is with the particular sentence: ""let $\theta^*$ denote a minimizer such that $\mathcal{L}(\theta) \to 0$ as $\Delta t \to 0$. Note that at least one minimizer exists as we can choose $s_{\theta^*}(x, t) = \nabla_x \operatorname{log} p_t(x)$."" While I agree that a minimizer to this convex functional exists regardless of the nature of  $s_\theta(x, t)$, I am not sure you can assume that the minimum value of the functional tends to 0 without any further assumption on the very nature of $s_\theta(x, t)$: take for instance a simplistic model that is not powerful enough to approximate $\nabla_x \operatorname{log} p_t(x)$...

**2. Additional painpoints**

**2.1 Title** IMO, there is a mismatch between the title and the article: the authors do not propose a denoising score matching method, but a radically different approach that they claim to be equivalent to score matching. 

**2.2 Spectral loss** Why is a spectral loss necessary (line 241)? Aren't l2 (or l1, often more efficient) sufficient? Choosing a spectral loss seems unusual to me, maybe linking to some other papers using a similar loss would be welcome?

**2.3 Cumberstone notations** Notations are sometimes difficult to follow, maybe clarifying them would be useful. Some examples: around line 167, $p_1$ and $p_{-1}$ clash with $p_0$ and $p_T$. Around line 63, $\Delta t = t_j-t_k$, but then the authors use the convention $(t_m)_{0 \leq m \leq M}$. Why not replacing $j$ and $k$ with $m+1$ and $m$? etc...

**2.4** Shouldn't the term inside the norm in eq. (2) be updated $x_{m+1}-x_m - \Delta t \cdots$?

**2.5** line 110 of the supplementary, should (9) not be (8) instead?

**2.6** Eq. (38) in supplementary: is the brownian term not missing?

**2.7** Table 1 from supplementary: DilatedConv --> Dil-ResNet?

**2.8** Line 426 of supplementary: 100% of what?

**2.9** line 565 of supplementary: define the exact expression for $s_1$ and $s_2$.


**References:**

[1] De Bortoli, Valentin, James Thornton, Jeremy Heng, and Arnaud Doucet. ""Diffusion Schrödinger bridge with applications to score-based generative modeling."" Advances in Neural Information Processing Systems 34 (2021): 17695-17709.

[2] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. ""Diffusion posterior sampling for general noisy inverse problems."" arXiv preprint arXiv:2209.14687 (2022).

[3] Zhu, Yuanzhi, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. ""Denoising Diffusion Models for Plug-and-Play Image Restoration."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1219-1229. 2023.

[4] Kawar, Bahjat, Michael Elad, Stefano Ermon, and Jiaming Song. ""Denoising diffusion restoration models."" Advances in Neural Information Processing Systems 35 (2022): 23593-23606.

[5] Rout, Litu, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, and Sanjay Shakkottai. ""Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models."" arXiv preprint arXiv:2307.00619 (2023).

[6] Adler, Jonas, and Ozan Öktem. ""Learned primal-dual reconstruction."" IEEE transactions on medical imaging 37, no. 6 (2018): 1322-1332.",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1689246198218,,,NeurIPS.cc/2023/Conference,2BpoGPSDCR,"['NeurIPS.cc/2023/Conference/Submission1882/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410810079,[],4,,,['everyone'],2BpoGPSDCR,['NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs'],1689246198218,1702410810079,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that they believe are relevant to the paper and should be included, such as:

* [1] De Bortoli et al. (2021) - Diffusion Schrödinger bridge with applications to score-based generative modeling
* [2] Chung et al. (2022) - Diffusion posterior sampling for general noisy inverse problems
* [3] Zhu et al. (2023) - Denoising Diffusion Models for Plug-and-Play Image Restoration
* [4] Kawar et al. (2022) - Denoising diffusion restoration models
* [5] Rout et al. (2023) - Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models
* [6] Adler and Öktem (2018) - Learned primal-dual reconstruction

The reviewer also mentions that the authors should provide a more detailed review of learning-based methods for solving PDEs/stochastic PDEs, and that they should include references to physics-informed diffusion approaches for inverse problems.",1,"2021, 2022, 2023, 2022, 2023, 2018",Diffusion posterior sampling for general noisy inverse problems
YbYdC23PoT,"This paper proposes a diffusion-based unrolled strategy for learning solve ordinary differential equations (stochastic or not). After introducing the problem at stake and proposing two training strategies for solving it (namely a 1-step loss approach and a multi-step approach), the authors draw a theoretical parallel with denoising score matching and with probability flow ODEs. More precisely, the authors claim an equivalence between training an architecture with the proposed 1-step loss and training the same architecture with a score matching objective; and the equivalence between minimizing the multi-step loss and maximizing a variational lower bound. Eventually, the authors investigate the performance of the proposed approach in 4 setups, including deterministic and stochastic problems, ranging from a simple 1D experiment to a navier-stokes simulation.","- The paper investigates an original idea proposing to link traditional 1-step or multi-step losses in trajectory estimation for dynamical systems with diffusion and score matching. To the best of my knowledge, this idea is new in the literature and makes a link between traditional training approaches and diffusion processes. 
- The authors perform a large number of experiments to support their claims.","- Albeit well written, the article is difficult to follow due to not always well organised sections and very large amount of information.
- The chosen baselines do not seem very relevant, because not relying on standard losses for training denoising diffusion architectures for inverse problems.
- There is a potential problem with one of the theoretical results (Theorem 2.1)
- The literature review is not sufficient; while important and recent works are appropriately cited, there lacks references to physics informed diffusion approaches for inverse problems.","The following points list my concerns / questions / suggestions for the authors.

**1. Main concerns**

**1.1 Difficulties to follow the article** My main concern is that the paper is very difficult to follow. While overall well written, I tend to lose track of what the authors are aiming to do. For instance, in ""method overview"", the problem is not clearly stated. In my view, equation (6) of the main is the model that the authors aim at solving everywhere, but it comes in the middle of a section explaining score matching. In fact, the problem setting description from the appendix is rather clear while the first paragraph of 2 is not clear. There is a numerical update rule, but where does it come from? What is P? What is x? How are they related? Maybe if (6) had come in the introduction, the ""Problem formulation"" from section 2 would have been clearer.

**1.2 Comparison with baselines** While the theoretical explanations and intuitions link quite clearly denoising score matching with the problem of interest, the case where a network $s_{\theta}(x, t)$ is trained as a denoiser with a time embedding in a diffusion fashion does not seem to be present in the experiments (in ISM and SSM, $s_\theta(x, t)$ are not trained as denoisers if I understand correctly; please correct me if I'm wrong). Moreover, these architectures do not scale to higher dimensional problems (see line 225). As such, I wonder whether these baselines provide fair comparisons. Given the embedding of $s_\theta(x, t)$, I would suggest to use a method relying on denoising diffusion such as [HJA20]. If not applicable, maybe Diffusion Schrodinger Bridge would be a strong baseline to compare to [1]. Furthermore, given the similarities between the problem the authors want to tackle and the one from [1], I think adding a brief explanation on how the problems / approaches differ might be welcome.

**1.3 PDE litterature** I have difficulties with the relations to other works. In my opinion, section 4 comes too late and should be inserted way earlier first - probably before section 2. More precisely: in ""Learned corrections for numerical errors"", a more detailed review of learning-based methods for solving PDEs / stochastic PDEs would be helpful. 

**1.4 Lack of references to physics informed diffusion approaches** In general I believe that a large part of the imaging inverse problems literature is not mentioned. While this is not the core topic of the paper, it remains interesting in its own right since many papers have proposed methods for incorporating a measurement operator (or P(x) in the authors' words) within diffusion models, thus making the diffusion process ""physics aware"", which is precisely what the authors want to do here. A cornerstone reference, which the authors included, is [Chu+22]. However, the authors mention that this work performs uncertainty quantification, and state ""either focus on the denoising objective common for tasks involving natural images, or the synthesis process of solutions does no directly consider the underlying physics."": I disagree with this, see e.g. Figure 4 of the paper. If this reference does not convince the authors, here are other references where underlying physics / acquisition procedures are taken into account in a diffusion process: [2, 3, 4, 5]. Note also that an extensive literature in the inverse imaging literature has focused on a similar approach to your multistep loss, via architectures known as unfolded architectures incorporating the physics model inside the architecture [6].

**1.5 Proof of Theorem 2.1** I wonder whether the proof of $\Rightarrow$ is correct. My concern is with the particular sentence: ""let $\theta^*$ denote a minimizer such that $\mathcal{L}(\theta) \to 0$ as $\Delta t \to 0$. Note that at least one minimizer exists as we can choose $s_{\theta^*}(x, t) = \nabla_x \operatorname{log} p_t(x)$."" While I agree that a minimizer to this convex functional exists regardless of the nature of  $s_\theta(x, t)$, I am not sure you can assume that the minimum value of the functional tends to 0 without any further assumption on the very nature of $s_\theta(x, t)$: take for instance a simplistic model that is not powerful enough to approximate $\nabla_x \operatorname{log} p_t(x)$...

**2. Additional painpoints**

**2.1 Title** IMO, there is a mismatch between the title and the article: the authors do not propose a denoising score matching method, but a radically different approach that they claim to be equivalent to score matching. 

**2.2 Spectral loss** Why is a spectral loss necessary (line 241)? Aren't l2 (or l1, often more efficient) sufficient? Choosing a spectral loss seems unusual to me, maybe linking to some other papers using a similar loss would be welcome?

**2.3 Cumberstone notations** Notations are sometimes difficult to follow, maybe clarifying them would be useful. Some examples: around line 167, $p_1$ and $p_{-1}$ clash with $p_0$ and $p_T$. Around line 63, $\Delta t = t_j-t_k$, but then the authors use the convention $(t_m)_{0 \leq m \leq M}$. Why not replacing $j$ and $k$ with $m+1$ and $m$? etc...

**2.4** Shouldn't the term inside the norm in eq. (2) be updated $x_{m+1}-x_m - \Delta t \cdots$?

**2.5** line 110 of the supplementary, should (9) not be (8) instead?

**2.6** Eq. (38) in supplementary: is the brownian term not missing?

**2.7** Table 1 from supplementary: DilatedConv --> Dil-ResNet?

**2.8** Line 426 of supplementary: 100% of what?

**2.9** line 565 of supplementary: define the exact expression for $s_1$ and $s_2$.


**References:**

[1] De Bortoli, Valentin, James Thornton, Jeremy Heng, and Arnaud Doucet. ""Diffusion Schrödinger bridge with applications to score-based generative modeling."" Advances in Neural Information Processing Systems 34 (2021): 17695-17709.

[2] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. ""Diffusion posterior sampling for general noisy inverse problems."" arXiv preprint arXiv:2209.14687 (2022).

[3] Zhu, Yuanzhi, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. ""Denoising Diffusion Models for Plug-and-Play Image Restoration."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1219-1229. 2023.

[4] Kawar, Bahjat, Michael Elad, Stefano Ermon, and Jiaming Song. ""Denoising diffusion restoration models."" Advances in Neural Information Processing Systems 35 (2022): 23593-23606.

[5] Rout, Litu, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, and Sanjay Shakkottai. ""Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models."" arXiv preprint arXiv:2307.00619 (2023).

[6] Adler, Jonas, and Ozan Öktem. ""Learned primal-dual reconstruction."" IEEE transactions on medical imaging 37, no. 6 (2018): 1322-1332.",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1689246198218,,,NeurIPS.cc/2023/Conference,2BpoGPSDCR,"['NeurIPS.cc/2023/Conference/Submission1882/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410810079,[],4,,,['everyone'],2BpoGPSDCR,['NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs'],1689246198218,1702410810079,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that they believe are relevant to the paper and should be included, such as:

* [1] De Bortoli et al. (2021) - Diffusion Schrödinger bridge with applications to score-based generative modeling
* [2] Chung et al. (2022) - Diffusion posterior sampling for general noisy inverse problems
* [3] Zhu et al. (2023) - Denoising Diffusion Models for Plug-and-Play Image Restoration
* [4] Kawar et al. (2022) - Denoising diffusion restoration models
* [5] Rout et al. (2023) - Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models
* [6] Adler and Öktem (2018) - Learned primal-dual reconstruction

The reviewer also mentions that the authors should provide a more detailed review of learning-based methods for solving PDEs/stochastic PDEs, and that they should include references to physics-informed diffusion approaches for inverse problems.",1,"2021, 2022, 2023, 2022, 2023, 2018",Denoising Diffusion Models for Plug-and-Play Image Restoration
YbYdC23PoT,"This paper proposes a diffusion-based unrolled strategy for learning solve ordinary differential equations (stochastic or not). After introducing the problem at stake and proposing two training strategies for solving it (namely a 1-step loss approach and a multi-step approach), the authors draw a theoretical parallel with denoising score matching and with probability flow ODEs. More precisely, the authors claim an equivalence between training an architecture with the proposed 1-step loss and training the same architecture with a score matching objective; and the equivalence between minimizing the multi-step loss and maximizing a variational lower bound. Eventually, the authors investigate the performance of the proposed approach in 4 setups, including deterministic and stochastic problems, ranging from a simple 1D experiment to a navier-stokes simulation.","- The paper investigates an original idea proposing to link traditional 1-step or multi-step losses in trajectory estimation for dynamical systems with diffusion and score matching. To the best of my knowledge, this idea is new in the literature and makes a link between traditional training approaches and diffusion processes. 
- The authors perform a large number of experiments to support their claims.","- Albeit well written, the article is difficult to follow due to not always well organised sections and very large amount of information.
- The chosen baselines do not seem very relevant, because not relying on standard losses for training denoising diffusion architectures for inverse problems.
- There is a potential problem with one of the theoretical results (Theorem 2.1)
- The literature review is not sufficient; while important and recent works are appropriately cited, there lacks references to physics informed diffusion approaches for inverse problems.","The following points list my concerns / questions / suggestions for the authors.

**1. Main concerns**

**1.1 Difficulties to follow the article** My main concern is that the paper is very difficult to follow. While overall well written, I tend to lose track of what the authors are aiming to do. For instance, in ""method overview"", the problem is not clearly stated. In my view, equation (6) of the main is the model that the authors aim at solving everywhere, but it comes in the middle of a section explaining score matching. In fact, the problem setting description from the appendix is rather clear while the first paragraph of 2 is not clear. There is a numerical update rule, but where does it come from? What is P? What is x? How are they related? Maybe if (6) had come in the introduction, the ""Problem formulation"" from section 2 would have been clearer.

**1.2 Comparison with baselines** While the theoretical explanations and intuitions link quite clearly denoising score matching with the problem of interest, the case where a network $s_{\theta}(x, t)$ is trained as a denoiser with a time embedding in a diffusion fashion does not seem to be present in the experiments (in ISM and SSM, $s_\theta(x, t)$ are not trained as denoisers if I understand correctly; please correct me if I'm wrong). Moreover, these architectures do not scale to higher dimensional problems (see line 225). As such, I wonder whether these baselines provide fair comparisons. Given the embedding of $s_\theta(x, t)$, I would suggest to use a method relying on denoising diffusion such as [HJA20]. If not applicable, maybe Diffusion Schrodinger Bridge would be a strong baseline to compare to [1]. Furthermore, given the similarities between the problem the authors want to tackle and the one from [1], I think adding a brief explanation on how the problems / approaches differ might be welcome.

**1.3 PDE litterature** I have difficulties with the relations to other works. In my opinion, section 4 comes too late and should be inserted way earlier first - probably before section 2. More precisely: in ""Learned corrections for numerical errors"", a more detailed review of learning-based methods for solving PDEs / stochastic PDEs would be helpful. 

**1.4 Lack of references to physics informed diffusion approaches** In general I believe that a large part of the imaging inverse problems literature is not mentioned. While this is not the core topic of the paper, it remains interesting in its own right since many papers have proposed methods for incorporating a measurement operator (or P(x) in the authors' words) within diffusion models, thus making the diffusion process ""physics aware"", which is precisely what the authors want to do here. A cornerstone reference, which the authors included, is [Chu+22]. However, the authors mention that this work performs uncertainty quantification, and state ""either focus on the denoising objective common for tasks involving natural images, or the synthesis process of solutions does no directly consider the underlying physics."": I disagree with this, see e.g. Figure 4 of the paper. If this reference does not convince the authors, here are other references where underlying physics / acquisition procedures are taken into account in a diffusion process: [2, 3, 4, 5]. Note also that an extensive literature in the inverse imaging literature has focused on a similar approach to your multistep loss, via architectures known as unfolded architectures incorporating the physics model inside the architecture [6].

**1.5 Proof of Theorem 2.1** I wonder whether the proof of $\Rightarrow$ is correct. My concern is with the particular sentence: ""let $\theta^*$ denote a minimizer such that $\mathcal{L}(\theta) \to 0$ as $\Delta t \to 0$. Note that at least one minimizer exists as we can choose $s_{\theta^*}(x, t) = \nabla_x \operatorname{log} p_t(x)$."" While I agree that a minimizer to this convex functional exists regardless of the nature of  $s_\theta(x, t)$, I am not sure you can assume that the minimum value of the functional tends to 0 without any further assumption on the very nature of $s_\theta(x, t)$: take for instance a simplistic model that is not powerful enough to approximate $\nabla_x \operatorname{log} p_t(x)$...

**2. Additional painpoints**

**2.1 Title** IMO, there is a mismatch between the title and the article: the authors do not propose a denoising score matching method, but a radically different approach that they claim to be equivalent to score matching. 

**2.2 Spectral loss** Why is a spectral loss necessary (line 241)? Aren't l2 (or l1, often more efficient) sufficient? Choosing a spectral loss seems unusual to me, maybe linking to some other papers using a similar loss would be welcome?

**2.3 Cumberstone notations** Notations are sometimes difficult to follow, maybe clarifying them would be useful. Some examples: around line 167, $p_1$ and $p_{-1}$ clash with $p_0$ and $p_T$. Around line 63, $\Delta t = t_j-t_k$, but then the authors use the convention $(t_m)_{0 \leq m \leq M}$. Why not replacing $j$ and $k$ with $m+1$ and $m$? etc...

**2.4** Shouldn't the term inside the norm in eq. (2) be updated $x_{m+1}-x_m - \Delta t \cdots$?

**2.5** line 110 of the supplementary, should (9) not be (8) instead?

**2.6** Eq. (38) in supplementary: is the brownian term not missing?

**2.7** Table 1 from supplementary: DilatedConv --> Dil-ResNet?

**2.8** Line 426 of supplementary: 100% of what?

**2.9** line 565 of supplementary: define the exact expression for $s_1$ and $s_2$.


**References:**

[1] De Bortoli, Valentin, James Thornton, Jeremy Heng, and Arnaud Doucet. ""Diffusion Schrödinger bridge with applications to score-based generative modeling."" Advances in Neural Information Processing Systems 34 (2021): 17695-17709.

[2] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. ""Diffusion posterior sampling for general noisy inverse problems."" arXiv preprint arXiv:2209.14687 (2022).

[3] Zhu, Yuanzhi, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. ""Denoising Diffusion Models for Plug-and-Play Image Restoration."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1219-1229. 2023.

[4] Kawar, Bahjat, Michael Elad, Stefano Ermon, and Jiaming Song. ""Denoising diffusion restoration models."" Advances in Neural Information Processing Systems 35 (2022): 23593-23606.

[5] Rout, Litu, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, and Sanjay Shakkottai. ""Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models."" arXiv preprint arXiv:2307.00619 (2023).

[6] Adler, Jonas, and Ozan Öktem. ""Learned primal-dual reconstruction."" IEEE transactions on medical imaging 37, no. 6 (2018): 1322-1332.",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1689246198218,,,NeurIPS.cc/2023/Conference,2BpoGPSDCR,"['NeurIPS.cc/2023/Conference/Submission1882/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410810079,[],4,,,['everyone'],2BpoGPSDCR,['NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs'],1689246198218,1702410810079,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that they believe are relevant to the paper and should be included, such as:

* [1] De Bortoli et al. (2021) - Diffusion Schrödinger bridge with applications to score-based generative modeling
* [2] Chung et al. (2022) - Diffusion posterior sampling for general noisy inverse problems
* [3] Zhu et al. (2023) - Denoising Diffusion Models for Plug-and-Play Image Restoration
* [4] Kawar et al. (2022) - Denoising diffusion restoration models
* [5] Rout et al. (2023) - Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models
* [6] Adler and Öktem (2018) - Learned primal-dual reconstruction

The reviewer also mentions that the authors should provide a more detailed review of learning-based methods for solving PDEs/stochastic PDEs, and that they should include references to physics-informed diffusion approaches for inverse problems.",1,"2021, 2022, 2023, 2022, 2023, 2018",Denoising diffusion restoration models
YbYdC23PoT,"This paper proposes a diffusion-based unrolled strategy for learning solve ordinary differential equations (stochastic or not). After introducing the problem at stake and proposing two training strategies for solving it (namely a 1-step loss approach and a multi-step approach), the authors draw a theoretical parallel with denoising score matching and with probability flow ODEs. More precisely, the authors claim an equivalence between training an architecture with the proposed 1-step loss and training the same architecture with a score matching objective; and the equivalence between minimizing the multi-step loss and maximizing a variational lower bound. Eventually, the authors investigate the performance of the proposed approach in 4 setups, including deterministic and stochastic problems, ranging from a simple 1D experiment to a navier-stokes simulation.","- The paper investigates an original idea proposing to link traditional 1-step or multi-step losses in trajectory estimation for dynamical systems with diffusion and score matching. To the best of my knowledge, this idea is new in the literature and makes a link between traditional training approaches and diffusion processes. 
- The authors perform a large number of experiments to support their claims.","- Albeit well written, the article is difficult to follow due to not always well organised sections and very large amount of information.
- The chosen baselines do not seem very relevant, because not relying on standard losses for training denoising diffusion architectures for inverse problems.
- There is a potential problem with one of the theoretical results (Theorem 2.1)
- The literature review is not sufficient; while important and recent works are appropriately cited, there lacks references to physics informed diffusion approaches for inverse problems.","The following points list my concerns / questions / suggestions for the authors.

**1. Main concerns**

**1.1 Difficulties to follow the article** My main concern is that the paper is very difficult to follow. While overall well written, I tend to lose track of what the authors are aiming to do. For instance, in ""method overview"", the problem is not clearly stated. In my view, equation (6) of the main is the model that the authors aim at solving everywhere, but it comes in the middle of a section explaining score matching. In fact, the problem setting description from the appendix is rather clear while the first paragraph of 2 is not clear. There is a numerical update rule, but where does it come from? What is P? What is x? How are they related? Maybe if (6) had come in the introduction, the ""Problem formulation"" from section 2 would have been clearer.

**1.2 Comparison with baselines** While the theoretical explanations and intuitions link quite clearly denoising score matching with the problem of interest, the case where a network $s_{\theta}(x, t)$ is trained as a denoiser with a time embedding in a diffusion fashion does not seem to be present in the experiments (in ISM and SSM, $s_\theta(x, t)$ are not trained as denoisers if I understand correctly; please correct me if I'm wrong). Moreover, these architectures do not scale to higher dimensional problems (see line 225). As such, I wonder whether these baselines provide fair comparisons. Given the embedding of $s_\theta(x, t)$, I would suggest to use a method relying on denoising diffusion such as [HJA20]. If not applicable, maybe Diffusion Schrodinger Bridge would be a strong baseline to compare to [1]. Furthermore, given the similarities between the problem the authors want to tackle and the one from [1], I think adding a brief explanation on how the problems / approaches differ might be welcome.

**1.3 PDE litterature** I have difficulties with the relations to other works. In my opinion, section 4 comes too late and should be inserted way earlier first - probably before section 2. More precisely: in ""Learned corrections for numerical errors"", a more detailed review of learning-based methods for solving PDEs / stochastic PDEs would be helpful. 

**1.4 Lack of references to physics informed diffusion approaches** In general I believe that a large part of the imaging inverse problems literature is not mentioned. While this is not the core topic of the paper, it remains interesting in its own right since many papers have proposed methods for incorporating a measurement operator (or P(x) in the authors' words) within diffusion models, thus making the diffusion process ""physics aware"", which is precisely what the authors want to do here. A cornerstone reference, which the authors included, is [Chu+22]. However, the authors mention that this work performs uncertainty quantification, and state ""either focus on the denoising objective common for tasks involving natural images, or the synthesis process of solutions does no directly consider the underlying physics."": I disagree with this, see e.g. Figure 4 of the paper. If this reference does not convince the authors, here are other references where underlying physics / acquisition procedures are taken into account in a diffusion process: [2, 3, 4, 5]. Note also that an extensive literature in the inverse imaging literature has focused on a similar approach to your multistep loss, via architectures known as unfolded architectures incorporating the physics model inside the architecture [6].

**1.5 Proof of Theorem 2.1** I wonder whether the proof of $\Rightarrow$ is correct. My concern is with the particular sentence: ""let $\theta^*$ denote a minimizer such that $\mathcal{L}(\theta) \to 0$ as $\Delta t \to 0$. Note that at least one minimizer exists as we can choose $s_{\theta^*}(x, t) = \nabla_x \operatorname{log} p_t(x)$."" While I agree that a minimizer to this convex functional exists regardless of the nature of  $s_\theta(x, t)$, I am not sure you can assume that the minimum value of the functional tends to 0 without any further assumption on the very nature of $s_\theta(x, t)$: take for instance a simplistic model that is not powerful enough to approximate $\nabla_x \operatorname{log} p_t(x)$...

**2. Additional painpoints**

**2.1 Title** IMO, there is a mismatch between the title and the article: the authors do not propose a denoising score matching method, but a radically different approach that they claim to be equivalent to score matching. 

**2.2 Spectral loss** Why is a spectral loss necessary (line 241)? Aren't l2 (or l1, often more efficient) sufficient? Choosing a spectral loss seems unusual to me, maybe linking to some other papers using a similar loss would be welcome?

**2.3 Cumberstone notations** Notations are sometimes difficult to follow, maybe clarifying them would be useful. Some examples: around line 167, $p_1$ and $p_{-1}$ clash with $p_0$ and $p_T$. Around line 63, $\Delta t = t_j-t_k$, but then the authors use the convention $(t_m)_{0 \leq m \leq M}$. Why not replacing $j$ and $k$ with $m+1$ and $m$? etc...

**2.4** Shouldn't the term inside the norm in eq. (2) be updated $x_{m+1}-x_m - \Delta t \cdots$?

**2.5** line 110 of the supplementary, should (9) not be (8) instead?

**2.6** Eq. (38) in supplementary: is the brownian term not missing?

**2.7** Table 1 from supplementary: DilatedConv --> Dil-ResNet?

**2.8** Line 426 of supplementary: 100% of what?

**2.9** line 565 of supplementary: define the exact expression for $s_1$ and $s_2$.


**References:**

[1] De Bortoli, Valentin, James Thornton, Jeremy Heng, and Arnaud Doucet. ""Diffusion Schrödinger bridge with applications to score-based generative modeling."" Advances in Neural Information Processing Systems 34 (2021): 17695-17709.

[2] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. ""Diffusion posterior sampling for general noisy inverse problems."" arXiv preprint arXiv:2209.14687 (2022).

[3] Zhu, Yuanzhi, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. ""Denoising Diffusion Models for Plug-and-Play Image Restoration."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1219-1229. 2023.

[4] Kawar, Bahjat, Michael Elad, Stefano Ermon, and Jiaming Song. ""Denoising diffusion restoration models."" Advances in Neural Information Processing Systems 35 (2022): 23593-23606.

[5] Rout, Litu, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, and Sanjay Shakkottai. ""Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models."" arXiv preprint arXiv:2307.00619 (2023).

[6] Adler, Jonas, and Ozan Öktem. ""Learned primal-dual reconstruction."" IEEE transactions on medical imaging 37, no. 6 (2018): 1322-1332.",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1689246198218,,,NeurIPS.cc/2023/Conference,2BpoGPSDCR,"['NeurIPS.cc/2023/Conference/Submission1882/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410810079,[],4,,,['everyone'],2BpoGPSDCR,['NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs'],1689246198218,1702410810079,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that they believe are relevant to the paper and should be included, such as:

* [1] De Bortoli et al. (2021) - Diffusion Schrödinger bridge with applications to score-based generative modeling
* [2] Chung et al. (2022) - Diffusion posterior sampling for general noisy inverse problems
* [3] Zhu et al. (2023) - Denoising Diffusion Models for Plug-and-Play Image Restoration
* [4] Kawar et al. (2022) - Denoising diffusion restoration models
* [5] Rout et al. (2023) - Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models
* [6] Adler and Öktem (2018) - Learned primal-dual reconstruction

The reviewer also mentions that the authors should provide a more detailed review of learning-based methods for solving PDEs/stochastic PDEs, and that they should include references to physics-informed diffusion approaches for inverse problems.",1,"2021, 2022, 2023, 2022, 2023, 2018",Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models
YbYdC23PoT,"This paper proposes a diffusion-based unrolled strategy for learning solve ordinary differential equations (stochastic or not). After introducing the problem at stake and proposing two training strategies for solving it (namely a 1-step loss approach and a multi-step approach), the authors draw a theoretical parallel with denoising score matching and with probability flow ODEs. More precisely, the authors claim an equivalence between training an architecture with the proposed 1-step loss and training the same architecture with a score matching objective; and the equivalence between minimizing the multi-step loss and maximizing a variational lower bound. Eventually, the authors investigate the performance of the proposed approach in 4 setups, including deterministic and stochastic problems, ranging from a simple 1D experiment to a navier-stokes simulation.","- The paper investigates an original idea proposing to link traditional 1-step or multi-step losses in trajectory estimation for dynamical systems with diffusion and score matching. To the best of my knowledge, this idea is new in the literature and makes a link between traditional training approaches and diffusion processes. 
- The authors perform a large number of experiments to support their claims.","- Albeit well written, the article is difficult to follow due to not always well organised sections and very large amount of information.
- The chosen baselines do not seem very relevant, because not relying on standard losses for training denoising diffusion architectures for inverse problems.
- There is a potential problem with one of the theoretical results (Theorem 2.1)
- The literature review is not sufficient; while important and recent works are appropriately cited, there lacks references to physics informed diffusion approaches for inverse problems.","The following points list my concerns / questions / suggestions for the authors.

**1. Main concerns**

**1.1 Difficulties to follow the article** My main concern is that the paper is very difficult to follow. While overall well written, I tend to lose track of what the authors are aiming to do. For instance, in ""method overview"", the problem is not clearly stated. In my view, equation (6) of the main is the model that the authors aim at solving everywhere, but it comes in the middle of a section explaining score matching. In fact, the problem setting description from the appendix is rather clear while the first paragraph of 2 is not clear. There is a numerical update rule, but where does it come from? What is P? What is x? How are they related? Maybe if (6) had come in the introduction, the ""Problem formulation"" from section 2 would have been clearer.

**1.2 Comparison with baselines** While the theoretical explanations and intuitions link quite clearly denoising score matching with the problem of interest, the case where a network $s_{\theta}(x, t)$ is trained as a denoiser with a time embedding in a diffusion fashion does not seem to be present in the experiments (in ISM and SSM, $s_\theta(x, t)$ are not trained as denoisers if I understand correctly; please correct me if I'm wrong). Moreover, these architectures do not scale to higher dimensional problems (see line 225). As such, I wonder whether these baselines provide fair comparisons. Given the embedding of $s_\theta(x, t)$, I would suggest to use a method relying on denoising diffusion such as [HJA20]. If not applicable, maybe Diffusion Schrodinger Bridge would be a strong baseline to compare to [1]. Furthermore, given the similarities between the problem the authors want to tackle and the one from [1], I think adding a brief explanation on how the problems / approaches differ might be welcome.

**1.3 PDE litterature** I have difficulties with the relations to other works. In my opinion, section 4 comes too late and should be inserted way earlier first - probably before section 2. More precisely: in ""Learned corrections for numerical errors"", a more detailed review of learning-based methods for solving PDEs / stochastic PDEs would be helpful. 

**1.4 Lack of references to physics informed diffusion approaches** In general I believe that a large part of the imaging inverse problems literature is not mentioned. While this is not the core topic of the paper, it remains interesting in its own right since many papers have proposed methods for incorporating a measurement operator (or P(x) in the authors' words) within diffusion models, thus making the diffusion process ""physics aware"", which is precisely what the authors want to do here. A cornerstone reference, which the authors included, is [Chu+22]. However, the authors mention that this work performs uncertainty quantification, and state ""either focus on the denoising objective common for tasks involving natural images, or the synthesis process of solutions does no directly consider the underlying physics."": I disagree with this, see e.g. Figure 4 of the paper. If this reference does not convince the authors, here are other references where underlying physics / acquisition procedures are taken into account in a diffusion process: [2, 3, 4, 5]. Note also that an extensive literature in the inverse imaging literature has focused on a similar approach to your multistep loss, via architectures known as unfolded architectures incorporating the physics model inside the architecture [6].

**1.5 Proof of Theorem 2.1** I wonder whether the proof of $\Rightarrow$ is correct. My concern is with the particular sentence: ""let $\theta^*$ denote a minimizer such that $\mathcal{L}(\theta) \to 0$ as $\Delta t \to 0$. Note that at least one minimizer exists as we can choose $s_{\theta^*}(x, t) = \nabla_x \operatorname{log} p_t(x)$."" While I agree that a minimizer to this convex functional exists regardless of the nature of  $s_\theta(x, t)$, I am not sure you can assume that the minimum value of the functional tends to 0 without any further assumption on the very nature of $s_\theta(x, t)$: take for instance a simplistic model that is not powerful enough to approximate $\nabla_x \operatorname{log} p_t(x)$...

**2. Additional painpoints**

**2.1 Title** IMO, there is a mismatch between the title and the article: the authors do not propose a denoising score matching method, but a radically different approach that they claim to be equivalent to score matching. 

**2.2 Spectral loss** Why is a spectral loss necessary (line 241)? Aren't l2 (or l1, often more efficient) sufficient? Choosing a spectral loss seems unusual to me, maybe linking to some other papers using a similar loss would be welcome?

**2.3 Cumberstone notations** Notations are sometimes difficult to follow, maybe clarifying them would be useful. Some examples: around line 167, $p_1$ and $p_{-1}$ clash with $p_0$ and $p_T$. Around line 63, $\Delta t = t_j-t_k$, but then the authors use the convention $(t_m)_{0 \leq m \leq M}$. Why not replacing $j$ and $k$ with $m+1$ and $m$? etc...

**2.4** Shouldn't the term inside the norm in eq. (2) be updated $x_{m+1}-x_m - \Delta t \cdots$?

**2.5** line 110 of the supplementary, should (9) not be (8) instead?

**2.6** Eq. (38) in supplementary: is the brownian term not missing?

**2.7** Table 1 from supplementary: DilatedConv --> Dil-ResNet?

**2.8** Line 426 of supplementary: 100% of what?

**2.9** line 565 of supplementary: define the exact expression for $s_1$ and $s_2$.


**References:**

[1] De Bortoli, Valentin, James Thornton, Jeremy Heng, and Arnaud Doucet. ""Diffusion Schrödinger bridge with applications to score-based generative modeling."" Advances in Neural Information Processing Systems 34 (2021): 17695-17709.

[2] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. ""Diffusion posterior sampling for general noisy inverse problems."" arXiv preprint arXiv:2209.14687 (2022).

[3] Zhu, Yuanzhi, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. ""Denoising Diffusion Models for Plug-and-Play Image Restoration."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1219-1229. 2023.

[4] Kawar, Bahjat, Michael Elad, Stefano Ermon, and Jiaming Song. ""Denoising diffusion restoration models."" Advances in Neural Information Processing Systems 35 (2022): 23593-23606.

[5] Rout, Litu, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, and Sanjay Shakkottai. ""Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models."" arXiv preprint arXiv:2307.00619 (2023).

[6] Adler, Jonas, and Ozan Öktem. ""Learned primal-dual reconstruction."" IEEE transactions on medical imaging 37, no. 6 (2018): 1322-1332.",Yes.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1689246198218,,,NeurIPS.cc/2023/Conference,2BpoGPSDCR,"['NeurIPS.cc/2023/Conference/Submission1882/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410810079,[],4,,,['everyone'],2BpoGPSDCR,['NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs'],1689246198218,1702410810079,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1882/Reviewer_riEs']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that they believe are relevant to the paper and should be included, such as:

* [1] De Bortoli et al. (2021) - Diffusion Schrödinger bridge with applications to score-based generative modeling
* [2] Chung et al. (2022) - Diffusion posterior sampling for general noisy inverse problems
* [3] Zhu et al. (2023) - Denoising Diffusion Models for Plug-and-Play Image Restoration
* [4] Kawar et al. (2022) - Denoising diffusion restoration models
* [5] Rout et al. (2023) - Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models
* [6] Adler and Öktem (2018) - Learned primal-dual reconstruction

The reviewer also mentions that the authors should provide a more detailed review of learning-based methods for solving PDEs/stochastic PDEs, and that they should include references to physics-informed diffusion approaches for inverse problems.",1,"2021, 2022, 2023, 2022, 2023, 2018",Learned primal-dual reconstruction
873EnOCKyA,"The paper proposes a new optimization method called Sharpness-Aware Minimization (SAM) that aims to improve generalization performance in deep learning models. The authors demonstrate that SAM can effectively reduce the generalization gap and improve the accuracy of various models on different datasets. The paper also provides a theoretical analysis of SAM and shows that it encourages the optimization process to converge to flatter minima, which can lead to better generalization. Overall, the paper presents a novel and promising approach to improving generalization in deep learning models.","1. The paper's contributions are significant in several ways. First, SAM is a promising optimization method that can improve the generalization performance of deep learning models. Second, the paper provides a mechanistic understanding of how SAM leads to low-rank features in neural networks, which can have implications for more efficient feature quantization and nearest neighbor retrieval. Finally, the paper's theoretical analysis of SAM can provide insights into the optimization landscape of deep learning models, which can lead to further improvements in optimization methods. Overall, the paper is a significant contribution to the field of deep learning optimization.

2. The paper presents a thorough empirical evaluation of SAM on various deep learning models and datasets. The authors demonstrate that SAM can effectively reduce the generalization gap and improve the accuracy of the models. The paper also provides a mechanistic understanding of how SAM leads to low-rank features in neural networks, which is a valuable contribution to the field.

3. The paper introduces a novel optimization method called Sharpness-Aware Minimization (SAM) that is different from traditional optimization methods. SAM aims to minimize the sharpness of the loss function, which encourages the optimization process to converge to flatter minima. This approach is different from other methods that focus on minimizing the loss function itself or its gradient. The authors also provide a theoretical analysis of SAM, which further demonstrates its originality.","1. Sensitivity to batch size: Since sharp minima are often observed in large batch size training, which is becoming increasingly important in current large model-based methods such as batch normalization and dropout, it is crucial to investigate the relationship between batch size and performance in the proposed method. However, the authors only show the results for batch sizes of 128 and 256 in this paper and the appendix, which limits the generalizability of the findings.

2. The observation that sharp minima hurt generalization is easier to make in large-scale training, especially for large models. Therefore, it is recommended that the authors test their proposed method on large-scale training, such as CLIP. Conducting research on the low-rank effect on larger models, such as CLIP or other large text-image training, would make the proposed method more valuable, considering the current trend.

3. The authors are recommended to compare and discuss a recent related work, ""Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization,"" which is a highlight paper at CVPR2023.

Note: If the authors respond with experiments, even preliminary results are acceptable. For example, showing the relationship between batch size and performance by investigating a few batch size training settings would be sufficient. Additionally, presenting some simple fine-tuning results on pre-trained CLIP using the proposed method would be highly appreciated.",Questions: Please see Weaknesses. I would like to update my evaluation after the discussion.,N.A.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,4 excellent,4 excellent,1688690494316,,,NeurIPS.cc/2023/Conference,29WbraPk8U,"['NeurIPS.cc/2023/Conference/Submission456/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410745308,[],3,,,['everyone'],29WbraPk8U,['NeurIPS.cc/2023/Conference/Submission456/Reviewer_fXy2'],1688690494316,1702410745308,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission456/Reviewer_fXy2']","Yes, this peer review explicitly suggests the authors of the paper to cite and compare with a specific literature: ""Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization,"" which is a highlight paper at CVPR2023.",1,,Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization
OybtbzZKxT,"This paper dives into the study of webly-supervised learning and aims to utilize the neglected alt-text of web images to enhance the learning process. To this end, the authors propose the approach called Cross-modality Aligned Prototypes (CAPro). CAPro is adopted with two modules, namely, text matching&enhacement, and collective bootstrapping. The former aims to assign text to the corresponding prototypes by resorting to the LLMs and cross-modal nearest neighbor mechanism. The latter provides smoother label reference by extending bootstrapping with collective knowledge. Extensive experiments have been conducted on several benchmarks to verify the effectiveness of the proposed CAPro.","Most existing webly-supervised learning works mainly focus on the web images and corresponding (noisy) labels while omitting the potential alt-texts (captions). This paper provides a new perspective that uses the alt-text to complement the webly-supervised learning. From this point, the motivation and idea of this paper are novel and interesting.
","1. My major concern is the differences between this work and the existing problem or techniques including the noisy correspondence, noise-robust learning from NNs, and noise removal. First, this paper claims that the existing webly-supervised learning works mainly address certain types of noise including label-flipping noise and out-of-distribution (OOD), while neglecting the semantic noise, namely, the misalignment between image contents and the associated texts. The claim might be correct for the webly-supervised learning community. However, the so-called semantic noise is very similar to the definition of noisy correspondence [61,62, 65]. The authors should provide more discussion to clarify the differences between the so-called noisy correspondence and semantic noise. If the two problems are somewhat similar, I think it would be better to give more discussion on the related works like 'Noisy Correspondence Learning with Meta Similarity Correction'. Second, the differences between the used KNN-graph mechanism and the works in 'Noise-Robust Learning from Neighbors' are encouraged to be further discussed. Third, the proposed noise removal strategy seems to be similar to the sophisticated WSL method (MoPro), which is an important baseline for WSL learning. 
2. The performance improvement is limited (See Table 1). This paper adopts a relatively complex pipeline and additional resort to the existing LLMs. However, the performance is marginal compared to the sophisticated WSL baseline (MoPro, ICLR 2020). 
2. There are some typos and unclear statements. For example: 
i) the definition of 'concept definition texts (Line 56-57)' is lacking
ii) 'with visual guidance from image neighbors (Line 61)' is unclear.
iii) 'selection We' (Line 62) ",My major concern is the  differences between this work and the existing problem and techniques as eloborated on in Weaknesses. Thanks,The authors have discussed the limitations.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688375280469,,,NeurIPS.cc/2023/Conference,25HiFHPcXg,"['NeurIPS.cc/2023/Conference/Submission455/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410745264,[],2,,,['everyone'],25HiFHPcXg,['NeurIPS.cc/2023/Conference/Submission455/Reviewer_SPGi'],1688375280469,1702410745264,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission455/Reviewer_SPGi']","Yes, the peer review explicitly suggests the authors of the paper to cite and discuss specific literature, including:

1. ""Noisy Correspondence Learning with Meta Similarity Correction"" to clarify the differences between the so-called ""semantic noise"" and ""noisy correspondence"".
2. Works related to ""Noise-Robust Learning from Neighbors"" to discuss the differences between the used KNN-graph mechanism and existing methods.
3. References [61, 62, 65] which are related to ""noisy correspondence"".

The reviewer is asking the authors to provide more discussion on the related works to clarify the differences between the proposed approach and existing techniques.",1,,Noisy Correspondence Learning with Meta Similarity Correction
u78CXxuHHt,"This paper introduces DiffuseBot, a physics-augmented diffusion model designed for generating and optimizing the morphologies and control mechanisms of soft robots. DiffuseBot aims to bridge the gap between virtually generated content and physical utility in the domain of soft robotics. Firstly, it combines the diffusion process with a physical simulation that serves as a performance certificate, thereby ensuring the feasibility and effectiveness of the generated designs. Secondly, it details a co-design procedure that simultaneously optimizes the physical design and control of the soft robots, leveraging insights from differentiable simulation. The paper validates the efficacy of this approach by presenting a variety of both simulated and physically fabricated robots, along with their diverse capabilities.","1. In general, the paper is well written, with only minor flaws. Even those unfamiliar with soft robot design will find the paper easy to comprehend.

2. Although diffusion models are expressive and powerful, their performance for tasks dealing with physical tasks often falls short. Thus, injecting a physics prior or 'physics-augmented diffusion model' is crucial. I think the method proposed in this paper is interesting and promising. 

3. The evaluation is comprehensive and thoughtful. The physical robot is impressive.
","Overall, I did not identify any major weaknesses in the paper, but here are a few points that could strengthen it:

1. While the writing is generally clear, certain sections could benefit from clearer exposition, such as:
*  The section on diffusion as co-design is not very intuitive, especially for audiences not familiar with soft robot design. Specifically, it should be clearer how gradient-based optimization benefits robot design and what exactly line 152's ""synergy"" means.
* It would be helpful if the authors clarify that the ""condition"" in this work actually refers to text.
2. The robot's actuator and stiffness seem oversimplified, having only constant stiffness. Given that the gradient of $\Psi_{act}$ is almost zero, it appears that the actuator and stiffness are solely determined by the geometry.
3. A similar idea of tuning in the embedding space is proposed in[1]. A discussion and connection to this existing work could be interesting.  
4. In general, the method the paper uses to inject a physics prior into the generation process could be applicable to more general scenarios. Works like Diffuser[2] or Decision Diffuser[3] generate state sequences with diffusion models, but the generated states can sometimes be physically implausible. A deeper discussion about the potential of the method could make the paper stronger.

[1] Gal, Rinon, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik and Daniel Cohen-Or. “An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.”, ICLR, 2023.

[2] Janner, Michael, Yilun Du, Joshua B. Tenenbaum and Sergey Levine. “Planning with Diffusion for Flexible Behavior Synthesis.”, ICML, 2022. 

[3] Ajay, Anurag, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, T. Jaakkola and Pulkit Agrawal. “Is Conditional Generative Modeling all you need for Decision-Making?” ICLR, 2023. ","
1. I do not fully understand how the k-means clustering is performed for actuator and stiffness generation. Specifically, what kind of feature is used for clustering?

2. In line 86, which structural biases are you referring to ?
",see weakness.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",,Yes,3 good,3 good,3 good,1689063372607,,,NeurIPS.cc/2023/Conference,1zo4iioUEs,"['NeurIPS.cc/2023/Conference/Submission10463/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411286621,[],3,,,['everyone'],1zo4iioUEs,['NeurIPS.cc/2023/Conference/Submission10463/Reviewer_zr6A'],1689063372607,1702411286621,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10463/Reviewer_zr6A']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions four specific papers:

1. Gal et al. (2023) - ""An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.""
2. Janner et al. (2022) - ""Planning with Diffusion for Flexible Behavior Synthesis.""
3. Ajay et al. (2023) - ""Is Conditional Generative Modeling all you need for Decision-Making?""

The reviewer suggests that discussing and connecting the proposed method to these existing works could strengthen the paper, particularly in regards to the idea of tuning in the embedding space and the potential applicability of the method to more general scenarios.",1,"2023, 2022, 2023",An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion
u78CXxuHHt,"This paper introduces DiffuseBot, a physics-augmented diffusion model designed for generating and optimizing the morphologies and control mechanisms of soft robots. DiffuseBot aims to bridge the gap between virtually generated content and physical utility in the domain of soft robotics. Firstly, it combines the diffusion process with a physical simulation that serves as a performance certificate, thereby ensuring the feasibility and effectiveness of the generated designs. Secondly, it details a co-design procedure that simultaneously optimizes the physical design and control of the soft robots, leveraging insights from differentiable simulation. The paper validates the efficacy of this approach by presenting a variety of both simulated and physically fabricated robots, along with their diverse capabilities.","1. In general, the paper is well written, with only minor flaws. Even those unfamiliar with soft robot design will find the paper easy to comprehend.

2. Although diffusion models are expressive and powerful, their performance for tasks dealing with physical tasks often falls short. Thus, injecting a physics prior or 'physics-augmented diffusion model' is crucial. I think the method proposed in this paper is interesting and promising. 

3. The evaluation is comprehensive and thoughtful. The physical robot is impressive.
","Overall, I did not identify any major weaknesses in the paper, but here are a few points that could strengthen it:

1. While the writing is generally clear, certain sections could benefit from clearer exposition, such as:
*  The section on diffusion as co-design is not very intuitive, especially for audiences not familiar with soft robot design. Specifically, it should be clearer how gradient-based optimization benefits robot design and what exactly line 152's ""synergy"" means.
* It would be helpful if the authors clarify that the ""condition"" in this work actually refers to text.
2. The robot's actuator and stiffness seem oversimplified, having only constant stiffness. Given that the gradient of $\Psi_{act}$ is almost zero, it appears that the actuator and stiffness are solely determined by the geometry.
3. A similar idea of tuning in the embedding space is proposed in[1]. A discussion and connection to this existing work could be interesting.  
4. In general, the method the paper uses to inject a physics prior into the generation process could be applicable to more general scenarios. Works like Diffuser[2] or Decision Diffuser[3] generate state sequences with diffusion models, but the generated states can sometimes be physically implausible. A deeper discussion about the potential of the method could make the paper stronger.

[1] Gal, Rinon, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik and Daniel Cohen-Or. “An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.”, ICLR, 2023.

[2] Janner, Michael, Yilun Du, Joshua B. Tenenbaum and Sergey Levine. “Planning with Diffusion for Flexible Behavior Synthesis.”, ICML, 2022. 

[3] Ajay, Anurag, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, T. Jaakkola and Pulkit Agrawal. “Is Conditional Generative Modeling all you need for Decision-Making?” ICLR, 2023. ","
1. I do not fully understand how the k-means clustering is performed for actuator and stiffness generation. Specifically, what kind of feature is used for clustering?

2. In line 86, which structural biases are you referring to ?
",see weakness.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",,Yes,3 good,3 good,3 good,1689063372607,,,NeurIPS.cc/2023/Conference,1zo4iioUEs,"['NeurIPS.cc/2023/Conference/Submission10463/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411286621,[],3,,,['everyone'],1zo4iioUEs,['NeurIPS.cc/2023/Conference/Submission10463/Reviewer_zr6A'],1689063372607,1702411286621,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10463/Reviewer_zr6A']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions four specific papers:

1. Gal et al. (2023) - ""An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.""
2. Janner et al. (2022) - ""Planning with Diffusion for Flexible Behavior Synthesis.""
3. Ajay et al. (2023) - ""Is Conditional Generative Modeling all you need for Decision-Making?""

The reviewer suggests that discussing and connecting the proposed method to these existing works could strengthen the paper, particularly in regards to the idea of tuning in the embedding space and the potential applicability of the method to more general scenarios.",1,"2023, 2022, 2023",Planning with Diffusion for Flexible Behavior Synthesis
u78CXxuHHt,"This paper introduces DiffuseBot, a physics-augmented diffusion model designed for generating and optimizing the morphologies and control mechanisms of soft robots. DiffuseBot aims to bridge the gap between virtually generated content and physical utility in the domain of soft robotics. Firstly, it combines the diffusion process with a physical simulation that serves as a performance certificate, thereby ensuring the feasibility and effectiveness of the generated designs. Secondly, it details a co-design procedure that simultaneously optimizes the physical design and control of the soft robots, leveraging insights from differentiable simulation. The paper validates the efficacy of this approach by presenting a variety of both simulated and physically fabricated robots, along with their diverse capabilities.","1. In general, the paper is well written, with only minor flaws. Even those unfamiliar with soft robot design will find the paper easy to comprehend.

2. Although diffusion models are expressive and powerful, their performance for tasks dealing with physical tasks often falls short. Thus, injecting a physics prior or 'physics-augmented diffusion model' is crucial. I think the method proposed in this paper is interesting and promising. 

3. The evaluation is comprehensive and thoughtful. The physical robot is impressive.
","Overall, I did not identify any major weaknesses in the paper, but here are a few points that could strengthen it:

1. While the writing is generally clear, certain sections could benefit from clearer exposition, such as:
*  The section on diffusion as co-design is not very intuitive, especially for audiences not familiar with soft robot design. Specifically, it should be clearer how gradient-based optimization benefits robot design and what exactly line 152's ""synergy"" means.
* It would be helpful if the authors clarify that the ""condition"" in this work actually refers to text.
2. The robot's actuator and stiffness seem oversimplified, having only constant stiffness. Given that the gradient of $\Psi_{act}$ is almost zero, it appears that the actuator and stiffness are solely determined by the geometry.
3. A similar idea of tuning in the embedding space is proposed in[1]. A discussion and connection to this existing work could be interesting.  
4. In general, the method the paper uses to inject a physics prior into the generation process could be applicable to more general scenarios. Works like Diffuser[2] or Decision Diffuser[3] generate state sequences with diffusion models, but the generated states can sometimes be physically implausible. A deeper discussion about the potential of the method could make the paper stronger.

[1] Gal, Rinon, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik and Daniel Cohen-Or. “An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.”, ICLR, 2023.

[2] Janner, Michael, Yilun Du, Joshua B. Tenenbaum and Sergey Levine. “Planning with Diffusion for Flexible Behavior Synthesis.”, ICML, 2022. 

[3] Ajay, Anurag, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, T. Jaakkola and Pulkit Agrawal. “Is Conditional Generative Modeling all you need for Decision-Making?” ICLR, 2023. ","
1. I do not fully understand how the k-means clustering is performed for actuator and stiffness generation. Specifically, what kind of feature is used for clustering?

2. In line 86, which structural biases are you referring to ?
",see weakness.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",,Yes,3 good,3 good,3 good,1689063372607,,,NeurIPS.cc/2023/Conference,1zo4iioUEs,"['NeurIPS.cc/2023/Conference/Submission10463/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411286621,[],3,,,['everyone'],1zo4iioUEs,['NeurIPS.cc/2023/Conference/Submission10463/Reviewer_zr6A'],1689063372607,1702411286621,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10463/Reviewer_zr6A']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions four specific papers:

1. Gal et al. (2023) - ""An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.""
2. Janner et al. (2022) - ""Planning with Diffusion for Flexible Behavior Synthesis.""
3. Ajay et al. (2023) - ""Is Conditional Generative Modeling all you need for Decision-Making?""

The reviewer suggests that discussing and connecting the proposed method to these existing works could strengthen the paper, particularly in regards to the idea of tuning in the embedding space and the potential applicability of the method to more general scenarios.",1,"2023, 2022, 2023",Is Conditional Generative Modeling all you need for Decision-Making
5bUGQxRnce,"This paper introduces a new measure of visual artificial network computation 'time', $\xi_{cRNN}$, as the time-averaged uncertainty of a convolutional RNN trained with an evidential deep learning loss (Sensoy et al.). The authors proceed to analyse the dynamics of this network as it solves a range of classification tasks. Importantly, beyond the network learning to solve the tasks, $\xi_{cRNN}$ is generally well correlated with human reaction times, with which it also shares several qualitative features across tasks.","Deep network models of the visual system have often focused purely on classification accuracy and neural representations when comparing to biological data. However, this neglects an important aspect of human behaviour, namely the time it takes to arrive at a decision. The present paper tackles this important question using recent ideas from the machine learning literature and represents an interesting step in the direction of capturing the temporal variability in human behaviour.
The authors analyse an impressive breadth of tasks and behavioural data and include many interesting analyses, both qualitative and quantitative.","The major weakness of the submission is that while there are strong _correlations_ between $\xi_{cRNN}$ and human reaction times, there is less evidence that $\xi_{cRNN}$ is mechanistically similar to human reaction times, since it is fundamentally a measure of uncertainty rather than computation time. In particular, human reaction times generally involve a tradeoff between computation/evidence accumulation and decision making (as in common drift diffusion models). On the contrary, the cRNN has a fixed computational budget and has no need or even capacity for evaluating this tradeoff. This is in contrast to a few previous deep learning models in the literature that are capable of explicitly trading off computation and actions (e.g. Graves et al., Pascanu et al., and refs by the authors in the ML literature, and Jensen et al. in the Neuro/Cogsci literature). These considerations are important because e.g. task difficulty is likely to correlate with both uncertainty and reaction time, and this raises the question of whether a model of one is automatically a model of the other.

It might the interesting to compare the current model to alternative models that more explicitly have adaptive/variable computation time, such as a cRNN that computes until $\epsilon$ reaches a certain threshold (akin to classical drift diffusion models).

As the authors mention in L326, another potential weakness is that the present approach does not easily generalize beyond classification tasks, which form a small subset of the types of problems humans and animals are faced with during natural behaviour.

_References:_ \
Graves et al.: ""Adaptive computation time for recurrent neural networks"", arXiv 2016.\
Pascanu et al.: ""Learning model-based planning from scratch"", arXiv 2017.\
Jensen et al.: ""A recurrent network model of planning explains hippocampal replay and human behavior"", bioRxiv 2023.\
Bansal et al.: ""End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking"", NeurIPS 2022.","L106: what does 'without loss of generality' mean here?

L123 ""our control models lack inference-time flexibility."" What does this mean?

L144: It might be worth comparing to or discussing recent work by Bansal et al. (NeurIPS 2022), which develops a way of stably training convolutional RNNs with long computation times that generalize from small to large problem settings.

L156 ""for a complete mathematical specification of this loss, refer to SI 1"". $\epsilon$ does not seem to be defined in either the main text or SI beyond a qualitative description as 'uncertainty'. It would be good to provide a precise mathematical definition somewhere.

L204: it would be good to briefly explain how this distance measure by Jeurissen et al. is defined (or at least refer to SI where it can be described in more detail).

Figure 3b and 5c: it's not entirely clear to me how these 'activity maps' are computed?

Figure 7b: it might be worth talking a bit more about the differences between the human and model data here instead of doing linear regression and calling it a day. The cRNN data is decidedly not linear and exhibits something resembling a plateau followed by a sharp drop at high discriminability. This seems rather different from the monotonic decrease in human RT, and the authors could perhaps speculate on or discuss this difference.

Supplementary material: it would be worth having a separate folder with just a few example videos of each condition in addition to the larger set of videos currently provided.",The authors have adequately discussed the limitations of the work.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1686804385884,,,NeurIPS.cc/2023/Conference,1xPsn2gCOe,"['NeurIPS.cc/2023/Conference/Submission8651/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411182256,[],1,,,['everyone'],1xPsn2gCOe,['NeurIPS.cc/2023/Conference/Submission8651/Reviewer_4B3C'],1686804385884,1702411182256,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8651/Reviewer_4B3C']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Graves et al.: ""Adaptive computation time for recurrent neural networks"", arXiv 2016.
2. Pascanu et al.: ""Learning model-based planning from scratch"", arXiv 2017.
3. Jensen et al.: ""A recurrent network model of planning explains hippocampal replay and human behavior"", bioRxiv 2023.
4. Bansal et al.: ""End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking"", NeurIPS 2022.

The reviewer also mentions that it might be worth comparing or discussing the work by Bansal et al. (NeurIPS 2022) in the context of stably training convolutional RNNs with long computation times.",1,"2020, 2020, 2023, 2022, 2022",Adaptive computation time for recurrent neural networks 
5bUGQxRnce,"This paper introduces a new measure of visual artificial network computation 'time', $\xi_{cRNN}$, as the time-averaged uncertainty of a convolutional RNN trained with an evidential deep learning loss (Sensoy et al.). The authors proceed to analyse the dynamics of this network as it solves a range of classification tasks. Importantly, beyond the network learning to solve the tasks, $\xi_{cRNN}$ is generally well correlated with human reaction times, with which it also shares several qualitative features across tasks.","Deep network models of the visual system have often focused purely on classification accuracy and neural representations when comparing to biological data. However, this neglects an important aspect of human behaviour, namely the time it takes to arrive at a decision. The present paper tackles this important question using recent ideas from the machine learning literature and represents an interesting step in the direction of capturing the temporal variability in human behaviour.
The authors analyse an impressive breadth of tasks and behavioural data and include many interesting analyses, both qualitative and quantitative.","The major weakness of the submission is that while there are strong _correlations_ between $\xi_{cRNN}$ and human reaction times, there is less evidence that $\xi_{cRNN}$ is mechanistically similar to human reaction times, since it is fundamentally a measure of uncertainty rather than computation time. In particular, human reaction times generally involve a tradeoff between computation/evidence accumulation and decision making (as in common drift diffusion models). On the contrary, the cRNN has a fixed computational budget and has no need or even capacity for evaluating this tradeoff. This is in contrast to a few previous deep learning models in the literature that are capable of explicitly trading off computation and actions (e.g. Graves et al., Pascanu et al., and refs by the authors in the ML literature, and Jensen et al. in the Neuro/Cogsci literature). These considerations are important because e.g. task difficulty is likely to correlate with both uncertainty and reaction time, and this raises the question of whether a model of one is automatically a model of the other.

It might the interesting to compare the current model to alternative models that more explicitly have adaptive/variable computation time, such as a cRNN that computes until $\epsilon$ reaches a certain threshold (akin to classical drift diffusion models).

As the authors mention in L326, another potential weakness is that the present approach does not easily generalize beyond classification tasks, which form a small subset of the types of problems humans and animals are faced with during natural behaviour.

_References:_ \
Graves et al.: ""Adaptive computation time for recurrent neural networks"", arXiv 2016.\
Pascanu et al.: ""Learning model-based planning from scratch"", arXiv 2017.\
Jensen et al.: ""A recurrent network model of planning explains hippocampal replay and human behavior"", bioRxiv 2023.\
Bansal et al.: ""End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking"", NeurIPS 2022.","L106: what does 'without loss of generality' mean here?

L123 ""our control models lack inference-time flexibility."" What does this mean?

L144: It might be worth comparing to or discussing recent work by Bansal et al. (NeurIPS 2022), which develops a way of stably training convolutional RNNs with long computation times that generalize from small to large problem settings.

L156 ""for a complete mathematical specification of this loss, refer to SI 1"". $\epsilon$ does not seem to be defined in either the main text or SI beyond a qualitative description as 'uncertainty'. It would be good to provide a precise mathematical definition somewhere.

L204: it would be good to briefly explain how this distance measure by Jeurissen et al. is defined (or at least refer to SI where it can be described in more detail).

Figure 3b and 5c: it's not entirely clear to me how these 'activity maps' are computed?

Figure 7b: it might be worth talking a bit more about the differences between the human and model data here instead of doing linear regression and calling it a day. The cRNN data is decidedly not linear and exhibits something resembling a plateau followed by a sharp drop at high discriminability. This seems rather different from the monotonic decrease in human RT, and the authors could perhaps speculate on or discuss this difference.

Supplementary material: it would be worth having a separate folder with just a few example videos of each condition in addition to the larger set of videos currently provided.",The authors have adequately discussed the limitations of the work.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1686804385884,,,NeurIPS.cc/2023/Conference,1xPsn2gCOe,"['NeurIPS.cc/2023/Conference/Submission8651/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411182256,[],1,,,['everyone'],1xPsn2gCOe,['NeurIPS.cc/2023/Conference/Submission8651/Reviewer_4B3C'],1686804385884,1702411182256,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8651/Reviewer_4B3C']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Graves et al.: ""Adaptive computation time for recurrent neural networks"", arXiv 2016.
2. Pascanu et al.: ""Learning model-based planning from scratch"", arXiv 2017.
3. Jensen et al.: ""A recurrent network model of planning explains hippocampal replay and human behavior"", bioRxiv 2023.
4. Bansal et al.: ""End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking"", NeurIPS 2022.

The reviewer also mentions that it might be worth comparing or discussing the work by Bansal et al. (NeurIPS 2022) in the context of stably training convolutional RNNs with long computation times.",1,"2020, 2020, 2023, 2022, 2022",Learning model-based planning from scratch 
5bUGQxRnce,"This paper introduces a new measure of visual artificial network computation 'time', $\xi_{cRNN}$, as the time-averaged uncertainty of a convolutional RNN trained with an evidential deep learning loss (Sensoy et al.). The authors proceed to analyse the dynamics of this network as it solves a range of classification tasks. Importantly, beyond the network learning to solve the tasks, $\xi_{cRNN}$ is generally well correlated with human reaction times, with which it also shares several qualitative features across tasks.","Deep network models of the visual system have often focused purely on classification accuracy and neural representations when comparing to biological data. However, this neglects an important aspect of human behaviour, namely the time it takes to arrive at a decision. The present paper tackles this important question using recent ideas from the machine learning literature and represents an interesting step in the direction of capturing the temporal variability in human behaviour.
The authors analyse an impressive breadth of tasks and behavioural data and include many interesting analyses, both qualitative and quantitative.","The major weakness of the submission is that while there are strong _correlations_ between $\xi_{cRNN}$ and human reaction times, there is less evidence that $\xi_{cRNN}$ is mechanistically similar to human reaction times, since it is fundamentally a measure of uncertainty rather than computation time. In particular, human reaction times generally involve a tradeoff between computation/evidence accumulation and decision making (as in common drift diffusion models). On the contrary, the cRNN has a fixed computational budget and has no need or even capacity for evaluating this tradeoff. This is in contrast to a few previous deep learning models in the literature that are capable of explicitly trading off computation and actions (e.g. Graves et al., Pascanu et al., and refs by the authors in the ML literature, and Jensen et al. in the Neuro/Cogsci literature). These considerations are important because e.g. task difficulty is likely to correlate with both uncertainty and reaction time, and this raises the question of whether a model of one is automatically a model of the other.

It might the interesting to compare the current model to alternative models that more explicitly have adaptive/variable computation time, such as a cRNN that computes until $\epsilon$ reaches a certain threshold (akin to classical drift diffusion models).

As the authors mention in L326, another potential weakness is that the present approach does not easily generalize beyond classification tasks, which form a small subset of the types of problems humans and animals are faced with during natural behaviour.

_References:_ \
Graves et al.: ""Adaptive computation time for recurrent neural networks"", arXiv 2016.\
Pascanu et al.: ""Learning model-based planning from scratch"", arXiv 2017.\
Jensen et al.: ""A recurrent network model of planning explains hippocampal replay and human behavior"", bioRxiv 2023.\
Bansal et al.: ""End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking"", NeurIPS 2022.","L106: what does 'without loss of generality' mean here?

L123 ""our control models lack inference-time flexibility."" What does this mean?

L144: It might be worth comparing to or discussing recent work by Bansal et al. (NeurIPS 2022), which develops a way of stably training convolutional RNNs with long computation times that generalize from small to large problem settings.

L156 ""for a complete mathematical specification of this loss, refer to SI 1"". $\epsilon$ does not seem to be defined in either the main text or SI beyond a qualitative description as 'uncertainty'. It would be good to provide a precise mathematical definition somewhere.

L204: it would be good to briefly explain how this distance measure by Jeurissen et al. is defined (or at least refer to SI where it can be described in more detail).

Figure 3b and 5c: it's not entirely clear to me how these 'activity maps' are computed?

Figure 7b: it might be worth talking a bit more about the differences between the human and model data here instead of doing linear regression and calling it a day. The cRNN data is decidedly not linear and exhibits something resembling a plateau followed by a sharp drop at high discriminability. This seems rather different from the monotonic decrease in human RT, and the authors could perhaps speculate on or discuss this difference.

Supplementary material: it would be worth having a separate folder with just a few example videos of each condition in addition to the larger set of videos currently provided.",The authors have adequately discussed the limitations of the work.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1686804385884,,,NeurIPS.cc/2023/Conference,1xPsn2gCOe,"['NeurIPS.cc/2023/Conference/Submission8651/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411182256,[],1,,,['everyone'],1xPsn2gCOe,['NeurIPS.cc/2023/Conference/Submission8651/Reviewer_4B3C'],1686804385884,1702411182256,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8651/Reviewer_4B3C']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Graves et al.: ""Adaptive computation time for recurrent neural networks"", arXiv 2016.
2. Pascanu et al.: ""Learning model-based planning from scratch"", arXiv 2017.
3. Jensen et al.: ""A recurrent network model of planning explains hippocampal replay and human behavior"", bioRxiv 2023.
4. Bansal et al.: ""End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking"", NeurIPS 2022.

The reviewer also mentions that it might be worth comparing or discussing the work by Bansal et al. (NeurIPS 2022) in the context of stably training convolutional RNNs with long computation times.",1,"2020, 2020, 2023, 2022, 2022",A recurrent network model of planning explains hippocampal replay and human behavior 
5bUGQxRnce,"This paper introduces a new measure of visual artificial network computation 'time', $\xi_{cRNN}$, as the time-averaged uncertainty of a convolutional RNN trained with an evidential deep learning loss (Sensoy et al.). The authors proceed to analyse the dynamics of this network as it solves a range of classification tasks. Importantly, beyond the network learning to solve the tasks, $\xi_{cRNN}$ is generally well correlated with human reaction times, with which it also shares several qualitative features across tasks.","Deep network models of the visual system have often focused purely on classification accuracy and neural representations when comparing to biological data. However, this neglects an important aspect of human behaviour, namely the time it takes to arrive at a decision. The present paper tackles this important question using recent ideas from the machine learning literature and represents an interesting step in the direction of capturing the temporal variability in human behaviour.
The authors analyse an impressive breadth of tasks and behavioural data and include many interesting analyses, both qualitative and quantitative.","The major weakness of the submission is that while there are strong _correlations_ between $\xi_{cRNN}$ and human reaction times, there is less evidence that $\xi_{cRNN}$ is mechanistically similar to human reaction times, since it is fundamentally a measure of uncertainty rather than computation time. In particular, human reaction times generally involve a tradeoff between computation/evidence accumulation and decision making (as in common drift diffusion models). On the contrary, the cRNN has a fixed computational budget and has no need or even capacity for evaluating this tradeoff. This is in contrast to a few previous deep learning models in the literature that are capable of explicitly trading off computation and actions (e.g. Graves et al., Pascanu et al., and refs by the authors in the ML literature, and Jensen et al. in the Neuro/Cogsci literature). These considerations are important because e.g. task difficulty is likely to correlate with both uncertainty and reaction time, and this raises the question of whether a model of one is automatically a model of the other.

It might the interesting to compare the current model to alternative models that more explicitly have adaptive/variable computation time, such as a cRNN that computes until $\epsilon$ reaches a certain threshold (akin to classical drift diffusion models).

As the authors mention in L326, another potential weakness is that the present approach does not easily generalize beyond classification tasks, which form a small subset of the types of problems humans and animals are faced with during natural behaviour.

_References:_ \
Graves et al.: ""Adaptive computation time for recurrent neural networks"", arXiv 2016.\
Pascanu et al.: ""Learning model-based planning from scratch"", arXiv 2017.\
Jensen et al.: ""A recurrent network model of planning explains hippocampal replay and human behavior"", bioRxiv 2023.\
Bansal et al.: ""End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking"", NeurIPS 2022.","L106: what does 'without loss of generality' mean here?

L123 ""our control models lack inference-time flexibility."" What does this mean?

L144: It might be worth comparing to or discussing recent work by Bansal et al. (NeurIPS 2022), which develops a way of stably training convolutional RNNs with long computation times that generalize from small to large problem settings.

L156 ""for a complete mathematical specification of this loss, refer to SI 1"". $\epsilon$ does not seem to be defined in either the main text or SI beyond a qualitative description as 'uncertainty'. It would be good to provide a precise mathematical definition somewhere.

L204: it would be good to briefly explain how this distance measure by Jeurissen et al. is defined (or at least refer to SI where it can be described in more detail).

Figure 3b and 5c: it's not entirely clear to me how these 'activity maps' are computed?

Figure 7b: it might be worth talking a bit more about the differences between the human and model data here instead of doing linear regression and calling it a day. The cRNN data is decidedly not linear and exhibits something resembling a plateau followed by a sharp drop at high discriminability. This seems rather different from the monotonic decrease in human RT, and the authors could perhaps speculate on or discuss this difference.

Supplementary material: it would be worth having a separate folder with just a few example videos of each condition in addition to the larger set of videos currently provided.",The authors have adequately discussed the limitations of the work.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1686804385884,,,NeurIPS.cc/2023/Conference,1xPsn2gCOe,"['NeurIPS.cc/2023/Conference/Submission8651/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411182256,[],1,,,['everyone'],1xPsn2gCOe,['NeurIPS.cc/2023/Conference/Submission8651/Reviewer_4B3C'],1686804385884,1702411182256,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8651/Reviewer_4B3C']","Yes, this peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. Graves et al.: ""Adaptive computation time for recurrent neural networks"", arXiv 2016.
2. Pascanu et al.: ""Learning model-based planning from scratch"", arXiv 2017.
3. Jensen et al.: ""A recurrent network model of planning explains hippocampal replay and human behavior"", bioRxiv 2023.
4. Bansal et al.: ""End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking"", NeurIPS 2022.

The reviewer also mentions that it might be worth comparing or discussing the work by Bansal et al. (NeurIPS 2022) in the context of stably training convolutional RNNs with long computation times.",1,"2020, 2020, 2023, 2022, 2022",End-to-end Algorithm Synthesis with Recurrent Networks Extrapolation without Overthinking
Qd6glSe4qq,"The paper introduces a synthetic data framework called Retinal Optic Flow Learning (ROFL) and uses that framework to test the performance of unsupervised models on two learning tasks: reconstructing grand truth variables and predicting the response of MT neurons. By imposing a latent hierarchical structure, the authors observed improvements along three axes: on the linear decodability of the ground truth, on the predictability of MT neuron responses and, finally, on identifying the causal structure of the world as a major factor driving these results.","As far as I understood (given the lack of clarity on a few aspects regarding the problem definition and related work, please look at my comment below), the authors  introduced a method to generate synthetic data and then modeled the generated data using an ensemble of model architectures of their choice. While I am dubious of the novelty and the sensibility of the idea, the experimental results presented seem to validate the argument. Another important strength of the paper is that the authors are pretty open and clear about the weaknesses of their method, which is something to comment them for.","I am dubious of the idea because it seems that the authors generate synthetic data and then choose an ensemble of architectures to model them. Also, it is hard to understand what is the main problem tackled by the paper, how this work relates to the state of the art and most importantly what is the improvement and contribution compared to the SOA. 

There’s no Related Work section in the manuscript and the method is not compared against any other methods in the experiments. Are there many papers that deal with the same problem? Reference 29 seems to be one such paper and it is compared against in the experiments. However, given that this reference has a different dataset and model, I see no common ground for comparison. And given the peculiar structure of this problem area (generating data and then modeling them), the authors need to find way to highlight their contributions.

All these make it hard to assess the paper. I recommend the authors add a related work section, clearly connecting their paper with the state of the art, and clarifying the novelty and contribution. 

In you related work section, you may want to consider referencing some papers that use hierarchical latent spaces for real-world data, such the following one (which is a newly published paper, so it’s understandable that you hadn’t included it).

Generative Decoding of Visual Stimuli, https://openreview.net/pdf?id=57OuafQmu8","“Importantly, this framework allows us to manipulate both the architectures of the models and the causal structure of the world they are trained on” -> As far as I understood, you chose the models and the method for generating synthetic data. Being able to manipulate the architecture and structure of data is somehow expected. Why is that then an important fact and statement to make? If it’s not, please consider removing it. 

“We found that a single inductive bias, hierarchical latent structure, yields several improvements.” -> What is the metric that you use to assess performance and what do you compare against? The metrics are only qualitatively described in the paper. Please consider condensing that information in a clearly defined section, something like “Metrics and Baselines”, and give a clear, quantitative definition of the metrics. 

“the brain engages in hierarchical Bayesian inference ” -> Based on what I read, I do not believe you have presented sufficient evidence to back up such a strong claim. The experimental study on predicting MT response does not suffice. If you meant to back the claim using more of the results you presented please make it more clear in the main manuscript or else remove it. 

The metrics that are used in the experimental section such as informativeness, disentanglement and completeness are not clearly defined. There are some papers cited where those metrics are mentioned in the text and I am sure that someone would find the definition if they look them up. However, given that these metrics are not widely used, I think the authors should give the definition in the text. 

What are the details of the data that you used for the MT learning task? There’s little to no information regarding what these data are, how they are collected, pre-processed etc. I understand that you cite a couple of papers that probably have these details, however, I think you have to give some details and explain how you use this dataset. 

What is the learning task for the MT prediction, i.e., what is the quantity you’re predicting? Is it the firing rate shown on figure 6? If so, how is that related to the hierarchical modeling in the previous section? Details on the dataset would help clarify. 

What is the AirSim dataset that the authors compare against in the MT prediction task? And how does it related to the newly introduced ROLF? If it’s a completely different synthetic data framework I do not see the point of comparing against it. 
 
Is the label in Figure 3 correct? There’s a reference to Figure 2. Maybe the author meant to refer to the bottom of Figure 3.

On lines 225-227 you say “all metrics for a broad range of β 227 values (Figure 5).” I am assuming that by all metrics you mean the  “untangling”, “disentengling”, and “brain-alignment” mentioned in line 195. However, the labels on figure 5 are slightly different, “Informativeness”, “Disentanglement” and “Completeness”. Please clarify what the metrics are and quantitatively define them, as they are not well known metrics.

Why the word “learning” in ROFL? There is no learning involved in the data generation, as far as I understood.",The authors have clearly and openly discussed the limitations of their work.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,2 fair,2 fair,1688498088530,,,NeurIPS.cc/2023/Conference,1wOkHN9JK8,"['NeurIPS.cc/2023/Conference/Submission15168/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411509327,[],2,,,['everyone'],1wOkHN9JK8,['NeurIPS.cc/2023/Conference/Submission15168/Reviewer_ZQpT'],1688498088530,1702411509327,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission15168/Reviewer_ZQpT']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, which is:

""Generative Decoding of Visual Stimuli, https://openreview.net/pdf?id=57OuafQmu8""

This is a newly published paper that uses hierarchical latent spaces for real-world data, and the reviewer suggests that the authors may want to consider referencing it in their related work section to highlight their contributions and connect their paper with the state of the art.",1,,Generative Decoding of Visual Stimuli
mLlyzOwMQ4,"This paper addresses a major weakness of Bayesian expected improvement acquisition functions, which are ubiquitously used for black-box optimization tasks such as computational hyperparameter tuning, materials science, and biomedical research. It is very common to use gradient-based optimizers to find local maxima of the acquisition surface, however expected-improvement acquisition functions have the very unfortunate pathology of a completely flat acquisition surface (where both the acquisition value and acquisition gradient are 0) on large regions of input space, particularly as optimization progresses and the best known solution improves. This pathology makes Bayesian optimization extremely sensitive to implementation decisions, particularly the initialization scheme of the acquisition maximization subproblem, which hinders BayesOpt practitioners in academia and industry. This paper rightfully places numerical precision and stability as one of the primary considerations in acquisition function design, and proposes simple, intuitive modifications to expected improvement acquisition functions that significantly improve performance.","I am strongly in favor of accepting this paper. The basic problem the authors are addressing is one I have often encountered myself, and I have even tried some similar ideas as those presented in this paper to try to address the problem, however I had to shelve the project due to competing demands for my time. I'm delighted to see the problem addressed so thoroughly here.

The greatest strength of this paper is the emphasis placed on how acquisition function design interacts with the optimization algorithms used to find their maxima. Generally speaking I feel this aspect of acquisition function design is often neglected in many Bayesian optimization papers, to the great detriment of the field.

Given the widespread use of BayesOpt across industries, and the use of EI-style acquisition functions in particular, I think this paper could have significant practical impact on multiple fields.","This work is ready for publication without any significant revisions. I would encourage reviewers in general to think about the opportunity cost of burdening authors with minor or tangential concerns, slowing the development of follow-up work.

It's worth noting that the weaknesses of EI-style acquisition functions are fairly well documented in latent-space BayesOpt papers, such as [1] and [2]. Both of those works employed a heuristic I didn't see mentioned in the paper, which is to scale the max_{x_i \in D} f(x_i) term in the acquisition function with some factor < 1 (e.g. 0.9). Some brief discussion in the related work on this point could help better communicate the potential impact of this paper.

[1] Tripp, A., Daxberger, E., and Hernandez-Lobato, J. M. ´
Sample-efficient optimization in the latent space of deep
generative models via weighted retraining. Advances in
Neural Information Processing Systems, 33, 2020.

[2] Stanton, S., Maddox, W., Gruver, N., Maffettone, P., Delaney, E., Greenside, P., & Wilson, A. G. (2022, June). Accelerating bayesian optimization for biological sequence design with denoising autoencoders. In International Conference on Machine Learning (pp. 20459-20478). PMLR.","Can you comment on whether you expect batch acquisition value optimization to outperform sequential greedy optimization when there are strong locality constraints placed on the inner loop problem (i.e. d(x_0, x_t) < \varepsilon for all x_t optimization iterates)? Intuitively it seems that the performance of batch acquisition optimization once again comes down to heuristics for choosing the right collection of points as the initial solution (e.g. [2]), since the iterates may not be able to move far enough from the initialization for the improved acquisition landscape to make much difference.","I think the discussion section could be expanded a bit. In particular I think the following rather vague sentence could be made more specific:

""While our contributions may not apply verbatim to other classes of acquisition functions, our key insights and strategies do translate and could help e.g. with improving information-based [20, 42], cost-aware [26, 36], and other types of acquisition functions that are prone to similar numerical challenges.""

I take this to mean that this paper has primarily focused on resolving numerical difficulties arising from the use of the max operator, and other acquisition functions may have numerical issues from operators that are not the max. If this is the case I think it could be stated more clearly, as it would give readers a clearer picture of avenues for future work.",['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,4 excellent,4 excellent,4 excellent,1687725555876,,,NeurIPS.cc/2023/Conference,1vyAG6j9PE,"['NeurIPS.cc/2023/Conference/Submission7919/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411137407,[],1,,,['everyone'],1vyAG6j9PE,['NeurIPS.cc/2023/Conference/Submission7919/Reviewer_UCkN'],1687725555876,1702411137407,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7919/Reviewer_UCkN']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. Tripp, A., Daxberger, E., and Hernandez-Lobato, J. M. (2020) - ""Sample-efficient optimization in the latent space of deep generative models via weighted retraining.""
2. Stanton, S., Maddox, W., Gruver, N., Maffettone, P., Delaney, E., Greenside, P., & Wilson, A. G. (2022) - ""Accelerating bayesian optimization for biological sequence design with denoising autoencoders.""

The reviewer mentions that the weaknesses of EI-style acquisition functions are fairly well documented in these latent-space BayesOpt papers and suggests that some brief discussion in the related work section could help better communicate the potential impact of the paper.",1,"2020, 2022",Sample-efficient optimization in the latent space of deep generative models via weighted retraining
mLlyzOwMQ4,"This paper addresses a major weakness of Bayesian expected improvement acquisition functions, which are ubiquitously used for black-box optimization tasks such as computational hyperparameter tuning, materials science, and biomedical research. It is very common to use gradient-based optimizers to find local maxima of the acquisition surface, however expected-improvement acquisition functions have the very unfortunate pathology of a completely flat acquisition surface (where both the acquisition value and acquisition gradient are 0) on large regions of input space, particularly as optimization progresses and the best known solution improves. This pathology makes Bayesian optimization extremely sensitive to implementation decisions, particularly the initialization scheme of the acquisition maximization subproblem, which hinders BayesOpt practitioners in academia and industry. This paper rightfully places numerical precision and stability as one of the primary considerations in acquisition function design, and proposes simple, intuitive modifications to expected improvement acquisition functions that significantly improve performance.","I am strongly in favor of accepting this paper. The basic problem the authors are addressing is one I have often encountered myself, and I have even tried some similar ideas as those presented in this paper to try to address the problem, however I had to shelve the project due to competing demands for my time. I'm delighted to see the problem addressed so thoroughly here.

The greatest strength of this paper is the emphasis placed on how acquisition function design interacts with the optimization algorithms used to find their maxima. Generally speaking I feel this aspect of acquisition function design is often neglected in many Bayesian optimization papers, to the great detriment of the field.

Given the widespread use of BayesOpt across industries, and the use of EI-style acquisition functions in particular, I think this paper could have significant practical impact on multiple fields.","This work is ready for publication without any significant revisions. I would encourage reviewers in general to think about the opportunity cost of burdening authors with minor or tangential concerns, slowing the development of follow-up work.

It's worth noting that the weaknesses of EI-style acquisition functions are fairly well documented in latent-space BayesOpt papers, such as [1] and [2]. Both of those works employed a heuristic I didn't see mentioned in the paper, which is to scale the max_{x_i \in D} f(x_i) term in the acquisition function with some factor < 1 (e.g. 0.9). Some brief discussion in the related work on this point could help better communicate the potential impact of this paper.

[1] Tripp, A., Daxberger, E., and Hernandez-Lobato, J. M. ´
Sample-efficient optimization in the latent space of deep
generative models via weighted retraining. Advances in
Neural Information Processing Systems, 33, 2020.

[2] Stanton, S., Maddox, W., Gruver, N., Maffettone, P., Delaney, E., Greenside, P., & Wilson, A. G. (2022, June). Accelerating bayesian optimization for biological sequence design with denoising autoencoders. In International Conference on Machine Learning (pp. 20459-20478). PMLR.","Can you comment on whether you expect batch acquisition value optimization to outperform sequential greedy optimization when there are strong locality constraints placed on the inner loop problem (i.e. d(x_0, x_t) < \varepsilon for all x_t optimization iterates)? Intuitively it seems that the performance of batch acquisition optimization once again comes down to heuristics for choosing the right collection of points as the initial solution (e.g. [2]), since the iterates may not be able to move far enough from the initialization for the improved acquisition landscape to make much difference.","I think the discussion section could be expanded a bit. In particular I think the following rather vague sentence could be made more specific:

""While our contributions may not apply verbatim to other classes of acquisition functions, our key insights and strategies do translate and could help e.g. with improving information-based [20, 42], cost-aware [26, 36], and other types of acquisition functions that are prone to similar numerical challenges.""

I take this to mean that this paper has primarily focused on resolving numerical difficulties arising from the use of the max operator, and other acquisition functions may have numerical issues from operators that are not the max. If this is the case I think it could be stated more clearly, as it would give readers a clearer picture of avenues for future work.",['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,4 excellent,4 excellent,4 excellent,1687725555876,,,NeurIPS.cc/2023/Conference,1vyAG6j9PE,"['NeurIPS.cc/2023/Conference/Submission7919/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411137407,[],1,,,['everyone'],1vyAG6j9PE,['NeurIPS.cc/2023/Conference/Submission7919/Reviewer_UCkN'],1687725555876,1702411137407,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7919/Reviewer_UCkN']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. Tripp, A., Daxberger, E., and Hernandez-Lobato, J. M. (2020) - ""Sample-efficient optimization in the latent space of deep generative models via weighted retraining.""
2. Stanton, S., Maddox, W., Gruver, N., Maffettone, P., Delaney, E., Greenside, P., & Wilson, A. G. (2022) - ""Accelerating bayesian optimization for biological sequence design with denoising autoencoders.""

The reviewer mentions that the weaknesses of EI-style acquisition functions are fairly well documented in these latent-space BayesOpt papers and suggests that some brief discussion in the related work section could help better communicate the potential impact of the paper.",1,"2020, 2022",Accelerating bayesian optimization for biological sequence design with denoising autoencoders
ti5YE6UJBH,"The paper proposes LogEI, family of acquisition functions with improved numerical stability over EI that makes it more suitable for gradient-based acquisition function optimization, all while retaining similar optima as EI. Pathologies of EI are visualized and analyzed, and the approximation error between qEI and qLogEI is theoretically bounded. Empirically, LogEI clearly outperforms EI on most tasks, suggesting that it can act as a drop-in replacement for EI.","__Good motivation__: Acquisition function optimization is an often-overlooked aspect of Bayesian optimization, and the paper does a good job of displaying the difficulties of acquisition function optimization (Fig. 1) and how the proposed approach remedies the issue.
__Simple, effective and extensible solution:__ Simple solutions that work are great, and LogEI (and its extensions) is good example. 
__Very good empirical performance:__ The improvement over EI in the results is striking on most tasks, suggesting it is simply a superior acquisition function to the default.","__Anecdotal evidence for similarity with EI:__ Intuitively, It is sensible that LogEI is similar to EI. However, there is little evidence (Fig. 1) and theory (Lemma 2) to support this.  I would greatly appreciate a similar Lemma for the analytic variant, and examples of when the two may not be identical. The performance gain of LogEI compared to EI is rather substantial on Ackley (~4x on Ackley-16!) and Mich, which suggests that the two may in fact not be very similar (but that LogEI may in fact simply be superior). Specifically, I don't believe the statement in Row 9, ""LogEI, whose members either have _identical or approximately equal optima_ as their canonical counterparts"" is well supported.

__Existing LogEI and Lacking references to Related work:__ The idea of a LogEI is not novel (LogEI was an acquisition function in SMAC at one point). Admittedly, that implementation regards log-transformations of the objective [1, 2] and would not help with numerical stability in the same manner. Nevertheless, I think these warrant citation and  _comparison in the experiments_, given the similarities. Moreover, it limits the novelty of the approach.

__Relevance in high dimensions:__ Currently, I am not convinced by the justification for LogEI in high dimensions. To me, LogEI aims to address the pathology where the acquisition function is zero, but I don't see that happening in high-dimensional problems due to the exuberance of high-uncertainty regions (which would make the acquisition function _non-zero, but constant?_). So, why is the proposed approach particularily important in high dimensions - i.e., why does LogEI help when the acquisition function is constant as opposed to (almost) zero? This would, in my opinion, require a separate motivation than Fig. 1, empirical results aside. Moreover, I think the _zero-value_ versus _constant-value_ distinction is very important, and should be emphasized more.

  With this in mind, I find Fig. 2 striking and odd. 60 data points (which is when almost all points have a zero-valued gradient) on an 8-dimensional problem is not a large amount of data (not even a 2x2x...x2 grid), yet the uncertainty is small to the point of ""EI and its gradients become numerically zero across most of the domain""? With all due respect, are the authors sure that this is not _just_ the gradient (and not the function value), or that the model has oddly long lengthscales?

__Minor:__
  - __Noisy tasks:__ Adaptation of LogEI to noisy problem settings are missing
  - __Lack of conventional benchmarks__: As a potential drop-in replacement for EI, seeing its performance of the method on the most conventional low-dimensional tasks (Branin, Hartmanns) would be informative. Moreover, it would be helpful for future benchmarking.

For all of these bullets, I believe that evidence to clarify (and not necessarily disprove) the remarks would substantially strengthen the paper. 

[1]. An experimental investigation of model-based parameter optimisation: SPO and beyond. F Hutter, HH Hoos, K Leyton-Brown, Kevin P. Murphy. _GECCO '09: Proceedings of the 11th Annual conference on Genetic and evolutionary computation_. 2009.

[2] Sequential model-based optimization for general algorithm configuration. F Hutter, HH Hoos, K Leyton-Brown. _Learning and Intelligent Optimization: 5th International Conference_. 2011.

LogEI in SMAC: https://github.com/automl/SMAC3/blob/29355618b35dcf4b3ce3e773d633109f036dba17/smac/optimizer/acquisition.py#L503","- Is there any setting where LogEI can _not_ act as a plug-in replacement for EI, or where performance would be expected to be worse?
- Have the authors experimented with LogEI non-continous search spaces, and if so, what are the findings?","Some suggestions for addressing limitations have been stated in the Weaknesses section, but are otherwise adequately addressed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,4 excellent,4 excellent,1688473292973,,,NeurIPS.cc/2023/Conference,1vyAG6j9PE,"['NeurIPS.cc/2023/Conference/Submission7919/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411137294,[],2,,,['everyone'],1vyAG6j9PE,['NeurIPS.cc/2023/Conference/Submission7919/Reviewer_u8yk'],1688473292973,1702411137294,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7919/Reviewer_u8yk']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, specifically:

1. ""An experimental investigation of model-based parameter optimisation: SPO and beyond"" by F. Hutter, H. H. Hoos, K. Leyton-Brown, and K. P. Murphy (2009)
2. ""Sequential model-based optimization for general algorithm configuration"" by F. Hutter, H. H. Hoos, and K. Leyton-Brown (2011)

These references are related to the use of LogEI in the SMAC (Sequential Model-based Algorithm Configuration) framework, which is mentioned in the review as a prior implementation of LogEI. The reviewer suggests that the authors should cite these references and compare their work to the existing LogEI implementation in SMAC.",1,"2009, 2011",An experimental investigation of model-based parameter optimisation SPO and beyond 
ti5YE6UJBH,"The paper proposes LogEI, family of acquisition functions with improved numerical stability over EI that makes it more suitable for gradient-based acquisition function optimization, all while retaining similar optima as EI. Pathologies of EI are visualized and analyzed, and the approximation error between qEI and qLogEI is theoretically bounded. Empirically, LogEI clearly outperforms EI on most tasks, suggesting that it can act as a drop-in replacement for EI.","__Good motivation__: Acquisition function optimization is an often-overlooked aspect of Bayesian optimization, and the paper does a good job of displaying the difficulties of acquisition function optimization (Fig. 1) and how the proposed approach remedies the issue.
__Simple, effective and extensible solution:__ Simple solutions that work are great, and LogEI (and its extensions) is good example. 
__Very good empirical performance:__ The improvement over EI in the results is striking on most tasks, suggesting it is simply a superior acquisition function to the default.","__Anecdotal evidence for similarity with EI:__ Intuitively, It is sensible that LogEI is similar to EI. However, there is little evidence (Fig. 1) and theory (Lemma 2) to support this.  I would greatly appreciate a similar Lemma for the analytic variant, and examples of when the two may not be identical. The performance gain of LogEI compared to EI is rather substantial on Ackley (~4x on Ackley-16!) and Mich, which suggests that the two may in fact not be very similar (but that LogEI may in fact simply be superior). Specifically, I don't believe the statement in Row 9, ""LogEI, whose members either have _identical or approximately equal optima_ as their canonical counterparts"" is well supported.

__Existing LogEI and Lacking references to Related work:__ The idea of a LogEI is not novel (LogEI was an acquisition function in SMAC at one point). Admittedly, that implementation regards log-transformations of the objective [1, 2] and would not help with numerical stability in the same manner. Nevertheless, I think these warrant citation and  _comparison in the experiments_, given the similarities. Moreover, it limits the novelty of the approach.

__Relevance in high dimensions:__ Currently, I am not convinced by the justification for LogEI in high dimensions. To me, LogEI aims to address the pathology where the acquisition function is zero, but I don't see that happening in high-dimensional problems due to the exuberance of high-uncertainty regions (which would make the acquisition function _non-zero, but constant?_). So, why is the proposed approach particularily important in high dimensions - i.e., why does LogEI help when the acquisition function is constant as opposed to (almost) zero? This would, in my opinion, require a separate motivation than Fig. 1, empirical results aside. Moreover, I think the _zero-value_ versus _constant-value_ distinction is very important, and should be emphasized more.

  With this in mind, I find Fig. 2 striking and odd. 60 data points (which is when almost all points have a zero-valued gradient) on an 8-dimensional problem is not a large amount of data (not even a 2x2x...x2 grid), yet the uncertainty is small to the point of ""EI and its gradients become numerically zero across most of the domain""? With all due respect, are the authors sure that this is not _just_ the gradient (and not the function value), or that the model has oddly long lengthscales?

__Minor:__
  - __Noisy tasks:__ Adaptation of LogEI to noisy problem settings are missing
  - __Lack of conventional benchmarks__: As a potential drop-in replacement for EI, seeing its performance of the method on the most conventional low-dimensional tasks (Branin, Hartmanns) would be informative. Moreover, it would be helpful for future benchmarking.

For all of these bullets, I believe that evidence to clarify (and not necessarily disprove) the remarks would substantially strengthen the paper. 

[1]. An experimental investigation of model-based parameter optimisation: SPO and beyond. F Hutter, HH Hoos, K Leyton-Brown, Kevin P. Murphy. _GECCO '09: Proceedings of the 11th Annual conference on Genetic and evolutionary computation_. 2009.

[2] Sequential model-based optimization for general algorithm configuration. F Hutter, HH Hoos, K Leyton-Brown. _Learning and Intelligent Optimization: 5th International Conference_. 2011.

LogEI in SMAC: https://github.com/automl/SMAC3/blob/29355618b35dcf4b3ce3e773d633109f036dba17/smac/optimizer/acquisition.py#L503","- Is there any setting where LogEI can _not_ act as a plug-in replacement for EI, or where performance would be expected to be worse?
- Have the authors experimented with LogEI non-continous search spaces, and if so, what are the findings?","Some suggestions for addressing limitations have been stated in the Weaknesses section, but are otherwise adequately addressed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,4 excellent,4 excellent,1688473292973,,,NeurIPS.cc/2023/Conference,1vyAG6j9PE,"['NeurIPS.cc/2023/Conference/Submission7919/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411137294,[],2,,,['everyone'],1vyAG6j9PE,['NeurIPS.cc/2023/Conference/Submission7919/Reviewer_u8yk'],1688473292973,1702411137294,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7919/Reviewer_u8yk']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, specifically:

1. ""An experimental investigation of model-based parameter optimisation: SPO and beyond"" by F. Hutter, H. H. Hoos, K. Leyton-Brown, and K. P. Murphy (2009)
2. ""Sequential model-based optimization for general algorithm configuration"" by F. Hutter, H. H. Hoos, and K. Leyton-Brown (2011)

These references are related to the use of LogEI in the SMAC (Sequential Model-based Algorithm Configuration) framework, which is mentioned in the review as a prior implementation of LogEI. The reviewer suggests that the authors should cite these references and compare their work to the existing LogEI implementation in SMAC.",1,"2009, 2011",Sequential model-based optimization for general algorithm configuration
jf1mOccpiF,"The paper considers deep networks as a model of the visual stream, specifically V1. The authors systematically study the impact of various biological additions to deep networks on alignment of deep nets' representations with V1 recordings.","The paper considers several features of the early visual stream that can be added to deep networks, and tests the influence of those features on image classification performance and V1 alignment. While some individual features have been considered before, the approach and especially ablation studies here are novel and, in my opinion, interesting to the community.

The final result is interesting too: combining all 4 architectural features alone resulted in the best V1 alignment (0.605 vs. 0.594 of the top1 V1 model www.brain-score.org/model/vision/623). Adding adversarial features improved it to 0.629, which seems very significant -- the median V1 score at www.brain-score.org/ is less than 0.5.","The results in Tab. 3 suggest that V1-like features significantly hurt ImageNet performance -- the best V1 model is 16% less accurate than the best ImageNet model. This is mostly due to adversarial training. I think the authors should discuss why it has such an effect. 

Two important ablation studies are missing:
1. Adversarial training only, since it has a big effect on both V1 alignment and ImageNet performance.
2. Untrained networks with all/some biological features. All discussed features change the distribution of neural responses even in untrained networks, so it might be that V1 improvements come from that distribution change alone, not from training with those features. 

I also suggest the authors include, at least in the appendix, Brain-Scores for other areas (V2, V4, IT) and behavioural data. This is the standard way to evaluate models on Brain-Score, so having all results would make comparisons to other models easier.","Is it possible to provide a baseline for the ""best possible"" V1 score by comparing Brain-Score neural data to itself (e.g. with K-fold cross-validation)? I don't think it was done in the original Brain-Score papers, so it's definitely not a hard requirement here. But it would be a great addition.

### Minor issues

> [Line 20] Advances in neuroscience have long been proposed as essential to realizing the next generation of artifical intelligence (AI). 

Is that true? I'm not quite sure… also misspelled artificial 

> [22] (e.g, convolutional neural networks and mechanisms of attention) owe their origins to biological intelligence

Conv nets need a citation; attention too, and I’m not sure if attention mechanism in transformers were even inspired by biology on the implementation level (see https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full which says ""While the spirit of
attention in machine learning is certainly inspired by psychology,
its implementations do not always track with what is known
about biological attention, as will be noted below."")

Overall: \cite doesn’t generate links to bibliography? 

> [45] In specific

I don’t think this phrase is commonly used. “Specifically” or “in particular” would read better.

> [151] these DoG convolutions were only applied to a fraction of the input feature map

Why?

Fig. 1D can benefit from a more detailed explanation. I want to say the original image is on the right but then the transformation doesn’t preserve retinotopy.

> [209] alternate

Alternative

Color-coding Tab. 3 would be great!

The code is in the supplementary, so the authors should indicate it in the main text (and perhaps add it to github afterwards)

### Rebuttal acknowledgement
I have read the rebuttal and responded to the authors. I think it addressed all (minor) concerns that I had, and I still think 7 (accept) is an appropriate score.",The limitations and impacts are addressed.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688610909790,,,NeurIPS.cc/2023/Conference,1uirUsR9E7,"['NeurIPS.cc/2023/Conference/Submission15381/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411515969,[],2,,,['everyone'],1uirUsR9E7,['NeurIPS.cc/2023/Conference/Submission15381/Reviewer_yZMG'],1688610909790,1702411515969,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission15381/Reviewer_yZMG']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. 

The peer review mentions that convolutional neural networks and mechanisms of attention need a citation. It also references a specific article (https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full) regarding the implementation of attention mechanisms in transformers, implying that the authors should consider citing this article to support their claims about the biological origins of attention mechanisms.",1,2020,Implementation of Attention Mechanisms in Transformers
Oez4fNLgsB,"The paper proposes an ensemble based approach to solve visual reasoning problems. The paper proposes to use an instruction fine-tuned large language model to integrate answers to visual reasoning problems provided by vision language models. The paper presents two variants of the aggregation model -- using fine-tuning and using in-context learning. The model is evaluated on the VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR datasets.","* The paper reports promising results on the VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR datasets.
* The paper includes a variety of ablations that show the effectiveness of the proposed method -- including model size, scaling, number of video-language models as ensemble members.
* The paper includes qualitative examples which highlight the effectiveness of the proposed method.
* The paper is well written and easy to understand.","* While the results are very promising, it would be helpful to add results on more complex datasets such as CLEVR (CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning) and GQA (GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering) which study compositional reasoning.

* The paper should also consider comparing to prior work such as Visual Programming: Compositional visual reasoning without training, CVPR 2023, which also uses an external large language model to coordinate vision / language-vision models.

* The paper claims in L291 ""This work demonstrates the first step toward applying language models for visual reasoning"", but Flamingo (Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS 2022) already shows zero-show visual reasoning on datasets such as Next-QA.

","* The paper should include further details on the computational resources used. L680 in the supplementary material just states that V100 or A100 GPUs were used, but the paper should include further details about the total computational resources used.

* The paper should include further motivational details on why the particular datasets VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR were used? Why were more datasets which require more complex reasoning abilities such as CLEVR and GQA were not used.",The paper does not include any discussion about its limitations.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688711567815,,,NeurIPS.cc/2023/Conference,1q0feiJ2i4,"['NeurIPS.cc/2023/Conference/Submission1859/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410808630,[],2,,,['everyone'],1q0feiJ2i4,['NeurIPS.cc/2023/Conference/Submission1859/Reviewer_qcEV'],1688711567815,1702410808630,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1859/Reviewer_qcEV']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. CLEVR (CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning)
2. GQA (GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering)
3. Visual Programming: Compositional visual reasoning without training, CVPR 2023
4. Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS 2022

The reviewer mentions that the paper should compare to prior work such as Visual Programming and notes that Flamingo already shows zero-shot visual reasoning on certain datasets, implying that the authors should be aware of and cite this work. Additionally, the reviewer suggests that the authors consider evaluating their method on more complex datasets such as CLEVR and GQA.",1,"2023, 2022",CLEVR A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning
Oez4fNLgsB,"The paper proposes an ensemble based approach to solve visual reasoning problems. The paper proposes to use an instruction fine-tuned large language model to integrate answers to visual reasoning problems provided by vision language models. The paper presents two variants of the aggregation model -- using fine-tuning and using in-context learning. The model is evaluated on the VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR datasets.","* The paper reports promising results on the VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR datasets.
* The paper includes a variety of ablations that show the effectiveness of the proposed method -- including model size, scaling, number of video-language models as ensemble members.
* The paper includes qualitative examples which highlight the effectiveness of the proposed method.
* The paper is well written and easy to understand.","* While the results are very promising, it would be helpful to add results on more complex datasets such as CLEVR (CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning) and GQA (GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering) which study compositional reasoning.

* The paper should also consider comparing to prior work such as Visual Programming: Compositional visual reasoning without training, CVPR 2023, which also uses an external large language model to coordinate vision / language-vision models.

* The paper claims in L291 ""This work demonstrates the first step toward applying language models for visual reasoning"", but Flamingo (Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS 2022) already shows zero-show visual reasoning on datasets such as Next-QA.

","* The paper should include further details on the computational resources used. L680 in the supplementary material just states that V100 or A100 GPUs were used, but the paper should include further details about the total computational resources used.

* The paper should include further motivational details on why the particular datasets VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR were used? Why were more datasets which require more complex reasoning abilities such as CLEVR and GQA were not used.",The paper does not include any discussion about its limitations.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688711567815,,,NeurIPS.cc/2023/Conference,1q0feiJ2i4,"['NeurIPS.cc/2023/Conference/Submission1859/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410808630,[],2,,,['everyone'],1q0feiJ2i4,['NeurIPS.cc/2023/Conference/Submission1859/Reviewer_qcEV'],1688711567815,1702410808630,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1859/Reviewer_qcEV']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. CLEVR (CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning)
2. GQA (GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering)
3. Visual Programming: Compositional visual reasoning without training, CVPR 2023
4. Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS 2022

The reviewer mentions that the paper should compare to prior work such as Visual Programming and notes that Flamingo already shows zero-shot visual reasoning on certain datasets, implying that the authors should be aware of and cite this work. Additionally, the reviewer suggests that the authors consider evaluating their method on more complex datasets such as CLEVR and GQA.",1,"2023, 2022",GQA A New Dataset for Real-World Visual Reasoning and Compositional Question Answering
Oez4fNLgsB,"The paper proposes an ensemble based approach to solve visual reasoning problems. The paper proposes to use an instruction fine-tuned large language model to integrate answers to visual reasoning problems provided by vision language models. The paper presents two variants of the aggregation model -- using fine-tuning and using in-context learning. The model is evaluated on the VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR datasets.","* The paper reports promising results on the VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR datasets.
* The paper includes a variety of ablations that show the effectiveness of the proposed method -- including model size, scaling, number of video-language models as ensemble members.
* The paper includes qualitative examples which highlight the effectiveness of the proposed method.
* The paper is well written and easy to understand.","* While the results are very promising, it would be helpful to add results on more complex datasets such as CLEVR (CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning) and GQA (GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering) which study compositional reasoning.

* The paper should also consider comparing to prior work such as Visual Programming: Compositional visual reasoning without training, CVPR 2023, which also uses an external large language model to coordinate vision / language-vision models.

* The paper claims in L291 ""This work demonstrates the first step toward applying language models for visual reasoning"", but Flamingo (Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS 2022) already shows zero-show visual reasoning on datasets such as Next-QA.

","* The paper should include further details on the computational resources used. L680 in the supplementary material just states that V100 or A100 GPUs were used, but the paper should include further details about the total computational resources used.

* The paper should include further motivational details on why the particular datasets VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR were used? Why were more datasets which require more complex reasoning abilities such as CLEVR and GQA were not used.",The paper does not include any discussion about its limitations.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688711567815,,,NeurIPS.cc/2023/Conference,1q0feiJ2i4,"['NeurIPS.cc/2023/Conference/Submission1859/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410808630,[],2,,,['everyone'],1q0feiJ2i4,['NeurIPS.cc/2023/Conference/Submission1859/Reviewer_qcEV'],1688711567815,1702410808630,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1859/Reviewer_qcEV']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. CLEVR (CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning)
2. GQA (GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering)
3. Visual Programming: Compositional visual reasoning without training, CVPR 2023
4. Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS 2022

The reviewer mentions that the paper should compare to prior work such as Visual Programming and notes that Flamingo already shows zero-shot visual reasoning on certain datasets, implying that the authors should be aware of and cite this work. Additionally, the reviewer suggests that the authors consider evaluating their method on more complex datasets such as CLEVR and GQA.",1,"2023, 2022",Visual Programming Compositional visual reasoning without training
Oez4fNLgsB,"The paper proposes an ensemble based approach to solve visual reasoning problems. The paper proposes to use an instruction fine-tuned large language model to integrate answers to visual reasoning problems provided by vision language models. The paper presents two variants of the aggregation model -- using fine-tuning and using in-context learning. The model is evaluated on the VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR datasets.","* The paper reports promising results on the VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR datasets.
* The paper includes a variety of ablations that show the effectiveness of the proposed method -- including model size, scaling, number of video-language models as ensemble members.
* The paper includes qualitative examples which highlight the effectiveness of the proposed method.
* The paper is well written and easy to understand.","* While the results are very promising, it would be helpful to add results on more complex datasets such as CLEVR (CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning) and GQA (GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering) which study compositional reasoning.

* The paper should also consider comparing to prior work such as Visual Programming: Compositional visual reasoning without training, CVPR 2023, which also uses an external large language model to coordinate vision / language-vision models.

* The paper claims in L291 ""This work demonstrates the first step toward applying language models for visual reasoning"", but Flamingo (Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS 2022) already shows zero-show visual reasoning on datasets such as Next-QA.

","* The paper should include further details on the computational resources used. L680 in the supplementary material just states that V100 or A100 GPUs were used, but the paper should include further details about the total computational resources used.

* The paper should include further motivational details on why the particular datasets VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE and VSR were used? Why were more datasets which require more complex reasoning abilities such as CLEVR and GQA were not used.",The paper does not include any discussion about its limitations.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688711567815,,,NeurIPS.cc/2023/Conference,1q0feiJ2i4,"['NeurIPS.cc/2023/Conference/Submission1859/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410808630,[],2,,,['everyone'],1q0feiJ2i4,['NeurIPS.cc/2023/Conference/Submission1859/Reviewer_qcEV'],1688711567815,1702410808630,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1859/Reviewer_qcEV']","Yes, the peer review explicitly suggests the authors of the paper to cite the following specific literature:

1. CLEVR (CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning)
2. GQA (GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering)
3. Visual Programming: Compositional visual reasoning without training, CVPR 2023
4. Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS 2022

The reviewer mentions that the paper should compare to prior work such as Visual Programming and notes that Flamingo already shows zero-shot visual reasoning on certain datasets, implying that the authors should be aware of and cite this work. Additionally, the reviewer suggests that the authors consider evaluating their method on more complex datasets such as CLEVR and GQA.",1,"2023, 2022",Flamingo a Visual Language Model for Few-Shot Learning
y6OtA0QGBf,"This paper delves into the issue of off-policy learning, the objective of which is to devise a new action selection policy based solely on the logged feedback derived from a logging policy. The paper pays particular attention to scenarios in which the logging policy remains unidentified and its estimation proves challenging. Under these conditions, common estimators, like IPS, may lose beneficial attributes such as unbiasedness. To address this complication, the paper introduces a new off-policy learning (OPL) approach called Uncertainty-aware off-policy learning. This new framework aims to optimize the uncertainty-aware objective function by employing a novel weighting scheme that is tuned by minimizing an upper bound of MSE in estimation. A local convergence based on the proposed method is also shown. Experimental results indicate that this proposed framework outperforms a range of benchmark methods on both semi-synthetic and real-world recommendation datasets.","- The paper addresses the practically relevant problem of dealing with uncertainty in logging policy estimation in off-policy learning.


- The paper proposes a reasonable and conceptually straightforward method to handle the issue of uncertain logging policies, providing theoretical guarantees regarding estimation and local convergence.


- The paper presents comprehensive experiments, not just basic performance comparisons, but also experiments on off-policy evaluation (OPE) and critical hyperparameters (some of which are included in the appendix).
","- Given that several papers already exist on the topic of distributionally robust off-policy learning (OPL), as discussed in the paper, the formulation of a problem addressing the uncertainty of logging policies may not be groundbreaking, even though I understand that their motivations differ somewhat.


- In the experiments, the issue might also be tackled by simply applying calibration during the estimation of the logging policy, as seen in the following paper:

Aniruddh Raghu, Omer Gottesman, Yao Liu, Matthieu Komorowski, Aldo Faisal, Finale Doshi-Velez, and Emma Brunskill. Behaviour Policy Estimation in Off-Policy Policy Evaluation: Calibration Matters. https://arxiv.org/pdf/1807.01066.pdf 


- In most of the experiments, CE performs quite well and is not substantially outperformed by UIPS. Therefore, considering the current experiment results, I may not use UIPS in practice and would rather rely on CE, which is much easier to implement (there is no need to estimate the logging policy when using CE), and does not require the tuning of additional hyperparameters as with UIPS.


- Related to the previous point, in most experiments, the second-best methods for each metric and dataset perform very similarly to UIPS. I am not sure how essential it truly is to address uncertainty in logging policy estimation. I understand that the results are statistically significant, but results can be deemed significant even with a slight performance difference if the sample size is sufficient. In this context, my focus is on the performance difference.
","- How do baseline methods perform in the experiments when they are combined with a calibrated logging policy estimator? Some additional results about this would be useful.


- Could you provide the results relative to the performance of the (true) logging policy? This enables us to see how much improvements the methods bring compared to the logging policy.


- When does UIPS become really crucial? That is, are there any situations where UIPS performs well while all other methods do not work satisfactory. In the current experiments, the second-best methods perform very similarly to UIPS in all datasets and metrics. Moreover, CE performs reasonably and stably for a range of metrics and datasets, which makes it a really good choice in practice indeed.
","The paper touches on the tightness of the bound as a limitation and future work in the last section.
",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688077010194,,,NeurIPS.cc/2023/Conference,1pWNhmbllE,"['NeurIPS.cc/2023/Conference/Submission9558/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411238946,[],1,,,['everyone'],1pWNhmbllE,['NeurIPS.cc/2023/Conference/Submission9558/Reviewer_19wU'],1688077010194,1702411238946,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9558/Reviewer_19wU']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, which is:

""Aniruddh Raghu, Omer Gottesman, Yao Liu, Matthieu Komorowski, Aldo Faisal, Finale Doshi-Velez, and Emma Brunskill. Behaviour Policy Estimation in Off-Policy Policy Evaluation: Calibration Matters. https://arxiv.org/pdf/1807.01066.pdf""

This is mentioned in the weaknesses section of the review, where the reviewer suggests that the issue of uncertain logging policies might also be tackled by applying calibration during the estimation of the logging policy, as seen in the referenced paper.",1,2018,Behaviour Policy Estimation in Off-Policy Policy Evaluation: Calibration Matters
vioqi1P4xK,"This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for off-policy learning, taking into account the uncertainty in the estimated logging policy. The authors demonstrate that the commonly used method of estimating the logging policy can lead to biased estimators, particularly for samples with small estimated logging probabilities. UIPS addresses this issue by reweighting the propensity scores based on the uncertainty of the estimated logging policy. The paper provides a theoretical analysis of the convergence properties of UIPS and presents experimental results on synthetic and real-world recommendation datasets, comparing against state-of-the-art baselines.","● The paper addresses an important problem in off-policy learning and proposes a novel method, UIPS, to improve the quality of the discovered policy.
● The authors provide a comprehensive theoretical analysis of UIPS, including a convergence guarantee.
● The experimental results demonstrate the effectiveness of UIPS compared to some baselines on both synthetic and real-world datasets.","● There remain some issues unsolved in the paper, such as the availability of the logging policy. See the questions for details.
● There are some related works that are not mentioned in this paper. In off-policy RL, several papers work on behavior-agnostic instance reweighting [1,2]. They compute the prioritization weight without the need of obtaining a behavior policy. There are also papers that discuss the importance ratio term when applying RL to recommendation systems [3,4].
● Introducing another neural network to estimate $\beta^*$ will increase the system complexity and the computational cost during training and testing. This may hinder the practical application of the algorithm.
● The synthetic dataset and the offline evaluation can give biased evaluation results of the algorithms.

[1] Sinha, Samarth, et al. ""Experience replay with likelihood-free importance weights."" Learning for Dynamics and Control Conference. PMLR, 2022.
[2] Liu, Xu-Hui, et al. ""Regret minimization experience replay in off-policy reinforcement learning."" Advances in Neural Information Processing Systems 34 (2021).
[3] Cai, Qingpeng, et al. ""Reinforcing User Retention in a Billion Scale Short Video Recommender System."" arXiv preprint arXiv:2302.01724 (2023).
[4] Chen, Minmin, et al. ""Off-policy actor-critic for recommender systems."" Proceedings of the 16th ACM Conference on Recommender Systems. 2022.
","1.How is this paper related to behavior-agnostic methods [1,2,5]? With a GAN-like estimator, these methods no longer reconstruct all those behavior policies. They may also be regarded as baselines to compare with.
2.Why are the probabilities $\beta^*(a|x)$ not recorded in the data? With stochastic logging policies, it is easy to store probabilities together with state and actions when generating data. With deterministic logging policies, a common practice is to sample actions from a Gaussian distribution, with policy output as mean and a certain standard deviation. The action probability will also be available.
3.Why do UIPS-O and UIPS-P lead to poor performance?

[5] Nachum, Ofir, et al. ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections."" Advances in neural information processing systems 32 (2019).","The usage of off-policy correction and uncertainty-based reweighting is limited to policy-based techniques based on the REINFORCE trick. Such techniques can have higher variance than value-based techniques such as TD3 and SAC, and may lead to unstable training.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688628580476,,,NeurIPS.cc/2023/Conference,1pWNhmbllE,"['NeurIPS.cc/2023/Conference/Submission9558/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411238759,[],3,,,['everyone'],1pWNhmbllE,['NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS'],1688628580476,1702411238759,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several related works that are not cited in the paper, including:

* Sinha et al. (2022) - ""Experience replay with likelihood-free importance weights""
* Liu et al. (2021) - ""Regret minimization experience replay in off-policy reinforcement learning""
* Cai et al. (2023) - ""Reinforcing User Retention in a Billion Scale Short Video Recommender System""
* Chen et al. (2022) - ""Off-policy actor-critic for recommender systems""
* Nachum et al. (2019) - ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections""

The reviewer suggests that these works may be relevant to the paper and could be considered as baselines for comparison.",1,"2022, 2021, 2023, 2022, 2019",Experience replay with likelihood-free importance weights
vioqi1P4xK,"This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for off-policy learning, taking into account the uncertainty in the estimated logging policy. The authors demonstrate that the commonly used method of estimating the logging policy can lead to biased estimators, particularly for samples with small estimated logging probabilities. UIPS addresses this issue by reweighting the propensity scores based on the uncertainty of the estimated logging policy. The paper provides a theoretical analysis of the convergence properties of UIPS and presents experimental results on synthetic and real-world recommendation datasets, comparing against state-of-the-art baselines.","● The paper addresses an important problem in off-policy learning and proposes a novel method, UIPS, to improve the quality of the discovered policy.
● The authors provide a comprehensive theoretical analysis of UIPS, including a convergence guarantee.
● The experimental results demonstrate the effectiveness of UIPS compared to some baselines on both synthetic and real-world datasets.","● There remain some issues unsolved in the paper, such as the availability of the logging policy. See the questions for details.
● There are some related works that are not mentioned in this paper. In off-policy RL, several papers work on behavior-agnostic instance reweighting [1,2]. They compute the prioritization weight without the need of obtaining a behavior policy. There are also papers that discuss the importance ratio term when applying RL to recommendation systems [3,4].
● Introducing another neural network to estimate $\beta^*$ will increase the system complexity and the computational cost during training and testing. This may hinder the practical application of the algorithm.
● The synthetic dataset and the offline evaluation can give biased evaluation results of the algorithms.

[1] Sinha, Samarth, et al. ""Experience replay with likelihood-free importance weights."" Learning for Dynamics and Control Conference. PMLR, 2022.
[2] Liu, Xu-Hui, et al. ""Regret minimization experience replay in off-policy reinforcement learning."" Advances in Neural Information Processing Systems 34 (2021).
[3] Cai, Qingpeng, et al. ""Reinforcing User Retention in a Billion Scale Short Video Recommender System."" arXiv preprint arXiv:2302.01724 (2023).
[4] Chen, Minmin, et al. ""Off-policy actor-critic for recommender systems."" Proceedings of the 16th ACM Conference on Recommender Systems. 2022.
","1.How is this paper related to behavior-agnostic methods [1,2,5]? With a GAN-like estimator, these methods no longer reconstruct all those behavior policies. They may also be regarded as baselines to compare with.
2.Why are the probabilities $\beta^*(a|x)$ not recorded in the data? With stochastic logging policies, it is easy to store probabilities together with state and actions when generating data. With deterministic logging policies, a common practice is to sample actions from a Gaussian distribution, with policy output as mean and a certain standard deviation. The action probability will also be available.
3.Why do UIPS-O and UIPS-P lead to poor performance?

[5] Nachum, Ofir, et al. ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections."" Advances in neural information processing systems 32 (2019).","The usage of off-policy correction and uncertainty-based reweighting is limited to policy-based techniques based on the REINFORCE trick. Such techniques can have higher variance than value-based techniques such as TD3 and SAC, and may lead to unstable training.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688628580476,,,NeurIPS.cc/2023/Conference,1pWNhmbllE,"['NeurIPS.cc/2023/Conference/Submission9558/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411238759,[],3,,,['everyone'],1pWNhmbllE,['NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS'],1688628580476,1702411238759,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several related works that are not cited in the paper, including:

* Sinha et al. (2022) - ""Experience replay with likelihood-free importance weights""
* Liu et al. (2021) - ""Regret minimization experience replay in off-policy reinforcement learning""
* Cai et al. (2023) - ""Reinforcing User Retention in a Billion Scale Short Video Recommender System""
* Chen et al. (2022) - ""Off-policy actor-critic for recommender systems""
* Nachum et al. (2019) - ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections""

The reviewer suggests that these works may be relevant to the paper and could be considered as baselines for comparison.",1,"2022, 2021, 2023, 2022, 2019",Regret minimization experience replay in off-policy reinforcement learning
vioqi1P4xK,"This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for off-policy learning, taking into account the uncertainty in the estimated logging policy. The authors demonstrate that the commonly used method of estimating the logging policy can lead to biased estimators, particularly for samples with small estimated logging probabilities. UIPS addresses this issue by reweighting the propensity scores based on the uncertainty of the estimated logging policy. The paper provides a theoretical analysis of the convergence properties of UIPS and presents experimental results on synthetic and real-world recommendation datasets, comparing against state-of-the-art baselines.","● The paper addresses an important problem in off-policy learning and proposes a novel method, UIPS, to improve the quality of the discovered policy.
● The authors provide a comprehensive theoretical analysis of UIPS, including a convergence guarantee.
● The experimental results demonstrate the effectiveness of UIPS compared to some baselines on both synthetic and real-world datasets.","● There remain some issues unsolved in the paper, such as the availability of the logging policy. See the questions for details.
● There are some related works that are not mentioned in this paper. In off-policy RL, several papers work on behavior-agnostic instance reweighting [1,2]. They compute the prioritization weight without the need of obtaining a behavior policy. There are also papers that discuss the importance ratio term when applying RL to recommendation systems [3,4].
● Introducing another neural network to estimate $\beta^*$ will increase the system complexity and the computational cost during training and testing. This may hinder the practical application of the algorithm.
● The synthetic dataset and the offline evaluation can give biased evaluation results of the algorithms.

[1] Sinha, Samarth, et al. ""Experience replay with likelihood-free importance weights."" Learning for Dynamics and Control Conference. PMLR, 2022.
[2] Liu, Xu-Hui, et al. ""Regret minimization experience replay in off-policy reinforcement learning."" Advances in Neural Information Processing Systems 34 (2021).
[3] Cai, Qingpeng, et al. ""Reinforcing User Retention in a Billion Scale Short Video Recommender System."" arXiv preprint arXiv:2302.01724 (2023).
[4] Chen, Minmin, et al. ""Off-policy actor-critic for recommender systems."" Proceedings of the 16th ACM Conference on Recommender Systems. 2022.
","1.How is this paper related to behavior-agnostic methods [1,2,5]? With a GAN-like estimator, these methods no longer reconstruct all those behavior policies. They may also be regarded as baselines to compare with.
2.Why are the probabilities $\beta^*(a|x)$ not recorded in the data? With stochastic logging policies, it is easy to store probabilities together with state and actions when generating data. With deterministic logging policies, a common practice is to sample actions from a Gaussian distribution, with policy output as mean and a certain standard deviation. The action probability will also be available.
3.Why do UIPS-O and UIPS-P lead to poor performance?

[5] Nachum, Ofir, et al. ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections."" Advances in neural information processing systems 32 (2019).","The usage of off-policy correction and uncertainty-based reweighting is limited to policy-based techniques based on the REINFORCE trick. Such techniques can have higher variance than value-based techniques such as TD3 and SAC, and may lead to unstable training.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688628580476,,,NeurIPS.cc/2023/Conference,1pWNhmbllE,"['NeurIPS.cc/2023/Conference/Submission9558/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411238759,[],3,,,['everyone'],1pWNhmbllE,['NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS'],1688628580476,1702411238759,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several related works that are not cited in the paper, including:

* Sinha et al. (2022) - ""Experience replay with likelihood-free importance weights""
* Liu et al. (2021) - ""Regret minimization experience replay in off-policy reinforcement learning""
* Cai et al. (2023) - ""Reinforcing User Retention in a Billion Scale Short Video Recommender System""
* Chen et al. (2022) - ""Off-policy actor-critic for recommender systems""
* Nachum et al. (2019) - ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections""

The reviewer suggests that these works may be relevant to the paper and could be considered as baselines for comparison.",1,"2022, 2021, 2023, 2022, 2019",Reinforcing User Retention in a Billion Scale Short Video Recommender System
vioqi1P4xK,"This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for off-policy learning, taking into account the uncertainty in the estimated logging policy. The authors demonstrate that the commonly used method of estimating the logging policy can lead to biased estimators, particularly for samples with small estimated logging probabilities. UIPS addresses this issue by reweighting the propensity scores based on the uncertainty of the estimated logging policy. The paper provides a theoretical analysis of the convergence properties of UIPS and presents experimental results on synthetic and real-world recommendation datasets, comparing against state-of-the-art baselines.","● The paper addresses an important problem in off-policy learning and proposes a novel method, UIPS, to improve the quality of the discovered policy.
● The authors provide a comprehensive theoretical analysis of UIPS, including a convergence guarantee.
● The experimental results demonstrate the effectiveness of UIPS compared to some baselines on both synthetic and real-world datasets.","● There remain some issues unsolved in the paper, such as the availability of the logging policy. See the questions for details.
● There are some related works that are not mentioned in this paper. In off-policy RL, several papers work on behavior-agnostic instance reweighting [1,2]. They compute the prioritization weight without the need of obtaining a behavior policy. There are also papers that discuss the importance ratio term when applying RL to recommendation systems [3,4].
● Introducing another neural network to estimate $\beta^*$ will increase the system complexity and the computational cost during training and testing. This may hinder the practical application of the algorithm.
● The synthetic dataset and the offline evaluation can give biased evaluation results of the algorithms.

[1] Sinha, Samarth, et al. ""Experience replay with likelihood-free importance weights."" Learning for Dynamics and Control Conference. PMLR, 2022.
[2] Liu, Xu-Hui, et al. ""Regret minimization experience replay in off-policy reinforcement learning."" Advances in Neural Information Processing Systems 34 (2021).
[3] Cai, Qingpeng, et al. ""Reinforcing User Retention in a Billion Scale Short Video Recommender System."" arXiv preprint arXiv:2302.01724 (2023).
[4] Chen, Minmin, et al. ""Off-policy actor-critic for recommender systems."" Proceedings of the 16th ACM Conference on Recommender Systems. 2022.
","1.How is this paper related to behavior-agnostic methods [1,2,5]? With a GAN-like estimator, these methods no longer reconstruct all those behavior policies. They may also be regarded as baselines to compare with.
2.Why are the probabilities $\beta^*(a|x)$ not recorded in the data? With stochastic logging policies, it is easy to store probabilities together with state and actions when generating data. With deterministic logging policies, a common practice is to sample actions from a Gaussian distribution, with policy output as mean and a certain standard deviation. The action probability will also be available.
3.Why do UIPS-O and UIPS-P lead to poor performance?

[5] Nachum, Ofir, et al. ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections."" Advances in neural information processing systems 32 (2019).","The usage of off-policy correction and uncertainty-based reweighting is limited to policy-based techniques based on the REINFORCE trick. Such techniques can have higher variance than value-based techniques such as TD3 and SAC, and may lead to unstable training.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688628580476,,,NeurIPS.cc/2023/Conference,1pWNhmbllE,"['NeurIPS.cc/2023/Conference/Submission9558/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411238759,[],3,,,['everyone'],1pWNhmbllE,['NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS'],1688628580476,1702411238759,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several related works that are not cited in the paper, including:

* Sinha et al. (2022) - ""Experience replay with likelihood-free importance weights""
* Liu et al. (2021) - ""Regret minimization experience replay in off-policy reinforcement learning""
* Cai et al. (2023) - ""Reinforcing User Retention in a Billion Scale Short Video Recommender System""
* Chen et al. (2022) - ""Off-policy actor-critic for recommender systems""
* Nachum et al. (2019) - ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections""

The reviewer suggests that these works may be relevant to the paper and could be considered as baselines for comparison.",1,"2022, 2021, 2023, 2022, 2019",Off-policy actor-critic for recommender systems
vioqi1P4xK,"This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for off-policy learning, taking into account the uncertainty in the estimated logging policy. The authors demonstrate that the commonly used method of estimating the logging policy can lead to biased estimators, particularly for samples with small estimated logging probabilities. UIPS addresses this issue by reweighting the propensity scores based on the uncertainty of the estimated logging policy. The paper provides a theoretical analysis of the convergence properties of UIPS and presents experimental results on synthetic and real-world recommendation datasets, comparing against state-of-the-art baselines.","● The paper addresses an important problem in off-policy learning and proposes a novel method, UIPS, to improve the quality of the discovered policy.
● The authors provide a comprehensive theoretical analysis of UIPS, including a convergence guarantee.
● The experimental results demonstrate the effectiveness of UIPS compared to some baselines on both synthetic and real-world datasets.","● There remain some issues unsolved in the paper, such as the availability of the logging policy. See the questions for details.
● There are some related works that are not mentioned in this paper. In off-policy RL, several papers work on behavior-agnostic instance reweighting [1,2]. They compute the prioritization weight without the need of obtaining a behavior policy. There are also papers that discuss the importance ratio term when applying RL to recommendation systems [3,4].
● Introducing another neural network to estimate $\beta^*$ will increase the system complexity and the computational cost during training and testing. This may hinder the practical application of the algorithm.
● The synthetic dataset and the offline evaluation can give biased evaluation results of the algorithms.

[1] Sinha, Samarth, et al. ""Experience replay with likelihood-free importance weights."" Learning for Dynamics and Control Conference. PMLR, 2022.
[2] Liu, Xu-Hui, et al. ""Regret minimization experience replay in off-policy reinforcement learning."" Advances in Neural Information Processing Systems 34 (2021).
[3] Cai, Qingpeng, et al. ""Reinforcing User Retention in a Billion Scale Short Video Recommender System."" arXiv preprint arXiv:2302.01724 (2023).
[4] Chen, Minmin, et al. ""Off-policy actor-critic for recommender systems."" Proceedings of the 16th ACM Conference on Recommender Systems. 2022.
","1.How is this paper related to behavior-agnostic methods [1,2,5]? With a GAN-like estimator, these methods no longer reconstruct all those behavior policies. They may also be regarded as baselines to compare with.
2.Why are the probabilities $\beta^*(a|x)$ not recorded in the data? With stochastic logging policies, it is easy to store probabilities together with state and actions when generating data. With deterministic logging policies, a common practice is to sample actions from a Gaussian distribution, with policy output as mean and a certain standard deviation. The action probability will also be available.
3.Why do UIPS-O and UIPS-P lead to poor performance?

[5] Nachum, Ofir, et al. ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections."" Advances in neural information processing systems 32 (2019).","The usage of off-policy correction and uncertainty-based reweighting is limited to policy-based techniques based on the REINFORCE trick. Such techniques can have higher variance than value-based techniques such as TD3 and SAC, and may lead to unstable training.",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688628580476,,,NeurIPS.cc/2023/Conference,1pWNhmbllE,"['NeurIPS.cc/2023/Conference/Submission9558/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411238759,[],3,,,['everyone'],1pWNhmbllE,['NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS'],1688628580476,1702411238759,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9558/Reviewer_QiPS']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several related works that are not cited in the paper, including:

* Sinha et al. (2022) - ""Experience replay with likelihood-free importance weights""
* Liu et al. (2021) - ""Regret minimization experience replay in off-policy reinforcement learning""
* Cai et al. (2023) - ""Reinforcing User Retention in a Billion Scale Short Video Recommender System""
* Chen et al. (2022) - ""Off-policy actor-critic for recommender systems""
* Nachum et al. (2019) - ""Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections""

The reviewer suggests that these works may be relevant to the paper and could be considered as baselines for comparison.",1,"2022, 2021, 2023, 2022, 2019",Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections
zz8mtgbET1,"The authors tackle the problem of synthesizing most exciting inputs for neurons in the higher visual cortex (V4 in their case) of macaque, with data collected using electrophysiology.

The paper makes two claimed contributions:

1. The authors propose a new encoding architecture which uses a data-driven CNN core with a cross-attention read out layer. The cross-attention layer is parameterized similar to traditional cross-attention in other machine learning papers. In the author's design, there is a learned per-neuron query vector, and spatial key/value embeddings derived from pixel-wise linear projections of the CNN feature map that is shared for all neurons. The authors compare this against a task-optimized backbone with learned gaussian readout. The authors show that the the attention encoder performs better with high probability (via a Wilcoxon test) on novel non-training images. 

2. The authors propose energy guided diffusion. Where by they modify the score-prediction with the derivative of vector valued function (not a score function corresponding to a well posed distribution). They propose a modification which does not require the energy function to take as input noisy images, they accomplish this by using the ""pred_xstart"" code provided by [1].

The authors validate their method by first comparing their method against gradient ascent + gradient blurring in MEI synthesis. They find that EGG is able to generate MEI in a faster fashion and with better cross-architecture generalization properties than GA. 

In the MENI experiment, they show that a tradeoff between naturalness and excitation by adjusting the strength of the gradient of the energy function. They find that their synthesized MENI ($\lambda=1$) are roughly comparable to imagenet top-1 images in cross-arch predicted activations.

In the third experiment, they authors experiment with stimulus reconstruction. They accomplish this by modifying the energy function to minimize the L2 distance between predicted and ground truth neuron activities. They find that EGG regularized stimulus reconstruction is more faithful.

[1] Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" Advances in Neural Information Processing Systems 34 (2021).

In the supplementary, the authors provide additional descriptions of data collection (32 channel) and experiment design. The authors provide further experiments that compare ResNet & Attention encoders, the strength of $\lambda$, and how pure gradient methods compare with EGG in stimulus reconstruction.","The paper on balance is well written, the authors are largely clear in their experimental design and their evaluation. The authors provide sufficient detail in the paper itself for reproduction, with additional code provided in the supplemental. The code is well written and easy to follow.

The use of diffusion models to regularize the synthesis of most exciting inputs for monkey V4 collected using electrophysiology is novel, and to the author's knowledge it has not been attempted before. 

The authors perform a variety of experiments, and I find their proposed design for the attention encoder and evaluation of the attention encoder to be convincing.","The authors did an good job writing a paper that has clarity and do a great job in providing details. But in my opinion, the authors overstate their contribution with regard to ""Energy Guidance"" (EGG). If the author can include additional citations, reduce their overly broad claims, and provide additional experiments/metrics, the paper would be improved significantly.


**The paper could stand out based on the experiments alone, but the authors have emphasized energy guidance to be a central contribution** without citing the vast number of papers in computer vision that have been published in the past two years that:
1. Similarly do not use a well-posed score function in the form of the derivative of a classifier. **This aspect is not novel.** In fact I would argue that most of gradient conditioning papers of diffusion models published today explicitly do not use a score function in the form of a classifier gradient. These papers are not cited.
2. Estimate a clean sample ($x_0$) and do not feed in a noise corrupted image to the model providing guidance. The paragraph in lines 162-174 seem to indicate that this approach is novel in the context of classifier/gradient guidance for diffusion models. However **similar approaches have been widely used in computer vision literature**. These papers are not cited. 

* On contributions and prior work
  * GLIDE from 2021 [1] proposed to perform image synthesis again using the gradient of the dot product of a CLIP image vector and a text vector to modify the diffusion output. Note that GLIDE used a noisy trained version of CLIP to perform guidance, however in this approach it seeks maximization of a dot product output which does not yield a ""proper"" distribution score function when you take the gradient.

  * GLIDE and the DALL-E 2 [2] paper cite crowsonkb's 2021 open source CLIP guidance work [3, 4]. These two codebases combine CLIP guidance with the pred_x0 trick (eq 6 of this paper) without retraining the diffusion or gradient model. Similarly, the hugginface diffusers library minimize the orthodromic distance (derivative is not a proper score function) in CLIP space rather than maximizing a dot product using CLIP, and also use the pred_x0 trick without retraining the diffusion or gradient model. This estimated $x_0$ trick has also been formally described in [5] eq's 3 and 4. I suggest the authors cite at least one of these papers and clarify their contributions.

* On the soundness of the experiments

  * I also found some of the experimental setups to be inconsistent. 
  * in the MEI experiment line 221, the naïve SGD optimizer is used, and this forms the basis of the claim in Figure 4 to show GA is much slower than EGG. However in the image reconstruction experiment line 282, the more sophisticated AdamW optimizer is used. There is no reason why the AdamW optimizer cannot be used with gaussian blur gradient conditioning via filtering at an higher stage of the backpropagation. 
  * for the MEI experiment, GA is run for 1,000 steps, while EGG was run for 100 steps. This does not seem to be an entirely fair comparison time wise. 
  * I'm not sure why the authors decided to normalize the image itself to 25 (line 230) for MEI, 50 (line 251) for MENI, and 60 (line 281) for reconstruction. This step seems to quite explicitly break the energy guidance output. Can you provide a justification in the text for why this is done, and why you use different norms for different tasks? Can the authors perform MEI/SGD/AdamW experiments without this step?

* On the lack of evaluation

  * There is a lack of quantitive evaluation metrics aside from predicted neural activity. The paper almost entirely relies on qualitative claims when it comes to MEI/MENI/Reconstruction output. There are no actual metrics to indicate that the MEI/MENI/Reconstruction outputs are more similar to the ground truth most exciting natural stimuli. 
  * I suggest that the authors use common vision metrics like SSIM/PSNR/MSE to evaluate the low level similarity of the images, or high-level image metrics like perceptual loss (VGG), CLIP cosine distance, or distribution-wise comparisons like FID/CLIP-FID (see question 3 in section below). I don't think all the suggested metrics here are needed, but at least a few (any of SSIM/Perceptual VGG/CLIP for image reconstruction, and any of FID/CLIP-FID for MENI; or if the previous metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images are better?) should be added where appropriate.

Overall I think the authors have presented an interesting system, but there is no citation or acknowledgement of prior work from computer vision which use non-classifier based gradient guidance of diffusion models, or using estimated $x_0$ to alleviate the need for noisy trained classifiers. Otherwise I think the paper is interesting and would be improved if the authors can clarify the scope of their contributions and better quantify their claims. I would happily re-evaluate if the authors can improve this paper in a subsequent revision.

[1] Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" arXiv preprint arXiv:2112.10741 (2021).

[2] Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" arXiv preprint arXiv:2204.06125 (2022).

[3] https://github.com/afiaka87/clip-guided-diffusion/blob/c1d5906225586bc8455bb17c29a3c2caf9a02766/cgd/cgd.py#L141

[4] https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj#scrollTo=X5gODNAMEUCR&line=41&uniqifier=1

[5] Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" arXiv preprint arXiv:2210.16031 (2022).","1. Could you clarify why the MEI experiment uses SGD, but the image reconstruction experiment uses AdamW?
2. For Figure 3, you make qualitative claims that the EGG MEIs are better, can you back this up with quantitive numbers like SSIM/Perceptual loss (VGG)/Inception/CLIP distance against the natural input that most excites the neuron?
3. For Figure 5, can you characterize the distributional similarity of the images using standard image metrics like Fréchet inception distance or CLIP-FID proposed by Kynkäänniemi (MENI vs top-k of natural images for a neuron)? Something like the Figure 4 of Imagen [1] Figure 4's pareto curves which measure how the energy scale affects the image distribution distance (FID/CLIP). For Q3/Q4, if the image metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images for the experiments are more similar/better.
4. For Table 1, Figure 3B, Figure 5C, and Figure 6B, could you clarify what is the ""base"" model, as in which model you use for image synthesis, and which model is the evaluating model?
5. Can you clarify the solver you use in the diffusion model? From the code, it seems like you use the DDPM solver, however there are a variety of stronger solvers (DDIM/PNDM/DPM-solver [2,3,4]) which yield convergence in as few as 10 steps. Is there any reason you decided to go with such an old solver?

Overall I think the clarity of the paper is good, but could be further improved with a few small clarifications and incorporation of standard vision metrics (SSIM/VGG perceptual loss/FID/CLIP-FID).

[1] Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" Advances in Neural Information Processing Systems 35 (2022): 36479-36494.

[2] Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" arXiv preprint arXiv:2010.02502 (2020).

[3] Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" arXiv preprint arXiv:2202.09778 (2022).

[4] Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" arXiv preprint arXiv:2206.00927 (2022).",The authors clearly describe the limitations of their experiments.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1687987775655,,,NeurIPS.cc/2023/Conference,1moStpWGUj,"['NeurIPS.cc/2023/Conference/Submission12152/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411366729,[],2,,,['everyone'],1moStpWGUj,['NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az'],1687987775655,1702411366729,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" (for the ""pred_xstart"" code)
2. Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" (for the use of gradient guidance in diffusion models)
3. Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" (for the use of CLIP guidance in diffusion models)
4. Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" (for the use of estimated $x_0$ to alleviate the need for noisy trained classifiers)
5. Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" (for the use of Fréchet inception distance and CLIP-FID metrics)
6. Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" (for the use of stronger solvers such as DDIM)
7. Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" (for the use of stronger solvers such as PNDM)
8. Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" (for the use of stronger solvers such as DPM-solver)

The reviewer also mentions several online resources, including GitHub repositories and Colab notebooks, that the authors should cite for their use of CLIP guidance and estimated $x_0$ in diffusion models.",1,,Diffusion models beat gans on image synthesis
zz8mtgbET1,"The authors tackle the problem of synthesizing most exciting inputs for neurons in the higher visual cortex (V4 in their case) of macaque, with data collected using electrophysiology.

The paper makes two claimed contributions:

1. The authors propose a new encoding architecture which uses a data-driven CNN core with a cross-attention read out layer. The cross-attention layer is parameterized similar to traditional cross-attention in other machine learning papers. In the author's design, there is a learned per-neuron query vector, and spatial key/value embeddings derived from pixel-wise linear projections of the CNN feature map that is shared for all neurons. The authors compare this against a task-optimized backbone with learned gaussian readout. The authors show that the the attention encoder performs better with high probability (via a Wilcoxon test) on novel non-training images. 

2. The authors propose energy guided diffusion. Where by they modify the score-prediction with the derivative of vector valued function (not a score function corresponding to a well posed distribution). They propose a modification which does not require the energy function to take as input noisy images, they accomplish this by using the ""pred_xstart"" code provided by [1].

The authors validate their method by first comparing their method against gradient ascent + gradient blurring in MEI synthesis. They find that EGG is able to generate MEI in a faster fashion and with better cross-architecture generalization properties than GA. 

In the MENI experiment, they show that a tradeoff between naturalness and excitation by adjusting the strength of the gradient of the energy function. They find that their synthesized MENI ($\lambda=1$) are roughly comparable to imagenet top-1 images in cross-arch predicted activations.

In the third experiment, they authors experiment with stimulus reconstruction. They accomplish this by modifying the energy function to minimize the L2 distance between predicted and ground truth neuron activities. They find that EGG regularized stimulus reconstruction is more faithful.

[1] Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" Advances in Neural Information Processing Systems 34 (2021).

In the supplementary, the authors provide additional descriptions of data collection (32 channel) and experiment design. The authors provide further experiments that compare ResNet & Attention encoders, the strength of $\lambda$, and how pure gradient methods compare with EGG in stimulus reconstruction.","The paper on balance is well written, the authors are largely clear in their experimental design and their evaluation. The authors provide sufficient detail in the paper itself for reproduction, with additional code provided in the supplemental. The code is well written and easy to follow.

The use of diffusion models to regularize the synthesis of most exciting inputs for monkey V4 collected using electrophysiology is novel, and to the author's knowledge it has not been attempted before. 

The authors perform a variety of experiments, and I find their proposed design for the attention encoder and evaluation of the attention encoder to be convincing.","The authors did an good job writing a paper that has clarity and do a great job in providing details. But in my opinion, the authors overstate their contribution with regard to ""Energy Guidance"" (EGG). If the author can include additional citations, reduce their overly broad claims, and provide additional experiments/metrics, the paper would be improved significantly.


**The paper could stand out based on the experiments alone, but the authors have emphasized energy guidance to be a central contribution** without citing the vast number of papers in computer vision that have been published in the past two years that:
1. Similarly do not use a well-posed score function in the form of the derivative of a classifier. **This aspect is not novel.** In fact I would argue that most of gradient conditioning papers of diffusion models published today explicitly do not use a score function in the form of a classifier gradient. These papers are not cited.
2. Estimate a clean sample ($x_0$) and do not feed in a noise corrupted image to the model providing guidance. The paragraph in lines 162-174 seem to indicate that this approach is novel in the context of classifier/gradient guidance for diffusion models. However **similar approaches have been widely used in computer vision literature**. These papers are not cited. 

* On contributions and prior work
  * GLIDE from 2021 [1] proposed to perform image synthesis again using the gradient of the dot product of a CLIP image vector and a text vector to modify the diffusion output. Note that GLIDE used a noisy trained version of CLIP to perform guidance, however in this approach it seeks maximization of a dot product output which does not yield a ""proper"" distribution score function when you take the gradient.

  * GLIDE and the DALL-E 2 [2] paper cite crowsonkb's 2021 open source CLIP guidance work [3, 4]. These two codebases combine CLIP guidance with the pred_x0 trick (eq 6 of this paper) without retraining the diffusion or gradient model. Similarly, the hugginface diffusers library minimize the orthodromic distance (derivative is not a proper score function) in CLIP space rather than maximizing a dot product using CLIP, and also use the pred_x0 trick without retraining the diffusion or gradient model. This estimated $x_0$ trick has also been formally described in [5] eq's 3 and 4. I suggest the authors cite at least one of these papers and clarify their contributions.

* On the soundness of the experiments

  * I also found some of the experimental setups to be inconsistent. 
  * in the MEI experiment line 221, the naïve SGD optimizer is used, and this forms the basis of the claim in Figure 4 to show GA is much slower than EGG. However in the image reconstruction experiment line 282, the more sophisticated AdamW optimizer is used. There is no reason why the AdamW optimizer cannot be used with gaussian blur gradient conditioning via filtering at an higher stage of the backpropagation. 
  * for the MEI experiment, GA is run for 1,000 steps, while EGG was run for 100 steps. This does not seem to be an entirely fair comparison time wise. 
  * I'm not sure why the authors decided to normalize the image itself to 25 (line 230) for MEI, 50 (line 251) for MENI, and 60 (line 281) for reconstruction. This step seems to quite explicitly break the energy guidance output. Can you provide a justification in the text for why this is done, and why you use different norms for different tasks? Can the authors perform MEI/SGD/AdamW experiments without this step?

* On the lack of evaluation

  * There is a lack of quantitive evaluation metrics aside from predicted neural activity. The paper almost entirely relies on qualitative claims when it comes to MEI/MENI/Reconstruction output. There are no actual metrics to indicate that the MEI/MENI/Reconstruction outputs are more similar to the ground truth most exciting natural stimuli. 
  * I suggest that the authors use common vision metrics like SSIM/PSNR/MSE to evaluate the low level similarity of the images, or high-level image metrics like perceptual loss (VGG), CLIP cosine distance, or distribution-wise comparisons like FID/CLIP-FID (see question 3 in section below). I don't think all the suggested metrics here are needed, but at least a few (any of SSIM/Perceptual VGG/CLIP for image reconstruction, and any of FID/CLIP-FID for MENI; or if the previous metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images are better?) should be added where appropriate.

Overall I think the authors have presented an interesting system, but there is no citation or acknowledgement of prior work from computer vision which use non-classifier based gradient guidance of diffusion models, or using estimated $x_0$ to alleviate the need for noisy trained classifiers. Otherwise I think the paper is interesting and would be improved if the authors can clarify the scope of their contributions and better quantify their claims. I would happily re-evaluate if the authors can improve this paper in a subsequent revision.

[1] Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" arXiv preprint arXiv:2112.10741 (2021).

[2] Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" arXiv preprint arXiv:2204.06125 (2022).

[3] https://github.com/afiaka87/clip-guided-diffusion/blob/c1d5906225586bc8455bb17c29a3c2caf9a02766/cgd/cgd.py#L141

[4] https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj#scrollTo=X5gODNAMEUCR&line=41&uniqifier=1

[5] Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" arXiv preprint arXiv:2210.16031 (2022).","1. Could you clarify why the MEI experiment uses SGD, but the image reconstruction experiment uses AdamW?
2. For Figure 3, you make qualitative claims that the EGG MEIs are better, can you back this up with quantitive numbers like SSIM/Perceptual loss (VGG)/Inception/CLIP distance against the natural input that most excites the neuron?
3. For Figure 5, can you characterize the distributional similarity of the images using standard image metrics like Fréchet inception distance or CLIP-FID proposed by Kynkäänniemi (MENI vs top-k of natural images for a neuron)? Something like the Figure 4 of Imagen [1] Figure 4's pareto curves which measure how the energy scale affects the image distribution distance (FID/CLIP). For Q3/Q4, if the image metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images for the experiments are more similar/better.
4. For Table 1, Figure 3B, Figure 5C, and Figure 6B, could you clarify what is the ""base"" model, as in which model you use for image synthesis, and which model is the evaluating model?
5. Can you clarify the solver you use in the diffusion model? From the code, it seems like you use the DDPM solver, however there are a variety of stronger solvers (DDIM/PNDM/DPM-solver [2,3,4]) which yield convergence in as few as 10 steps. Is there any reason you decided to go with such an old solver?

Overall I think the clarity of the paper is good, but could be further improved with a few small clarifications and incorporation of standard vision metrics (SSIM/VGG perceptual loss/FID/CLIP-FID).

[1] Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" Advances in Neural Information Processing Systems 35 (2022): 36479-36494.

[2] Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" arXiv preprint arXiv:2010.02502 (2020).

[3] Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" arXiv preprint arXiv:2202.09778 (2022).

[4] Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" arXiv preprint arXiv:2206.00927 (2022).",The authors clearly describe the limitations of their experiments.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1687987775655,,,NeurIPS.cc/2023/Conference,1moStpWGUj,"['NeurIPS.cc/2023/Conference/Submission12152/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411366729,[],2,,,['everyone'],1moStpWGUj,['NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az'],1687987775655,1702411366729,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" (for the ""pred_xstart"" code)
2. Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" (for the use of gradient guidance in diffusion models)
3. Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" (for the use of CLIP guidance in diffusion models)
4. Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" (for the use of estimated $x_0$ to alleviate the need for noisy trained classifiers)
5. Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" (for the use of Fréchet inception distance and CLIP-FID metrics)
6. Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" (for the use of stronger solvers such as DDIM)
7. Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" (for the use of stronger solvers such as PNDM)
8. Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" (for the use of stronger solvers such as DPM-solver)

The reviewer also mentions several online resources, including GitHub repositories and Colab notebooks, that the authors should cite for their use of CLIP guidance and estimated $x_0$ in diffusion models.",1,,Glide Towards photorealistic image generation and editing with text-guided diffusion models
zz8mtgbET1,"The authors tackle the problem of synthesizing most exciting inputs for neurons in the higher visual cortex (V4 in their case) of macaque, with data collected using electrophysiology.

The paper makes two claimed contributions:

1. The authors propose a new encoding architecture which uses a data-driven CNN core with a cross-attention read out layer. The cross-attention layer is parameterized similar to traditional cross-attention in other machine learning papers. In the author's design, there is a learned per-neuron query vector, and spatial key/value embeddings derived from pixel-wise linear projections of the CNN feature map that is shared for all neurons. The authors compare this against a task-optimized backbone with learned gaussian readout. The authors show that the the attention encoder performs better with high probability (via a Wilcoxon test) on novel non-training images. 

2. The authors propose energy guided diffusion. Where by they modify the score-prediction with the derivative of vector valued function (not a score function corresponding to a well posed distribution). They propose a modification which does not require the energy function to take as input noisy images, they accomplish this by using the ""pred_xstart"" code provided by [1].

The authors validate their method by first comparing their method against gradient ascent + gradient blurring in MEI synthesis. They find that EGG is able to generate MEI in a faster fashion and with better cross-architecture generalization properties than GA. 

In the MENI experiment, they show that a tradeoff between naturalness and excitation by adjusting the strength of the gradient of the energy function. They find that their synthesized MENI ($\lambda=1$) are roughly comparable to imagenet top-1 images in cross-arch predicted activations.

In the third experiment, they authors experiment with stimulus reconstruction. They accomplish this by modifying the energy function to minimize the L2 distance between predicted and ground truth neuron activities. They find that EGG regularized stimulus reconstruction is more faithful.

[1] Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" Advances in Neural Information Processing Systems 34 (2021).

In the supplementary, the authors provide additional descriptions of data collection (32 channel) and experiment design. The authors provide further experiments that compare ResNet & Attention encoders, the strength of $\lambda$, and how pure gradient methods compare with EGG in stimulus reconstruction.","The paper on balance is well written, the authors are largely clear in their experimental design and their evaluation. The authors provide sufficient detail in the paper itself for reproduction, with additional code provided in the supplemental. The code is well written and easy to follow.

The use of diffusion models to regularize the synthesis of most exciting inputs for monkey V4 collected using electrophysiology is novel, and to the author's knowledge it has not been attempted before. 

The authors perform a variety of experiments, and I find their proposed design for the attention encoder and evaluation of the attention encoder to be convincing.","The authors did an good job writing a paper that has clarity and do a great job in providing details. But in my opinion, the authors overstate their contribution with regard to ""Energy Guidance"" (EGG). If the author can include additional citations, reduce their overly broad claims, and provide additional experiments/metrics, the paper would be improved significantly.


**The paper could stand out based on the experiments alone, but the authors have emphasized energy guidance to be a central contribution** without citing the vast number of papers in computer vision that have been published in the past two years that:
1. Similarly do not use a well-posed score function in the form of the derivative of a classifier. **This aspect is not novel.** In fact I would argue that most of gradient conditioning papers of diffusion models published today explicitly do not use a score function in the form of a classifier gradient. These papers are not cited.
2. Estimate a clean sample ($x_0$) and do not feed in a noise corrupted image to the model providing guidance. The paragraph in lines 162-174 seem to indicate that this approach is novel in the context of classifier/gradient guidance for diffusion models. However **similar approaches have been widely used in computer vision literature**. These papers are not cited. 

* On contributions and prior work
  * GLIDE from 2021 [1] proposed to perform image synthesis again using the gradient of the dot product of a CLIP image vector and a text vector to modify the diffusion output. Note that GLIDE used a noisy trained version of CLIP to perform guidance, however in this approach it seeks maximization of a dot product output which does not yield a ""proper"" distribution score function when you take the gradient.

  * GLIDE and the DALL-E 2 [2] paper cite crowsonkb's 2021 open source CLIP guidance work [3, 4]. These two codebases combine CLIP guidance with the pred_x0 trick (eq 6 of this paper) without retraining the diffusion or gradient model. Similarly, the hugginface diffusers library minimize the orthodromic distance (derivative is not a proper score function) in CLIP space rather than maximizing a dot product using CLIP, and also use the pred_x0 trick without retraining the diffusion or gradient model. This estimated $x_0$ trick has also been formally described in [5] eq's 3 and 4. I suggest the authors cite at least one of these papers and clarify their contributions.

* On the soundness of the experiments

  * I also found some of the experimental setups to be inconsistent. 
  * in the MEI experiment line 221, the naïve SGD optimizer is used, and this forms the basis of the claim in Figure 4 to show GA is much slower than EGG. However in the image reconstruction experiment line 282, the more sophisticated AdamW optimizer is used. There is no reason why the AdamW optimizer cannot be used with gaussian blur gradient conditioning via filtering at an higher stage of the backpropagation. 
  * for the MEI experiment, GA is run for 1,000 steps, while EGG was run for 100 steps. This does not seem to be an entirely fair comparison time wise. 
  * I'm not sure why the authors decided to normalize the image itself to 25 (line 230) for MEI, 50 (line 251) for MENI, and 60 (line 281) for reconstruction. This step seems to quite explicitly break the energy guidance output. Can you provide a justification in the text for why this is done, and why you use different norms for different tasks? Can the authors perform MEI/SGD/AdamW experiments without this step?

* On the lack of evaluation

  * There is a lack of quantitive evaluation metrics aside from predicted neural activity. The paper almost entirely relies on qualitative claims when it comes to MEI/MENI/Reconstruction output. There are no actual metrics to indicate that the MEI/MENI/Reconstruction outputs are more similar to the ground truth most exciting natural stimuli. 
  * I suggest that the authors use common vision metrics like SSIM/PSNR/MSE to evaluate the low level similarity of the images, or high-level image metrics like perceptual loss (VGG), CLIP cosine distance, or distribution-wise comparisons like FID/CLIP-FID (see question 3 in section below). I don't think all the suggested metrics here are needed, but at least a few (any of SSIM/Perceptual VGG/CLIP for image reconstruction, and any of FID/CLIP-FID for MENI; or if the previous metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images are better?) should be added where appropriate.

Overall I think the authors have presented an interesting system, but there is no citation or acknowledgement of prior work from computer vision which use non-classifier based gradient guidance of diffusion models, or using estimated $x_0$ to alleviate the need for noisy trained classifiers. Otherwise I think the paper is interesting and would be improved if the authors can clarify the scope of their contributions and better quantify their claims. I would happily re-evaluate if the authors can improve this paper in a subsequent revision.

[1] Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" arXiv preprint arXiv:2112.10741 (2021).

[2] Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" arXiv preprint arXiv:2204.06125 (2022).

[3] https://github.com/afiaka87/clip-guided-diffusion/blob/c1d5906225586bc8455bb17c29a3c2caf9a02766/cgd/cgd.py#L141

[4] https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj#scrollTo=X5gODNAMEUCR&line=41&uniqifier=1

[5] Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" arXiv preprint arXiv:2210.16031 (2022).","1. Could you clarify why the MEI experiment uses SGD, but the image reconstruction experiment uses AdamW?
2. For Figure 3, you make qualitative claims that the EGG MEIs are better, can you back this up with quantitive numbers like SSIM/Perceptual loss (VGG)/Inception/CLIP distance against the natural input that most excites the neuron?
3. For Figure 5, can you characterize the distributional similarity of the images using standard image metrics like Fréchet inception distance or CLIP-FID proposed by Kynkäänniemi (MENI vs top-k of natural images for a neuron)? Something like the Figure 4 of Imagen [1] Figure 4's pareto curves which measure how the energy scale affects the image distribution distance (FID/CLIP). For Q3/Q4, if the image metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images for the experiments are more similar/better.
4. For Table 1, Figure 3B, Figure 5C, and Figure 6B, could you clarify what is the ""base"" model, as in which model you use for image synthesis, and which model is the evaluating model?
5. Can you clarify the solver you use in the diffusion model? From the code, it seems like you use the DDPM solver, however there are a variety of stronger solvers (DDIM/PNDM/DPM-solver [2,3,4]) which yield convergence in as few as 10 steps. Is there any reason you decided to go with such an old solver?

Overall I think the clarity of the paper is good, but could be further improved with a few small clarifications and incorporation of standard vision metrics (SSIM/VGG perceptual loss/FID/CLIP-FID).

[1] Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" Advances in Neural Information Processing Systems 35 (2022): 36479-36494.

[2] Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" arXiv preprint arXiv:2010.02502 (2020).

[3] Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" arXiv preprint arXiv:2202.09778 (2022).

[4] Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" arXiv preprint arXiv:2206.00927 (2022).",The authors clearly describe the limitations of their experiments.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1687987775655,,,NeurIPS.cc/2023/Conference,1moStpWGUj,"['NeurIPS.cc/2023/Conference/Submission12152/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411366729,[],2,,,['everyone'],1moStpWGUj,['NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az'],1687987775655,1702411366729,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" (for the ""pred_xstart"" code)
2. Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" (for the use of gradient guidance in diffusion models)
3. Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" (for the use of CLIP guidance in diffusion models)
4. Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" (for the use of estimated $x_0$ to alleviate the need for noisy trained classifiers)
5. Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" (for the use of Fréchet inception distance and CLIP-FID metrics)
6. Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" (for the use of stronger solvers such as DDIM)
7. Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" (for the use of stronger solvers such as PNDM)
8. Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" (for the use of stronger solvers such as DPM-solver)

The reviewer also mentions several online resources, including GitHub repositories and Colab notebooks, that the authors should cite for their use of CLIP guidance and estimated $x_0$ in diffusion models.",1,,Hierarchical text-conditional image generation with clip latents
zz8mtgbET1,"The authors tackle the problem of synthesizing most exciting inputs for neurons in the higher visual cortex (V4 in their case) of macaque, with data collected using electrophysiology.

The paper makes two claimed contributions:

1. The authors propose a new encoding architecture which uses a data-driven CNN core with a cross-attention read out layer. The cross-attention layer is parameterized similar to traditional cross-attention in other machine learning papers. In the author's design, there is a learned per-neuron query vector, and spatial key/value embeddings derived from pixel-wise linear projections of the CNN feature map that is shared for all neurons. The authors compare this against a task-optimized backbone with learned gaussian readout. The authors show that the the attention encoder performs better with high probability (via a Wilcoxon test) on novel non-training images. 

2. The authors propose energy guided diffusion. Where by they modify the score-prediction with the derivative of vector valued function (not a score function corresponding to a well posed distribution). They propose a modification which does not require the energy function to take as input noisy images, they accomplish this by using the ""pred_xstart"" code provided by [1].

The authors validate their method by first comparing their method against gradient ascent + gradient blurring in MEI synthesis. They find that EGG is able to generate MEI in a faster fashion and with better cross-architecture generalization properties than GA. 

In the MENI experiment, they show that a tradeoff between naturalness and excitation by adjusting the strength of the gradient of the energy function. They find that their synthesized MENI ($\lambda=1$) are roughly comparable to imagenet top-1 images in cross-arch predicted activations.

In the third experiment, they authors experiment with stimulus reconstruction. They accomplish this by modifying the energy function to minimize the L2 distance between predicted and ground truth neuron activities. They find that EGG regularized stimulus reconstruction is more faithful.

[1] Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" Advances in Neural Information Processing Systems 34 (2021).

In the supplementary, the authors provide additional descriptions of data collection (32 channel) and experiment design. The authors provide further experiments that compare ResNet & Attention encoders, the strength of $\lambda$, and how pure gradient methods compare with EGG in stimulus reconstruction.","The paper on balance is well written, the authors are largely clear in their experimental design and their evaluation. The authors provide sufficient detail in the paper itself for reproduction, with additional code provided in the supplemental. The code is well written and easy to follow.

The use of diffusion models to regularize the synthesis of most exciting inputs for monkey V4 collected using electrophysiology is novel, and to the author's knowledge it has not been attempted before. 

The authors perform a variety of experiments, and I find their proposed design for the attention encoder and evaluation of the attention encoder to be convincing.","The authors did an good job writing a paper that has clarity and do a great job in providing details. But in my opinion, the authors overstate their contribution with regard to ""Energy Guidance"" (EGG). If the author can include additional citations, reduce their overly broad claims, and provide additional experiments/metrics, the paper would be improved significantly.


**The paper could stand out based on the experiments alone, but the authors have emphasized energy guidance to be a central contribution** without citing the vast number of papers in computer vision that have been published in the past two years that:
1. Similarly do not use a well-posed score function in the form of the derivative of a classifier. **This aspect is not novel.** In fact I would argue that most of gradient conditioning papers of diffusion models published today explicitly do not use a score function in the form of a classifier gradient. These papers are not cited.
2. Estimate a clean sample ($x_0$) and do not feed in a noise corrupted image to the model providing guidance. The paragraph in lines 162-174 seem to indicate that this approach is novel in the context of classifier/gradient guidance for diffusion models. However **similar approaches have been widely used in computer vision literature**. These papers are not cited. 

* On contributions and prior work
  * GLIDE from 2021 [1] proposed to perform image synthesis again using the gradient of the dot product of a CLIP image vector and a text vector to modify the diffusion output. Note that GLIDE used a noisy trained version of CLIP to perform guidance, however in this approach it seeks maximization of a dot product output which does not yield a ""proper"" distribution score function when you take the gradient.

  * GLIDE and the DALL-E 2 [2] paper cite crowsonkb's 2021 open source CLIP guidance work [3, 4]. These two codebases combine CLIP guidance with the pred_x0 trick (eq 6 of this paper) without retraining the diffusion or gradient model. Similarly, the hugginface diffusers library minimize the orthodromic distance (derivative is not a proper score function) in CLIP space rather than maximizing a dot product using CLIP, and also use the pred_x0 trick without retraining the diffusion or gradient model. This estimated $x_0$ trick has also been formally described in [5] eq's 3 and 4. I suggest the authors cite at least one of these papers and clarify their contributions.

* On the soundness of the experiments

  * I also found some of the experimental setups to be inconsistent. 
  * in the MEI experiment line 221, the naïve SGD optimizer is used, and this forms the basis of the claim in Figure 4 to show GA is much slower than EGG. However in the image reconstruction experiment line 282, the more sophisticated AdamW optimizer is used. There is no reason why the AdamW optimizer cannot be used with gaussian blur gradient conditioning via filtering at an higher stage of the backpropagation. 
  * for the MEI experiment, GA is run for 1,000 steps, while EGG was run for 100 steps. This does not seem to be an entirely fair comparison time wise. 
  * I'm not sure why the authors decided to normalize the image itself to 25 (line 230) for MEI, 50 (line 251) for MENI, and 60 (line 281) for reconstruction. This step seems to quite explicitly break the energy guidance output. Can you provide a justification in the text for why this is done, and why you use different norms for different tasks? Can the authors perform MEI/SGD/AdamW experiments without this step?

* On the lack of evaluation

  * There is a lack of quantitive evaluation metrics aside from predicted neural activity. The paper almost entirely relies on qualitative claims when it comes to MEI/MENI/Reconstruction output. There are no actual metrics to indicate that the MEI/MENI/Reconstruction outputs are more similar to the ground truth most exciting natural stimuli. 
  * I suggest that the authors use common vision metrics like SSIM/PSNR/MSE to evaluate the low level similarity of the images, or high-level image metrics like perceptual loss (VGG), CLIP cosine distance, or distribution-wise comparisons like FID/CLIP-FID (see question 3 in section below). I don't think all the suggested metrics here are needed, but at least a few (any of SSIM/Perceptual VGG/CLIP for image reconstruction, and any of FID/CLIP-FID for MENI; or if the previous metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images are better?) should be added where appropriate.

Overall I think the authors have presented an interesting system, but there is no citation or acknowledgement of prior work from computer vision which use non-classifier based gradient guidance of diffusion models, or using estimated $x_0$ to alleviate the need for noisy trained classifiers. Otherwise I think the paper is interesting and would be improved if the authors can clarify the scope of their contributions and better quantify their claims. I would happily re-evaluate if the authors can improve this paper in a subsequent revision.

[1] Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" arXiv preprint arXiv:2112.10741 (2021).

[2] Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" arXiv preprint arXiv:2204.06125 (2022).

[3] https://github.com/afiaka87/clip-guided-diffusion/blob/c1d5906225586bc8455bb17c29a3c2caf9a02766/cgd/cgd.py#L141

[4] https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj#scrollTo=X5gODNAMEUCR&line=41&uniqifier=1

[5] Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" arXiv preprint arXiv:2210.16031 (2022).","1. Could you clarify why the MEI experiment uses SGD, but the image reconstruction experiment uses AdamW?
2. For Figure 3, you make qualitative claims that the EGG MEIs are better, can you back this up with quantitive numbers like SSIM/Perceptual loss (VGG)/Inception/CLIP distance against the natural input that most excites the neuron?
3. For Figure 5, can you characterize the distributional similarity of the images using standard image metrics like Fréchet inception distance or CLIP-FID proposed by Kynkäänniemi (MENI vs top-k of natural images for a neuron)? Something like the Figure 4 of Imagen [1] Figure 4's pareto curves which measure how the energy scale affects the image distribution distance (FID/CLIP). For Q3/Q4, if the image metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images for the experiments are more similar/better.
4. For Table 1, Figure 3B, Figure 5C, and Figure 6B, could you clarify what is the ""base"" model, as in which model you use for image synthesis, and which model is the evaluating model?
5. Can you clarify the solver you use in the diffusion model? From the code, it seems like you use the DDPM solver, however there are a variety of stronger solvers (DDIM/PNDM/DPM-solver [2,3,4]) which yield convergence in as few as 10 steps. Is there any reason you decided to go with such an old solver?

Overall I think the clarity of the paper is good, but could be further improved with a few small clarifications and incorporation of standard vision metrics (SSIM/VGG perceptual loss/FID/CLIP-FID).

[1] Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" Advances in Neural Information Processing Systems 35 (2022): 36479-36494.

[2] Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" arXiv preprint arXiv:2010.02502 (2020).

[3] Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" arXiv preprint arXiv:2202.09778 (2022).

[4] Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" arXiv preprint arXiv:2206.00927 (2022).",The authors clearly describe the limitations of their experiments.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1687987775655,,,NeurIPS.cc/2023/Conference,1moStpWGUj,"['NeurIPS.cc/2023/Conference/Submission12152/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411366729,[],2,,,['everyone'],1moStpWGUj,['NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az'],1687987775655,1702411366729,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" (for the ""pred_xstart"" code)
2. Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" (for the use of gradient guidance in diffusion models)
3. Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" (for the use of CLIP guidance in diffusion models)
4. Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" (for the use of estimated $x_0$ to alleviate the need for noisy trained classifiers)
5. Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" (for the use of Fréchet inception distance and CLIP-FID metrics)
6. Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" (for the use of stronger solvers such as DDIM)
7. Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" (for the use of stronger solvers such as PNDM)
8. Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" (for the use of stronger solvers such as DPM-solver)

The reviewer also mentions several online resources, including GitHub repositories and Colab notebooks, that the authors should cite for their use of CLIP guidance and estimated $x_0$ in diffusion models.",1,,UPainting Unified Text-to-Image Diffusion Generation with Cross-modal Guidance
zz8mtgbET1,"The authors tackle the problem of synthesizing most exciting inputs for neurons in the higher visual cortex (V4 in their case) of macaque, with data collected using electrophysiology.

The paper makes two claimed contributions:

1. The authors propose a new encoding architecture which uses a data-driven CNN core with a cross-attention read out layer. The cross-attention layer is parameterized similar to traditional cross-attention in other machine learning papers. In the author's design, there is a learned per-neuron query vector, and spatial key/value embeddings derived from pixel-wise linear projections of the CNN feature map that is shared for all neurons. The authors compare this against a task-optimized backbone with learned gaussian readout. The authors show that the the attention encoder performs better with high probability (via a Wilcoxon test) on novel non-training images. 

2. The authors propose energy guided diffusion. Where by they modify the score-prediction with the derivative of vector valued function (not a score function corresponding to a well posed distribution). They propose a modification which does not require the energy function to take as input noisy images, they accomplish this by using the ""pred_xstart"" code provided by [1].

The authors validate their method by first comparing their method against gradient ascent + gradient blurring in MEI synthesis. They find that EGG is able to generate MEI in a faster fashion and with better cross-architecture generalization properties than GA. 

In the MENI experiment, they show that a tradeoff between naturalness and excitation by adjusting the strength of the gradient of the energy function. They find that their synthesized MENI ($\lambda=1$) are roughly comparable to imagenet top-1 images in cross-arch predicted activations.

In the third experiment, they authors experiment with stimulus reconstruction. They accomplish this by modifying the energy function to minimize the L2 distance between predicted and ground truth neuron activities. They find that EGG regularized stimulus reconstruction is more faithful.

[1] Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" Advances in Neural Information Processing Systems 34 (2021).

In the supplementary, the authors provide additional descriptions of data collection (32 channel) and experiment design. The authors provide further experiments that compare ResNet & Attention encoders, the strength of $\lambda$, and how pure gradient methods compare with EGG in stimulus reconstruction.","The paper on balance is well written, the authors are largely clear in their experimental design and their evaluation. The authors provide sufficient detail in the paper itself for reproduction, with additional code provided in the supplemental. The code is well written and easy to follow.

The use of diffusion models to regularize the synthesis of most exciting inputs for monkey V4 collected using electrophysiology is novel, and to the author's knowledge it has not been attempted before. 

The authors perform a variety of experiments, and I find their proposed design for the attention encoder and evaluation of the attention encoder to be convincing.","The authors did an good job writing a paper that has clarity and do a great job in providing details. But in my opinion, the authors overstate their contribution with regard to ""Energy Guidance"" (EGG). If the author can include additional citations, reduce their overly broad claims, and provide additional experiments/metrics, the paper would be improved significantly.


**The paper could stand out based on the experiments alone, but the authors have emphasized energy guidance to be a central contribution** without citing the vast number of papers in computer vision that have been published in the past two years that:
1. Similarly do not use a well-posed score function in the form of the derivative of a classifier. **This aspect is not novel.** In fact I would argue that most of gradient conditioning papers of diffusion models published today explicitly do not use a score function in the form of a classifier gradient. These papers are not cited.
2. Estimate a clean sample ($x_0$) and do not feed in a noise corrupted image to the model providing guidance. The paragraph in lines 162-174 seem to indicate that this approach is novel in the context of classifier/gradient guidance for diffusion models. However **similar approaches have been widely used in computer vision literature**. These papers are not cited. 

* On contributions and prior work
  * GLIDE from 2021 [1] proposed to perform image synthesis again using the gradient of the dot product of a CLIP image vector and a text vector to modify the diffusion output. Note that GLIDE used a noisy trained version of CLIP to perform guidance, however in this approach it seeks maximization of a dot product output which does not yield a ""proper"" distribution score function when you take the gradient.

  * GLIDE and the DALL-E 2 [2] paper cite crowsonkb's 2021 open source CLIP guidance work [3, 4]. These two codebases combine CLIP guidance with the pred_x0 trick (eq 6 of this paper) without retraining the diffusion or gradient model. Similarly, the hugginface diffusers library minimize the orthodromic distance (derivative is not a proper score function) in CLIP space rather than maximizing a dot product using CLIP, and also use the pred_x0 trick without retraining the diffusion or gradient model. This estimated $x_0$ trick has also been formally described in [5] eq's 3 and 4. I suggest the authors cite at least one of these papers and clarify their contributions.

* On the soundness of the experiments

  * I also found some of the experimental setups to be inconsistent. 
  * in the MEI experiment line 221, the naïve SGD optimizer is used, and this forms the basis of the claim in Figure 4 to show GA is much slower than EGG. However in the image reconstruction experiment line 282, the more sophisticated AdamW optimizer is used. There is no reason why the AdamW optimizer cannot be used with gaussian blur gradient conditioning via filtering at an higher stage of the backpropagation. 
  * for the MEI experiment, GA is run for 1,000 steps, while EGG was run for 100 steps. This does not seem to be an entirely fair comparison time wise. 
  * I'm not sure why the authors decided to normalize the image itself to 25 (line 230) for MEI, 50 (line 251) for MENI, and 60 (line 281) for reconstruction. This step seems to quite explicitly break the energy guidance output. Can you provide a justification in the text for why this is done, and why you use different norms for different tasks? Can the authors perform MEI/SGD/AdamW experiments without this step?

* On the lack of evaluation

  * There is a lack of quantitive evaluation metrics aside from predicted neural activity. The paper almost entirely relies on qualitative claims when it comes to MEI/MENI/Reconstruction output. There are no actual metrics to indicate that the MEI/MENI/Reconstruction outputs are more similar to the ground truth most exciting natural stimuli. 
  * I suggest that the authors use common vision metrics like SSIM/PSNR/MSE to evaluate the low level similarity of the images, or high-level image metrics like perceptual loss (VGG), CLIP cosine distance, or distribution-wise comparisons like FID/CLIP-FID (see question 3 in section below). I don't think all the suggested metrics here are needed, but at least a few (any of SSIM/Perceptual VGG/CLIP for image reconstruction, and any of FID/CLIP-FID for MENI; or if the previous metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images are better?) should be added where appropriate.

Overall I think the authors have presented an interesting system, but there is no citation or acknowledgement of prior work from computer vision which use non-classifier based gradient guidance of diffusion models, or using estimated $x_0$ to alleviate the need for noisy trained classifiers. Otherwise I think the paper is interesting and would be improved if the authors can clarify the scope of their contributions and better quantify their claims. I would happily re-evaluate if the authors can improve this paper in a subsequent revision.

[1] Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" arXiv preprint arXiv:2112.10741 (2021).

[2] Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" arXiv preprint arXiv:2204.06125 (2022).

[3] https://github.com/afiaka87/clip-guided-diffusion/blob/c1d5906225586bc8455bb17c29a3c2caf9a02766/cgd/cgd.py#L141

[4] https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj#scrollTo=X5gODNAMEUCR&line=41&uniqifier=1

[5] Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" arXiv preprint arXiv:2210.16031 (2022).","1. Could you clarify why the MEI experiment uses SGD, but the image reconstruction experiment uses AdamW?
2. For Figure 3, you make qualitative claims that the EGG MEIs are better, can you back this up with quantitive numbers like SSIM/Perceptual loss (VGG)/Inception/CLIP distance against the natural input that most excites the neuron?
3. For Figure 5, can you characterize the distributional similarity of the images using standard image metrics like Fréchet inception distance or CLIP-FID proposed by Kynkäänniemi (MENI vs top-k of natural images for a neuron)? Something like the Figure 4 of Imagen [1] Figure 4's pareto curves which measure how the energy scale affects the image distribution distance (FID/CLIP). For Q3/Q4, if the image metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images for the experiments are more similar/better.
4. For Table 1, Figure 3B, Figure 5C, and Figure 6B, could you clarify what is the ""base"" model, as in which model you use for image synthesis, and which model is the evaluating model?
5. Can you clarify the solver you use in the diffusion model? From the code, it seems like you use the DDPM solver, however there are a variety of stronger solvers (DDIM/PNDM/DPM-solver [2,3,4]) which yield convergence in as few as 10 steps. Is there any reason you decided to go with such an old solver?

Overall I think the clarity of the paper is good, but could be further improved with a few small clarifications and incorporation of standard vision metrics (SSIM/VGG perceptual loss/FID/CLIP-FID).

[1] Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" Advances in Neural Information Processing Systems 35 (2022): 36479-36494.

[2] Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" arXiv preprint arXiv:2010.02502 (2020).

[3] Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" arXiv preprint arXiv:2202.09778 (2022).

[4] Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" arXiv preprint arXiv:2206.00927 (2022).",The authors clearly describe the limitations of their experiments.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1687987775655,,,NeurIPS.cc/2023/Conference,1moStpWGUj,"['NeurIPS.cc/2023/Conference/Submission12152/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411366729,[],2,,,['everyone'],1moStpWGUj,['NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az'],1687987775655,1702411366729,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" (for the ""pred_xstart"" code)
2. Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" (for the use of gradient guidance in diffusion models)
3. Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" (for the use of CLIP guidance in diffusion models)
4. Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" (for the use of estimated $x_0$ to alleviate the need for noisy trained classifiers)
5. Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" (for the use of Fréchet inception distance and CLIP-FID metrics)
6. Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" (for the use of stronger solvers such as DDIM)
7. Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" (for the use of stronger solvers such as PNDM)
8. Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" (for the use of stronger solvers such as DPM-solver)

The reviewer also mentions several online resources, including GitHub repositories and Colab notebooks, that the authors should cite for their use of CLIP guidance and estimated $x_0$ in diffusion models.",1,,Photorealistic text-to-image diffusion models with deep language understanding
zz8mtgbET1,"The authors tackle the problem of synthesizing most exciting inputs for neurons in the higher visual cortex (V4 in their case) of macaque, with data collected using electrophysiology.

The paper makes two claimed contributions:

1. The authors propose a new encoding architecture which uses a data-driven CNN core with a cross-attention read out layer. The cross-attention layer is parameterized similar to traditional cross-attention in other machine learning papers. In the author's design, there is a learned per-neuron query vector, and spatial key/value embeddings derived from pixel-wise linear projections of the CNN feature map that is shared for all neurons. The authors compare this against a task-optimized backbone with learned gaussian readout. The authors show that the the attention encoder performs better with high probability (via a Wilcoxon test) on novel non-training images. 

2. The authors propose energy guided diffusion. Where by they modify the score-prediction with the derivative of vector valued function (not a score function corresponding to a well posed distribution). They propose a modification which does not require the energy function to take as input noisy images, they accomplish this by using the ""pred_xstart"" code provided by [1].

The authors validate their method by first comparing their method against gradient ascent + gradient blurring in MEI synthesis. They find that EGG is able to generate MEI in a faster fashion and with better cross-architecture generalization properties than GA. 

In the MENI experiment, they show that a tradeoff between naturalness and excitation by adjusting the strength of the gradient of the energy function. They find that their synthesized MENI ($\lambda=1$) are roughly comparable to imagenet top-1 images in cross-arch predicted activations.

In the third experiment, they authors experiment with stimulus reconstruction. They accomplish this by modifying the energy function to minimize the L2 distance between predicted and ground truth neuron activities. They find that EGG regularized stimulus reconstruction is more faithful.

[1] Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" Advances in Neural Information Processing Systems 34 (2021).

In the supplementary, the authors provide additional descriptions of data collection (32 channel) and experiment design. The authors provide further experiments that compare ResNet & Attention encoders, the strength of $\lambda$, and how pure gradient methods compare with EGG in stimulus reconstruction.","The paper on balance is well written, the authors are largely clear in their experimental design and their evaluation. The authors provide sufficient detail in the paper itself for reproduction, with additional code provided in the supplemental. The code is well written and easy to follow.

The use of diffusion models to regularize the synthesis of most exciting inputs for monkey V4 collected using electrophysiology is novel, and to the author's knowledge it has not been attempted before. 

The authors perform a variety of experiments, and I find their proposed design for the attention encoder and evaluation of the attention encoder to be convincing.","The authors did an good job writing a paper that has clarity and do a great job in providing details. But in my opinion, the authors overstate their contribution with regard to ""Energy Guidance"" (EGG). If the author can include additional citations, reduce their overly broad claims, and provide additional experiments/metrics, the paper would be improved significantly.


**The paper could stand out based on the experiments alone, but the authors have emphasized energy guidance to be a central contribution** without citing the vast number of papers in computer vision that have been published in the past two years that:
1. Similarly do not use a well-posed score function in the form of the derivative of a classifier. **This aspect is not novel.** In fact I would argue that most of gradient conditioning papers of diffusion models published today explicitly do not use a score function in the form of a classifier gradient. These papers are not cited.
2. Estimate a clean sample ($x_0$) and do not feed in a noise corrupted image to the model providing guidance. The paragraph in lines 162-174 seem to indicate that this approach is novel in the context of classifier/gradient guidance for diffusion models. However **similar approaches have been widely used in computer vision literature**. These papers are not cited. 

* On contributions and prior work
  * GLIDE from 2021 [1] proposed to perform image synthesis again using the gradient of the dot product of a CLIP image vector and a text vector to modify the diffusion output. Note that GLIDE used a noisy trained version of CLIP to perform guidance, however in this approach it seeks maximization of a dot product output which does not yield a ""proper"" distribution score function when you take the gradient.

  * GLIDE and the DALL-E 2 [2] paper cite crowsonkb's 2021 open source CLIP guidance work [3, 4]. These two codebases combine CLIP guidance with the pred_x0 trick (eq 6 of this paper) without retraining the diffusion or gradient model. Similarly, the hugginface diffusers library minimize the orthodromic distance (derivative is not a proper score function) in CLIP space rather than maximizing a dot product using CLIP, and also use the pred_x0 trick without retraining the diffusion or gradient model. This estimated $x_0$ trick has also been formally described in [5] eq's 3 and 4. I suggest the authors cite at least one of these papers and clarify their contributions.

* On the soundness of the experiments

  * I also found some of the experimental setups to be inconsistent. 
  * in the MEI experiment line 221, the naïve SGD optimizer is used, and this forms the basis of the claim in Figure 4 to show GA is much slower than EGG. However in the image reconstruction experiment line 282, the more sophisticated AdamW optimizer is used. There is no reason why the AdamW optimizer cannot be used with gaussian blur gradient conditioning via filtering at an higher stage of the backpropagation. 
  * for the MEI experiment, GA is run for 1,000 steps, while EGG was run for 100 steps. This does not seem to be an entirely fair comparison time wise. 
  * I'm not sure why the authors decided to normalize the image itself to 25 (line 230) for MEI, 50 (line 251) for MENI, and 60 (line 281) for reconstruction. This step seems to quite explicitly break the energy guidance output. Can you provide a justification in the text for why this is done, and why you use different norms for different tasks? Can the authors perform MEI/SGD/AdamW experiments without this step?

* On the lack of evaluation

  * There is a lack of quantitive evaluation metrics aside from predicted neural activity. The paper almost entirely relies on qualitative claims when it comes to MEI/MENI/Reconstruction output. There are no actual metrics to indicate that the MEI/MENI/Reconstruction outputs are more similar to the ground truth most exciting natural stimuli. 
  * I suggest that the authors use common vision metrics like SSIM/PSNR/MSE to evaluate the low level similarity of the images, or high-level image metrics like perceptual loss (VGG), CLIP cosine distance, or distribution-wise comparisons like FID/CLIP-FID (see question 3 in section below). I don't think all the suggested metrics here are needed, but at least a few (any of SSIM/Perceptual VGG/CLIP for image reconstruction, and any of FID/CLIP-FID for MENI; or if the previous metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images are better?) should be added where appropriate.

Overall I think the authors have presented an interesting system, but there is no citation or acknowledgement of prior work from computer vision which use non-classifier based gradient guidance of diffusion models, or using estimated $x_0$ to alleviate the need for noisy trained classifiers. Otherwise I think the paper is interesting and would be improved if the authors can clarify the scope of their contributions and better quantify their claims. I would happily re-evaluate if the authors can improve this paper in a subsequent revision.

[1] Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" arXiv preprint arXiv:2112.10741 (2021).

[2] Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" arXiv preprint arXiv:2204.06125 (2022).

[3] https://github.com/afiaka87/clip-guided-diffusion/blob/c1d5906225586bc8455bb17c29a3c2caf9a02766/cgd/cgd.py#L141

[4] https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj#scrollTo=X5gODNAMEUCR&line=41&uniqifier=1

[5] Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" arXiv preprint arXiv:2210.16031 (2022).","1. Could you clarify why the MEI experiment uses SGD, but the image reconstruction experiment uses AdamW?
2. For Figure 3, you make qualitative claims that the EGG MEIs are better, can you back this up with quantitive numbers like SSIM/Perceptual loss (VGG)/Inception/CLIP distance against the natural input that most excites the neuron?
3. For Figure 5, can you characterize the distributional similarity of the images using standard image metrics like Fréchet inception distance or CLIP-FID proposed by Kynkäänniemi (MENI vs top-k of natural images for a neuron)? Something like the Figure 4 of Imagen [1] Figure 4's pareto curves which measure how the energy scale affects the image distribution distance (FID/CLIP). For Q3/Q4, if the image metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images for the experiments are more similar/better.
4. For Table 1, Figure 3B, Figure 5C, and Figure 6B, could you clarify what is the ""base"" model, as in which model you use for image synthesis, and which model is the evaluating model?
5. Can you clarify the solver you use in the diffusion model? From the code, it seems like you use the DDPM solver, however there are a variety of stronger solvers (DDIM/PNDM/DPM-solver [2,3,4]) which yield convergence in as few as 10 steps. Is there any reason you decided to go with such an old solver?

Overall I think the clarity of the paper is good, but could be further improved with a few small clarifications and incorporation of standard vision metrics (SSIM/VGG perceptual loss/FID/CLIP-FID).

[1] Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" Advances in Neural Information Processing Systems 35 (2022): 36479-36494.

[2] Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" arXiv preprint arXiv:2010.02502 (2020).

[3] Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" arXiv preprint arXiv:2202.09778 (2022).

[4] Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" arXiv preprint arXiv:2206.00927 (2022).",The authors clearly describe the limitations of their experiments.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1687987775655,,,NeurIPS.cc/2023/Conference,1moStpWGUj,"['NeurIPS.cc/2023/Conference/Submission12152/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411366729,[],2,,,['everyone'],1moStpWGUj,['NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az'],1687987775655,1702411366729,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" (for the ""pred_xstart"" code)
2. Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" (for the use of gradient guidance in diffusion models)
3. Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" (for the use of CLIP guidance in diffusion models)
4. Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" (for the use of estimated $x_0$ to alleviate the need for noisy trained classifiers)
5. Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" (for the use of Fréchet inception distance and CLIP-FID metrics)
6. Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" (for the use of stronger solvers such as DDIM)
7. Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" (for the use of stronger solvers such as PNDM)
8. Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" (for the use of stronger solvers such as DPM-solver)

The reviewer also mentions several online resources, including GitHub repositories and Colab notebooks, that the authors should cite for their use of CLIP guidance and estimated $x_0$ in diffusion models.",1,,Denoising diffusion implicit models
zz8mtgbET1,"The authors tackle the problem of synthesizing most exciting inputs for neurons in the higher visual cortex (V4 in their case) of macaque, with data collected using electrophysiology.

The paper makes two claimed contributions:

1. The authors propose a new encoding architecture which uses a data-driven CNN core with a cross-attention read out layer. The cross-attention layer is parameterized similar to traditional cross-attention in other machine learning papers. In the author's design, there is a learned per-neuron query vector, and spatial key/value embeddings derived from pixel-wise linear projections of the CNN feature map that is shared for all neurons. The authors compare this against a task-optimized backbone with learned gaussian readout. The authors show that the the attention encoder performs better with high probability (via a Wilcoxon test) on novel non-training images. 

2. The authors propose energy guided diffusion. Where by they modify the score-prediction with the derivative of vector valued function (not a score function corresponding to a well posed distribution). They propose a modification which does not require the energy function to take as input noisy images, they accomplish this by using the ""pred_xstart"" code provided by [1].

The authors validate their method by first comparing their method against gradient ascent + gradient blurring in MEI synthesis. They find that EGG is able to generate MEI in a faster fashion and with better cross-architecture generalization properties than GA. 

In the MENI experiment, they show that a tradeoff between naturalness and excitation by adjusting the strength of the gradient of the energy function. They find that their synthesized MENI ($\lambda=1$) are roughly comparable to imagenet top-1 images in cross-arch predicted activations.

In the third experiment, they authors experiment with stimulus reconstruction. They accomplish this by modifying the energy function to minimize the L2 distance between predicted and ground truth neuron activities. They find that EGG regularized stimulus reconstruction is more faithful.

[1] Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" Advances in Neural Information Processing Systems 34 (2021).

In the supplementary, the authors provide additional descriptions of data collection (32 channel) and experiment design. The authors provide further experiments that compare ResNet & Attention encoders, the strength of $\lambda$, and how pure gradient methods compare with EGG in stimulus reconstruction.","The paper on balance is well written, the authors are largely clear in their experimental design and their evaluation. The authors provide sufficient detail in the paper itself for reproduction, with additional code provided in the supplemental. The code is well written and easy to follow.

The use of diffusion models to regularize the synthesis of most exciting inputs for monkey V4 collected using electrophysiology is novel, and to the author's knowledge it has not been attempted before. 

The authors perform a variety of experiments, and I find their proposed design for the attention encoder and evaluation of the attention encoder to be convincing.","The authors did an good job writing a paper that has clarity and do a great job in providing details. But in my opinion, the authors overstate their contribution with regard to ""Energy Guidance"" (EGG). If the author can include additional citations, reduce their overly broad claims, and provide additional experiments/metrics, the paper would be improved significantly.


**The paper could stand out based on the experiments alone, but the authors have emphasized energy guidance to be a central contribution** without citing the vast number of papers in computer vision that have been published in the past two years that:
1. Similarly do not use a well-posed score function in the form of the derivative of a classifier. **This aspect is not novel.** In fact I would argue that most of gradient conditioning papers of diffusion models published today explicitly do not use a score function in the form of a classifier gradient. These papers are not cited.
2. Estimate a clean sample ($x_0$) and do not feed in a noise corrupted image to the model providing guidance. The paragraph in lines 162-174 seem to indicate that this approach is novel in the context of classifier/gradient guidance for diffusion models. However **similar approaches have been widely used in computer vision literature**. These papers are not cited. 

* On contributions and prior work
  * GLIDE from 2021 [1] proposed to perform image synthesis again using the gradient of the dot product of a CLIP image vector and a text vector to modify the diffusion output. Note that GLIDE used a noisy trained version of CLIP to perform guidance, however in this approach it seeks maximization of a dot product output which does not yield a ""proper"" distribution score function when you take the gradient.

  * GLIDE and the DALL-E 2 [2] paper cite crowsonkb's 2021 open source CLIP guidance work [3, 4]. These two codebases combine CLIP guidance with the pred_x0 trick (eq 6 of this paper) without retraining the diffusion or gradient model. Similarly, the hugginface diffusers library minimize the orthodromic distance (derivative is not a proper score function) in CLIP space rather than maximizing a dot product using CLIP, and also use the pred_x0 trick without retraining the diffusion or gradient model. This estimated $x_0$ trick has also been formally described in [5] eq's 3 and 4. I suggest the authors cite at least one of these papers and clarify their contributions.

* On the soundness of the experiments

  * I also found some of the experimental setups to be inconsistent. 
  * in the MEI experiment line 221, the naïve SGD optimizer is used, and this forms the basis of the claim in Figure 4 to show GA is much slower than EGG. However in the image reconstruction experiment line 282, the more sophisticated AdamW optimizer is used. There is no reason why the AdamW optimizer cannot be used with gaussian blur gradient conditioning via filtering at an higher stage of the backpropagation. 
  * for the MEI experiment, GA is run for 1,000 steps, while EGG was run for 100 steps. This does not seem to be an entirely fair comparison time wise. 
  * I'm not sure why the authors decided to normalize the image itself to 25 (line 230) for MEI, 50 (line 251) for MENI, and 60 (line 281) for reconstruction. This step seems to quite explicitly break the energy guidance output. Can you provide a justification in the text for why this is done, and why you use different norms for different tasks? Can the authors perform MEI/SGD/AdamW experiments without this step?

* On the lack of evaluation

  * There is a lack of quantitive evaluation metrics aside from predicted neural activity. The paper almost entirely relies on qualitative claims when it comes to MEI/MENI/Reconstruction output. There are no actual metrics to indicate that the MEI/MENI/Reconstruction outputs are more similar to the ground truth most exciting natural stimuli. 
  * I suggest that the authors use common vision metrics like SSIM/PSNR/MSE to evaluate the low level similarity of the images, or high-level image metrics like perceptual loss (VGG), CLIP cosine distance, or distribution-wise comparisons like FID/CLIP-FID (see question 3 in section below). I don't think all the suggested metrics here are needed, but at least a few (any of SSIM/Perceptual VGG/CLIP for image reconstruction, and any of FID/CLIP-FID for MENI; or if the previous metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images are better?) should be added where appropriate.

Overall I think the authors have presented an interesting system, but there is no citation or acknowledgement of prior work from computer vision which use non-classifier based gradient guidance of diffusion models, or using estimated $x_0$ to alleviate the need for noisy trained classifiers. Otherwise I think the paper is interesting and would be improved if the authors can clarify the scope of their contributions and better quantify their claims. I would happily re-evaluate if the authors can improve this paper in a subsequent revision.

[1] Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" arXiv preprint arXiv:2112.10741 (2021).

[2] Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" arXiv preprint arXiv:2204.06125 (2022).

[3] https://github.com/afiaka87/clip-guided-diffusion/blob/c1d5906225586bc8455bb17c29a3c2caf9a02766/cgd/cgd.py#L141

[4] https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj#scrollTo=X5gODNAMEUCR&line=41&uniqifier=1

[5] Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" arXiv preprint arXiv:2210.16031 (2022).","1. Could you clarify why the MEI experiment uses SGD, but the image reconstruction experiment uses AdamW?
2. For Figure 3, you make qualitative claims that the EGG MEIs are better, can you back this up with quantitive numbers like SSIM/Perceptual loss (VGG)/Inception/CLIP distance against the natural input that most excites the neuron?
3. For Figure 5, can you characterize the distributional similarity of the images using standard image metrics like Fréchet inception distance or CLIP-FID proposed by Kynkäänniemi (MENI vs top-k of natural images for a neuron)? Something like the Figure 4 of Imagen [1] Figure 4's pareto curves which measure how the energy scale affects the image distribution distance (FID/CLIP). For Q3/Q4, if the image metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images for the experiments are more similar/better.
4. For Table 1, Figure 3B, Figure 5C, and Figure 6B, could you clarify what is the ""base"" model, as in which model you use for image synthesis, and which model is the evaluating model?
5. Can you clarify the solver you use in the diffusion model? From the code, it seems like you use the DDPM solver, however there are a variety of stronger solvers (DDIM/PNDM/DPM-solver [2,3,4]) which yield convergence in as few as 10 steps. Is there any reason you decided to go with such an old solver?

Overall I think the clarity of the paper is good, but could be further improved with a few small clarifications and incorporation of standard vision metrics (SSIM/VGG perceptual loss/FID/CLIP-FID).

[1] Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" Advances in Neural Information Processing Systems 35 (2022): 36479-36494.

[2] Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" arXiv preprint arXiv:2010.02502 (2020).

[3] Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" arXiv preprint arXiv:2202.09778 (2022).

[4] Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" arXiv preprint arXiv:2206.00927 (2022).",The authors clearly describe the limitations of their experiments.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1687987775655,,,NeurIPS.cc/2023/Conference,1moStpWGUj,"['NeurIPS.cc/2023/Conference/Submission12152/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411366729,[],2,,,['everyone'],1moStpWGUj,['NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az'],1687987775655,1702411366729,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" (for the ""pred_xstart"" code)
2. Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" (for the use of gradient guidance in diffusion models)
3. Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" (for the use of CLIP guidance in diffusion models)
4. Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" (for the use of estimated $x_0$ to alleviate the need for noisy trained classifiers)
5. Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" (for the use of Fréchet inception distance and CLIP-FID metrics)
6. Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" (for the use of stronger solvers such as DDIM)
7. Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" (for the use of stronger solvers such as PNDM)
8. Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" (for the use of stronger solvers such as DPM-solver)

The reviewer also mentions several online resources, including GitHub repositories and Colab notebooks, that the authors should cite for their use of CLIP guidance and estimated $x_0$ in diffusion models.",1,,Pseudo numerical methods for diffusion models on manifolds
zz8mtgbET1,"The authors tackle the problem of synthesizing most exciting inputs for neurons in the higher visual cortex (V4 in their case) of macaque, with data collected using electrophysiology.

The paper makes two claimed contributions:

1. The authors propose a new encoding architecture which uses a data-driven CNN core with a cross-attention read out layer. The cross-attention layer is parameterized similar to traditional cross-attention in other machine learning papers. In the author's design, there is a learned per-neuron query vector, and spatial key/value embeddings derived from pixel-wise linear projections of the CNN feature map that is shared for all neurons. The authors compare this against a task-optimized backbone with learned gaussian readout. The authors show that the the attention encoder performs better with high probability (via a Wilcoxon test) on novel non-training images. 

2. The authors propose energy guided diffusion. Where by they modify the score-prediction with the derivative of vector valued function (not a score function corresponding to a well posed distribution). They propose a modification which does not require the energy function to take as input noisy images, they accomplish this by using the ""pred_xstart"" code provided by [1].

The authors validate their method by first comparing their method against gradient ascent + gradient blurring in MEI synthesis. They find that EGG is able to generate MEI in a faster fashion and with better cross-architecture generalization properties than GA. 

In the MENI experiment, they show that a tradeoff between naturalness and excitation by adjusting the strength of the gradient of the energy function. They find that their synthesized MENI ($\lambda=1$) are roughly comparable to imagenet top-1 images in cross-arch predicted activations.

In the third experiment, they authors experiment with stimulus reconstruction. They accomplish this by modifying the energy function to minimize the L2 distance between predicted and ground truth neuron activities. They find that EGG regularized stimulus reconstruction is more faithful.

[1] Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" Advances in Neural Information Processing Systems 34 (2021).

In the supplementary, the authors provide additional descriptions of data collection (32 channel) and experiment design. The authors provide further experiments that compare ResNet & Attention encoders, the strength of $\lambda$, and how pure gradient methods compare with EGG in stimulus reconstruction.","The paper on balance is well written, the authors are largely clear in their experimental design and their evaluation. The authors provide sufficient detail in the paper itself for reproduction, with additional code provided in the supplemental. The code is well written and easy to follow.

The use of diffusion models to regularize the synthesis of most exciting inputs for monkey V4 collected using electrophysiology is novel, and to the author's knowledge it has not been attempted before. 

The authors perform a variety of experiments, and I find their proposed design for the attention encoder and evaluation of the attention encoder to be convincing.","The authors did an good job writing a paper that has clarity and do a great job in providing details. But in my opinion, the authors overstate their contribution with regard to ""Energy Guidance"" (EGG). If the author can include additional citations, reduce their overly broad claims, and provide additional experiments/metrics, the paper would be improved significantly.


**The paper could stand out based on the experiments alone, but the authors have emphasized energy guidance to be a central contribution** without citing the vast number of papers in computer vision that have been published in the past two years that:
1. Similarly do not use a well-posed score function in the form of the derivative of a classifier. **This aspect is not novel.** In fact I would argue that most of gradient conditioning papers of diffusion models published today explicitly do not use a score function in the form of a classifier gradient. These papers are not cited.
2. Estimate a clean sample ($x_0$) and do not feed in a noise corrupted image to the model providing guidance. The paragraph in lines 162-174 seem to indicate that this approach is novel in the context of classifier/gradient guidance for diffusion models. However **similar approaches have been widely used in computer vision literature**. These papers are not cited. 

* On contributions and prior work
  * GLIDE from 2021 [1] proposed to perform image synthesis again using the gradient of the dot product of a CLIP image vector and a text vector to modify the diffusion output. Note that GLIDE used a noisy trained version of CLIP to perform guidance, however in this approach it seeks maximization of a dot product output which does not yield a ""proper"" distribution score function when you take the gradient.

  * GLIDE and the DALL-E 2 [2] paper cite crowsonkb's 2021 open source CLIP guidance work [3, 4]. These two codebases combine CLIP guidance with the pred_x0 trick (eq 6 of this paper) without retraining the diffusion or gradient model. Similarly, the hugginface diffusers library minimize the orthodromic distance (derivative is not a proper score function) in CLIP space rather than maximizing a dot product using CLIP, and also use the pred_x0 trick without retraining the diffusion or gradient model. This estimated $x_0$ trick has also been formally described in [5] eq's 3 and 4. I suggest the authors cite at least one of these papers and clarify their contributions.

* On the soundness of the experiments

  * I also found some of the experimental setups to be inconsistent. 
  * in the MEI experiment line 221, the naïve SGD optimizer is used, and this forms the basis of the claim in Figure 4 to show GA is much slower than EGG. However in the image reconstruction experiment line 282, the more sophisticated AdamW optimizer is used. There is no reason why the AdamW optimizer cannot be used with gaussian blur gradient conditioning via filtering at an higher stage of the backpropagation. 
  * for the MEI experiment, GA is run for 1,000 steps, while EGG was run for 100 steps. This does not seem to be an entirely fair comparison time wise. 
  * I'm not sure why the authors decided to normalize the image itself to 25 (line 230) for MEI, 50 (line 251) for MENI, and 60 (line 281) for reconstruction. This step seems to quite explicitly break the energy guidance output. Can you provide a justification in the text for why this is done, and why you use different norms for different tasks? Can the authors perform MEI/SGD/AdamW experiments without this step?

* On the lack of evaluation

  * There is a lack of quantitive evaluation metrics aside from predicted neural activity. The paper almost entirely relies on qualitative claims when it comes to MEI/MENI/Reconstruction output. There are no actual metrics to indicate that the MEI/MENI/Reconstruction outputs are more similar to the ground truth most exciting natural stimuli. 
  * I suggest that the authors use common vision metrics like SSIM/PSNR/MSE to evaluate the low level similarity of the images, or high-level image metrics like perceptual loss (VGG), CLIP cosine distance, or distribution-wise comparisons like FID/CLIP-FID (see question 3 in section below). I don't think all the suggested metrics here are needed, but at least a few (any of SSIM/Perceptual VGG/CLIP for image reconstruction, and any of FID/CLIP-FID for MENI; or if the previous metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images are better?) should be added where appropriate.

Overall I think the authors have presented an interesting system, but there is no citation or acknowledgement of prior work from computer vision which use non-classifier based gradient guidance of diffusion models, or using estimated $x_0$ to alleviate the need for noisy trained classifiers. Otherwise I think the paper is interesting and would be improved if the authors can clarify the scope of their contributions and better quantify their claims. I would happily re-evaluate if the authors can improve this paper in a subsequent revision.

[1] Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" arXiv preprint arXiv:2112.10741 (2021).

[2] Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" arXiv preprint arXiv:2204.06125 (2022).

[3] https://github.com/afiaka87/clip-guided-diffusion/blob/c1d5906225586bc8455bb17c29a3c2caf9a02766/cgd/cgd.py#L141

[4] https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj#scrollTo=X5gODNAMEUCR&line=41&uniqifier=1

[5] Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" arXiv preprint arXiv:2210.16031 (2022).","1. Could you clarify why the MEI experiment uses SGD, but the image reconstruction experiment uses AdamW?
2. For Figure 3, you make qualitative claims that the EGG MEIs are better, can you back this up with quantitive numbers like SSIM/Perceptual loss (VGG)/Inception/CLIP distance against the natural input that most excites the neuron?
3. For Figure 5, can you characterize the distributional similarity of the images using standard image metrics like Fréchet inception distance or CLIP-FID proposed by Kynkäänniemi (MENI vs top-k of natural images for a neuron)? Something like the Figure 4 of Imagen [1] Figure 4's pareto curves which measure how the energy scale affects the image distribution distance (FID/CLIP). For Q3/Q4, if the image metrics are not possible, perhaps a human survey on Mechanical Turk/Prolific to evaluate if the images for the experiments are more similar/better.
4. For Table 1, Figure 3B, Figure 5C, and Figure 6B, could you clarify what is the ""base"" model, as in which model you use for image synthesis, and which model is the evaluating model?
5. Can you clarify the solver you use in the diffusion model? From the code, it seems like you use the DDPM solver, however there are a variety of stronger solvers (DDIM/PNDM/DPM-solver [2,3,4]) which yield convergence in as few as 10 steps. Is there any reason you decided to go with such an old solver?

Overall I think the clarity of the paper is good, but could be further improved with a few small clarifications and incorporation of standard vision metrics (SSIM/VGG perceptual loss/FID/CLIP-FID).

[1] Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" Advances in Neural Information Processing Systems 35 (2022): 36479-36494.

[2] Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" arXiv preprint arXiv:2010.02502 (2020).

[3] Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" arXiv preprint arXiv:2202.09778 (2022).

[4] Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" arXiv preprint arXiv:2206.00927 (2022).",The authors clearly describe the limitations of their experiments.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1687987775655,,,NeurIPS.cc/2023/Conference,1moStpWGUj,"['NeurIPS.cc/2023/Conference/Submission12152/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411366729,[],2,,,['everyone'],1moStpWGUj,['NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az'],1687987775655,1702411366729,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12152/Reviewer_H4Az']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. Dhariwal, Prafulla, and Alexander Nichol. ""Diffusion models beat gans on image synthesis."" (for the ""pred_xstart"" code)
2. Nichol, Alex, et al. ""Glide: Towards photorealistic image generation and editing with text-guided diffusion models."" (for the use of gradient guidance in diffusion models)
3. Ramesh, Aditya, et al. ""Hierarchical text-conditional image generation with clip latents."" (for the use of CLIP guidance in diffusion models)
4. Li, Wei, et al. ""UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance."" (for the use of estimated $x_0$ to alleviate the need for noisy trained classifiers)
5. Saharia, Chitwan, et al. ""Photorealistic text-to-image diffusion models with deep language understanding."" (for the use of Fréchet inception distance and CLIP-FID metrics)
6. Song, Jiaming, Chenlin Meng, and Stefano Ermon. ""Denoising diffusion implicit models."" (for the use of stronger solvers such as DDIM)
7. Liu, Luping, et al. ""Pseudo numerical methods for diffusion models on manifolds."" (for the use of stronger solvers such as PNDM)
8. Lu, Cheng, et al. ""Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps."" (for the use of stronger solvers such as DPM-solver)

The reviewer also mentions several online resources, including GitHub repositories and Colab notebooks, that the authors should cite for their use of CLIP guidance and estimated $x_0$ in diffusion models.",1,,Dpm-solver A fast ode solver for diffusion probabilistic model sampling in around 10 steps
SjOa6LEghV,The paper provides a theoretical analysis of the estimation error of SDE and ODE methods along with some numerical experiments.,"By perturbing the score function, the authors study how the estimation error changes in ODE and SDE methods. ","It is commonly known that the sample generation error consists of three parts: discretization error, estimation error, and initialization error, see e.g. [1, 2]. It is also commonly known that usually the ODE works better with fewer NFE and SDE works better with more NFE due to the interplay between discretization error and estimation error. The estimation error might not even dominates the total error. I don't think only studying the continuous time model provides very useful insight into how to choose those two methods optimally. There is a significant lack of connection between this theory and what happens in reality. Please comment on this.

[1] Chen et al., Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions


[2] Lee et al., Convergence of score-based generative modeling for general data distributions",see weaknesses.,yes,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,1 poor,2 fair,2 fair,1689229706587,,,NeurIPS.cc/2023/Conference,1mJQq6zYaE,"['NeurIPS.cc/2023/Conference/Submission5830/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411031643,[],4,,,['everyone'],1mJQq6zYaE,['NeurIPS.cc/2023/Conference/Submission5830/Reviewer_LAed'],1689229706587,1702411031643,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5830/Reviewer_LAed']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. Chen et al., ""Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions"" ([1])
2. Lee et al., ""Convergence of score-based generative modeling for general data distributions"" ([2])

These references are mentioned in the ""weaknesses"" section of the review, where the reviewer argues that the authors' analysis of estimation error is incomplete and that they should consider the interplay between discretization error, estimation error, and initialization error, as discussed in the cited references.",1,,Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions 
SjOa6LEghV,The paper provides a theoretical analysis of the estimation error of SDE and ODE methods along with some numerical experiments.,"By perturbing the score function, the authors study how the estimation error changes in ODE and SDE methods. ","It is commonly known that the sample generation error consists of three parts: discretization error, estimation error, and initialization error, see e.g. [1, 2]. It is also commonly known that usually the ODE works better with fewer NFE and SDE works better with more NFE due to the interplay between discretization error and estimation error. The estimation error might not even dominates the total error. I don't think only studying the continuous time model provides very useful insight into how to choose those two methods optimally. There is a significant lack of connection between this theory and what happens in reality. Please comment on this.

[1] Chen et al., Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions


[2] Lee et al., Convergence of score-based generative modeling for general data distributions",see weaknesses.,yes,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,1 poor,2 fair,2 fair,1689229706587,,,NeurIPS.cc/2023/Conference,1mJQq6zYaE,"['NeurIPS.cc/2023/Conference/Submission5830/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411031643,[],4,,,['everyone'],1mJQq6zYaE,['NeurIPS.cc/2023/Conference/Submission5830/Reviewer_LAed'],1689229706587,1702411031643,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5830/Reviewer_LAed']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. Chen et al., ""Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions"" ([1])
2. Lee et al., ""Convergence of score-based generative modeling for general data distributions"" ([2])

These references are mentioned in the ""weaknesses"" section of the review, where the reviewer argues that the authors' analysis of estimation error is incomplete and that they should consider the interplay between discretization error, estimation error, and initialization error, as discussed in the cited references.",1,,Convergence of score-based generative modeling for general data distributions
tKt9rbZEQH,"The authors propose Laplacian canonization, a way to select canonical Laplacian embeddings that resolve the sign and basis ambiguities often present in graph embeddings. The proposed method is a preprocessing step that is relatively fast. The authors perform experiments to evaluate the performance of Laplacian canonization. ","- Originality: Laplacian canonization is an original idea. To the best of my knowledge, this is a novel contribution. 

- Quality: I see this paper as mainly making a methodological contribution. As such, the quality of the experiments are sufficiently extensive to be convincing, and the arguments/theoretical derivations are sound, to the best of my knowledge. 

- Clarity: The presentation of the paper and the motivations are clear.  

- Significance: It is certainly very relevant and important to study graph embeddings for GNNs these days. The approach is novel and works well. It is significant enough to warrant publication at a venue like NeurIPS. ","- One potential weakness is the theoretical portion of the paper, whose results are rather marginal and unsurprising. However, I see this as a methods paper, and the empirical good performance of the proposed method more than makes up for it. 

- One potential limitation of this approach is that not all eigenvectors can be canonized (even though in the datasets 90% of them can be). The authors seem to regard the 90% canonizable rate as a good feature of their approach, rather than a shortcoming. This is a fair perspective. But to provide a more balanced discussion, I would like to see the authors discuss more on this potential limitation, especially since 1. other methods for resolving ambiguities do not suffer from this issue and 2. it is unclear to me whether it is possible that the non-canonizable eigenvectors share any common patterns/structures in real datasets that might bias the results. 

- There is a literature (beyond GNN) that also considers spectral embeddings of graphs and going around the basis/sign ambiguity problems. For example, in point cloud registration, Lai and Zhao's ""Multiscale Nonrigid Point Cloud Registration Using Rotation-Invariant Sliced-Wasserstein Distance via Laplace-Beltrami Eigenmap. SIAM J. Imaging Sci. 10(2): 449-483 (2017)"", and in graph comparison Tam and Dunson's  ""Multiscale graph comparison via the embedded laplacian distance. arXiv preprint arXiv:2201.12064 (2022)."" I suggest incorporating these references and others to round out the prior work section in the appendix. 


- Typos: 

line 347: practical",- See above section ,- See above section on weakness. ,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1688685092654,,,NeurIPS.cc/2023/Conference,1mAYtdoYw6,"['NeurIPS.cc/2023/Conference/Submission5861/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411033388,[],4,,,['everyone'],1mAYtdoYw6,['NeurIPS.cc/2023/Conference/Submission5861/Reviewer_zwuu'],1688685092654,1702411033388,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5861/Reviewer_zwuu']","Yes, the peer review explicitly suggests the authors to cite specific literature. The reviewer mentions two specific papers:

1. Lai and Zhao's ""Multiscale Nonrigid Point Cloud Registration Using Rotation-Invariant Sliced-Wasserstein Distance via Laplace-Beltrami Eigenmap. SIAM J. Imaging Sci. 10(2): 449-483 (2017)""
2. Tam and Dunson's  ""Multiscale graph comparison via the embedded laplacian distance. arXiv preprint arXiv:2201.12064 (2022)""

The reviewer suggests incorporating these references, along with others, to round out the prior work section in the appendix.",1,"2017, 2022, 2022",Multiscale Nonrigid Point Cloud Registration Using Rotation-Invariant Sliced-Wasserstein Distance via Laplace-Beltrami Eigenmap 
tKt9rbZEQH,"The authors propose Laplacian canonization, a way to select canonical Laplacian embeddings that resolve the sign and basis ambiguities often present in graph embeddings. The proposed method is a preprocessing step that is relatively fast. The authors perform experiments to evaluate the performance of Laplacian canonization. ","- Originality: Laplacian canonization is an original idea. To the best of my knowledge, this is a novel contribution. 

- Quality: I see this paper as mainly making a methodological contribution. As such, the quality of the experiments are sufficiently extensive to be convincing, and the arguments/theoretical derivations are sound, to the best of my knowledge. 

- Clarity: The presentation of the paper and the motivations are clear.  

- Significance: It is certainly very relevant and important to study graph embeddings for GNNs these days. The approach is novel and works well. It is significant enough to warrant publication at a venue like NeurIPS. ","- One potential weakness is the theoretical portion of the paper, whose results are rather marginal and unsurprising. However, I see this as a methods paper, and the empirical good performance of the proposed method more than makes up for it. 

- One potential limitation of this approach is that not all eigenvectors can be canonized (even though in the datasets 90% of them can be). The authors seem to regard the 90% canonizable rate as a good feature of their approach, rather than a shortcoming. This is a fair perspective. But to provide a more balanced discussion, I would like to see the authors discuss more on this potential limitation, especially since 1. other methods for resolving ambiguities do not suffer from this issue and 2. it is unclear to me whether it is possible that the non-canonizable eigenvectors share any common patterns/structures in real datasets that might bias the results. 

- There is a literature (beyond GNN) that also considers spectral embeddings of graphs and going around the basis/sign ambiguity problems. For example, in point cloud registration, Lai and Zhao's ""Multiscale Nonrigid Point Cloud Registration Using Rotation-Invariant Sliced-Wasserstein Distance via Laplace-Beltrami Eigenmap. SIAM J. Imaging Sci. 10(2): 449-483 (2017)"", and in graph comparison Tam and Dunson's  ""Multiscale graph comparison via the embedded laplacian distance. arXiv preprint arXiv:2201.12064 (2022)."" I suggest incorporating these references and others to round out the prior work section in the appendix. 


- Typos: 

line 347: practical",- See above section ,- See above section on weakness. ,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1688685092654,,,NeurIPS.cc/2023/Conference,1mAYtdoYw6,"['NeurIPS.cc/2023/Conference/Submission5861/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411033388,[],4,,,['everyone'],1mAYtdoYw6,['NeurIPS.cc/2023/Conference/Submission5861/Reviewer_zwuu'],1688685092654,1702411033388,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5861/Reviewer_zwuu']","Yes, the peer review explicitly suggests the authors to cite specific literature. The reviewer mentions two specific papers:

1. Lai and Zhao's ""Multiscale Nonrigid Point Cloud Registration Using Rotation-Invariant Sliced-Wasserstein Distance via Laplace-Beltrami Eigenmap. SIAM J. Imaging Sci. 10(2): 449-483 (2017)""
2. Tam and Dunson's  ""Multiscale graph comparison via the embedded laplacian distance. arXiv preprint arXiv:2201.12064 (2022)""

The reviewer suggests incorporating these references, along with others, to round out the prior work section in the appendix.",1,"2017, 2022, 2022",Multiscale graph comparison via the embedded laplacian distance
L4kMD5yg0m,"This paper proposed a correlation loss to decrease the correlation among spatially-neighbored elements in the latent space. By only modifying the loss function, this method acts as a plug-in method for the existing neural compression methods with no complexity increasing. Experiments show improvement in the compression performance to several baseline models.","The main contribution of the paper is the correlation loss proposed to decrease the correlation among spatially in latent features. 

Experiments show improvement in the compression performance to several baseline models which provide some insights about designing better neural image compression networks from the prospective of feature map decorrelation.","1) For SwinT and Cheng with checkerboard, the correlation loss seems to only work for lower bitrates, the author may provide some explanation. Besides, The current tested bitrate range is relatively low. RD performance at higher bitrates (>1bpp) should also be provided. 

2) The paper writing can be improved. The citation format in the article looks messy. For equations, Some are ‘eq’, but some are ‘Equation.’

3) Experiments on factorized Ball\'e method [1] and HP+AR+correlation loss can be provided to makes the experiments and evaluation more complete.

[1] Johannes Ballé, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.","Minor issues:

1) Some experiments in the supplementary materials are kind of important and can be put into the main paper.

2) Some citations are still preprint version (e.g. [1][2][3] in reference). Their officially published version should be cited.

",,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,2 fair,3 good,1688449826474,,,NeurIPS.cc/2023/Conference,1ihGy9vAIg,"['NeurIPS.cc/2023/Conference/Submission7715/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411127278,[],2,,,['everyone'],1ihGy9vAIg,['NeurIPS.cc/2023/Conference/Submission7715/Reviewer_vwAa'],1688449826474,1702411127278,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7715/Reviewer_vwAa']","Yes, the peer review explicitly suggests the authors of the paper to cite the officially published version of references [1], [2], and [3], instead of the preprint version. Additionally, it mentions a specific paper by Johannes Ballé, Valero Laparra, and Eero P Simoncelli, titled ""End-to-end optimized image compression"", which is referenced as [1] in the review, implying that the authors should consider experimenting with or citing this work.",1,,End-to-end optimized image compression
cbTdabmPzm,"This paper presents a new visual navigation setting, where the goal is not specified by objects or images but described by a sentence. The sentence encodes the essential information to search for specific objects during navigation. Different from VLN, the task is able to analyze the demand within each sentence rather than step-by-step language guidance. Common sense and knowledge will be explicitly extracted from a large language model. To align the visual and attribute features, CLIP is employed. Overall, the motivation of this work is reasonable and interesting. 

# Post rebuttal
The authors have addressed most of my concerns. If the authors can provide some visualization results, that would be more convincing. Therefore I changed my score to accept.","The motivation of this work is interesting. Providing a demand description to an agent would enable the agent to search for not only one object in order to complete the specified goals. 

The authors also provide semi-automatically generated data for this new task. This would be complementary to the existing VLN or VN tasks.","The introduction part is overly lengthy. The authors exert three pages to describe the motivations of this work, making reading quite tedious. I highly suggest the authors could trim the introduction part a bit.

The natural questions come to this task is whether the proposed method can complete object-goal navigation after training? For example, after the network is trained based on demand driven sentences, whether it can be used as a object-goal navigation agent?

The common knowledge or sense is pre-defined. In the illustrative figures, it seems a demand may correspond to three different objects. Whether this would restrict the options?

In L189, the WG mappings are different depending on the environment. I am not sure whether this implies that these WG mappings need to be specified manually. If so, this may contradict with the original motivation of this work, where humans may not know the environment in advance.","My questions mainly focus on two tasks:

In this comparisons, the results of some baseline methods are significantly lower than the results reported by their original papers. Therefore, I am wondering how the authors adapt their methods to this setting? 

A few works leverage CLIP for VLN or VN tasks. 
CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation
Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments
These works can be adopted for comparison.

As the searched objects have strong association with the demand, what if some demands cannot be processed properly? For example, I am thirsty but I cannot drink cold. Simply providing demands and their corresponding objects would lead to overfitting.",There is no negative societal impact.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,3 good,1688561570639,,,NeurIPS.cc/2023/Conference,1hZwxBgQ3G,"['NeurIPS.cc/2023/Conference/Submission2423/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410834218,[],2,,,['everyone'],1hZwxBgQ3G,['NeurIPS.cc/2023/Conference/Submission2423/Reviewer_i2zg'],1688561570639,1702410834218,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2423/Reviewer_i2zg']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, specifically:

1. CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation
2. Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments

The reviewer mentions that these works can be adopted for comparison, implying that the authors should consider citing them in their paper.",1,,CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation 
cbTdabmPzm,"This paper presents a new visual navigation setting, where the goal is not specified by objects or images but described by a sentence. The sentence encodes the essential information to search for specific objects during navigation. Different from VLN, the task is able to analyze the demand within each sentence rather than step-by-step language guidance. Common sense and knowledge will be explicitly extracted from a large language model. To align the visual and attribute features, CLIP is employed. Overall, the motivation of this work is reasonable and interesting. 

# Post rebuttal
The authors have addressed most of my concerns. If the authors can provide some visualization results, that would be more convincing. Therefore I changed my score to accept.","The motivation of this work is interesting. Providing a demand description to an agent would enable the agent to search for not only one object in order to complete the specified goals. 

The authors also provide semi-automatically generated data for this new task. This would be complementary to the existing VLN or VN tasks.","The introduction part is overly lengthy. The authors exert three pages to describe the motivations of this work, making reading quite tedious. I highly suggest the authors could trim the introduction part a bit.

The natural questions come to this task is whether the proposed method can complete object-goal navigation after training? For example, after the network is trained based on demand driven sentences, whether it can be used as a object-goal navigation agent?

The common knowledge or sense is pre-defined. In the illustrative figures, it seems a demand may correspond to three different objects. Whether this would restrict the options?

In L189, the WG mappings are different depending on the environment. I am not sure whether this implies that these WG mappings need to be specified manually. If so, this may contradict with the original motivation of this work, where humans may not know the environment in advance.","My questions mainly focus on two tasks:

In this comparisons, the results of some baseline methods are significantly lower than the results reported by their original papers. Therefore, I am wondering how the authors adapt their methods to this setting? 

A few works leverage CLIP for VLN or VN tasks. 
CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation
Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments
These works can be adopted for comparison.

As the searched objects have strong association with the demand, what if some demands cannot be processed properly? For example, I am thirsty but I cannot drink cold. Simply providing demands and their corresponding objects would lead to overfitting.",There is no negative societal impact.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,3 good,1688561570639,,,NeurIPS.cc/2023/Conference,1hZwxBgQ3G,"['NeurIPS.cc/2023/Conference/Submission2423/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410834218,[],2,,,['everyone'],1hZwxBgQ3G,['NeurIPS.cc/2023/Conference/Submission2423/Reviewer_i2zg'],1688561570639,1702410834218,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2423/Reviewer_i2zg']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, specifically:

1. CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation
2. Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments

The reviewer mentions that these works can be adopted for comparison, implying that the authors should consider citing them in their paper.",1,,Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments
AOeA63ePf5,"This paper studies the exact conditions under which a learned CBF $b$ with ReLU activations yields the positive invariance property. Even though the set of inputs for which the CBF is non-differentiable has measure zero, the paper shows by example that safety can be still violated. This is because the slope of $b(x)$ can be discontinuous at the boundary $b(x) = 0$. To address this, the paper derives a theoretical result (Proposition 1) that provides a necessary and sufficient condition for a valid ReLU CBF. Based on this central result, the paper presents a verification algorithm that consists of state space discretization and search, activation set enumeration, and solving nonlinear programs. The experiments show faster safety verification for low-dimensional systems than SMT-based approaches.","* ReLU is a popular activation function that is often used for constructing feedforward neural networks. The paper addresses the important theoretical problem of verifying learned CBFs with ReLU activations.
* The paper provides a novel, central theoretical result (Proposition 1) that gives necessary and sufficient conditions for a given ReLU neural CBF to be valid.
* As shown in Section 5, the proposed verification algorithm yields more efficient run-time than SMT-based approaches.
","* From a practical point of view, an engineer or a researcher can always choose a differentiable activation function to design a neural CBF, such as tanh or softplus. Then, the CBF remains differentiable. This leads to a motivational question of why we should use ReLU activations for modeling CBFs in the first place. Specifically, the proposed approach seems to possess several disadvantages compared to learning differentiable CBFs and verifying them. 1) The verification algorithm involves state-space discretization and activation set enumeration, both of which have inherent poor scalability with respect to the state-space dimensionality and the size of the neural network, respectively. In particular, the enumeration seems to suffer a lot as the network becomes complex, as observed in Table 2. 2) The online optimization problem for control (i.e. equation 10) requires solving possibly multiple quadratic programs, adding complexity to the standard CBF-QP.

* The presentation can be improved so that mathematical descriptions are easier to follow. Especially, it is recommended that the paper 1) clearly define dimensionality of every vector and matrix-valued variables introduced in the paper, such as $W_i$, $W_{ij}$. 2) Define clearly any non-trivial variables in the main statement of mathematical results. For instance, the definition of $\bar{W}_0$ and $\bar{r}_0$ should be provided in the main statement of Lemma 1, not in the proofs in the appendix.
","* What are the theoretical and practical advantages of using ReLU activations over differentiable activations (as done in [1], for example) for modeling neural CBFs?

* Shouldn't the last formula in the proof of Lemma 1 use $W_{Lj}$, not $W_{ij}$?

[1] Dawson, Charles, Zengyi Qin, Sicun Gao, and Chuchu Fan. ""Safe nonlinear control using robust neural lyapunov-barrier functions."" In Conference on Robot Learning, pp. 1724-1735. PMLR, 2022.","* The most concerning limitation is the scalability of the verification method. Even if the state space has a low-dimensionality, the usage of more layers or neurons can lead to combinatorial explosion of the runtime complexity.",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1688340857292,,,NeurIPS.cc/2023/Conference,1h2TAUEfc4,"['NeurIPS.cc/2023/Conference/Submission13261/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411420556,[],1,,,['everyone'],1h2TAUEfc4,['NeurIPS.cc/2023/Conference/Submission13261/Reviewer_hbj4'],1688340857292,1702411420556,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13261/Reviewer_hbj4']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

Dawson, Charles, Zengyi Qin, Sicun Gao, and Chuchu Fan. ""Safe nonlinear control using robust neural lyapunov-barrier functions."" In Conference on Robot Learning, pp. 1724-1735. PMLR, 2022.

This is referenced as [1] in the review.",1,2022,Safe nonlinear control using robust neural lyapunov-barrier functions
ctJWoi0w2f,"The authors consider the problem of synthesising control barrier functions parametrised as ReLU neural networks for non-linear deterministic dynamical systems. The authors first extend standard approaches for synthesising control barrier functions to the case where the barrier function is non-differentiable in a zero-measure set. Then, they encode the synthesis of a neural control barrier function as a nonlinear optimisation problem and illustrate the effectiveness of the approach on three benchmarks","-	The theory is sound and the extension of standard results for synthesizing control barrier functions to non-differentiable barriers is surely of interest
-	Paper is overall well written and the problem considered of interest
","-	The main weakness is the scalability of the approach wrt the complexity of the neural barrier function. While I acknowledge that the algorithm is superior in terms of scalability than SMT based approaches, still experimental results are limited to neural networks of 1 hidden layer and 20 neurons at most 
-	Some of the statements in the related works are not precise. In fact, the authors claim that it is not possible to use convex programming to synthesize/verify neural barrier functions. This is not accurate, in fact recent approaches rely on piecewise linear uncertain relaxations of neural networks to encode the problem of verifying neural barrier functions using linear programming [Mathiesen, Frederik Baymler, et al. ""Safety certification for stochastic systems via neural barrier functions."" IEEE Control Systems Letters 7 (2022): 973-978.] or SDP [Mazouz, Rayan, et al. ""Safety guarantees for neural network dynamic systems via stochastic barrier functions."" Advances in Neural Information Processing Systems 35 (2022): 9672-9686.]. Of course, the resulting approach will be more conservative compared to the one proposed by the authors, but more scalable. Also, please expand the discussion of why approaches employed to synthesize neural Lyapunov functions, e.g. [Abate, Alessandro, et al. ""Formal synthesis of Lyapunov neural networks."" IEEE Control Systems Letters 5.3 (2020): 773-778], cannot be employed in the setting of this paper
-	To demonstrate the importance of using neural control barrier functions, I believe that in the experiments there should be at least one experiment where the authors compare with the standard approaches commonly employed to synthesise control barrier functions, e.g. parametrising them as a SoS polynomial. This should serve to empirically demonstrate the advantages in being able to make use of the flexibility of neural networks in the context of barrier functions.
","Please, see Weaknesses Section and in addition consider the following points:

- Equation (7) and (8) quantifies over pairs $(i, j)$ of the set $\mathbf{T}(x) \cap \mathbf{S}$. However, $\mathbf{T}(x)$ is the set of unstable neurons produced by input $x$, hence it is unclear what pairs the quantification is referring to. Is it any two unstable neurons in the set $\mathbf{T}(x) \cap \mathbf{S}$?

- Set $\bar{\mathcal{X}}(\mathbf{S})$ is defined both inside Lemma 1 and right after the Lemma. Please, be consistent
","Please, see Weaknesses Section",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1688685930234,,,NeurIPS.cc/2023/Conference,1h2TAUEfc4,"['NeurIPS.cc/2023/Conference/Submission13261/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411420368,[],3,,,['everyone'],1h2TAUEfc4,['NeurIPS.cc/2023/Conference/Submission13261/Reviewer_U7BK'],1688685930234,1702411420368,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13261/Reviewer_U7BK']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions the following specific papers that the authors should be aware of and possibly cite:
1. Mathiesen, Frederik Baymler, et al. ""Safety certification for stochastic systems via neural barrier functions."" IEEE Control Systems Letters 7 (2022): 973-978.
2. Mazouz, Rayan, et al. ""Safety guarantees for neural network dynamic systems via stochastic barrier functions."" Advances in Neural Information Processing Systems 35 (2022): 9672-9686.
3. Abate, Alessandro, et al. ""Formal synthesis of Lyapunov neural networks."" IEEE Control Systems Letters 5.3 (2020): 773-778.

These papers are mentioned as examples of recent approaches that use convex programming to synthesize/verify neural barrier functions, which contradicts the authors' claim that this is not possible. The reviewer suggests that the authors should be aware of these approaches and discuss why they cannot be employed in the setting of the paper.",1,"2022, 2022, 2020",Safety certification for stochastic systems via neural barrier functions
ctJWoi0w2f,"The authors consider the problem of synthesising control barrier functions parametrised as ReLU neural networks for non-linear deterministic dynamical systems. The authors first extend standard approaches for synthesising control barrier functions to the case where the barrier function is non-differentiable in a zero-measure set. Then, they encode the synthesis of a neural control barrier function as a nonlinear optimisation problem and illustrate the effectiveness of the approach on three benchmarks","-	The theory is sound and the extension of standard results for synthesizing control barrier functions to non-differentiable barriers is surely of interest
-	Paper is overall well written and the problem considered of interest
","-	The main weakness is the scalability of the approach wrt the complexity of the neural barrier function. While I acknowledge that the algorithm is superior in terms of scalability than SMT based approaches, still experimental results are limited to neural networks of 1 hidden layer and 20 neurons at most 
-	Some of the statements in the related works are not precise. In fact, the authors claim that it is not possible to use convex programming to synthesize/verify neural barrier functions. This is not accurate, in fact recent approaches rely on piecewise linear uncertain relaxations of neural networks to encode the problem of verifying neural barrier functions using linear programming [Mathiesen, Frederik Baymler, et al. ""Safety certification for stochastic systems via neural barrier functions."" IEEE Control Systems Letters 7 (2022): 973-978.] or SDP [Mazouz, Rayan, et al. ""Safety guarantees for neural network dynamic systems via stochastic barrier functions."" Advances in Neural Information Processing Systems 35 (2022): 9672-9686.]. Of course, the resulting approach will be more conservative compared to the one proposed by the authors, but more scalable. Also, please expand the discussion of why approaches employed to synthesize neural Lyapunov functions, e.g. [Abate, Alessandro, et al. ""Formal synthesis of Lyapunov neural networks."" IEEE Control Systems Letters 5.3 (2020): 773-778], cannot be employed in the setting of this paper
-	To demonstrate the importance of using neural control barrier functions, I believe that in the experiments there should be at least one experiment where the authors compare with the standard approaches commonly employed to synthesise control barrier functions, e.g. parametrising them as a SoS polynomial. This should serve to empirically demonstrate the advantages in being able to make use of the flexibility of neural networks in the context of barrier functions.
","Please, see Weaknesses Section and in addition consider the following points:

- Equation (7) and (8) quantifies over pairs $(i, j)$ of the set $\mathbf{T}(x) \cap \mathbf{S}$. However, $\mathbf{T}(x)$ is the set of unstable neurons produced by input $x$, hence it is unclear what pairs the quantification is referring to. Is it any two unstable neurons in the set $\mathbf{T}(x) \cap \mathbf{S}$?

- Set $\bar{\mathcal{X}}(\mathbf{S})$ is defined both inside Lemma 1 and right after the Lemma. Please, be consistent
","Please, see Weaknesses Section",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1688685930234,,,NeurIPS.cc/2023/Conference,1h2TAUEfc4,"['NeurIPS.cc/2023/Conference/Submission13261/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411420368,[],3,,,['everyone'],1h2TAUEfc4,['NeurIPS.cc/2023/Conference/Submission13261/Reviewer_U7BK'],1688685930234,1702411420368,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13261/Reviewer_U7BK']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions the following specific papers that the authors should be aware of and possibly cite:
1. Mathiesen, Frederik Baymler, et al. ""Safety certification for stochastic systems via neural barrier functions."" IEEE Control Systems Letters 7 (2022): 973-978.
2. Mazouz, Rayan, et al. ""Safety guarantees for neural network dynamic systems via stochastic barrier functions."" Advances in Neural Information Processing Systems 35 (2022): 9672-9686.
3. Abate, Alessandro, et al. ""Formal synthesis of Lyapunov neural networks."" IEEE Control Systems Letters 5.3 (2020): 773-778.

These papers are mentioned as examples of recent approaches that use convex programming to synthesize/verify neural barrier functions, which contradicts the authors' claim that this is not possible. The reviewer suggests that the authors should be aware of these approaches and discuss why they cannot be employed in the setting of the paper.",1,"2022, 2022, 2020",Safety guarantees for neural network dynamic systems via stochastic barrier functions
ctJWoi0w2f,"The authors consider the problem of synthesising control barrier functions parametrised as ReLU neural networks for non-linear deterministic dynamical systems. The authors first extend standard approaches for synthesising control barrier functions to the case where the barrier function is non-differentiable in a zero-measure set. Then, they encode the synthesis of a neural control barrier function as a nonlinear optimisation problem and illustrate the effectiveness of the approach on three benchmarks","-	The theory is sound and the extension of standard results for synthesizing control barrier functions to non-differentiable barriers is surely of interest
-	Paper is overall well written and the problem considered of interest
","-	The main weakness is the scalability of the approach wrt the complexity of the neural barrier function. While I acknowledge that the algorithm is superior in terms of scalability than SMT based approaches, still experimental results are limited to neural networks of 1 hidden layer and 20 neurons at most 
-	Some of the statements in the related works are not precise. In fact, the authors claim that it is not possible to use convex programming to synthesize/verify neural barrier functions. This is not accurate, in fact recent approaches rely on piecewise linear uncertain relaxations of neural networks to encode the problem of verifying neural barrier functions using linear programming [Mathiesen, Frederik Baymler, et al. ""Safety certification for stochastic systems via neural barrier functions."" IEEE Control Systems Letters 7 (2022): 973-978.] or SDP [Mazouz, Rayan, et al. ""Safety guarantees for neural network dynamic systems via stochastic barrier functions."" Advances in Neural Information Processing Systems 35 (2022): 9672-9686.]. Of course, the resulting approach will be more conservative compared to the one proposed by the authors, but more scalable. Also, please expand the discussion of why approaches employed to synthesize neural Lyapunov functions, e.g. [Abate, Alessandro, et al. ""Formal synthesis of Lyapunov neural networks."" IEEE Control Systems Letters 5.3 (2020): 773-778], cannot be employed in the setting of this paper
-	To demonstrate the importance of using neural control barrier functions, I believe that in the experiments there should be at least one experiment where the authors compare with the standard approaches commonly employed to synthesise control barrier functions, e.g. parametrising them as a SoS polynomial. This should serve to empirically demonstrate the advantages in being able to make use of the flexibility of neural networks in the context of barrier functions.
","Please, see Weaknesses Section and in addition consider the following points:

- Equation (7) and (8) quantifies over pairs $(i, j)$ of the set $\mathbf{T}(x) \cap \mathbf{S}$. However, $\mathbf{T}(x)$ is the set of unstable neurons produced by input $x$, hence it is unclear what pairs the quantification is referring to. Is it any two unstable neurons in the set $\mathbf{T}(x) \cap \mathbf{S}$?

- Set $\bar{\mathcal{X}}(\mathbf{S})$ is defined both inside Lemma 1 and right after the Lemma. Please, be consistent
","Please, see Weaknesses Section",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1688685930234,,,NeurIPS.cc/2023/Conference,1h2TAUEfc4,"['NeurIPS.cc/2023/Conference/Submission13261/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411420368,[],3,,,['everyone'],1h2TAUEfc4,['NeurIPS.cc/2023/Conference/Submission13261/Reviewer_U7BK'],1688685930234,1702411420368,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13261/Reviewer_U7BK']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions the following specific papers that the authors should be aware of and possibly cite:
1. Mathiesen, Frederik Baymler, et al. ""Safety certification for stochastic systems via neural barrier functions."" IEEE Control Systems Letters 7 (2022): 973-978.
2. Mazouz, Rayan, et al. ""Safety guarantees for neural network dynamic systems via stochastic barrier functions."" Advances in Neural Information Processing Systems 35 (2022): 9672-9686.
3. Abate, Alessandro, et al. ""Formal synthesis of Lyapunov neural networks."" IEEE Control Systems Letters 5.3 (2020): 773-778.

These papers are mentioned as examples of recent approaches that use convex programming to synthesize/verify neural barrier functions, which contradicts the authors' claim that this is not possible. The reviewer suggests that the authors should be aware of these approaches and discuss why they cannot be employed in the setting of the paper.",1,"2022, 2022, 2020",Formal synthesis of Lyapunov neural networks
6uQawkUcf2,This paper introduces a method that can enhance the performance of multimodal models in scenarios involving unseen modality interactions.,"1, The issue of ""unseen modality interaction"" explored in this paper is quite novel.

2, This paper maps the features of different modalities into the same space and merges different modalities by simple addition. As the number of modalities increases, the number of parameters required for fusion does not significantly increase.","1, The performance of the baseline in this paper is so low that it makes the entire method proposed by the paper unconvincing. Specifically, although the method of this paper surpasses its own set baseline methods on EPIC-Kitchens, it only achieves an accuracy of 23.7%. Meanwhile, the most naive baseline in the paper [1] also has an accuracy of 23.7%. Therefore, it's hard for me to be convinced by the experimental conclusions of this paper.

2, In Table 2, the authors compare the performance of different methods on different datasets, but why can't many of these methods outperform a simple late-fusion? Is it because there are significant differences in certain settings? If the author cannot clarify the situation here, I would consider the experimental comparison to be very unfair.

3, Also in Table 2, the performance of DEQ Fusion in MM-IMDB is 61.52/53.38, while the performance of MMBTl[2] is 66.8/61.6 (Micro F1/Macro F1). Why choose not to report this method? It's a well-known classic approach that has already one hundred of citations.

[1] What Makes Training Multi-modal Classification Networks Hard?

[2] Supervised Multimodal Bitransformers for Classifying Images and Text",See Weaknesses,"I don't think the experimental results of this paper convince me, so I tend to reject this paper. ",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688528747292,,,NeurIPS.cc/2023/Conference,1g0A9kE8Id,"['NeurIPS.cc/2023/Conference/Submission2917/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410871792,[],3,,,['everyone'],1g0A9kE8Id,['NeurIPS.cc/2023/Conference/Submission2917/Reviewer_adMX'],1688528747292,1702410871792,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2917/Reviewer_adMX']","Yes, the peer review explicitly mentions two specific literature references ([1] and [2]) that the authors should be aware of. 

Reference [1] is ""What Makes Training Multi-modal Classification Networks Hard?"" which is mentioned as a naive baseline method.

Reference [2] is ""Supervised Multimodal Bitransformers for Classifying Images and Text"" (MMBT), which is mentioned as a well-known classic approach that has already received hundreds of citations and achieved better performance than the proposed method in the MM-IMDB dataset. The reviewer suggests that the authors should have reported this method in their comparison.",1,,What Makes Training Multi-modal Classification Networks Hard 
6uQawkUcf2,This paper introduces a method that can enhance the performance of multimodal models in scenarios involving unseen modality interactions.,"1, The issue of ""unseen modality interaction"" explored in this paper is quite novel.

2, This paper maps the features of different modalities into the same space and merges different modalities by simple addition. As the number of modalities increases, the number of parameters required for fusion does not significantly increase.","1, The performance of the baseline in this paper is so low that it makes the entire method proposed by the paper unconvincing. Specifically, although the method of this paper surpasses its own set baseline methods on EPIC-Kitchens, it only achieves an accuracy of 23.7%. Meanwhile, the most naive baseline in the paper [1] also has an accuracy of 23.7%. Therefore, it's hard for me to be convinced by the experimental conclusions of this paper.

2, In Table 2, the authors compare the performance of different methods on different datasets, but why can't many of these methods outperform a simple late-fusion? Is it because there are significant differences in certain settings? If the author cannot clarify the situation here, I would consider the experimental comparison to be very unfair.

3, Also in Table 2, the performance of DEQ Fusion in MM-IMDB is 61.52/53.38, while the performance of MMBTl[2] is 66.8/61.6 (Micro F1/Macro F1). Why choose not to report this method? It's a well-known classic approach that has already one hundred of citations.

[1] What Makes Training Multi-modal Classification Networks Hard?

[2] Supervised Multimodal Bitransformers for Classifying Images and Text",See Weaknesses,"I don't think the experimental results of this paper convince me, so I tend to reject this paper. ",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688528747292,,,NeurIPS.cc/2023/Conference,1g0A9kE8Id,"['NeurIPS.cc/2023/Conference/Submission2917/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410871792,[],3,,,['everyone'],1g0A9kE8Id,['NeurIPS.cc/2023/Conference/Submission2917/Reviewer_adMX'],1688528747292,1702410871792,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission2917/Reviewer_adMX']","Yes, the peer review explicitly mentions two specific literature references ([1] and [2]) that the authors should be aware of. 

Reference [1] is ""What Makes Training Multi-modal Classification Networks Hard?"" which is mentioned as a naive baseline method.

Reference [2] is ""Supervised Multimodal Bitransformers for Classifying Images and Text"" (MMBT), which is mentioned as a well-known classic approach that has already received hundreds of citations and achieved better performance than the proposed method in the MM-IMDB dataset. The reviewer suggests that the authors should have reported this method in their comparison.",1,,Supervised Multimodal Bitransformers for Classifying Images and Text
oRfl0ghqsV,"The paper proposes a theoretical analysis of the PPO algorithm. Some novel techniques such as batch-wise update are proposed so the algorithm can work on the adversarial setting of linear MDPs. A regret bound is given, which is better then or comparable to  previous results. ","The paper is well written, with a clear presentation and is easy to follow. The topic studied in the paper is important. The proposed technique is novel and the theoretical result is solid. ","1. See Question 1. One concern is the similarity between the proposed method and NPG. And additional comparison is needed so the readers could understand the novelty and contribution of the paper more easily. 

2. The algorithm design of OPPO+ is similar to OPPO. The difference is OPPO+ replaces the step-wise update with a multi-batched update, and considers the linear MDP setting so the state-action feature $\phi$ can be directly obtained instead of doing integration. The authors claimed the weakness of OPPO, or linear mixture MDP, is that the integration is computationally expensive. But I would rather just regard it as a separate model setting, instead of a weakness. Therefore, when doing comparisons with previous work, besides the literature on linear MDP, it could be great if the authors can also give a more detailed explanation on the setting of linear mixture MDP. In particular, given that the algorithm formulations are so similar, it might be helpful if the author can explain why it's not straightforward to migrate the proof of OPPO to the linear MDP setting.  For example, it can be added to the challenge & novelty section, which will greatly strengthen their argument. ","1. The updating rule of PPO has a very similar formula to Natural Policy Gradient (NPG). There also have been literatures that apply NPG to linear MDPs, can the authors make a comparison between their approach and these literatures? 

For example, in section 4.2 of [1], the authors applied NPG to linear MDPs, and obtained a convergence rate of order $d^2H^6/\varepsilon^3$, which should imply a regret bound of order $d^{1/2}H^{3/2}K^{3/4}$. 

[1] Liu et al., Optimistic Natural Policy Gradient: a Simple Policy Optimization Algorithm for Online Learning in Linear MDPs, https://arxiv.org/pdf/2305.11032.pdf 

2. Is it possible to extend the analysis to the general function approximation setting? 

3. One novelty of the paper is its ability to handle adversarial rewards. Can the authors explain which part of their algorithm is crucial to achieving this goal? Still take [1] as an example, I think the algorithms have similar formulation, and the difference of [1] is it doesn't use a multi-batched update. Does that mean the multi-batched updating rule is the crucial part for the adversarial setting? ","Since the paper is focused on a theoretical side, it's unlikely to have potential negative social impact. And some limitations and future research directions are mentioned in the Conclusion section. ",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1688347241905,,,NeurIPS.cc/2023/Conference,1bTG4sJ7tN,"['NeurIPS.cc/2023/Conference/Submission7010/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411089103,[],2,,,['everyone'],1bTG4sJ7tN,['NeurIPS.cc/2023/Conference/Submission7010/Reviewer_r8ZK'],1688347241905,1702411089103,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7010/Reviewer_r8ZK']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

Liu et al., Optimistic Natural Policy Gradient: a Simple Policy Optimization Algorithm for Online Learning in Linear MDPs, https://arxiv.org/pdf/2305.11032.pdf

This literature is mentioned in Question 1 of the review, where the reviewer asks the authors to make a comparison between their approach and the literature by Liu et al.",1,2023,Optimistic Natural Policy Gradient a Simple Policy Optimization Algorithm for Online Learning in Linear MDPs
9wvlXn0H5g,"The paper builds upon ConvRNNs and proposes the use of SSM (State Space Models) as a replacement for RNNs. This allows for efficient computation using parallel scan. The proposed method is evaluated on the Long Horizon Moving-MNIST Generation and Long-range 3D Environment Benchmarks datasets, where it achieves promising results.","The organization of this paper is clear and easy to understand. It starts from ConvRNN and replaces RNN with SSM, leading to the instantiation of ConvSSM known as ConvS5.","I think the main issue with this paper is the lack of motivation for the proposed approach, as replacing the matrix multiplication of SSM with convolution seems trivial.","1. The motivation for the proposed approach should be made more explicit, whether it is based on intuitive explanations or considerations of efficiency, among other factors.

2. The experiments conducted in the paper are not comprehensive enough. It would be beneficial to compare the proposed method with other efficient attention-based approaches, such as kernel-based linear attention, 1+elu, performer, and cosformer, on long sequence tasks.

3. More ablation studies are needed to validate the rationale behind the design, specifically regarding the convolutional aspect. The existing ablation studies primarily focus on validating the initialization, but it is equally important to investigate and verify the effectiveness of the convolutional components.

References:
[1] Fast Autoregressive Transformers with Linear Attention
[2] Rethinking Attention with Performers
[3] cosFormer: Rethinking Softmax in Attention",Yes.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1688470748574,,,NeurIPS.cc/2023/Conference,1ZvEtnrHS1,"['NeurIPS.cc/2023/Conference/Submission4031/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410931120,[],3,,,['everyone'],1ZvEtnrHS1,['NeurIPS.cc/2023/Conference/Submission4031/Reviewer_R5qx'],1688470748574,1702410931120,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4031/Reviewer_R5qx']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that the authors should compare their proposed method with, including:

1. Kernel-based linear attention
2. 1+elu
3. Performer ([2] Rethinking Attention with Performers)
4. Cosformer ([3] cosFormer: Rethinking Softmax in Attention)
5. Fast Autoregressive Transformers with Linear Attention ([1])

The reviewer suggests that the authors should conduct experiments to compare their method with these approaches on long sequence tasks.",1,,Rethinking Attention with Performers 
9wvlXn0H5g,"The paper builds upon ConvRNNs and proposes the use of SSM (State Space Models) as a replacement for RNNs. This allows for efficient computation using parallel scan. The proposed method is evaluated on the Long Horizon Moving-MNIST Generation and Long-range 3D Environment Benchmarks datasets, where it achieves promising results.","The organization of this paper is clear and easy to understand. It starts from ConvRNN and replaces RNN with SSM, leading to the instantiation of ConvSSM known as ConvS5.","I think the main issue with this paper is the lack of motivation for the proposed approach, as replacing the matrix multiplication of SSM with convolution seems trivial.","1. The motivation for the proposed approach should be made more explicit, whether it is based on intuitive explanations or considerations of efficiency, among other factors.

2. The experiments conducted in the paper are not comprehensive enough. It would be beneficial to compare the proposed method with other efficient attention-based approaches, such as kernel-based linear attention, 1+elu, performer, and cosformer, on long sequence tasks.

3. More ablation studies are needed to validate the rationale behind the design, specifically regarding the convolutional aspect. The existing ablation studies primarily focus on validating the initialization, but it is equally important to investigate and verify the effectiveness of the convolutional components.

References:
[1] Fast Autoregressive Transformers with Linear Attention
[2] Rethinking Attention with Performers
[3] cosFormer: Rethinking Softmax in Attention",Yes.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1688470748574,,,NeurIPS.cc/2023/Conference,1ZvEtnrHS1,"['NeurIPS.cc/2023/Conference/Submission4031/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410931120,[],3,,,['everyone'],1ZvEtnrHS1,['NeurIPS.cc/2023/Conference/Submission4031/Reviewer_R5qx'],1688470748574,1702410931120,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4031/Reviewer_R5qx']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that the authors should compare their proposed method with, including:

1. Kernel-based linear attention
2. 1+elu
3. Performer ([2] Rethinking Attention with Performers)
4. Cosformer ([3] cosFormer: Rethinking Softmax in Attention)
5. Fast Autoregressive Transformers with Linear Attention ([1])

The reviewer suggests that the authors should conduct experiments to compare their method with these approaches on long sequence tasks.",1,,cosFormer Rethinking Softmax in Attention 
9wvlXn0H5g,"The paper builds upon ConvRNNs and proposes the use of SSM (State Space Models) as a replacement for RNNs. This allows for efficient computation using parallel scan. The proposed method is evaluated on the Long Horizon Moving-MNIST Generation and Long-range 3D Environment Benchmarks datasets, where it achieves promising results.","The organization of this paper is clear and easy to understand. It starts from ConvRNN and replaces RNN with SSM, leading to the instantiation of ConvSSM known as ConvS5.","I think the main issue with this paper is the lack of motivation for the proposed approach, as replacing the matrix multiplication of SSM with convolution seems trivial.","1. The motivation for the proposed approach should be made more explicit, whether it is based on intuitive explanations or considerations of efficiency, among other factors.

2. The experiments conducted in the paper are not comprehensive enough. It would be beneficial to compare the proposed method with other efficient attention-based approaches, such as kernel-based linear attention, 1+elu, performer, and cosformer, on long sequence tasks.

3. More ablation studies are needed to validate the rationale behind the design, specifically regarding the convolutional aspect. The existing ablation studies primarily focus on validating the initialization, but it is equally important to investigate and verify the effectiveness of the convolutional components.

References:
[1] Fast Autoregressive Transformers with Linear Attention
[2] Rethinking Attention with Performers
[3] cosFormer: Rethinking Softmax in Attention",Yes.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1688470748574,,,NeurIPS.cc/2023/Conference,1ZvEtnrHS1,"['NeurIPS.cc/2023/Conference/Submission4031/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410931120,[],3,,,['everyone'],1ZvEtnrHS1,['NeurIPS.cc/2023/Conference/Submission4031/Reviewer_R5qx'],1688470748574,1702410931120,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4031/Reviewer_R5qx']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that the authors should compare their proposed method with, including:

1. Kernel-based linear attention
2. 1+elu
3. Performer ([2] Rethinking Attention with Performers)
4. Cosformer ([3] cosFormer: Rethinking Softmax in Attention)
5. Fast Autoregressive Transformers with Linear Attention ([1])

The reviewer suggests that the authors should conduct experiments to compare their method with these approaches on long sequence tasks.",1,,Fast Autoregressive Transformers with Linear Attention
L0IrfdzQjX,"This paper introduces a method for long-term sequential modeling. The authors extend prior RNN-based work for sequential modeling, i.e. S5[20],  by substituting the linear operations with convolutions.   The authors show the superiority of their method on the future prediction task with transformers. A favorable property of the RNN methods, in general, compared to transformers is that they scale linearly with respect to the sequence length, while the transformer's complexity is quadratic with respect to time.","1) The proposed method makes sense and is a natural extension of prior work.
2) The paper is well-written and easy to follow.","1) Compared to the transformers, the linear complexity with respect to the sequence length is clearly favorable. However, it would be informative to compare the actual computational cost, e.g. in terms of flops.

2) Despite transformers, RNNs are notorious for being difficult to train. It would be great to compare the training of two architecture as well. This can be a potential blocker for scaling the method for more complex datasets.

3) I am not an expert in this domain to have a proper evaluation on the impact of the experiments, but the contributions of this paper sound marginal with the NeurIPS standards. Also, it looks like the paper is missing some prior work to compare[1].

[1] Gao et al,  Simvp: Simpler yet better video prediction, CVPR 2022. ",Please see the weaknesses. ,"No, they have not. However, I do not see a particular negative social impact associated for this work.",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",,Yes,2 fair,3 good,2 fair,1689098906539,,,NeurIPS.cc/2023/Conference,1ZvEtnrHS1,"['NeurIPS.cc/2023/Conference/Submission4031/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410930967,[],5,,,['everyone'],1ZvEtnrHS1,['NeurIPS.cc/2023/Conference/Submission4031/Reviewer_P1WH'],1689098906539,1702410930967,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4031/Reviewer_P1WH']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

""Gao et al, Simvp: Simpler yet better video prediction, CVPR 2022.""

This is mentioned in the weaknesses section, where the reviewer states that ""it looks like the paper is missing some prior work to compare[1]"", with [1] referring to the Gao et al paper.",1,2022,Simvp: Simpler yet better video prediction
lpwRbO06vw,"This paper proves the hypothesis that asymmetric saturation benefits network performance by swapping the position of Batch Normalization and Tanh activation functions. The Swap model generates high sparsity and asymmetric saturation which enables Tanh to behave like a one-sided activation function. Experimental results show the asymmetric distributions consistently outperforms the symmetric ones. However, because the BN and Tanh combination hardly appears in modern networks and there is no convincing theory facilitating network architecture design, the contribution of this paper is of less significance.","1) This paper is easy to follow.
2) This paper provides a comprehensive experimental demonstration to validate asymmetric activation functions are superior to symmetric ones. This implies that ReLU-like activation functions are better than tanh activation functions.
","1) One major concern is the lack of prevalence of the BN and Tanh combination in modern networks. As a result, the analysis and experiments conducted in this study hold limited empirical significance.
2) The benefits of asymmetric activation functions have previously been demonstrated from the perspectives of gradient [1] and expressivity [2]. However, this paper fails to contribute new explanations or adequately discuss the limitations of existing works.
3) The experiments in this paper are restricted to a limited range of network architectures. For example, even the widely used ResNet18 and ResNet50 models have not been evaluated on the ImageNet dataset. 
4) A noticeable error can be identified in Figure 1, where the right bottom subplot indicates that 'BN' exhibits lower sparsity compared to 'Tanh,' contradicting the accompanying label stating 'High Sparsity.'

[1] Maas, A. L., Hannun, A. Y., & Ng, A. Y. Rectifier nonlinearities improve neural network acoustic models. 2013.
[2] Hanin, Boris, and David Rolnick. Complexity of linear regions in deep networks. 2019.
","1）The paper discusses sparsity and asymmetry, which have distinct mathematical definitions. However, it remains unclear which factor is the key determinant of network performance.
2） Whether the proposed swap model effectively addresses the issue of vanishing gradients?
",The authors did not discuss Limitations in the paper.,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,1 poor,2 fair,2 fair,1688527493320,,,NeurIPS.cc/2023/Conference,1WpmOipyYI,"['NeurIPS.cc/2023/Conference/Submission336/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410739397,[],1,,,['everyone'],1WpmOipyYI,['NeurIPS.cc/2023/Conference/Submission336/Reviewer_hcmY'],1688527493320,1702410739397,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission336/Reviewer_hcmY']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific papers:

1. Maas, A. L., Hannun, A. Y., & Ng, A. Y. Rectifier nonlinearities improve neural network acoustic models. 2013.
2. Hanin, Boris, and David Rolnick. Complexity of linear regions in deep networks. 2019.

The reviewer states that the benefits of asymmetric activation functions have previously been demonstrated from the perspectives of gradient [1] and expressivity [2], implying that the authors should be aware of and cite these existing works.",1,"2013, 2019",Rectifier nonlinearities improve neural network acoustic models 
lpwRbO06vw,"This paper proves the hypothesis that asymmetric saturation benefits network performance by swapping the position of Batch Normalization and Tanh activation functions. The Swap model generates high sparsity and asymmetric saturation which enables Tanh to behave like a one-sided activation function. Experimental results show the asymmetric distributions consistently outperforms the symmetric ones. However, because the BN and Tanh combination hardly appears in modern networks and there is no convincing theory facilitating network architecture design, the contribution of this paper is of less significance.","1) This paper is easy to follow.
2) This paper provides a comprehensive experimental demonstration to validate asymmetric activation functions are superior to symmetric ones. This implies that ReLU-like activation functions are better than tanh activation functions.
","1) One major concern is the lack of prevalence of the BN and Tanh combination in modern networks. As a result, the analysis and experiments conducted in this study hold limited empirical significance.
2) The benefits of asymmetric activation functions have previously been demonstrated from the perspectives of gradient [1] and expressivity [2]. However, this paper fails to contribute new explanations or adequately discuss the limitations of existing works.
3) The experiments in this paper are restricted to a limited range of network architectures. For example, even the widely used ResNet18 and ResNet50 models have not been evaluated on the ImageNet dataset. 
4) A noticeable error can be identified in Figure 1, where the right bottom subplot indicates that 'BN' exhibits lower sparsity compared to 'Tanh,' contradicting the accompanying label stating 'High Sparsity.'

[1] Maas, A. L., Hannun, A. Y., & Ng, A. Y. Rectifier nonlinearities improve neural network acoustic models. 2013.
[2] Hanin, Boris, and David Rolnick. Complexity of linear regions in deep networks. 2019.
","1）The paper discusses sparsity and asymmetry, which have distinct mathematical definitions. However, it remains unclear which factor is the key determinant of network performance.
2） Whether the proposed swap model effectively addresses the issue of vanishing gradients?
",The authors did not discuss Limitations in the paper.,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,1 poor,2 fair,2 fair,1688527493320,,,NeurIPS.cc/2023/Conference,1WpmOipyYI,"['NeurIPS.cc/2023/Conference/Submission336/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410739397,[],1,,,['everyone'],1WpmOipyYI,['NeurIPS.cc/2023/Conference/Submission336/Reviewer_hcmY'],1688527493320,1702410739397,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission336/Reviewer_hcmY']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific papers:

1. Maas, A. L., Hannun, A. Y., & Ng, A. Y. Rectifier nonlinearities improve neural network acoustic models. 2013.
2. Hanin, Boris, and David Rolnick. Complexity of linear regions in deep networks. 2019.

The reviewer states that the benefits of asymmetric activation functions have previously been demonstrated from the perspectives of gradient [1] and expressivity [2], implying that the authors should be aware of and cite these existing works.",1,"2013, 2019",Complexity of linear regions in deep networks
6qnwrdiyix,"The paper presents an extension of ""Tiered-RL"", a multi-fidelity RL framework where a ""low-fidelity"" environment is executed in parallel with the ""high-fidelity"" environment, with the purpose of training faster while keeping near-optimal regret. The paper is a theoretical exploration without empirical evaluation","- The paper presents a deep theoretical evaluation of a setting that is very relevant yet not very methodologically-explored: When we have related tasks of varied importances to be solved (such as sim2real), and one task can be leveraged to learn faster another one.
- An algorithm able to guarantee near-optimal regret while training in multiple tasks might be of use in security-critical applications such as robotics or medical domains.","- While the theoretical results sound exciting, I would expect at least a simple empirical evaluation of the proposed algorithm to be provided to show how hard it is to actually implement the algorithm in a practical domain. 

- The ""Tiered RL"" setting sounds very similar (if not exactly the same) as the multi-fidelity RL modeling, that wasn't even cited by the authors. While there is still a novelty in the exact way the problem is solved by running all ""tiers"" (or fidelities) in parallel, the problem formulation seems exactly like multi-fidelity MDPs to me, and I would suggest to use the same multi-fidelity MDP formulation to keep consistency in the literature. Multi-fidelity MDPs are explicitly modeled in this paper:
 
Silva, Felipe Leno, et al. ""Toward Multi-Fidelity Reinforcement Learning for Symbolic Optimization."", Adaptive and Learning Agents (ALA) workshop, 2023.

And the multi-fidelity RL problem has been explored in a similar way in the following papers

Sami Khairy and Prasanna Balaprakash. 2022. Multifidelity reinforcement learning with control variates. arXiv preprint arXiv:2206.05165 (2022).

Mark Cutler, Thomas J Walsh, and Jonathan P How. 2014. Reinforcement learning with multi-fidelity simulators. In 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 3888–3895.

At the very least all of those papers should have been included to your related works section.","- A doubt regarding the setting. Would the agent care about the performance in all tiers? Or is it only important to optimize the performance in the highest tier? If only the highest tier matter, the setting is exactly the same as in multi-fidelity RL as said in the ""weaknesses"" section, otherwise, I would need more clarification of in which practical application this modeling would be useful.",No foreseeable negative societal impact,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1686366925737,,,NeurIPS.cc/2023/Conference,1WMdoiVMov,"['NeurIPS.cc/2023/Conference/Submission3323/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410893340,[],1,,,['everyone'],1WMdoiVMov,['NeurIPS.cc/2023/Conference/Submission3323/Reviewer_MBdo'],1686366925737,1702410893340,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3323/Reviewer_MBdo']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature related to multi-fidelity RL and multi-fidelity MDPs, mentioning the following papers:

1. Silva, Felipe Leno, et al. ""Toward Multi-Fidelity Reinforcement Learning for Symbolic Optimization."", Adaptive and Learning Agents (ALA) workshop, 2023.
2. Sami Khairy and Prasanna Balaprakash. 2022. Multifidelity reinforcement learning with control variates. arXiv preprint arXiv:2206.05165 (2022).
3. Mark Cutler, Thomas J Walsh, and Jonathan P How. 2014. Reinforcement learning with multi-fidelity simulators. In 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 3888–3895.

The reviewer suggests that these papers should be included in the related works section to maintain consistency in the literature and to acknowledge the similarity between the ""Tiered RL"" setting and multi-fidelity RL.",1,"2023, 2022, 2022, 2022, 2014, 2014",Toward Multi-Fidelity Reinforcement Learning for Symbolic Optimization
6qnwrdiyix,"The paper presents an extension of ""Tiered-RL"", a multi-fidelity RL framework where a ""low-fidelity"" environment is executed in parallel with the ""high-fidelity"" environment, with the purpose of training faster while keeping near-optimal regret. The paper is a theoretical exploration without empirical evaluation","- The paper presents a deep theoretical evaluation of a setting that is very relevant yet not very methodologically-explored: When we have related tasks of varied importances to be solved (such as sim2real), and one task can be leveraged to learn faster another one.
- An algorithm able to guarantee near-optimal regret while training in multiple tasks might be of use in security-critical applications such as robotics or medical domains.","- While the theoretical results sound exciting, I would expect at least a simple empirical evaluation of the proposed algorithm to be provided to show how hard it is to actually implement the algorithm in a practical domain. 

- The ""Tiered RL"" setting sounds very similar (if not exactly the same) as the multi-fidelity RL modeling, that wasn't even cited by the authors. While there is still a novelty in the exact way the problem is solved by running all ""tiers"" (or fidelities) in parallel, the problem formulation seems exactly like multi-fidelity MDPs to me, and I would suggest to use the same multi-fidelity MDP formulation to keep consistency in the literature. Multi-fidelity MDPs are explicitly modeled in this paper:
 
Silva, Felipe Leno, et al. ""Toward Multi-Fidelity Reinforcement Learning for Symbolic Optimization."", Adaptive and Learning Agents (ALA) workshop, 2023.

And the multi-fidelity RL problem has been explored in a similar way in the following papers

Sami Khairy and Prasanna Balaprakash. 2022. Multifidelity reinforcement learning with control variates. arXiv preprint arXiv:2206.05165 (2022).

Mark Cutler, Thomas J Walsh, and Jonathan P How. 2014. Reinforcement learning with multi-fidelity simulators. In 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 3888–3895.

At the very least all of those papers should have been included to your related works section.","- A doubt regarding the setting. Would the agent care about the performance in all tiers? Or is it only important to optimize the performance in the highest tier? If only the highest tier matter, the setting is exactly the same as in multi-fidelity RL as said in the ""weaknesses"" section, otherwise, I would need more clarification of in which practical application this modeling would be useful.",No foreseeable negative societal impact,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1686366925737,,,NeurIPS.cc/2023/Conference,1WMdoiVMov,"['NeurIPS.cc/2023/Conference/Submission3323/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410893340,[],1,,,['everyone'],1WMdoiVMov,['NeurIPS.cc/2023/Conference/Submission3323/Reviewer_MBdo'],1686366925737,1702410893340,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3323/Reviewer_MBdo']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature related to multi-fidelity RL and multi-fidelity MDPs, mentioning the following papers:

1. Silva, Felipe Leno, et al. ""Toward Multi-Fidelity Reinforcement Learning for Symbolic Optimization."", Adaptive and Learning Agents (ALA) workshop, 2023.
2. Sami Khairy and Prasanna Balaprakash. 2022. Multifidelity reinforcement learning with control variates. arXiv preprint arXiv:2206.05165 (2022).
3. Mark Cutler, Thomas J Walsh, and Jonathan P How. 2014. Reinforcement learning with multi-fidelity simulators. In 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 3888–3895.

The reviewer suggests that these papers should be included in the related works section to maintain consistency in the literature and to acknowledge the similarity between the ""Tiered RL"" setting and multi-fidelity RL.",1,"2023, 2022, 2022, 2022, 2014, 2014",Multifidelity reinforcement learning with control variates
6qnwrdiyix,"The paper presents an extension of ""Tiered-RL"", a multi-fidelity RL framework where a ""low-fidelity"" environment is executed in parallel with the ""high-fidelity"" environment, with the purpose of training faster while keeping near-optimal regret. The paper is a theoretical exploration without empirical evaluation","- The paper presents a deep theoretical evaluation of a setting that is very relevant yet not very methodologically-explored: When we have related tasks of varied importances to be solved (such as sim2real), and one task can be leveraged to learn faster another one.
- An algorithm able to guarantee near-optimal regret while training in multiple tasks might be of use in security-critical applications such as robotics or medical domains.","- While the theoretical results sound exciting, I would expect at least a simple empirical evaluation of the proposed algorithm to be provided to show how hard it is to actually implement the algorithm in a practical domain. 

- The ""Tiered RL"" setting sounds very similar (if not exactly the same) as the multi-fidelity RL modeling, that wasn't even cited by the authors. While there is still a novelty in the exact way the problem is solved by running all ""tiers"" (or fidelities) in parallel, the problem formulation seems exactly like multi-fidelity MDPs to me, and I would suggest to use the same multi-fidelity MDP formulation to keep consistency in the literature. Multi-fidelity MDPs are explicitly modeled in this paper:
 
Silva, Felipe Leno, et al. ""Toward Multi-Fidelity Reinforcement Learning for Symbolic Optimization."", Adaptive and Learning Agents (ALA) workshop, 2023.

And the multi-fidelity RL problem has been explored in a similar way in the following papers

Sami Khairy and Prasanna Balaprakash. 2022. Multifidelity reinforcement learning with control variates. arXiv preprint arXiv:2206.05165 (2022).

Mark Cutler, Thomas J Walsh, and Jonathan P How. 2014. Reinforcement learning with multi-fidelity simulators. In 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 3888–3895.

At the very least all of those papers should have been included to your related works section.","- A doubt regarding the setting. Would the agent care about the performance in all tiers? Or is it only important to optimize the performance in the highest tier? If only the highest tier matter, the setting is exactly the same as in multi-fidelity RL as said in the ""weaknesses"" section, otherwise, I would need more clarification of in which practical application this modeling would be useful.",No foreseeable negative societal impact,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.,,Yes,2 fair,2 fair,3 good,1686366925737,,,NeurIPS.cc/2023/Conference,1WMdoiVMov,"['NeurIPS.cc/2023/Conference/Submission3323/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410893340,[],1,,,['everyone'],1WMdoiVMov,['NeurIPS.cc/2023/Conference/Submission3323/Reviewer_MBdo'],1686366925737,1702410893340,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3323/Reviewer_MBdo']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature related to multi-fidelity RL and multi-fidelity MDPs, mentioning the following papers:

1. Silva, Felipe Leno, et al. ""Toward Multi-Fidelity Reinforcement Learning for Symbolic Optimization."", Adaptive and Learning Agents (ALA) workshop, 2023.
2. Sami Khairy and Prasanna Balaprakash. 2022. Multifidelity reinforcement learning with control variates. arXiv preprint arXiv:2206.05165 (2022).
3. Mark Cutler, Thomas J Walsh, and Jonathan P How. 2014. Reinforcement learning with multi-fidelity simulators. In 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 3888–3895.

The reviewer suggests that these papers should be included in the related works section to maintain consistency in the literature and to acknowledge the similarity between the ""Tiered RL"" setting and multi-fidelity RL.",1,"2023, 2022, 2022, 2022, 2014, 2014",Reinforcement learning with multi-fidelity simulators
r3KtcuzZR0,"A neural network that learns the filtration for persistent homology on given point cloud data is introduced, theoretically justified, and evaluated experimentally on 2 data sets.","(S1) If this is indeed the first work that considers learning filtrations on point clouds, I find the idea very relevant.

(S2) The filtration learning approach is very nicely motivated and described (Lines 185 – 196).","(W1) The need for learned filtrations should be better motivated. Think of an example point cloud where some other learnable filtration is more meaningful than Rips or DTM, visualize all three filtrations and their PDs. For example, we know that DTM is more suitable than Rips in the presence of outliers, but when is another filtration better than Rips and DTM? This would provide guidance to readers when it would make sense to use your approach, instead of relying simply on the Rips and DTM filtration. From Table 1 results, it seems that some answers might lie in the protein data set you consider. 

(W2) [1] and [2] seem to be related work, but are not referenced?

(W3) The improvement with a learned filtration is good for protein classification, but it is for sure not convincing for the 3D CAD data (whereas it is much more complicated and computationally difficult). It would therefore be useful to provide more information about the data (including visualizations), and more detailed insights. I took a look at Appendix C, but this does not provide answers to these questions. Negative insights are also meaningful, i.e., that learning a filtration might not be useful for a lot of problems, and that relying on Rips or DTM filtration is good enough.

(W4) You write: “… the classification accuracy is better when our method was concatenated with DeepSets/PointNet compared to using DeepSets/PointNet alone. The accuracy when we combine our method and PointMLP is not higher than that of PointMLP. This would mean that concatenating the topological feature is effective when the number of parameters of a DNN-based method is relatively small.” I find this argument extremely flawed, since the number of parameters for PointNet and PointMLP is very comparable?


[1] Zhang, Simon, Soham Mukherjee, and Tamal K. Dey. ""GEFL: Extended Filtration Learning for Graph Classification."" Learning on Graphs Conference. PMLR, 2022.  
[2] Horn, Max, et al. ""Topological graph neural networks."" arXiv preprint arXiv:2102.07835 (2021).
","(Q1) PH wrt some filtrations (e.g., height, useful to distinguish MNIST digit 6 from digit 9) is not isometry-invariant, so why do you impose this condition? 

(Q2) Figure 1: Comment more why this approach is meaningful, since 2 holes are not recognized with 1-dim PD for any of the two point clouds? Consider plotting (next to) more reasonable weights.

(Q3) You never mention simplicial complexes, so that it remains unclear why the filtration discussed on lines 143-144 is Rips, and not Čech?

(Q4) How does your weighted Rips filtration compare to the weighted Rips filtration discussed in [3] (Proposition 3.5), where point cloud point x appears according to its filtration function value (weight) f(x), and an edge (x, y) appears when f(x), f(y) and distance d(x, y) satisfy certain properties?

(Q4) Related to (Q3) and (Q4), do all point cloud points immediately appear in your filtration? In particular, if a point has a really large weight, the balls centered at this point will expand very late in the filtration, but will the point (ball with radius 0) be there from the beginning? This is important e.g. if the point is an outlier, since we commonly want to ignore such a point.

(Q5) “Although (a) and (b) can be learned together since the output of the resulting feature is differentiable with all parameters, it would make the optimization unstable.” Why, can you explain more?

(Q6) In Section 5, can you provide more intuition on what is captured with DistMatrixNet? 


Other minor comments:

-	The homology is persistent (not persistence homology), but we talk about persistence landscapes and images (not persistent landscape or image), rephrase throughout the paper.
-	Line 86: Explicitly mention DeepSets.
-	Line 142: “one can take function S defined by” -> “one can take S to be the distance to point cloud, defined by”
-	Line 156:  “which can also [add: be] computed only by distance matrices”. Do you not need the weights too?
-	Line 168: What is function u, where is it used?
-	Line 169: “we can vectorize persistent a persistent diagram”. Rephrase.
-	Line 234: Do topological features and DNN features have to have the same dimension L?
-	Line 251: “we can approximate any continuous function on X x [0,1]^m can be approximated”. Rephrase.
-	Line 253: Cite specific Appendix.
-	Line 283: “we replaced our topological featured replaced”. Rephrase.
-	Line 285: “Note that we use DistMatrixNet not for the computation of filtration weights.” This sentence seems weird, what do you mean?
-	Table 2 caption: “concatenated the feature” -> “concatenated with the feature”?
-	Mention explicitly that the code is made publicly available.
-	References: Check capitalization of acronyms in paper titles (e.g., Dtm, Perslay, Ripsnet, Toposeg, Homcloud, Pointnet, 3d, Pointet++, Pi-net, 3d shapenets, Sgmnet, …), and be consistent between capital case vs. lower case for journal names.

[3] Anai, Hirokazu, et al. ""DTM-based filtrations."" Topological Data Analysis: The Abel Symposium 2018. Springer International Publishing, 2020.
","Some limitations and corresponding future research directions are mentioned, but they do not address the lack of insights on when a learned filtration can be expected to be beneficial (compared to e.g. Rips and DTM filtrations).",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,2 fair,1688666593969,,,NeurIPS.cc/2023/Conference,1TJaITmK2Q,"['NeurIPS.cc/2023/Conference/Submission1558/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410793530,[],3,,,['everyone'],1TJaITmK2Q,['NeurIPS.cc/2023/Conference/Submission1558/Reviewer_L8GX'],1688666593969,1702410793530,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1558/Reviewer_L8GX']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Zhang, Simon, Soham Mukherjee, and Tamal K. Dey. ""GEFL: Extended Filtration Learning for Graph Classification."" Learning on Graphs Conference. PMLR, 2022.
2. Horn, Max, et al. ""Topological graph neural networks."" arXiv preprint arXiv:2102.07835 (2021).
3. Anai, Hirokazu, et al. ""DTM-based filtrations."" Topological Data Analysis: The Abel Symposium 2018. Springer International Publishing, 2020.

These references are mentioned in the weaknesses section (W2) and questions section (Q4) of the review, indicating that the reviewer believes they are relevant to the topic and should be considered by the authors.",1,"2022, 2021, 2021, 2018, 2020",GEFL Extended Filtration Learning for Graph Classification 
r3KtcuzZR0,"A neural network that learns the filtration for persistent homology on given point cloud data is introduced, theoretically justified, and evaluated experimentally on 2 data sets.","(S1) If this is indeed the first work that considers learning filtrations on point clouds, I find the idea very relevant.

(S2) The filtration learning approach is very nicely motivated and described (Lines 185 – 196).","(W1) The need for learned filtrations should be better motivated. Think of an example point cloud where some other learnable filtration is more meaningful than Rips or DTM, visualize all three filtrations and their PDs. For example, we know that DTM is more suitable than Rips in the presence of outliers, but when is another filtration better than Rips and DTM? This would provide guidance to readers when it would make sense to use your approach, instead of relying simply on the Rips and DTM filtration. From Table 1 results, it seems that some answers might lie in the protein data set you consider. 

(W2) [1] and [2] seem to be related work, but are not referenced?

(W3) The improvement with a learned filtration is good for protein classification, but it is for sure not convincing for the 3D CAD data (whereas it is much more complicated and computationally difficult). It would therefore be useful to provide more information about the data (including visualizations), and more detailed insights. I took a look at Appendix C, but this does not provide answers to these questions. Negative insights are also meaningful, i.e., that learning a filtration might not be useful for a lot of problems, and that relying on Rips or DTM filtration is good enough.

(W4) You write: “… the classification accuracy is better when our method was concatenated with DeepSets/PointNet compared to using DeepSets/PointNet alone. The accuracy when we combine our method and PointMLP is not higher than that of PointMLP. This would mean that concatenating the topological feature is effective when the number of parameters of a DNN-based method is relatively small.” I find this argument extremely flawed, since the number of parameters for PointNet and PointMLP is very comparable?


[1] Zhang, Simon, Soham Mukherjee, and Tamal K. Dey. ""GEFL: Extended Filtration Learning for Graph Classification."" Learning on Graphs Conference. PMLR, 2022.  
[2] Horn, Max, et al. ""Topological graph neural networks."" arXiv preprint arXiv:2102.07835 (2021).
","(Q1) PH wrt some filtrations (e.g., height, useful to distinguish MNIST digit 6 from digit 9) is not isometry-invariant, so why do you impose this condition? 

(Q2) Figure 1: Comment more why this approach is meaningful, since 2 holes are not recognized with 1-dim PD for any of the two point clouds? Consider plotting (next to) more reasonable weights.

(Q3) You never mention simplicial complexes, so that it remains unclear why the filtration discussed on lines 143-144 is Rips, and not Čech?

(Q4) How does your weighted Rips filtration compare to the weighted Rips filtration discussed in [3] (Proposition 3.5), where point cloud point x appears according to its filtration function value (weight) f(x), and an edge (x, y) appears when f(x), f(y) and distance d(x, y) satisfy certain properties?

(Q4) Related to (Q3) and (Q4), do all point cloud points immediately appear in your filtration? In particular, if a point has a really large weight, the balls centered at this point will expand very late in the filtration, but will the point (ball with radius 0) be there from the beginning? This is important e.g. if the point is an outlier, since we commonly want to ignore such a point.

(Q5) “Although (a) and (b) can be learned together since the output of the resulting feature is differentiable with all parameters, it would make the optimization unstable.” Why, can you explain more?

(Q6) In Section 5, can you provide more intuition on what is captured with DistMatrixNet? 


Other minor comments:

-	The homology is persistent (not persistence homology), but we talk about persistence landscapes and images (not persistent landscape or image), rephrase throughout the paper.
-	Line 86: Explicitly mention DeepSets.
-	Line 142: “one can take function S defined by” -> “one can take S to be the distance to point cloud, defined by”
-	Line 156:  “which can also [add: be] computed only by distance matrices”. Do you not need the weights too?
-	Line 168: What is function u, where is it used?
-	Line 169: “we can vectorize persistent a persistent diagram”. Rephrase.
-	Line 234: Do topological features and DNN features have to have the same dimension L?
-	Line 251: “we can approximate any continuous function on X x [0,1]^m can be approximated”. Rephrase.
-	Line 253: Cite specific Appendix.
-	Line 283: “we replaced our topological featured replaced”. Rephrase.
-	Line 285: “Note that we use DistMatrixNet not for the computation of filtration weights.” This sentence seems weird, what do you mean?
-	Table 2 caption: “concatenated the feature” -> “concatenated with the feature”?
-	Mention explicitly that the code is made publicly available.
-	References: Check capitalization of acronyms in paper titles (e.g., Dtm, Perslay, Ripsnet, Toposeg, Homcloud, Pointnet, 3d, Pointet++, Pi-net, 3d shapenets, Sgmnet, …), and be consistent between capital case vs. lower case for journal names.

[3] Anai, Hirokazu, et al. ""DTM-based filtrations."" Topological Data Analysis: The Abel Symposium 2018. Springer International Publishing, 2020.
","Some limitations and corresponding future research directions are mentioned, but they do not address the lack of insights on when a learned filtration can be expected to be beneficial (compared to e.g. Rips and DTM filtrations).",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,2 fair,1688666593969,,,NeurIPS.cc/2023/Conference,1TJaITmK2Q,"['NeurIPS.cc/2023/Conference/Submission1558/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410793530,[],3,,,['everyone'],1TJaITmK2Q,['NeurIPS.cc/2023/Conference/Submission1558/Reviewer_L8GX'],1688666593969,1702410793530,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1558/Reviewer_L8GX']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Zhang, Simon, Soham Mukherjee, and Tamal K. Dey. ""GEFL: Extended Filtration Learning for Graph Classification."" Learning on Graphs Conference. PMLR, 2022.
2. Horn, Max, et al. ""Topological graph neural networks."" arXiv preprint arXiv:2102.07835 (2021).
3. Anai, Hirokazu, et al. ""DTM-based filtrations."" Topological Data Analysis: The Abel Symposium 2018. Springer International Publishing, 2020.

These references are mentioned in the weaknesses section (W2) and questions section (Q4) of the review, indicating that the reviewer believes they are relevant to the topic and should be considered by the authors.",1,"2022, 2021, 2021, 2018, 2020",Topological graph neural networks 
r3KtcuzZR0,"A neural network that learns the filtration for persistent homology on given point cloud data is introduced, theoretically justified, and evaluated experimentally on 2 data sets.","(S1) If this is indeed the first work that considers learning filtrations on point clouds, I find the idea very relevant.

(S2) The filtration learning approach is very nicely motivated and described (Lines 185 – 196).","(W1) The need for learned filtrations should be better motivated. Think of an example point cloud where some other learnable filtration is more meaningful than Rips or DTM, visualize all three filtrations and their PDs. For example, we know that DTM is more suitable than Rips in the presence of outliers, but when is another filtration better than Rips and DTM? This would provide guidance to readers when it would make sense to use your approach, instead of relying simply on the Rips and DTM filtration. From Table 1 results, it seems that some answers might lie in the protein data set you consider. 

(W2) [1] and [2] seem to be related work, but are not referenced?

(W3) The improvement with a learned filtration is good for protein classification, but it is for sure not convincing for the 3D CAD data (whereas it is much more complicated and computationally difficult). It would therefore be useful to provide more information about the data (including visualizations), and more detailed insights. I took a look at Appendix C, but this does not provide answers to these questions. Negative insights are also meaningful, i.e., that learning a filtration might not be useful for a lot of problems, and that relying on Rips or DTM filtration is good enough.

(W4) You write: “… the classification accuracy is better when our method was concatenated with DeepSets/PointNet compared to using DeepSets/PointNet alone. The accuracy when we combine our method and PointMLP is not higher than that of PointMLP. This would mean that concatenating the topological feature is effective when the number of parameters of a DNN-based method is relatively small.” I find this argument extremely flawed, since the number of parameters for PointNet and PointMLP is very comparable?


[1] Zhang, Simon, Soham Mukherjee, and Tamal K. Dey. ""GEFL: Extended Filtration Learning for Graph Classification."" Learning on Graphs Conference. PMLR, 2022.  
[2] Horn, Max, et al. ""Topological graph neural networks."" arXiv preprint arXiv:2102.07835 (2021).
","(Q1) PH wrt some filtrations (e.g., height, useful to distinguish MNIST digit 6 from digit 9) is not isometry-invariant, so why do you impose this condition? 

(Q2) Figure 1: Comment more why this approach is meaningful, since 2 holes are not recognized with 1-dim PD for any of the two point clouds? Consider plotting (next to) more reasonable weights.

(Q3) You never mention simplicial complexes, so that it remains unclear why the filtration discussed on lines 143-144 is Rips, and not Čech?

(Q4) How does your weighted Rips filtration compare to the weighted Rips filtration discussed in [3] (Proposition 3.5), where point cloud point x appears according to its filtration function value (weight) f(x), and an edge (x, y) appears when f(x), f(y) and distance d(x, y) satisfy certain properties?

(Q4) Related to (Q3) and (Q4), do all point cloud points immediately appear in your filtration? In particular, if a point has a really large weight, the balls centered at this point will expand very late in the filtration, but will the point (ball with radius 0) be there from the beginning? This is important e.g. if the point is an outlier, since we commonly want to ignore such a point.

(Q5) “Although (a) and (b) can be learned together since the output of the resulting feature is differentiable with all parameters, it would make the optimization unstable.” Why, can you explain more?

(Q6) In Section 5, can you provide more intuition on what is captured with DistMatrixNet? 


Other minor comments:

-	The homology is persistent (not persistence homology), but we talk about persistence landscapes and images (not persistent landscape or image), rephrase throughout the paper.
-	Line 86: Explicitly mention DeepSets.
-	Line 142: “one can take function S defined by” -> “one can take S to be the distance to point cloud, defined by”
-	Line 156:  “which can also [add: be] computed only by distance matrices”. Do you not need the weights too?
-	Line 168: What is function u, where is it used?
-	Line 169: “we can vectorize persistent a persistent diagram”. Rephrase.
-	Line 234: Do topological features and DNN features have to have the same dimension L?
-	Line 251: “we can approximate any continuous function on X x [0,1]^m can be approximated”. Rephrase.
-	Line 253: Cite specific Appendix.
-	Line 283: “we replaced our topological featured replaced”. Rephrase.
-	Line 285: “Note that we use DistMatrixNet not for the computation of filtration weights.” This sentence seems weird, what do you mean?
-	Table 2 caption: “concatenated the feature” -> “concatenated with the feature”?
-	Mention explicitly that the code is made publicly available.
-	References: Check capitalization of acronyms in paper titles (e.g., Dtm, Perslay, Ripsnet, Toposeg, Homcloud, Pointnet, 3d, Pointet++, Pi-net, 3d shapenets, Sgmnet, …), and be consistent between capital case vs. lower case for journal names.

[3] Anai, Hirokazu, et al. ""DTM-based filtrations."" Topological Data Analysis: The Abel Symposium 2018. Springer International Publishing, 2020.
","Some limitations and corresponding future research directions are mentioned, but they do not address the lack of insights on when a learned filtration can be expected to be beneficial (compared to e.g. Rips and DTM filtrations).",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,2 fair,1688666593969,,,NeurIPS.cc/2023/Conference,1TJaITmK2Q,"['NeurIPS.cc/2023/Conference/Submission1558/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410793530,[],3,,,['everyone'],1TJaITmK2Q,['NeurIPS.cc/2023/Conference/Submission1558/Reviewer_L8GX'],1688666593969,1702410793530,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1558/Reviewer_L8GX']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature, including:

1. Zhang, Simon, Soham Mukherjee, and Tamal K. Dey. ""GEFL: Extended Filtration Learning for Graph Classification."" Learning on Graphs Conference. PMLR, 2022.
2. Horn, Max, et al. ""Topological graph neural networks."" arXiv preprint arXiv:2102.07835 (2021).
3. Anai, Hirokazu, et al. ""DTM-based filtrations."" Topological Data Analysis: The Abel Symposium 2018. Springer International Publishing, 2020.

These references are mentioned in the weaknesses section (W2) and questions section (Q4) of the review, indicating that the reviewer believes they are relevant to the topic and should be considered by the authors.",1,"2022, 2021, 2021, 2018, 2020",DTM-based filtrations
sBPGMoloFR,"This paper proposes a novel offline RL algorithm BCOL that builds on the idea of limiting the numbers of counterfactual decisions. Instead of enforcing policy or value regularization, BCOL follows the decisions of the behavioral policy in the majority of the states, and only makes counterfactual decisions for a limited number of times. The strategy to spend the fixed amount of opportunities for counterfactual decisions is learned through a dynamic programming algorithm. Experimental results using two different implementations of the proposed algorithm on a wide range of offline RL tasks are presented. ","It is great that the authors present all details about their experiments and the implementation details of their proposed algorithm. 

The authors present two implementations based on different state-of-the-art RL algorithms (TD3 and SAC). 

The presentation about the empirical results (figures, tables, etc.) is very clear. The ablation study on the hyper-parameters is great.    
","### Method 
It is well-known in the offline RL community that the extrapolation needs to be careful. Limiting the level of extrapolation itself is not challenging, one can copy the behavioral policy, leading to zero extrapolation. The challenging thing is to find out where to extrapolate. The authors have argued for many times in the paper that assigning an upper bound for the number of counterfactual decisions can effectively constrain the level of extrapolation, leading to a balance between the gain of counterfactual decisions and the risk of extrapolation. However, this only explains how the level of extrapolation is limited but not the more important question: why the proposed BCOL algorithm can learn where to extrapolate. 

On the intuitive level, the explanation is not enough and not clear. On the formal side, it would be great if the authors can provide some theoretical guarantees for the proposed algorithm so that the benefits of BCOL become more clear. In the current version of the paper, I fail to see enough support, either intuitive or theoretical, for the efficacy of the proposed algorithm. 

### Algorithm 
The proposed algorithm induces an extra burden for learning because the Q(s,b,a), unlike the regular state-action value function, needs to approximate the value of the budget well. This at least linearly increases the difficulty of the learning problem because for every b, Q(s,b,a) needs to be approximated well so that the proposed Counterfactual-Budgeting Bellman Operator can work well. 

### Experiments
It is appreciated that the details about the experiments are presented well. 

However, the performance of BCOL doesn’t seem that impressive given the increased training cost. For example,although the BCOL(SAC) has the highest total score, it is outperformed by CQL and CDC on about half of the tasks. 

Moreover, the baseline methods, as diverse as they are, are dated algorithms. It may be better to include more latest baselines that also report SOTA performance in their papers (see for example, [1] and [2]) .  

[1] Bhardwaj, Mohak, et al. ""Adversarial model for offline reinforcement learning."" arXiv preprint arXiv:2302.11048 (2023).
[2] Kang, Bingyi, et al. ""Improving and Benchmarking Offline Reinforcement Learning Algorithms."" arXiv preprint arXiv:2306.00972 (2023). 
","Is the algorithm still working if there is more than one behavior policy for collecting the offline data? 

The sentence “Thus, the choice of policy on each step needs to balance between the Q value gain from the current step and the potential benefit from future counterfactual decisions.” is very confusing. Could the authors please elaborate more on it? 

What is the formal definition of Q(s,b,a) in Eq. (4)? It looks like the exact definition of Q(s,b,a) doesn’t matter, and the only thing that matters is the fixed point of $\mathcal{T}_{CB}$. 

I understand that by definition of $\mathcal{T}_{CB}$ that are at most B backup steps taking the max operation as the maximum value. But why does this “intuitively upper bounds the amount of extrapolation that the Q function can take”?",Yes. The authors have discussed the limitations of this work and pointed out future directions for continuing research. ,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688350877704,,,NeurIPS.cc/2023/Conference,1MUxtSBUox,"['NeurIPS.cc/2023/Conference/Submission4132/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410936732,[],1,,,['everyone'],1MUxtSBUox,['NeurIPS.cc/2023/Conference/Submission4132/Reviewer_bnLz'],1688350877704,1702410936732,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4132/Reviewer_bnLz']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Bhardwaj, Mohak, et al. ""Adversarial model for offline reinforcement learning."" arXiv preprint arXiv:2302.11048 (2023).
2. Kang, Bingyi, et al. ""Improving and Benchmarking Offline Reinforcement Learning Algorithms."" arXiv preprint arXiv:2306.00972 (2023).

The reviewer mentions that including these latest baselines, which report state-of-the-art (SOTA) performance in their papers, would be beneficial for the comparison.",1,"2023, 2023, 2023, 2023",Adversarial model for offline reinforcement learning 
sBPGMoloFR,"This paper proposes a novel offline RL algorithm BCOL that builds on the idea of limiting the numbers of counterfactual decisions. Instead of enforcing policy or value regularization, BCOL follows the decisions of the behavioral policy in the majority of the states, and only makes counterfactual decisions for a limited number of times. The strategy to spend the fixed amount of opportunities for counterfactual decisions is learned through a dynamic programming algorithm. Experimental results using two different implementations of the proposed algorithm on a wide range of offline RL tasks are presented. ","It is great that the authors present all details about their experiments and the implementation details of their proposed algorithm. 

The authors present two implementations based on different state-of-the-art RL algorithms (TD3 and SAC). 

The presentation about the empirical results (figures, tables, etc.) is very clear. The ablation study on the hyper-parameters is great.    
","### Method 
It is well-known in the offline RL community that the extrapolation needs to be careful. Limiting the level of extrapolation itself is not challenging, one can copy the behavioral policy, leading to zero extrapolation. The challenging thing is to find out where to extrapolate. The authors have argued for many times in the paper that assigning an upper bound for the number of counterfactual decisions can effectively constrain the level of extrapolation, leading to a balance between the gain of counterfactual decisions and the risk of extrapolation. However, this only explains how the level of extrapolation is limited but not the more important question: why the proposed BCOL algorithm can learn where to extrapolate. 

On the intuitive level, the explanation is not enough and not clear. On the formal side, it would be great if the authors can provide some theoretical guarantees for the proposed algorithm so that the benefits of BCOL become more clear. In the current version of the paper, I fail to see enough support, either intuitive or theoretical, for the efficacy of the proposed algorithm. 

### Algorithm 
The proposed algorithm induces an extra burden for learning because the Q(s,b,a), unlike the regular state-action value function, needs to approximate the value of the budget well. This at least linearly increases the difficulty of the learning problem because for every b, Q(s,b,a) needs to be approximated well so that the proposed Counterfactual-Budgeting Bellman Operator can work well. 

### Experiments
It is appreciated that the details about the experiments are presented well. 

However, the performance of BCOL doesn’t seem that impressive given the increased training cost. For example,although the BCOL(SAC) has the highest total score, it is outperformed by CQL and CDC on about half of the tasks. 

Moreover, the baseline methods, as diverse as they are, are dated algorithms. It may be better to include more latest baselines that also report SOTA performance in their papers (see for example, [1] and [2]) .  

[1] Bhardwaj, Mohak, et al. ""Adversarial model for offline reinforcement learning."" arXiv preprint arXiv:2302.11048 (2023).
[2] Kang, Bingyi, et al. ""Improving and Benchmarking Offline Reinforcement Learning Algorithms."" arXiv preprint arXiv:2306.00972 (2023). 
","Is the algorithm still working if there is more than one behavior policy for collecting the offline data? 

The sentence “Thus, the choice of policy on each step needs to balance between the Q value gain from the current step and the potential benefit from future counterfactual decisions.” is very confusing. Could the authors please elaborate more on it? 

What is the formal definition of Q(s,b,a) in Eq. (4)? It looks like the exact definition of Q(s,b,a) doesn’t matter, and the only thing that matters is the fixed point of $\mathcal{T}_{CB}$. 

I understand that by definition of $\mathcal{T}_{CB}$ that are at most B backup steps taking the max operation as the maximum value. But why does this “intuitively upper bounds the amount of extrapolation that the Q function can take”?",Yes. The authors have discussed the limitations of this work and pointed out future directions for continuing research. ,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688350877704,,,NeurIPS.cc/2023/Conference,1MUxtSBUox,"['NeurIPS.cc/2023/Conference/Submission4132/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410936732,[],1,,,['everyone'],1MUxtSBUox,['NeurIPS.cc/2023/Conference/Submission4132/Reviewer_bnLz'],1688350877704,1702410936732,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4132/Reviewer_bnLz']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Bhardwaj, Mohak, et al. ""Adversarial model for offline reinforcement learning."" arXiv preprint arXiv:2302.11048 (2023).
2. Kang, Bingyi, et al. ""Improving and Benchmarking Offline Reinforcement Learning Algorithms."" arXiv preprint arXiv:2306.00972 (2023).

The reviewer mentions that including these latest baselines, which report state-of-the-art (SOTA) performance in their papers, would be beneficial for the comparison.",1,"2023, 2023, 2023, 2023",Improving and Benchmarking Offline Reinforcement Learning Algorithms
5PkalXhQuJ,"The authors develop a framework for linking discrete (message passing) GNNs to continuous graph diffusion functional networks using Euler-Lagrange equations of heat kernels.  Via this framework, they analyze the causes of oversmoothing in current GNNs. By noting that the main cause of oversmoothing is the minimization of the quadratic graph smoothness term in the diffusion equation, their main contribution is to replace this by a Total Variation (TV) term (as used in image reconstruction, restoration etc.), yielding a new objective function. Further since the $l_1$ norm term of TV is non-differentiable at 0, they develop a dual min-max approach for solving this objective function iteratively by first minimizing for X and then maximizing for Z. This solution technique then results in a new GNN architecture with a separate feature learning layer (Fully Connected layer) for $l^{th}$ layer embeddings $X^l$ followed by a diffusion clip layer for generating the Z terms. They then apply their method for a specific application of predicting spreading flow dynamics. Experimental results are presented on 3 benchmark citation datasets and 6 GNN models including Vanilla GCN and  one other diffusion GCN (GRAND).

TL;DR takeaway of the problem setup/motivation is how to preserve community/contextual embeddings of adjacent (dissimilar) boundary nodes in different labeled communities by preventing oversmoothing through localized message passing. The proposed solution is via the Total Variation parameter Z that should penalize inter community information diffusion.
","1.	Avoiding local oversmoothing by replacing the quadratic graph smoothness term in the diffusion equation by an $l_1$ norm regularize which is the TV term. This seems to be a novel application of TV to diffusion GNNs.

2.	Formal derivation of an iterative dual min-max method for solving the non-differentiable objective function with the TV term. While this type of iterative technique has gained increasing popularity starting with ADMM, the detailed derivation is a good contribution. 

3.	For small layers (2 layers), their method shows performance improvement over the other baselines. This might be due to ameliorating the oversmoothing process due to the TV Z parameter.
","1.	I am surprised the authors do not have references to classic papers that analyze the root causes of oversmoothing in GNNs, for example, the DropEdge paper [1] “Tackling Over-Smoothing for General Graph Convolutional Networks”, W Huang∗ , Yu Rong∗  et al.. IEEE TPAMI Aug. 2015. This paper analyzes oversmoothing by looking at a spectral analysis of the underlying adjacency matrix. It would be interesting to see if there is a connection between the $beta$ parameter and degree connectivity of equation (3) and results from [1].

2.	The baseline model comparisons are too limited - vanilla GCN, GAT, one diffusion GNN (GRAND).
	a. There have been several papers that either explicitly focus on deep sampling while mitigating oversmoothing e.g., [2] “Decoupling the Depth and Scope of Graph Neural Networks”, H. Zeng, M. Zhang et al. Neurips 21. 
	b. They test their model on citation networks which are considered highly homophilic. However, dissimilar nodes (as they illustrate in fig. 1) that would ideally prove their oversmoothing claims are adjacent to each other primarily in heterophilic networks. It would be useful if they can show their results on heterophilic graphs.
There needs to be an extended comparison with GNN  models  such as [2] as well as others that look at heterophilic graphs e.g., $H_2GCN$ [3] “Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,” J. Zhu, Y. Yan et al, Arxiv. As they claim their method prevents over-smoothing, it would be very valuable to see comparative results  on heterophilic graphs and if they can show improvements over other models.

3.	Experimental Results:  [3] $H_2GCN$, [4] GeomGCN [5] GPRGNN   seem to show better results. For instance, accuracy of this paper on Pubmed is 80% while H2GCN, GeomGCN, GPRGNN show 90% accuracy. Similar results for Cora and Citeseer . 
[4] Geom-GCN: “Geometric Graph Convolutional Networks. In International Conference on Learning Representations”, Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. ICLR 2020.
[5] GPRGNN: “Adaptive Universal Generalized PageRank Graph Neural Network”, ICLR2021.

4. There may be some typos in experimental results -  published results are not consistent with published tabular data in the previous works.  E.g. GCNII results for Pubmed are 90% in Table 1 in the following nips 22 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf but Table 1 in this paper only shows around 79% for GCNII on Pubmed. Similarly please check Table 5 in the following nips 20 paper: https://arxiv.org/pdf/2006.11468.pdf for other discrepancies. I suggest rerunning experiments and checking for typos or otherwise undertanding why numbers are different. In general, the margin of improvement is around 2-4 points over vanilla GCNs for $H_2GCN$ and GPRGNN while the margin is also in that range for this paper, even though they show lower scores for vanilla GNN and GAT. It would definitely improve this paper if you added these papers as baselines for direct comparison.

5. The diffusion flow application that is used in the paper to validate the diffusion model seems to a very niche application. It would greatly add value to the paper if you validated the model over a realistic flow model such as a traffic flow problem.

6. The authors note that their iterative solution technique is not computationally intensive (appendix). However, the method was tried only on 4 limited datasets that are all citation networks. In general, for any iterative solution process the tradeoff between convergence time and accuracy requires more sophisticated and detailed evaluation. Can you improve accuracy on Pubmed (see above) at the cost of increased computation time?






","Please consider different graph setups to validate your oversmoothing diffusion models such as heterophilic graphs in which dissimilar nodes are close by and similar nodes can be far away.

More extensive evaluation of tradeoffs between convergence time and accuracy would be helpful, especially to improve accuracy on the 4 datasets as compared to the newer baselines suggested above.

Consider a more realistic diffusion flow application to validate your diffusion model such as a traffic flow problem.","The authors have addressed the limitations of this work in the appendix, primarily the lack of diversity in evaluation datasets. In general, as pointed earlier, paper could be improved with more extensive comparisons on diverse datasets and models.

The authors have stated that the proposed topic doesn't have any negative societal impacts. To the contrary they state that ""From the application perspective, the new deep model for uncovering the in-vivo propagation flows has great potential to establish new underpinning of disease progression and disentangle the heterogeneity of diverse neurodegeneration trajectories."" While technical correct, this comment in general applies to any work on sensitive datasets in the medical field.


",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1690421568570,,,NeurIPS.cc/2023/Conference,1M8nDkUU9b,"['NeurIPS.cc/2023/Conference/Submission5487/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411011299,[],5,,,['everyone'],1M8nDkUU9b,['NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D'],1690421568570,1702411011299,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Tackling Over-Smoothing for General Graph Convolutional Networks"" by W. Huang et al. (IEEE TPAMI, 2015)
2. ""Decoupling the Depth and Scope of Graph Neural Networks"" by H. Zeng et al. (Neurips, 2021)
3. ""Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs"" by J. Zhu et al. (Arxiv)
4. ""Geometric Graph Convolutional Networks"" by H. Pei et al. (ICLR, 2020)
5. ""Adaptive Universal Generalized PageRank Graph Neural Network"" by ICLR2021

The reviewer also mentions specific papers and tables to check for discrepancies in experimental results, such as:

1. Table 1 in the NIPS 2022 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf
2. Table 5 in the NIPS 2020 paper: https://arxiv.org/pdf/2006.11468.pdf

The reviewer suggests that citing these papers and addressing the discrepancies in experimental results would improve the paper.",1,"2015, 2021, 2020, 2022, 2022, 2020, 2020",Tackling Over-Smoothing for General Graph Convolutional Networks
5PkalXhQuJ,"The authors develop a framework for linking discrete (message passing) GNNs to continuous graph diffusion functional networks using Euler-Lagrange equations of heat kernels.  Via this framework, they analyze the causes of oversmoothing in current GNNs. By noting that the main cause of oversmoothing is the minimization of the quadratic graph smoothness term in the diffusion equation, their main contribution is to replace this by a Total Variation (TV) term (as used in image reconstruction, restoration etc.), yielding a new objective function. Further since the $l_1$ norm term of TV is non-differentiable at 0, they develop a dual min-max approach for solving this objective function iteratively by first minimizing for X and then maximizing for Z. This solution technique then results in a new GNN architecture with a separate feature learning layer (Fully Connected layer) for $l^{th}$ layer embeddings $X^l$ followed by a diffusion clip layer for generating the Z terms. They then apply their method for a specific application of predicting spreading flow dynamics. Experimental results are presented on 3 benchmark citation datasets and 6 GNN models including Vanilla GCN and  one other diffusion GCN (GRAND).

TL;DR takeaway of the problem setup/motivation is how to preserve community/contextual embeddings of adjacent (dissimilar) boundary nodes in different labeled communities by preventing oversmoothing through localized message passing. The proposed solution is via the Total Variation parameter Z that should penalize inter community information diffusion.
","1.	Avoiding local oversmoothing by replacing the quadratic graph smoothness term in the diffusion equation by an $l_1$ norm regularize which is the TV term. This seems to be a novel application of TV to diffusion GNNs.

2.	Formal derivation of an iterative dual min-max method for solving the non-differentiable objective function with the TV term. While this type of iterative technique has gained increasing popularity starting with ADMM, the detailed derivation is a good contribution. 

3.	For small layers (2 layers), their method shows performance improvement over the other baselines. This might be due to ameliorating the oversmoothing process due to the TV Z parameter.
","1.	I am surprised the authors do not have references to classic papers that analyze the root causes of oversmoothing in GNNs, for example, the DropEdge paper [1] “Tackling Over-Smoothing for General Graph Convolutional Networks”, W Huang∗ , Yu Rong∗  et al.. IEEE TPAMI Aug. 2015. This paper analyzes oversmoothing by looking at a spectral analysis of the underlying adjacency matrix. It would be interesting to see if there is a connection between the $beta$ parameter and degree connectivity of equation (3) and results from [1].

2.	The baseline model comparisons are too limited - vanilla GCN, GAT, one diffusion GNN (GRAND).
	a. There have been several papers that either explicitly focus on deep sampling while mitigating oversmoothing e.g., [2] “Decoupling the Depth and Scope of Graph Neural Networks”, H. Zeng, M. Zhang et al. Neurips 21. 
	b. They test their model on citation networks which are considered highly homophilic. However, dissimilar nodes (as they illustrate in fig. 1) that would ideally prove their oversmoothing claims are adjacent to each other primarily in heterophilic networks. It would be useful if they can show their results on heterophilic graphs.
There needs to be an extended comparison with GNN  models  such as [2] as well as others that look at heterophilic graphs e.g., $H_2GCN$ [3] “Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,” J. Zhu, Y. Yan et al, Arxiv. As they claim their method prevents over-smoothing, it would be very valuable to see comparative results  on heterophilic graphs and if they can show improvements over other models.

3.	Experimental Results:  [3] $H_2GCN$, [4] GeomGCN [5] GPRGNN   seem to show better results. For instance, accuracy of this paper on Pubmed is 80% while H2GCN, GeomGCN, GPRGNN show 90% accuracy. Similar results for Cora and Citeseer . 
[4] Geom-GCN: “Geometric Graph Convolutional Networks. In International Conference on Learning Representations”, Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. ICLR 2020.
[5] GPRGNN: “Adaptive Universal Generalized PageRank Graph Neural Network”, ICLR2021.

4. There may be some typos in experimental results -  published results are not consistent with published tabular data in the previous works.  E.g. GCNII results for Pubmed are 90% in Table 1 in the following nips 22 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf but Table 1 in this paper only shows around 79% for GCNII on Pubmed. Similarly please check Table 5 in the following nips 20 paper: https://arxiv.org/pdf/2006.11468.pdf for other discrepancies. I suggest rerunning experiments and checking for typos or otherwise undertanding why numbers are different. In general, the margin of improvement is around 2-4 points over vanilla GCNs for $H_2GCN$ and GPRGNN while the margin is also in that range for this paper, even though they show lower scores for vanilla GNN and GAT. It would definitely improve this paper if you added these papers as baselines for direct comparison.

5. The diffusion flow application that is used in the paper to validate the diffusion model seems to a very niche application. It would greatly add value to the paper if you validated the model over a realistic flow model such as a traffic flow problem.

6. The authors note that their iterative solution technique is not computationally intensive (appendix). However, the method was tried only on 4 limited datasets that are all citation networks. In general, for any iterative solution process the tradeoff between convergence time and accuracy requires more sophisticated and detailed evaluation. Can you improve accuracy on Pubmed (see above) at the cost of increased computation time?






","Please consider different graph setups to validate your oversmoothing diffusion models such as heterophilic graphs in which dissimilar nodes are close by and similar nodes can be far away.

More extensive evaluation of tradeoffs between convergence time and accuracy would be helpful, especially to improve accuracy on the 4 datasets as compared to the newer baselines suggested above.

Consider a more realistic diffusion flow application to validate your diffusion model such as a traffic flow problem.","The authors have addressed the limitations of this work in the appendix, primarily the lack of diversity in evaluation datasets. In general, as pointed earlier, paper could be improved with more extensive comparisons on diverse datasets and models.

The authors have stated that the proposed topic doesn't have any negative societal impacts. To the contrary they state that ""From the application perspective, the new deep model for uncovering the in-vivo propagation flows has great potential to establish new underpinning of disease progression and disentangle the heterogeneity of diverse neurodegeneration trajectories."" While technical correct, this comment in general applies to any work on sensitive datasets in the medical field.


",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1690421568570,,,NeurIPS.cc/2023/Conference,1M8nDkUU9b,"['NeurIPS.cc/2023/Conference/Submission5487/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411011299,[],5,,,['everyone'],1M8nDkUU9b,['NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D'],1690421568570,1702411011299,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Tackling Over-Smoothing for General Graph Convolutional Networks"" by W. Huang et al. (IEEE TPAMI, 2015)
2. ""Decoupling the Depth and Scope of Graph Neural Networks"" by H. Zeng et al. (Neurips, 2021)
3. ""Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs"" by J. Zhu et al. (Arxiv)
4. ""Geometric Graph Convolutional Networks"" by H. Pei et al. (ICLR, 2020)
5. ""Adaptive Universal Generalized PageRank Graph Neural Network"" by ICLR2021

The reviewer also mentions specific papers and tables to check for discrepancies in experimental results, such as:

1. Table 1 in the NIPS 2022 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf
2. Table 5 in the NIPS 2020 paper: https://arxiv.org/pdf/2006.11468.pdf

The reviewer suggests that citing these papers and addressing the discrepancies in experimental results would improve the paper.",1,"2015, 2021, 2020, 2022, 2022, 2020, 2020",Decoupling the Depth and Scope of Graph Neural Networks
5PkalXhQuJ,"The authors develop a framework for linking discrete (message passing) GNNs to continuous graph diffusion functional networks using Euler-Lagrange equations of heat kernels.  Via this framework, they analyze the causes of oversmoothing in current GNNs. By noting that the main cause of oversmoothing is the minimization of the quadratic graph smoothness term in the diffusion equation, their main contribution is to replace this by a Total Variation (TV) term (as used in image reconstruction, restoration etc.), yielding a new objective function. Further since the $l_1$ norm term of TV is non-differentiable at 0, they develop a dual min-max approach for solving this objective function iteratively by first minimizing for X and then maximizing for Z. This solution technique then results in a new GNN architecture with a separate feature learning layer (Fully Connected layer) for $l^{th}$ layer embeddings $X^l$ followed by a diffusion clip layer for generating the Z terms. They then apply their method for a specific application of predicting spreading flow dynamics. Experimental results are presented on 3 benchmark citation datasets and 6 GNN models including Vanilla GCN and  one other diffusion GCN (GRAND).

TL;DR takeaway of the problem setup/motivation is how to preserve community/contextual embeddings of adjacent (dissimilar) boundary nodes in different labeled communities by preventing oversmoothing through localized message passing. The proposed solution is via the Total Variation parameter Z that should penalize inter community information diffusion.
","1.	Avoiding local oversmoothing by replacing the quadratic graph smoothness term in the diffusion equation by an $l_1$ norm regularize which is the TV term. This seems to be a novel application of TV to diffusion GNNs.

2.	Formal derivation of an iterative dual min-max method for solving the non-differentiable objective function with the TV term. While this type of iterative technique has gained increasing popularity starting with ADMM, the detailed derivation is a good contribution. 

3.	For small layers (2 layers), their method shows performance improvement over the other baselines. This might be due to ameliorating the oversmoothing process due to the TV Z parameter.
","1.	I am surprised the authors do not have references to classic papers that analyze the root causes of oversmoothing in GNNs, for example, the DropEdge paper [1] “Tackling Over-Smoothing for General Graph Convolutional Networks”, W Huang∗ , Yu Rong∗  et al.. IEEE TPAMI Aug. 2015. This paper analyzes oversmoothing by looking at a spectral analysis of the underlying adjacency matrix. It would be interesting to see if there is a connection between the $beta$ parameter and degree connectivity of equation (3) and results from [1].

2.	The baseline model comparisons are too limited - vanilla GCN, GAT, one diffusion GNN (GRAND).
	a. There have been several papers that either explicitly focus on deep sampling while mitigating oversmoothing e.g., [2] “Decoupling the Depth and Scope of Graph Neural Networks”, H. Zeng, M. Zhang et al. Neurips 21. 
	b. They test their model on citation networks which are considered highly homophilic. However, dissimilar nodes (as they illustrate in fig. 1) that would ideally prove their oversmoothing claims are adjacent to each other primarily in heterophilic networks. It would be useful if they can show their results on heterophilic graphs.
There needs to be an extended comparison with GNN  models  such as [2] as well as others that look at heterophilic graphs e.g., $H_2GCN$ [3] “Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,” J. Zhu, Y. Yan et al, Arxiv. As they claim their method prevents over-smoothing, it would be very valuable to see comparative results  on heterophilic graphs and if they can show improvements over other models.

3.	Experimental Results:  [3] $H_2GCN$, [4] GeomGCN [5] GPRGNN   seem to show better results. For instance, accuracy of this paper on Pubmed is 80% while H2GCN, GeomGCN, GPRGNN show 90% accuracy. Similar results for Cora and Citeseer . 
[4] Geom-GCN: “Geometric Graph Convolutional Networks. In International Conference on Learning Representations”, Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. ICLR 2020.
[5] GPRGNN: “Adaptive Universal Generalized PageRank Graph Neural Network”, ICLR2021.

4. There may be some typos in experimental results -  published results are not consistent with published tabular data in the previous works.  E.g. GCNII results for Pubmed are 90% in Table 1 in the following nips 22 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf but Table 1 in this paper only shows around 79% for GCNII on Pubmed. Similarly please check Table 5 in the following nips 20 paper: https://arxiv.org/pdf/2006.11468.pdf for other discrepancies. I suggest rerunning experiments and checking for typos or otherwise undertanding why numbers are different. In general, the margin of improvement is around 2-4 points over vanilla GCNs for $H_2GCN$ and GPRGNN while the margin is also in that range for this paper, even though they show lower scores for vanilla GNN and GAT. It would definitely improve this paper if you added these papers as baselines for direct comparison.

5. The diffusion flow application that is used in the paper to validate the diffusion model seems to a very niche application. It would greatly add value to the paper if you validated the model over a realistic flow model such as a traffic flow problem.

6. The authors note that their iterative solution technique is not computationally intensive (appendix). However, the method was tried only on 4 limited datasets that are all citation networks. In general, for any iterative solution process the tradeoff between convergence time and accuracy requires more sophisticated and detailed evaluation. Can you improve accuracy on Pubmed (see above) at the cost of increased computation time?






","Please consider different graph setups to validate your oversmoothing diffusion models such as heterophilic graphs in which dissimilar nodes are close by and similar nodes can be far away.

More extensive evaluation of tradeoffs between convergence time and accuracy would be helpful, especially to improve accuracy on the 4 datasets as compared to the newer baselines suggested above.

Consider a more realistic diffusion flow application to validate your diffusion model such as a traffic flow problem.","The authors have addressed the limitations of this work in the appendix, primarily the lack of diversity in evaluation datasets. In general, as pointed earlier, paper could be improved with more extensive comparisons on diverse datasets and models.

The authors have stated that the proposed topic doesn't have any negative societal impacts. To the contrary they state that ""From the application perspective, the new deep model for uncovering the in-vivo propagation flows has great potential to establish new underpinning of disease progression and disentangle the heterogeneity of diverse neurodegeneration trajectories."" While technical correct, this comment in general applies to any work on sensitive datasets in the medical field.


",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1690421568570,,,NeurIPS.cc/2023/Conference,1M8nDkUU9b,"['NeurIPS.cc/2023/Conference/Submission5487/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411011299,[],5,,,['everyone'],1M8nDkUU9b,['NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D'],1690421568570,1702411011299,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Tackling Over-Smoothing for General Graph Convolutional Networks"" by W. Huang et al. (IEEE TPAMI, 2015)
2. ""Decoupling the Depth and Scope of Graph Neural Networks"" by H. Zeng et al. (Neurips, 2021)
3. ""Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs"" by J. Zhu et al. (Arxiv)
4. ""Geometric Graph Convolutional Networks"" by H. Pei et al. (ICLR, 2020)
5. ""Adaptive Universal Generalized PageRank Graph Neural Network"" by ICLR2021

The reviewer also mentions specific papers and tables to check for discrepancies in experimental results, such as:

1. Table 1 in the NIPS 2022 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf
2. Table 5 in the NIPS 2020 paper: https://arxiv.org/pdf/2006.11468.pdf

The reviewer suggests that citing these papers and addressing the discrepancies in experimental results would improve the paper.",1,"2015, 2021, 2020, 2022, 2022, 2020, 2020",Beyond Homophily in Graph Neural Networks Current Limitations and Effective Designs
5PkalXhQuJ,"The authors develop a framework for linking discrete (message passing) GNNs to continuous graph diffusion functional networks using Euler-Lagrange equations of heat kernels.  Via this framework, they analyze the causes of oversmoothing in current GNNs. By noting that the main cause of oversmoothing is the minimization of the quadratic graph smoothness term in the diffusion equation, their main contribution is to replace this by a Total Variation (TV) term (as used in image reconstruction, restoration etc.), yielding a new objective function. Further since the $l_1$ norm term of TV is non-differentiable at 0, they develop a dual min-max approach for solving this objective function iteratively by first minimizing for X and then maximizing for Z. This solution technique then results in a new GNN architecture with a separate feature learning layer (Fully Connected layer) for $l^{th}$ layer embeddings $X^l$ followed by a diffusion clip layer for generating the Z terms. They then apply their method for a specific application of predicting spreading flow dynamics. Experimental results are presented on 3 benchmark citation datasets and 6 GNN models including Vanilla GCN and  one other diffusion GCN (GRAND).

TL;DR takeaway of the problem setup/motivation is how to preserve community/contextual embeddings of adjacent (dissimilar) boundary nodes in different labeled communities by preventing oversmoothing through localized message passing. The proposed solution is via the Total Variation parameter Z that should penalize inter community information diffusion.
","1.	Avoiding local oversmoothing by replacing the quadratic graph smoothness term in the diffusion equation by an $l_1$ norm regularize which is the TV term. This seems to be a novel application of TV to diffusion GNNs.

2.	Formal derivation of an iterative dual min-max method for solving the non-differentiable objective function with the TV term. While this type of iterative technique has gained increasing popularity starting with ADMM, the detailed derivation is a good contribution. 

3.	For small layers (2 layers), their method shows performance improvement over the other baselines. This might be due to ameliorating the oversmoothing process due to the TV Z parameter.
","1.	I am surprised the authors do not have references to classic papers that analyze the root causes of oversmoothing in GNNs, for example, the DropEdge paper [1] “Tackling Over-Smoothing for General Graph Convolutional Networks”, W Huang∗ , Yu Rong∗  et al.. IEEE TPAMI Aug. 2015. This paper analyzes oversmoothing by looking at a spectral analysis of the underlying adjacency matrix. It would be interesting to see if there is a connection between the $beta$ parameter and degree connectivity of equation (3) and results from [1].

2.	The baseline model comparisons are too limited - vanilla GCN, GAT, one diffusion GNN (GRAND).
	a. There have been several papers that either explicitly focus on deep sampling while mitigating oversmoothing e.g., [2] “Decoupling the Depth and Scope of Graph Neural Networks”, H. Zeng, M. Zhang et al. Neurips 21. 
	b. They test their model on citation networks which are considered highly homophilic. However, dissimilar nodes (as they illustrate in fig. 1) that would ideally prove their oversmoothing claims are adjacent to each other primarily in heterophilic networks. It would be useful if they can show their results on heterophilic graphs.
There needs to be an extended comparison with GNN  models  such as [2] as well as others that look at heterophilic graphs e.g., $H_2GCN$ [3] “Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,” J. Zhu, Y. Yan et al, Arxiv. As they claim their method prevents over-smoothing, it would be very valuable to see comparative results  on heterophilic graphs and if they can show improvements over other models.

3.	Experimental Results:  [3] $H_2GCN$, [4] GeomGCN [5] GPRGNN   seem to show better results. For instance, accuracy of this paper on Pubmed is 80% while H2GCN, GeomGCN, GPRGNN show 90% accuracy. Similar results for Cora and Citeseer . 
[4] Geom-GCN: “Geometric Graph Convolutional Networks. In International Conference on Learning Representations”, Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. ICLR 2020.
[5] GPRGNN: “Adaptive Universal Generalized PageRank Graph Neural Network”, ICLR2021.

4. There may be some typos in experimental results -  published results are not consistent with published tabular data in the previous works.  E.g. GCNII results for Pubmed are 90% in Table 1 in the following nips 22 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf but Table 1 in this paper only shows around 79% for GCNII on Pubmed. Similarly please check Table 5 in the following nips 20 paper: https://arxiv.org/pdf/2006.11468.pdf for other discrepancies. I suggest rerunning experiments and checking for typos or otherwise undertanding why numbers are different. In general, the margin of improvement is around 2-4 points over vanilla GCNs for $H_2GCN$ and GPRGNN while the margin is also in that range for this paper, even though they show lower scores for vanilla GNN and GAT. It would definitely improve this paper if you added these papers as baselines for direct comparison.

5. The diffusion flow application that is used in the paper to validate the diffusion model seems to a very niche application. It would greatly add value to the paper if you validated the model over a realistic flow model such as a traffic flow problem.

6. The authors note that their iterative solution technique is not computationally intensive (appendix). However, the method was tried only on 4 limited datasets that are all citation networks. In general, for any iterative solution process the tradeoff between convergence time and accuracy requires more sophisticated and detailed evaluation. Can you improve accuracy on Pubmed (see above) at the cost of increased computation time?






","Please consider different graph setups to validate your oversmoothing diffusion models such as heterophilic graphs in which dissimilar nodes are close by and similar nodes can be far away.

More extensive evaluation of tradeoffs between convergence time and accuracy would be helpful, especially to improve accuracy on the 4 datasets as compared to the newer baselines suggested above.

Consider a more realistic diffusion flow application to validate your diffusion model such as a traffic flow problem.","The authors have addressed the limitations of this work in the appendix, primarily the lack of diversity in evaluation datasets. In general, as pointed earlier, paper could be improved with more extensive comparisons on diverse datasets and models.

The authors have stated that the proposed topic doesn't have any negative societal impacts. To the contrary they state that ""From the application perspective, the new deep model for uncovering the in-vivo propagation flows has great potential to establish new underpinning of disease progression and disentangle the heterogeneity of diverse neurodegeneration trajectories."" While technical correct, this comment in general applies to any work on sensitive datasets in the medical field.


",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1690421568570,,,NeurIPS.cc/2023/Conference,1M8nDkUU9b,"['NeurIPS.cc/2023/Conference/Submission5487/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411011299,[],5,,,['everyone'],1M8nDkUU9b,['NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D'],1690421568570,1702411011299,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Tackling Over-Smoothing for General Graph Convolutional Networks"" by W. Huang et al. (IEEE TPAMI, 2015)
2. ""Decoupling the Depth and Scope of Graph Neural Networks"" by H. Zeng et al. (Neurips, 2021)
3. ""Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs"" by J. Zhu et al. (Arxiv)
4. ""Geometric Graph Convolutional Networks"" by H. Pei et al. (ICLR, 2020)
5. ""Adaptive Universal Generalized PageRank Graph Neural Network"" by ICLR2021

The reviewer also mentions specific papers and tables to check for discrepancies in experimental results, such as:

1. Table 1 in the NIPS 2022 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf
2. Table 5 in the NIPS 2020 paper: https://arxiv.org/pdf/2006.11468.pdf

The reviewer suggests that citing these papers and addressing the discrepancies in experimental results would improve the paper.",1,"2015, 2021, 2020, 2022, 2022, 2020, 2020",Geometric Graph Convolutional Networks
5PkalXhQuJ,"The authors develop a framework for linking discrete (message passing) GNNs to continuous graph diffusion functional networks using Euler-Lagrange equations of heat kernels.  Via this framework, they analyze the causes of oversmoothing in current GNNs. By noting that the main cause of oversmoothing is the minimization of the quadratic graph smoothness term in the diffusion equation, their main contribution is to replace this by a Total Variation (TV) term (as used in image reconstruction, restoration etc.), yielding a new objective function. Further since the $l_1$ norm term of TV is non-differentiable at 0, they develop a dual min-max approach for solving this objective function iteratively by first minimizing for X and then maximizing for Z. This solution technique then results in a new GNN architecture with a separate feature learning layer (Fully Connected layer) for $l^{th}$ layer embeddings $X^l$ followed by a diffusion clip layer for generating the Z terms. They then apply their method for a specific application of predicting spreading flow dynamics. Experimental results are presented on 3 benchmark citation datasets and 6 GNN models including Vanilla GCN and  one other diffusion GCN (GRAND).

TL;DR takeaway of the problem setup/motivation is how to preserve community/contextual embeddings of adjacent (dissimilar) boundary nodes in different labeled communities by preventing oversmoothing through localized message passing. The proposed solution is via the Total Variation parameter Z that should penalize inter community information diffusion.
","1.	Avoiding local oversmoothing by replacing the quadratic graph smoothness term in the diffusion equation by an $l_1$ norm regularize which is the TV term. This seems to be a novel application of TV to diffusion GNNs.

2.	Formal derivation of an iterative dual min-max method for solving the non-differentiable objective function with the TV term. While this type of iterative technique has gained increasing popularity starting with ADMM, the detailed derivation is a good contribution. 

3.	For small layers (2 layers), their method shows performance improvement over the other baselines. This might be due to ameliorating the oversmoothing process due to the TV Z parameter.
","1.	I am surprised the authors do not have references to classic papers that analyze the root causes of oversmoothing in GNNs, for example, the DropEdge paper [1] “Tackling Over-Smoothing for General Graph Convolutional Networks”, W Huang∗ , Yu Rong∗  et al.. IEEE TPAMI Aug. 2015. This paper analyzes oversmoothing by looking at a spectral analysis of the underlying adjacency matrix. It would be interesting to see if there is a connection between the $beta$ parameter and degree connectivity of equation (3) and results from [1].

2.	The baseline model comparisons are too limited - vanilla GCN, GAT, one diffusion GNN (GRAND).
	a. There have been several papers that either explicitly focus on deep sampling while mitigating oversmoothing e.g., [2] “Decoupling the Depth and Scope of Graph Neural Networks”, H. Zeng, M. Zhang et al. Neurips 21. 
	b. They test their model on citation networks which are considered highly homophilic. However, dissimilar nodes (as they illustrate in fig. 1) that would ideally prove their oversmoothing claims are adjacent to each other primarily in heterophilic networks. It would be useful if they can show their results on heterophilic graphs.
There needs to be an extended comparison with GNN  models  such as [2] as well as others that look at heterophilic graphs e.g., $H_2GCN$ [3] “Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,” J. Zhu, Y. Yan et al, Arxiv. As they claim their method prevents over-smoothing, it would be very valuable to see comparative results  on heterophilic graphs and if they can show improvements over other models.

3.	Experimental Results:  [3] $H_2GCN$, [4] GeomGCN [5] GPRGNN   seem to show better results. For instance, accuracy of this paper on Pubmed is 80% while H2GCN, GeomGCN, GPRGNN show 90% accuracy. Similar results for Cora and Citeseer . 
[4] Geom-GCN: “Geometric Graph Convolutional Networks. In International Conference on Learning Representations”, Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. ICLR 2020.
[5] GPRGNN: “Adaptive Universal Generalized PageRank Graph Neural Network”, ICLR2021.

4. There may be some typos in experimental results -  published results are not consistent with published tabular data in the previous works.  E.g. GCNII results for Pubmed are 90% in Table 1 in the following nips 22 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf but Table 1 in this paper only shows around 79% for GCNII on Pubmed. Similarly please check Table 5 in the following nips 20 paper: https://arxiv.org/pdf/2006.11468.pdf for other discrepancies. I suggest rerunning experiments and checking for typos or otherwise undertanding why numbers are different. In general, the margin of improvement is around 2-4 points over vanilla GCNs for $H_2GCN$ and GPRGNN while the margin is also in that range for this paper, even though they show lower scores for vanilla GNN and GAT. It would definitely improve this paper if you added these papers as baselines for direct comparison.

5. The diffusion flow application that is used in the paper to validate the diffusion model seems to a very niche application. It would greatly add value to the paper if you validated the model over a realistic flow model such as a traffic flow problem.

6. The authors note that their iterative solution technique is not computationally intensive (appendix). However, the method was tried only on 4 limited datasets that are all citation networks. In general, for any iterative solution process the tradeoff between convergence time and accuracy requires more sophisticated and detailed evaluation. Can you improve accuracy on Pubmed (see above) at the cost of increased computation time?






","Please consider different graph setups to validate your oversmoothing diffusion models such as heterophilic graphs in which dissimilar nodes are close by and similar nodes can be far away.

More extensive evaluation of tradeoffs between convergence time and accuracy would be helpful, especially to improve accuracy on the 4 datasets as compared to the newer baselines suggested above.

Consider a more realistic diffusion flow application to validate your diffusion model such as a traffic flow problem.","The authors have addressed the limitations of this work in the appendix, primarily the lack of diversity in evaluation datasets. In general, as pointed earlier, paper could be improved with more extensive comparisons on diverse datasets and models.

The authors have stated that the proposed topic doesn't have any negative societal impacts. To the contrary they state that ""From the application perspective, the new deep model for uncovering the in-vivo propagation flows has great potential to establish new underpinning of disease progression and disentangle the heterogeneity of diverse neurodegeneration trajectories."" While technical correct, this comment in general applies to any work on sensitive datasets in the medical field.


",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1690421568570,,,NeurIPS.cc/2023/Conference,1M8nDkUU9b,"['NeurIPS.cc/2023/Conference/Submission5487/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411011299,[],5,,,['everyone'],1M8nDkUU9b,['NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D'],1690421568570,1702411011299,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5487/Reviewer_VF1D']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Tackling Over-Smoothing for General Graph Convolutional Networks"" by W. Huang et al. (IEEE TPAMI, 2015)
2. ""Decoupling the Depth and Scope of Graph Neural Networks"" by H. Zeng et al. (Neurips, 2021)
3. ""Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs"" by J. Zhu et al. (Arxiv)
4. ""Geometric Graph Convolutional Networks"" by H. Pei et al. (ICLR, 2020)
5. ""Adaptive Universal Generalized PageRank Graph Neural Network"" by ICLR2021

The reviewer also mentions specific papers and tables to check for discrepancies in experimental results, such as:

1. Table 1 in the NIPS 2022 paper: https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf
2. Table 5 in the NIPS 2020 paper: https://arxiv.org/pdf/2006.11468.pdf

The reviewer suggests that citing these papers and addressing the discrepancies in experimental results would improve the paper.",1,"2015, 2021, 2020, 2022, 2022, 2020, 2020",Adaptive Universal Generalized PageRank Graph Neural Network 
H51Uf1lVnk,"This paper proposes a new Blurred-Dilated method for generating transfer attacks. The authors focus on generating transfer attacks as a more realistic attack model by looking at how the substitute model's architecture can be changed to increase the transferability of adversarial attacks. By introducing blurred downsampling and dilated convolutions in the substitute network, the authors try to focus on preserving important features to increase transferability. The authors evaluate transferability on ImageNet across several naturally trained architectures.


======= POST REBUTTAL =========
After the rebuttal, I have raised my score due to experiments on transfer-based defenses.","1. Interesting idea. The paper proposes an interesting approach, which is whether or not transfer attacks can be carried out on the substitute model side. If there are techniques we can do on the substitute model side, this makes the attacks more possible and practical.

2. Compared to ILA, ILA++, and LinBP, the BD attacks seem to do quite well, transferring across naturally trained architectures at a higher rate than these other attacks.

3. Examining the attention maps in the experiments is interesting, and helps add some insight into what would otherwise be an empirical paper.","1. Some design choices could be defended stronger. For example, why are the dilated convolutions only applied at the later layers? Why is blurred-downsampling better than convolving with higher stride? Also, the blurred filters would still lose information. What formally is being preserved with the BD filters and not with the regular convolutions or max pooling or average pooling? More formal one-to-one comparisons between each choice may be helpful. I am still a bit unsure about what fundamentally is important about these operations.

2. Unstated practicality concerns. While the results are good against naturally trained models, what if the defense is trained to stop transfer attacks? E.g., ""Ensemble Adversarial Training: Attacks and Defenses"" (Tramer et al. 2018) or ""TRS: Transferability Reduced Ensemble via Encouraging Gradient Diversity and Model Smoothness"" (Yang et al. 2021). The introduction of such defenses may render these attacks ineffective. I wonder how effective these defenses would be in stopping BD attacks. In addition, the assumption is that ImageNet as the domain is known but the architecture may be different. How realistic is this in the real world? One might not know the dataset that a machine learning model was trained on, and then would have to extract a substitute with model extraction. Then, the practical cost of extracting a model and then performing a transfer attack would have to be compared with the cost of performing query-based black-box attacks.","1. Why are dilated convolutions only applied at the later layers?

2. Why are blurred-downsampling filters better than convolving with higher stride?

3. How does BD perform against defenses trained to stop transfer attacks?",The paper is an attack paper and does not extensively talk about the implications of such a method. A discussion on possible countermeasures and how this impacts the overall aim of creating robust and reliable machine learning models would be helpful.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688486197960,,,NeurIPS.cc/2023/Conference,1JlAV2paGu,"['NeurIPS.cc/2023/Conference/Submission9171/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411214322,[],3,,,['everyone'],1JlAV2paGu,['NeurIPS.cc/2023/Conference/Submission9171/Reviewer_1sbs'],1688486197960,1702411214322,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9171/Reviewer_1sbs']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific papers: 

1. ""Ensemble Adversarial Training: Attacks and Defenses"" (Tramer et al. 2018)
2. ""TRS: Transferability Reduced Ensemble via Encouraging Gradient Diversity and Model Smoothness"" (Yang et al. 2021)

The reviewer is interested in knowing how effective the proposed Blurred-Dilated (BD) attacks would be against defenses trained to stop transfer attacks, such as those described in these papers.",1,"2018, 2021",Ensemble Adversarial Training Attacks and Defenses 
H51Uf1lVnk,"This paper proposes a new Blurred-Dilated method for generating transfer attacks. The authors focus on generating transfer attacks as a more realistic attack model by looking at how the substitute model's architecture can be changed to increase the transferability of adversarial attacks. By introducing blurred downsampling and dilated convolutions in the substitute network, the authors try to focus on preserving important features to increase transferability. The authors evaluate transferability on ImageNet across several naturally trained architectures.


======= POST REBUTTAL =========
After the rebuttal, I have raised my score due to experiments on transfer-based defenses.","1. Interesting idea. The paper proposes an interesting approach, which is whether or not transfer attacks can be carried out on the substitute model side. If there are techniques we can do on the substitute model side, this makes the attacks more possible and practical.

2. Compared to ILA, ILA++, and LinBP, the BD attacks seem to do quite well, transferring across naturally trained architectures at a higher rate than these other attacks.

3. Examining the attention maps in the experiments is interesting, and helps add some insight into what would otherwise be an empirical paper.","1. Some design choices could be defended stronger. For example, why are the dilated convolutions only applied at the later layers? Why is blurred-downsampling better than convolving with higher stride? Also, the blurred filters would still lose information. What formally is being preserved with the BD filters and not with the regular convolutions or max pooling or average pooling? More formal one-to-one comparisons between each choice may be helpful. I am still a bit unsure about what fundamentally is important about these operations.

2. Unstated practicality concerns. While the results are good against naturally trained models, what if the defense is trained to stop transfer attacks? E.g., ""Ensemble Adversarial Training: Attacks and Defenses"" (Tramer et al. 2018) or ""TRS: Transferability Reduced Ensemble via Encouraging Gradient Diversity and Model Smoothness"" (Yang et al. 2021). The introduction of such defenses may render these attacks ineffective. I wonder how effective these defenses would be in stopping BD attacks. In addition, the assumption is that ImageNet as the domain is known but the architecture may be different. How realistic is this in the real world? One might not know the dataset that a machine learning model was trained on, and then would have to extract a substitute with model extraction. Then, the practical cost of extracting a model and then performing a transfer attack would have to be compared with the cost of performing query-based black-box attacks.","1. Why are dilated convolutions only applied at the later layers?

2. Why are blurred-downsampling filters better than convolving with higher stride?

3. How does BD perform against defenses trained to stop transfer attacks?",The paper is an attack paper and does not extensively talk about the implications of such a method. A discussion on possible countermeasures and how this impacts the overall aim of creating robust and reliable machine learning models would be helpful.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688486197960,,,NeurIPS.cc/2023/Conference,1JlAV2paGu,"['NeurIPS.cc/2023/Conference/Submission9171/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411214322,[],3,,,['everyone'],1JlAV2paGu,['NeurIPS.cc/2023/Conference/Submission9171/Reviewer_1sbs'],1688486197960,1702411214322,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9171/Reviewer_1sbs']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific papers: 

1. ""Ensemble Adversarial Training: Attacks and Defenses"" (Tramer et al. 2018)
2. ""TRS: Transferability Reduced Ensemble via Encouraging Gradient Diversity and Model Smoothness"" (Yang et al. 2021)

The reviewer is interested in knowing how effective the proposed Blurred-Dilated (BD) attacks would be against defenses trained to stop transfer attacks, such as those described in these papers.",1,"2018, 2021",TRS Transferability Reduced Ensemble via Encouraging Gradient Diversity and Model Smoothness
0XUZ6lt5Pk,"The authors introduce a method for object part segmentation of point clouds that is equivariant/invariant to SE(3) part transformations. The core of their method is a neural network with points and segmentation as input and segmentation as output. The network is assumed to be contractive and is then used to perform Banach fixed point iterations towards the correct segmentation during inference. The Banach fixed point network is embedded in a larger architecture using Vector Neurons. For implementation, a message passing network is used, which uses the input segmentation for determining message strength. 

The proposed method is evaluated for articulated object part segmentation (on Shape2Motion) and for segmentation of multi-object scans (on DynLab), where it compares favorably against baselines.","- The theoretical framework of Banach iterations is elegant and an interesting perspective on iterative networks during inference. In practice, similar ideas on point clouds have been there before, understanding such networks as reweighting functions for iterative reweighting least square schemes, EM iterations, etc. However, I think there is value in this specific perspective, especially since it comes with a convergence guarantee under Lipschitz constraints. To my knowledge, this is a novel contribution.
- The presented algorithm seems to solve the given tasks well, outperforming the given baselines clearly (however, there are some concerns regarding missing baselines, see below).
- The paper is nicely presented and easy to follow, given the slightly more complex nature.
- The result of generalization from synthetic to real chairs is strong","- The first part of the paper is all about the consequences of a Lipschitz-constraint network and then there is no Lipschitz constraint in practice. The authors claim that the local message passing serves as some type of Lipschitz regularization. However, there is no reference given for this claim and I don't know one either.  I think it would make the paper stronger if this aspect would be supported by experiments or theory, which is isn't.  The authors only ablate on model performance under different noise levels, which does not give a full picture about convergence.
- In articulated object part segmentation,  the paper seems to leave out important comparisons to previous work, e.g., [1,2,3]. All three papers are referenced in related work but not compared against. It would be good if the authors would provide a comparison or a discussion, why the comparison is not necessary.

I think this is a very interesting paper with a new perspective on iterative networks. However, the above points are a bit concerning, which is why I am not fully convinced to give a better score.

[1] Kawana et al.: Unsupervised pose-aware part decomposition for 3d articulated objects.

[2] Kawana et al.: Neural star domain as primitive representation

[3] Chen et al.: BAE-NET: Branched Autoencoder for Shape Co-Segmentation","- I suspect that equation 1 wants to express linear blending of part transforms. However, I don't think it is correct: (1) sum of matrix $\mathbf{R}$ and vector \mathbf{t}, (2) $\mathbf{t}_p$ not weighted by $\mathbf{y}_{np}$ . Could the authors clarify?
- Please provide further evidence that the network indeed behaves contractive or a theoretical justification for the Lipschitz regularization via local message passing.
- Please discuss or provide missing comparisons with previous methods for part segmentation on Shape2Motion.",The authors discuss limitations and societal impact.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1687635275991,,,NeurIPS.cc/2023/Conference,1IOU2329Za,"['NeurIPS.cc/2023/Conference/Submission862/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410760895,[],2,,,['everyone'],1IOU2329Za,['NeurIPS.cc/2023/Conference/Submission862/Reviewer_T7gc'],1687635275991,1702410760895,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission862/Reviewer_T7gc']","Yes, the peer review explicitly suggests the authors of the paper to cite or compare with the following specific literature:

1. Kawana et al.: Unsupervised pose-aware part decomposition for 3d articulated objects.
2. Kawana et al.: Neural star domain as primitive representation
3. Chen et al.: BAE-NET: Branched Autoencoder for Shape Co-Segmentation

These papers are mentioned as important previous work in articulated object part segmentation that the authors should compare their method with or discuss why a comparison is not necessary.",1,,Unsupervised pose-aware part decomposition for 3d articulated objects
0XUZ6lt5Pk,"The authors introduce a method for object part segmentation of point clouds that is equivariant/invariant to SE(3) part transformations. The core of their method is a neural network with points and segmentation as input and segmentation as output. The network is assumed to be contractive and is then used to perform Banach fixed point iterations towards the correct segmentation during inference. The Banach fixed point network is embedded in a larger architecture using Vector Neurons. For implementation, a message passing network is used, which uses the input segmentation for determining message strength. 

The proposed method is evaluated for articulated object part segmentation (on Shape2Motion) and for segmentation of multi-object scans (on DynLab), where it compares favorably against baselines.","- The theoretical framework of Banach iterations is elegant and an interesting perspective on iterative networks during inference. In practice, similar ideas on point clouds have been there before, understanding such networks as reweighting functions for iterative reweighting least square schemes, EM iterations, etc. However, I think there is value in this specific perspective, especially since it comes with a convergence guarantee under Lipschitz constraints. To my knowledge, this is a novel contribution.
- The presented algorithm seems to solve the given tasks well, outperforming the given baselines clearly (however, there are some concerns regarding missing baselines, see below).
- The paper is nicely presented and easy to follow, given the slightly more complex nature.
- The result of generalization from synthetic to real chairs is strong","- The first part of the paper is all about the consequences of a Lipschitz-constraint network and then there is no Lipschitz constraint in practice. The authors claim that the local message passing serves as some type of Lipschitz regularization. However, there is no reference given for this claim and I don't know one either.  I think it would make the paper stronger if this aspect would be supported by experiments or theory, which is isn't.  The authors only ablate on model performance under different noise levels, which does not give a full picture about convergence.
- In articulated object part segmentation,  the paper seems to leave out important comparisons to previous work, e.g., [1,2,3]. All three papers are referenced in related work but not compared against. It would be good if the authors would provide a comparison or a discussion, why the comparison is not necessary.

I think this is a very interesting paper with a new perspective on iterative networks. However, the above points are a bit concerning, which is why I am not fully convinced to give a better score.

[1] Kawana et al.: Unsupervised pose-aware part decomposition for 3d articulated objects.

[2] Kawana et al.: Neural star domain as primitive representation

[3] Chen et al.: BAE-NET: Branched Autoencoder for Shape Co-Segmentation","- I suspect that equation 1 wants to express linear blending of part transforms. However, I don't think it is correct: (1) sum of matrix $\mathbf{R}$ and vector \mathbf{t}, (2) $\mathbf{t}_p$ not weighted by $\mathbf{y}_{np}$ . Could the authors clarify?
- Please provide further evidence that the network indeed behaves contractive or a theoretical justification for the Lipschitz regularization via local message passing.
- Please discuss or provide missing comparisons with previous methods for part segmentation on Shape2Motion.",The authors discuss limitations and societal impact.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1687635275991,,,NeurIPS.cc/2023/Conference,1IOU2329Za,"['NeurIPS.cc/2023/Conference/Submission862/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410760895,[],2,,,['everyone'],1IOU2329Za,['NeurIPS.cc/2023/Conference/Submission862/Reviewer_T7gc'],1687635275991,1702410760895,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission862/Reviewer_T7gc']","Yes, the peer review explicitly suggests the authors of the paper to cite or compare with the following specific literature:

1. Kawana et al.: Unsupervised pose-aware part decomposition for 3d articulated objects.
2. Kawana et al.: Neural star domain as primitive representation
3. Chen et al.: BAE-NET: Branched Autoencoder for Shape Co-Segmentation

These papers are mentioned as important previous work in articulated object part segmentation that the authors should compare their method with or discuss why a comparison is not necessary.",1,,Neural star domain as primitive representation
0XUZ6lt5Pk,"The authors introduce a method for object part segmentation of point clouds that is equivariant/invariant to SE(3) part transformations. The core of their method is a neural network with points and segmentation as input and segmentation as output. The network is assumed to be contractive and is then used to perform Banach fixed point iterations towards the correct segmentation during inference. The Banach fixed point network is embedded in a larger architecture using Vector Neurons. For implementation, a message passing network is used, which uses the input segmentation for determining message strength. 

The proposed method is evaluated for articulated object part segmentation (on Shape2Motion) and for segmentation of multi-object scans (on DynLab), where it compares favorably against baselines.","- The theoretical framework of Banach iterations is elegant and an interesting perspective on iterative networks during inference. In practice, similar ideas on point clouds have been there before, understanding such networks as reweighting functions for iterative reweighting least square schemes, EM iterations, etc. However, I think there is value in this specific perspective, especially since it comes with a convergence guarantee under Lipschitz constraints. To my knowledge, this is a novel contribution.
- The presented algorithm seems to solve the given tasks well, outperforming the given baselines clearly (however, there are some concerns regarding missing baselines, see below).
- The paper is nicely presented and easy to follow, given the slightly more complex nature.
- The result of generalization from synthetic to real chairs is strong","- The first part of the paper is all about the consequences of a Lipschitz-constraint network and then there is no Lipschitz constraint in practice. The authors claim that the local message passing serves as some type of Lipschitz regularization. However, there is no reference given for this claim and I don't know one either.  I think it would make the paper stronger if this aspect would be supported by experiments or theory, which is isn't.  The authors only ablate on model performance under different noise levels, which does not give a full picture about convergence.
- In articulated object part segmentation,  the paper seems to leave out important comparisons to previous work, e.g., [1,2,3]. All three papers are referenced in related work but not compared against. It would be good if the authors would provide a comparison or a discussion, why the comparison is not necessary.

I think this is a very interesting paper with a new perspective on iterative networks. However, the above points are a bit concerning, which is why I am not fully convinced to give a better score.

[1] Kawana et al.: Unsupervised pose-aware part decomposition for 3d articulated objects.

[2] Kawana et al.: Neural star domain as primitive representation

[3] Chen et al.: BAE-NET: Branched Autoencoder for Shape Co-Segmentation","- I suspect that equation 1 wants to express linear blending of part transforms. However, I don't think it is correct: (1) sum of matrix $\mathbf{R}$ and vector \mathbf{t}, (2) $\mathbf{t}_p$ not weighted by $\mathbf{y}_{np}$ . Could the authors clarify?
- Please provide further evidence that the network indeed behaves contractive or a theoretical justification for the Lipschitz regularization via local message passing.
- Please discuss or provide missing comparisons with previous methods for part segmentation on Shape2Motion.",The authors discuss limitations and societal impact.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1687635275991,,,NeurIPS.cc/2023/Conference,1IOU2329Za,"['NeurIPS.cc/2023/Conference/Submission862/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410760895,[],2,,,['everyone'],1IOU2329Za,['NeurIPS.cc/2023/Conference/Submission862/Reviewer_T7gc'],1687635275991,1702410760895,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission862/Reviewer_T7gc']","Yes, the peer review explicitly suggests the authors of the paper to cite or compare with the following specific literature:

1. Kawana et al.: Unsupervised pose-aware part decomposition for 3d articulated objects.
2. Kawana et al.: Neural star domain as primitive representation
3. Chen et al.: BAE-NET: Branched Autoencoder for Shape Co-Segmentation

These papers are mentioned as important previous work in articulated object part segmentation that the authors should compare their method with or discuss why a comparison is not necessary.",1,,BAE-NET: Branched Autoencoder for Shape Co-Segmentation
MszzvIfnDc,"The authors propose a novel technique that allows identifying an invariant subnetwork in a student model that mirrors the characteristics of the teacher in terms of computing neurons, path distribution, and topological attributes."," - The manuscript is clearly structured, and the subject of research is relevant
 - The authors have developed a novel technique to identify invariant characteristics of a student model mirroring key characteristics of the teacher network."," - The authors have used a single synthetic dataset to perform the experiments.
 - There is little reference to related work, and no baselines are considered when comparing the proposed approach","GENERAL COMMENTS:
 - (1) The authors have used a single synthetic dataset to perform the experiment. The results could be strengthened by considering multiple datasets (different synthetic parameterizations and publicly available datasets).
 - (2) We miss a more detailed related work. The authors provide a brief introduction, but references to related work are scarce. The authors may reference some works related to model distillation and pruning. In particular, we think the following works may be useful, given they mention some of the concepts and works related to those used in the manuscript:
 	- Liang, Tailin, et al. ""Pruning and quantization for deep neural network acceleration: A survey."" Neurocomputing 461 (2021): 370-403.
 	- (old, but relevant) Elizondo, David, and Emile Fiesler. ""A survey of partially connected neural networks."" International journal of neural systems 8.05n06 (1997): 535-558.
 	- Gou, Jianping, et al. ""Knowledge distillation: A survey."" International Journal of Computer Vision 129 (2021): 1789-1819.
 - (3) The authors should acknowledge the limitations of their research, discuss whether these limitations impact the results, and provide some insights on how these limitations could be addressed in future work.


FIGURES: 
 - (4) Figure 1: enhance the wording of the caption. ""Histogram for the quantities (10) (in blue) and (9) (in orange)"" -> ""Histogram for the quantities (Eq. 10) (in blue) and (Eq. 9) (in orange)""

SPELLING/WORDING:
 - (5) ""Remarkably several topological proprieties of the teacher"" -> ""Remarkably several topological properties of the teacher""
 - (6) ""These latter parameters are denoted λ(k) for reasons that will become clear in the following, "" -> in the following?
 - (7) ""The dataset will be generated by choosing the probability distribution"" -> ""The dataset was generated by choosing the probability distribution""?
 - (8) ""values stay stably across all choices of h above 20"" -> ""values stay stable across all choices of h above 20""
 - (9) ""To assess feature localization within the network, we generated aggregated histograms of the scalars (9) and (10)."" -> ""To assess feature localization within the network, we generated aggregated histograms of the scalars (Eq. 9) and (Eq. 10).""
 - (10) ""To ensure that the achieved conclusions are not influenced by the size of the second hidden layer, various student configurations have been tested, yielding equivalent results."" -> Please provide some details.
 - (11) ""The blue histograms stands for (9),"" -> ""The blue histograms stands for (Eq. 9),""",The authors should acknowledge the limitations of their research and provide some insights on how these limitations could be addressed in future work.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688646766971,,,NeurIPS.cc/2023/Conference,1FVmMlifl7,"['NeurIPS.cc/2023/Conference/Submission4811/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410971461,[],1,,,['everyone'],1FVmMlifl7,['NeurIPS.cc/2023/Conference/Submission4811/Reviewer_YyTW'],1688646766971,1702410971461,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4811/Reviewer_YyTW']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions three specific works that may be useful for the authors to reference, given their relevance to the concepts and works used in the manuscript:

1. Liang, Tailin, et al. ""Pruning and quantization for deep neural network acceleration: A survey."" Neurocomputing 461 (2021): 370-403.
2. Elizondo, David, and Emile Fiesler. ""A survey of partially connected neural networks."" International journal of neural systems 8.05n06 (1997): 535-558.
3. Gou, Jianping, et al. ""Knowledge distillation: A survey."" International Journal of Computer Vision 129 (2021): 1789-1819.

The reviewer suggests that the authors should provide a more detailed related work section and consider referencing these works, as they are related to model distillation and pruning, which are relevant to the manuscript.",1,"2021, 1997, 2021",Pruning and quantization for deep neural network acceleration A survey 
MszzvIfnDc,"The authors propose a novel technique that allows identifying an invariant subnetwork in a student model that mirrors the characteristics of the teacher in terms of computing neurons, path distribution, and topological attributes."," - The manuscript is clearly structured, and the subject of research is relevant
 - The authors have developed a novel technique to identify invariant characteristics of a student model mirroring key characteristics of the teacher network."," - The authors have used a single synthetic dataset to perform the experiments.
 - There is little reference to related work, and no baselines are considered when comparing the proposed approach","GENERAL COMMENTS:
 - (1) The authors have used a single synthetic dataset to perform the experiment. The results could be strengthened by considering multiple datasets (different synthetic parameterizations and publicly available datasets).
 - (2) We miss a more detailed related work. The authors provide a brief introduction, but references to related work are scarce. The authors may reference some works related to model distillation and pruning. In particular, we think the following works may be useful, given they mention some of the concepts and works related to those used in the manuscript:
 	- Liang, Tailin, et al. ""Pruning and quantization for deep neural network acceleration: A survey."" Neurocomputing 461 (2021): 370-403.
 	- (old, but relevant) Elizondo, David, and Emile Fiesler. ""A survey of partially connected neural networks."" International journal of neural systems 8.05n06 (1997): 535-558.
 	- Gou, Jianping, et al. ""Knowledge distillation: A survey."" International Journal of Computer Vision 129 (2021): 1789-1819.
 - (3) The authors should acknowledge the limitations of their research, discuss whether these limitations impact the results, and provide some insights on how these limitations could be addressed in future work.


FIGURES: 
 - (4) Figure 1: enhance the wording of the caption. ""Histogram for the quantities (10) (in blue) and (9) (in orange)"" -> ""Histogram for the quantities (Eq. 10) (in blue) and (Eq. 9) (in orange)""

SPELLING/WORDING:
 - (5) ""Remarkably several topological proprieties of the teacher"" -> ""Remarkably several topological properties of the teacher""
 - (6) ""These latter parameters are denoted λ(k) for reasons that will become clear in the following, "" -> in the following?
 - (7) ""The dataset will be generated by choosing the probability distribution"" -> ""The dataset was generated by choosing the probability distribution""?
 - (8) ""values stay stably across all choices of h above 20"" -> ""values stay stable across all choices of h above 20""
 - (9) ""To assess feature localization within the network, we generated aggregated histograms of the scalars (9) and (10)."" -> ""To assess feature localization within the network, we generated aggregated histograms of the scalars (Eq. 9) and (Eq. 10).""
 - (10) ""To ensure that the achieved conclusions are not influenced by the size of the second hidden layer, various student configurations have been tested, yielding equivalent results."" -> Please provide some details.
 - (11) ""The blue histograms stands for (9),"" -> ""The blue histograms stands for (Eq. 9),""",The authors should acknowledge the limitations of their research and provide some insights on how these limitations could be addressed in future work.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688646766971,,,NeurIPS.cc/2023/Conference,1FVmMlifl7,"['NeurIPS.cc/2023/Conference/Submission4811/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410971461,[],1,,,['everyone'],1FVmMlifl7,['NeurIPS.cc/2023/Conference/Submission4811/Reviewer_YyTW'],1688646766971,1702410971461,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4811/Reviewer_YyTW']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions three specific works that may be useful for the authors to reference, given their relevance to the concepts and works used in the manuscript:

1. Liang, Tailin, et al. ""Pruning and quantization for deep neural network acceleration: A survey."" Neurocomputing 461 (2021): 370-403.
2. Elizondo, David, and Emile Fiesler. ""A survey of partially connected neural networks."" International journal of neural systems 8.05n06 (1997): 535-558.
3. Gou, Jianping, et al. ""Knowledge distillation: A survey."" International Journal of Computer Vision 129 (2021): 1789-1819.

The reviewer suggests that the authors should provide a more detailed related work section and consider referencing these works, as they are related to model distillation and pruning, which are relevant to the manuscript.",1,"2021, 1997, 2021",A survey of partially connected neural networks 
MszzvIfnDc,"The authors propose a novel technique that allows identifying an invariant subnetwork in a student model that mirrors the characteristics of the teacher in terms of computing neurons, path distribution, and topological attributes."," - The manuscript is clearly structured, and the subject of research is relevant
 - The authors have developed a novel technique to identify invariant characteristics of a student model mirroring key characteristics of the teacher network."," - The authors have used a single synthetic dataset to perform the experiments.
 - There is little reference to related work, and no baselines are considered when comparing the proposed approach","GENERAL COMMENTS:
 - (1) The authors have used a single synthetic dataset to perform the experiment. The results could be strengthened by considering multiple datasets (different synthetic parameterizations and publicly available datasets).
 - (2) We miss a more detailed related work. The authors provide a brief introduction, but references to related work are scarce. The authors may reference some works related to model distillation and pruning. In particular, we think the following works may be useful, given they mention some of the concepts and works related to those used in the manuscript:
 	- Liang, Tailin, et al. ""Pruning and quantization for deep neural network acceleration: A survey."" Neurocomputing 461 (2021): 370-403.
 	- (old, but relevant) Elizondo, David, and Emile Fiesler. ""A survey of partially connected neural networks."" International journal of neural systems 8.05n06 (1997): 535-558.
 	- Gou, Jianping, et al. ""Knowledge distillation: A survey."" International Journal of Computer Vision 129 (2021): 1789-1819.
 - (3) The authors should acknowledge the limitations of their research, discuss whether these limitations impact the results, and provide some insights on how these limitations could be addressed in future work.


FIGURES: 
 - (4) Figure 1: enhance the wording of the caption. ""Histogram for the quantities (10) (in blue) and (9) (in orange)"" -> ""Histogram for the quantities (Eq. 10) (in blue) and (Eq. 9) (in orange)""

SPELLING/WORDING:
 - (5) ""Remarkably several topological proprieties of the teacher"" -> ""Remarkably several topological properties of the teacher""
 - (6) ""These latter parameters are denoted λ(k) for reasons that will become clear in the following, "" -> in the following?
 - (7) ""The dataset will be generated by choosing the probability distribution"" -> ""The dataset was generated by choosing the probability distribution""?
 - (8) ""values stay stably across all choices of h above 20"" -> ""values stay stable across all choices of h above 20""
 - (9) ""To assess feature localization within the network, we generated aggregated histograms of the scalars (9) and (10)."" -> ""To assess feature localization within the network, we generated aggregated histograms of the scalars (Eq. 9) and (Eq. 10).""
 - (10) ""To ensure that the achieved conclusions are not influenced by the size of the second hidden layer, various student configurations have been tested, yielding equivalent results."" -> Please provide some details.
 - (11) ""The blue histograms stands for (9),"" -> ""The blue histograms stands for (Eq. 9),""",The authors should acknowledge the limitations of their research and provide some insights on how these limitations could be addressed in future work.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688646766971,,,NeurIPS.cc/2023/Conference,1FVmMlifl7,"['NeurIPS.cc/2023/Conference/Submission4811/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410971461,[],1,,,['everyone'],1FVmMlifl7,['NeurIPS.cc/2023/Conference/Submission4811/Reviewer_YyTW'],1688646766971,1702410971461,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4811/Reviewer_YyTW']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions three specific works that may be useful for the authors to reference, given their relevance to the concepts and works used in the manuscript:

1. Liang, Tailin, et al. ""Pruning and quantization for deep neural network acceleration: A survey."" Neurocomputing 461 (2021): 370-403.
2. Elizondo, David, and Emile Fiesler. ""A survey of partially connected neural networks."" International journal of neural systems 8.05n06 (1997): 535-558.
3. Gou, Jianping, et al. ""Knowledge distillation: A survey."" International Journal of Computer Vision 129 (2021): 1789-1819.

The reviewer suggests that the authors should provide a more detailed related work section and consider referencing these works, as they are related to model distillation and pruning, which are relevant to the manuscript.",1,"2021, 1997, 2021",Knowledge distillation A survey
2Tf9ydc3LV,"Transformer models have the potential to acquire essentially arbitrary patterns of attention during training. But what patterns do they acquire in practice? This is the question taken up in the present paper. The data for the paper are 40 word sequences from the classic Brown corpus. The language models examined are GPT-2, LLaMA, and BERT. The paper introduces a word-swap procedure for evaluating integration. It argues that the large language models exhibit a transition from exponential to power-law dynamics across the layers of the network. It describes the power-law windows as structure-yoked (in the context of the study, this means yoked to sentence boundaries) and the exponential windows as position-yoked.",The paper raises a good question about what patterns of attention are actually acquired during the training of large language models. It undertakes to characterize the patterns in terms of functional forms. This sets up an important point of potential contact between machine learning and the study of scaling laws in physics. ,"The author(s) assert that the integration windows are surprisingly well fit by a convex combination of an exponential and a power law. They do not rigorously evaluate any alternative fits. They do not appear to be aware of the substantial research literature on power laws, and appear to have overlooked the following points.
1) Power laws can themselves be generated as mixtures of exponentials.
2) To statistically distinguish power laws from other similar-looking distributions, it would be necessary to explore a very much greater range of time scales than appear in this study.
Here are a few of the very large number of references bearing on this issue:
MEJ Newman (2005) Power laws, Pareto distributions, and Zipfs’s law. Contemporary Physics
M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions. 
RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11

By equating structure-yoking with sentence boundaries, the author(s) disregard all other types of linguistic structures. These range from smaller components of syntactic structures (such as the structure of noun phrases) to larger structures that control discourse coherence. There seems to be no justification for thinking that all these other structures are ""position yoked"". 

Finally the decision to report results only the 1979 Brown corpus is puzzling (the paper states that similar results were obtained using the BookCorpus, but provides no details about that). The Brown corpus contains only 1 million words, hence the vocabulary is only the more frequent words of English. It does not contain examples of various genres that figured in the training sets for the LLMs. It would be important to understand how LLMs work on the range and variety of material they were trained on. 
","If revising this paper, it will be important to provide rigorous comparisons of different model fits, and to test the approach on a more complete range of linguistic material and a more carefully articulated set of structures. ","The author(s) acknowledge that the relationship between natural language and the integration windows learned by the LLMs was not explored in depth. As mentioned under ""Weaknesses"", they do not appear to have a complete grasp of what is needed to justify the functional forms they selected.",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,2 fair,1688480543887,,,NeurIPS.cc/2023/Conference,1EYKYJeZtR,"['NeurIPS.cc/2023/Conference/Submission8902/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411197212,[],2,,,['everyone'],1EYKYJeZtR,['NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr'],1688480543887,1702411197212,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that are relevant to the topic of power laws and their analysis, including:

1. MEJ Newman (2005) Power laws, Pareto distributions, and Zipf's law. Contemporary Physics
2. M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions.
3. RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
4. S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
5. Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11)

The reviewer implies that the authors should be aware of and cite this literature in order to properly understand and analyze power laws in the context of their research.",1,"2005, 2004, 2008, 2009, 2009",Power laws Pareto distributions and Zipf's law 
2Tf9ydc3LV,"Transformer models have the potential to acquire essentially arbitrary patterns of attention during training. But what patterns do they acquire in practice? This is the question taken up in the present paper. The data for the paper are 40 word sequences from the classic Brown corpus. The language models examined are GPT-2, LLaMA, and BERT. The paper introduces a word-swap procedure for evaluating integration. It argues that the large language models exhibit a transition from exponential to power-law dynamics across the layers of the network. It describes the power-law windows as structure-yoked (in the context of the study, this means yoked to sentence boundaries) and the exponential windows as position-yoked.",The paper raises a good question about what patterns of attention are actually acquired during the training of large language models. It undertakes to characterize the patterns in terms of functional forms. This sets up an important point of potential contact between machine learning and the study of scaling laws in physics. ,"The author(s) assert that the integration windows are surprisingly well fit by a convex combination of an exponential and a power law. They do not rigorously evaluate any alternative fits. They do not appear to be aware of the substantial research literature on power laws, and appear to have overlooked the following points.
1) Power laws can themselves be generated as mixtures of exponentials.
2) To statistically distinguish power laws from other similar-looking distributions, it would be necessary to explore a very much greater range of time scales than appear in this study.
Here are a few of the very large number of references bearing on this issue:
MEJ Newman (2005) Power laws, Pareto distributions, and Zipfs’s law. Contemporary Physics
M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions. 
RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11

By equating structure-yoking with sentence boundaries, the author(s) disregard all other types of linguistic structures. These range from smaller components of syntactic structures (such as the structure of noun phrases) to larger structures that control discourse coherence. There seems to be no justification for thinking that all these other structures are ""position yoked"". 

Finally the decision to report results only the 1979 Brown corpus is puzzling (the paper states that similar results were obtained using the BookCorpus, but provides no details about that). The Brown corpus contains only 1 million words, hence the vocabulary is only the more frequent words of English. It does not contain examples of various genres that figured in the training sets for the LLMs. It would be important to understand how LLMs work on the range and variety of material they were trained on. 
","If revising this paper, it will be important to provide rigorous comparisons of different model fits, and to test the approach on a more complete range of linguistic material and a more carefully articulated set of structures. ","The author(s) acknowledge that the relationship between natural language and the integration windows learned by the LLMs was not explored in depth. As mentioned under ""Weaknesses"", they do not appear to have a complete grasp of what is needed to justify the functional forms they selected.",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,2 fair,1688480543887,,,NeurIPS.cc/2023/Conference,1EYKYJeZtR,"['NeurIPS.cc/2023/Conference/Submission8902/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411197212,[],2,,,['everyone'],1EYKYJeZtR,['NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr'],1688480543887,1702411197212,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that are relevant to the topic of power laws and their analysis, including:

1. MEJ Newman (2005) Power laws, Pareto distributions, and Zipf's law. Contemporary Physics
2. M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions.
3. RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
4. S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
5. Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11)

The reviewer implies that the authors should be aware of and cite this literature in order to properly understand and analyze power laws in the context of their research.",1,"2005, 2004, 2008, 2009, 2009",A brief history of generative models for power law and lognormal distributions 
2Tf9ydc3LV,"Transformer models have the potential to acquire essentially arbitrary patterns of attention during training. But what patterns do they acquire in practice? This is the question taken up in the present paper. The data for the paper are 40 word sequences from the classic Brown corpus. The language models examined are GPT-2, LLaMA, and BERT. The paper introduces a word-swap procedure for evaluating integration. It argues that the large language models exhibit a transition from exponential to power-law dynamics across the layers of the network. It describes the power-law windows as structure-yoked (in the context of the study, this means yoked to sentence boundaries) and the exponential windows as position-yoked.",The paper raises a good question about what patterns of attention are actually acquired during the training of large language models. It undertakes to characterize the patterns in terms of functional forms. This sets up an important point of potential contact between machine learning and the study of scaling laws in physics. ,"The author(s) assert that the integration windows are surprisingly well fit by a convex combination of an exponential and a power law. They do not rigorously evaluate any alternative fits. They do not appear to be aware of the substantial research literature on power laws, and appear to have overlooked the following points.
1) Power laws can themselves be generated as mixtures of exponentials.
2) To statistically distinguish power laws from other similar-looking distributions, it would be necessary to explore a very much greater range of time scales than appear in this study.
Here are a few of the very large number of references bearing on this issue:
MEJ Newman (2005) Power laws, Pareto distributions, and Zipfs’s law. Contemporary Physics
M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions. 
RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11

By equating structure-yoking with sentence boundaries, the author(s) disregard all other types of linguistic structures. These range from smaller components of syntactic structures (such as the structure of noun phrases) to larger structures that control discourse coherence. There seems to be no justification for thinking that all these other structures are ""position yoked"". 

Finally the decision to report results only the 1979 Brown corpus is puzzling (the paper states that similar results were obtained using the BookCorpus, but provides no details about that). The Brown corpus contains only 1 million words, hence the vocabulary is only the more frequent words of English. It does not contain examples of various genres that figured in the training sets for the LLMs. It would be important to understand how LLMs work on the range and variety of material they were trained on. 
","If revising this paper, it will be important to provide rigorous comparisons of different model fits, and to test the approach on a more complete range of linguistic material and a more carefully articulated set of structures. ","The author(s) acknowledge that the relationship between natural language and the integration windows learned by the LLMs was not explored in depth. As mentioned under ""Weaknesses"", they do not appear to have a complete grasp of what is needed to justify the functional forms they selected.",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,2 fair,1688480543887,,,NeurIPS.cc/2023/Conference,1EYKYJeZtR,"['NeurIPS.cc/2023/Conference/Submission8902/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411197212,[],2,,,['everyone'],1EYKYJeZtR,['NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr'],1688480543887,1702411197212,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that are relevant to the topic of power laws and their analysis, including:

1. MEJ Newman (2005) Power laws, Pareto distributions, and Zipf's law. Contemporary Physics
2. M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions.
3. RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
4. S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
5. Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11)

The reviewer implies that the authors should be aware of and cite this literature in order to properly understand and analyze power laws in the context of their research.",1,"2005, 2004, 2008, 2009, 2009",A Poissonian explanation for heavy tails in email communications 
2Tf9ydc3LV,"Transformer models have the potential to acquire essentially arbitrary patterns of attention during training. But what patterns do they acquire in practice? This is the question taken up in the present paper. The data for the paper are 40 word sequences from the classic Brown corpus. The language models examined are GPT-2, LLaMA, and BERT. The paper introduces a word-swap procedure for evaluating integration. It argues that the large language models exhibit a transition from exponential to power-law dynamics across the layers of the network. It describes the power-law windows as structure-yoked (in the context of the study, this means yoked to sentence boundaries) and the exponential windows as position-yoked.",The paper raises a good question about what patterns of attention are actually acquired during the training of large language models. It undertakes to characterize the patterns in terms of functional forms. This sets up an important point of potential contact between machine learning and the study of scaling laws in physics. ,"The author(s) assert that the integration windows are surprisingly well fit by a convex combination of an exponential and a power law. They do not rigorously evaluate any alternative fits. They do not appear to be aware of the substantial research literature on power laws, and appear to have overlooked the following points.
1) Power laws can themselves be generated as mixtures of exponentials.
2) To statistically distinguish power laws from other similar-looking distributions, it would be necessary to explore a very much greater range of time scales than appear in this study.
Here are a few of the very large number of references bearing on this issue:
MEJ Newman (2005) Power laws, Pareto distributions, and Zipfs’s law. Contemporary Physics
M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions. 
RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11

By equating structure-yoking with sentence boundaries, the author(s) disregard all other types of linguistic structures. These range from smaller components of syntactic structures (such as the structure of noun phrases) to larger structures that control discourse coherence. There seems to be no justification for thinking that all these other structures are ""position yoked"". 

Finally the decision to report results only the 1979 Brown corpus is puzzling (the paper states that similar results were obtained using the BookCorpus, but provides no details about that). The Brown corpus contains only 1 million words, hence the vocabulary is only the more frequent words of English. It does not contain examples of various genres that figured in the training sets for the LLMs. It would be important to understand how LLMs work on the range and variety of material they were trained on. 
","If revising this paper, it will be important to provide rigorous comparisons of different model fits, and to test the approach on a more complete range of linguistic material and a more carefully articulated set of structures. ","The author(s) acknowledge that the relationship between natural language and the integration windows learned by the LLMs was not explored in depth. As mentioned under ""Weaknesses"", they do not appear to have a complete grasp of what is needed to justify the functional forms they selected.",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,2 fair,1688480543887,,,NeurIPS.cc/2023/Conference,1EYKYJeZtR,"['NeurIPS.cc/2023/Conference/Submission8902/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411197212,[],2,,,['everyone'],1EYKYJeZtR,['NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr'],1688480543887,1702411197212,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that are relevant to the topic of power laws and their analysis, including:

1. MEJ Newman (2005) Power laws, Pareto distributions, and Zipf's law. Contemporary Physics
2. M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions.
3. RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
4. S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
5. Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11)

The reviewer implies that the authors should be aware of and cite this literature in order to properly understand and analyze power laws in the context of their research.",1,"2005, 2004, 2008, 2009, 2009",Superlinear scaling for innovation in cities 
2Tf9ydc3LV,"Transformer models have the potential to acquire essentially arbitrary patterns of attention during training. But what patterns do they acquire in practice? This is the question taken up in the present paper. The data for the paper are 40 word sequences from the classic Brown corpus. The language models examined are GPT-2, LLaMA, and BERT. The paper introduces a word-swap procedure for evaluating integration. It argues that the large language models exhibit a transition from exponential to power-law dynamics across the layers of the network. It describes the power-law windows as structure-yoked (in the context of the study, this means yoked to sentence boundaries) and the exponential windows as position-yoked.",The paper raises a good question about what patterns of attention are actually acquired during the training of large language models. It undertakes to characterize the patterns in terms of functional forms. This sets up an important point of potential contact between machine learning and the study of scaling laws in physics. ,"The author(s) assert that the integration windows are surprisingly well fit by a convex combination of an exponential and a power law. They do not rigorously evaluate any alternative fits. They do not appear to be aware of the substantial research literature on power laws, and appear to have overlooked the following points.
1) Power laws can themselves be generated as mixtures of exponentials.
2) To statistically distinguish power laws from other similar-looking distributions, it would be necessary to explore a very much greater range of time scales than appear in this study.
Here are a few of the very large number of references bearing on this issue:
MEJ Newman (2005) Power laws, Pareto distributions, and Zipfs’s law. Contemporary Physics
M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions. 
RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11

By equating structure-yoking with sentence boundaries, the author(s) disregard all other types of linguistic structures. These range from smaller components of syntactic structures (such as the structure of noun phrases) to larger structures that control discourse coherence. There seems to be no justification for thinking that all these other structures are ""position yoked"". 

Finally the decision to report results only the 1979 Brown corpus is puzzling (the paper states that similar results were obtained using the BookCorpus, but provides no details about that). The Brown corpus contains only 1 million words, hence the vocabulary is only the more frequent words of English. It does not contain examples of various genres that figured in the training sets for the LLMs. It would be important to understand how LLMs work on the range and variety of material they were trained on. 
","If revising this paper, it will be important to provide rigorous comparisons of different model fits, and to test the approach on a more complete range of linguistic material and a more carefully articulated set of structures. ","The author(s) acknowledge that the relationship between natural language and the integration windows learned by the LLMs was not explored in depth. As mentioned under ""Weaknesses"", they do not appear to have a complete grasp of what is needed to justify the functional forms they selected.",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,2 fair,2 fair,1688480543887,,,NeurIPS.cc/2023/Conference,1EYKYJeZtR,"['NeurIPS.cc/2023/Conference/Submission8902/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411197212,[],2,,,['everyone'],1EYKYJeZtR,['NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr'],1688480543887,1702411197212,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8902/Reviewer_NDTr']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references that are relevant to the topic of power laws and their analysis, including:

1. MEJ Newman (2005) Power laws, Pareto distributions, and Zipf's law. Contemporary Physics
2. M Mitzenmacher (2004) A brief history of generative models for power law and lognormal distributions.
3. RD Malmgren et al. (2008) A Poissonian explanation for heavy tails in email communications. PNAS 105(47)
4. S Arbesman et al (2009) Superlinear scaling for innovation in cities. Phys. Rev. E 79, 016115
5. Altmann et al. (2009) Bursts, Lulls, and Scaling in the Temporal Distributions of words. PLoS One 4(11)

The reviewer implies that the authors should be aware of and cite this literature in order to properly understand and analyze power laws in the context of their research.",1,"2005, 2004, 2008, 2009, 2009",Bursts Lulls and Scaling in the Temporal Distributions of words
JQCcCDIjSc,"This paper studies a new combinatorial bandit problem that generalizes the existing cascading and assortment bandits. The authors first propose a UCB-based algorithm, UCB-CCA, that achieves a tighter regret bound than existing bounds for cascading contextual bandits by eliminating the dependence on cascade length $K$. They also introduce an improved algorithm, UCB-CCA+, and use a Bernstein-type concentration to prove a regret bound without $\kappa^{-1}$ dependence, where $\kappa$ is a problem-dependent constant in the regret bound of UCB-CCA. Numerical experiments validate the effectiveness of the proposed algorithms.","1) This paper is the first to study the combination of contextual cascading and assortment bandits. This new problem is well-motivated by real-world applications in recommender systems.
2) One of the main technical contributions is the new Lipschitz continuity of the expected reward function for contextual cascading assortment bandits in Lemma 4.2, which helps prove the regret bound of UCB-CCA is independent of $K$ and $M$. (However, I have a question about the proof of this Lipschitz continuity; see below.)
3) The proposed UCB-CCA+ algorithm achieves an improved regret bound than that of UCB-CCA, solving the two technical challenges (dependence on cascade length and $\kappa$) faced by contextual cascading and assortment bandits simultaneously.","1) For contextual combinatorial bandits, there is a recent result [A] that provides a regret bound independent of the cascade length $K$ using a variance adaptive algorithm. Moreover, its regret bound can get rid of $p^*$, which raises a concern that whether the optimistic exposure swapping in Section 3.3 is necessary or whether the $p^*$ issue can be resolved by a more involved analysis.
2) The Lipschitz continuity in Lemma 4.2 is a key component of the analysis. However, the proof in line 402-405 is unclear to me. I would appreciate it if the authors could add more details about the proof; a simple example of the non-contextual cascading bandit can also be helpful.
3) Although UCB-CCA+ achieves a good regret bound, there is no discussion on the lower bound of contextual assortment combinatorial bandits: would it be similar to that of the contextual combinatorial bandits or contextual assortment bandits?

[A] Xutong Liu, Jinhang Zuo, Siwei Wang, John CS Lui, Mohammad Hajiesmaili, Adam Wierman, and Wei Chen. Contextual Combinatorial Bandits with Probabilistically Triggered Arms. In International Conference on Machine Learning, 2023.",Typo: line 207: In round (at every round) $t$,See Weaknesses.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1689261186636,,,NeurIPS.cc/2023/Conference,1DTCoyAFiV,"['NeurIPS.cc/2023/Conference/Submission10893/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411306297,[],4,,,['everyone'],1DTCoyAFiV,['NeurIPS.cc/2023/Conference/Submission10893/Reviewer_anEU'],1689261186636,1702411306297,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10893/Reviewer_anEU']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, which is:

Xutong Liu, Jinhang Zuo, Siwei Wang, John CS Lui, Mohammad Hajiesmaili, Adam Wierman, and Wei Chen. Contextual Combinatorial Bandits with Probabilistically Triggered Arms. In International Conference on Machine Learning, 2023.

This is referred to as [A] in the review. The reviewer mentions this paper as a recent result that provides a regret bound independent of the cascade length $K$ using a variance adaptive algorithm, and raises a concern about whether the optimistic exposure swapping in Section 3.3 is necessary or whether the $p^*$ issue can be resolved by a more involved analysis.",1,2023,Contextual Combinatorial Bandits with Probabilistically Triggered Arms
NQF3sTEcUN,"The authors of the paper present Protein Evolutionary Transformer (PoET), an autoregressive transformer-based model that is able to generate sets of related protein sequences, enabled by the proposed novel Transformer layer. This sequences-of-sequences generating method benefits from transfer learning, is able to handle novel indels, and does not need a multiple sequence alignment (MSA) as its input. The model can be conditioned on sequences of a protein family of interest for generation and scoring. The proposed method shows similar or superior performance compared to other models in fitness prediction tasks, while being faster without requiring retraining.","This paper is very well-written, with clear explanations, great examples and figures, and a comprehensive Related Work section. The experiments are explained well, they align with the claims made in the paper, and the results are discussed adequately. Moreover, there's an abundance of extra information in the appendix.","I enjoyed reading this paper, I do not have any major concerns. However, I did miss a ""Limitations"" section or something equivalent, either in the main text or in the supplementary. Moreover, I think some results (mainly those in the ""Ablation"" section 5.2) could be made stronger, for example by averaging performance over multiple runs and showing error bars, because the observed trends are not always that convincing. Finally, and this is more of a general issue, even though the results show improved/competitive performance, the average correlation values to experimental data is relatively low, i.e. around 0.5, which is still quite a weak correlation. The mismatch between density estimation and fitness prediction has been discussed before, for example in [1], and it might be worth discussing to some extent in this paper as well.

[1] Weinstein, Eli, et al. ""Non-identifiability and the Blessings of Misspecification in Models of Molecular Fitness."" Advances in Neural Information Processing Systems 35 (2022): 5484-5497.","1. As mentioned in ""Weaknesses"": Limitations section missing.
2. As mentioned in ""Weaknesses"": perhaps include some discussion on the (mis)match between density estimation and fitness prediction.
3. As mentioned in ""Weaknesses"": if at all possible, it would be very informative to report averages over multiple runs, especially for the ablation results (Figure 3) but also for Table 1, to make the observations more convincing.

Minor comments:

4. The abstract states that PoET *outperforms* other models. However, from the number of boxed values in Table 1, this claim might be a bit too strong.
5. Do you report the dimensionality of embedding size $d$ somewhere? I might have missed it.
6. If I understand correctly, the relative positional encoding scheme would probably not be beneficial when there are big differences in sequence length amongst homologous proteins. In that case, there must be a better encoding possible (some MSA-like). Could you discuss this or perhaps mention it as a limitation?
7. The results on ensembling PoET with other models (Table 1 + Table 6 in the appendix) are interesting. Did you also try ensembling PoET with one or multiple other PoET model(s)?
8. Section 5.2.1: 
    * In general, it could be considered ""cheating"" to monitor the correlation during training since this is essentially your test data. If it's just for these experiments then it's fine, I'm just checking if it's not something you used as a stopping criterion in general?
    * Context Length: apart from the earlier suggestion to do multiple runs here, it's also worth pointing out that the drawn conclusions depend quite strongly on when it was decided to stop training. 
    * Model Size: the stopping criterion of ""when the performance seemed to plateau"" looks a bit arbitrary. If we compare the lines in the right graph in Figure 3, the blue line was cut off quite early while similar ""plateaus"" can be found in the purple and brown lines, which were allowed to train longer.
9. Missing references:
    * Figure 1a is never referenced.
    * Paper reference missing for RoPE?
    * Not all appendices are referenced in the main text (B, E, F, I, and O are missing).","Even though some limitations are touched upon in the main text, a thorough discussion of limitations is lacking.",['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,3 good,1688551303246,,,NeurIPS.cc/2023/Conference,1CJ8D7P8RZ,"['NeurIPS.cc/2023/Conference/Submission8546/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411176765,[],3,,,['everyone'],1CJ8D7P8RZ,['NeurIPS.cc/2023/Conference/Submission8546/Reviewer_sUvw'],1688551303246,1702411176765,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8546/Reviewer_sUvw']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

Weinstein, Eli, et al. ""Non-identifiability and the Blessings of Misspecification in Models of Molecular Fitness."" Advances in Neural Information Processing Systems 35 (2022): 5484-5497.

This is mentioned in the ""Weaknesses"" section, where the reviewer suggests that the authors discuss the mismatch between density estimation and fitness prediction, and cites this paper as an example of previous work that has discussed this issue.",1,2022,Non-identifiability and the Blessings of Misspecification in Models of Molecular Fitness
rCOLpCOlXO,"This paper proposes a novel multi-agent reinforcement learning algorithm with agents parameterized with a pre-trained GPT architecture for de novo drug design.  The authors propose a modified objective function with an intrinsic reward inspired bonus to encourage diversity between agents and also propose to use a constraint to keep the fine-tuned agents close to the pre-trained agents.  The authors evaluate their algorithm on Guacamol, by generating a number of inhibitors for SARS-CoV2 targets.  They also perform ablations on their method with the GSK3$\beta$ and JNK3 maximization tasks.","Some strengths of this paper are:

- An intrinsic reward like term added to the agents' loss which encourages diversity.
- The method performs favorably to the other methods compared against on Guacamol benchmark.
- It performs comprehensive ablations on the GSK3$\beta$ and JNK3 maximization tasks","My main complaint about this paper is that I am not convinced of the algorithm's superiority over rivaling methods based upon the experiments section.  I feel that the paper has both missed some necessary baseline methods and that the experiments as they stand are insufficient in demonstrating the paper's main claim that its method leads to improved diversity over other methods.  I also have concerns about missing related work in multi-agent RL in which there is already a body of literature on encouraging diversity among agents, as well as other competing methods which have been applied to molecular drug generation such as diffusion models and GFlowNets.  I also have concerns regarding this paper's reproducibility.  I will go over these concerns one by one.

## Experiments section
1. The main claim of this paper is that its approach leads to superior diversity, but I did not see any experiments _comparing_ the diversity of molecules it generates to molecules generated by other methods.  Indeed, there were experiments looking at the diversity of molecules generated by their method, but it was only their method.  There isn't an indication whether their main claim of improved diversity is true if there is no comparison to other methods.
2. While the generated molecules do look diverse to my non-chemist eye, I would like to see more generated molecules and critically some measurement of diversity of the generated molecules to be convinced.  Also, these results were on only one seed which is insufficient.  At minimum there should be three seeds, ideally quite a bit more (see [here](https://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html)).
3. There are missing baselines in the experiments.  The authors should have compared to an existing LLM molecule generation method such as MolGPT, but this was missing from their experiments.  There are other methods for encouraging diversity for molecule design with RL-inspired machinery such as GFlowNets (https://arxiv.org/abs/2106.04399) that should be compared against.  Also, it would have been nice to see a comparison with one of the recent works on diffusion for molecular design (e.g., https://arxiv.org/pdf/2203.17003.pdf, https://arxiv.org/pdf/2305.01140.pdf), or at minimum a compelling reason for why not to compare to these methods.
4. In the Guacamol experiments it seems experiments were run over one seed.  This is insufficient.
5. A more minor comment: it's hard to understand the significance of the Guacamol task when the tasks are ordered 1-20 without context of what the tasks actually are.

## Missing references
1. I mentioned in the last point, but it would have been nice to see some discussion of other methods such as GFlowNets or diffusion models which also try to encourage diversity in molecular design.
2. There is already a body of literature on encouraging diversity in multi-agent RL, but I did not see references to this literature.  Some representative papers may be https://arxiv.org/abs/2106.02195 and https://openreview.net/forum?id=H-6iczs__Ro.

## Reproducibility
1. All experiments seem to have been run with one seed.  There is no way to know if the results would hold with more seeds.
2. There is no listing of the hyperparameters used or the hyperparameter tuning methods used (or values tried if a grid search).
3. There is no (anonymized) submitted code available to verify or reproduce the authors' claims.

## Clarity issues
1. In the section explaining the loss function, the indexing used is rather confusing.  E.g., the authors use a loss $L_1$ which seems fixed, then also a loss $L_k$ which seems to index the different agents (so what about when $k=1$?).","1. In the loss function, why does the sum over agents only go up to $k-1$?  Shouldn't the second loss term be something like $\sum_{j=1}^n s(x) \left|P_k(x) - P_j(x)\right|$?  Why compare to only the agents before this index as the ordering seems arbitrary.
2. Did the authors consider using a method for encouraging diversity which already exists in the multi-agent RL literature?  If so, why did they not use it and did they run any comparisons?","The main and most important limitation of this work is that I do not know whether the proposed method actually is competitive with rival methods due to some missing experiments, baselines, and insufficient reproducibility.",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,2 fair,2 fair,1690314505964,,,NeurIPS.cc/2023/Conference,1B6YKnHYBb,"['NeurIPS.cc/2023/Conference/Submission7922/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411137374,[],3,,,['everyone'],1B6YKnHYBb,['NeurIPS.cc/2023/Conference/Submission7922/Reviewer_xd9X'],1690314505964,1702411137374,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7922/Reviewer_xd9X']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that they believe are relevant to the work and should be discussed or compared to, including:

* GFlowNets (https://arxiv.org/abs/2106.04399)
* Diffusion models for molecular design (e.g., https://arxiv.org/pdf/2203.17003.pdf, https://arxiv.org/pdf/2305.01140.pdf)
* Papers on encouraging diversity in multi-agent RL (e.g., https://arxiv.org/abs/2106.02195, https://openreview.net/forum?id=H-6iczs__Ro)
* MolGPT, an existing LLM molecule generation method.

The reviewer also asks questions about why the authors did not consider using existing methods for encouraging diversity from the multi-agent RL literature, and suggests that the authors should have compared their method to these existing methods.",1,"2021, 2022, 2023, 2021",GFlowNets 
rCOLpCOlXO,"This paper proposes a novel multi-agent reinforcement learning algorithm with agents parameterized with a pre-trained GPT architecture for de novo drug design.  The authors propose a modified objective function with an intrinsic reward inspired bonus to encourage diversity between agents and also propose to use a constraint to keep the fine-tuned agents close to the pre-trained agents.  The authors evaluate their algorithm on Guacamol, by generating a number of inhibitors for SARS-CoV2 targets.  They also perform ablations on their method with the GSK3$\beta$ and JNK3 maximization tasks.","Some strengths of this paper are:

- An intrinsic reward like term added to the agents' loss which encourages diversity.
- The method performs favorably to the other methods compared against on Guacamol benchmark.
- It performs comprehensive ablations on the GSK3$\beta$ and JNK3 maximization tasks","My main complaint about this paper is that I am not convinced of the algorithm's superiority over rivaling methods based upon the experiments section.  I feel that the paper has both missed some necessary baseline methods and that the experiments as they stand are insufficient in demonstrating the paper's main claim that its method leads to improved diversity over other methods.  I also have concerns about missing related work in multi-agent RL in which there is already a body of literature on encouraging diversity among agents, as well as other competing methods which have been applied to molecular drug generation such as diffusion models and GFlowNets.  I also have concerns regarding this paper's reproducibility.  I will go over these concerns one by one.

## Experiments section
1. The main claim of this paper is that its approach leads to superior diversity, but I did not see any experiments _comparing_ the diversity of molecules it generates to molecules generated by other methods.  Indeed, there were experiments looking at the diversity of molecules generated by their method, but it was only their method.  There isn't an indication whether their main claim of improved diversity is true if there is no comparison to other methods.
2. While the generated molecules do look diverse to my non-chemist eye, I would like to see more generated molecules and critically some measurement of diversity of the generated molecules to be convinced.  Also, these results were on only one seed which is insufficient.  At minimum there should be three seeds, ideally quite a bit more (see [here](https://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html)).
3. There are missing baselines in the experiments.  The authors should have compared to an existing LLM molecule generation method such as MolGPT, but this was missing from their experiments.  There are other methods for encouraging diversity for molecule design with RL-inspired machinery such as GFlowNets (https://arxiv.org/abs/2106.04399) that should be compared against.  Also, it would have been nice to see a comparison with one of the recent works on diffusion for molecular design (e.g., https://arxiv.org/pdf/2203.17003.pdf, https://arxiv.org/pdf/2305.01140.pdf), or at minimum a compelling reason for why not to compare to these methods.
4. In the Guacamol experiments it seems experiments were run over one seed.  This is insufficient.
5. A more minor comment: it's hard to understand the significance of the Guacamol task when the tasks are ordered 1-20 without context of what the tasks actually are.

## Missing references
1. I mentioned in the last point, but it would have been nice to see some discussion of other methods such as GFlowNets or diffusion models which also try to encourage diversity in molecular design.
2. There is already a body of literature on encouraging diversity in multi-agent RL, but I did not see references to this literature.  Some representative papers may be https://arxiv.org/abs/2106.02195 and https://openreview.net/forum?id=H-6iczs__Ro.

## Reproducibility
1. All experiments seem to have been run with one seed.  There is no way to know if the results would hold with more seeds.
2. There is no listing of the hyperparameters used or the hyperparameter tuning methods used (or values tried if a grid search).
3. There is no (anonymized) submitted code available to verify or reproduce the authors' claims.

## Clarity issues
1. In the section explaining the loss function, the indexing used is rather confusing.  E.g., the authors use a loss $L_1$ which seems fixed, then also a loss $L_k$ which seems to index the different agents (so what about when $k=1$?).","1. In the loss function, why does the sum over agents only go up to $k-1$?  Shouldn't the second loss term be something like $\sum_{j=1}^n s(x) \left|P_k(x) - P_j(x)\right|$?  Why compare to only the agents before this index as the ordering seems arbitrary.
2. Did the authors consider using a method for encouraging diversity which already exists in the multi-agent RL literature?  If so, why did they not use it and did they run any comparisons?","The main and most important limitation of this work is that I do not know whether the proposed method actually is competitive with rival methods due to some missing experiments, baselines, and insufficient reproducibility.",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,2 fair,2 fair,1690314505964,,,NeurIPS.cc/2023/Conference,1B6YKnHYBb,"['NeurIPS.cc/2023/Conference/Submission7922/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411137374,[],3,,,['everyone'],1B6YKnHYBb,['NeurIPS.cc/2023/Conference/Submission7922/Reviewer_xd9X'],1690314505964,1702411137374,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7922/Reviewer_xd9X']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that they believe are relevant to the work and should be discussed or compared to, including:

* GFlowNets (https://arxiv.org/abs/2106.04399)
* Diffusion models for molecular design (e.g., https://arxiv.org/pdf/2203.17003.pdf, https://arxiv.org/pdf/2305.01140.pdf)
* Papers on encouraging diversity in multi-agent RL (e.g., https://arxiv.org/abs/2106.02195, https://openreview.net/forum?id=H-6iczs__Ro)
* MolGPT, an existing LLM molecule generation method.

The reviewer also asks questions about why the authors did not consider using existing methods for encouraging diversity from the multi-agent RL literature, and suggests that the authors should have compared their method to these existing methods.",1,"2021, 2022, 2023, 2021",Diffusion models for molecular design 
rCOLpCOlXO,"This paper proposes a novel multi-agent reinforcement learning algorithm with agents parameterized with a pre-trained GPT architecture for de novo drug design.  The authors propose a modified objective function with an intrinsic reward inspired bonus to encourage diversity between agents and also propose to use a constraint to keep the fine-tuned agents close to the pre-trained agents.  The authors evaluate their algorithm on Guacamol, by generating a number of inhibitors for SARS-CoV2 targets.  They also perform ablations on their method with the GSK3$\beta$ and JNK3 maximization tasks.","Some strengths of this paper are:

- An intrinsic reward like term added to the agents' loss which encourages diversity.
- The method performs favorably to the other methods compared against on Guacamol benchmark.
- It performs comprehensive ablations on the GSK3$\beta$ and JNK3 maximization tasks","My main complaint about this paper is that I am not convinced of the algorithm's superiority over rivaling methods based upon the experiments section.  I feel that the paper has both missed some necessary baseline methods and that the experiments as they stand are insufficient in demonstrating the paper's main claim that its method leads to improved diversity over other methods.  I also have concerns about missing related work in multi-agent RL in which there is already a body of literature on encouraging diversity among agents, as well as other competing methods which have been applied to molecular drug generation such as diffusion models and GFlowNets.  I also have concerns regarding this paper's reproducibility.  I will go over these concerns one by one.

## Experiments section
1. The main claim of this paper is that its approach leads to superior diversity, but I did not see any experiments _comparing_ the diversity of molecules it generates to molecules generated by other methods.  Indeed, there were experiments looking at the diversity of molecules generated by their method, but it was only their method.  There isn't an indication whether their main claim of improved diversity is true if there is no comparison to other methods.
2. While the generated molecules do look diverse to my non-chemist eye, I would like to see more generated molecules and critically some measurement of diversity of the generated molecules to be convinced.  Also, these results were on only one seed which is insufficient.  At minimum there should be three seeds, ideally quite a bit more (see [here](https://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html)).
3. There are missing baselines in the experiments.  The authors should have compared to an existing LLM molecule generation method such as MolGPT, but this was missing from their experiments.  There are other methods for encouraging diversity for molecule design with RL-inspired machinery such as GFlowNets (https://arxiv.org/abs/2106.04399) that should be compared against.  Also, it would have been nice to see a comparison with one of the recent works on diffusion for molecular design (e.g., https://arxiv.org/pdf/2203.17003.pdf, https://arxiv.org/pdf/2305.01140.pdf), or at minimum a compelling reason for why not to compare to these methods.
4. In the Guacamol experiments it seems experiments were run over one seed.  This is insufficient.
5. A more minor comment: it's hard to understand the significance of the Guacamol task when the tasks are ordered 1-20 without context of what the tasks actually are.

## Missing references
1. I mentioned in the last point, but it would have been nice to see some discussion of other methods such as GFlowNets or diffusion models which also try to encourage diversity in molecular design.
2. There is already a body of literature on encouraging diversity in multi-agent RL, but I did not see references to this literature.  Some representative papers may be https://arxiv.org/abs/2106.02195 and https://openreview.net/forum?id=H-6iczs__Ro.

## Reproducibility
1. All experiments seem to have been run with one seed.  There is no way to know if the results would hold with more seeds.
2. There is no listing of the hyperparameters used or the hyperparameter tuning methods used (or values tried if a grid search).
3. There is no (anonymized) submitted code available to verify or reproduce the authors' claims.

## Clarity issues
1. In the section explaining the loss function, the indexing used is rather confusing.  E.g., the authors use a loss $L_1$ which seems fixed, then also a loss $L_k$ which seems to index the different agents (so what about when $k=1$?).","1. In the loss function, why does the sum over agents only go up to $k-1$?  Shouldn't the second loss term be something like $\sum_{j=1}^n s(x) \left|P_k(x) - P_j(x)\right|$?  Why compare to only the agents before this index as the ordering seems arbitrary.
2. Did the authors consider using a method for encouraging diversity which already exists in the multi-agent RL literature?  If so, why did they not use it and did they run any comparisons?","The main and most important limitation of this work is that I do not know whether the proposed method actually is competitive with rival methods due to some missing experiments, baselines, and insufficient reproducibility.",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,2 fair,2 fair,1690314505964,,,NeurIPS.cc/2023/Conference,1B6YKnHYBb,"['NeurIPS.cc/2023/Conference/Submission7922/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411137374,[],3,,,['everyone'],1B6YKnHYBb,['NeurIPS.cc/2023/Conference/Submission7922/Reviewer_xd9X'],1690314505964,1702411137374,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7922/Reviewer_xd9X']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that they believe are relevant to the work and should be discussed or compared to, including:

* GFlowNets (https://arxiv.org/abs/2106.04399)
* Diffusion models for molecular design (e.g., https://arxiv.org/pdf/2203.17003.pdf, https://arxiv.org/pdf/2305.01140.pdf)
* Papers on encouraging diversity in multi-agent RL (e.g., https://arxiv.org/abs/2106.02195, https://openreview.net/forum?id=H-6iczs__Ro)
* MolGPT, an existing LLM molecule generation method.

The reviewer also asks questions about why the authors did not consider using existing methods for encouraging diversity from the multi-agent RL literature, and suggests that the authors should have compared their method to these existing methods.",1,"2021, 2022, 2023, 2021",Papers on encouraging diversity in multi-agent RL 
rCOLpCOlXO,"This paper proposes a novel multi-agent reinforcement learning algorithm with agents parameterized with a pre-trained GPT architecture for de novo drug design.  The authors propose a modified objective function with an intrinsic reward inspired bonus to encourage diversity between agents and also propose to use a constraint to keep the fine-tuned agents close to the pre-trained agents.  The authors evaluate their algorithm on Guacamol, by generating a number of inhibitors for SARS-CoV2 targets.  They also perform ablations on their method with the GSK3$\beta$ and JNK3 maximization tasks.","Some strengths of this paper are:

- An intrinsic reward like term added to the agents' loss which encourages diversity.
- The method performs favorably to the other methods compared against on Guacamol benchmark.
- It performs comprehensive ablations on the GSK3$\beta$ and JNK3 maximization tasks","My main complaint about this paper is that I am not convinced of the algorithm's superiority over rivaling methods based upon the experiments section.  I feel that the paper has both missed some necessary baseline methods and that the experiments as they stand are insufficient in demonstrating the paper's main claim that its method leads to improved diversity over other methods.  I also have concerns about missing related work in multi-agent RL in which there is already a body of literature on encouraging diversity among agents, as well as other competing methods which have been applied to molecular drug generation such as diffusion models and GFlowNets.  I also have concerns regarding this paper's reproducibility.  I will go over these concerns one by one.

## Experiments section
1. The main claim of this paper is that its approach leads to superior diversity, but I did not see any experiments _comparing_ the diversity of molecules it generates to molecules generated by other methods.  Indeed, there were experiments looking at the diversity of molecules generated by their method, but it was only their method.  There isn't an indication whether their main claim of improved diversity is true if there is no comparison to other methods.
2. While the generated molecules do look diverse to my non-chemist eye, I would like to see more generated molecules and critically some measurement of diversity of the generated molecules to be convinced.  Also, these results were on only one seed which is insufficient.  At minimum there should be three seeds, ideally quite a bit more (see [here](https://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html)).
3. There are missing baselines in the experiments.  The authors should have compared to an existing LLM molecule generation method such as MolGPT, but this was missing from their experiments.  There are other methods for encouraging diversity for molecule design with RL-inspired machinery such as GFlowNets (https://arxiv.org/abs/2106.04399) that should be compared against.  Also, it would have been nice to see a comparison with one of the recent works on diffusion for molecular design (e.g., https://arxiv.org/pdf/2203.17003.pdf, https://arxiv.org/pdf/2305.01140.pdf), or at minimum a compelling reason for why not to compare to these methods.
4. In the Guacamol experiments it seems experiments were run over one seed.  This is insufficient.
5. A more minor comment: it's hard to understand the significance of the Guacamol task when the tasks are ordered 1-20 without context of what the tasks actually are.

## Missing references
1. I mentioned in the last point, but it would have been nice to see some discussion of other methods such as GFlowNets or diffusion models which also try to encourage diversity in molecular design.
2. There is already a body of literature on encouraging diversity in multi-agent RL, but I did not see references to this literature.  Some representative papers may be https://arxiv.org/abs/2106.02195 and https://openreview.net/forum?id=H-6iczs__Ro.

## Reproducibility
1. All experiments seem to have been run with one seed.  There is no way to know if the results would hold with more seeds.
2. There is no listing of the hyperparameters used or the hyperparameter tuning methods used (or values tried if a grid search).
3. There is no (anonymized) submitted code available to verify or reproduce the authors' claims.

## Clarity issues
1. In the section explaining the loss function, the indexing used is rather confusing.  E.g., the authors use a loss $L_1$ which seems fixed, then also a loss $L_k$ which seems to index the different agents (so what about when $k=1$?).","1. In the loss function, why does the sum over agents only go up to $k-1$?  Shouldn't the second loss term be something like $\sum_{j=1}^n s(x) \left|P_k(x) - P_j(x)\right|$?  Why compare to only the agents before this index as the ordering seems arbitrary.
2. Did the authors consider using a method for encouraging diversity which already exists in the multi-agent RL literature?  If so, why did they not use it and did they run any comparisons?","The main and most important limitation of this work is that I do not know whether the proposed method actually is competitive with rival methods due to some missing experiments, baselines, and insufficient reproducibility.",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,2 fair,2 fair,1690314505964,,,NeurIPS.cc/2023/Conference,1B6YKnHYBb,"['NeurIPS.cc/2023/Conference/Submission7922/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411137374,[],3,,,['everyone'],1B6YKnHYBb,['NeurIPS.cc/2023/Conference/Submission7922/Reviewer_xd9X'],1690314505964,1702411137374,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7922/Reviewer_xd9X']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers that they believe are relevant to the work and should be discussed or compared to, including:

* GFlowNets (https://arxiv.org/abs/2106.04399)
* Diffusion models for molecular design (e.g., https://arxiv.org/pdf/2203.17003.pdf, https://arxiv.org/pdf/2305.01140.pdf)
* Papers on encouraging diversity in multi-agent RL (e.g., https://arxiv.org/abs/2106.02195, https://openreview.net/forum?id=H-6iczs__Ro)
* MolGPT, an existing LLM molecule generation method.

The reviewer also asks questions about why the authors did not consider using existing methods for encouraging diversity from the multi-agent RL literature, and suggests that the authors should have compared their method to these existing methods.",1,"2021, 2022, 2023, 2021",MolGPT
1aqiGXON3M,"This paper presents a method to transfer learned or planned goal-directed skills in domains to novel tasks represented by linear temporal logic within the same domain. The key idea of this paper is to train goal conditioned policies to achieve (and avoid) Boolean goals, and to compose them temporally to achieve temporal logic goals. 

The paper proposes to first convert the automaton corresponding to the LTL specification into a Buchi automaton, which is then converted into a directed graph with a target state. The algorithm also estimates the cost-to-go heuristic at each node to estimate edge traversal costs, and finally combines an optimal graph search along with learned goal conditioned policies to achieve the temporal logic goals. 

The authors primarily benchmark against LTL2Action where the specification is embedded into a feature vector using a graph neural network, and this embedded latent feature vector is used alongside a state-feature vector in a through a Deep-RL algorithm to compute the final policy. The authors demonstrate that their approach beats LTL2Action on range of randomly sampled tasks, and two specific avoidance tasks.  ","**Sound problem definition**: The authors are correct in their statement that with competent goal-conditioned policies available to the agent, the agent can solve temporal LTL tasks through composition of these policies. There has been a lot of recent interest in this approach, and the authors have demonstrated that in two navigational environments that have been utilized in research on RL + temporal goals. There are however some issues with the assumptions made by the authors as I describe in the following sections.

**Evaluations**: The authors might have chosen just two navigational domains, but have focused on evaluating over a wide range of temporal logic formulas. Such evaluations are much more valuable in the space of RL for temporal tasks as is considered in this paper.

**Originality and significance**: Prior work suffers from lack of transferrability to all novel tasks as the library of learned skills is inadequate to transfer to all possible tasks. The authors propose to pre-train a goal conditioned skills that should offer more coverage of the logical transition-space. The idea is well demonstrated in the zones and letters environment in the paper, but there are some additional concerns that I highlight in the next section.

Overall, the idea of composing pre-learned skills to novel task scenarios is not original in and of itself. But the combination presented in this paper is novel to the best of my knowledge. However, there are elements of similarity with prior work that have not been addressed adequately ","**Positioning in context of prior work**: The core idea of skill reuse is not entirely novel. There are prior works addressing this [1],[2],[3] that appear to be missing from discussion. Infact both these works handle a wider variety of logical composition for the transition edges which appear to not be considered by this paper. While the core idea in this paper is distinct, it deserves to be discussed in context of these works that appear to handle the problem with greater generality.

**Correctness concerns**: The authors claim that their approach is applicable to all $\omega$-regular automata. However their approach appears to have a strong reliance on a single unique accepting state, in all the examples that they test on. Generalized Buchi acceptance condition requires that the system visit atleast one state in each accepting set infinitely often, and the approach described here is incompatible with such a specification. An example of this would be the patrolling task $\square \diamond a \wedge \square \diamond b$. Here an accepting run would require the agent to visit both $a$, and $b$ infinitely often, but this would not be discoverable by the graph search algorithm described here.

A second correctness concern relates to the type of edge transitions that can be accomplished by the goal conditioned policy. In general an edge transition in automata is described by the self-transition edge that is maintained until the transition trigerring truth evaluation is reached. The goal conditioned policy implicitly assumes that the trigger transition is reached as the first distinct transition in the truth values of the propositions. This might be true for navigational tasks, where each state has at most one proposition true, but may not be true in general when not all propositions are controlled by the agent, or even in cases where simultaneous satisfaction of multiple propositions might be required. For instance the specification $\diamond(a \wedge b)$. [2] would appropriately identify this specification as unsatisfiable, and abort task execution, whereas the behavior of the system proposed in this paper is uncertain. In particular, none of the goal conditioned policies are applicable to this outcome. In contrast [1] can compose the policies logically to satisfy the specification if it is indeed satisfiable. If we study the type of transitions occuring within automata, then there are many such edge cases pertaining to self-transition, and simultaneous truth value changes that cannot be handled by this approach. These limitations must be explicitly acknowledged in the submission.

**Difficulty of training goal-conditioned policies**: This approach relies on having a good goal conditioned policy to perform the task. However, this is in general a challenging problem, and I am not aware of any works that have managed to train competent goal conditioned policies beyond grid world domains that achieve good coverage over all possible logical goals. I would appreciate if the authors add some text to evaluate the quality of goal conditioned policies before using them for this algorithm, or point to works that address this issue.

[1] - Nangue Tasse, G., James, S. and Rosman, B., 2020. A boolean task algebra for reinforcement learning. Advances in Neural Information Processing Systems, 33, pp.9497-9507.

[2] - Liu, J.X., Shah, A., Rosen, E., Konidaris, G. and Tellex, S., 2022. Skill transfer for temporally-extended task specifications. arXiv preprint arXiv:2206.05096.

[3] - Xu, D. and Fekri, F., 2022. Generalizing LTL Instructions via Future Dependent Options. arXiv preprint arXiv:2212.04576.","These questions pertain to the limitations of the approach proposed, please clarify if your approach can handle it, or update the limitations to include them:
1. Can this approach handle simultaneity? for example ensure goals $a$ and $b$ are true at the same time instant as in $\diamond (a \wedge b)$
2. Can this approach handle self transition that requires maintaining some proposition as true until a goal is reached? For example $\diamond b \wedge a U b$
3. Can the graph search be adopted for recurrence type formulas from Manna and Pnueli's temporal hierarchy [4] (TLDR https://spot.lre.epita.fr/hierarchy.html) $\square \diamond a \wedge \square \diamond b$
4. What are the implicit assumptions you are making on the environment for the validity of the proposed approach? Please clarify these explicitly

[4] - Manna, Z. and Pnueli, A., 1990, August. A hierarchy of temporal properties (invited paper, 1989). In Proceedings of the ninth annual ACM symposium on Principles of distributed computing (pp. 377-410).",I do not believe that the limitations are adequately identified and acknowledged. Please refer to the weakness and questions section.,['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,3 good,3 good,1688660084688,,,NeurIPS.cc/2023/Conference,19AgWnmyoV,"['NeurIPS.cc/2023/Conference/Submission14481/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411480683,[],3,,,['everyone'],19AgWnmyoV,['NeurIPS.cc/2023/Conference/Submission14481/Reviewer_Q7B2'],1688660084688,1702411480683,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14481/Reviewer_Q7B2']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions three specific papers ([1], [2], and [3]) that appear to be missing from the discussion and handle the problem with greater generality. The reviewer also mentions that the core idea of skill reuse is not entirely novel and that these prior works should be discussed in the context of the paper. 

Specifically, the reviewer suggests citing the following papers:
- Nangue Tasse, G., James, S. and Rosman, B., 2020. A boolean task algebra for reinforcement learning. Advances in Neural Information Processing Systems, 33, pp.9497-9507.
- Liu, J.X., Shah, A., Rosen, E., Konidaris, G. and Tellex, S., 2022. Skill transfer for temporally-extended task specifications. arXiv preprint arXiv:2206.05096.
- Xu, D. and Fekri, F., 2022. Generalizing LTL Instructions via Future Dependent Options. arXiv preprint arXiv:2212.04576.",1,"2020, 2022, 2022, 2022, 2022",A boolean task algebra for reinforcement learning 
1aqiGXON3M,"This paper presents a method to transfer learned or planned goal-directed skills in domains to novel tasks represented by linear temporal logic within the same domain. The key idea of this paper is to train goal conditioned policies to achieve (and avoid) Boolean goals, and to compose them temporally to achieve temporal logic goals. 

The paper proposes to first convert the automaton corresponding to the LTL specification into a Buchi automaton, which is then converted into a directed graph with a target state. The algorithm also estimates the cost-to-go heuristic at each node to estimate edge traversal costs, and finally combines an optimal graph search along with learned goal conditioned policies to achieve the temporal logic goals. 

The authors primarily benchmark against LTL2Action where the specification is embedded into a feature vector using a graph neural network, and this embedded latent feature vector is used alongside a state-feature vector in a through a Deep-RL algorithm to compute the final policy. The authors demonstrate that their approach beats LTL2Action on range of randomly sampled tasks, and two specific avoidance tasks.  ","**Sound problem definition**: The authors are correct in their statement that with competent goal-conditioned policies available to the agent, the agent can solve temporal LTL tasks through composition of these policies. There has been a lot of recent interest in this approach, and the authors have demonstrated that in two navigational environments that have been utilized in research on RL + temporal goals. There are however some issues with the assumptions made by the authors as I describe in the following sections.

**Evaluations**: The authors might have chosen just two navigational domains, but have focused on evaluating over a wide range of temporal logic formulas. Such evaluations are much more valuable in the space of RL for temporal tasks as is considered in this paper.

**Originality and significance**: Prior work suffers from lack of transferrability to all novel tasks as the library of learned skills is inadequate to transfer to all possible tasks. The authors propose to pre-train a goal conditioned skills that should offer more coverage of the logical transition-space. The idea is well demonstrated in the zones and letters environment in the paper, but there are some additional concerns that I highlight in the next section.

Overall, the idea of composing pre-learned skills to novel task scenarios is not original in and of itself. But the combination presented in this paper is novel to the best of my knowledge. However, there are elements of similarity with prior work that have not been addressed adequately ","**Positioning in context of prior work**: The core idea of skill reuse is not entirely novel. There are prior works addressing this [1],[2],[3] that appear to be missing from discussion. Infact both these works handle a wider variety of logical composition for the transition edges which appear to not be considered by this paper. While the core idea in this paper is distinct, it deserves to be discussed in context of these works that appear to handle the problem with greater generality.

**Correctness concerns**: The authors claim that their approach is applicable to all $\omega$-regular automata. However their approach appears to have a strong reliance on a single unique accepting state, in all the examples that they test on. Generalized Buchi acceptance condition requires that the system visit atleast one state in each accepting set infinitely often, and the approach described here is incompatible with such a specification. An example of this would be the patrolling task $\square \diamond a \wedge \square \diamond b$. Here an accepting run would require the agent to visit both $a$, and $b$ infinitely often, but this would not be discoverable by the graph search algorithm described here.

A second correctness concern relates to the type of edge transitions that can be accomplished by the goal conditioned policy. In general an edge transition in automata is described by the self-transition edge that is maintained until the transition trigerring truth evaluation is reached. The goal conditioned policy implicitly assumes that the trigger transition is reached as the first distinct transition in the truth values of the propositions. This might be true for navigational tasks, where each state has at most one proposition true, but may not be true in general when not all propositions are controlled by the agent, or even in cases where simultaneous satisfaction of multiple propositions might be required. For instance the specification $\diamond(a \wedge b)$. [2] would appropriately identify this specification as unsatisfiable, and abort task execution, whereas the behavior of the system proposed in this paper is uncertain. In particular, none of the goal conditioned policies are applicable to this outcome. In contrast [1] can compose the policies logically to satisfy the specification if it is indeed satisfiable. If we study the type of transitions occuring within automata, then there are many such edge cases pertaining to self-transition, and simultaneous truth value changes that cannot be handled by this approach. These limitations must be explicitly acknowledged in the submission.

**Difficulty of training goal-conditioned policies**: This approach relies on having a good goal conditioned policy to perform the task. However, this is in general a challenging problem, and I am not aware of any works that have managed to train competent goal conditioned policies beyond grid world domains that achieve good coverage over all possible logical goals. I would appreciate if the authors add some text to evaluate the quality of goal conditioned policies before using them for this algorithm, or point to works that address this issue.

[1] - Nangue Tasse, G., James, S. and Rosman, B., 2020. A boolean task algebra for reinforcement learning. Advances in Neural Information Processing Systems, 33, pp.9497-9507.

[2] - Liu, J.X., Shah, A., Rosen, E., Konidaris, G. and Tellex, S., 2022. Skill transfer for temporally-extended task specifications. arXiv preprint arXiv:2206.05096.

[3] - Xu, D. and Fekri, F., 2022. Generalizing LTL Instructions via Future Dependent Options. arXiv preprint arXiv:2212.04576.","These questions pertain to the limitations of the approach proposed, please clarify if your approach can handle it, or update the limitations to include them:
1. Can this approach handle simultaneity? for example ensure goals $a$ and $b$ are true at the same time instant as in $\diamond (a \wedge b)$
2. Can this approach handle self transition that requires maintaining some proposition as true until a goal is reached? For example $\diamond b \wedge a U b$
3. Can the graph search be adopted for recurrence type formulas from Manna and Pnueli's temporal hierarchy [4] (TLDR https://spot.lre.epita.fr/hierarchy.html) $\square \diamond a \wedge \square \diamond b$
4. What are the implicit assumptions you are making on the environment for the validity of the proposed approach? Please clarify these explicitly

[4] - Manna, Z. and Pnueli, A., 1990, August. A hierarchy of temporal properties (invited paper, 1989). In Proceedings of the ninth annual ACM symposium on Principles of distributed computing (pp. 377-410).",I do not believe that the limitations are adequately identified and acknowledged. Please refer to the weakness and questions section.,['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,3 good,3 good,1688660084688,,,NeurIPS.cc/2023/Conference,19AgWnmyoV,"['NeurIPS.cc/2023/Conference/Submission14481/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411480683,[],3,,,['everyone'],19AgWnmyoV,['NeurIPS.cc/2023/Conference/Submission14481/Reviewer_Q7B2'],1688660084688,1702411480683,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14481/Reviewer_Q7B2']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions three specific papers ([1], [2], and [3]) that appear to be missing from the discussion and handle the problem with greater generality. The reviewer also mentions that the core idea of skill reuse is not entirely novel and that these prior works should be discussed in the context of the paper. 

Specifically, the reviewer suggests citing the following papers:
- Nangue Tasse, G., James, S. and Rosman, B., 2020. A boolean task algebra for reinforcement learning. Advances in Neural Information Processing Systems, 33, pp.9497-9507.
- Liu, J.X., Shah, A., Rosen, E., Konidaris, G. and Tellex, S., 2022. Skill transfer for temporally-extended task specifications. arXiv preprint arXiv:2206.05096.
- Xu, D. and Fekri, F., 2022. Generalizing LTL Instructions via Future Dependent Options. arXiv preprint arXiv:2212.04576.",1,"2020, 2022, 2022, 2022, 2022",Skill transfer for temporally-extended task specifications 
1aqiGXON3M,"This paper presents a method to transfer learned or planned goal-directed skills in domains to novel tasks represented by linear temporal logic within the same domain. The key idea of this paper is to train goal conditioned policies to achieve (and avoid) Boolean goals, and to compose them temporally to achieve temporal logic goals. 

The paper proposes to first convert the automaton corresponding to the LTL specification into a Buchi automaton, which is then converted into a directed graph with a target state. The algorithm also estimates the cost-to-go heuristic at each node to estimate edge traversal costs, and finally combines an optimal graph search along with learned goal conditioned policies to achieve the temporal logic goals. 

The authors primarily benchmark against LTL2Action where the specification is embedded into a feature vector using a graph neural network, and this embedded latent feature vector is used alongside a state-feature vector in a through a Deep-RL algorithm to compute the final policy. The authors demonstrate that their approach beats LTL2Action on range of randomly sampled tasks, and two specific avoidance tasks.  ","**Sound problem definition**: The authors are correct in their statement that with competent goal-conditioned policies available to the agent, the agent can solve temporal LTL tasks through composition of these policies. There has been a lot of recent interest in this approach, and the authors have demonstrated that in two navigational environments that have been utilized in research on RL + temporal goals. There are however some issues with the assumptions made by the authors as I describe in the following sections.

**Evaluations**: The authors might have chosen just two navigational domains, but have focused on evaluating over a wide range of temporal logic formulas. Such evaluations are much more valuable in the space of RL for temporal tasks as is considered in this paper.

**Originality and significance**: Prior work suffers from lack of transferrability to all novel tasks as the library of learned skills is inadequate to transfer to all possible tasks. The authors propose to pre-train a goal conditioned skills that should offer more coverage of the logical transition-space. The idea is well demonstrated in the zones and letters environment in the paper, but there are some additional concerns that I highlight in the next section.

Overall, the idea of composing pre-learned skills to novel task scenarios is not original in and of itself. But the combination presented in this paper is novel to the best of my knowledge. However, there are elements of similarity with prior work that have not been addressed adequately ","**Positioning in context of prior work**: The core idea of skill reuse is not entirely novel. There are prior works addressing this [1],[2],[3] that appear to be missing from discussion. Infact both these works handle a wider variety of logical composition for the transition edges which appear to not be considered by this paper. While the core idea in this paper is distinct, it deserves to be discussed in context of these works that appear to handle the problem with greater generality.

**Correctness concerns**: The authors claim that their approach is applicable to all $\omega$-regular automata. However their approach appears to have a strong reliance on a single unique accepting state, in all the examples that they test on. Generalized Buchi acceptance condition requires that the system visit atleast one state in each accepting set infinitely often, and the approach described here is incompatible with such a specification. An example of this would be the patrolling task $\square \diamond a \wedge \square \diamond b$. Here an accepting run would require the agent to visit both $a$, and $b$ infinitely often, but this would not be discoverable by the graph search algorithm described here.

A second correctness concern relates to the type of edge transitions that can be accomplished by the goal conditioned policy. In general an edge transition in automata is described by the self-transition edge that is maintained until the transition trigerring truth evaluation is reached. The goal conditioned policy implicitly assumes that the trigger transition is reached as the first distinct transition in the truth values of the propositions. This might be true for navigational tasks, where each state has at most one proposition true, but may not be true in general when not all propositions are controlled by the agent, or even in cases where simultaneous satisfaction of multiple propositions might be required. For instance the specification $\diamond(a \wedge b)$. [2] would appropriately identify this specification as unsatisfiable, and abort task execution, whereas the behavior of the system proposed in this paper is uncertain. In particular, none of the goal conditioned policies are applicable to this outcome. In contrast [1] can compose the policies logically to satisfy the specification if it is indeed satisfiable. If we study the type of transitions occuring within automata, then there are many such edge cases pertaining to self-transition, and simultaneous truth value changes that cannot be handled by this approach. These limitations must be explicitly acknowledged in the submission.

**Difficulty of training goal-conditioned policies**: This approach relies on having a good goal conditioned policy to perform the task. However, this is in general a challenging problem, and I am not aware of any works that have managed to train competent goal conditioned policies beyond grid world domains that achieve good coverage over all possible logical goals. I would appreciate if the authors add some text to evaluate the quality of goal conditioned policies before using them for this algorithm, or point to works that address this issue.

[1] - Nangue Tasse, G., James, S. and Rosman, B., 2020. A boolean task algebra for reinforcement learning. Advances in Neural Information Processing Systems, 33, pp.9497-9507.

[2] - Liu, J.X., Shah, A., Rosen, E., Konidaris, G. and Tellex, S., 2022. Skill transfer for temporally-extended task specifications. arXiv preprint arXiv:2206.05096.

[3] - Xu, D. and Fekri, F., 2022. Generalizing LTL Instructions via Future Dependent Options. arXiv preprint arXiv:2212.04576.","These questions pertain to the limitations of the approach proposed, please clarify if your approach can handle it, or update the limitations to include them:
1. Can this approach handle simultaneity? for example ensure goals $a$ and $b$ are true at the same time instant as in $\diamond (a \wedge b)$
2. Can this approach handle self transition that requires maintaining some proposition as true until a goal is reached? For example $\diamond b \wedge a U b$
3. Can the graph search be adopted for recurrence type formulas from Manna and Pnueli's temporal hierarchy [4] (TLDR https://spot.lre.epita.fr/hierarchy.html) $\square \diamond a \wedge \square \diamond b$
4. What are the implicit assumptions you are making on the environment for the validity of the proposed approach? Please clarify these explicitly

[4] - Manna, Z. and Pnueli, A., 1990, August. A hierarchy of temporal properties (invited paper, 1989). In Proceedings of the ninth annual ACM symposium on Principles of distributed computing (pp. 377-410).",I do not believe that the limitations are adequately identified and acknowledged. Please refer to the weakness and questions section.,['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,3 good,3 good,1688660084688,,,NeurIPS.cc/2023/Conference,19AgWnmyoV,"['NeurIPS.cc/2023/Conference/Submission14481/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411480683,[],3,,,['everyone'],19AgWnmyoV,['NeurIPS.cc/2023/Conference/Submission14481/Reviewer_Q7B2'],1688660084688,1702411480683,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14481/Reviewer_Q7B2']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions three specific papers ([1], [2], and [3]) that appear to be missing from the discussion and handle the problem with greater generality. The reviewer also mentions that the core idea of skill reuse is not entirely novel and that these prior works should be discussed in the context of the paper. 

Specifically, the reviewer suggests citing the following papers:
- Nangue Tasse, G., James, S. and Rosman, B., 2020. A boolean task algebra for reinforcement learning. Advances in Neural Information Processing Systems, 33, pp.9497-9507.
- Liu, J.X., Shah, A., Rosen, E., Konidaris, G. and Tellex, S., 2022. Skill transfer for temporally-extended task specifications. arXiv preprint arXiv:2206.05096.
- Xu, D. and Fekri, F., 2022. Generalizing LTL Instructions via Future Dependent Options. arXiv preprint arXiv:2212.04576.",1,"2020, 2022, 2022, 2022, 2022",Generalizing LTL Instructions via Future Dependent Options
0Ix1cpxwi2,"This paper considers the problem of learning to solve a linear temporal logic
(LTL) tasks in a Markov Decision Process (MDP). Given a fixed Markov Decision
Process, this is done by:

1. Pre-training a goal-conditioned policy to solve a uniform sampling of reach-avoid tasks,
   where goals correspond to atomic propositions.
1. The input LTL sentence is translated into a Buchi automata.
1. The Buchi automata is transformed into a weighted graph.
   - Weights are determined using the value function of the pre-trained policy.
1. A path is generated by solving a sequence of shortest path problems.

The approach is then experimentally validated against the LTL2Action method using 
the prior works domain and concept class.


---- update ----

After re-reading and being pointed to Appendix section D, it seems my major concerns are accounted for. What remains is the question of how to incorporate this into the main text. As such I am increasing my score to erring toward accept.","The approach tackles an important problem. Namely, learning to solve sparse
tasks represented in a formal specification language defined over infinite runs
of the system.","1. The proposed approach is ultimately heuristic and is susceptible to being
   ""catastrophically myopic."" In particular, the greedy sequence of shortest
   path problems is necessarily biased toward solutions that work for finite
   horizons, but says nothing about the infinite time behavior, e.g., the ""lassos""
   generated by sequence of shortest path queries. It is not hard to imagine
   constructing an adversarial Buchi automata that uses a sequence of easy
   to reach accepting states to lead the agent into a long term bad position.

1. The paper claims to address infinite horizon specifications, but then
   compares against a regular language benchmark. This undercuts the stated
   motivation. For example, all of the base-line problems have a finite
   accepting prefix, e.g., as opposed to G(x -> F y).

1. The approach should be compared to hierarchical, meta RL, and compositional
   RL works. For example, the graph approach seems very similar to [1] but adapted
   to goal conditioned policies.
   
[1] Jothimurugan, Kishor, Rajeev Alur, and Osbert Bastani. ""A composable specification language for reinforcement learning tasks."" Advances in Neural Information Processing Systems 32 (2019).","1. How does the system generalize to infinite horizon queries?
1. When is the proposed heuristic guaranteed to work vs have arbitrary failures.",See weakness 1.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,1 poor,2 fair,2 fair,1688706547951,,,NeurIPS.cc/2023/Conference,19AgWnmyoV,"['NeurIPS.cc/2023/Conference/Submission14481/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411480545,[],4,,,['everyone'],19AgWnmyoV,['NeurIPS.cc/2023/Conference/Submission14481/Reviewer_VFCf'],1688706547951,1702411480545,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission14481/Reviewer_VFCf']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

* Jothimurugan, Kishor, Rajeev Alur, and Osbert Bastani. ""A composable specification language for reinforcement learning tasks."" Advances in Neural Information Processing Systems 32 (2019).

The reviewer mentions that the graph approach in the paper seems very similar to this work, but adapted to goal-conditioned policies, implying that the authors should consider citing this reference to acknowledge the similarity and potentially build upon or distinguish their work from it.",1,2019,A composable specification language for reinforcement learning tasks
iNkSZln4rW,"The paper presents a novel framework, CaST, designed to address the challenges of temporal Out-Of-Distribution (OOD) issues and modeling the underlying dynamic spatial causation in Spatio-Temporal Graph (STG) forecasting. The authors construct a Structural Causal Model (SCM) to uncover the causal mechanisms of STG data, inspiring them to employ back-door and front-door adjustments to mitigate the confounding bias induced by temporal environment and spatial context, respectively. Moreover, they utilize the Hodge-Laplacian (HL) operator for edge-level convolution to capture the ripple effect of causation along space and time. Empirical results from experiments on three real-world datasets demonstrate that CaST outperforms existing methods while maintaining interpretability.
","1. The authors provide an in-depth analysis of the STG forecasting problem, revealing the underlying generation mechanisms of STG data and identifying the key sources of spatio-temporal distribution shift through the lens of causality.
2. The authors offer an interpretation of the specialties of temporal and spatial confounders (i.e., temporal environment and spatial contexts), making the application of backdoor and frontdoor adjustments in CaST targeted and well-motivated.
3. Extensive experiments validate the model's superiority, including comparisons against state-of-the-art STGNNs, ablation studies, hyperparameter sensitivity analysis, and case studies with detailed visualizations of the model's ability to capture spatial causality and identify temporal environments.
4. The figures provide readers with an intuitive understanding of the main problem, key ideas, and compelling experimental results.","1. The concept of temporal disentanglement is not entirely novel. Although the specific designs of disentanglement headers for environment feature and entity feature may have some novelty, as shown in Figure 4(a), the authors did not adequately clarify this in the main content. 

2. For the backdoor adjustment and corresponding temporal disentanglement module: a) The paper lacks a clear explanation regarding the connection between estimating $p(Y|do(X))$ via backdoor adjustment and temporal disentanglement, raising doubts about the effectiveness of the temporal disentanglement module. b) The proposed mutual information regularization cannot ensure that no causal features are leaked into the disentangled environment features $H_e$, possibly leading to the loss of essential causal information.

3. For the frontdoor adjustment and corresponding spatial context filtering module: a) The interpretation of the mediator variable $\hat{H}_i$ is confusing. The authors believe that $\hat{H}_i$ should be ""a node representation containing only information propagated based on genuine causation within their spatial context"". However, the spatial context $C$ is a confounder, and a mediator containing information from the confounder cannot satisfy the frontdoor criterion due to a backdoor path from the mediator to the label $Y,$ i.e., $\hat{H}_i \leftarrow C \rightarrow Y.$ b) The paper lacks a sufficient explanation regarding the connection between estimating $p(Y|do(X))$ via frontdoor adjustment and spatial context filtering, raising doubts about the effectiveness of the spatial context filtering. c) The reviewer fails to find an explanation for why using the HL operator on edge-graph can learn causation while other GNNs like GAT cannot.

4. The reference [1] addressed the spatio-temporal distribution shift in dynamic graphs. Since the spatio-temporal graph is a special type of dynamic graph, the authors should compare CaST with the approach described in [1].

5. The ablation studies are insufficient. Validating the fact that **w/o Env** and **w/o Ent** can cause performance degradation is unnecessary, as it is a natural result of eliminating information predictive of the label. Additionally, the ablation studies on loss functions corresponding to the two disentanglers are missing.

6. The visualization of dynamic spatial causal relationships in section 5.2 is insufficient since the authors do not check whether other SOTA STGNNs can also capture these relationships.

7. The model's sensitivity to the hyperparameters $\alpha$ and $\beta$, which control the importance of different terms in the final loss function, should be discussed.

[1] Zeyang Zhang, et al. Dynamic Graph Neural Networks Under Spatio-Temporal Distribution Shift. In NIPS 2022.","1. Why do the authors assume that temporal environment $E$ and spatial context $C$ are independent? For example, the states of neighboring nodes of node i will also be influenced by the change of $E$.
2. How do the proposed modules output statistical estimands that are required to calculate $p(Y|do(X))$ in Eqn. (1) and (2)?
3. Why do the authors focus on learning $p(Y|do(X))$ rather than $p(Y|do(X), E, C)?$ Although $E$ and $C$ are confounders, they can also provide additional information for accurate prediction.
4. In ablation studies, why **w/o Edge**, i.e., replacing 'GCN with causal scores' with GCN, causes such a performance drop in PEMS08? The STGCN also adopts GCN for spatial message passing, but it achieves MAE 18.60 in Table 1.",see weakness,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1688394101386,,,NeurIPS.cc/2023/Conference,17Zkztjlgt,"['NeurIPS.cc/2023/Conference/Submission1432/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410786637,[],2,,,['everyone'],17Zkztjlgt,['NeurIPS.cc/2023/Conference/Submission1432/Reviewer_kwLE'],1688394101386,1702410786637,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1432/Reviewer_kwLE']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, which is:

[1] Zeyang Zhang, et al. Dynamic Graph Neural Networks Under Spatio-Temporal Distribution Shift. In NIPS 2022.

The reviewer mentions that since the spatio-temporal graph is a special type of dynamic graph, the authors should compare CaST with the approach described in [1].",1,2022,Dynamic Graph Neural Networks Under Spatio-Temporal Distribution Shift
5bVbcAzqSh,"This paper studies the problem of Spatio-Temporal Graph forecasting under the lens of causal treatments. The authors proposed a framework consisting of two major components to mitigate the commonly seen limitations for spatial-temporal GNNs, i.e., 1) the backdoor environment disentanglement block, which models the temporal environmental changes, and 2) the Hodge Laplacian deconfounder capturing the spatial context. The proposed framework is applied in the air and traffic flow datasets to study spatial-temporal interactions. ","1. [Originality/Significance] The framework is proposed to deal with the limitation of the current STGNN models, i.e., the out of distribution issue and dynamic spatial causation. This paper’s originality comes from a novel combination of causal back-door treatment for temporal components, and a Hodge Laplacian decoupling layer for the spatial contexts. 
1. [Quality] The overall experiments presented in this paper for supporting the proposed CaST pipeline are quite nice. It contains different aspects such as ablation study on the core components of the model, which can provide deep insight into the model. 
1. [Clarity] The paper is well-written, with detailed descriptions starting from the background, STG data generation causal graph, causal treatment formulation, to each building block in the CaST pipeline. 
","1. From Appendix D (L646), it looks like the Hodge Laplacian used here is the down Hodge Laplacian $\partial_1^\top\partial_1$, rather than the full Laplacian (because $\partial_2$ is set to zero here). I would suggest mentioning this in the main text, because this is a big assumption to make. Additionally, there is some existing work using Hodge Laplacian on graphs (e.g., [A]), I would suggest citing this paper for completeness. 
1. Related to #1, after applying the edge filter you created, the filtered edge signal will be divergence free [B,C] (due to the low frequency/null space of the down Laplacian being the curl or harmonic flows). This implies that any gradient edge signal (i.e., a flow $x\in\mathbb R^{|E|}$ that can be expressed as $x = \partial_1 y$ for some node function $y \in \mathbb R^{|V|}$) will very likely be filtered out after the convolutional layer, suggesting that the HL deconfounder can learn more information when the flow is divergence free (compared with a curl-free flow). Given that the traffic flow is incompressible (thus divergence free) while air (PM2.5) flow is not, it might also suggest why we see huge drop in performance for the ablation study (Figure 5a) when removing edge signal on the PEM508 (traffic flow) dataset, compared with the AIR_BJ (air flow). I would suggest discussing this assumption/limitation in more detail in the main text. 

---
[A] Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. “Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian.” SIAM Review 62, no. 2 (2020): 353–91.

[B] Chen, Yu-Chia, Marina Meilă, and Ioannis G. Kevrekidis. “Helmholtzian Eigenmap: Topological Feature Discovery & Edge Flow Learning from Point Cloud Data.” ArXiv:2103.07626 [Stat.ML], March 13, 2021. https://arxiv.org/abs/2103.07626v1.

[C] Schaub, M. T., and S. Segarra. “Flow Smoothing And Denoising: Graph Signal Processing In The Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39, 2018. https://doi.org/10.1109/GlobalSIP.2018.8646701.
","1. Related to the weakness #1, have you tried/considered constructing a $\partial_2$ from e.g., a clique-complex (filling all triangles in the edges), and see if there is performance gain by using the full up and down Hodge Laplacian?
1. In L275, is there any specific reason to regularize only the scaling $\beta$ of $\mathcal L_{mi}$ rather than $\mathcal L_{cod}$? I understand that there is a regularization parameter $\alpha$ in Eq. (6), but how do you make sure the first term of $\mathcal L_{cod}$ is comparable with the log-probability ($\mathcal L_{pre}$)?
1. In L745-750 of Section G, the authors mentioned the edge filter being computationally intensive. If there is space, I would like to see an empirical comparison on the runtime.
1. How should I correctly understand the removal of the Environment codebook in the ablation study (Figure 5a)? Specifically, it seems like the gain for adding Env feature is not huge, does it mean that the environment disentangler is not disentangle the entity from environment well enough (so that some “environmental features” are still presented in the entity features)?
","The authors have discussed extensively the limitations as well as the social impact in the Appendix, therefore, I do not have any other aspects to add. While I appreciate the detailed discussions in Section G, I believe that it will improve the paper even more if the authors can at least have a brief overview (3-5 sentences) of the limitation/social impact in the main text. 
",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",,Yes,3 good,3 good,3 good,1688685095267,,,NeurIPS.cc/2023/Conference,17Zkztjlgt,"['NeurIPS.cc/2023/Conference/Submission1432/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410786473,[],4,,,['everyone'],17Zkztjlgt,['NeurIPS.cc/2023/Conference/Submission1432/Reviewer_o65E'],1688685095267,1702410786473,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1432/Reviewer_o65E']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions three papers that they think the authors should cite for completeness:

1. Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. “Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian.” SIAM Review 62, no. 2 (2020): 353–91.
2. Chen, Yu-Chia, Marina Meilă, and Ioannis G. Kevrekidis. “Helmholtzian Eigenmap: Topological Feature Discovery & Edge Flow Learning from Point Cloud Data.” ArXiv:2103.07626 [Stat.ML], March 13, 2021.
3. Schaub, M. T., and S. Segarra. “Flow Smoothing And Denoising: Graph Signal Processing In The Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39, 2018.

The reviewer suggests that citing these papers would provide more context and background information for the use of Hodge Laplacian in the paper.",1,"2020, 2021, 2018, 2018",Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian 
5bVbcAzqSh,"This paper studies the problem of Spatio-Temporal Graph forecasting under the lens of causal treatments. The authors proposed a framework consisting of two major components to mitigate the commonly seen limitations for spatial-temporal GNNs, i.e., 1) the backdoor environment disentanglement block, which models the temporal environmental changes, and 2) the Hodge Laplacian deconfounder capturing the spatial context. The proposed framework is applied in the air and traffic flow datasets to study spatial-temporal interactions. ","1. [Originality/Significance] The framework is proposed to deal with the limitation of the current STGNN models, i.e., the out of distribution issue and dynamic spatial causation. This paper’s originality comes from a novel combination of causal back-door treatment for temporal components, and a Hodge Laplacian decoupling layer for the spatial contexts. 
1. [Quality] The overall experiments presented in this paper for supporting the proposed CaST pipeline are quite nice. It contains different aspects such as ablation study on the core components of the model, which can provide deep insight into the model. 
1. [Clarity] The paper is well-written, with detailed descriptions starting from the background, STG data generation causal graph, causal treatment formulation, to each building block in the CaST pipeline. 
","1. From Appendix D (L646), it looks like the Hodge Laplacian used here is the down Hodge Laplacian $\partial_1^\top\partial_1$, rather than the full Laplacian (because $\partial_2$ is set to zero here). I would suggest mentioning this in the main text, because this is a big assumption to make. Additionally, there is some existing work using Hodge Laplacian on graphs (e.g., [A]), I would suggest citing this paper for completeness. 
1. Related to #1, after applying the edge filter you created, the filtered edge signal will be divergence free [B,C] (due to the low frequency/null space of the down Laplacian being the curl or harmonic flows). This implies that any gradient edge signal (i.e., a flow $x\in\mathbb R^{|E|}$ that can be expressed as $x = \partial_1 y$ for some node function $y \in \mathbb R^{|V|}$) will very likely be filtered out after the convolutional layer, suggesting that the HL deconfounder can learn more information when the flow is divergence free (compared with a curl-free flow). Given that the traffic flow is incompressible (thus divergence free) while air (PM2.5) flow is not, it might also suggest why we see huge drop in performance for the ablation study (Figure 5a) when removing edge signal on the PEM508 (traffic flow) dataset, compared with the AIR_BJ (air flow). I would suggest discussing this assumption/limitation in more detail in the main text. 

---
[A] Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. “Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian.” SIAM Review 62, no. 2 (2020): 353–91.

[B] Chen, Yu-Chia, Marina Meilă, and Ioannis G. Kevrekidis. “Helmholtzian Eigenmap: Topological Feature Discovery & Edge Flow Learning from Point Cloud Data.” ArXiv:2103.07626 [Stat.ML], March 13, 2021. https://arxiv.org/abs/2103.07626v1.

[C] Schaub, M. T., and S. Segarra. “Flow Smoothing And Denoising: Graph Signal Processing In The Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39, 2018. https://doi.org/10.1109/GlobalSIP.2018.8646701.
","1. Related to the weakness #1, have you tried/considered constructing a $\partial_2$ from e.g., a clique-complex (filling all triangles in the edges), and see if there is performance gain by using the full up and down Hodge Laplacian?
1. In L275, is there any specific reason to regularize only the scaling $\beta$ of $\mathcal L_{mi}$ rather than $\mathcal L_{cod}$? I understand that there is a regularization parameter $\alpha$ in Eq. (6), but how do you make sure the first term of $\mathcal L_{cod}$ is comparable with the log-probability ($\mathcal L_{pre}$)?
1. In L745-750 of Section G, the authors mentioned the edge filter being computationally intensive. If there is space, I would like to see an empirical comparison on the runtime.
1. How should I correctly understand the removal of the Environment codebook in the ablation study (Figure 5a)? Specifically, it seems like the gain for adding Env feature is not huge, does it mean that the environment disentangler is not disentangle the entity from environment well enough (so that some “environmental features” are still presented in the entity features)?
","The authors have discussed extensively the limitations as well as the social impact in the Appendix, therefore, I do not have any other aspects to add. While I appreciate the detailed discussions in Section G, I believe that it will improve the paper even more if the authors can at least have a brief overview (3-5 sentences) of the limitation/social impact in the main text. 
",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",,Yes,3 good,3 good,3 good,1688685095267,,,NeurIPS.cc/2023/Conference,17Zkztjlgt,"['NeurIPS.cc/2023/Conference/Submission1432/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410786473,[],4,,,['everyone'],17Zkztjlgt,['NeurIPS.cc/2023/Conference/Submission1432/Reviewer_o65E'],1688685095267,1702410786473,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1432/Reviewer_o65E']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions three papers that they think the authors should cite for completeness:

1. Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. “Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian.” SIAM Review 62, no. 2 (2020): 353–91.
2. Chen, Yu-Chia, Marina Meilă, and Ioannis G. Kevrekidis. “Helmholtzian Eigenmap: Topological Feature Discovery & Edge Flow Learning from Point Cloud Data.” ArXiv:2103.07626 [Stat.ML], March 13, 2021.
3. Schaub, M. T., and S. Segarra. “Flow Smoothing And Denoising: Graph Signal Processing In The Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39, 2018.

The reviewer suggests that citing these papers would provide more context and background information for the use of Hodge Laplacian in the paper.",1,"2020, 2021, 2018, 2018",Helmholtzian Eigenmap Topological Feature Discovery & Edge Flow Learning from Point Cloud Data 
5bVbcAzqSh,"This paper studies the problem of Spatio-Temporal Graph forecasting under the lens of causal treatments. The authors proposed a framework consisting of two major components to mitigate the commonly seen limitations for spatial-temporal GNNs, i.e., 1) the backdoor environment disentanglement block, which models the temporal environmental changes, and 2) the Hodge Laplacian deconfounder capturing the spatial context. The proposed framework is applied in the air and traffic flow datasets to study spatial-temporal interactions. ","1. [Originality/Significance] The framework is proposed to deal with the limitation of the current STGNN models, i.e., the out of distribution issue and dynamic spatial causation. This paper’s originality comes from a novel combination of causal back-door treatment for temporal components, and a Hodge Laplacian decoupling layer for the spatial contexts. 
1. [Quality] The overall experiments presented in this paper for supporting the proposed CaST pipeline are quite nice. It contains different aspects such as ablation study on the core components of the model, which can provide deep insight into the model. 
1. [Clarity] The paper is well-written, with detailed descriptions starting from the background, STG data generation causal graph, causal treatment formulation, to each building block in the CaST pipeline. 
","1. From Appendix D (L646), it looks like the Hodge Laplacian used here is the down Hodge Laplacian $\partial_1^\top\partial_1$, rather than the full Laplacian (because $\partial_2$ is set to zero here). I would suggest mentioning this in the main text, because this is a big assumption to make. Additionally, there is some existing work using Hodge Laplacian on graphs (e.g., [A]), I would suggest citing this paper for completeness. 
1. Related to #1, after applying the edge filter you created, the filtered edge signal will be divergence free [B,C] (due to the low frequency/null space of the down Laplacian being the curl or harmonic flows). This implies that any gradient edge signal (i.e., a flow $x\in\mathbb R^{|E|}$ that can be expressed as $x = \partial_1 y$ for some node function $y \in \mathbb R^{|V|}$) will very likely be filtered out after the convolutional layer, suggesting that the HL deconfounder can learn more information when the flow is divergence free (compared with a curl-free flow). Given that the traffic flow is incompressible (thus divergence free) while air (PM2.5) flow is not, it might also suggest why we see huge drop in performance for the ablation study (Figure 5a) when removing edge signal on the PEM508 (traffic flow) dataset, compared with the AIR_BJ (air flow). I would suggest discussing this assumption/limitation in more detail in the main text. 

---
[A] Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. “Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian.” SIAM Review 62, no. 2 (2020): 353–91.

[B] Chen, Yu-Chia, Marina Meilă, and Ioannis G. Kevrekidis. “Helmholtzian Eigenmap: Topological Feature Discovery & Edge Flow Learning from Point Cloud Data.” ArXiv:2103.07626 [Stat.ML], March 13, 2021. https://arxiv.org/abs/2103.07626v1.

[C] Schaub, M. T., and S. Segarra. “Flow Smoothing And Denoising: Graph Signal Processing In The Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39, 2018. https://doi.org/10.1109/GlobalSIP.2018.8646701.
","1. Related to the weakness #1, have you tried/considered constructing a $\partial_2$ from e.g., a clique-complex (filling all triangles in the edges), and see if there is performance gain by using the full up and down Hodge Laplacian?
1. In L275, is there any specific reason to regularize only the scaling $\beta$ of $\mathcal L_{mi}$ rather than $\mathcal L_{cod}$? I understand that there is a regularization parameter $\alpha$ in Eq. (6), but how do you make sure the first term of $\mathcal L_{cod}$ is comparable with the log-probability ($\mathcal L_{pre}$)?
1. In L745-750 of Section G, the authors mentioned the edge filter being computationally intensive. If there is space, I would like to see an empirical comparison on the runtime.
1. How should I correctly understand the removal of the Environment codebook in the ablation study (Figure 5a)? Specifically, it seems like the gain for adding Env feature is not huge, does it mean that the environment disentangler is not disentangle the entity from environment well enough (so that some “environmental features” are still presented in the entity features)?
","The authors have discussed extensively the limitations as well as the social impact in the Appendix, therefore, I do not have any other aspects to add. While I appreciate the detailed discussions in Section G, I believe that it will improve the paper even more if the authors can at least have a brief overview (3-5 sentences) of the limitation/social impact in the main text. 
",['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",,Yes,3 good,3 good,3 good,1688685095267,,,NeurIPS.cc/2023/Conference,17Zkztjlgt,"['NeurIPS.cc/2023/Conference/Submission1432/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410786473,[],4,,,['everyone'],17Zkztjlgt,['NeurIPS.cc/2023/Conference/Submission1432/Reviewer_o65E'],1688685095267,1702410786473,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1432/Reviewer_o65E']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions three papers that they think the authors should cite for completeness:

1. Schaub, Michael T., Austin R. Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. “Random Walks on Simplicial Complexes and the Normalized Hodge 1-Laplacian.” SIAM Review 62, no. 2 (2020): 353–91.
2. Chen, Yu-Chia, Marina Meilă, and Ioannis G. Kevrekidis. “Helmholtzian Eigenmap: Topological Feature Discovery & Edge Flow Learning from Point Cloud Data.” ArXiv:2103.07626 [Stat.ML], March 13, 2021.
3. Schaub, M. T., and S. Segarra. “Flow Smoothing And Denoising: Graph Signal Processing In The Edge-Space.” In 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 735–39, 2018.

The reviewer suggests that citing these papers would provide more context and background information for the use of Hodge Laplacian in the paper.",1,"2020, 2021, 2018, 2018",Flow Smoothing And Denoising Graph Signal Processing In The Edge-Space
Fb6X63mkYc,"This work performs a theoretical investigation of Gaussian SVGD, a special case of SVGD restricted to the submanifold of Gaussian densities by means of a bilinear kernel. The authors characterize the mean-field dynamics of Gaussian SVGD, with a particular focus on Gaussian targets and obtain finite-particle guarantees in certain settings. Moving beyond Gaussian targets, the authors show that the dynamics of mean-field Gaussian SVGD converges to the best Gaussian approximation (In KL divergence) to the target density, and obtain convergence rates for the mean-field dynamics with the target density is log-smooth and log-concave",The work provides a thorough treatment of the mean-field dynamics of Gaussian SVGD. The experimental evaluation is also satisfactory. ,"My primary concerns are as follows :

1. Most of the results presented in this work consider continuous-time mean-field dynamics under the assumption of a Bilinear kernel and Gaussian target density. In my opinion, these assumptions adversely impact the significance of these results, as they fail to explain one of the major appeals of SVGD, i.e., the fact that it provides a non-parametric approximation to a large class of target densities. (e.g. [1] shows discrete-time mean-field guarantees for subgaussian densities and [2] extends it to the class of densities satisfying a generalized Talagrand’s inequality). While the exponentially fast convergence guarantees for the mean-field dynamics are seemingly appealing, they fall short of providing a satisfactory explanation of the behavior of SVGD in practically relevant settings. 


2. Despite the simplifying assumption of Gaussian targets and bilinear kernels, the work does not provide any quantitative discrete-time finite-particle convergence rates. To the best of my understanding, Theorem 3.9 is the only result that considers finite-particle discrete-time dynamics, but it does not provide any quantitative rates.


3. It is not clear why one should prefer Gaussian SVGD over the Bures-Wasserstein Gradient Flow [3] since the only setting where it seems to outperform Bures-Wasserstein Gradient Flow is that of centered Gaussians, which is a highly restrictive condition. 


4. The results in Section 4 leave much to be desired. Considering the fact that the best known results for Bures-Wasserstein gradient flow [3] cover both log-concave and strongly log-concave densities (see also [4] for a computable JKO discretization of the Bures-Wasserstein flow with convergence guarantees for log-concave and log-strongly concave densities), it is not at all clear what the benefits of Gaussian SVGD are when the target is not Gaussian. 


5. The overall presentation of the results requires a lot of work. For instance, it is not clearly stated where exactly in the Appendix each Theorem is proved. 


6. In addition to the limited applicability of these results in explaining the behavior of SVGD (as the most practically relevant case is that of non-logconcave targets) and the absence of satisfactory finite-particle discrete-time guarantees, I am somewhat concerned about the technical novelty of these results. To the best of my understanding, It seems like the mean-field continuous-time guarantees of this work can be easily obtained from the well-established results on SVGD [5,6] by restricting to the (finite-dimensional) Gaussian submanifold. While limited theoretical contribution in itself is not a major weakness, I find it difficult to recommend acceptance given the unsatisfactory practical usefulness of these results. 


[1] Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1”

[2] Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition”

[3] Lambert et. al., “Variational inference via Wasserstein gradient flows”

[4] Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space”

[5] Duncan et. al., “On the geometry of Stein variational gradient descent”

[6] Liu, “Stein Variational Gradient Descent as Gradient Flow”


","1. Could you obtain discrete-time finite-particle rates for Gaussian SVGD under the settings considered in your results. If not, could you highlight what would be the technical challenges involved in obtaining such a result?


2. Could you clarify why the mean-field continuous-time Gaussian SVGD dynamics is of interest, when (to the best of my knowledge) : 1) It does not satisfactorily explain the behavior of SVGD in practical scenarios, 2) It is outperformed by Bures-Wasserstein Gradient Flows (as well as the practically implementable Bures-JKO scheme) in most settings


3. Could you comment on the technical novelty of these results? To the best of my understanding, the mean-field guarantees can be obtained by adapting the existing analysis of the Gradient Flow induced by SVGD [5,6] to the submanifold of Gaussian densities (a technique which is well-established for Wasserstein gradient flows in [3]). 


4. I would recommend reworking the overall presentation. Explicitly stating where each theorem is proven in the Appendix would be a good starting point. 
","I found the discussion on the limitations of this work to be somewhat unsatisfactory. The authors state that limitations are discussed on Section 6 (Other Related Works) and Section 7 (Conclusion). Section 6 discusses some of the prior literature and Section 7 highlights some future directions. Neither of these sections adequately discuss the limitations of this work (e.g. applicability is primarily restricted to centered Gaussian targets for most of the results, finite-particle discrete-time rates are absent in most settings)",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,1 poor,3 good,1688931567296,,,NeurIPS.cc/2023/Conference,14ZM7FfPx8,"['NeurIPS.cc/2023/Conference/Submission8391/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411166418,[],4,,,['everyone'],14ZM7FfPx8,['NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ'],1688931567296,1702411166418,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers, including:

1. Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1” ([1])
2. Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition” ([2])
3. Lambert et. al., “Variational inference via Wasserstein gradient flows” ([3])
4. Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space” ([4])
5. Duncan et. al., “On the geometry of Stein variational gradient descent” ([5])
6. Liu, “Stein Variational Gradient Descent as Gradient Flow” ([6])

The reviewer references these papers to support their criticisms and suggestions for improvement, implying that the authors should be aware of and engage with this existing literature in their work.",1,,A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1
Fb6X63mkYc,"This work performs a theoretical investigation of Gaussian SVGD, a special case of SVGD restricted to the submanifold of Gaussian densities by means of a bilinear kernel. The authors characterize the mean-field dynamics of Gaussian SVGD, with a particular focus on Gaussian targets and obtain finite-particle guarantees in certain settings. Moving beyond Gaussian targets, the authors show that the dynamics of mean-field Gaussian SVGD converges to the best Gaussian approximation (In KL divergence) to the target density, and obtain convergence rates for the mean-field dynamics with the target density is log-smooth and log-concave",The work provides a thorough treatment of the mean-field dynamics of Gaussian SVGD. The experimental evaluation is also satisfactory. ,"My primary concerns are as follows :

1. Most of the results presented in this work consider continuous-time mean-field dynamics under the assumption of a Bilinear kernel and Gaussian target density. In my opinion, these assumptions adversely impact the significance of these results, as they fail to explain one of the major appeals of SVGD, i.e., the fact that it provides a non-parametric approximation to a large class of target densities. (e.g. [1] shows discrete-time mean-field guarantees for subgaussian densities and [2] extends it to the class of densities satisfying a generalized Talagrand’s inequality). While the exponentially fast convergence guarantees for the mean-field dynamics are seemingly appealing, they fall short of providing a satisfactory explanation of the behavior of SVGD in practically relevant settings. 


2. Despite the simplifying assumption of Gaussian targets and bilinear kernels, the work does not provide any quantitative discrete-time finite-particle convergence rates. To the best of my understanding, Theorem 3.9 is the only result that considers finite-particle discrete-time dynamics, but it does not provide any quantitative rates.


3. It is not clear why one should prefer Gaussian SVGD over the Bures-Wasserstein Gradient Flow [3] since the only setting where it seems to outperform Bures-Wasserstein Gradient Flow is that of centered Gaussians, which is a highly restrictive condition. 


4. The results in Section 4 leave much to be desired. Considering the fact that the best known results for Bures-Wasserstein gradient flow [3] cover both log-concave and strongly log-concave densities (see also [4] for a computable JKO discretization of the Bures-Wasserstein flow with convergence guarantees for log-concave and log-strongly concave densities), it is not at all clear what the benefits of Gaussian SVGD are when the target is not Gaussian. 


5. The overall presentation of the results requires a lot of work. For instance, it is not clearly stated where exactly in the Appendix each Theorem is proved. 


6. In addition to the limited applicability of these results in explaining the behavior of SVGD (as the most practically relevant case is that of non-logconcave targets) and the absence of satisfactory finite-particle discrete-time guarantees, I am somewhat concerned about the technical novelty of these results. To the best of my understanding, It seems like the mean-field continuous-time guarantees of this work can be easily obtained from the well-established results on SVGD [5,6] by restricting to the (finite-dimensional) Gaussian submanifold. While limited theoretical contribution in itself is not a major weakness, I find it difficult to recommend acceptance given the unsatisfactory practical usefulness of these results. 


[1] Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1”

[2] Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition”

[3] Lambert et. al., “Variational inference via Wasserstein gradient flows”

[4] Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space”

[5] Duncan et. al., “On the geometry of Stein variational gradient descent”

[6] Liu, “Stein Variational Gradient Descent as Gradient Flow”


","1. Could you obtain discrete-time finite-particle rates for Gaussian SVGD under the settings considered in your results. If not, could you highlight what would be the technical challenges involved in obtaining such a result?


2. Could you clarify why the mean-field continuous-time Gaussian SVGD dynamics is of interest, when (to the best of my knowledge) : 1) It does not satisfactorily explain the behavior of SVGD in practical scenarios, 2) It is outperformed by Bures-Wasserstein Gradient Flows (as well as the practically implementable Bures-JKO scheme) in most settings


3. Could you comment on the technical novelty of these results? To the best of my understanding, the mean-field guarantees can be obtained by adapting the existing analysis of the Gradient Flow induced by SVGD [5,6] to the submanifold of Gaussian densities (a technique which is well-established for Wasserstein gradient flows in [3]). 


4. I would recommend reworking the overall presentation. Explicitly stating where each theorem is proven in the Appendix would be a good starting point. 
","I found the discussion on the limitations of this work to be somewhat unsatisfactory. The authors state that limitations are discussed on Section 6 (Other Related Works) and Section 7 (Conclusion). Section 6 discusses some of the prior literature and Section 7 highlights some future directions. Neither of these sections adequately discuss the limitations of this work (e.g. applicability is primarily restricted to centered Gaussian targets for most of the results, finite-particle discrete-time rates are absent in most settings)",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,1 poor,3 good,1688931567296,,,NeurIPS.cc/2023/Conference,14ZM7FfPx8,"['NeurIPS.cc/2023/Conference/Submission8391/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411166418,[],4,,,['everyone'],14ZM7FfPx8,['NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ'],1688931567296,1702411166418,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers, including:

1. Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1” ([1])
2. Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition” ([2])
3. Lambert et. al., “Variational inference via Wasserstein gradient flows” ([3])
4. Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space” ([4])
5. Duncan et. al., “On the geometry of Stein variational gradient descent” ([5])
6. Liu, “Stein Variational Gradient Descent as Gradient Flow” ([6])

The reviewer references these papers to support their criticisms and suggestions for improvement, implying that the authors should be aware of and engage with this existing literature in their work.",1,,Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition
Fb6X63mkYc,"This work performs a theoretical investigation of Gaussian SVGD, a special case of SVGD restricted to the submanifold of Gaussian densities by means of a bilinear kernel. The authors characterize the mean-field dynamics of Gaussian SVGD, with a particular focus on Gaussian targets and obtain finite-particle guarantees in certain settings. Moving beyond Gaussian targets, the authors show that the dynamics of mean-field Gaussian SVGD converges to the best Gaussian approximation (In KL divergence) to the target density, and obtain convergence rates for the mean-field dynamics with the target density is log-smooth and log-concave",The work provides a thorough treatment of the mean-field dynamics of Gaussian SVGD. The experimental evaluation is also satisfactory. ,"My primary concerns are as follows :

1. Most of the results presented in this work consider continuous-time mean-field dynamics under the assumption of a Bilinear kernel and Gaussian target density. In my opinion, these assumptions adversely impact the significance of these results, as they fail to explain one of the major appeals of SVGD, i.e., the fact that it provides a non-parametric approximation to a large class of target densities. (e.g. [1] shows discrete-time mean-field guarantees for subgaussian densities and [2] extends it to the class of densities satisfying a generalized Talagrand’s inequality). While the exponentially fast convergence guarantees for the mean-field dynamics are seemingly appealing, they fall short of providing a satisfactory explanation of the behavior of SVGD in practically relevant settings. 


2. Despite the simplifying assumption of Gaussian targets and bilinear kernels, the work does not provide any quantitative discrete-time finite-particle convergence rates. To the best of my understanding, Theorem 3.9 is the only result that considers finite-particle discrete-time dynamics, but it does not provide any quantitative rates.


3. It is not clear why one should prefer Gaussian SVGD over the Bures-Wasserstein Gradient Flow [3] since the only setting where it seems to outperform Bures-Wasserstein Gradient Flow is that of centered Gaussians, which is a highly restrictive condition. 


4. The results in Section 4 leave much to be desired. Considering the fact that the best known results for Bures-Wasserstein gradient flow [3] cover both log-concave and strongly log-concave densities (see also [4] for a computable JKO discretization of the Bures-Wasserstein flow with convergence guarantees for log-concave and log-strongly concave densities), it is not at all clear what the benefits of Gaussian SVGD are when the target is not Gaussian. 


5. The overall presentation of the results requires a lot of work. For instance, it is not clearly stated where exactly in the Appendix each Theorem is proved. 


6. In addition to the limited applicability of these results in explaining the behavior of SVGD (as the most practically relevant case is that of non-logconcave targets) and the absence of satisfactory finite-particle discrete-time guarantees, I am somewhat concerned about the technical novelty of these results. To the best of my understanding, It seems like the mean-field continuous-time guarantees of this work can be easily obtained from the well-established results on SVGD [5,6] by restricting to the (finite-dimensional) Gaussian submanifold. While limited theoretical contribution in itself is not a major weakness, I find it difficult to recommend acceptance given the unsatisfactory practical usefulness of these results. 


[1] Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1”

[2] Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition”

[3] Lambert et. al., “Variational inference via Wasserstein gradient flows”

[4] Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space”

[5] Duncan et. al., “On the geometry of Stein variational gradient descent”

[6] Liu, “Stein Variational Gradient Descent as Gradient Flow”


","1. Could you obtain discrete-time finite-particle rates for Gaussian SVGD under the settings considered in your results. If not, could you highlight what would be the technical challenges involved in obtaining such a result?


2. Could you clarify why the mean-field continuous-time Gaussian SVGD dynamics is of interest, when (to the best of my knowledge) : 1) It does not satisfactorily explain the behavior of SVGD in practical scenarios, 2) It is outperformed by Bures-Wasserstein Gradient Flows (as well as the practically implementable Bures-JKO scheme) in most settings


3. Could you comment on the technical novelty of these results? To the best of my understanding, the mean-field guarantees can be obtained by adapting the existing analysis of the Gradient Flow induced by SVGD [5,6] to the submanifold of Gaussian densities (a technique which is well-established for Wasserstein gradient flows in [3]). 


4. I would recommend reworking the overall presentation. Explicitly stating where each theorem is proven in the Appendix would be a good starting point. 
","I found the discussion on the limitations of this work to be somewhat unsatisfactory. The authors state that limitations are discussed on Section 6 (Other Related Works) and Section 7 (Conclusion). Section 6 discusses some of the prior literature and Section 7 highlights some future directions. Neither of these sections adequately discuss the limitations of this work (e.g. applicability is primarily restricted to centered Gaussian targets for most of the results, finite-particle discrete-time rates are absent in most settings)",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,1 poor,3 good,1688931567296,,,NeurIPS.cc/2023/Conference,14ZM7FfPx8,"['NeurIPS.cc/2023/Conference/Submission8391/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411166418,[],4,,,['everyone'],14ZM7FfPx8,['NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ'],1688931567296,1702411166418,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers, including:

1. Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1” ([1])
2. Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition” ([2])
3. Lambert et. al., “Variational inference via Wasserstein gradient flows” ([3])
4. Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space” ([4])
5. Duncan et. al., “On the geometry of Stein variational gradient descent” ([5])
6. Liu, “Stein Variational Gradient Descent as Gradient Flow” ([6])

The reviewer references these papers to support their criticisms and suggestions for improvement, implying that the authors should be aware of and engage with this existing literature in their work.",1,,Variational inference via Wasserstein gradient flows
Fb6X63mkYc,"This work performs a theoretical investigation of Gaussian SVGD, a special case of SVGD restricted to the submanifold of Gaussian densities by means of a bilinear kernel. The authors characterize the mean-field dynamics of Gaussian SVGD, with a particular focus on Gaussian targets and obtain finite-particle guarantees in certain settings. Moving beyond Gaussian targets, the authors show that the dynamics of mean-field Gaussian SVGD converges to the best Gaussian approximation (In KL divergence) to the target density, and obtain convergence rates for the mean-field dynamics with the target density is log-smooth and log-concave",The work provides a thorough treatment of the mean-field dynamics of Gaussian SVGD. The experimental evaluation is also satisfactory. ,"My primary concerns are as follows :

1. Most of the results presented in this work consider continuous-time mean-field dynamics under the assumption of a Bilinear kernel and Gaussian target density. In my opinion, these assumptions adversely impact the significance of these results, as they fail to explain one of the major appeals of SVGD, i.e., the fact that it provides a non-parametric approximation to a large class of target densities. (e.g. [1] shows discrete-time mean-field guarantees for subgaussian densities and [2] extends it to the class of densities satisfying a generalized Talagrand’s inequality). While the exponentially fast convergence guarantees for the mean-field dynamics are seemingly appealing, they fall short of providing a satisfactory explanation of the behavior of SVGD in practically relevant settings. 


2. Despite the simplifying assumption of Gaussian targets and bilinear kernels, the work does not provide any quantitative discrete-time finite-particle convergence rates. To the best of my understanding, Theorem 3.9 is the only result that considers finite-particle discrete-time dynamics, but it does not provide any quantitative rates.


3. It is not clear why one should prefer Gaussian SVGD over the Bures-Wasserstein Gradient Flow [3] since the only setting where it seems to outperform Bures-Wasserstein Gradient Flow is that of centered Gaussians, which is a highly restrictive condition. 


4. The results in Section 4 leave much to be desired. Considering the fact that the best known results for Bures-Wasserstein gradient flow [3] cover both log-concave and strongly log-concave densities (see also [4] for a computable JKO discretization of the Bures-Wasserstein flow with convergence guarantees for log-concave and log-strongly concave densities), it is not at all clear what the benefits of Gaussian SVGD are when the target is not Gaussian. 


5. The overall presentation of the results requires a lot of work. For instance, it is not clearly stated where exactly in the Appendix each Theorem is proved. 


6. In addition to the limited applicability of these results in explaining the behavior of SVGD (as the most practically relevant case is that of non-logconcave targets) and the absence of satisfactory finite-particle discrete-time guarantees, I am somewhat concerned about the technical novelty of these results. To the best of my understanding, It seems like the mean-field continuous-time guarantees of this work can be easily obtained from the well-established results on SVGD [5,6] by restricting to the (finite-dimensional) Gaussian submanifold. While limited theoretical contribution in itself is not a major weakness, I find it difficult to recommend acceptance given the unsatisfactory practical usefulness of these results. 


[1] Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1”

[2] Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition”

[3] Lambert et. al., “Variational inference via Wasserstein gradient flows”

[4] Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space”

[5] Duncan et. al., “On the geometry of Stein variational gradient descent”

[6] Liu, “Stein Variational Gradient Descent as Gradient Flow”


","1. Could you obtain discrete-time finite-particle rates for Gaussian SVGD under the settings considered in your results. If not, could you highlight what would be the technical challenges involved in obtaining such a result?


2. Could you clarify why the mean-field continuous-time Gaussian SVGD dynamics is of interest, when (to the best of my knowledge) : 1) It does not satisfactorily explain the behavior of SVGD in practical scenarios, 2) It is outperformed by Bures-Wasserstein Gradient Flows (as well as the practically implementable Bures-JKO scheme) in most settings


3. Could you comment on the technical novelty of these results? To the best of my understanding, the mean-field guarantees can be obtained by adapting the existing analysis of the Gradient Flow induced by SVGD [5,6] to the submanifold of Gaussian densities (a technique which is well-established for Wasserstein gradient flows in [3]). 


4. I would recommend reworking the overall presentation. Explicitly stating where each theorem is proven in the Appendix would be a good starting point. 
","I found the discussion on the limitations of this work to be somewhat unsatisfactory. The authors state that limitations are discussed on Section 6 (Other Related Works) and Section 7 (Conclusion). Section 6 discusses some of the prior literature and Section 7 highlights some future directions. Neither of these sections adequately discuss the limitations of this work (e.g. applicability is primarily restricted to centered Gaussian targets for most of the results, finite-particle discrete-time rates are absent in most settings)",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,1 poor,3 good,1688931567296,,,NeurIPS.cc/2023/Conference,14ZM7FfPx8,"['NeurIPS.cc/2023/Conference/Submission8391/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411166418,[],4,,,['everyone'],14ZM7FfPx8,['NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ'],1688931567296,1702411166418,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers, including:

1. Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1” ([1])
2. Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition” ([2])
3. Lambert et. al., “Variational inference via Wasserstein gradient flows” ([3])
4. Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space” ([4])
5. Duncan et. al., “On the geometry of Stein variational gradient descent” ([5])
6. Liu, “Stein Variational Gradient Descent as Gradient Flow” ([6])

The reviewer references these papers to support their criticisms and suggestions for improvement, implying that the authors should be aware of and engage with this existing literature in their work.",1,,Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space
Fb6X63mkYc,"This work performs a theoretical investigation of Gaussian SVGD, a special case of SVGD restricted to the submanifold of Gaussian densities by means of a bilinear kernel. The authors characterize the mean-field dynamics of Gaussian SVGD, with a particular focus on Gaussian targets and obtain finite-particle guarantees in certain settings. Moving beyond Gaussian targets, the authors show that the dynamics of mean-field Gaussian SVGD converges to the best Gaussian approximation (In KL divergence) to the target density, and obtain convergence rates for the mean-field dynamics with the target density is log-smooth and log-concave",The work provides a thorough treatment of the mean-field dynamics of Gaussian SVGD. The experimental evaluation is also satisfactory. ,"My primary concerns are as follows :

1. Most of the results presented in this work consider continuous-time mean-field dynamics under the assumption of a Bilinear kernel and Gaussian target density. In my opinion, these assumptions adversely impact the significance of these results, as they fail to explain one of the major appeals of SVGD, i.e., the fact that it provides a non-parametric approximation to a large class of target densities. (e.g. [1] shows discrete-time mean-field guarantees for subgaussian densities and [2] extends it to the class of densities satisfying a generalized Talagrand’s inequality). While the exponentially fast convergence guarantees for the mean-field dynamics are seemingly appealing, they fall short of providing a satisfactory explanation of the behavior of SVGD in practically relevant settings. 


2. Despite the simplifying assumption of Gaussian targets and bilinear kernels, the work does not provide any quantitative discrete-time finite-particle convergence rates. To the best of my understanding, Theorem 3.9 is the only result that considers finite-particle discrete-time dynamics, but it does not provide any quantitative rates.


3. It is not clear why one should prefer Gaussian SVGD over the Bures-Wasserstein Gradient Flow [3] since the only setting where it seems to outperform Bures-Wasserstein Gradient Flow is that of centered Gaussians, which is a highly restrictive condition. 


4. The results in Section 4 leave much to be desired. Considering the fact that the best known results for Bures-Wasserstein gradient flow [3] cover both log-concave and strongly log-concave densities (see also [4] for a computable JKO discretization of the Bures-Wasserstein flow with convergence guarantees for log-concave and log-strongly concave densities), it is not at all clear what the benefits of Gaussian SVGD are when the target is not Gaussian. 


5. The overall presentation of the results requires a lot of work. For instance, it is not clearly stated where exactly in the Appendix each Theorem is proved. 


6. In addition to the limited applicability of these results in explaining the behavior of SVGD (as the most practically relevant case is that of non-logconcave targets) and the absence of satisfactory finite-particle discrete-time guarantees, I am somewhat concerned about the technical novelty of these results. To the best of my understanding, It seems like the mean-field continuous-time guarantees of this work can be easily obtained from the well-established results on SVGD [5,6] by restricting to the (finite-dimensional) Gaussian submanifold. While limited theoretical contribution in itself is not a major weakness, I find it difficult to recommend acceptance given the unsatisfactory practical usefulness of these results. 


[1] Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1”

[2] Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition”

[3] Lambert et. al., “Variational inference via Wasserstein gradient flows”

[4] Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space”

[5] Duncan et. al., “On the geometry of Stein variational gradient descent”

[6] Liu, “Stein Variational Gradient Descent as Gradient Flow”


","1. Could you obtain discrete-time finite-particle rates for Gaussian SVGD under the settings considered in your results. If not, could you highlight what would be the technical challenges involved in obtaining such a result?


2. Could you clarify why the mean-field continuous-time Gaussian SVGD dynamics is of interest, when (to the best of my knowledge) : 1) It does not satisfactorily explain the behavior of SVGD in practical scenarios, 2) It is outperformed by Bures-Wasserstein Gradient Flows (as well as the practically implementable Bures-JKO scheme) in most settings


3. Could you comment on the technical novelty of these results? To the best of my understanding, the mean-field guarantees can be obtained by adapting the existing analysis of the Gradient Flow induced by SVGD [5,6] to the submanifold of Gaussian densities (a technique which is well-established for Wasserstein gradient flows in [3]). 


4. I would recommend reworking the overall presentation. Explicitly stating where each theorem is proven in the Appendix would be a good starting point. 
","I found the discussion on the limitations of this work to be somewhat unsatisfactory. The authors state that limitations are discussed on Section 6 (Other Related Works) and Section 7 (Conclusion). Section 6 discusses some of the prior literature and Section 7 highlights some future directions. Neither of these sections adequately discuss the limitations of this work (e.g. applicability is primarily restricted to centered Gaussian targets for most of the results, finite-particle discrete-time rates are absent in most settings)",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,1 poor,3 good,1688931567296,,,NeurIPS.cc/2023/Conference,14ZM7FfPx8,"['NeurIPS.cc/2023/Conference/Submission8391/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411166418,[],4,,,['everyone'],14ZM7FfPx8,['NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ'],1688931567296,1702411166418,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers, including:

1. Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1” ([1])
2. Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition” ([2])
3. Lambert et. al., “Variational inference via Wasserstein gradient flows” ([3])
4. Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space” ([4])
5. Duncan et. al., “On the geometry of Stein variational gradient descent” ([5])
6. Liu, “Stein Variational Gradient Descent as Gradient Flow” ([6])

The reviewer references these papers to support their criticisms and suggestions for improvement, implying that the authors should be aware of and engage with this existing literature in their work.",1,,On the geometry of Stein variational gradient descent
Fb6X63mkYc,"This work performs a theoretical investigation of Gaussian SVGD, a special case of SVGD restricted to the submanifold of Gaussian densities by means of a bilinear kernel. The authors characterize the mean-field dynamics of Gaussian SVGD, with a particular focus on Gaussian targets and obtain finite-particle guarantees in certain settings. Moving beyond Gaussian targets, the authors show that the dynamics of mean-field Gaussian SVGD converges to the best Gaussian approximation (In KL divergence) to the target density, and obtain convergence rates for the mean-field dynamics with the target density is log-smooth and log-concave",The work provides a thorough treatment of the mean-field dynamics of Gaussian SVGD. The experimental evaluation is also satisfactory. ,"My primary concerns are as follows :

1. Most of the results presented in this work consider continuous-time mean-field dynamics under the assumption of a Bilinear kernel and Gaussian target density. In my opinion, these assumptions adversely impact the significance of these results, as they fail to explain one of the major appeals of SVGD, i.e., the fact that it provides a non-parametric approximation to a large class of target densities. (e.g. [1] shows discrete-time mean-field guarantees for subgaussian densities and [2] extends it to the class of densities satisfying a generalized Talagrand’s inequality). While the exponentially fast convergence guarantees for the mean-field dynamics are seemingly appealing, they fall short of providing a satisfactory explanation of the behavior of SVGD in practically relevant settings. 


2. Despite the simplifying assumption of Gaussian targets and bilinear kernels, the work does not provide any quantitative discrete-time finite-particle convergence rates. To the best of my understanding, Theorem 3.9 is the only result that considers finite-particle discrete-time dynamics, but it does not provide any quantitative rates.


3. It is not clear why one should prefer Gaussian SVGD over the Bures-Wasserstein Gradient Flow [3] since the only setting where it seems to outperform Bures-Wasserstein Gradient Flow is that of centered Gaussians, which is a highly restrictive condition. 


4. The results in Section 4 leave much to be desired. Considering the fact that the best known results for Bures-Wasserstein gradient flow [3] cover both log-concave and strongly log-concave densities (see also [4] for a computable JKO discretization of the Bures-Wasserstein flow with convergence guarantees for log-concave and log-strongly concave densities), it is not at all clear what the benefits of Gaussian SVGD are when the target is not Gaussian. 


5. The overall presentation of the results requires a lot of work. For instance, it is not clearly stated where exactly in the Appendix each Theorem is proved. 


6. In addition to the limited applicability of these results in explaining the behavior of SVGD (as the most practically relevant case is that of non-logconcave targets) and the absence of satisfactory finite-particle discrete-time guarantees, I am somewhat concerned about the technical novelty of these results. To the best of my understanding, It seems like the mean-field continuous-time guarantees of this work can be easily obtained from the well-established results on SVGD [5,6] by restricting to the (finite-dimensional) Gaussian submanifold. While limited theoretical contribution in itself is not a major weakness, I find it difficult to recommend acceptance given the unsatisfactory practical usefulness of these results. 


[1] Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1”

[2] Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition”

[3] Lambert et. al., “Variational inference via Wasserstein gradient flows”

[4] Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space”

[5] Duncan et. al., “On the geometry of Stein variational gradient descent”

[6] Liu, “Stein Variational Gradient Descent as Gradient Flow”


","1. Could you obtain discrete-time finite-particle rates for Gaussian SVGD under the settings considered in your results. If not, could you highlight what would be the technical challenges involved in obtaining such a result?


2. Could you clarify why the mean-field continuous-time Gaussian SVGD dynamics is of interest, when (to the best of my knowledge) : 1) It does not satisfactorily explain the behavior of SVGD in practical scenarios, 2) It is outperformed by Bures-Wasserstein Gradient Flows (as well as the practically implementable Bures-JKO scheme) in most settings


3. Could you comment on the technical novelty of these results? To the best of my understanding, the mean-field guarantees can be obtained by adapting the existing analysis of the Gradient Flow induced by SVGD [5,6] to the submanifold of Gaussian densities (a technique which is well-established for Wasserstein gradient flows in [3]). 


4. I would recommend reworking the overall presentation. Explicitly stating where each theorem is proven in the Appendix would be a good starting point. 
","I found the discussion on the limitations of this work to be somewhat unsatisfactory. The authors state that limitations are discussed on Section 6 (Other Related Works) and Section 7 (Conclusion). Section 6 discusses some of the prior literature and Section 7 highlights some future directions. Neither of these sections adequately discuss the limitations of this work (e.g. applicability is primarily restricted to centered Gaussian targets for most of the results, finite-particle discrete-time rates are absent in most settings)",['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,1 poor,3 good,1688931567296,,,NeurIPS.cc/2023/Conference,14ZM7FfPx8,"['NeurIPS.cc/2023/Conference/Submission8391/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411166418,[],4,,,['everyone'],14ZM7FfPx8,['NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ'],1688931567296,1702411166418,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8391/Reviewer_1EPQ']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several papers, including:

1. Salim et. al., “A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1” ([1])
2. Sun et. al., “Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition” ([2])
3. Lambert et. al., “Variational inference via Wasserstein gradient flows” ([3])
4. Diao et. al., “Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space” ([4])
5. Duncan et. al., “On the geometry of Stein variational gradient descent” ([5])
6. Liu, “Stein Variational Gradient Descent as Gradient Flow” ([6])

The reviewer references these papers to support their criticisms and suggestions for improvement, implying that the authors should be aware of and engage with this existing literature in their work.",1,,Stein Variational Gradient Descent as Gradient Flow
DkF4UZ36o6,"This paper studies connections between machine unlearning and differential privacy (DP). In machine unlearning, the goal is to remove up to, say, $m$ of the examples from a dataset of size $n$, in such a way that the produced model is close (in some form of statistical or computational distance) to the model that would have been produced if we did not have the m examples to begin with.

A recent paper of SAKS'21 formulates unlearning with the same style that DP is usually defined. This paper follows the same definition.

The main question studied in this paper is about ""deletion capacity"". Namely, how many examples from the dataset can we delete while we lose the accuracy of the model up to a given parameter $\alpha$ and keep the machine unlearning ""secure"" with specified parameters $(\epsilon,\delta)$ (defined similarly to DP)? More formally, $\alpha$ is the regret in the agnostic setting, which is the extra risk compared to the best model in the family.

The main result of the paper is to establish matching upper and lower bounds (up to constant factors) on the deletion capacity for algorithms that basically do not do any deletion and when certain (natural, but still limiting) properties hold on the models and the loss function. Namely, the paper studies how to achieve $(\epsilon,\delta)$ privacy when the comparison is made between datasets that have hamming distance $m$, rather than $1$, and while the regret is bounded by $\alpha$.

At a technical level, the paper achieves its tight upper and lower bound (on the deletion capacity, within its own defined framework) by actually *not* caring about achieving differential privacy in its standard sense and directly aiming to satisfy the DP Lipschitz property over $m$-close databases out of the box. To achieve tight bounds the paper moves to other notions of DP (based on Reny divergence) first and then comes back to DP after a more effective composition theorem (that exist for such DP style definitions) is applied.

The paper also studies composition of unlearning, but only the statements are stated in the paper and no discussion is presented.","At a technical level, studying the DP for $m$-close databases (with the motivation of doing nothing for unlearning!) is interesting, and finding tight bounds for such problem is cool. but I would have preferred a more direct depiction of the result up front by saying that what happens here is not really unlearning and is about DP for $m$-close datasets. Then, maybe depicting unlearning as a potential application would be good, since the application to unlearning comes with some limitations that prevent us from using the full capacity of what unlearning allows.

The proofs also look interesting and as far as I could tell, the difference between this work and previous work is explained well.
","As explained above, I think the connection to unlearning is a bit far fetched, as it comes with strong limitations.

(has a related question)
What I understand is that the paper studies tight bounds for settings in which the unlearning is *not even done* and the closeness of the produced models holds due to the DP-like property over $m$-close data sets. I am not sure why this is equivalent to ""not storing anything besides the model itself"". If they are equivalent, this needs a proof.

Citations are not in good shape. Examples:
The work of Cohen et al is cited for initiating a formal study of ""the right to be forgotten"", while works like: https://eprint.iacr.org/2020/254 are done earlier.

The main question of the paper is about privacy vs unlearning, which is also studied in previous (uncited) works like
https://www.usenix.org/conference/usenixsecurity20/presentation/salem
https://arxiv.org/abs/2005.02205
https://arxiv.org/abs/2202.03460:
","See the question in the section above. In addition:

why is theorem 1.6 is called a composition theorem? it seems it would be more natural for a composition theorem to allow up to k batches of deletion in an adaptive way.

line 207.5: why is the distribution p and not D ?

line 229: why is F* defined like this, and not like how it was defined in line 209 ?

the def of loss in line 277: why is the loss defined like that? The loss should take a model and a full *labeled* example and then output something. Your notion ignores the label (maybe x itself has the label already?) and is defined for a set (it would be a risk, in that case, and it typically takes average).","The paper is clear in that they only study specific ""unearning"" methods that come with limitations. It is mentioned that the limitation is not to store anything other than the model, but my understanding is that their limitation is to not do anything when unlearning. These seem different to me, but hopefully the author(s) will clarify this limitation in rebuttal.",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1688560735662,,,NeurIPS.cc/2023/Conference,14R8QBKzFH,"['NeurIPS.cc/2023/Conference/Submission10089/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411267929,[],2,,,['everyone'],14R8QBKzFH,['NeurIPS.cc/2023/Conference/Submission10089/Reviewer_Hbaj'],1688560735662,1702411267929,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10089/Reviewer_Hbaj']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several uncited works that have studied the topic of privacy vs unlearning, including:

* https://eprint.iacr.org/2020/254
* https://www.usenix.org/conference/usenixsecurity20/presentation/salem
* https://arxiv.org/abs/2005.02205
* https://arxiv.org/abs/2202.03460

The reviewer notes that the work of Cohen et al is cited for initiating a formal study of ""the right to be forgotten"", but points out that earlier works like the one at https://eprint.iacr.org/2020/254 should also be cited. The reviewer suggests that the authors should cite these works to provide a more comprehensive overview of the existing literature on the topic.",1,"2020, 2020, 2022, 2020",Formalizing Data Deletion in the Context of the Right to be Forgotten
DkF4UZ36o6,"This paper studies connections between machine unlearning and differential privacy (DP). In machine unlearning, the goal is to remove up to, say, $m$ of the examples from a dataset of size $n$, in such a way that the produced model is close (in some form of statistical or computational distance) to the model that would have been produced if we did not have the m examples to begin with.

A recent paper of SAKS'21 formulates unlearning with the same style that DP is usually defined. This paper follows the same definition.

The main question studied in this paper is about ""deletion capacity"". Namely, how many examples from the dataset can we delete while we lose the accuracy of the model up to a given parameter $\alpha$ and keep the machine unlearning ""secure"" with specified parameters $(\epsilon,\delta)$ (defined similarly to DP)? More formally, $\alpha$ is the regret in the agnostic setting, which is the extra risk compared to the best model in the family.

The main result of the paper is to establish matching upper and lower bounds (up to constant factors) on the deletion capacity for algorithms that basically do not do any deletion and when certain (natural, but still limiting) properties hold on the models and the loss function. Namely, the paper studies how to achieve $(\epsilon,\delta)$ privacy when the comparison is made between datasets that have hamming distance $m$, rather than $1$, and while the regret is bounded by $\alpha$.

At a technical level, the paper achieves its tight upper and lower bound (on the deletion capacity, within its own defined framework) by actually *not* caring about achieving differential privacy in its standard sense and directly aiming to satisfy the DP Lipschitz property over $m$-close databases out of the box. To achieve tight bounds the paper moves to other notions of DP (based on Reny divergence) first and then comes back to DP after a more effective composition theorem (that exist for such DP style definitions) is applied.

The paper also studies composition of unlearning, but only the statements are stated in the paper and no discussion is presented.","At a technical level, studying the DP for $m$-close databases (with the motivation of doing nothing for unlearning!) is interesting, and finding tight bounds for such problem is cool. but I would have preferred a more direct depiction of the result up front by saying that what happens here is not really unlearning and is about DP for $m$-close datasets. Then, maybe depicting unlearning as a potential application would be good, since the application to unlearning comes with some limitations that prevent us from using the full capacity of what unlearning allows.

The proofs also look interesting and as far as I could tell, the difference between this work and previous work is explained well.
","As explained above, I think the connection to unlearning is a bit far fetched, as it comes with strong limitations.

(has a related question)
What I understand is that the paper studies tight bounds for settings in which the unlearning is *not even done* and the closeness of the produced models holds due to the DP-like property over $m$-close data sets. I am not sure why this is equivalent to ""not storing anything besides the model itself"". If they are equivalent, this needs a proof.

Citations are not in good shape. Examples:
The work of Cohen et al is cited for initiating a formal study of ""the right to be forgotten"", while works like: https://eprint.iacr.org/2020/254 are done earlier.

The main question of the paper is about privacy vs unlearning, which is also studied in previous (uncited) works like
https://www.usenix.org/conference/usenixsecurity20/presentation/salem
https://arxiv.org/abs/2005.02205
https://arxiv.org/abs/2202.03460:
","See the question in the section above. In addition:

why is theorem 1.6 is called a composition theorem? it seems it would be more natural for a composition theorem to allow up to k batches of deletion in an adaptive way.

line 207.5: why is the distribution p and not D ?

line 229: why is F* defined like this, and not like how it was defined in line 209 ?

the def of loss in line 277: why is the loss defined like that? The loss should take a model and a full *labeled* example and then output something. Your notion ignores the label (maybe x itself has the label already?) and is defined for a set (it would be a risk, in that case, and it typically takes average).","The paper is clear in that they only study specific ""unearning"" methods that come with limitations. It is mentioned that the limitation is not to store anything other than the model, but my understanding is that their limitation is to not do anything when unlearning. These seem different to me, but hopefully the author(s) will clarify this limitation in rebuttal.",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1688560735662,,,NeurIPS.cc/2023/Conference,14R8QBKzFH,"['NeurIPS.cc/2023/Conference/Submission10089/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411267929,[],2,,,['everyone'],14R8QBKzFH,['NeurIPS.cc/2023/Conference/Submission10089/Reviewer_Hbaj'],1688560735662,1702411267929,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10089/Reviewer_Hbaj']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several uncited works that have studied the topic of privacy vs unlearning, including:

* https://eprint.iacr.org/2020/254
* https://www.usenix.org/conference/usenixsecurity20/presentation/salem
* https://arxiv.org/abs/2005.02205
* https://arxiv.org/abs/2202.03460

The reviewer notes that the work of Cohen et al is cited for initiating a formal study of ""the right to be forgotten"", but points out that earlier works like the one at https://eprint.iacr.org/2020/254 should also be cited. The reviewer suggests that the authors should cite these works to provide a more comprehensive overview of the existing literature on the topic.",1,"2020, 2020, 2022, 2020",Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning
DkF4UZ36o6,"This paper studies connections between machine unlearning and differential privacy (DP). In machine unlearning, the goal is to remove up to, say, $m$ of the examples from a dataset of size $n$, in such a way that the produced model is close (in some form of statistical or computational distance) to the model that would have been produced if we did not have the m examples to begin with.

A recent paper of SAKS'21 formulates unlearning with the same style that DP is usually defined. This paper follows the same definition.

The main question studied in this paper is about ""deletion capacity"". Namely, how many examples from the dataset can we delete while we lose the accuracy of the model up to a given parameter $\alpha$ and keep the machine unlearning ""secure"" with specified parameters $(\epsilon,\delta)$ (defined similarly to DP)? More formally, $\alpha$ is the regret in the agnostic setting, which is the extra risk compared to the best model in the family.

The main result of the paper is to establish matching upper and lower bounds (up to constant factors) on the deletion capacity for algorithms that basically do not do any deletion and when certain (natural, but still limiting) properties hold on the models and the loss function. Namely, the paper studies how to achieve $(\epsilon,\delta)$ privacy when the comparison is made between datasets that have hamming distance $m$, rather than $1$, and while the regret is bounded by $\alpha$.

At a technical level, the paper achieves its tight upper and lower bound (on the deletion capacity, within its own defined framework) by actually *not* caring about achieving differential privacy in its standard sense and directly aiming to satisfy the DP Lipschitz property over $m$-close databases out of the box. To achieve tight bounds the paper moves to other notions of DP (based on Reny divergence) first and then comes back to DP after a more effective composition theorem (that exist for such DP style definitions) is applied.

The paper also studies composition of unlearning, but only the statements are stated in the paper and no discussion is presented.","At a technical level, studying the DP for $m$-close databases (with the motivation of doing nothing for unlearning!) is interesting, and finding tight bounds for such problem is cool. but I would have preferred a more direct depiction of the result up front by saying that what happens here is not really unlearning and is about DP for $m$-close datasets. Then, maybe depicting unlearning as a potential application would be good, since the application to unlearning comes with some limitations that prevent us from using the full capacity of what unlearning allows.

The proofs also look interesting and as far as I could tell, the difference between this work and previous work is explained well.
","As explained above, I think the connection to unlearning is a bit far fetched, as it comes with strong limitations.

(has a related question)
What I understand is that the paper studies tight bounds for settings in which the unlearning is *not even done* and the closeness of the produced models holds due to the DP-like property over $m$-close data sets. I am not sure why this is equivalent to ""not storing anything besides the model itself"". If they are equivalent, this needs a proof.

Citations are not in good shape. Examples:
The work of Cohen et al is cited for initiating a formal study of ""the right to be forgotten"", while works like: https://eprint.iacr.org/2020/254 are done earlier.

The main question of the paper is about privacy vs unlearning, which is also studied in previous (uncited) works like
https://www.usenix.org/conference/usenixsecurity20/presentation/salem
https://arxiv.org/abs/2005.02205
https://arxiv.org/abs/2202.03460:
","See the question in the section above. In addition:

why is theorem 1.6 is called a composition theorem? it seems it would be more natural for a composition theorem to allow up to k batches of deletion in an adaptive way.

line 207.5: why is the distribution p and not D ?

line 229: why is F* defined like this, and not like how it was defined in line 209 ?

the def of loss in line 277: why is the loss defined like that? The loss should take a model and a full *labeled* example and then output something. Your notion ignores the label (maybe x itself has the label already?) and is defined for a set (it would be a risk, in that case, and it typically takes average).","The paper is clear in that they only study specific ""unearning"" methods that come with limitations. It is mentioned that the limitation is not to store anything other than the model, but my understanding is that their limitation is to not do anything when unlearning. These seem different to me, but hopefully the author(s) will clarify this limitation in rebuttal.",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1688560735662,,,NeurIPS.cc/2023/Conference,14R8QBKzFH,"['NeurIPS.cc/2023/Conference/Submission10089/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411267929,[],2,,,['everyone'],14R8QBKzFH,['NeurIPS.cc/2023/Conference/Submission10089/Reviewer_Hbaj'],1688560735662,1702411267929,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10089/Reviewer_Hbaj']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several uncited works that have studied the topic of privacy vs unlearning, including:

* https://eprint.iacr.org/2020/254
* https://www.usenix.org/conference/usenixsecurity20/presentation/salem
* https://arxiv.org/abs/2005.02205
* https://arxiv.org/abs/2202.03460

The reviewer notes that the work of Cohen et al is cited for initiating a formal study of ""the right to be forgotten"", but points out that earlier works like the one at https://eprint.iacr.org/2020/254 should also be cited. The reviewer suggests that the authors should cite these works to provide a more comprehensive overview of the existing literature on the topic.",1,"2020, 2020, 2022, 2020",When Machine Unlearning Jeopardizes Privacy
DkF4UZ36o6,"This paper studies connections between machine unlearning and differential privacy (DP). In machine unlearning, the goal is to remove up to, say, $m$ of the examples from a dataset of size $n$, in such a way that the produced model is close (in some form of statistical or computational distance) to the model that would have been produced if we did not have the m examples to begin with.

A recent paper of SAKS'21 formulates unlearning with the same style that DP is usually defined. This paper follows the same definition.

The main question studied in this paper is about ""deletion capacity"". Namely, how many examples from the dataset can we delete while we lose the accuracy of the model up to a given parameter $\alpha$ and keep the machine unlearning ""secure"" with specified parameters $(\epsilon,\delta)$ (defined similarly to DP)? More formally, $\alpha$ is the regret in the agnostic setting, which is the extra risk compared to the best model in the family.

The main result of the paper is to establish matching upper and lower bounds (up to constant factors) on the deletion capacity for algorithms that basically do not do any deletion and when certain (natural, but still limiting) properties hold on the models and the loss function. Namely, the paper studies how to achieve $(\epsilon,\delta)$ privacy when the comparison is made between datasets that have hamming distance $m$, rather than $1$, and while the regret is bounded by $\alpha$.

At a technical level, the paper achieves its tight upper and lower bound (on the deletion capacity, within its own defined framework) by actually *not* caring about achieving differential privacy in its standard sense and directly aiming to satisfy the DP Lipschitz property over $m$-close databases out of the box. To achieve tight bounds the paper moves to other notions of DP (based on Reny divergence) first and then comes back to DP after a more effective composition theorem (that exist for such DP style definitions) is applied.

The paper also studies composition of unlearning, but only the statements are stated in the paper and no discussion is presented.","At a technical level, studying the DP for $m$-close databases (with the motivation of doing nothing for unlearning!) is interesting, and finding tight bounds for such problem is cool. but I would have preferred a more direct depiction of the result up front by saying that what happens here is not really unlearning and is about DP for $m$-close datasets. Then, maybe depicting unlearning as a potential application would be good, since the application to unlearning comes with some limitations that prevent us from using the full capacity of what unlearning allows.

The proofs also look interesting and as far as I could tell, the difference between this work and previous work is explained well.
","As explained above, I think the connection to unlearning is a bit far fetched, as it comes with strong limitations.

(has a related question)
What I understand is that the paper studies tight bounds for settings in which the unlearning is *not even done* and the closeness of the produced models holds due to the DP-like property over $m$-close data sets. I am not sure why this is equivalent to ""not storing anything besides the model itself"". If they are equivalent, this needs a proof.

Citations are not in good shape. Examples:
The work of Cohen et al is cited for initiating a formal study of ""the right to be forgotten"", while works like: https://eprint.iacr.org/2020/254 are done earlier.

The main question of the paper is about privacy vs unlearning, which is also studied in previous (uncited) works like
https://www.usenix.org/conference/usenixsecurity20/presentation/salem
https://arxiv.org/abs/2005.02205
https://arxiv.org/abs/2202.03460:
","See the question in the section above. In addition:

why is theorem 1.6 is called a composition theorem? it seems it would be more natural for a composition theorem to allow up to k batches of deletion in an adaptive way.

line 207.5: why is the distribution p and not D ?

line 229: why is F* defined like this, and not like how it was defined in line 209 ?

the def of loss in line 277: why is the loss defined like that? The loss should take a model and a full *labeled* example and then output something. Your notion ignores the label (maybe x itself has the label already?) and is defined for a set (it would be a risk, in that case, and it typically takes average).","The paper is clear in that they only study specific ""unearning"" methods that come with limitations. It is mentioned that the limitation is not to store anything other than the model, but my understanding is that their limitation is to not do anything when unlearning. These seem different to me, but hopefully the author(s) will clarify this limitation in rebuttal.",['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,3 good,1688560735662,,,NeurIPS.cc/2023/Conference,14R8QBKzFH,"['NeurIPS.cc/2023/Conference/Submission10089/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411267929,[],2,,,['everyone'],14R8QBKzFH,['NeurIPS.cc/2023/Conference/Submission10089/Reviewer_Hbaj'],1688560735662,1702411267929,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission10089/Reviewer_Hbaj']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several uncited works that have studied the topic of privacy vs unlearning, including:

* https://eprint.iacr.org/2020/254
* https://www.usenix.org/conference/usenixsecurity20/presentation/salem
* https://arxiv.org/abs/2005.02205
* https://arxiv.org/abs/2202.03460

The reviewer notes that the work of Cohen et al is cited for initiating a formal study of ""the right to be forgotten"", but points out that earlier works like the one at https://eprint.iacr.org/2020/254 should also be cited. The reviewer suggests that the authors should cite these works to provide a more comprehensive overview of the existing literature on the topic.",1,"2020, 2020, 2022, 2020","Deletion Inference, Reconstruction, and Compliance in Machine (Un)Learning"
4rstqCp9Ie,"This paper centers on graph-level representation learning, aimed at converting graphs into vectors useful for downstream tasks like graph classification. The authors propose a unique learning principle named the Lovász principle, inspired by the Lovász number in graph theory. The Lovász number, a real number serving as an upper bound for a graph's Shannon capacity, has strong ties to various global graph characteristics. The authors suggest that the handle vector, used for calculating the Lovász number, could be an effective choice for graph representation given its ability to capture global graph properties. However, its direct application poses challenges. To address these, the authors propose using neural networks to offer the Lovász principle. They also present an enhanced Lovász principle capable of directly and efficiently utilizing subgraph Lovász numbers. Experimental results demonstrate competitive performance of these Lovász principles in comparison to the baselines in both unsupervised and semi-supervised graph-level representation learning tasks.","1. The authors' proposal of Lovász theta kernels for graph representation learning shows great promise, thanks to its theoretical foundation.
2. The paper includes thorough experiments conducted in various settings, including unsupervised, semi-supervised, and transfer learning. These experiments provide strong evidence of the effectiveness of the proposed methods.","1. Although the author proposed a new graph kernel, the contribution seems slightly small to me.
2. The author did not demonstrate the framework of the model.
3. Some classic baselines were not compared.","1. I strongly suggest that the author discuss the application of graph kernel in updated scenarios, such as graph prompt learning. It is highly recommended that the authors consider citing the prompt learning on graph[1,2] paper and explore its application in conjunction with the proposed method in this paper, especially for unsupervised settings. This would enhance the discussion and potential application of prompt-based models.

 [1] Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Z Liu, X Yu, Y Fang, X Zhang - Proceedings of the ACM Web Conference 2023, 2023

 [2] Sun, M., Zhou, K., He, X., Wang, Y. and Wang, X., 2022, August. Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.

2. Providing a framework of the proposed model can indeed enhance the clarity of its functionality.
3. Why semi-supervised learning baseliens such as GCN, GAT, GIN are not compared in the paper?",See weaknesses.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688549634615,,,NeurIPS.cc/2023/Conference,0vdEHDwamk,"['NeurIPS.cc/2023/Conference/Submission4298/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410946839,[],3,,,['everyone'],0vdEHDwamk,['NeurIPS.cc/2023/Conference/Submission4298/Reviewer_4DKt'],1688549634615,1702410946839,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4298/Reviewer_4DKt']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature:

1. ""Graphprompt: Unifying pre-training and downstream tasks for graph neural networks"" by Z Liu, X Yu, Y Fang, X Zhang (2023)
2. ""Gppt: Graph pre-training and prompt tuning to generalize graph neural networks"" by Sun, M., Zhou, K., He, X., Wang, Y. and Wang, X. (2022)

The reviewer recommends that the authors consider citing these papers and explore the application of prompt-based models in conjunction with the proposed method, especially for unsupervised settings.",1,"2023, 2022",Graphprompt Unifying pre-training and downstream tasks for graph neural networks 
4rstqCp9Ie,"This paper centers on graph-level representation learning, aimed at converting graphs into vectors useful for downstream tasks like graph classification. The authors propose a unique learning principle named the Lovász principle, inspired by the Lovász number in graph theory. The Lovász number, a real number serving as an upper bound for a graph's Shannon capacity, has strong ties to various global graph characteristics. The authors suggest that the handle vector, used for calculating the Lovász number, could be an effective choice for graph representation given its ability to capture global graph properties. However, its direct application poses challenges. To address these, the authors propose using neural networks to offer the Lovász principle. They also present an enhanced Lovász principle capable of directly and efficiently utilizing subgraph Lovász numbers. Experimental results demonstrate competitive performance of these Lovász principles in comparison to the baselines in both unsupervised and semi-supervised graph-level representation learning tasks.","1. The authors' proposal of Lovász theta kernels for graph representation learning shows great promise, thanks to its theoretical foundation.
2. The paper includes thorough experiments conducted in various settings, including unsupervised, semi-supervised, and transfer learning. These experiments provide strong evidence of the effectiveness of the proposed methods.","1. Although the author proposed a new graph kernel, the contribution seems slightly small to me.
2. The author did not demonstrate the framework of the model.
3. Some classic baselines were not compared.","1. I strongly suggest that the author discuss the application of graph kernel in updated scenarios, such as graph prompt learning. It is highly recommended that the authors consider citing the prompt learning on graph[1,2] paper and explore its application in conjunction with the proposed method in this paper, especially for unsupervised settings. This would enhance the discussion and potential application of prompt-based models.

 [1] Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Z Liu, X Yu, Y Fang, X Zhang - Proceedings of the ACM Web Conference 2023, 2023

 [2] Sun, M., Zhou, K., He, X., Wang, Y. and Wang, X., 2022, August. Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.

2. Providing a framework of the proposed model can indeed enhance the clarity of its functionality.
3. Why semi-supervised learning baseliens such as GCN, GAT, GIN are not compared in the paper?",See weaknesses.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688549634615,,,NeurIPS.cc/2023/Conference,0vdEHDwamk,"['NeurIPS.cc/2023/Conference/Submission4298/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410946839,[],3,,,['everyone'],0vdEHDwamk,['NeurIPS.cc/2023/Conference/Submission4298/Reviewer_4DKt'],1688549634615,1702410946839,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4298/Reviewer_4DKt']","Yes, the peer review explicitly suggests the authors of the paper to cite two specific literature:

1. ""Graphprompt: Unifying pre-training and downstream tasks for graph neural networks"" by Z Liu, X Yu, Y Fang, X Zhang (2023)
2. ""Gppt: Graph pre-training and prompt tuning to generalize graph neural networks"" by Sun, M., Zhou, K., He, X., Wang, Y. and Wang, X. (2022)

The reviewer recommends that the authors consider citing these papers and explore the application of prompt-based models in conjunction with the proposed method, especially for unsupervised settings.",1,"2023, 2022",Gppt Graph pre-training and prompt tuning to generalize graph neural networks
J2z1RvNm3g,"This submission presents a new approach to graph-level representation learning inspired by the Lovász number in graph theory. Specifically, the study proposes using the Lovász principle as a novel framework for unsupervised/semi-supervised graph representation learning. It offers a method to utilize handle vectors, which capture a graph's global features through neural networks. An enhanced Lovász principle is also proposed, which efficiently uses subgraph Lovász numbers, thereby ensuring similar graphs have similar representations. Experimental results show that the Lovász principles outperform other graph representation methods (such as InfoMax) in unsupervised learning, semi-supervised learning, and transfer learning. Therefore, the Lovász principles offer a promising new approach to graph-level representation learning.","There are three main strengths:

1. A new approach based on the Lovász principle is proposed for graph-level representation learning. This principle, inspired by the Lovász number in graph theory, adds a new perspective and strategy to graph learning, shifting from traditional methods.

2. Some subgraph tricks are used to enhance the performance of Loasz-based GNN models.

3. The Lovász principles outperform other methods in experiments on unsupervised learning, semi-supervised learning, and transfer learning. 
","Weaknesses:

1. Compared with current GNN models, such as Infomax-based and kernel-based methods, there are indeed some gains in terms of ACC on the task of graph classification. However, the overall gained performance in Table 1,2, and 3 is not very significant, especially when you consider the variances. This may question whether Lovász Principle can provide more useful global information than InfoMax. The other concern is that it is unclear how the approximation affects the whole performance since the final vectors used in the proposed models are approximated.

2. In terms of the novelty of using the Lovász Principle for designing GNN models, one of my main concerns is that this idea is not very novel. There is a lack of discussion on the difference between current work and previous works. See related works in [1]. In terms of run-time comparison, there is no significant reduction compared with InfoMax based on the run-time table in the appendix.

[1] Yadav, P., Nimishakavi, M., Yadati, N., Vashishth, S., Rajkumar, A. and Talukdar, P., 2019, April. Lovasz convolutional networks. In The 22nd international conference on artificial intelligence and statistics (pp. 1978-1987). PMLR.

","Overall, this paper has a good idea of using the Lovasz Principle (the handle vector) to present the graph and incorporate this idea to design GNN models. However, there is some related work, and the authors should have discussed it in the submission.

Q1: How could you deal with large-size graphs? For example, the datasets RDT-B and RDT-M5K were used in [2]. My concern is that it is unclear how you can estimate C_{S_i,S_j} accurately when n, the number of nodes in the graph, is large. 

Q2. Compared with InfoMax, why the performance of InfoMax-based (in terms of best ones) are very close to the proposed? More discussions are needed.

[2] Sun, Fan-Yun, Jordan Hoffmann, Vikas Verma, and Jian Tang. ""Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization."" arXiv preprint arXiv:1908.01000 (2019).

Some minors:

1. It is helpful to list out all dataset statistics for people who are unfamiliar with this area.

",None.,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688698090178,,,NeurIPS.cc/2023/Conference,0vdEHDwamk,"['NeurIPS.cc/2023/Conference/Submission4298/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410946694,[],5,,,['everyone'],0vdEHDwamk,['NeurIPS.cc/2023/Conference/Submission4298/Reviewer_JpeL'],1688698090178,1702410946694,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4298/Reviewer_JpeL']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific papers:

1. Yadav, P., Nimishakavi, M., Yadati, N., Vashishth, S., Rajkumar, A. and Talukdar, P., 2019, April. Lovasz convolutional networks. In The 22nd international conference on artificial intelligence and statistics (pp. 1978-1987). PMLR. (referred to as [1])
2. Sun, Fan-Yun, Jordan Hoffmann, Vikas Verma, and Jian Tang. ""Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization."" arXiv preprint arXiv:1908.01000 (2019). (referred to as [2])

The reviewer suggests that the authors should discuss the related work, particularly the paper by Yadav et al. (2019), to clarify the novelty of their approach.",1,"2019, 1978, 1987, 2019, 2019, 2019",Lovasz convolutional networks 
J2z1RvNm3g,"This submission presents a new approach to graph-level representation learning inspired by the Lovász number in graph theory. Specifically, the study proposes using the Lovász principle as a novel framework for unsupervised/semi-supervised graph representation learning. It offers a method to utilize handle vectors, which capture a graph's global features through neural networks. An enhanced Lovász principle is also proposed, which efficiently uses subgraph Lovász numbers, thereby ensuring similar graphs have similar representations. Experimental results show that the Lovász principles outperform other graph representation methods (such as InfoMax) in unsupervised learning, semi-supervised learning, and transfer learning. Therefore, the Lovász principles offer a promising new approach to graph-level representation learning.","There are three main strengths:

1. A new approach based on the Lovász principle is proposed for graph-level representation learning. This principle, inspired by the Lovász number in graph theory, adds a new perspective and strategy to graph learning, shifting from traditional methods.

2. Some subgraph tricks are used to enhance the performance of Loasz-based GNN models.

3. The Lovász principles outperform other methods in experiments on unsupervised learning, semi-supervised learning, and transfer learning. 
","Weaknesses:

1. Compared with current GNN models, such as Infomax-based and kernel-based methods, there are indeed some gains in terms of ACC on the task of graph classification. However, the overall gained performance in Table 1,2, and 3 is not very significant, especially when you consider the variances. This may question whether Lovász Principle can provide more useful global information than InfoMax. The other concern is that it is unclear how the approximation affects the whole performance since the final vectors used in the proposed models are approximated.

2. In terms of the novelty of using the Lovász Principle for designing GNN models, one of my main concerns is that this idea is not very novel. There is a lack of discussion on the difference between current work and previous works. See related works in [1]. In terms of run-time comparison, there is no significant reduction compared with InfoMax based on the run-time table in the appendix.

[1] Yadav, P., Nimishakavi, M., Yadati, N., Vashishth, S., Rajkumar, A. and Talukdar, P., 2019, April. Lovasz convolutional networks. In The 22nd international conference on artificial intelligence and statistics (pp. 1978-1987). PMLR.

","Overall, this paper has a good idea of using the Lovasz Principle (the handle vector) to present the graph and incorporate this idea to design GNN models. However, there is some related work, and the authors should have discussed it in the submission.

Q1: How could you deal with large-size graphs? For example, the datasets RDT-B and RDT-M5K were used in [2]. My concern is that it is unclear how you can estimate C_{S_i,S_j} accurately when n, the number of nodes in the graph, is large. 

Q2. Compared with InfoMax, why the performance of InfoMax-based (in terms of best ones) are very close to the proposed? More discussions are needed.

[2] Sun, Fan-Yun, Jordan Hoffmann, Vikas Verma, and Jian Tang. ""Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization."" arXiv preprint arXiv:1908.01000 (2019).

Some minors:

1. It is helpful to list out all dataset statistics for people who are unfamiliar with this area.

",None.,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688698090178,,,NeurIPS.cc/2023/Conference,0vdEHDwamk,"['NeurIPS.cc/2023/Conference/Submission4298/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410946694,[],5,,,['everyone'],0vdEHDwamk,['NeurIPS.cc/2023/Conference/Submission4298/Reviewer_JpeL'],1688698090178,1702410946694,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission4298/Reviewer_JpeL']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific papers:

1. Yadav, P., Nimishakavi, M., Yadati, N., Vashishth, S., Rajkumar, A. and Talukdar, P., 2019, April. Lovasz convolutional networks. In The 22nd international conference on artificial intelligence and statistics (pp. 1978-1987). PMLR. (referred to as [1])
2. Sun, Fan-Yun, Jordan Hoffmann, Vikas Verma, and Jian Tang. ""Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization."" arXiv preprint arXiv:1908.01000 (2019). (referred to as [2])

The reviewer suggests that the authors should discuss the related work, particularly the paper by Yadav et al. (2019), to clarify the novelty of their approach.",1,"2019, 1978, 1987, 2019, 2019, 2019",Infograph Unsupervised and semi-supervised graph-level representation learning via mutual information maximization
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Optnet Differentiable optimization as a layer in neural networks 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Melding the data-decisions pipeline Decision focused learning for combinatorial optimization 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Interior point solving for LP-based prediction+optimization 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",PyEPO A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Faster predict-and-optimize using three-operator splitting 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Backpropagation through combinatorial solvers Identity with projection works 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Learning with differentiable perturbed optimizers 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Differentiation of blackbox combinatorial solvers 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Faster predict-and-optimize using three-operator splitting 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Model-agnostic meta-learning for fast adaptation of deep networks 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Predict+Optimize for packing and covering LPs with unknown parameters in constraints 
wlV5sT7hEC,"Predict+Optimize is an emerging paradigm that lies in the intersection of classical optimization (particularly mixed integer programming) and machine learning. Specifically, it considers the setting where a parameterized optimization problem:
$$ x^{\star}(\theta) = \operatorname*{argmin}_{x} f(x;\theta) \text{ subject to } C(x;\theta) $$
must be solved yet the parameters $\theta$ are unknown. Here $C(x;\theta)$ represents constraints that must be respected, which could be equality, inequality, or inclusion. In the last 4--5 years this topic has seen heightened interest. Most approaches, including the one proposed in this paper, assume access to observed features $A$ that correlate with $\theta$, and aim to learn a predictor $f(A)$ so that $\hat{\theta} := f(A) \approx \theta$. Then, a problem of the form above is solved with $\hat{\theta}$ in place of $\theta$ to yield a predicted minimizer $x^{\star}(\hat{\theta})$. Hopefully,  $x^{\star}(\hat{\theta}) \approx x^{\star}(\theta)$.


The paper in question is one of the first to actively consider the setting where the constraints depend on $\theta$; prior work mostly considered constraints of the form $C(x)$. The method proposed is interesting, intuitive and general. They start from the observation that any scheme handling parametrized constraints must be allowed to make post-hoc adjustments to $x^{\star}(\hat{\theta})$  after the true parameters $\theta$ are revealed, as $x^{\star}(\hat{\theta})$ might not be feasible. Thus, they propose to solve a second, penalized problem:

$$ x_2^{\star} = \operatorname{argmin}_x f(x;\theta) + r(x^{\star}(\hat{\theta}), x) \text{ subject to } C(x;\theta) $$

where $r(\bullet, \bullet)$ penalizes the discrepancy between the stage 1 solution $x^{\star}(\hat{\theta})$ and the stage 2 solution $x_2^{\star}$.  

The bulk of the paper is devoted to motivating, introducing and implementing this proposed new method. Numerical experiments illustrate that this new method performs well. Finally, there is a little bit of theory (contained in the appendix) relating this new approach to older approaches to predict+optimize with parametrized constraint sets.


 ","- I agree with the assertion in the appendix that the proposed approach is _simple and powerful_. I think this is an elegant solution to an important problem.
- The paper is well-written; I did not find any typos or unclear sentences. The running example of stocking a store is useful.
- I appreciated the choice of experiments---they are closer to ""real-world"" problems than many experiments I have encountered in Neurips papers.
- The Appendices are thoughtfully written, addressing many potential reader's questions and providing several nice extensions of the model at hand.
- The framework proposed is indeed original, but I think some additional comparisons with existing literature is needed, see ""weaknesses"".


","- With regard to novelty, I think the authors should compare their work to various differentiable quadratic program (QP) solvers. For example, in [1] formulas for the derivatives of $x^{\star}$ with respect to constraint (_e.g._ $\partial x^{\star}/\partial A$, to reference the notation of your equation (5)) are given. Note that the MILP in (4) can be turned into a QP by using a quadratic regularizer (as done in [2]) instead of the logarithmic regularizer of Mandi & Guns.

 - I found the use of the name ""Two-stage Predict+Optimize"" for your proposed framework a bit confusing. In much of the Predict+Optimize literature, the two-stage approach refers to the approach of training a predictor $f(A)$ to minimize the mean square error $\|f(A) - \theta\|^2$ (see for example [3]) I strongly suggest you add a remark  discussing this towards the beginning of your paper.

- I think the benchmarks considered are a little weak. For example, all 5 classical regression methods are slight variations on the ""two-stage approach"" mentioned above. You should include some SOTA approaches for (one-stage) predict+optimize, e.g. training with the SPO+ loss, Perturbed Optimization [7], blackbox backpropagation [8] etc or justify why such approaches are incompatible with the proposed Two-Stage Predict+Optimize framework. The PyEPO benchmarking suite [4] could be useful.

- I'd like to see more discussion on the computational cost of your proposed approach. Given that it is a tri-level (!) problem, I think this is something to address. For the experiments, could you indicate the dimension of $x$ and the number of constraints, as well as the time required to train using the 2S and IntOpt-C approaches? It might be nice to add a remark about possible approaches to scaling your proposed framework, see [5,6].

[1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.

[2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019

[3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020

[4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022

[5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022

[7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020

[8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019

","- The two-stage nature of your proposed approach reminds me a lot of Model-agnostic Meta-Learning (MAML) [2]. Moreover, I think there is an interesting analogy between the connection between your work and that of Hu et al [3] and the connection between MAML and later implicit MAML (I-MAML) works [4]. I think the appeal of your work could be broadened by adding a discussion of this connection.

- An interesting variant of predict+optimize is the setting where at train time the true cost vectors ($c$ in the notation of your equation (4)) are never revealed, rather only the true solutions $x^{\star}$ are accessible (see [1] for a discussion on this). Do you have any thoughts on how to extend your approach to this setting?

[1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023

[2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017

[3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022

[4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

-----

After discussion with the authors I've raised my score 6--> 7

-------","As mentioned above, I think the computational complexity of the proposed approach could be a limitation. This should be discussed.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688417072149,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021695,[],1,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a'],1688417072149,1702411021695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Jj2a']","Yes, the peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. [1] _Optnet: Differentiable optimization as a layer in neural networks_ Amos & Kolter, 2017.
2. [2] _Melding the data-decisions pipeline: Decision focused learning for combinatorial optimization_ Wilder _et al_, 2019
3. [3] _Interior point solving for LP-based prediction+optimization_ Mandi & Guns, 2020
4. [4] _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ Tang & Khalil 2022
5. [5] _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023
6. [6] _Backpropagation through combinatorial solvers: Identity with projection works_, Sahoo _et al_, 2022
7. [7] _Learning with differentiable perturbed optimizers_ Berthet _et al_, 2020
8. [8] _Differentiation of blackbox combinatorial solvers_ Vlastelica _et al_, 2019
9. [1]  _Faster predict-and-optimize using three-operator splitting_ McKenzie _et al_, 2023 (again, in a different context)
10. [2] _Model-agnostic meta-learning for fast adaptation of deep networks_ Finn _et al_, 2017
11. [3] _Predict+Optimize for packing and covering LPs with unknown parameters in constraints_ Hu _et al_ 2022
12. [4] _Meta-learning with implicit gradients_ Rajeswaran _et al_, 2019.

The reviewer suggests that the authors should compare their work to these existing literature and discuss the connections and differences between their approach and these prior works.",1,"2017, 2019, 2020, 2022, 2023, 2022, 2020, 2019, 2023, 2017, 2022, 2019",Meta-learning with implicit gradients
Zb1mx0tjSw,"The paper presents a novel '2-stage' framework for Predict+Optimize with uncertain parameters in the constraints. In the first ""stage"" a soft commitment is made based on the predictions, and in the second, the commitment is updated based on updated information in such a way that the objective value plus a penalty for deviating from the commitment is minimized. This generalizes the framework of Hu et al. [9] ","1. Great idea: I find the new framework simple and sensible, I think this is the right framework for Predict+Optimize with unknown constraints. The biggest plus is that it allows for improving soft commitments in which constraints aren't violated, which is not something that Hu et al. allow for, but makes a lot of practical sense. It also removes the need to create a differentiable projection.
1. Clarity: The paper is well written and covers most bases (the tables could be bigger, though!)","1. Requires linear penalties: The penalties have to be linear in the decision variables. Hu et al. [9] doesn't require that.
1. Is slower than Hu et al.: Because Hu et al. only require running a differentiable projection, which can be quite cheap, it is cheaper than 2S (Appendix E confirms this).
1. Generalization of Hu et al.'s framework: While Hu et al. propose a specific projection that is limited to packing and constraint problems, there has been work in the literature about differential projections for enforcing feasibility constraints generally [A] and efficiently [B] that could be used instead. It would have been great to compare against those in the experiments.
1. I understand that the space is limited, but a lot of important information, like the description of the experiments and runtimes are in the Appendix. It would have been useful to have summaries in the main text...

_References:_

[A] Chen, Bingqing, et al. ""Enforcing policy feasibility constraints through differentiable projection for energy optimization."" Proceedings of the Twelfth ACM International Conference on Future Energy Systems. 2021.

[B] Sanket, Shah, et al. ""Solving online threat screening games using constrained action space reinforcement learning."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 02. 2020.","1. In Table 3, the difference between 2S and the other methods *decreases* as the penalty factor increases. Why is that? I would have expected the opposite (as in Tables 2 and 4).
1. On line 355 the paper says, ""On the other hand, the advantage of our 2S method over other approaches actually becomes more significant as the capacity increases, demonstrating the superior accuracy of our approach."" However, the difference between 2S and other approaches for the lowest penalty factor actually decreases in absolute value?","The limitations are not described, despite having responded 'yes'. The fact that the paper does not acknowledge any of these limitations highlighted in the `weaknesses' section is worrying.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688570792381,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021608,[],2,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Pw5Y'],1688570792381,1702411021608,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Pw5Y']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific references, [A] and [B], which are:

[A] Chen, Bingqing, et al. ""Enforcing policy feasibility constraints through differentiable projection for energy optimization."" Proceedings of the Twelfth ACM International Conference on Future Energy Systems. 2021.

[B] Sanket, Shah, et al. ""Solving online threat screening games using constrained action space reinforcement learning."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 02. 2020.

The reviewer suggests that the authors should have compared their method to the approaches presented in these references, which propose differentiable projections for enforcing feasibility constraints.",1,"2021, 2020",Enforcing policy feasibility constraints through differentiable projection for energy optimization 
Zb1mx0tjSw,"The paper presents a novel '2-stage' framework for Predict+Optimize with uncertain parameters in the constraints. In the first ""stage"" a soft commitment is made based on the predictions, and in the second, the commitment is updated based on updated information in such a way that the objective value plus a penalty for deviating from the commitment is minimized. This generalizes the framework of Hu et al. [9] ","1. Great idea: I find the new framework simple and sensible, I think this is the right framework for Predict+Optimize with unknown constraints. The biggest plus is that it allows for improving soft commitments in which constraints aren't violated, which is not something that Hu et al. allow for, but makes a lot of practical sense. It also removes the need to create a differentiable projection.
1. Clarity: The paper is well written and covers most bases (the tables could be bigger, though!)","1. Requires linear penalties: The penalties have to be linear in the decision variables. Hu et al. [9] doesn't require that.
1. Is slower than Hu et al.: Because Hu et al. only require running a differentiable projection, which can be quite cheap, it is cheaper than 2S (Appendix E confirms this).
1. Generalization of Hu et al.'s framework: While Hu et al. propose a specific projection that is limited to packing and constraint problems, there has been work in the literature about differential projections for enforcing feasibility constraints generally [A] and efficiently [B] that could be used instead. It would have been great to compare against those in the experiments.
1. I understand that the space is limited, but a lot of important information, like the description of the experiments and runtimes are in the Appendix. It would have been useful to have summaries in the main text...

_References:_

[A] Chen, Bingqing, et al. ""Enforcing policy feasibility constraints through differentiable projection for energy optimization."" Proceedings of the Twelfth ACM International Conference on Future Energy Systems. 2021.

[B] Sanket, Shah, et al. ""Solving online threat screening games using constrained action space reinforcement learning."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 02. 2020.","1. In Table 3, the difference between 2S and the other methods *decreases* as the penalty factor increases. Why is that? I would have expected the opposite (as in Tables 2 and 4).
1. On line 355 the paper says, ""On the other hand, the advantage of our 2S method over other approaches actually becomes more significant as the capacity increases, demonstrating the superior accuracy of our approach."" However, the difference between 2S and other approaches for the lowest penalty factor actually decreases in absolute value?","The limitations are not described, despite having responded 'yes'. The fact that the paper does not acknowledge any of these limitations highlighted in the `weaknesses' section is worrying.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688570792381,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021608,[],2,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Pw5Y'],1688570792381,1702411021608,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Pw5Y']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific references, [A] and [B], which are:

[A] Chen, Bingqing, et al. ""Enforcing policy feasibility constraints through differentiable projection for energy optimization."" Proceedings of the Twelfth ACM International Conference on Future Energy Systems. 2021.

[B] Sanket, Shah, et al. ""Solving online threat screening games using constrained action space reinforcement learning."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 02. 2020.

The reviewer suggests that the authors should have compared their method to the approaches presented in these references, which propose differentiable projections for enforcing feasibility constraints.",1,"2021, 2020",Solving online threat screening games using constrained action space reinforcement learning
FRBBwo6x7y,"The authors propose a framework for learning latent variables in optimization problems that appear both in the constraints and objective. In this setting, the user is given features and asked to provide a solution to an optimization problem where the objective and constraints of the optimization problem are partially observed and related to the features. Additionally, the user can optimally modify the solution once the true parameters are known at a cost based on the change. The overall goal is to ensure that the total regret is low where the regret is the total value of the final solution after fixing minus the fixing cost and minus the objective value of the optimal solution in hindsight. The authors propose that the second stage solution should be considered the output of an optimization problem which is given the first stage solution as input and then backpropagates through both optimization problems to update the weights of the predictive model predicting parameters for the first stage optimization problem. The authors propose differentiating through continuous relaxations of these optimization problems using previous work that differentiates through iterates of an interior point method.

The authors evaluate their approach on several settings to demonstrate improved predictive performance over the investigates baselines.
","The main strength of the work is that it considers penalizing the recourse using a flexible optimization problem rather than having a domain-dependent method of doing so as was done in previous work for packing and covering. Additionally, the paper itself is easy to read and ","Given that the main contribution of this work is that the framework has good empirical performance, it would be good to strengthen the experiments by evaluating against relevant baselines.

The work does not compare against relevant baselines and claiming generality to MILP to discount several differentiable continuous optimization baselines when in practice, the proposed approach simply relaxes the integrality constraints to consider differentiating through a continuous LP. Given that this approach considers differentiation of MILP with respect to the constraints as simply differentiating the constraints of the LP relaxation, the authors should evaluate against methods which can differentiate through constraints of an LP which includes cvxpylayers [1], and OptNet with quadratic regularization to differentiate through LP [2,3].

Additionally, the previous CombOptNet work [19 in the paper] which is formulated explicitly for learning constraints in combinatorial settings, and whose datasets are used for two of the three settings, is not compared against.


[1] Agrawal, Akshay, et al. ""Differentiable convex optimization layers."" Advances in neural information processing systems 32 (2019).
[2] Amos, Brandon, and J. Zico Kolter. ""Optnet: Differentiable optimization as a layer in neural networks."" International Conference on Machine Learning. PMLR, 2017.
[3] Wilder, Bryan, Bistra Dilkina, and Milind Tambe. ""Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.
","Are there any specific reasons that previous work that learns constraints for LP or comboptnet are not applicable to the investigated settings?

Are there any specific components of this method which are specialized to handle integrality that cannot be handled by simply applying previous approaches for differentiating through continuous problems?

It might be helpful to compare the gradients of this approach on a subjective level as well. In equation (3) are the gradients from one component relatively large compared to gradients from the other? How do they differ overall from previous work?
",Relevant limitations are addressed.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688655119192,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021529,[],3,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Xm8w'],1688655119192,1702411021529,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Xm8w']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references, including:

* Agrawal et al. (2019) - ""Differentiable convex optimization layers"" ([1])
* Amos and Kolter (2017) - ""Optnet: Differentiable optimization as a layer in neural networks"" ([2])
* Wilder et al. (2019) - ""Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization"" ([3])

The reviewer suggests that the authors should evaluate their approach against methods presented in these papers, such as cvxpylayers, OptNet with quadratic regularization, and CombOptNet, to strengthen the experiments and demonstrate the generality of their framework.",1,"2019, 2017, 2019",Differentiable convex optimization layers 
FRBBwo6x7y,"The authors propose a framework for learning latent variables in optimization problems that appear both in the constraints and objective. In this setting, the user is given features and asked to provide a solution to an optimization problem where the objective and constraints of the optimization problem are partially observed and related to the features. Additionally, the user can optimally modify the solution once the true parameters are known at a cost based on the change. The overall goal is to ensure that the total regret is low where the regret is the total value of the final solution after fixing minus the fixing cost and minus the objective value of the optimal solution in hindsight. The authors propose that the second stage solution should be considered the output of an optimization problem which is given the first stage solution as input and then backpropagates through both optimization problems to update the weights of the predictive model predicting parameters for the first stage optimization problem. The authors propose differentiating through continuous relaxations of these optimization problems using previous work that differentiates through iterates of an interior point method.

The authors evaluate their approach on several settings to demonstrate improved predictive performance over the investigates baselines.
","The main strength of the work is that it considers penalizing the recourse using a flexible optimization problem rather than having a domain-dependent method of doing so as was done in previous work for packing and covering. Additionally, the paper itself is easy to read and ","Given that the main contribution of this work is that the framework has good empirical performance, it would be good to strengthen the experiments by evaluating against relevant baselines.

The work does not compare against relevant baselines and claiming generality to MILP to discount several differentiable continuous optimization baselines when in practice, the proposed approach simply relaxes the integrality constraints to consider differentiating through a continuous LP. Given that this approach considers differentiation of MILP with respect to the constraints as simply differentiating the constraints of the LP relaxation, the authors should evaluate against methods which can differentiate through constraints of an LP which includes cvxpylayers [1], and OptNet with quadratic regularization to differentiate through LP [2,3].

Additionally, the previous CombOptNet work [19 in the paper] which is formulated explicitly for learning constraints in combinatorial settings, and whose datasets are used for two of the three settings, is not compared against.


[1] Agrawal, Akshay, et al. ""Differentiable convex optimization layers."" Advances in neural information processing systems 32 (2019).
[2] Amos, Brandon, and J. Zico Kolter. ""Optnet: Differentiable optimization as a layer in neural networks."" International Conference on Machine Learning. PMLR, 2017.
[3] Wilder, Bryan, Bistra Dilkina, and Milind Tambe. ""Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.
","Are there any specific reasons that previous work that learns constraints for LP or comboptnet are not applicable to the investigated settings?

Are there any specific components of this method which are specialized to handle integrality that cannot be handled by simply applying previous approaches for differentiating through continuous problems?

It might be helpful to compare the gradients of this approach on a subjective level as well. In equation (3) are the gradients from one component relatively large compared to gradients from the other? How do they differ overall from previous work?
",Relevant limitations are addressed.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688655119192,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021529,[],3,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Xm8w'],1688655119192,1702411021529,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Xm8w']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references, including:

* Agrawal et al. (2019) - ""Differentiable convex optimization layers"" ([1])
* Amos and Kolter (2017) - ""Optnet: Differentiable optimization as a layer in neural networks"" ([2])
* Wilder et al. (2019) - ""Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization"" ([3])

The reviewer suggests that the authors should evaluate their approach against methods presented in these papers, such as cvxpylayers, OptNet with quadratic regularization, and CombOptNet, to strengthen the experiments and demonstrate the generality of their framework.",1,"2019, 2017, 2019",Optnet 
FRBBwo6x7y,"The authors propose a framework for learning latent variables in optimization problems that appear both in the constraints and objective. In this setting, the user is given features and asked to provide a solution to an optimization problem where the objective and constraints of the optimization problem are partially observed and related to the features. Additionally, the user can optimally modify the solution once the true parameters are known at a cost based on the change. The overall goal is to ensure that the total regret is low where the regret is the total value of the final solution after fixing minus the fixing cost and minus the objective value of the optimal solution in hindsight. The authors propose that the second stage solution should be considered the output of an optimization problem which is given the first stage solution as input and then backpropagates through both optimization problems to update the weights of the predictive model predicting parameters for the first stage optimization problem. The authors propose differentiating through continuous relaxations of these optimization problems using previous work that differentiates through iterates of an interior point method.

The authors evaluate their approach on several settings to demonstrate improved predictive performance over the investigates baselines.
","The main strength of the work is that it considers penalizing the recourse using a flexible optimization problem rather than having a domain-dependent method of doing so as was done in previous work for packing and covering. Additionally, the paper itself is easy to read and ","Given that the main contribution of this work is that the framework has good empirical performance, it would be good to strengthen the experiments by evaluating against relevant baselines.

The work does not compare against relevant baselines and claiming generality to MILP to discount several differentiable continuous optimization baselines when in practice, the proposed approach simply relaxes the integrality constraints to consider differentiating through a continuous LP. Given that this approach considers differentiation of MILP with respect to the constraints as simply differentiating the constraints of the LP relaxation, the authors should evaluate against methods which can differentiate through constraints of an LP which includes cvxpylayers [1], and OptNet with quadratic regularization to differentiate through LP [2,3].

Additionally, the previous CombOptNet work [19 in the paper] which is formulated explicitly for learning constraints in combinatorial settings, and whose datasets are used for two of the three settings, is not compared against.


[1] Agrawal, Akshay, et al. ""Differentiable convex optimization layers."" Advances in neural information processing systems 32 (2019).
[2] Amos, Brandon, and J. Zico Kolter. ""Optnet: Differentiable optimization as a layer in neural networks."" International Conference on Machine Learning. PMLR, 2017.
[3] Wilder, Bryan, Bistra Dilkina, and Milind Tambe. ""Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.
","Are there any specific reasons that previous work that learns constraints for LP or comboptnet are not applicable to the investigated settings?

Are there any specific components of this method which are specialized to handle integrality that cannot be handled by simply applying previous approaches for differentiating through continuous problems?

It might be helpful to compare the gradients of this approach on a subjective level as well. In equation (3) are the gradients from one component relatively large compared to gradients from the other? How do they differ overall from previous work?
",Relevant limitations are addressed.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688655119192,,,NeurIPS.cc/2023/Conference,0tnhFpyWjb,"['NeurIPS.cc/2023/Conference/Submission5675/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411021529,[],3,,,['everyone'],0tnhFpyWjb,['NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Xm8w'],1688655119192,1702411021529,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5675/Reviewer_Xm8w']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions several references, including:

* Agrawal et al. (2019) - ""Differentiable convex optimization layers"" ([1])
* Amos and Kolter (2017) - ""Optnet: Differentiable optimization as a layer in neural networks"" ([2])
* Wilder et al. (2019) - ""Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization"" ([3])

The reviewer suggests that the authors should evaluate their approach against methods presented in these papers, such as cvxpylayers, OptNet with quadratic regularization, and CombOptNet, to strengthen the experiments and demonstrate the generality of their framework.",1,"2019, 2017, 2019",Melding the data-decisions pipeline
YhNK4NnzSv,"The paper introduces TPSR, a Transformer-based Planning strategy for Symbolic Regression. TPSR incorporates Monte Carlo Tree Search into the transformer decoding process, enabling the integration of non-differentiable feedback such as accuracy and complexity. Experimental results show that TPSR outperforms existing methods in terms of fitting-complexity trade-off, extrapolation abilities, and robustness to noise.","- The paper is well written and easy to understand.
- The idea of enhancing large scale pre-trained Transformers with improved search capablities is very promising in the context of symbolic regression
- The model shows good performance both compared to the E2E baseline and the GP methods.","- My main concern is about the novelty of the approach. A very similar idea has been recently investigated in [1] where the authors also combine MCTS with pre-trained Transformers. I would be grateful if the authors could clarify any eventual differences between the two approaches.
- The impact of $\lambda$ seems quite significant in your experiements. However, it is not clear to me how one should select it in practice.


[1] Kamienny, Pierre-Alexandre, Guillaume Lample, and Marco Virgolin. ""Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search."" (2023).",Please refer to the weakness part above.,,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688295200705,,,NeurIPS.cc/2023/Conference,0rVXQEeFEL,"['NeurIPS.cc/2023/Conference/Submission13018/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411410294,[],1,,,['everyone'],0rVXQEeFEL,['NeurIPS.cc/2023/Conference/Submission13018/Reviewer_ijKU'],1688295200705,1702411410294,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission13018/Reviewer_ijKU']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, namely [1] ""Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search"" by Kamienny, Pierre-Alexandre, Guillaume Lample, and Marco Virgolin (2023), in order to clarify the differences between the two approaches.",1,2023,Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search
pIntq1EXoH,This paper considers the benefit of leveraging sparsity to improve standard unlearning techniques. They empirically verify that across a wide range of datasets and architectures. the sparsity benefits unlearning. ,"1. The recap of this paper on the unlearning literature and metrics is very thorough and helpful for readers who are more unfamiliar with the setting of unlearning. 
2. The intuition of this paper is quite elegant and helpful. The address of different pruning methodologies is insightful.
3. The empirical results are seemingly quite promising. 

Overall, this paper uses a simple idea to yield strong benefits. While a theoretical analysis would have been nice, the paper is strong as it is. I vote for acceptance strongly then. 

I also believe this work is missing a citation. Pruning has also been shown in in its relation to generalization error (see ""Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching""). This, however, does not affect my score. ","1. There is a slight inconsistency with the empirical results. There are times when sparsity significantly hurts the unlearning. There does not seem to be an observable pattern to this. I believe such an analysis is necessary.
2. Is it possible for the authors to conduct a small experiment to see how the effect of sparsity on unlearning scales with model size? For example, with varying Resnet sizes, does the effect of sparsity on unlearning change? I think this is a slight concern for me.
3. There are no theoretical analyses here. However, I won't be too harsh about this as the paper does mention this in the limitations. ","1. My main question is, do the readers have an intuition for when sparsity helps or hurts the unlearning process? Across several of the experiments, while it is true that sparsity generally improves unlearning, there are several cases where this is not the case. Are there patterns the authors noticed? For a practitioner, I think a deeper understanding of this is helpful.
2. It seems that most of the architectures are restricted to convolution-based architectures. Have the authors tried extending this to other architecture, such as transformers?","I would have liked it if the authors had recognized the slight inconsistency of their empirical results. However, on the whole, the limitations are addressed. ",['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,3 good,1686725833178,,,NeurIPS.cc/2023/Conference,0jZH883i34,"['NeurIPS.cc/2023/Conference/Submission8589/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411179127,[],1,,,['everyone'],0jZH883i34,['NeurIPS.cc/2023/Conference/Submission8589/Reviewer_8xJa'],1686725833178,1702411179127,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8589/Reviewer_8xJa']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely ""Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching"", which discusses the relation between pruning and generalization error.",1,,Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching
JyAsNNFA2c,"The paper proposes that model sparsity leads to models that are easier to ""unlearn from"". The authors discuss in depth the technical measures that are and that should be used to evaluate various methods of unlearning, and suggest and demonstrate that sparsity is an effective tool in boosting these measures across a wide variety of unlearning applications.","The paper strongly supports its main claim, with extensive discussion and experimental evaluation that shows beyond much doubt that model sparsity leads to easier and stronger unlearning results in practice.

The writing is extremely clear and all internal referencing and definitions help readers easily follow and understand the motivations and results. I particularly appreciate the use of emphasis and acronyms, and the way in which the paper makes it easy to flip back and forth to find where a term was defined or where a term is used.

Weaknesses and questions below notwithstanding, the paper is quite solid albeit with a narrow scope. I appreciate the time and detail that went into the experimental evaluations.
","I have two main concerns:

1) The authors don't seem to discuss or acknowledge that sparsity seems to lead to simply better (read: better generalizing) models, and as such we would expect better generalization to lead to less dependence on specific subsets of the training data, and thus easier unlearning. In essence, how does sparsity as a proxy for generalization aid unlearning more than other generalization methods? I would of course expect that strong regularization would affect performance, but perhaps other methods for better generalization that maintain performance may also ""simplify machine unlearning"".

2) My other main concern with this work is the sidelining of the body of work on $\epsilon-\delta$ forgetting. The authors reference this work in their Related Work section (probabilistic, DP), but there are many places within the main text up to that point where I was left wondering how those approaches may stack up. Simple methods such as Guo et al.'s [54] seem like an easy enough place to do a quick comparison. I acknowledge that those methods tend to depend heavily on hyperparameter choices (as stated by the authors), but a lot of that work seems highly relevant. The discussion in Section 2 around Proposition 1 is very suggestive and brings to mind work in Sekhari [57], and this citing paper seems to be trying to solve a similar problem as the authors here with approximating the Hessian inverse:
Deep Unlearning via Randomized Conditionally Independent Hessians. Ronak Mehta, Sourav Pal, Vikas Singh, Sathya N. Ravi. CVPR 2022.
(not major: I wonder if updating a subset of parameters is ""similar"" in some form to a sparsity approach?)","1. Why were approximate methods in eps-delta not compared? If the authors strongly feel that this is out of scope I think it needs to be adequately justified.
2. Classwise and random-data methods were primarily evaluated; why not individual samples?
3. Gradient norms are often used to evaluate removal success, any reason why there were excluded?
4. Some main paper experimental results were limited to 95% sparsity, and there are a large number of references to the appendix with additional results. 

Minor:
1. Depending on how the authors treat the approximate/DP/eps-delta, those might be included in the ""approximate MU methods"" in Section 2; I was concerned that this highly relevant work was not mentioned as I was reading through.
2. The metrics described in Section 2 are very similar to the ""read-out functions"" in [12], might be worth mentioning/referencing.
3. It could be helpful to clearly indicate ""higher is better"" ""lower is better"" for the various metrics, perhaps using simple up or down arrows, in the text and and in the tables. I do appreciate the authors detailed discussion of results however, careful reading covers all bases. Just thought it may help people skimming. ","1. It's unclear if sparsity-promoting methods help unlearning moreso than other methods that improve model performance generally.
2. A reasonable set of related work in the form of $(\epsilon,\delta)$ forgetting is largely left un-evaluated.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,4 excellent,4 excellent,1688750641196,,,NeurIPS.cc/2023/Conference,0jZH883i34,"['NeurIPS.cc/2023/Conference/Submission8589/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411178847,[],4,,,['everyone'],0jZH883i34,['NeurIPS.cc/2023/Conference/Submission8589/Reviewer_MsfJ'],1688750641196,1702411178847,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8589/Reviewer_MsfJ']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several specific papers and authors, such as:
- Guo et al.'s [54]
- Sekhari [57]
- Ronak Mehta, Sourav Pal, Vikas Singh, Sathya N. Ravi's work ""Deep Unlearning via Randomized Conditionally Independent Hessians"" (CVPR 2022)
- [12], which discusses ""read-out functions""

The reviewer also mentions the body of work on $\epsilon-\delta$ forgetting, which includes probabilistic and DP approaches, and suggests that the authors should compare their method with these approaches.",1,2022,Deep Unlearning via Randomized Conditionally Independent Hessians
cPohw3QX9S,"Brief Summary: The paper presents a new task Described Object Detection which extends open vocabulary object detection (OVD) to use phrases. This, in turn, extends referring expression (REC) to include objects not seen in the training data. To this end, the authors introduce a new dataset called D3 building on existing GRD dataset [44]. 

The key idea behind creating D3 are to have complete annotation (i.e. each referring expression has a bounding box in each image if present), have natural language (extends OVD), include absence expression (i.e. objects NOT having a particular feature / attribute such as blackboard with no signs), and one expression referring to more than one instance. 

The authors experiment on the provided D3 dataset and provide a detailed benchmark with multiple baselines ranging from REC methods like OFA, OVD methods like OWL-ViT and bifunctional method like Grounding-DINO. The authors further propose a new baseline OFA-DOD which changes some pre-training schemes such as including additional localization tasks and find that it outperforms competitive baselines.
","Pros:

1. Dataset contribution is always welcome. It is clear the authors have put thought into the dataset construction. In particular, absence expression are quite interesting.

2. A number of baselines are considered and the takeaways are quite interesting that REC methods fail at this task due to being unable to effectively choose more than one boxes. While OVD methods  perform better, there is a very large gap compared to bifunctional methods like grounding-dino. The proposed baseline OFA-DOD makes sense and it is good to see that it outperforms other baselines. 

3. Visualization of the dataset as well as results in suppl. are very useful. Ablative studies on the baseline such as effect of training data (Table 5b) are interesting.","Cons:

1. The authors should compare their work with zero-shot grounding [Ref1] which also extends REC to new objects. The obvious difference is that DOD can have more than one instances but a clear distinction would be helpful.

2. The main idea behind DOD is to encompass both OVD and REC. The authors need to motivate this setting more. In my opinion, having the two cases separate can be much more revealing than trying to combine the two. OVD is strictly object detection (with phrases) while REC explicitly requires disambiguation between different objects. For instance, ""oversized glove on left-hand"" in suppl fig 2 (last row), is simply object detection of glove and doesn't require reasoning whether it is ""oversized"" or on ""left-hand"". To me a more natural setting is to separate the two. I would like to know about the author's motivation for the task.

3. It is unclear in the text but in OFA-DOD which OFA is chosen, base or large? To have a fair comparison with Grounding-DINO it should be base but it is not clear (in main text as well as in suppl).

4. In suppl. Table 2, it seems Grounding-DINO outperforms OFA-DOD by considerable margin on Average Recall. Why is this the case? For practical use, wouldn't one prefer using Grounding-DINO?


[Ref1]: Sadhu, Arka, Kan Chen, and Ram Nevatia. ""Zero-shot grounding of objects from natural language queries."" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4694-4703. 2019.","See weakness section. 

","Yes, the authors have discussed some limitations but more could be included such as:

1. The evaluation is heavily dependent on the choice of 412 phrases and the dataset used for GRD. 

",['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,3 good,3 good,1688711267548,,,NeurIPS.cc/2023/Conference,0hwq2vOHT4,"['NeurIPS.cc/2023/Conference/Submission3467/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410900707,[],4,,,['everyone'],0hwq2vOHT4,['NeurIPS.cc/2023/Conference/Submission3467/Reviewer_SdJt'],1688711267548,1702410900707,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3467/Reviewer_SdJt']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

Sadhu, Arka, Kan Chen, and Ram Nevatia. ""Zero-shot grounding of objects from natural language queries."" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4694-4703. 2019.

This is mentioned in the weaknesses section, point 1, where the reviewer states that the authors should compare their work with zero-shot grounding, referencing this specific paper.",1,2019,Zero-shot grounding of objects from natural language queries
HDn9ab3fNo,"The paper proposes a task called described object detection which involves detecting objects through free form text queries, encompassing referring expression comprehension as well as open vocabulary detection.","## Clarity
The paper is written quite clearly.

## Originality, Quality and Significance: Please see weaknesses","# Major issues: 

## Quality [Annotating using CLIP is insufficient]
* DOD does not provide manually annotated explicit negative certificates for the images that are deemed as a negative for a given text query. The negatives are extracted using a CLIP matching score for each image with all the possible text queries. It has been demonstrated in several works that CLIP has no fine-grained understanding of the image [1,2,3], is largely incapable
of performing spatial reasoning off-the-shelf, obtaining chance performance even on simple synthetic images [1] and also behaves as a bag of words when understanding the text [2]. This implies that any complex query that is more than just a category name, cannot be appropriately distinguished as being relevant or not for a given image, and using this as a step in the annotation pipeline is sure to introduce errors. Using any sort of image-text model during the annotation process inherits the biases of the underlying model and in my opinion is not a viable approach for constructing an evaluation benchmark. For a benchmark that is characterized as a detection dataset, having accurate negatives is paramount, and the authors have not demonstrated that this criteria can be met using a CLIP matching in the annotation process. 
* Further, testing the ability of models to distinguish the presence or absence of a textual query, and localize it, would be truly tested in the cases of having unlikely phrases or combinations of objects and attributes or unlikely relations. Using CLIP in the annotation process would completely fail in these cases, as it has been shown that CLIP has a strong Concept Association Bias [3], frequently giving the highest matching score to the most likely completion, without paying attention to the image. This would further exacerbate the difficulty in accurately evaluating models on examples that might be especially hard and interesting (especially relevant to the more challenging ""absence"" type of queries).

## Originality, Significance [Proposed task is equivalent to existing benchmarks]
* As far as I can tell, the proposed task does not differ from the Phrase Detection task proposed by [5] which addresses the problem of both identifying whether the phrase is relevant to an image and also localizing the phrase, across a whole dataset. Departing from referring expression comprehension, they allow prediction of multiple boxes per phrase, and different from object detection, they evaluate text prompts that are longer than simple category names. Missing this line of literature completely is quite a red flag given that it has been around for quite some years now. 
* Another benchmark, ""COPS-Ref"" has also been proposed [6] that focuses on referring expressions with varying degrees of complexity and in which the localization must be done across multiple images, also containing distractors. This work also does not acknowledge or differentiate from COPS-Ref. 


[1] ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. Sanjay Subramania et al, 2022

[2] When and why vision-language models behave like bags-of-words, and what to do about it? Mert Yuksekgonul et al, 2023. 

[3] When are Lemons Purple? The Concept Association Bias of CLIP. Yutaro Yamada et al, 2022

[4] Revisiting Image-Language Networks for Open-ended Phrase Detection. Plummer et al, 2020

[5] Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension. Zhenfang Chen, 2020","## Suggestions 
* Line 103: ""Currently, OFA holds the SOTA among REC methods."" I believe this comment is a quite outdated and can be updated (joint OVD & REC methods such as FIBER [1] outperform it). 
* For the rebuttal, the authors could explain the difference between the proposed task and phrase detection.

[1]  Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. Dou et al, 2022",Yes,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,1 poor,3 good,1 poor,1689040028653,,,NeurIPS.cc/2023/Conference,0hwq2vOHT4,"['NeurIPS.cc/2023/Conference/Submission3467/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410900637,[],5,,,['everyone'],0hwq2vOHT4,['NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG'],1689040028653,1702410900637,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that the authors should be aware of, including:

1. ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension (Sanjay Subramania et al, 2022) [1]
2. When and why vision-language models behave like bags-of-words, and what to do about it? (Mert Yuksekgonul et al, 2023) [2]
3. When are Lemons Purple? The Concept Association Bias of CLIP (Yutaro Yamada et al, 2022) [3]
4. Revisiting Image-Language Networks for Open-ended Phrase Detection (Plummer et al, 2020) [4]
5. Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension (Zhenfang Chen, 2020) [5]
6. Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone (Dou et al, 2022) [1]

The reviewer suggests that the authors should cite these papers to demonstrate their awareness of the existing literature and to differentiate their proposed task from existing benchmarks.",1,"2022, 2023, 2022, 2020, 2020, 2022",ReCLIP A Strong Zero-Shot Baseline for Referring Expression Comprehension 
HDn9ab3fNo,"The paper proposes a task called described object detection which involves detecting objects through free form text queries, encompassing referring expression comprehension as well as open vocabulary detection.","## Clarity
The paper is written quite clearly.

## Originality, Quality and Significance: Please see weaknesses","# Major issues: 

## Quality [Annotating using CLIP is insufficient]
* DOD does not provide manually annotated explicit negative certificates for the images that are deemed as a negative for a given text query. The negatives are extracted using a CLIP matching score for each image with all the possible text queries. It has been demonstrated in several works that CLIP has no fine-grained understanding of the image [1,2,3], is largely incapable
of performing spatial reasoning off-the-shelf, obtaining chance performance even on simple synthetic images [1] and also behaves as a bag of words when understanding the text [2]. This implies that any complex query that is more than just a category name, cannot be appropriately distinguished as being relevant or not for a given image, and using this as a step in the annotation pipeline is sure to introduce errors. Using any sort of image-text model during the annotation process inherits the biases of the underlying model and in my opinion is not a viable approach for constructing an evaluation benchmark. For a benchmark that is characterized as a detection dataset, having accurate negatives is paramount, and the authors have not demonstrated that this criteria can be met using a CLIP matching in the annotation process. 
* Further, testing the ability of models to distinguish the presence or absence of a textual query, and localize it, would be truly tested in the cases of having unlikely phrases or combinations of objects and attributes or unlikely relations. Using CLIP in the annotation process would completely fail in these cases, as it has been shown that CLIP has a strong Concept Association Bias [3], frequently giving the highest matching score to the most likely completion, without paying attention to the image. This would further exacerbate the difficulty in accurately evaluating models on examples that might be especially hard and interesting (especially relevant to the more challenging ""absence"" type of queries).

## Originality, Significance [Proposed task is equivalent to existing benchmarks]
* As far as I can tell, the proposed task does not differ from the Phrase Detection task proposed by [5] which addresses the problem of both identifying whether the phrase is relevant to an image and also localizing the phrase, across a whole dataset. Departing from referring expression comprehension, they allow prediction of multiple boxes per phrase, and different from object detection, they evaluate text prompts that are longer than simple category names. Missing this line of literature completely is quite a red flag given that it has been around for quite some years now. 
* Another benchmark, ""COPS-Ref"" has also been proposed [6] that focuses on referring expressions with varying degrees of complexity and in which the localization must be done across multiple images, also containing distractors. This work also does not acknowledge or differentiate from COPS-Ref. 


[1] ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. Sanjay Subramania et al, 2022

[2] When and why vision-language models behave like bags-of-words, and what to do about it? Mert Yuksekgonul et al, 2023. 

[3] When are Lemons Purple? The Concept Association Bias of CLIP. Yutaro Yamada et al, 2022

[4] Revisiting Image-Language Networks for Open-ended Phrase Detection. Plummer et al, 2020

[5] Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension. Zhenfang Chen, 2020","## Suggestions 
* Line 103: ""Currently, OFA holds the SOTA among REC methods."" I believe this comment is a quite outdated and can be updated (joint OVD & REC methods such as FIBER [1] outperform it). 
* For the rebuttal, the authors could explain the difference between the proposed task and phrase detection.

[1]  Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. Dou et al, 2022",Yes,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,1 poor,3 good,1 poor,1689040028653,,,NeurIPS.cc/2023/Conference,0hwq2vOHT4,"['NeurIPS.cc/2023/Conference/Submission3467/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410900637,[],5,,,['everyone'],0hwq2vOHT4,['NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG'],1689040028653,1702410900637,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that the authors should be aware of, including:

1. ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension (Sanjay Subramania et al, 2022) [1]
2. When and why vision-language models behave like bags-of-words, and what to do about it? (Mert Yuksekgonul et al, 2023) [2]
3. When are Lemons Purple? The Concept Association Bias of CLIP (Yutaro Yamada et al, 2022) [3]
4. Revisiting Image-Language Networks for Open-ended Phrase Detection (Plummer et al, 2020) [4]
5. Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension (Zhenfang Chen, 2020) [5]
6. Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone (Dou et al, 2022) [1]

The reviewer suggests that the authors should cite these papers to demonstrate their awareness of the existing literature and to differentiate their proposed task from existing benchmarks.",1,"2022, 2023, 2022, 2020, 2020, 2022",When and why vision-language models behave like bags-of-words and what to do about it 
HDn9ab3fNo,"The paper proposes a task called described object detection which involves detecting objects through free form text queries, encompassing referring expression comprehension as well as open vocabulary detection.","## Clarity
The paper is written quite clearly.

## Originality, Quality and Significance: Please see weaknesses","# Major issues: 

## Quality [Annotating using CLIP is insufficient]
* DOD does not provide manually annotated explicit negative certificates for the images that are deemed as a negative for a given text query. The negatives are extracted using a CLIP matching score for each image with all the possible text queries. It has been demonstrated in several works that CLIP has no fine-grained understanding of the image [1,2,3], is largely incapable
of performing spatial reasoning off-the-shelf, obtaining chance performance even on simple synthetic images [1] and also behaves as a bag of words when understanding the text [2]. This implies that any complex query that is more than just a category name, cannot be appropriately distinguished as being relevant or not for a given image, and using this as a step in the annotation pipeline is sure to introduce errors. Using any sort of image-text model during the annotation process inherits the biases of the underlying model and in my opinion is not a viable approach for constructing an evaluation benchmark. For a benchmark that is characterized as a detection dataset, having accurate negatives is paramount, and the authors have not demonstrated that this criteria can be met using a CLIP matching in the annotation process. 
* Further, testing the ability of models to distinguish the presence or absence of a textual query, and localize it, would be truly tested in the cases of having unlikely phrases or combinations of objects and attributes or unlikely relations. Using CLIP in the annotation process would completely fail in these cases, as it has been shown that CLIP has a strong Concept Association Bias [3], frequently giving the highest matching score to the most likely completion, without paying attention to the image. This would further exacerbate the difficulty in accurately evaluating models on examples that might be especially hard and interesting (especially relevant to the more challenging ""absence"" type of queries).

## Originality, Significance [Proposed task is equivalent to existing benchmarks]
* As far as I can tell, the proposed task does not differ from the Phrase Detection task proposed by [5] which addresses the problem of both identifying whether the phrase is relevant to an image and also localizing the phrase, across a whole dataset. Departing from referring expression comprehension, they allow prediction of multiple boxes per phrase, and different from object detection, they evaluate text prompts that are longer than simple category names. Missing this line of literature completely is quite a red flag given that it has been around for quite some years now. 
* Another benchmark, ""COPS-Ref"" has also been proposed [6] that focuses on referring expressions with varying degrees of complexity and in which the localization must be done across multiple images, also containing distractors. This work also does not acknowledge or differentiate from COPS-Ref. 


[1] ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. Sanjay Subramania et al, 2022

[2] When and why vision-language models behave like bags-of-words, and what to do about it? Mert Yuksekgonul et al, 2023. 

[3] When are Lemons Purple? The Concept Association Bias of CLIP. Yutaro Yamada et al, 2022

[4] Revisiting Image-Language Networks for Open-ended Phrase Detection. Plummer et al, 2020

[5] Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension. Zhenfang Chen, 2020","## Suggestions 
* Line 103: ""Currently, OFA holds the SOTA among REC methods."" I believe this comment is a quite outdated and can be updated (joint OVD & REC methods such as FIBER [1] outperform it). 
* For the rebuttal, the authors could explain the difference between the proposed task and phrase detection.

[1]  Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. Dou et al, 2022",Yes,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,1 poor,3 good,1 poor,1689040028653,,,NeurIPS.cc/2023/Conference,0hwq2vOHT4,"['NeurIPS.cc/2023/Conference/Submission3467/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410900637,[],5,,,['everyone'],0hwq2vOHT4,['NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG'],1689040028653,1702410900637,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that the authors should be aware of, including:

1. ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension (Sanjay Subramania et al, 2022) [1]
2. When and why vision-language models behave like bags-of-words, and what to do about it? (Mert Yuksekgonul et al, 2023) [2]
3. When are Lemons Purple? The Concept Association Bias of CLIP (Yutaro Yamada et al, 2022) [3]
4. Revisiting Image-Language Networks for Open-ended Phrase Detection (Plummer et al, 2020) [4]
5. Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension (Zhenfang Chen, 2020) [5]
6. Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone (Dou et al, 2022) [1]

The reviewer suggests that the authors should cite these papers to demonstrate their awareness of the existing literature and to differentiate their proposed task from existing benchmarks.",1,"2022, 2023, 2022, 2020, 2020, 2022",When are Lemons Purple The Concept Association Bias of CLIP 
HDn9ab3fNo,"The paper proposes a task called described object detection which involves detecting objects through free form text queries, encompassing referring expression comprehension as well as open vocabulary detection.","## Clarity
The paper is written quite clearly.

## Originality, Quality and Significance: Please see weaknesses","# Major issues: 

## Quality [Annotating using CLIP is insufficient]
* DOD does not provide manually annotated explicit negative certificates for the images that are deemed as a negative for a given text query. The negatives are extracted using a CLIP matching score for each image with all the possible text queries. It has been demonstrated in several works that CLIP has no fine-grained understanding of the image [1,2,3], is largely incapable
of performing spatial reasoning off-the-shelf, obtaining chance performance even on simple synthetic images [1] and also behaves as a bag of words when understanding the text [2]. This implies that any complex query that is more than just a category name, cannot be appropriately distinguished as being relevant or not for a given image, and using this as a step in the annotation pipeline is sure to introduce errors. Using any sort of image-text model during the annotation process inherits the biases of the underlying model and in my opinion is not a viable approach for constructing an evaluation benchmark. For a benchmark that is characterized as a detection dataset, having accurate negatives is paramount, and the authors have not demonstrated that this criteria can be met using a CLIP matching in the annotation process. 
* Further, testing the ability of models to distinguish the presence or absence of a textual query, and localize it, would be truly tested in the cases of having unlikely phrases or combinations of objects and attributes or unlikely relations. Using CLIP in the annotation process would completely fail in these cases, as it has been shown that CLIP has a strong Concept Association Bias [3], frequently giving the highest matching score to the most likely completion, without paying attention to the image. This would further exacerbate the difficulty in accurately evaluating models on examples that might be especially hard and interesting (especially relevant to the more challenging ""absence"" type of queries).

## Originality, Significance [Proposed task is equivalent to existing benchmarks]
* As far as I can tell, the proposed task does not differ from the Phrase Detection task proposed by [5] which addresses the problem of both identifying whether the phrase is relevant to an image and also localizing the phrase, across a whole dataset. Departing from referring expression comprehension, they allow prediction of multiple boxes per phrase, and different from object detection, they evaluate text prompts that are longer than simple category names. Missing this line of literature completely is quite a red flag given that it has been around for quite some years now. 
* Another benchmark, ""COPS-Ref"" has also been proposed [6] that focuses on referring expressions with varying degrees of complexity and in which the localization must be done across multiple images, also containing distractors. This work also does not acknowledge or differentiate from COPS-Ref. 


[1] ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. Sanjay Subramania et al, 2022

[2] When and why vision-language models behave like bags-of-words, and what to do about it? Mert Yuksekgonul et al, 2023. 

[3] When are Lemons Purple? The Concept Association Bias of CLIP. Yutaro Yamada et al, 2022

[4] Revisiting Image-Language Networks for Open-ended Phrase Detection. Plummer et al, 2020

[5] Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension. Zhenfang Chen, 2020","## Suggestions 
* Line 103: ""Currently, OFA holds the SOTA among REC methods."" I believe this comment is a quite outdated and can be updated (joint OVD & REC methods such as FIBER [1] outperform it). 
* For the rebuttal, the authors could explain the difference between the proposed task and phrase detection.

[1]  Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. Dou et al, 2022",Yes,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,1 poor,3 good,1 poor,1689040028653,,,NeurIPS.cc/2023/Conference,0hwq2vOHT4,"['NeurIPS.cc/2023/Conference/Submission3467/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410900637,[],5,,,['everyone'],0hwq2vOHT4,['NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG'],1689040028653,1702410900637,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that the authors should be aware of, including:

1. ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension (Sanjay Subramania et al, 2022) [1]
2. When and why vision-language models behave like bags-of-words, and what to do about it? (Mert Yuksekgonul et al, 2023) [2]
3. When are Lemons Purple? The Concept Association Bias of CLIP (Yutaro Yamada et al, 2022) [3]
4. Revisiting Image-Language Networks for Open-ended Phrase Detection (Plummer et al, 2020) [4]
5. Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension (Zhenfang Chen, 2020) [5]
6. Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone (Dou et al, 2022) [1]

The reviewer suggests that the authors should cite these papers to demonstrate their awareness of the existing literature and to differentiate their proposed task from existing benchmarks.",1,"2022, 2023, 2022, 2020, 2020, 2022",Revisiting Image-Language Networks for Open-ended Phrase Detection 
HDn9ab3fNo,"The paper proposes a task called described object detection which involves detecting objects through free form text queries, encompassing referring expression comprehension as well as open vocabulary detection.","## Clarity
The paper is written quite clearly.

## Originality, Quality and Significance: Please see weaknesses","# Major issues: 

## Quality [Annotating using CLIP is insufficient]
* DOD does not provide manually annotated explicit negative certificates for the images that are deemed as a negative for a given text query. The negatives are extracted using a CLIP matching score for each image with all the possible text queries. It has been demonstrated in several works that CLIP has no fine-grained understanding of the image [1,2,3], is largely incapable
of performing spatial reasoning off-the-shelf, obtaining chance performance even on simple synthetic images [1] and also behaves as a bag of words when understanding the text [2]. This implies that any complex query that is more than just a category name, cannot be appropriately distinguished as being relevant or not for a given image, and using this as a step in the annotation pipeline is sure to introduce errors. Using any sort of image-text model during the annotation process inherits the biases of the underlying model and in my opinion is not a viable approach for constructing an evaluation benchmark. For a benchmark that is characterized as a detection dataset, having accurate negatives is paramount, and the authors have not demonstrated that this criteria can be met using a CLIP matching in the annotation process. 
* Further, testing the ability of models to distinguish the presence or absence of a textual query, and localize it, would be truly tested in the cases of having unlikely phrases or combinations of objects and attributes or unlikely relations. Using CLIP in the annotation process would completely fail in these cases, as it has been shown that CLIP has a strong Concept Association Bias [3], frequently giving the highest matching score to the most likely completion, without paying attention to the image. This would further exacerbate the difficulty in accurately evaluating models on examples that might be especially hard and interesting (especially relevant to the more challenging ""absence"" type of queries).

## Originality, Significance [Proposed task is equivalent to existing benchmarks]
* As far as I can tell, the proposed task does not differ from the Phrase Detection task proposed by [5] which addresses the problem of both identifying whether the phrase is relevant to an image and also localizing the phrase, across a whole dataset. Departing from referring expression comprehension, they allow prediction of multiple boxes per phrase, and different from object detection, they evaluate text prompts that are longer than simple category names. Missing this line of literature completely is quite a red flag given that it has been around for quite some years now. 
* Another benchmark, ""COPS-Ref"" has also been proposed [6] that focuses on referring expressions with varying degrees of complexity and in which the localization must be done across multiple images, also containing distractors. This work also does not acknowledge or differentiate from COPS-Ref. 


[1] ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. Sanjay Subramania et al, 2022

[2] When and why vision-language models behave like bags-of-words, and what to do about it? Mert Yuksekgonul et al, 2023. 

[3] When are Lemons Purple? The Concept Association Bias of CLIP. Yutaro Yamada et al, 2022

[4] Revisiting Image-Language Networks for Open-ended Phrase Detection. Plummer et al, 2020

[5] Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension. Zhenfang Chen, 2020","## Suggestions 
* Line 103: ""Currently, OFA holds the SOTA among REC methods."" I believe this comment is a quite outdated and can be updated (joint OVD & REC methods such as FIBER [1] outperform it). 
* For the rebuttal, the authors could explain the difference between the proposed task and phrase detection.

[1]  Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. Dou et al, 2022",Yes,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,1 poor,3 good,1 poor,1689040028653,,,NeurIPS.cc/2023/Conference,0hwq2vOHT4,"['NeurIPS.cc/2023/Conference/Submission3467/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410900637,[],5,,,['everyone'],0hwq2vOHT4,['NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG'],1689040028653,1702410900637,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that the authors should be aware of, including:

1. ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension (Sanjay Subramania et al, 2022) [1]
2. When and why vision-language models behave like bags-of-words, and what to do about it? (Mert Yuksekgonul et al, 2023) [2]
3. When are Lemons Purple? The Concept Association Bias of CLIP (Yutaro Yamada et al, 2022) [3]
4. Revisiting Image-Language Networks for Open-ended Phrase Detection (Plummer et al, 2020) [4]
5. Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension (Zhenfang Chen, 2020) [5]
6. Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone (Dou et al, 2022) [1]

The reviewer suggests that the authors should cite these papers to demonstrate their awareness of the existing literature and to differentiate their proposed task from existing benchmarks.",1,"2022, 2023, 2022, 2020, 2020, 2022",Cops-Ref A new Dataset and Task on Compositional Referring Expression Comprehension 
HDn9ab3fNo,"The paper proposes a task called described object detection which involves detecting objects through free form text queries, encompassing referring expression comprehension as well as open vocabulary detection.","## Clarity
The paper is written quite clearly.

## Originality, Quality and Significance: Please see weaknesses","# Major issues: 

## Quality [Annotating using CLIP is insufficient]
* DOD does not provide manually annotated explicit negative certificates for the images that are deemed as a negative for a given text query. The negatives are extracted using a CLIP matching score for each image with all the possible text queries. It has been demonstrated in several works that CLIP has no fine-grained understanding of the image [1,2,3], is largely incapable
of performing spatial reasoning off-the-shelf, obtaining chance performance even on simple synthetic images [1] and also behaves as a bag of words when understanding the text [2]. This implies that any complex query that is more than just a category name, cannot be appropriately distinguished as being relevant or not for a given image, and using this as a step in the annotation pipeline is sure to introduce errors. Using any sort of image-text model during the annotation process inherits the biases of the underlying model and in my opinion is not a viable approach for constructing an evaluation benchmark. For a benchmark that is characterized as a detection dataset, having accurate negatives is paramount, and the authors have not demonstrated that this criteria can be met using a CLIP matching in the annotation process. 
* Further, testing the ability of models to distinguish the presence or absence of a textual query, and localize it, would be truly tested in the cases of having unlikely phrases or combinations of objects and attributes or unlikely relations. Using CLIP in the annotation process would completely fail in these cases, as it has been shown that CLIP has a strong Concept Association Bias [3], frequently giving the highest matching score to the most likely completion, without paying attention to the image. This would further exacerbate the difficulty in accurately evaluating models on examples that might be especially hard and interesting (especially relevant to the more challenging ""absence"" type of queries).

## Originality, Significance [Proposed task is equivalent to existing benchmarks]
* As far as I can tell, the proposed task does not differ from the Phrase Detection task proposed by [5] which addresses the problem of both identifying whether the phrase is relevant to an image and also localizing the phrase, across a whole dataset. Departing from referring expression comprehension, they allow prediction of multiple boxes per phrase, and different from object detection, they evaluate text prompts that are longer than simple category names. Missing this line of literature completely is quite a red flag given that it has been around for quite some years now. 
* Another benchmark, ""COPS-Ref"" has also been proposed [6] that focuses on referring expressions with varying degrees of complexity and in which the localization must be done across multiple images, also containing distractors. This work also does not acknowledge or differentiate from COPS-Ref. 


[1] ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. Sanjay Subramania et al, 2022

[2] When and why vision-language models behave like bags-of-words, and what to do about it? Mert Yuksekgonul et al, 2023. 

[3] When are Lemons Purple? The Concept Association Bias of CLIP. Yutaro Yamada et al, 2022

[4] Revisiting Image-Language Networks for Open-ended Phrase Detection. Plummer et al, 2020

[5] Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension. Zhenfang Chen, 2020","## Suggestions 
* Line 103: ""Currently, OFA holds the SOTA among REC methods."" I believe this comment is a quite outdated and can be updated (joint OVD & REC methods such as FIBER [1] outperform it). 
* For the rebuttal, the authors could explain the difference between the proposed task and phrase detection.

[1]  Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. Dou et al, 2022",Yes,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,1 poor,3 good,1 poor,1689040028653,,,NeurIPS.cc/2023/Conference,0hwq2vOHT4,"['NeurIPS.cc/2023/Conference/Submission3467/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410900637,[],5,,,['everyone'],0hwq2vOHT4,['NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG'],1689040028653,1702410900637,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3467/Reviewer_Q5JG']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that the authors should be aware of, including:

1. ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension (Sanjay Subramania et al, 2022) [1]
2. When and why vision-language models behave like bags-of-words, and what to do about it? (Mert Yuksekgonul et al, 2023) [2]
3. When are Lemons Purple? The Concept Association Bias of CLIP (Yutaro Yamada et al, 2022) [3]
4. Revisiting Image-Language Networks for Open-ended Phrase Detection (Plummer et al, 2020) [4]
5. Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension (Zhenfang Chen, 2020) [5]
6. Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone (Dou et al, 2022) [1]

The reviewer suggests that the authors should cite these papers to demonstrate their awareness of the existing literature and to differentiate their proposed task from existing benchmarks.",1,"2022, 2023, 2022, 2020, 2020, 2022",Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone
X6csqZWObG,This paper studies the problem of gauge equivariant convolutional and attentional architectures on meshes and proposes to introduce non-linear activations to enhance the model. The experiments on three models shows the performance of the proposed method. ,"S1. The studied problem is important.

S2. The presentation is good. 

S3. Equivariance is an important property in message passing neural networks.","W1. The most important point is the novelty. Actually, Equivariant Mesh Attention Networks have combined Gauge Equivariance with MPNN. Your work adds non-linear activation [16] into the original models. The technical contribution is week.

W2. Given that your claim that ""the combination of nonlinear message passing and gauge equivariance has not been proposed"", you should introduce ""nonlinear"" into the title and give more emphasis on the significance of it. 

W3. Besides the performance, I didn't see too many deep insights about the benefit of nonlinear terms. For example, how to choose the non-linear activation? How the ""non-linear"" activation function influence the solution of non-linear PDE function. Since the difference of our method is minor, you should introduce deep insights to strength your contribution. 

W4. How about add non-linear activation in different places? I hope to get some deep results.

W5. The performance of your methods seems to not good on FAUST used in EMAN. Why you skip TOSCA? Can your method compared with [R1,R2,R3] on some standard benchmarks reported in [R1,R2,R3].  


[R1] Learning Mesh-Based Simulation with Graph Networks, ICLR 21

[R2] EAGLE- Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers, ICLR 23

[R3] Predicting Physics in Mesh-reduced Space with Temporal Attention, ICLR 22.
",See weaknesses. ,See weaknesses. ,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,3 good,2 fair,1686885503202,,,NeurIPS.cc/2023/Conference,0eXniewIvr,"['NeurIPS.cc/2023/Conference/Submission9446/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411232607,[],1,,,['everyone'],0eXniewIvr,['NeurIPS.cc/2023/Conference/Submission9446/Reviewer_P6sJ'],1686885503202,1702411232607,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9446/Reviewer_P6sJ']","Yes, the peer review explicitly suggests the authors of the paper to compare their method with and possibly cite the following specific literature:

1. [R1] Learning Mesh-Based Simulation with Graph Networks, ICLR 21
2. [R2] EAGLE- Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers, ICLR 23
3. [R3] Predicting Physics in Mesh-reduced Space with Temporal Attention, ICLR 22

Additionally, it mentions Equivariant Mesh Attention Networks, which has combined Gauge Equivariance with MPNN, implying that the authors should be aware of this existing work. However, it does not provide a specific reference for this. The reference [16] is also mentioned as the original model into which the authors have added non-linear activation.",1,,Learning Mesh-Based Simulation with Graph Networks 
X6csqZWObG,This paper studies the problem of gauge equivariant convolutional and attentional architectures on meshes and proposes to introduce non-linear activations to enhance the model. The experiments on three models shows the performance of the proposed method. ,"S1. The studied problem is important.

S2. The presentation is good. 

S3. Equivariance is an important property in message passing neural networks.","W1. The most important point is the novelty. Actually, Equivariant Mesh Attention Networks have combined Gauge Equivariance with MPNN. Your work adds non-linear activation [16] into the original models. The technical contribution is week.

W2. Given that your claim that ""the combination of nonlinear message passing and gauge equivariance has not been proposed"", you should introduce ""nonlinear"" into the title and give more emphasis on the significance of it. 

W3. Besides the performance, I didn't see too many deep insights about the benefit of nonlinear terms. For example, how to choose the non-linear activation? How the ""non-linear"" activation function influence the solution of non-linear PDE function. Since the difference of our method is minor, you should introduce deep insights to strength your contribution. 

W4. How about add non-linear activation in different places? I hope to get some deep results.

W5. The performance of your methods seems to not good on FAUST used in EMAN. Why you skip TOSCA? Can your method compared with [R1,R2,R3] on some standard benchmarks reported in [R1,R2,R3].  


[R1] Learning Mesh-Based Simulation with Graph Networks, ICLR 21

[R2] EAGLE- Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers, ICLR 23

[R3] Predicting Physics in Mesh-reduced Space with Temporal Attention, ICLR 22.
",See weaknesses. ,See weaknesses. ,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,3 good,2 fair,1686885503202,,,NeurIPS.cc/2023/Conference,0eXniewIvr,"['NeurIPS.cc/2023/Conference/Submission9446/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411232607,[],1,,,['everyone'],0eXniewIvr,['NeurIPS.cc/2023/Conference/Submission9446/Reviewer_P6sJ'],1686885503202,1702411232607,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9446/Reviewer_P6sJ']","Yes, the peer review explicitly suggests the authors of the paper to compare their method with and possibly cite the following specific literature:

1. [R1] Learning Mesh-Based Simulation with Graph Networks, ICLR 21
2. [R2] EAGLE- Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers, ICLR 23
3. [R3] Predicting Physics in Mesh-reduced Space with Temporal Attention, ICLR 22

Additionally, it mentions Equivariant Mesh Attention Networks, which has combined Gauge Equivariance with MPNN, implying that the authors should be aware of this existing work. However, it does not provide a specific reference for this. The reference [16] is also mentioned as the original model into which the authors have added non-linear activation.",1,,EAGLE- Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers 
X6csqZWObG,This paper studies the problem of gauge equivariant convolutional and attentional architectures on meshes and proposes to introduce non-linear activations to enhance the model. The experiments on three models shows the performance of the proposed method. ,"S1. The studied problem is important.

S2. The presentation is good. 

S3. Equivariance is an important property in message passing neural networks.","W1. The most important point is the novelty. Actually, Equivariant Mesh Attention Networks have combined Gauge Equivariance with MPNN. Your work adds non-linear activation [16] into the original models. The technical contribution is week.

W2. Given that your claim that ""the combination of nonlinear message passing and gauge equivariance has not been proposed"", you should introduce ""nonlinear"" into the title and give more emphasis on the significance of it. 

W3. Besides the performance, I didn't see too many deep insights about the benefit of nonlinear terms. For example, how to choose the non-linear activation? How the ""non-linear"" activation function influence the solution of non-linear PDE function. Since the difference of our method is minor, you should introduce deep insights to strength your contribution. 

W4. How about add non-linear activation in different places? I hope to get some deep results.

W5. The performance of your methods seems to not good on FAUST used in EMAN. Why you skip TOSCA? Can your method compared with [R1,R2,R3] on some standard benchmarks reported in [R1,R2,R3].  


[R1] Learning Mesh-Based Simulation with Graph Networks, ICLR 21

[R2] EAGLE- Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers, ICLR 23

[R3] Predicting Physics in Mesh-reduced Space with Temporal Attention, ICLR 22.
",See weaknesses. ,See weaknesses. ,['No ethics review needed.'],"3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,2 fair,3 good,2 fair,1686885503202,,,NeurIPS.cc/2023/Conference,0eXniewIvr,"['NeurIPS.cc/2023/Conference/Submission9446/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411232607,[],1,,,['everyone'],0eXniewIvr,['NeurIPS.cc/2023/Conference/Submission9446/Reviewer_P6sJ'],1686885503202,1702411232607,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9446/Reviewer_P6sJ']","Yes, the peer review explicitly suggests the authors of the paper to compare their method with and possibly cite the following specific literature:

1. [R1] Learning Mesh-Based Simulation with Graph Networks, ICLR 21
2. [R2] EAGLE- Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers, ICLR 23
3. [R3] Predicting Physics in Mesh-reduced Space with Temporal Attention, ICLR 22

Additionally, it mentions Equivariant Mesh Attention Networks, which has combined Gauge Equivariance with MPNN, implying that the authors should be aware of this existing work. However, it does not provide a specific reference for this. The reference [16] is also mentioned as the original model into which the authors have added non-linear activation.",1,,Predicting Physics in Mesh-reduced Space with Temporal Attention
9w92DeuME7,"The authors propose a gauge equivariant method for simulating PDEs on the surface of meshes. Different from the convolutional and attentional prior works, the authors use non-linear message passing with gauge equivariant layers. They compare to the prior works in the FAUST shape classification and the simulation of three PDEs. They find that the convolutional method is best for the shape classification, while on the PDEs, their method achieves the best results.","- It's great to see the code included
- I agree with the authors that it's good to have non-linear message passing as an additional gauge-equivariant method.
- The paper is clearly written.","- The authors should include a reference to [1], which simulates fluid dynamics with gauge equivariant methods.
- It appears like the PDE experiments merely using the scalar features without information on geometry. This seems insufficient. Both [1] and [2] have suggestions for gauge equivariant geometric input features.
- The paper would benefit from an evaluation on more problems, as well as a comparison to pointcloud based methods (as done in [1]).



----------
The weaknesses have been sufficiently addressed by the rebuttal. I increase my score.","- In [3], it is noted in a footnote that the kernel could be made dependent on the radius, but that the authors found it not beneficial. The method proposed here seems to also have a kernel insensitive to distance. Have the authors verified whether this is still the best choice?
- Could the authors clarify in their manuscript that the edge feature $e_{pq}$ is a feature situated on the fiber at $p$ ?
- Could the authors add a discussion of the computational cost of the various method in their paper?
- The regular non-linearity used in [3] is only approximately equivariant. Could the authors clarify that in their paper?

References:
- [1] Suk, Julian, Pim de Haan, Phillip Lippe, Christoph Brune, and Jelmer M. Wolterink. 2022. “Mesh Neural Networks for SE(3)-Equivariant Hemodynamics Estimation on the Artery Wall.”
- [2] Basu, Sourya, Jose Gallego-Posada, Francesco Viganò, James Rowbottom, and Taco Cohen. 2022. “Equivariant Mesh Attention Networks.” 
- [3] De Haan, P., M. Weiler, T. Cohen, and M. Welling. 2020. “Gauge Equivariant Mesh CNNs: Anisotropic Convolutions on Geometric Graphs.”",The authors fairly reflect the limitations of their work.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,3 good,1688538696134,,,NeurIPS.cc/2023/Conference,0eXniewIvr,"['NeurIPS.cc/2023/Conference/Submission9446/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411232428,[],3,,,['everyone'],0eXniewIvr,['NeurIPS.cc/2023/Conference/Submission9446/Reviewer_yPBJ'],1688538696134,1702411232428,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9446/Reviewer_yPBJ']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, namely:

* [1] Suk, Julian, Pim de Haan, Phillip Lippe, Christoph Brune, and Jelmer M. Wolterink. 2022. “Mesh Neural Networks for SE(3)-Equivariant Hemodynamics Estimation on the Artery Wall.”

This is mentioned in the ""weaknesses"" section of the review, where the reviewer states: ""The authors should include a reference to [1], which simulates fluid dynamics with gauge equivariant methods.""",1,2022,Mesh Neural Networks for SE(3)-Equivariant Hemodynamics Estimation on the Artery Wall
0PSGMKTj29,"This paper aims at solving complex partial differential equations on surfaces. Given the fact that most existing work neither incorporate surface geometry nor consider local gauge symmetries of the manifolds, this paper proposes a novel gauge equivariant network, known as Hermes, that can achieve higher performance than existing convolutional or attentional networks in certain cases. In addition, authors investigate in which cases their method has advantages over other methods. ","- Propose a new gauge equivariant network, Hermes, for learning signal on meshes. 
- Hermes outperforms both convolutional and attentional architectures on complex and nonlinear dynamics such as surface PDEs.
- Authors investigate in which situations nonlinear message passing should be preferred over convolutional or attentional counterparts.
","- It is not clear what is the relationship between Hermes and previous methods that use graph networks to perform mesh-based simulation, such as [1]. Could authors elaborate on the differences with the method in [1]? And I would like to see the performance comparison between Hermes and [1] in some scenarios, for example, FlagDynamic and CylinderFlow in [1]. 
- In addition, Geo-FNO [2] can also perform PDE learning on irregular geometries, what is the advantages of Hermes over Geo-FNO in terms of accuracy in solving PDE on irregular domains. 


[1] Pfaff, Tobias, et al. ""Learning mesh-based simulation with graph networks."" arXiv preprint arXiv:2010.03409 (2020).

[2] Li, Zongyi, et al. ""Fourier neural operator with learned deformations for pdes on general geometries."" arXiv preprint arXiv:2207.05209 (2022).",- See the weakness part. I would like to raise the score if the authors can well address my concerns.,The limitations of this paper are well discussed.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1688676678827,,,NeurIPS.cc/2023/Conference,0eXniewIvr,"['NeurIPS.cc/2023/Conference/Submission9446/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411232341,[],4,,,['everyone'],0eXniewIvr,['NeurIPS.cc/2023/Conference/Submission9446/Reviewer_ASko'],1688676678827,1702411232341,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9446/Reviewer_ASko']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Pfaff, Tobias, et al. ""Learning mesh-based simulation with graph networks."" arXiv preprint arXiv:2010.03409 (2020), referred to as [1].
2. Li, Zongyi, et al. ""Fourier neural operator with learned deformations for pdes on general geometries."" arXiv preprint arXiv:2207.05209 (2022), referred to as [2].

The reviewer asks the authors to elaborate on the differences between their method (Hermes) and the methods in [1] and [2], and to provide performance comparisons with these methods in certain scenarios.",1,"2020, 2020, 2022, 2022",Learning mesh-based simulation with graph networks 
0PSGMKTj29,"This paper aims at solving complex partial differential equations on surfaces. Given the fact that most existing work neither incorporate surface geometry nor consider local gauge symmetries of the manifolds, this paper proposes a novel gauge equivariant network, known as Hermes, that can achieve higher performance than existing convolutional or attentional networks in certain cases. In addition, authors investigate in which cases their method has advantages over other methods. ","- Propose a new gauge equivariant network, Hermes, for learning signal on meshes. 
- Hermes outperforms both convolutional and attentional architectures on complex and nonlinear dynamics such as surface PDEs.
- Authors investigate in which situations nonlinear message passing should be preferred over convolutional or attentional counterparts.
","- It is not clear what is the relationship between Hermes and previous methods that use graph networks to perform mesh-based simulation, such as [1]. Could authors elaborate on the differences with the method in [1]? And I would like to see the performance comparison between Hermes and [1] in some scenarios, for example, FlagDynamic and CylinderFlow in [1]. 
- In addition, Geo-FNO [2] can also perform PDE learning on irregular geometries, what is the advantages of Hermes over Geo-FNO in terms of accuracy in solving PDE on irregular domains. 


[1] Pfaff, Tobias, et al. ""Learning mesh-based simulation with graph networks."" arXiv preprint arXiv:2010.03409 (2020).

[2] Li, Zongyi, et al. ""Fourier neural operator with learned deformations for pdes on general geometries."" arXiv preprint arXiv:2207.05209 (2022).",- See the weakness part. I would like to raise the score if the authors can well address my concerns.,The limitations of this paper are well discussed.,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1688676678827,,,NeurIPS.cc/2023/Conference,0eXniewIvr,"['NeurIPS.cc/2023/Conference/Submission9446/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411232341,[],4,,,['everyone'],0eXniewIvr,['NeurIPS.cc/2023/Conference/Submission9446/Reviewer_ASko'],1688676678827,1702411232341,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9446/Reviewer_ASko']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Pfaff, Tobias, et al. ""Learning mesh-based simulation with graph networks."" arXiv preprint arXiv:2010.03409 (2020), referred to as [1].
2. Li, Zongyi, et al. ""Fourier neural operator with learned deformations for pdes on general geometries."" arXiv preprint arXiv:2207.05209 (2022), referred to as [2].

The reviewer asks the authors to elaborate on the differences between their method (Hermes) and the methods in [1] and [2], and to provide performance comparisons with these methods in certain scenarios.",1,"2020, 2020, 2022, 2022",Fourier neural operator with learned deformations for pdes on general geometries
ZqJajqknuS,"The authors propose SAMoSSA, an algorithm that combines deterministic trend estimation via mSSA with estimation of an autoregressive component of a time series.  They provide error rates for trend estimation, estimation of the AR coefficients, as well as the prediction error.   In addition, they consider real and simulated examples and show that the proposed method offers very competitive performance.","- While the time series literature is vast and well-developed, it does appear that theoretical results that analyze the effect of trend estimation on AR estimates and the prediction error were quite limited.  Detrending a time series and then modeling using ARMA or VAR is a common routine so this is a practically important problem.  
- The derived error bounds appear to be novel.  
- The paper is well-written.  Assumptions and implications of theorems are for the most part explained well.","- The authors could do a more thorough literature review when it comes to inference for trends.  For example, there is a statistics literature on testing for trends. See Chen and Wu (2019): Testing for Trends in High-Dimensional Time Series and references therein.  
- While it is a nice result, the implications for real-world data are questionable due to the conditions on the trend as well as the assumed autoregressive structure.",The more general ARMA model is first discussed in the introduction.  What challenges do estimating the MA components pose for the analysis?,"The authors do acknowledge some limitations with the studied framework, including assuming stationarity of the stochastic component.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",,Yes,3 good,3 good,4 excellent,1688660027073,,,NeurIPS.cc/2023/Conference,0e4eiXoUn5,"['NeurIPS.cc/2023/Conference/Submission7662/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411124267,[],1,,,['everyone'],0e4eiXoUn5,['NeurIPS.cc/2023/Conference/Submission7662/Reviewer_xDu7'],1688660027073,1702411124267,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7662/Reviewer_xDu7']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, namely ""Chen and Wu (2019): Testing for Trends in High-Dimensional Time Series and references therein"" for a more thorough literature review on inference for trends.",1,2019,Testing for Trends in High-Dimensional Time Series
d6aHvrjtj3,"This paper studies the problem of reinforcement learning under adversarial reward and transitions. When evaluating the regret against the best fixed policy in hindsight, the algorithm proposed in this paper achieves the optimal regret O(\sqrt{T} + C^P), which is followed by other favorable extensions including being agnostic to the corruption amount $C^P$ and $C^L$, as well as gap-dependent regret bounds. ","The theoretical results presented in this paper is solid. Several new techniques, including a new model selection framework that allows adversarial environments, is of independent interestis.","The main weakness is the lack of justification for the specific type of regret studied in this paper. To the best of my knowledge, this is the only paper that studies the regret against the best policy in hindsight in a mild-corruption setting, i.e. C^P \leq o(T), so some words justifying this new notion would be helpful. I'm not sure when such a regret notion is desirable to study in this scenario, since if C^P \leq o(T), the best policy in hindsight will converge to the optimal policy in the underlying uncorrupted MDP, so the two notions would converge anyways.

Minor: 

There is a missing reference for corruption-robust RL

1. Zhang, X., Chen, Y., Zhu, X., & Sun, W. (2021, July). Robust policy gradient against strong data corruption. In International Conference on Machine Learning (pp. 12391-12401). PMLR.",See above.,,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,4 excellent,3 good,1688626396255,,,NeurIPS.cc/2023/Conference,0WLMVDdvDF,"['NeurIPS.cc/2023/Conference/Submission7579/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411120019,[],5,,,['everyone'],0WLMVDdvDF,['NeurIPS.cc/2023/Conference/Submission7579/Reviewer_tv4V'],1688626396255,1702411120019,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission7579/Reviewer_tv4V']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, which is:

Zhang, X., Chen, Y., Zhu, X., & Sun, W. (2021, July). Robust policy gradient against strong data corruption. In International Conference on Machine Learning (pp. 12391-12401). PMLR.

This is mentioned in the ""Minor"" section of the review, where it states ""There is a missing reference for corruption-robust RL"" and then provides the reference.",1,2021,Robust policy gradient against strong data corruption
y4B7DUSn0R,The paper studies how to train a global policy using distributed data in reinforcement learning. The authors propose a distributed natural policy gradient method by employing ADMM to approximately compute a natural policy gradient direction. The communication complexity is linear in the dimension of policy parameters. The authors also prove the sublinear iteration complexity for achieving a stationary point. The effectiveness of the proposed method is finally verified in MuJoCo experiments. ,"**originality**

High communication complexity is a bottleneck for implementing many RL algorithms in federated training. I believe the novelty of this work lies in a new application of ADMM to a popular RL algorithm: natural policy gradient. This method achieves better communication complexity than one for the naive method. I am not aware of this method in the literature.  

**quality**

- The authors provide the stationary-point analysis of the proposed method in a distributed training setting.

- The authors verify the use of the proposed method in experiments. 

**clarity**

The proposed method and theory are clearly explained.

**significance**

- The distributed training algorithm is important for applying RL in distributed systems. 

- The stationary-point convergence guarantee is provided. ","- The naive method of fererated natural policy gradient only has quadratic communication complexity with respect to the dimension of policy parameter. This does not seem to be a real bottleneck.

- Directly extending centralized RL algorithms is not very significant, since fault-tolerance and robustness issues are more important to distributed systems. 

- It is important to compare communication complexity with the literature, e.g., Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee. 

- Some assumptions can be strong in practice, e.g., the invertibility of Hessian. It is useful to provide examples when Hessian is invertible. 

- The provided convergence property can be arbitrarily sub-optimal, and it is not clear how to check all assumptions. The stationary-point analysis has a gap with the convergence rate analysis of natural policy gradient in the literature. 

- The experimental setting is artificial since it is not originally designed for federated learning. There are no comparable baselines in experiments. ","
Some questions are raised in Weaknesses. Here are some other questions. 

- Can the authors provide robustness or fault-tolerance guarantees?  

- Can ADMM-based update be applied to other policy gradient methods? It is useful to discuss generalizability. 

- Since the global convergence exists in many policy gradient methods, can the authors strengthen the convergence guarantees? 

- Can the authors compare the proposed method with other baselines, e.g., Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee?",Yes.,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,2 fair,1688424403494,,,NeurIPS.cc/2023/Conference,0ORqsMY6OL,"['NeurIPS.cc/2023/Conference/Submission1340/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410782369,[],2,,,['everyone'],0ORqsMY6OL,['NeurIPS.cc/2023/Conference/Submission1340/Reviewer_pYuC'],1688424403494,1702410782369,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1340/Reviewer_pYuC']","Yes, this peer review explicitly suggests the authors of the paper to cite a specific literature, which is ""Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee"". The reviewer mentions it as a comparison to be made with the proposed method in terms of communication complexity.",1,,Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee
bKfVIOFqrm,"The paper introduces a universal prompt-based tuning method called Graph Prompt Feature (GPF) and its variation (GPF-plus) for pre-trained Graph Neural Network (GNN) models. GPF is a universal method that can be applied to any pre-trained GNN model under any pre-training strategy. It operates on the input graph's feature space and can achieve an equivalent effect to any form of prompting function. 
GPF introduces a learnable vector $p$ of dimension $F$, which is added to the node features, where $F$ corresponds to the dimensionality of the node features. The authors also purpose GPF-plus, which assigns an independent learnable vector $p_i$ to each node $u_i$ in the graph, instead of a single vector $p$. Experimental results show that GPF outperforms fine-tuning, with an average improvement of about 1.4% in full-shot scenarios and 3.2% in few-shot scenarios.","1) Theoretical Guarantees: The authors provide rigorous derivations and theoretical analyses to demonstrate the universality and effectiveness of GPF. They prove that GPF can achieve an equivalent effect to any form of prompting function and can outperform fine-tuning in certain cases. This theoretical foundation strengthens the credibility of their proposed method.

2) Experimental Validation - Reproducibility: The paper includes extensive experiments conducted under various pre-training strategies, including both full-shot and few-shot scenarios. The experimental results consistently show that GPF outperforms fine-tuning, achieving an average improvement of about 1.4% in full-shot scenarios and 3.2% in few-shot scenarios. Furthermore, GPF surpasses existing specialized prompt-based tuning methods designed for specific pre-training strategies. Moreover, the availability of the source code enhances the reproducibility of the experiments. 

","1) A weakness of the paper is the lack of a clear explanation or motivation regarding why the addition of the learnable vector p to the input features in the GPF model leads to better results compared to linear probing with a trainable layer in the final layer. While the Appendix demonstrates the superior performance of GPF, the paper does not provide a comprehensive analysis or reasoning behind this improvement. The absence of a clear explanation may leave readers questioning the underlying mechanisms and factors that contribute to the observed performance gain. Without a proper understanding of the motivations and justifications for the proposed approach, it becomes challenging to assess the significance and generalizability of the findings. 

2) Theorem 2 assumes a simple linear 1-layer GNN (without activations). While the theorem provides theoretical insights into the convergence properties of this specific type of GNN, it may not accurately capture the behavior of more complex GNN architectures commonly used in practice. In real-world applications, GNNs often incorporate activation functions to introduce non-linearity and capture more intricate patterns in graph data. By focusing solely on a linear 1-layer GNN without activations, the theorem may limit the generalizability of its conclusions and overlook important aspects of GNN models commonly used in practical scenarios. Therefore, the applicability of the theorem's findings to more sophisticated GNN architectures with non-linear activations remains uncertain.

3) One weakness of the paper is the exclusive use of Graph Isomorphism Network (GIN) as the backbone GNN for finetuning. While GIN is a widely used GNN architecture, it may not be the most expressive or optimal choice for every graph-related task[1,2]. The paper does not provide a comprehensive exploration or comparison of different backbone GNN architectures in the finetuning process. Moreover, maybe a more powerful GNN would be able to achieve better performance in the fine-tuning stage. Therefore, it would be interesting for the authors to examine if their approach leads also to better results when a more powerful GNN is used. 

[1] Frasca, Fabrizio, et al. ""Understanding and extending subgraph gnns by rethinking their symmetries."" Advances in Neural Information Processing Systems 35 (2022): 31376-31390.
[2]  Morris, Christopher, et al. ""Weisfeiler and leman go neural: Higher-order graph neural networks."" Proceedings of the AAAI conference on artificial intelligence. Vol. 33. No. 01. 2019.



","1) Explanation of the relationship between GPF and linear probing: The paper compares GPF with linear probing, which involves a trainable layer in the final layer. It would be beneficial if the authors could discuss the relationship between these two approaches and explain why GPF, with its learnable vector added to the input features, achieves superior performance compared to linear probing. Is there any inherent advantage or characteristic of GPF that enables it to outperform linear probing?

2) Generalizability of Theorem 2: The paper presents Theorem 2, which focuses on a simple linear 1-layer GNN without activations. However, many practical GNN architectures incorporate activation functions to introduce non-linearity. It would be helpful if the authors could discuss the generalizability of the theorem's conclusions to more complex GNN models commonly used in real-world applications. Can the findings of Theorem 2 be extended to GNNs with non-linear activations? If not, what are the limitations or implications of the theorem in the context of practical GNN architectures?

3) Have you considered or experimented with using more powerful GNN architectures, such as subgraph GNNs or higher order GNNs, as the backbone GNN for the finetuning stage? If so, how does the performance of GPF and GPF-plus compare when utilizing these alternative GNN architectures?

I will be happy to increase my score for the paper, if the authors adequately address the mentioned weaknesses and provide satisfactory explanations and improvements in response to the questions and suggestions raised. 


*** After rebuttal increased score to ""Borderline Accept"" ** ",yes,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1688391163935,,,NeurIPS.cc/2023/Conference,0LmWBhIYLi,"['NeurIPS.cc/2023/Conference/Submission1762/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410804084,[],1,,,['everyone'],0LmWBhIYLi,['NeurIPS.cc/2023/Conference/Submission1762/Reviewer_KwHM'],1688391163935,1702410804084,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1762/Reviewer_KwHM']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific papers:

1. Frasca, Fabrizio, et al. ""Understanding and extending subgraph gnns by rethinking their symmetries."" Advances in Neural Information Processing Systems 35 (2022): 31376-31390.
2. Morris, Christopher, et al. ""Weisfeiler and leman go neural: Higher-order graph neural networks."" Proceedings of the AAAI conference on artificial intelligence. Vol. 33. No. 01. 2019.

The reviewer references these papers to support their point that the exclusive use of Graph Isomorphism Network (GIN) as the backbone GNN for fine-tuning may not be the most expressive or optimal choice for every graph-related task, and that other GNN architectures, such as subgraph GNNs or higher-order GNNs, may be more powerful.",1,"2022, 2019",Understanding and extending subgraph gnns by rethinking their symmetries 
bKfVIOFqrm,"The paper introduces a universal prompt-based tuning method called Graph Prompt Feature (GPF) and its variation (GPF-plus) for pre-trained Graph Neural Network (GNN) models. GPF is a universal method that can be applied to any pre-trained GNN model under any pre-training strategy. It operates on the input graph's feature space and can achieve an equivalent effect to any form of prompting function. 
GPF introduces a learnable vector $p$ of dimension $F$, which is added to the node features, where $F$ corresponds to the dimensionality of the node features. The authors also purpose GPF-plus, which assigns an independent learnable vector $p_i$ to each node $u_i$ in the graph, instead of a single vector $p$. Experimental results show that GPF outperforms fine-tuning, with an average improvement of about 1.4% in full-shot scenarios and 3.2% in few-shot scenarios.","1) Theoretical Guarantees: The authors provide rigorous derivations and theoretical analyses to demonstrate the universality and effectiveness of GPF. They prove that GPF can achieve an equivalent effect to any form of prompting function and can outperform fine-tuning in certain cases. This theoretical foundation strengthens the credibility of their proposed method.

2) Experimental Validation - Reproducibility: The paper includes extensive experiments conducted under various pre-training strategies, including both full-shot and few-shot scenarios. The experimental results consistently show that GPF outperforms fine-tuning, achieving an average improvement of about 1.4% in full-shot scenarios and 3.2% in few-shot scenarios. Furthermore, GPF surpasses existing specialized prompt-based tuning methods designed for specific pre-training strategies. Moreover, the availability of the source code enhances the reproducibility of the experiments. 

","1) A weakness of the paper is the lack of a clear explanation or motivation regarding why the addition of the learnable vector p to the input features in the GPF model leads to better results compared to linear probing with a trainable layer in the final layer. While the Appendix demonstrates the superior performance of GPF, the paper does not provide a comprehensive analysis or reasoning behind this improvement. The absence of a clear explanation may leave readers questioning the underlying mechanisms and factors that contribute to the observed performance gain. Without a proper understanding of the motivations and justifications for the proposed approach, it becomes challenging to assess the significance and generalizability of the findings. 

2) Theorem 2 assumes a simple linear 1-layer GNN (without activations). While the theorem provides theoretical insights into the convergence properties of this specific type of GNN, it may not accurately capture the behavior of more complex GNN architectures commonly used in practice. In real-world applications, GNNs often incorporate activation functions to introduce non-linearity and capture more intricate patterns in graph data. By focusing solely on a linear 1-layer GNN without activations, the theorem may limit the generalizability of its conclusions and overlook important aspects of GNN models commonly used in practical scenarios. Therefore, the applicability of the theorem's findings to more sophisticated GNN architectures with non-linear activations remains uncertain.

3) One weakness of the paper is the exclusive use of Graph Isomorphism Network (GIN) as the backbone GNN for finetuning. While GIN is a widely used GNN architecture, it may not be the most expressive or optimal choice for every graph-related task[1,2]. The paper does not provide a comprehensive exploration or comparison of different backbone GNN architectures in the finetuning process. Moreover, maybe a more powerful GNN would be able to achieve better performance in the fine-tuning stage. Therefore, it would be interesting for the authors to examine if their approach leads also to better results when a more powerful GNN is used. 

[1] Frasca, Fabrizio, et al. ""Understanding and extending subgraph gnns by rethinking their symmetries."" Advances in Neural Information Processing Systems 35 (2022): 31376-31390.
[2]  Morris, Christopher, et al. ""Weisfeiler and leman go neural: Higher-order graph neural networks."" Proceedings of the AAAI conference on artificial intelligence. Vol. 33. No. 01. 2019.



","1) Explanation of the relationship between GPF and linear probing: The paper compares GPF with linear probing, which involves a trainable layer in the final layer. It would be beneficial if the authors could discuss the relationship between these two approaches and explain why GPF, with its learnable vector added to the input features, achieves superior performance compared to linear probing. Is there any inherent advantage or characteristic of GPF that enables it to outperform linear probing?

2) Generalizability of Theorem 2: The paper presents Theorem 2, which focuses on a simple linear 1-layer GNN without activations. However, many practical GNN architectures incorporate activation functions to introduce non-linearity. It would be helpful if the authors could discuss the generalizability of the theorem's conclusions to more complex GNN models commonly used in real-world applications. Can the findings of Theorem 2 be extended to GNNs with non-linear activations? If not, what are the limitations or implications of the theorem in the context of practical GNN architectures?

3) Have you considered or experimented with using more powerful GNN architectures, such as subgraph GNNs or higher order GNNs, as the backbone GNN for the finetuning stage? If so, how does the performance of GPF and GPF-plus compare when utilizing these alternative GNN architectures?

I will be happy to increase my score for the paper, if the authors adequately address the mentioned weaknesses and provide satisfactory explanations and improvements in response to the questions and suggestions raised. 


*** After rebuttal increased score to ""Borderline Accept"" ** ",yes,['No ethics review needed.'],"5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,3 good,2 fair,1688391163935,,,NeurIPS.cc/2023/Conference,0LmWBhIYLi,"['NeurIPS.cc/2023/Conference/Submission1762/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410804084,[],1,,,['everyone'],0LmWBhIYLi,['NeurIPS.cc/2023/Conference/Submission1762/Reviewer_KwHM'],1688391163935,1702410804084,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1762/Reviewer_KwHM']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature. The reviewer mentions two specific papers:

1. Frasca, Fabrizio, et al. ""Understanding and extending subgraph gnns by rethinking their symmetries."" Advances in Neural Information Processing Systems 35 (2022): 31376-31390.
2. Morris, Christopher, et al. ""Weisfeiler and leman go neural: Higher-order graph neural networks."" Proceedings of the AAAI conference on artificial intelligence. Vol. 33. No. 01. 2019.

The reviewer references these papers to support their point that the exclusive use of Graph Isomorphism Network (GIN) as the backbone GNN for fine-tuning may not be the most expressive or optimal choice for every graph-related task, and that other GNN architectures, such as subgraph GNNs or higher-order GNNs, may be more powerful.",1,"2022, 2019",Weisfeiler and leman go neural: Higher-order graph neural networks
4BMseEtp0n,"This work studies non-monotone submodular maximization subject to a cardinality
constraint in a fully dynamic setting, i.e., maintaining a good solution as
elements are inserted and deleted from the ""current"" ground set. Studying
non-monotone submodular maximization in this model is the natural follow-up to
the works of:
- Monemizadeh (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(k^2 \varepsilon^{-3} \log^{5} n)$.
- Lattanzi et al. (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(\log^4(k) \log^2 (n) / \varepsilon^{7})$.
- Chen and Peng (STOC 2020): proves query complexity lower bounds for approximation ratios strictly greater than $0.5$ for monotone dynamic submodular maximization. Gives $(1-1/e-\varepsilon)$-approximation for ""insertion-only"" streams for matroid constraints.

This work gives a $(1/8-\varepsilon)$-approximation for the *non-monotone*
fully dynamic cardinality-constrained problem with amortized query complexity
$O(\varepsilon^{-3} k^3 \log^3 n \log k)$ queries per update. The proposed
algorithm builds on connections to thresholding algorithms for *monotone*
submodular maximization and then altering these solutions to get guarantees for
non-monotone functions. The authors include experiments and compare their
algorithm to the Simple-Streaming in Feldman-Karbasi-Kazemi (NeurIPS 2018).","- Gives $(1/8 - \varepsilon)$-approximation for fully-dynamic non-monotone
  submodular maximization subject to a cardinality constraint, answering an
  open question in Chen-Peng (STOC 2020).
- Builds on thresholding techniques commonly used for monotone submodular
  maximization. This helps connect the toolkits for each problem type. Some
  missing references on line 54 when discussing thresholding:
  1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
  2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
  3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
  4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
","- The biggest weakness of this paper are the experiments. They almost check all
  of the boxes, but they aren't very motivating. This paper investigates:
  1. video frame summary on the entire set, which doesn't use any aspect of
     streaming but is a reasonable starting place to show what the algorithms
     output.
  2. sliding window model of length $W$, which is fully dynamic but not in the
     most interesting way (though possibly the most practical way).
  3. only compares against the Sample-Streaming algorithm of
     Feldman-Karbasi-Kazemi (NeurIPS 2018). It would be nice to include
     comparisons to the three papers discussed in the abstract, too, even though
     they are for monotone submodular functions.
- Given that there is randomness in this paper's Update algorithm (line 4 of
  SubsetSelection), it is important for these experiments to be averaged over
  several trials with standard deviation error bars. Both the oracle calls and
  objective value plots appear somewhat noisy and non-monotone.
","**Questions**
- [line  74] This is less of a question and more of a comment:
  The authors are correct in questioning whether thresholding works in a
  non-monotone setting when a randomly sampled set is added to the current
  solution, i.e., $f(S_t) \ge k \tau$. See Chen-Kuhnle (2022) for an example
  where this property does not hold. However, also see Chen-Kuhnle (2022) and
  Fahrbach-Mirrokni-Zadimoghaddam (ICML 2019, arXiv:1808.06932v3) for a method
  to circumvent this problem.

**Typos and suggestions**
- [line 27] Two relevant missing works for non-monotone submodular maximization:
  1. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)
  2. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
- [line  38] suggestion: The description of the dynamic setting ""... set of
  elements that are inserted but not deleted after their last insertion time
  till time $t$"" is not clear and should be improved. The description in Section
  1.1 is better, but could still be improved (i.e., saying that $V_t$ is the set
  of ``active'' elements at time $t$).
- [line  71] typo: ""The only such a result"" --> ""The only such result""
- [line 184] suggestion: Consider using $\text{Insert}_{i}(v, \tau)$ to avoid
  a double subscript.
- [line 313] typo in Figure 3 description: ""submdoular""
",,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,4 excellent,3 good,1687117910241,,,NeurIPS.cc/2023/Conference,0K1ZTfHZ0N,"['NeurIPS.cc/2023/Conference/Submission12517/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411386307,[],1,,,['everyone'],0K1ZTfHZ0N,['NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4'],1687117910241,1702411386307,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
5. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)

These citations are suggested to be added to the paper to improve its completeness and to provide more context for the readers.",1,"2019, 2019, 2020, 2020, 2022, 2019",Submodular Optimization in the MapReduce Model 
4BMseEtp0n,"This work studies non-monotone submodular maximization subject to a cardinality
constraint in a fully dynamic setting, i.e., maintaining a good solution as
elements are inserted and deleted from the ""current"" ground set. Studying
non-monotone submodular maximization in this model is the natural follow-up to
the works of:
- Monemizadeh (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(k^2 \varepsilon^{-3} \log^{5} n)$.
- Lattanzi et al. (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(\log^4(k) \log^2 (n) / \varepsilon^{7})$.
- Chen and Peng (STOC 2020): proves query complexity lower bounds for approximation ratios strictly greater than $0.5$ for monotone dynamic submodular maximization. Gives $(1-1/e-\varepsilon)$-approximation for ""insertion-only"" streams for matroid constraints.

This work gives a $(1/8-\varepsilon)$-approximation for the *non-monotone*
fully dynamic cardinality-constrained problem with amortized query complexity
$O(\varepsilon^{-3} k^3 \log^3 n \log k)$ queries per update. The proposed
algorithm builds on connections to thresholding algorithms for *monotone*
submodular maximization and then altering these solutions to get guarantees for
non-monotone functions. The authors include experiments and compare their
algorithm to the Simple-Streaming in Feldman-Karbasi-Kazemi (NeurIPS 2018).","- Gives $(1/8 - \varepsilon)$-approximation for fully-dynamic non-monotone
  submodular maximization subject to a cardinality constraint, answering an
  open question in Chen-Peng (STOC 2020).
- Builds on thresholding techniques commonly used for monotone submodular
  maximization. This helps connect the toolkits for each problem type. Some
  missing references on line 54 when discussing thresholding:
  1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
  2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
  3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
  4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
","- The biggest weakness of this paper are the experiments. They almost check all
  of the boxes, but they aren't very motivating. This paper investigates:
  1. video frame summary on the entire set, which doesn't use any aspect of
     streaming but is a reasonable starting place to show what the algorithms
     output.
  2. sliding window model of length $W$, which is fully dynamic but not in the
     most interesting way (though possibly the most practical way).
  3. only compares against the Sample-Streaming algorithm of
     Feldman-Karbasi-Kazemi (NeurIPS 2018). It would be nice to include
     comparisons to the three papers discussed in the abstract, too, even though
     they are for monotone submodular functions.
- Given that there is randomness in this paper's Update algorithm (line 4 of
  SubsetSelection), it is important for these experiments to be averaged over
  several trials with standard deviation error bars. Both the oracle calls and
  objective value plots appear somewhat noisy and non-monotone.
","**Questions**
- [line  74] This is less of a question and more of a comment:
  The authors are correct in questioning whether thresholding works in a
  non-monotone setting when a randomly sampled set is added to the current
  solution, i.e., $f(S_t) \ge k \tau$. See Chen-Kuhnle (2022) for an example
  where this property does not hold. However, also see Chen-Kuhnle (2022) and
  Fahrbach-Mirrokni-Zadimoghaddam (ICML 2019, arXiv:1808.06932v3) for a method
  to circumvent this problem.

**Typos and suggestions**
- [line 27] Two relevant missing works for non-monotone submodular maximization:
  1. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)
  2. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
- [line  38] suggestion: The description of the dynamic setting ""... set of
  elements that are inserted but not deleted after their last insertion time
  till time $t$"" is not clear and should be improved. The description in Section
  1.1 is better, but could still be improved (i.e., saying that $V_t$ is the set
  of ``active'' elements at time $t$).
- [line  71] typo: ""The only such a result"" --> ""The only such result""
- [line 184] suggestion: Consider using $\text{Insert}_{i}(v, \tau)$ to avoid
  a double subscript.
- [line 313] typo in Figure 3 description: ""submdoular""
",,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,4 excellent,3 good,1687117910241,,,NeurIPS.cc/2023/Conference,0K1ZTfHZ0N,"['NeurIPS.cc/2023/Conference/Submission12517/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411386307,[],1,,,['everyone'],0K1ZTfHZ0N,['NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4'],1687117910241,1702411386307,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
5. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)

These citations are suggested to be added to the paper to improve its completeness and to provide more context for the readers.",1,"2019, 2019, 2020, 2020, 2022, 2019","Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity "
4BMseEtp0n,"This work studies non-monotone submodular maximization subject to a cardinality
constraint in a fully dynamic setting, i.e., maintaining a good solution as
elements are inserted and deleted from the ""current"" ground set. Studying
non-monotone submodular maximization in this model is the natural follow-up to
the works of:
- Monemizadeh (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(k^2 \varepsilon^{-3} \log^{5} n)$.
- Lattanzi et al. (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(\log^4(k) \log^2 (n) / \varepsilon^{7})$.
- Chen and Peng (STOC 2020): proves query complexity lower bounds for approximation ratios strictly greater than $0.5$ for monotone dynamic submodular maximization. Gives $(1-1/e-\varepsilon)$-approximation for ""insertion-only"" streams for matroid constraints.

This work gives a $(1/8-\varepsilon)$-approximation for the *non-monotone*
fully dynamic cardinality-constrained problem with amortized query complexity
$O(\varepsilon^{-3} k^3 \log^3 n \log k)$ queries per update. The proposed
algorithm builds on connections to thresholding algorithms for *monotone*
submodular maximization and then altering these solutions to get guarantees for
non-monotone functions. The authors include experiments and compare their
algorithm to the Simple-Streaming in Feldman-Karbasi-Kazemi (NeurIPS 2018).","- Gives $(1/8 - \varepsilon)$-approximation for fully-dynamic non-monotone
  submodular maximization subject to a cardinality constraint, answering an
  open question in Chen-Peng (STOC 2020).
- Builds on thresholding techniques commonly used for monotone submodular
  maximization. This helps connect the toolkits for each problem type. Some
  missing references on line 54 when discussing thresholding:
  1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
  2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
  3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
  4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
","- The biggest weakness of this paper are the experiments. They almost check all
  of the boxes, but they aren't very motivating. This paper investigates:
  1. video frame summary on the entire set, which doesn't use any aspect of
     streaming but is a reasonable starting place to show what the algorithms
     output.
  2. sliding window model of length $W$, which is fully dynamic but not in the
     most interesting way (though possibly the most practical way).
  3. only compares against the Sample-Streaming algorithm of
     Feldman-Karbasi-Kazemi (NeurIPS 2018). It would be nice to include
     comparisons to the three papers discussed in the abstract, too, even though
     they are for monotone submodular functions.
- Given that there is randomness in this paper's Update algorithm (line 4 of
  SubsetSelection), it is important for these experiments to be averaged over
  several trials with standard deviation error bars. Both the oracle calls and
  objective value plots appear somewhat noisy and non-monotone.
","**Questions**
- [line  74] This is less of a question and more of a comment:
  The authors are correct in questioning whether thresholding works in a
  non-monotone setting when a randomly sampled set is added to the current
  solution, i.e., $f(S_t) \ge k \tau$. See Chen-Kuhnle (2022) for an example
  where this property does not hold. However, also see Chen-Kuhnle (2022) and
  Fahrbach-Mirrokni-Zadimoghaddam (ICML 2019, arXiv:1808.06932v3) for a method
  to circumvent this problem.

**Typos and suggestions**
- [line 27] Two relevant missing works for non-monotone submodular maximization:
  1. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)
  2. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
- [line  38] suggestion: The description of the dynamic setting ""... set of
  elements that are inserted but not deleted after their last insertion time
  till time $t$"" is not clear and should be improved. The description in Section
  1.1 is better, but could still be improved (i.e., saying that $V_t$ is the set
  of ``active'' elements at time $t$).
- [line  71] typo: ""The only such a result"" --> ""The only such result""
- [line 184] suggestion: Consider using $\text{Insert}_{i}(v, \tau)$ to avoid
  a double subscript.
- [line 313] typo in Figure 3 description: ""submdoular""
",,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,4 excellent,3 good,1687117910241,,,NeurIPS.cc/2023/Conference,0K1ZTfHZ0N,"['NeurIPS.cc/2023/Conference/Submission12517/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411386307,[],1,,,['everyone'],0K1ZTfHZ0N,['NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4'],1687117910241,1702411386307,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
5. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)

These citations are suggested to be added to the paper to improve its completeness and to provide more context for the readers.",1,"2019, 2019, 2020, 2020, 2022, 2019",Fully Dynamic Algorithm for Constrained Submodular Optimization 
4BMseEtp0n,"This work studies non-monotone submodular maximization subject to a cardinality
constraint in a fully dynamic setting, i.e., maintaining a good solution as
elements are inserted and deleted from the ""current"" ground set. Studying
non-monotone submodular maximization in this model is the natural follow-up to
the works of:
- Monemizadeh (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(k^2 \varepsilon^{-3} \log^{5} n)$.
- Lattanzi et al. (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(\log^4(k) \log^2 (n) / \varepsilon^{7})$.
- Chen and Peng (STOC 2020): proves query complexity lower bounds for approximation ratios strictly greater than $0.5$ for monotone dynamic submodular maximization. Gives $(1-1/e-\varepsilon)$-approximation for ""insertion-only"" streams for matroid constraints.

This work gives a $(1/8-\varepsilon)$-approximation for the *non-monotone*
fully dynamic cardinality-constrained problem with amortized query complexity
$O(\varepsilon^{-3} k^3 \log^3 n \log k)$ queries per update. The proposed
algorithm builds on connections to thresholding algorithms for *monotone*
submodular maximization and then altering these solutions to get guarantees for
non-monotone functions. The authors include experiments and compare their
algorithm to the Simple-Streaming in Feldman-Karbasi-Kazemi (NeurIPS 2018).","- Gives $(1/8 - \varepsilon)$-approximation for fully-dynamic non-monotone
  submodular maximization subject to a cardinality constraint, answering an
  open question in Chen-Peng (STOC 2020).
- Builds on thresholding techniques commonly used for monotone submodular
  maximization. This helps connect the toolkits for each problem type. Some
  missing references on line 54 when discussing thresholding:
  1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
  2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
  3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
  4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
","- The biggest weakness of this paper are the experiments. They almost check all
  of the boxes, but they aren't very motivating. This paper investigates:
  1. video frame summary on the entire set, which doesn't use any aspect of
     streaming but is a reasonable starting place to show what the algorithms
     output.
  2. sliding window model of length $W$, which is fully dynamic but not in the
     most interesting way (though possibly the most practical way).
  3. only compares against the Sample-Streaming algorithm of
     Feldman-Karbasi-Kazemi (NeurIPS 2018). It would be nice to include
     comparisons to the three papers discussed in the abstract, too, even though
     they are for monotone submodular functions.
- Given that there is randomness in this paper's Update algorithm (line 4 of
  SubsetSelection), it is important for these experiments to be averaged over
  several trials with standard deviation error bars. Both the oracle calls and
  objective value plots appear somewhat noisy and non-monotone.
","**Questions**
- [line  74] This is less of a question and more of a comment:
  The authors are correct in questioning whether thresholding works in a
  non-monotone setting when a randomly sampled set is added to the current
  solution, i.e., $f(S_t) \ge k \tau$. See Chen-Kuhnle (2022) for an example
  where this property does not hold. However, also see Chen-Kuhnle (2022) and
  Fahrbach-Mirrokni-Zadimoghaddam (ICML 2019, arXiv:1808.06932v3) for a method
  to circumvent this problem.

**Typos and suggestions**
- [line 27] Two relevant missing works for non-monotone submodular maximization:
  1. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)
  2. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
- [line  38] suggestion: The description of the dynamic setting ""... set of
  elements that are inserted but not deleted after their last insertion time
  till time $t$"" is not clear and should be improved. The description in Section
  1.1 is better, but could still be improved (i.e., saying that $V_t$ is the set
  of ``active'' elements at time $t$).
- [line  71] typo: ""The only such a result"" --> ""The only such result""
- [line 184] suggestion: Consider using $\text{Insert}_{i}(v, \tau)$ to avoid
  a double subscript.
- [line 313] typo in Figure 3 description: ""submdoular""
",,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,4 excellent,3 good,1687117910241,,,NeurIPS.cc/2023/Conference,0K1ZTfHZ0N,"['NeurIPS.cc/2023/Conference/Submission12517/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411386307,[],1,,,['everyone'],0K1ZTfHZ0N,['NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4'],1687117910241,1702411386307,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
5. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)

These citations are suggested to be added to the paper to improve its completeness and to provide more context for the readers.",1,"2019, 2019, 2020, 2020, 2022, 2019",Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint 
4BMseEtp0n,"This work studies non-monotone submodular maximization subject to a cardinality
constraint in a fully dynamic setting, i.e., maintaining a good solution as
elements are inserted and deleted from the ""current"" ground set. Studying
non-monotone submodular maximization in this model is the natural follow-up to
the works of:
- Monemizadeh (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(k^2 \varepsilon^{-3} \log^{5} n)$.
- Lattanzi et al. (NeurIPS 2020): $(0.5 - \varepsilon)$-approximation for *monotone* dynamic submodular maximization with amortized update time $O(\log^4(k) \log^2 (n) / \varepsilon^{7})$.
- Chen and Peng (STOC 2020): proves query complexity lower bounds for approximation ratios strictly greater than $0.5$ for monotone dynamic submodular maximization. Gives $(1-1/e-\varepsilon)$-approximation for ""insertion-only"" streams for matroid constraints.

This work gives a $(1/8-\varepsilon)$-approximation for the *non-monotone*
fully dynamic cardinality-constrained problem with amortized query complexity
$O(\varepsilon^{-3} k^3 \log^3 n \log k)$ queries per update. The proposed
algorithm builds on connections to thresholding algorithms for *monotone*
submodular maximization and then altering these solutions to get guarantees for
non-monotone functions. The authors include experiments and compare their
algorithm to the Simple-Streaming in Feldman-Karbasi-Kazemi (NeurIPS 2018).","- Gives $(1/8 - \varepsilon)$-approximation for fully-dynamic non-monotone
  submodular maximization subject to a cardinality constraint, answering an
  open question in Chen-Peng (STOC 2020).
- Builds on thresholding techniques commonly used for monotone submodular
  maximization. This helps connect the toolkits for each problem type. Some
  missing references on line 54 when discussing thresholding:
  1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
  2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
  3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
  4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
","- The biggest weakness of this paper are the experiments. They almost check all
  of the boxes, but they aren't very motivating. This paper investigates:
  1. video frame summary on the entire set, which doesn't use any aspect of
     streaming but is a reasonable starting place to show what the algorithms
     output.
  2. sliding window model of length $W$, which is fully dynamic but not in the
     most interesting way (though possibly the most practical way).
  3. only compares against the Sample-Streaming algorithm of
     Feldman-Karbasi-Kazemi (NeurIPS 2018). It would be nice to include
     comparisons to the three papers discussed in the abstract, too, even though
     they are for monotone submodular functions.
- Given that there is randomness in this paper's Update algorithm (line 4 of
  SubsetSelection), it is important for these experiments to be averaged over
  several trials with standard deviation error bars. Both the oracle calls and
  objective value plots appear somewhat noisy and non-monotone.
","**Questions**
- [line  74] This is less of a question and more of a comment:
  The authors are correct in questioning whether thresholding works in a
  non-monotone setting when a randomly sampled set is added to the current
  solution, i.e., $f(S_t) \ge k \tau$. See Chen-Kuhnle (2022) for an example
  where this property does not hold. However, also see Chen-Kuhnle (2022) and
  Fahrbach-Mirrokni-Zadimoghaddam (ICML 2019, arXiv:1808.06932v3) for a method
  to circumvent this problem.

**Typos and suggestions**
- [line 27] Two relevant missing works for non-monotone submodular maximization:
  1. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)
  2. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
- [line  38] suggestion: The description of the dynamic setting ""... set of
  elements that are inserted but not deleted after their last insertion time
  till time $t$"" is not clear and should be improved. The description in Section
  1.1 is better, but could still be improved (i.e., saying that $V_t$ is the set
  of ``active'' elements at time $t$).
- [line  71] typo: ""The only such a result"" --> ""The only such result""
- [line 184] suggestion: Consider using $\text{Insert}_{i}(v, \tau)$ to avoid
  a double subscript.
- [line 313] typo in Figure 3 description: ""submdoular""
",,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,4 excellent,4 excellent,3 good,1687117910241,,,NeurIPS.cc/2023/Conference,0K1ZTfHZ0N,"['NeurIPS.cc/2023/Conference/Submission12517/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411386307,[],1,,,['everyone'],0K1ZTfHZ0N,['NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4'],1687117910241,1702411386307,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12517/Reviewer_dvm4']","Yes, this peer review explicitly suggests the authors of the paper to cite several specific literature, including:

1. ""Submodular Optimization in the MapReduce Model"" (Liu-Vondrak, SOSA 2019)
2. ""Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, SODA 2019)
3. ""Fully Dynamic Algorithm for Constrained Submodular Optimization"" (Lattanzi et al., NeurIPS 2020)
4. ""Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint"" (Chen-Kuhnle, arXiv:2009.01947, 2022)
5. ""Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity"" (Fahrbach-Mirrokni-Zadimoghaddam, ICML 2019)

These citations are suggested to be added to the paper to improve its completeness and to provide more context for the readers.",1,"2019, 2019, 2020, 2020, 2022, 2019",Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity
4ptvbYI8hg,"The paper introduces a meta-learning method to initialize the OOMD algorithm. Combined with the introduced initialization-dependent convergence guarantees, authors then can show faster convergence when the meta-learning initialization is close.","The paper is well written, and the prior work is well referenced. 


The authors also provide extensive theoretical analysis of their algorithms. I think the most interesting result is Theorem 1 (other theorems often seem like an application of Theorem 1). I believe that the result is novel and interesting.


I also appreciate that authors analyze two-player zero-sum markov games as well as markov potential games
","First, I agree that “closest to this work” is Harris, Keegan, et al. ""Meta-learning in games."" arXiv preprint arXiv:2209.14110 (2022). More recently, a closely related (also meta learning in games and regret minimization) paper presents an algorithm combining meta-learning and regret guarantees: Sychrovsky, David, et al. ""Learning not to Regret."" arXiv preprint arXiv:2303.01074 (2023). I believe that paper should be included in the related work.


My biggest issue though is the empirical analysis (Section 6 - Simulations).
There are no details in the main text and one can only find it in the appendix. It is fine to move details to the appendix, but the authors do not even mention the size of the games in the main text.
Looking into the appendix, I think the games are trivially small (2x2 matrix games). Furthermore, the games in the meta-learning sequence seem very similar (the epsilon noisy seems very small). I think this is rather confirmed by Figure 1c. It is then not a big surprise that the convergence is so fast, when the games are so small and the Nash Equilibria very similar/close. I think the authors should to evaluate substantially more interesting/diverse/larger games (e.g. some epsilon/noisy parametrization of small poker games).


If the authors add experiments in larger games than the trivially small currently included, I am happy to move my score to ""Accept"" as otherwise I think the paper is good.


----------------------------------------------------------------------------------------------------
Authors addressed my comments and included more experiments, increasing the score.
",Did you have a chance to run the algorithm on larger games than the small ones included in the current version of the submission?,Sufficient,['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,4 excellent,1688503697893,,,NeurIPS.cc/2023/Conference,0Iw2dLh8uq,"['NeurIPS.cc/2023/Conference/Submission3873/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410923418,[],2,,,['everyone'],0Iw2dLh8uq,['NeurIPS.cc/2023/Conference/Submission3873/Reviewer_wdob'],1688503697893,1702410923418,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3873/Reviewer_wdob']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Harris, Keegan, et al. ""Meta-learning in games."" arXiv preprint arXiv:2209.14110 (2022)
2. Sychrovsky, David, et al. ""Learning not to Regret."" arXiv preprint arXiv:2303.01074 (2023)

The reviewer mentions that these papers are closely related to the work presented in the submission and should be included in the related work section.",1,"2022, 2022, 2023, 2023",Meta-learning in games 
4ptvbYI8hg,"The paper introduces a meta-learning method to initialize the OOMD algorithm. Combined with the introduced initialization-dependent convergence guarantees, authors then can show faster convergence when the meta-learning initialization is close.","The paper is well written, and the prior work is well referenced. 


The authors also provide extensive theoretical analysis of their algorithms. I think the most interesting result is Theorem 1 (other theorems often seem like an application of Theorem 1). I believe that the result is novel and interesting.


I also appreciate that authors analyze two-player zero-sum markov games as well as markov potential games
","First, I agree that “closest to this work” is Harris, Keegan, et al. ""Meta-learning in games."" arXiv preprint arXiv:2209.14110 (2022). More recently, a closely related (also meta learning in games and regret minimization) paper presents an algorithm combining meta-learning and regret guarantees: Sychrovsky, David, et al. ""Learning not to Regret."" arXiv preprint arXiv:2303.01074 (2023). I believe that paper should be included in the related work.


My biggest issue though is the empirical analysis (Section 6 - Simulations).
There are no details in the main text and one can only find it in the appendix. It is fine to move details to the appendix, but the authors do not even mention the size of the games in the main text.
Looking into the appendix, I think the games are trivially small (2x2 matrix games). Furthermore, the games in the meta-learning sequence seem very similar (the epsilon noisy seems very small). I think this is rather confirmed by Figure 1c. It is then not a big surprise that the convergence is so fast, when the games are so small and the Nash Equilibria very similar/close. I think the authors should to evaluate substantially more interesting/diverse/larger games (e.g. some epsilon/noisy parametrization of small poker games).


If the authors add experiments in larger games than the trivially small currently included, I am happy to move my score to ""Accept"" as otherwise I think the paper is good.


----------------------------------------------------------------------------------------------------
Authors addressed my comments and included more experiments, increasing the score.
",Did you have a chance to run the algorithm on larger games than the small ones included in the current version of the submission?,Sufficient,['No ethics review needed.'],"8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,4 excellent,1688503697893,,,NeurIPS.cc/2023/Conference,0Iw2dLh8uq,"['NeurIPS.cc/2023/Conference/Submission3873/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410923418,[],2,,,['everyone'],0Iw2dLh8uq,['NeurIPS.cc/2023/Conference/Submission3873/Reviewer_wdob'],1688503697893,1702410923418,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3873/Reviewer_wdob']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Harris, Keegan, et al. ""Meta-learning in games."" arXiv preprint arXiv:2209.14110 (2022)
2. Sychrovsky, David, et al. ""Learning not to Regret."" arXiv preprint arXiv:2303.01074 (2023)

The reviewer mentions that these papers are closely related to the work presented in the submission and should be included in the related work section.",1,"2022, 2022, 2023, 2023",Learning not to Regret
RWEoyXK4dz,"The authors proposed a meta-learning approach based on MAML for multi-agent domains where tasks with similar NE policies, when learned sequentially, converges faster to desired equilibria solutions. ","* Originality 
The authors investigated theoretical convergence properties of multi-agent learning through the lens of meta-learning of a sequence of similar tasks. This is novel and the problem setting is relevant as non-stationarity is inherent to multi-agent learning that may benefit from the MAML family of methodologies.  

* Quality
I find the theoretical results thorough but the work could benefit significantly with more substantial results on a few more standard benchmark domains. 

* Clarity
The paper is generally well written and easy to follow. ","* Task similarity metric: my main reservation with the proposed approach is related to the proposed task similarity metric (Sec 3.2) which translates to measuring similarity in different tasks' NE policy. Wouldn't selecting tasks in such way naturally promote faster convergence to NE policies when learning on ""similar"" tasks sequentially? I would appreciate further clarification on why this metric is used and if alternatives have been considered. 

* Multi-agent learning inherently deals with non-stationarity during learning as the environment dynamics (from the perspective of any one player) is non-stationary. Quite often such non-stationarity does not translate to similar NE policy though (e.g. rock-paper-scissors, where NE polices would be quite different). Could you clarify if the proposed method should be applicable to such ""similar tasks"" of this nature?
","L82-83: "" ... weaker solution concepts such as (C)CE ..."", perhaps clarify that in general-sum games NE would not allow for coordination and is therefore restrictive in such games? By ""weaker"" perhaps you meant computationally tractable beyond two-player zero-sum games?

L49: ""... convergence guarantees for MARL."", do you mean convergence to equilibria specifically? Perhaps clarify in writing that convergence bounds are wrt equilibria upfront? 

Related Works: ""meta-learning"" is overloaded in the RL literature and another line of work in meta-learning follows from prior works such as  [1-3] where a policy is conditioned on prior belief over tasks and can infer Bayes-optimally through interaction at test time to adapt to different tasks. This idea is then extended to multi-agent setting [3-4], with a focus on faster convergence / transfer learning via shared representation learning. Would it make sense to include a brief discussion of this line of meta-learning works in related works? 

[1] Meta-learning of Sequential Strategies: https://arxiv.org/abs/1905.03030
[2] Meta reinforcement learning as task inference: https://arxiv.org/abs/1905.06424
[3] Meta-Learning with Memory-Augmented Neural Networks: https://proceedings.mlr.press/v48/santoro16.pdf
[3] NeuPL: Neural Population Learning: https://arxiv.org/abs/2202.07415
[4] Simplex NeuPL: https://arxiv.org/abs/2205.15879",Please see my comments on the choice of task similarity metric which may be a limitation on when and where the proposed method could be effectively applied. ,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1688557794835,,,NeurIPS.cc/2023/Conference,0Iw2dLh8uq,"['NeurIPS.cc/2023/Conference/Submission3873/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410923303,[],3,,,['everyone'],0Iw2dLh8uq,['NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow'],1688557794835,1702410923303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature related to meta-learning, particularly works such as:

1. ""Meta-learning of Sequential Strategies"" (https://arxiv.org/abs/1905.03030)
2. ""Meta reinforcement learning as task inference"" (https://arxiv.org/abs/1905.06424)
3. ""Meta-Learning with Memory-Augmented Neural Networks"" (https://proceedings.mlr.press/v48/santoro16.pdf)
4. ""NeuPL: Neural Population Learning"" (https://arxiv.org/abs/2202.07415)
5. ""Simplex NeuPL"" (https://arxiv.org/abs/2205.15879)

The reviewer recommends including a brief discussion of this line of meta-learning works in the related works section.",1,"2019, 2019, 2022, 2022",Meta-learning of Sequential Strategies
RWEoyXK4dz,"The authors proposed a meta-learning approach based on MAML for multi-agent domains where tasks with similar NE policies, when learned sequentially, converges faster to desired equilibria solutions. ","* Originality 
The authors investigated theoretical convergence properties of multi-agent learning through the lens of meta-learning of a sequence of similar tasks. This is novel and the problem setting is relevant as non-stationarity is inherent to multi-agent learning that may benefit from the MAML family of methodologies.  

* Quality
I find the theoretical results thorough but the work could benefit significantly with more substantial results on a few more standard benchmark domains. 

* Clarity
The paper is generally well written and easy to follow. ","* Task similarity metric: my main reservation with the proposed approach is related to the proposed task similarity metric (Sec 3.2) which translates to measuring similarity in different tasks' NE policy. Wouldn't selecting tasks in such way naturally promote faster convergence to NE policies when learning on ""similar"" tasks sequentially? I would appreciate further clarification on why this metric is used and if alternatives have been considered. 

* Multi-agent learning inherently deals with non-stationarity during learning as the environment dynamics (from the perspective of any one player) is non-stationary. Quite often such non-stationarity does not translate to similar NE policy though (e.g. rock-paper-scissors, where NE polices would be quite different). Could you clarify if the proposed method should be applicable to such ""similar tasks"" of this nature?
","L82-83: "" ... weaker solution concepts such as (C)CE ..."", perhaps clarify that in general-sum games NE would not allow for coordination and is therefore restrictive in such games? By ""weaker"" perhaps you meant computationally tractable beyond two-player zero-sum games?

L49: ""... convergence guarantees for MARL."", do you mean convergence to equilibria specifically? Perhaps clarify in writing that convergence bounds are wrt equilibria upfront? 

Related Works: ""meta-learning"" is overloaded in the RL literature and another line of work in meta-learning follows from prior works such as  [1-3] where a policy is conditioned on prior belief over tasks and can infer Bayes-optimally through interaction at test time to adapt to different tasks. This idea is then extended to multi-agent setting [3-4], with a focus on faster convergence / transfer learning via shared representation learning. Would it make sense to include a brief discussion of this line of meta-learning works in related works? 

[1] Meta-learning of Sequential Strategies: https://arxiv.org/abs/1905.03030
[2] Meta reinforcement learning as task inference: https://arxiv.org/abs/1905.06424
[3] Meta-Learning with Memory-Augmented Neural Networks: https://proceedings.mlr.press/v48/santoro16.pdf
[3] NeuPL: Neural Population Learning: https://arxiv.org/abs/2202.07415
[4] Simplex NeuPL: https://arxiv.org/abs/2205.15879",Please see my comments on the choice of task similarity metric which may be a limitation on when and where the proposed method could be effectively applied. ,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1688557794835,,,NeurIPS.cc/2023/Conference,0Iw2dLh8uq,"['NeurIPS.cc/2023/Conference/Submission3873/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410923303,[],3,,,['everyone'],0Iw2dLh8uq,['NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow'],1688557794835,1702410923303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature related to meta-learning, particularly works such as:

1. ""Meta-learning of Sequential Strategies"" (https://arxiv.org/abs/1905.03030)
2. ""Meta reinforcement learning as task inference"" (https://arxiv.org/abs/1905.06424)
3. ""Meta-Learning with Memory-Augmented Neural Networks"" (https://proceedings.mlr.press/v48/santoro16.pdf)
4. ""NeuPL: Neural Population Learning"" (https://arxiv.org/abs/2202.07415)
5. ""Simplex NeuPL"" (https://arxiv.org/abs/2205.15879)

The reviewer recommends including a brief discussion of this line of meta-learning works in the related works section.",1,"2019, 2019, 2022, 2022",Meta reinforcement learning as task inference
RWEoyXK4dz,"The authors proposed a meta-learning approach based on MAML for multi-agent domains where tasks with similar NE policies, when learned sequentially, converges faster to desired equilibria solutions. ","* Originality 
The authors investigated theoretical convergence properties of multi-agent learning through the lens of meta-learning of a sequence of similar tasks. This is novel and the problem setting is relevant as non-stationarity is inherent to multi-agent learning that may benefit from the MAML family of methodologies.  

* Quality
I find the theoretical results thorough but the work could benefit significantly with more substantial results on a few more standard benchmark domains. 

* Clarity
The paper is generally well written and easy to follow. ","* Task similarity metric: my main reservation with the proposed approach is related to the proposed task similarity metric (Sec 3.2) which translates to measuring similarity in different tasks' NE policy. Wouldn't selecting tasks in such way naturally promote faster convergence to NE policies when learning on ""similar"" tasks sequentially? I would appreciate further clarification on why this metric is used and if alternatives have been considered. 

* Multi-agent learning inherently deals with non-stationarity during learning as the environment dynamics (from the perspective of any one player) is non-stationary. Quite often such non-stationarity does not translate to similar NE policy though (e.g. rock-paper-scissors, where NE polices would be quite different). Could you clarify if the proposed method should be applicable to such ""similar tasks"" of this nature?
","L82-83: "" ... weaker solution concepts such as (C)CE ..."", perhaps clarify that in general-sum games NE would not allow for coordination and is therefore restrictive in such games? By ""weaker"" perhaps you meant computationally tractable beyond two-player zero-sum games?

L49: ""... convergence guarantees for MARL."", do you mean convergence to equilibria specifically? Perhaps clarify in writing that convergence bounds are wrt equilibria upfront? 

Related Works: ""meta-learning"" is overloaded in the RL literature and another line of work in meta-learning follows from prior works such as  [1-3] where a policy is conditioned on prior belief over tasks and can infer Bayes-optimally through interaction at test time to adapt to different tasks. This idea is then extended to multi-agent setting [3-4], with a focus on faster convergence / transfer learning via shared representation learning. Would it make sense to include a brief discussion of this line of meta-learning works in related works? 

[1] Meta-learning of Sequential Strategies: https://arxiv.org/abs/1905.03030
[2] Meta reinforcement learning as task inference: https://arxiv.org/abs/1905.06424
[3] Meta-Learning with Memory-Augmented Neural Networks: https://proceedings.mlr.press/v48/santoro16.pdf
[3] NeuPL: Neural Population Learning: https://arxiv.org/abs/2202.07415
[4] Simplex NeuPL: https://arxiv.org/abs/2205.15879",Please see my comments on the choice of task similarity metric which may be a limitation on when and where the proposed method could be effectively applied. ,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1688557794835,,,NeurIPS.cc/2023/Conference,0Iw2dLh8uq,"['NeurIPS.cc/2023/Conference/Submission3873/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410923303,[],3,,,['everyone'],0Iw2dLh8uq,['NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow'],1688557794835,1702410923303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature related to meta-learning, particularly works such as:

1. ""Meta-learning of Sequential Strategies"" (https://arxiv.org/abs/1905.03030)
2. ""Meta reinforcement learning as task inference"" (https://arxiv.org/abs/1905.06424)
3. ""Meta-Learning with Memory-Augmented Neural Networks"" (https://proceedings.mlr.press/v48/santoro16.pdf)
4. ""NeuPL: Neural Population Learning"" (https://arxiv.org/abs/2202.07415)
5. ""Simplex NeuPL"" (https://arxiv.org/abs/2205.15879)

The reviewer recommends including a brief discussion of this line of meta-learning works in the related works section.",1,"2019, 2019, 2022, 2022",Meta-Learning with Memory-Augmented Neural Networks
RWEoyXK4dz,"The authors proposed a meta-learning approach based on MAML for multi-agent domains where tasks with similar NE policies, when learned sequentially, converges faster to desired equilibria solutions. ","* Originality 
The authors investigated theoretical convergence properties of multi-agent learning through the lens of meta-learning of a sequence of similar tasks. This is novel and the problem setting is relevant as non-stationarity is inherent to multi-agent learning that may benefit from the MAML family of methodologies.  

* Quality
I find the theoretical results thorough but the work could benefit significantly with more substantial results on a few more standard benchmark domains. 

* Clarity
The paper is generally well written and easy to follow. ","* Task similarity metric: my main reservation with the proposed approach is related to the proposed task similarity metric (Sec 3.2) which translates to measuring similarity in different tasks' NE policy. Wouldn't selecting tasks in such way naturally promote faster convergence to NE policies when learning on ""similar"" tasks sequentially? I would appreciate further clarification on why this metric is used and if alternatives have been considered. 

* Multi-agent learning inherently deals with non-stationarity during learning as the environment dynamics (from the perspective of any one player) is non-stationary. Quite often such non-stationarity does not translate to similar NE policy though (e.g. rock-paper-scissors, where NE polices would be quite different). Could you clarify if the proposed method should be applicable to such ""similar tasks"" of this nature?
","L82-83: "" ... weaker solution concepts such as (C)CE ..."", perhaps clarify that in general-sum games NE would not allow for coordination and is therefore restrictive in such games? By ""weaker"" perhaps you meant computationally tractable beyond two-player zero-sum games?

L49: ""... convergence guarantees for MARL."", do you mean convergence to equilibria specifically? Perhaps clarify in writing that convergence bounds are wrt equilibria upfront? 

Related Works: ""meta-learning"" is overloaded in the RL literature and another line of work in meta-learning follows from prior works such as  [1-3] where a policy is conditioned on prior belief over tasks and can infer Bayes-optimally through interaction at test time to adapt to different tasks. This idea is then extended to multi-agent setting [3-4], with a focus on faster convergence / transfer learning via shared representation learning. Would it make sense to include a brief discussion of this line of meta-learning works in related works? 

[1] Meta-learning of Sequential Strategies: https://arxiv.org/abs/1905.03030
[2] Meta reinforcement learning as task inference: https://arxiv.org/abs/1905.06424
[3] Meta-Learning with Memory-Augmented Neural Networks: https://proceedings.mlr.press/v48/santoro16.pdf
[3] NeuPL: Neural Population Learning: https://arxiv.org/abs/2202.07415
[4] Simplex NeuPL: https://arxiv.org/abs/2205.15879",Please see my comments on the choice of task similarity metric which may be a limitation on when and where the proposed method could be effectively applied. ,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1688557794835,,,NeurIPS.cc/2023/Conference,0Iw2dLh8uq,"['NeurIPS.cc/2023/Conference/Submission3873/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410923303,[],3,,,['everyone'],0Iw2dLh8uq,['NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow'],1688557794835,1702410923303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature related to meta-learning, particularly works such as:

1. ""Meta-learning of Sequential Strategies"" (https://arxiv.org/abs/1905.03030)
2. ""Meta reinforcement learning as task inference"" (https://arxiv.org/abs/1905.06424)
3. ""Meta-Learning with Memory-Augmented Neural Networks"" (https://proceedings.mlr.press/v48/santoro16.pdf)
4. ""NeuPL: Neural Population Learning"" (https://arxiv.org/abs/2202.07415)
5. ""Simplex NeuPL"" (https://arxiv.org/abs/2205.15879)

The reviewer recommends including a brief discussion of this line of meta-learning works in the related works section.",1,"2019, 2019, 2022, 2022",NeuPL Neural Population Learning
RWEoyXK4dz,"The authors proposed a meta-learning approach based on MAML for multi-agent domains where tasks with similar NE policies, when learned sequentially, converges faster to desired equilibria solutions. ","* Originality 
The authors investigated theoretical convergence properties of multi-agent learning through the lens of meta-learning of a sequence of similar tasks. This is novel and the problem setting is relevant as non-stationarity is inherent to multi-agent learning that may benefit from the MAML family of methodologies.  

* Quality
I find the theoretical results thorough but the work could benefit significantly with more substantial results on a few more standard benchmark domains. 

* Clarity
The paper is generally well written and easy to follow. ","* Task similarity metric: my main reservation with the proposed approach is related to the proposed task similarity metric (Sec 3.2) which translates to measuring similarity in different tasks' NE policy. Wouldn't selecting tasks in such way naturally promote faster convergence to NE policies when learning on ""similar"" tasks sequentially? I would appreciate further clarification on why this metric is used and if alternatives have been considered. 

* Multi-agent learning inherently deals with non-stationarity during learning as the environment dynamics (from the perspective of any one player) is non-stationary. Quite often such non-stationarity does not translate to similar NE policy though (e.g. rock-paper-scissors, where NE polices would be quite different). Could you clarify if the proposed method should be applicable to such ""similar tasks"" of this nature?
","L82-83: "" ... weaker solution concepts such as (C)CE ..."", perhaps clarify that in general-sum games NE would not allow for coordination and is therefore restrictive in such games? By ""weaker"" perhaps you meant computationally tractable beyond two-player zero-sum games?

L49: ""... convergence guarantees for MARL."", do you mean convergence to equilibria specifically? Perhaps clarify in writing that convergence bounds are wrt equilibria upfront? 

Related Works: ""meta-learning"" is overloaded in the RL literature and another line of work in meta-learning follows from prior works such as  [1-3] where a policy is conditioned on prior belief over tasks and can infer Bayes-optimally through interaction at test time to adapt to different tasks. This idea is then extended to multi-agent setting [3-4], with a focus on faster convergence / transfer learning via shared representation learning. Would it make sense to include a brief discussion of this line of meta-learning works in related works? 

[1] Meta-learning of Sequential Strategies: https://arxiv.org/abs/1905.03030
[2] Meta reinforcement learning as task inference: https://arxiv.org/abs/1905.06424
[3] Meta-Learning with Memory-Augmented Neural Networks: https://proceedings.mlr.press/v48/santoro16.pdf
[3] NeuPL: Neural Population Learning: https://arxiv.org/abs/2202.07415
[4] Simplex NeuPL: https://arxiv.org/abs/2205.15879",Please see my comments on the choice of task similarity metric which may be a limitation on when and where the proposed method could be effectively applied. ,['No ethics review needed.'],"4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,2 fair,3 good,2 fair,1688557794835,,,NeurIPS.cc/2023/Conference,0Iw2dLh8uq,"['NeurIPS.cc/2023/Conference/Submission3873/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410923303,[],3,,,['everyone'],0Iw2dLh8uq,['NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow'],1688557794835,1702410923303,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission3873/Reviewer_CSow']","Yes, this peer review explicitly suggests the authors of the paper to cite specific literature related to meta-learning, particularly works such as:

1. ""Meta-learning of Sequential Strategies"" (https://arxiv.org/abs/1905.03030)
2. ""Meta reinforcement learning as task inference"" (https://arxiv.org/abs/1905.06424)
3. ""Meta-Learning with Memory-Augmented Neural Networks"" (https://proceedings.mlr.press/v48/santoro16.pdf)
4. ""NeuPL: Neural Population Learning"" (https://arxiv.org/abs/2202.07415)
5. ""Simplex NeuPL"" (https://arxiv.org/abs/2205.15879)

The reviewer recommends including a brief discussion of this line of meta-learning works in the related works section.",1,"2019, 2019, 2022, 2022",Simplex NeuPL
zoLLgkBwsX,"This work studies the problem of dynamic online allocation under constraints with bandit feedback, and derives a generic algorithm applicable to various input settings (stochastic, adversarial, $\delta$-corrupted, ergodic, periodic). It recovers regret rates close to those of the lower bounds in each of these settings. The algorithm uses a dual gradient descent over $\lambda_t$ to decouple the decisions over time by considering the lagrangian, a gradient ascent for the optimal choice of $x_t$ (that also uses the technique from Flaxman et al (2004) to handle the bandit feedback), and finally a multiplicative weight update to adapt the learning rates used to the correct input setting. ","- The algorithm generalizes to multiple input settings and general constraints the problems related to Bandits with knapsack constraints, and of online learning with constraints. (to be clear this is an important strength of this paper!)
- The problem is well motivated through the consideration of running multiple ad campaigns
","- The assumption that there exists some safe action of level $\beta$ do simplify the problem of constraint satisfaction by guaranteeing to be able to satisfy the constraint at the end
- Some of the upper bounds (stochastic, periodic, corrupted) are a bit loose compared to their respective lower bounds
- The writing can be improved, in particular in the proofs which are lacking discussion about their main ideas and intuition. As an example page $8$ of the appendices is almost only a sequence of inequality and is hard to follow. For instance the inequalities could be cut in multiple parts, and some comments could be added to better explain the goal of the proof. There are also some typos.
","- Do the $T^{2/3}$ rates come from the use of the method to handle bandit feedback? How would a generic gradient feedback affect the rates, would we be able to derive tight rates with respect to the lower bounds presented? I think it might have been better to first present the results with gradient feedback, and mention in the appendix that bandit feedback can be handled by a standard technique. This would allow the main part of the paper to focus on the new contributions. 
- Could you give some intuition on what this optimal dynamic sequence looks like in the various settings? For instance, if my understanding is correct, the optimal dynamic sequence in the stochastic setting is simply a unique point (because the data is i.i.d). Could the optimal sequence for the ergodic setting be a function of $\kappa$ close to the unique optimal point with respect to the stationary measure? 
- How much does the meta algorithm degrade the regret? (compared to assuming that we know the input setting)
- l 657 Why is it $... +D \dots$ and not $...+(t-1)D\dots $ ?
 
Comments/Typos:
- L304 and l324 stochstic ->  stochastic
- I find the notation of $\lambda \in [0,F e/ \beta]$ confusing, is it $\lambda \in [0,F/\beta]^K$?
- L553 ‘which states $\max_{x \in \mathcal{X}} f(x)$: is a word missing?
- L608 I am not sure to understand this statement, could you include a reference for this result? Is the sup taken over $(g_{\tau},f_{\tau})$ - over $[t]$ or $[T]$? 
- I think it would be nice to cite the paper of Mannor et al (Online learning with sample path constraints), which deals with very similar problem and started the works on online convex optimization with varying constraints
- Equation below l643: $v$ -> $\Vert v \Vert$, and the last equality should be an inequality (as it is in the ball, not the sphere)
- Equation below l655 I think some of the gradients are missing $\Vert \nabla_t \Vert^2$ and $x^i_{t+}$ ->  $\tilde{x}^{i}_{t}$. Why are some of the gradients bolded and not the others? 
- Same thing for the gradients below l657, in addition I am not sure where $P(y_1:T)$ is defined.
- L 664 ‘$a=$ and $b=$’ it is unfinished 

I have not read through all the proofs, but I would recommend to read it again to look for additional typos
",Yes the authors address some limitations in the section $5$ of the paper regarding the lower bounds. ,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,2 fair,3 good,1688675990562,,,NeurIPS.cc/2023/Conference,09bZyE9tfp,"['NeurIPS.cc/2023/Conference/Submission9270/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411221018,[],2,,,['everyone'],09bZyE9tfp,['NeurIPS.cc/2023/Conference/Submission9270/Reviewer_EvVQ'],1688675990562,1702411221018,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission9270/Reviewer_EvVQ']","Yes, this peer review explicitly suggests the authors of the paper to cite the paper of Mannor et al (Online learning with sample path constraints), which deals with very similar problems and started the works on online convex optimization with varying constraints.",1,,Online learning with sample path constraints
ZRN0rfczvh,"The paper notices that while outlier exposure has shown promising potential in improving OoD detection performance, all previous studies on outlier exposure have been limited to utilizing visual outliers. The paper uncovers the benefits of using textual outliers by replacing real or virtual outliers in the image domain with textual equivalents. Then, it proposes various ways of generating preferable textual outliers. The extensive experiments demonstrate that generated textual outliers achieve competitive performance on large-scale OoD and hard OoD benchmarks.","1. The paper is written well and is easy to understand.
2. The studied problem is very important. section 3 is quite interesting.
3. The results seem to outperform state-of-the-art.
","1. I am curious about section 4.1.1 and section 4.1.3, why do the authors choose to use the images from the validation set of the ID dataset for generation?
2. It is not clear that if the class labels of the outlier dataset is missing, how can we generation description-level supervision in section 4.1.2?
3. I am curious about the performance of the method without the large-scale pretrained models (CLIP) as the classification backbone. The current approach seems to be highly model-specific, which hinders its general usage.
4. The comparison with a reasonable baseline, i.e., NPOS [1] is missing. 

[1] Leitian Tao, Xuefeng Du, Jerry Zhu, and Yixuan Li. Non-parametric outlier synthesis. In The Eleventh International Conference on Learning Representations, 2023.",see above,yes,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,2 fair,3 good,1688263300091,,,NeurIPS.cc/2023/Conference,090ORrOAPL,"['NeurIPS.cc/2023/Conference/Submission5564/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411015695,[],1,,,['everyone'],090ORrOAPL,['NeurIPS.cc/2023/Conference/Submission5564/Reviewer_yebH'],1688263300091,1702411015695,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5564/Reviewer_yebH']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, namely ""Leitian Tao, Xuefeng Du, Jerry Zhu, and Yixuan Li. Non-parametric outlier synthesis. In The Eleventh International Conference on Learning Representations, 2023"" (referred to as [1] in the review), as a reasonable baseline for comparison.",1,2023,Non-parametric outlier synthesis
HCjuzZW5ku,"
This paper addresses the challenge of detecting Out-of-Distribution (OOD) data by introducing ""textual outlier exposure"" as an alternative to visual outliers. Instead of relying on visual examples, the authors explore the benefits of using textual equivalents in OOD detection. They propose various methods for generating textual outliers, which are validated through extensive experiments on large-scale OOD benchmarks. The findings demonstrate that the generated textual outliers outperform visual outliers and establish criteria for effective textual outliers, including their proximity to the data distribution, descriptiveness, and incorporation of visual semantics.

The contributions of this work include investigating the potential of textual outlier exposure with multi-modal neural networks, utilizing large-language models for generating textual outliers at different levels of detail, and validating their effectiveness in various OOD detection scenarios. The paper presents a novel and promising approach to OOD detection, showcasing the advantages of textual outliers over visual counterparts and providing valuable insights for designing impactful textual outliers.","- The paper presents an innovative and compelling idea, offering valuable insights for open-world learning, Out-of-Distribution (OOD) detection, and online learning. The findings of this study have significant implications for various research studies in these domains.

- The clarity and coherence of the paper are commendable, making it easy to comprehend and follow the authors' methodological approach. 

- The inclusion of illustrative examples effectively enhances the understanding of key concepts.

- The figures and plots presented in the paper are visually clear, aiding in the visualization and interpretation of the experimental results.

- The paper offers comprehensive experimental studies, demonstrating the efficacy of the proposed method. The promising performance observed in these experiments further strengthens the validity and potential impact of the proposed approach.


","While the paper presents detailed experimental studies, there are two notable weaknesses that should be addressed:

- The focus of the experimental studies is primarily limited to outlier exposure methods, neglecting the exploration of other types of Out-of-Distribution (OOD) detection methods. It would be valuable to consider and compare the proposed approach against alternative techniques in order to provide a more comprehensive evaluation.

- The omission of an ablation study to assess the impact of network architecture on the proposed methodology is a notable oversight. Investigating the influence of different network architectures on the performance of the proposed approach would enhance the understanding of its strengths and limitations.","
The experimental evaluation in your paper is commendable. However, I have two inquiries regarding the experiments:

- It appears that the recent state-of-the-art methods, such as ASH [1] and GradNorm [2], have not been included in your experimental comparisons. Could you provide insights into the rationale behind this omission? It would be valuable to report and compare the performance of your proposed method with these state-of-the-art approaches to provide a comprehensive assessment.

- Additionally, considering the impact of different network architectures on your method is crucial. Could you elaborate on how varying network architectures affect the performance and effectiveness of your proposed approach? Investigating this aspect would provide deeper insights into the robustness and generalizability of your method.

[1] Djurisic, Andrija, et al. ""Extremely simple activation shaping for out-of-distribution detection."" arXiv preprint arXiv:2209.09858 (2022).

[2] Huang, Rui, Andrew Geng, and Yixuan Li. ""On the importance of gradients for detecting distributional shifts in the wild."" Advances in Neural Information Processing Systems 34 (2021): 677-689.







",No negative social impact.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688627753121,,,NeurIPS.cc/2023/Conference,090ORrOAPL,"['NeurIPS.cc/2023/Conference/Submission5564/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411015444,[],4,,,['everyone'],090ORrOAPL,['NeurIPS.cc/2023/Conference/Submission5564/Reviewer_T6K7'],1688627753121,1702411015444,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5564/Reviewer_T6K7']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. Djurisic, Andrija, et al. ""Extremely simple activation shaping for out-of-distribution detection."" arXiv preprint arXiv:2209.09858 (2022), referred to as [1].
2. Huang, Rui, Andrew Geng, and Yixuan Li. ""On the importance of gradients for detecting distributional shifts in the wild."" Advances in Neural Information Processing Systems 34 (2021): 677-689, referred to as [2].

The reviewer mentions that including these state-of-the-art methods (ASH and GradNorm) in the experimental comparisons would provide a more comprehensive assessment of the proposed approach.",1,"2022, 2022, 2021",Extremely simple activation shaping for out-of-distribution detection 
HCjuzZW5ku,"
This paper addresses the challenge of detecting Out-of-Distribution (OOD) data by introducing ""textual outlier exposure"" as an alternative to visual outliers. Instead of relying on visual examples, the authors explore the benefits of using textual equivalents in OOD detection. They propose various methods for generating textual outliers, which are validated through extensive experiments on large-scale OOD benchmarks. The findings demonstrate that the generated textual outliers outperform visual outliers and establish criteria for effective textual outliers, including their proximity to the data distribution, descriptiveness, and incorporation of visual semantics.

The contributions of this work include investigating the potential of textual outlier exposure with multi-modal neural networks, utilizing large-language models for generating textual outliers at different levels of detail, and validating their effectiveness in various OOD detection scenarios. The paper presents a novel and promising approach to OOD detection, showcasing the advantages of textual outliers over visual counterparts and providing valuable insights for designing impactful textual outliers.","- The paper presents an innovative and compelling idea, offering valuable insights for open-world learning, Out-of-Distribution (OOD) detection, and online learning. The findings of this study have significant implications for various research studies in these domains.

- The clarity and coherence of the paper are commendable, making it easy to comprehend and follow the authors' methodological approach. 

- The inclusion of illustrative examples effectively enhances the understanding of key concepts.

- The figures and plots presented in the paper are visually clear, aiding in the visualization and interpretation of the experimental results.

- The paper offers comprehensive experimental studies, demonstrating the efficacy of the proposed method. The promising performance observed in these experiments further strengthens the validity and potential impact of the proposed approach.


","While the paper presents detailed experimental studies, there are two notable weaknesses that should be addressed:

- The focus of the experimental studies is primarily limited to outlier exposure methods, neglecting the exploration of other types of Out-of-Distribution (OOD) detection methods. It would be valuable to consider and compare the proposed approach against alternative techniques in order to provide a more comprehensive evaluation.

- The omission of an ablation study to assess the impact of network architecture on the proposed methodology is a notable oversight. Investigating the influence of different network architectures on the performance of the proposed approach would enhance the understanding of its strengths and limitations.","
The experimental evaluation in your paper is commendable. However, I have two inquiries regarding the experiments:

- It appears that the recent state-of-the-art methods, such as ASH [1] and GradNorm [2], have not been included in your experimental comparisons. Could you provide insights into the rationale behind this omission? It would be valuable to report and compare the performance of your proposed method with these state-of-the-art approaches to provide a comprehensive assessment.

- Additionally, considering the impact of different network architectures on your method is crucial. Could you elaborate on how varying network architectures affect the performance and effectiveness of your proposed approach? Investigating this aspect would provide deeper insights into the robustness and generalizability of your method.

[1] Djurisic, Andrija, et al. ""Extremely simple activation shaping for out-of-distribution detection."" arXiv preprint arXiv:2209.09858 (2022).

[2] Huang, Rui, Andrew Geng, and Yixuan Li. ""On the importance of gradients for detecting distributional shifts in the wild."" Advances in Neural Information Processing Systems 34 (2021): 677-689.







",No negative social impact.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,3 good,1688627753121,,,NeurIPS.cc/2023/Conference,090ORrOAPL,"['NeurIPS.cc/2023/Conference/Submission5564/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411015444,[],4,,,['everyone'],090ORrOAPL,['NeurIPS.cc/2023/Conference/Submission5564/Reviewer_T6K7'],1688627753121,1702411015444,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission5564/Reviewer_T6K7']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. Djurisic, Andrija, et al. ""Extremely simple activation shaping for out-of-distribution detection."" arXiv preprint arXiv:2209.09858 (2022), referred to as [1].
2. Huang, Rui, Andrew Geng, and Yixuan Li. ""On the importance of gradients for detecting distributional shifts in the wild."" Advances in Neural Information Processing Systems 34 (2021): 677-689, referred to as [2].

The reviewer mentions that including these state-of-the-art methods (ASH and GradNorm) in the experimental comparisons would provide a more comprehensive assessment of the proposed approach.",1,"2022, 2022, 2021",On the importance of gradients for detecting distributional shifts in the wild
5FRqKwP4rb,"In this paper, the authors conduct a thorough investigation into the limitations of the Frechet Inception Distance (FID) metric for evaluating generative models. They address this issue by performing human evaluation and proposing a superior alternative for automatic generative model evaluation. Through dedicatedly designed experiments, the authors empirically demonstrate that FID, which relies on a pre-trained InceptionV3 model, exhibits a weak correlation with human evaluation. However, by replacing the InceptionV3 with a self-supervised model like DINO, the automatic evaluation becomes more closely aligned with human evaluation. The insights presented by the authors offer valuable guidance on the appropriate approach for evaluating image generative models.","[1] This paper addresses a highly significant issue within the generative modeling community. The authors' findings, demonstrating the lack of correlation between FID and human fidelity judgment, are particularly intriguing to me.

[2] The proposed alternative, FD_{DINO}, appears to be a sensible solution and holds promise for future evaluations of generative models.

[3] I think the provided benchmark tables are helpful for researchers trying to evaluate their generative models.

[4] In my opinion, the paper is well-written and makes a noteworthy contribution to the community.

","[1] One potential concern is that all interpretations in the paper are based on the assumption that human evaluation is entirely correct, which may introduce some inherent risks.

[2] Providing further explanations of the evaluation metrics would enhance the paper's accessibility for individuals who are not experts in this particular field.

[3] Adding more detailed explanations about the differences between the authors' paper and closely related work [R1] would be beneficial for better understanding the novelty and contributions of the presented work.

[R1] M. Yang, C. Yang, Y. Zhang, Q. Bai, Y. Shen, and B. Dai. Revisiting the evaluation of image synthesis with GANs. arXiv preprint arXiv:2304.01999, 2023.",please refer to the weaknesses.,"I believe this paper delivers a crucial message to the community by emphasizing the inadequacy of FID as a metric for evaluating image generative models. Additionally, the authors propose a substantial improvement by changing from InceptionV3 to DINO as the backbone, which significantly mitigates this issue. While it may not completely resolve the problem, it remains a commendable contribution. Considering these contributions, I would give a score of 7 to this paper.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,,Yes,3 good,3 good,4 excellent,1688513393366,,,NeurIPS.cc/2023/Conference,08zf7kTOoh,"['NeurIPS.cc/2023/Conference/Submission8503/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411173776,[],3,,,['everyone'],08zf7kTOoh,['NeurIPS.cc/2023/Conference/Submission8503/Reviewer_6HZb'],1688513393366,1702411173776,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission8503/Reviewer_6HZb']","Yes, the peer review explicitly suggests the authors of the paper to cite a specific literature, [R1] M. Yang, C. Yang, Y. Zhang, Q. Bai, Y. Shen, and B. Dai. Revisiting the evaluation of image synthesis with GANs. arXiv preprint arXiv:2304.01999, 2023, in order to provide more detailed explanations about the differences between the authors' paper and closely related work.",1,"2023, 2023",Revisiting the evaluation of image synthesis with GANs
uznJSU1uVJ,"This paper investigates the adversarial training of two-layer neural networks on linearly separable data. The authors propose to reflect the commonly used convex surrogate loss during the inner loop that generates adversarial attack via the PGD method, and derives guarantee on the convergence of the attack. Meanwhile, this paper also provides theoretical results on iteration complexity for the adversarial training on linearly separable data that holds for any width and initialization of the network. Numerical studies are conducted to show the performance.","1.	The investigated topic of adversarial training of neural networks is important and has many applications in real world, while the theoretical aspects have not been well-studied. The results presented in this paper are non-trivial contribution to the field, and might help with better understanding of adversarial training of neural networks in more complicated settings. 
2.	The theoretical analysis seems solid. The authors introduce some interesting terms such as $\beta$-effective attack and $\beta$-robust to help the analysis, which might help in other study.
3.	Overall, the paper is well-organized and easy to follow.
","1.	The empirical results with MNIST and CIFAR-10 do not show significant difference between the performance of standard adversarial training and adversarial training using proposed reflected loss function.

2.	There are some space of improvement regarding the experiments. Specifically, please consider the followings:

(1)	The result presented in Figure 2 does not seem convincing to me because (a) the setting is too simple and hand-crafted, while the real data can behave very differently; (b) Only one specific data point ($x=[3,2,1]$) is considered. It is doubtful whether it is cherry-picked or not.

(2)	In Table 1, the robust testing error for standard training model under standard PGD attack (0.033) is much smaller than FGSM (0.286), which is unlikely since PGD usually generates stronger attack through multiple iterations. Please double-check the correctness. 

(3)	In Table 1, please consider including the evaluation of different models over clean testing data (un-attacked) for better comparison. Also, for Tables 1 and 2, please consider including the evaluation under other attacks, e.g., CW attack [1] and AutoAttack [2].

Please also consider my questions in the next part.


[1] Carlini, N., & Wagner, D. (2017, May). Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp) (pp. 39-57). ieee.

[2] Croce, F., & Hein, M. (2020, November). Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning (pp. 2206-2216). PMLR.
","1.	What feature of the reflected loss function enables the derivation of the theoretical results, or why the standard loss function make the analysis difficult? Can one replace the reflected loss function with a more general loss function? 

2.	Theorem 3.4 says that ""in at most $T_{tr}\cdots$ iterations, Algorithm 2 $\cdots$ $\textit{finds an iterate}$ $\tau$” with certain property. What does that mean exactly? Is it guaranteed that the model at the end of iteration $T_{tr}$ maintain such property? Or, is any early-stopping criteria necessary in order to guarantee the property?
",The authors have adequately addressed the limitations of the work.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688883156165,,,NeurIPS.cc/2023/Conference,02Uc0G2Cym,"['NeurIPS.cc/2023/Conference/Submission12654/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411392916,[],4,,,['everyone'],02Uc0G2Cym,['NeurIPS.cc/2023/Conference/Submission12654/Reviewer_avt7'],1688883156165,1702411392916,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12654/Reviewer_avt7']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Carlini, N., & Wagner, D. (2017, May). Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp) (pp. 39-57). ieee. (referred to as [1])
2. Croce, F., & Hein, M. (2020, November). Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning (pp. 2206-2216). PMLR. (referred to as [2])

The reviewer suggests that the authors consider including the evaluation of their models under other attacks, such as CW attack [1] and AutoAttack [2], in Tables 1 and 2.",1,"2017, 2017, 2020",Towards evaluating the robustness of neural networks 
uznJSU1uVJ,"This paper investigates the adversarial training of two-layer neural networks on linearly separable data. The authors propose to reflect the commonly used convex surrogate loss during the inner loop that generates adversarial attack via the PGD method, and derives guarantee on the convergence of the attack. Meanwhile, this paper also provides theoretical results on iteration complexity for the adversarial training on linearly separable data that holds for any width and initialization of the network. Numerical studies are conducted to show the performance.","1.	The investigated topic of adversarial training of neural networks is important and has many applications in real world, while the theoretical aspects have not been well-studied. The results presented in this paper are non-trivial contribution to the field, and might help with better understanding of adversarial training of neural networks in more complicated settings. 
2.	The theoretical analysis seems solid. The authors introduce some interesting terms such as $\beta$-effective attack and $\beta$-robust to help the analysis, which might help in other study.
3.	Overall, the paper is well-organized and easy to follow.
","1.	The empirical results with MNIST and CIFAR-10 do not show significant difference between the performance of standard adversarial training and adversarial training using proposed reflected loss function.

2.	There are some space of improvement regarding the experiments. Specifically, please consider the followings:

(1)	The result presented in Figure 2 does not seem convincing to me because (a) the setting is too simple and hand-crafted, while the real data can behave very differently; (b) Only one specific data point ($x=[3,2,1]$) is considered. It is doubtful whether it is cherry-picked or not.

(2)	In Table 1, the robust testing error for standard training model under standard PGD attack (0.033) is much smaller than FGSM (0.286), which is unlikely since PGD usually generates stronger attack through multiple iterations. Please double-check the correctness. 

(3)	In Table 1, please consider including the evaluation of different models over clean testing data (un-attacked) for better comparison. Also, for Tables 1 and 2, please consider including the evaluation under other attacks, e.g., CW attack [1] and AutoAttack [2].

Please also consider my questions in the next part.


[1] Carlini, N., & Wagner, D. (2017, May). Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp) (pp. 39-57). ieee.

[2] Croce, F., & Hein, M. (2020, November). Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning (pp. 2206-2216). PMLR.
","1.	What feature of the reflected loss function enables the derivation of the theoretical results, or why the standard loss function make the analysis difficult? Can one replace the reflected loss function with a more general loss function? 

2.	Theorem 3.4 says that ""in at most $T_{tr}\cdots$ iterations, Algorithm 2 $\cdots$ $\textit{finds an iterate}$ $\tau$” with certain property. What does that mean exactly? Is it guaranteed that the model at the end of iteration $T_{tr}$ maintain such property? Or, is any early-stopping criteria necessary in order to guarantee the property?
",The authors have adequately addressed the limitations of the work.,['No ethics review needed.'],"6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,3 good,3 good,2 fair,1688883156165,,,NeurIPS.cc/2023/Conference,02Uc0G2Cym,"['NeurIPS.cc/2023/Conference/Submission12654/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411392916,[],4,,,['everyone'],02Uc0G2Cym,['NeurIPS.cc/2023/Conference/Submission12654/Reviewer_avt7'],1688883156165,1702411392916,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission12654/Reviewer_avt7']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature:

1. Carlini, N., & Wagner, D. (2017, May). Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp) (pp. 39-57). ieee. (referred to as [1])
2. Croce, F., & Hein, M. (2020, November). Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning (pp. 2206-2216). PMLR. (referred to as [2])

The reviewer suggests that the authors consider including the evaluation of their models under other attacks, such as CW attack [1] and AutoAttack [2], in Tables 1 and 2.",1,"2017, 2017, 2020",Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks
MqCLgMExUl,The manuscript presents a method to reparameterize and solve multiple inverse problems jointly using neural networks. The manuscript tests the proposed method on multiple inverse problems (including some chaotic problems) and compares against Neural Adjoint and BFGS baselines to show measurable performance improvements.,"* The method is simple, and the authors haven't tuned architecture for problems, which would allow their usage as drop-in replacements.
* Comparison against baselines shows the method provides noticeable improvements.","* The main downside of these methods is the added training cost (which the authors have mentioned in the limitations section)
    * I would recommend adding the training wall clock times + solving times in a table to give potential users of this method a proper estimate.
* Adding benchmarks for the same problems used in the Neural Adjoint paper would strengthen the paper.","* The paper uses BFGS to refine the solution. Would it be possible to use the solver as a predictor-corrector and incorporate BFGS in the training pipeline? Similar to methods used in Deep Equilibrium Networks [1] [2].

[1] [Neural Deep Equilibrium Solvers](https://openreview.net/forum?id=B0oHOwT5ENL)

[2] [Continuous Deep Equilibrium Models: Training Neural ODEs Faster by Integrating Them to Infinity](https://arxiv.org/abs/2201.12240)",All limitations are clearly stated.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1688662660858,,,NeurIPS.cc/2023/Conference,01GQK1gwe3,"['NeurIPS.cc/2023/Conference/Submission1581/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410794650,[],3,,,['everyone'],01GQK1gwe3,['NeurIPS.cc/2023/Conference/Submission1581/Reviewer_VvA5'],1688662660858,1702410794650,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1581/Reviewer_VvA5']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. [Neural Deep Equilibrium Solvers](https://openreview.net/forum?id=B0oHOwT5ENL)
2. [Continuous Deep Equilibrium Models: Training Neural ODEs Faster by Integrating Them to Infinity](https://arxiv.org/abs/2201.12240)

These citations are mentioned in the ""questions"" section of the review, where the reviewer asks if it would be possible to use the solver as a predictor-corrector and incorporate BFGS in the training pipeline, similar to methods used in the referenced papers.",1,2022,Neural Deep Equilibrium Solvers 
MqCLgMExUl,The manuscript presents a method to reparameterize and solve multiple inverse problems jointly using neural networks. The manuscript tests the proposed method on multiple inverse problems (including some chaotic problems) and compares against Neural Adjoint and BFGS baselines to show measurable performance improvements.,"* The method is simple, and the authors haven't tuned architecture for problems, which would allow their usage as drop-in replacements.
* Comparison against baselines shows the method provides noticeable improvements.","* The main downside of these methods is the added training cost (which the authors have mentioned in the limitations section)
    * I would recommend adding the training wall clock times + solving times in a table to give potential users of this method a proper estimate.
* Adding benchmarks for the same problems used in the Neural Adjoint paper would strengthen the paper.","* The paper uses BFGS to refine the solution. Would it be possible to use the solver as a predictor-corrector and incorporate BFGS in the training pipeline? Similar to methods used in Deep Equilibrium Networks [1] [2].

[1] [Neural Deep Equilibrium Solvers](https://openreview.net/forum?id=B0oHOwT5ENL)

[2] [Continuous Deep Equilibrium Models: Training Neural ODEs Faster by Integrating Them to Infinity](https://arxiv.org/abs/2201.12240)",All limitations are clearly stated.,['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,,Yes,3 good,3 good,3 good,1688662660858,,,NeurIPS.cc/2023/Conference,01GQK1gwe3,"['NeurIPS.cc/2023/Conference/Submission1581/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702410794650,[],3,,,['everyone'],01GQK1gwe3,['NeurIPS.cc/2023/Conference/Submission1581/Reviewer_VvA5'],1688662660858,1702410794650,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission1581/Reviewer_VvA5']","Yes, this peer review explicitly suggests the authors of the paper to cite two specific literature references:

1. [Neural Deep Equilibrium Solvers](https://openreview.net/forum?id=B0oHOwT5ENL)
2. [Continuous Deep Equilibrium Models: Training Neural ODEs Faster by Integrating Them to Infinity](https://arxiv.org/abs/2201.12240)

These citations are mentioned in the ""questions"" section of the review, where the reviewer asks if it would be possible to use the solver as a predictor-corrector and incorporate BFGS in the training pipeline, similar to methods used in the referenced papers.",1,2022,Continuous Deep Equilibrium Models Training Neural ODEs Faster by Integrating Them to Infinity
BAtxpdURoI,"The paper first proposes a new framework to analyze latent spaces in the context of generative models.
This framework takes inspiration from prior results about GANs, which allowed interpreting the min-max training objective as computing a distance between distributions to be minimized, to define a similar distance between the latent distribution and the data distribution.
From their analysis, they derive a simple two-step training for auto-encoders to learn better latents and reconstruction, in which they first train the encoder with a weak decoder to extract good latents, and then train a larger decoder to get better reconstructions.

They perform experiments in a simple toy case, and then with commonly used models such as GANs, VQGAN, and DiTs.","- The paper is easy to follow
- It proposes a novel view on latent codes, including an explanation of some different properties between SSL and generative latent codes, and a theoretical framework to describe why a powerful encoder/decoder pair can't learn a good latent code.
- The proposed practical solution is simple and their experiments show that it can improve performances in a variety of generative settings.","- While the fresh view on latent codes is interesting, it doesn't provide any theoretical guarantees.
As far as I understand, the main conclusion is that in order to obtain a good latent code the encoder and decoder should not be too powerful but is not able to give an indication about how the correct balance. Note that in the context of Variational Autoencoders, a similar conclusion had been reached before: that a too powerful decoder is a good explanation for the phenomenon called *posterior collapse* that describes a state in which the latent code is completely uninformative [1].
It is good that it is formally extended to other auto-encoders, but unsurprising.

- Because of this lack of quantification of optimal complexity, it is quite unclear whether the positive results obtained in VQGAN and DiT settings are actually related to the theoretical conclusions or not. It could very well be completely unrelated and just confirmation bias.

- Related work present different generative models with loose links to the proposed method, but does not discuss any study about the latent spaces of generative models. In addition to links with posterior collapse in VAEs [1], I would also be very interested to read what the authors have to say about sparsity and disentanglement properties of VAE [2], and PCA directions in GAN space [3] among other things.
Moreover, I have to point out that contrary to the paragraph in related works, Mask Autoencoders have been explored for their generative capabilities [4].


--- 
[1] Fixing a Broken ELBO. ICML 2018.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, Kevin Murphy.

[2] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. ICML 2019.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem

[3] GANSpace: Discovering Interpretable GAN Controls. NeurIPS 2020.
Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris

[4] MaskGIT: Masked Generative Image Transformer. CVPR 2022.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman","One minor thing that was not clear to me is Remark 3.5. Could the authors explicit why scaling invariance would be a problem in this context?

In any case, despite its limitations, I still find the paper interesting for both its theoretical and practical contributions.
I trust the authors will fix the related work section, and my current rating anticipates that the authors will demonstrate their willingness to do so.

-----
Post-rebuttal update:
Having taken into consideration the rebuttal, discussions and comments from the other reviewers, I updated my score from WA to Accept.","
The authors are very open regarding the limitations of their submission.

Potential negative societal impacts of generative models are not. Widely acknowledge ones include amplification of biases, potential misuse for propagating fake information, and concerns about lack of attribution and copyright infringements.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688594254077,,,NeurIPS.cc/2023/Conference,00EKYYu3fD,"['NeurIPS.cc/2023/Conference/Submission6838/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411081233,[],3,,,['everyone'],00EKYYu3fD,['NeurIPS.cc/2023/Conference/Submission6838/Reviewer_2zRJ'],1688594254077,1702411081233,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission6838/Reviewer_2zRJ']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that they think are relevant to the work and should be discussed in the related work section, including:
1. ""Fixing a Broken ELBO"" (ICML 2018) regarding posterior collapse in VAEs.
2. ""Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"" (ICML 2019) regarding sparsity and disentanglement properties of VAE.
3. ""GANSpace: Discovering Interpretable GAN Controls"" (NeurIPS 2020) regarding PCA directions in GAN space.
4. ""MaskGIT: Masked Generative Image Transformer"" (CVPR 2022) regarding Mask Autoencoders for generative capabilities.

The reviewer also mentions that they trust the authors will fix the related work section, implying that they expect the authors to add these citations.",1,"2018, 2019, 2020, 2022",Fixing a Broken ELBO
BAtxpdURoI,"The paper first proposes a new framework to analyze latent spaces in the context of generative models.
This framework takes inspiration from prior results about GANs, which allowed interpreting the min-max training objective as computing a distance between distributions to be minimized, to define a similar distance between the latent distribution and the data distribution.
From their analysis, they derive a simple two-step training for auto-encoders to learn better latents and reconstruction, in which they first train the encoder with a weak decoder to extract good latents, and then train a larger decoder to get better reconstructions.

They perform experiments in a simple toy case, and then with commonly used models such as GANs, VQGAN, and DiTs.","- The paper is easy to follow
- It proposes a novel view on latent codes, including an explanation of some different properties between SSL and generative latent codes, and a theoretical framework to describe why a powerful encoder/decoder pair can't learn a good latent code.
- The proposed practical solution is simple and their experiments show that it can improve performances in a variety of generative settings.","- While the fresh view on latent codes is interesting, it doesn't provide any theoretical guarantees.
As far as I understand, the main conclusion is that in order to obtain a good latent code the encoder and decoder should not be too powerful but is not able to give an indication about how the correct balance. Note that in the context of Variational Autoencoders, a similar conclusion had been reached before: that a too powerful decoder is a good explanation for the phenomenon called *posterior collapse* that describes a state in which the latent code is completely uninformative [1].
It is good that it is formally extended to other auto-encoders, but unsurprising.

- Because of this lack of quantification of optimal complexity, it is quite unclear whether the positive results obtained in VQGAN and DiT settings are actually related to the theoretical conclusions or not. It could very well be completely unrelated and just confirmation bias.

- Related work present different generative models with loose links to the proposed method, but does not discuss any study about the latent spaces of generative models. In addition to links with posterior collapse in VAEs [1], I would also be very interested to read what the authors have to say about sparsity and disentanglement properties of VAE [2], and PCA directions in GAN space [3] among other things.
Moreover, I have to point out that contrary to the paragraph in related works, Mask Autoencoders have been explored for their generative capabilities [4].


--- 
[1] Fixing a Broken ELBO. ICML 2018.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, Kevin Murphy.

[2] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. ICML 2019.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem

[3] GANSpace: Discovering Interpretable GAN Controls. NeurIPS 2020.
Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris

[4] MaskGIT: Masked Generative Image Transformer. CVPR 2022.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman","One minor thing that was not clear to me is Remark 3.5. Could the authors explicit why scaling invariance would be a problem in this context?

In any case, despite its limitations, I still find the paper interesting for both its theoretical and practical contributions.
I trust the authors will fix the related work section, and my current rating anticipates that the authors will demonstrate their willingness to do so.

-----
Post-rebuttal update:
Having taken into consideration the rebuttal, discussions and comments from the other reviewers, I updated my score from WA to Accept.","
The authors are very open regarding the limitations of their submission.

Potential negative societal impacts of generative models are not. Widely acknowledge ones include amplification of biases, potential misuse for propagating fake information, and concerns about lack of attribution and copyright infringements.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688594254077,,,NeurIPS.cc/2023/Conference,00EKYYu3fD,"['NeurIPS.cc/2023/Conference/Submission6838/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411081233,[],3,,,['everyone'],00EKYYu3fD,['NeurIPS.cc/2023/Conference/Submission6838/Reviewer_2zRJ'],1688594254077,1702411081233,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission6838/Reviewer_2zRJ']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that they think are relevant to the work and should be discussed in the related work section, including:
1. ""Fixing a Broken ELBO"" (ICML 2018) regarding posterior collapse in VAEs.
2. ""Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"" (ICML 2019) regarding sparsity and disentanglement properties of VAE.
3. ""GANSpace: Discovering Interpretable GAN Controls"" (NeurIPS 2020) regarding PCA directions in GAN space.
4. ""MaskGIT: Masked Generative Image Transformer"" (CVPR 2022) regarding Mask Autoencoders for generative capabilities.

The reviewer also mentions that they trust the authors will fix the related work section, implying that they expect the authors to add these citations.",1,"2018, 2019, 2020, 2022",Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations
BAtxpdURoI,"The paper first proposes a new framework to analyze latent spaces in the context of generative models.
This framework takes inspiration from prior results about GANs, which allowed interpreting the min-max training objective as computing a distance between distributions to be minimized, to define a similar distance between the latent distribution and the data distribution.
From their analysis, they derive a simple two-step training for auto-encoders to learn better latents and reconstruction, in which they first train the encoder with a weak decoder to extract good latents, and then train a larger decoder to get better reconstructions.

They perform experiments in a simple toy case, and then with commonly used models such as GANs, VQGAN, and DiTs.","- The paper is easy to follow
- It proposes a novel view on latent codes, including an explanation of some different properties between SSL and generative latent codes, and a theoretical framework to describe why a powerful encoder/decoder pair can't learn a good latent code.
- The proposed practical solution is simple and their experiments show that it can improve performances in a variety of generative settings.","- While the fresh view on latent codes is interesting, it doesn't provide any theoretical guarantees.
As far as I understand, the main conclusion is that in order to obtain a good latent code the encoder and decoder should not be too powerful but is not able to give an indication about how the correct balance. Note that in the context of Variational Autoencoders, a similar conclusion had been reached before: that a too powerful decoder is a good explanation for the phenomenon called *posterior collapse* that describes a state in which the latent code is completely uninformative [1].
It is good that it is formally extended to other auto-encoders, but unsurprising.

- Because of this lack of quantification of optimal complexity, it is quite unclear whether the positive results obtained in VQGAN and DiT settings are actually related to the theoretical conclusions or not. It could very well be completely unrelated and just confirmation bias.

- Related work present different generative models with loose links to the proposed method, but does not discuss any study about the latent spaces of generative models. In addition to links with posterior collapse in VAEs [1], I would also be very interested to read what the authors have to say about sparsity and disentanglement properties of VAE [2], and PCA directions in GAN space [3] among other things.
Moreover, I have to point out that contrary to the paragraph in related works, Mask Autoencoders have been explored for their generative capabilities [4].


--- 
[1] Fixing a Broken ELBO. ICML 2018.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, Kevin Murphy.

[2] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. ICML 2019.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem

[3] GANSpace: Discovering Interpretable GAN Controls. NeurIPS 2020.
Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris

[4] MaskGIT: Masked Generative Image Transformer. CVPR 2022.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman","One minor thing that was not clear to me is Remark 3.5. Could the authors explicit why scaling invariance would be a problem in this context?

In any case, despite its limitations, I still find the paper interesting for both its theoretical and practical contributions.
I trust the authors will fix the related work section, and my current rating anticipates that the authors will demonstrate their willingness to do so.

-----
Post-rebuttal update:
Having taken into consideration the rebuttal, discussions and comments from the other reviewers, I updated my score from WA to Accept.","
The authors are very open regarding the limitations of their submission.

Potential negative societal impacts of generative models are not. Widely acknowledge ones include amplification of biases, potential misuse for propagating fake information, and concerns about lack of attribution and copyright infringements.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688594254077,,,NeurIPS.cc/2023/Conference,00EKYYu3fD,"['NeurIPS.cc/2023/Conference/Submission6838/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411081233,[],3,,,['everyone'],00EKYYu3fD,['NeurIPS.cc/2023/Conference/Submission6838/Reviewer_2zRJ'],1688594254077,1702411081233,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission6838/Reviewer_2zRJ']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that they think are relevant to the work and should be discussed in the related work section, including:
1. ""Fixing a Broken ELBO"" (ICML 2018) regarding posterior collapse in VAEs.
2. ""Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"" (ICML 2019) regarding sparsity and disentanglement properties of VAE.
3. ""GANSpace: Discovering Interpretable GAN Controls"" (NeurIPS 2020) regarding PCA directions in GAN space.
4. ""MaskGIT: Masked Generative Image Transformer"" (CVPR 2022) regarding Mask Autoencoders for generative capabilities.

The reviewer also mentions that they trust the authors will fix the related work section, implying that they expect the authors to add these citations.",1,"2018, 2019, 2020, 2022",GANSpace: Discovering Interpretable GAN Controls
BAtxpdURoI,"The paper first proposes a new framework to analyze latent spaces in the context of generative models.
This framework takes inspiration from prior results about GANs, which allowed interpreting the min-max training objective as computing a distance between distributions to be minimized, to define a similar distance between the latent distribution and the data distribution.
From their analysis, they derive a simple two-step training for auto-encoders to learn better latents and reconstruction, in which they first train the encoder with a weak decoder to extract good latents, and then train a larger decoder to get better reconstructions.

They perform experiments in a simple toy case, and then with commonly used models such as GANs, VQGAN, and DiTs.","- The paper is easy to follow
- It proposes a novel view on latent codes, including an explanation of some different properties between SSL and generative latent codes, and a theoretical framework to describe why a powerful encoder/decoder pair can't learn a good latent code.
- The proposed practical solution is simple and their experiments show that it can improve performances in a variety of generative settings.","- While the fresh view on latent codes is interesting, it doesn't provide any theoretical guarantees.
As far as I understand, the main conclusion is that in order to obtain a good latent code the encoder and decoder should not be too powerful but is not able to give an indication about how the correct balance. Note that in the context of Variational Autoencoders, a similar conclusion had been reached before: that a too powerful decoder is a good explanation for the phenomenon called *posterior collapse* that describes a state in which the latent code is completely uninformative [1].
It is good that it is formally extended to other auto-encoders, but unsurprising.

- Because of this lack of quantification of optimal complexity, it is quite unclear whether the positive results obtained in VQGAN and DiT settings are actually related to the theoretical conclusions or not. It could very well be completely unrelated and just confirmation bias.

- Related work present different generative models with loose links to the proposed method, but does not discuss any study about the latent spaces of generative models. In addition to links with posterior collapse in VAEs [1], I would also be very interested to read what the authors have to say about sparsity and disentanglement properties of VAE [2], and PCA directions in GAN space [3] among other things.
Moreover, I have to point out that contrary to the paragraph in related works, Mask Autoencoders have been explored for their generative capabilities [4].


--- 
[1] Fixing a Broken ELBO. ICML 2018.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, Kevin Murphy.

[2] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. ICML 2019.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem

[3] GANSpace: Discovering Interpretable GAN Controls. NeurIPS 2020.
Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris

[4] MaskGIT: Masked Generative Image Transformer. CVPR 2022.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman","One minor thing that was not clear to me is Remark 3.5. Could the authors explicit why scaling invariance would be a problem in this context?

In any case, despite its limitations, I still find the paper interesting for both its theoretical and practical contributions.
I trust the authors will fix the related work section, and my current rating anticipates that the authors will demonstrate their willingness to do so.

-----
Post-rebuttal update:
Having taken into consideration the rebuttal, discussions and comments from the other reviewers, I updated my score from WA to Accept.","
The authors are very open regarding the limitations of their submission.

Potential negative societal impacts of generative models are not. Widely acknowledge ones include amplification of biases, potential misuse for propagating fake information, and concerns about lack of attribution and copyright infringements.",['No ethics review needed.'],"7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",,Yes,2 fair,2 fair,3 good,1688594254077,,,NeurIPS.cc/2023/Conference,00EKYYu3fD,"['NeurIPS.cc/2023/Conference/Submission6838/-/Official_Review', 'NeurIPS.cc/2023/Conference/-/Edit']",CC BY 4.0,1702411081233,[],3,,,['everyone'],00EKYYu3fD,['NeurIPS.cc/2023/Conference/Submission6838/Reviewer_2zRJ'],1688594254077,1702411081233,"['NeurIPS.cc/2023/Conference', 'NeurIPS.cc/2023/Conference/Submission6838/Reviewer_2zRJ']","Yes, the peer review explicitly suggests the authors of the paper to cite specific literature. 

The reviewer mentions several papers that they think are relevant to the work and should be discussed in the related work section, including:
1. ""Fixing a Broken ELBO"" (ICML 2018) regarding posterior collapse in VAEs.
2. ""Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"" (ICML 2019) regarding sparsity and disentanglement properties of VAE.
3. ""GANSpace: Discovering Interpretable GAN Controls"" (NeurIPS 2020) regarding PCA directions in GAN space.
4. ""MaskGIT: Masked Generative Image Transformer"" (CVPR 2022) regarding Mask Autoencoders for generative capabilities.

The reviewer also mentions that they trust the authors will fix the related work section, implying that they expect the authors to add these citations.",1,"2018, 2019, 2020, 2022",MaskGIT: Masked Generative Image Transformer
