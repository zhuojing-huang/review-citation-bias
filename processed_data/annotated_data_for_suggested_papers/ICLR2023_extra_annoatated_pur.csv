id,summary_of_the_paper,strength_and_weaknesses,"clarity,_quality,_novelty_and_reproducibility",summary_of_the_review,confidence,correctness,technical_novelty_and_significance,empirical_novelty_and_significance,flag_for_ethics_review,recommendation,cdate,ddate,forum,invitation,mdate,nonreaders,number,original,readers,replyto,signatures,tcdate,tddate,tmdate,writers,response,binary_label,suggested_years,paper_info
x0ZPj9ku30,"Test-time adaptation updates a model to reduce generalization on shifted data.
Such adaptation methods need to choose a loss for adaptation and parameters to update, and this work's main contribution is to introduce visual prompts as a parameterization.
The proposed Data-efficient Prompt Tuning (DePT) method includes additional learnable tokens in a ViT during training, then updates these tokens and the classification token during testing, by entropy minimization through pseudo-labeling and other losses.
The adaptation parameters are limited to the classification head and the proposed layer-wise prompts, which strikes a balance between expressivity and efficiency, where the number of prompts can be varied from just one layer to all layers.
This parameterization is justified by the accuracy achieved (Table 1) and analysis experiments that try alternatives like updating layer normalization parameters (Appendix B.1).
The other losses include a self-supervised regularization scheme derived from DINO, in which projections of the prompts (local representations) and the class token (the global representation) are updated to maximize their similarity.
Experiments evaluate DePT against the prior state-of-the-art AdaContrast, reproduced with the same ViT backbone as the proposed method, and other test-time methods with ResNet backbones (which are therefore not comparable), along with unsupervised domain adaptation methods that require both source and target data.
DePT achieves higher accuracies by offline adaptation on the whole test set and online adaptation on the streaming test set, with a larger margin of improvement for online adaptation.
These results are shown for domain adaptation and generalization datasets, VisDA-C and DomainNet-126, but not corruption or other robustness benchmarks like ImageNet-C or ImageNet-R.
Ablation studies show that each component of the method helps.
Although pseudo-labeling makes the largest difference, and that is already a popular part of test-time adaptation, it is pseudo-labeling as a loss to update the proposed parameterization by prompts that is helping so much.

","*Strengths*:

- This work brings test-time adaptation up-to-date w.r.t. current model architectures and parameters: ViTs are current, strong models and prompting is a timely topic.
  The choice of parameterization is novel for this purpose, low-dimensional for efficiency, and specifically-chosen for ViTs.
- Online adaptation accuracy improves on the prior state-of-the-art AdaContrast by 4 points on VISDA-C and 2 points on DomainNet (which are both standard benchmarks).
  Offline adaptation accuracy is no worse than the state-of-the-art and perhaps slightly better.
- The choice of datasets is sound. VISDA-C is a standard benchmark, and DomainNet is a more recent benchmark that has also seen adoption. The evaluation protocol for these datasets follows prior work.
- There is a self-supervised loss that does not depend on the pseudo-labels, for bottom-up adaptation to the target inputs, which may complement top-down adaptation to the model predictions.
  This loss is closely-related to DINO, which is credited accordingly, but it is somewhat customized to the use of visual prompts in this work.

*Weaknesses*:

- While DePT efficient in its parameter dimension, that does not mean it is efficient in its update computation. For DePT, the prompts are distributed throughout the model, and so optimization has to compute many gradients across layers even if the parameter gradients are themselves much smaller.
  The amount of computation required during testing is not described, nor is it compared against baselines.
- There is only one comparable baseline with the same model architecture (AdaContrast). For thoroughness, it would be useful to adapt a prior method that reported results with ResNets to ViTs for comparison. Tent for instance simply minimizes the entropy of predictions for a given choice of parameters, and could be ported to ViTs by updating the layer norm parameters (for one example).
- There are no results for other common choices of robustness or domain generalization benchmark, such as ImageNet-C (or its other variations ImageNet-V2, ImageNet-R, etc.), or PACS/VLCS/etc. These are included in many prior works such as TENT, SHOT, and TTT.
- There is missing related work on learning and tuning visual prompts, although neither addresses the application of visual prompts to test-time adaptation against shifts, as done in this work.
  - [Interactive Image Segmentation via Backpropagating Refinement Scheme](https://vcg.seas.harvard.edu/publications/interactive-image-segmentation-via-backpropagating-refinement-scheme/paper). Jang & Kim, CVPR'19. This paper prefigures visual prompts by perturbing the input according to a model's own predictions as a kind of self-training to improve accuracy. Citing it would help better trace the history of ideas.
  - [Exploring Visual Prompts for Adapting Large-Scale Models](https://arxiv.org/abs/2203.17274). Bahng et al. arXiv'22. This is highly related to the cited VPT by Jia et al. 2022 and should be considered for reference alongside it.
- The proposed method is not fully test-time because the prompts must be jointly trained with the source model parameters. Its setting should be better identified in the text, and the experiments should compare to the latest source-free methods accordingly, like SHOT++ (Source Data-absent Unsupervised Domain Adaptation through Hypothesis Transfer and Labeling Transfer. Liang et al. PAMI'21.)

","*Clarity*:

- This work can be understood, but it could be more accessible. The exposition would be improved by making introductory statements more definite and citing established vocabulary when it is first used. There are grammatical errors that take time to read through, so proofreading would also make this work more readable.
- The introduction does not fully explain the setting. It should define the test-time adaptation setting and contrast it with source-free adaptation, as the two are sometimes confused, and the established unsupervised domain adaptation setting, which is often synonymous with ""domain adaptation"" in the literature.
- The summarization of the memory bank in Figure 2(B) does not adequately explain the aggregated prompt or how the contents of the memory bank are initialized and updated. The caption should explain the multiple losses, if only briefly, and indicate how the memory bank alters the pseudo-labels.
- Examples are relegated to the appendix, such as appendix B.1, which would better serve the reader as part of the main paper. B.1 in particular has key motivating results for visual prompts as parameters.
- Overall the writing, figures, and tables are adequate for communicating the method and results, with only a few exceptions.

*Quality*:

- The results improve on standard benchmarks, by as much as +4 points when comparing to the prir state-of-the-art and controlling for backbone, which is a larger margin than is sometimes reported in papers on test-time adaptation (where improvements may be +1 or +2 points).
- The method is sensible, as visual prompting has been shown to work elsewhere such as for transfer learning, and the experiments justify this choice of parameterization for the purpose of test-time adaptation.
- The related work could better credit other visual prompting work that has been done (see weaknesses), but at least the application contributed here for test-time adaptation is executed well and delivers an improvement.

*Novelty*:

- Test-time adaptation to shift is a novel application of visual prompts in the form of additional learnable tokens. The experiments on choices of parameterization for test-time adaptation are empirically novel and informative.
- The pseudo-labeling update (Section 3.2.1) is not novel, as moving statistics in the form of exponential moving averages and student-teacher updates are common for self-supervision (MoCo, ODIN, ...) and adaptation (SHOT, DINE, ...).
- The self-supervision update (Section 3.2.2) is essentially DINO, as cited, but it has a few of its own implementation details including a diversity regularizer across the leraned prompts.
- Adapting to smaller amounts of data (Figure 3) is not novel, as claimed in the abstract, because online test-time adaptation methods already only adapt to the data given for testing.
  The significance of its insensitivity to the amount of data for adaptation is also more theoretical than practical, as knowing when to adapt or not is itself a problem, which is why prior methods like TTT or Tent keep adapting.

*Reproducibility*:

The explanation and appendices have a fair amount of detail, but given the number of terms in the loss and variety of update schemes (teacher-student, online memory, EMA, etc.) this work may not be reproducible from the paper alone.
There is no statement about releasing the code.

","In short, the method does work as shown by its evaluation on the standard datasets of VisDA-C and DomainNet.
However, the evaluation could be broader in its datasets and more thorough in its comparable baselines by reproducing more than one method with the ViT backbone.
This is an expectation for work in this area, as many past test-time adaptation papers have evaluated on several datasets of shifts as a way to shown some measure of generality.
All in all, there is value in demonstrating that test-time adaptation applies to ViT models too, and in exploring the right choice of parameterization for this purpose.
While this work could be improved, pushing test-time adaptation to use the most accurate architectures at present may be sufficiently informative to the community.

*For Rebuttal*

1. Please measure the time efficiency of DePT updates, and relate the computation required to update the prompts to the computation required to update normalization layers or other choices of parameters, as used by the compared methods like TENT and SHOT.
2. Please discuss DePT as a source-free (SHOT) vs. test-time (TENT) vs. intermediate (TTT) method. The choice of setting should guide the choice of comparisons.
3. Please report results for ImageNet-C, if possible, because it is a common benchmark for robustness and adaptation to corruption. (It is alright if this is not possible due to computational limitations, but doing it would be a plus.)
4. Please motivate the low data regime reported in the results. Is there a practical deployment that this setting addresses?

*Other Feedback*

- Title
  - Is the method a test-time adaptation method? Test-time methods update during testing, as DePT does, but in some usages ""test-time"" also indicates that the method can update online and that it does not require changes to training. DePT seems to be more like SHOT or TTT in that it alters training.
- Abstract
  - Name the datasets, benchmarks, and settings evaluated. A precise summary of experiments is more eye-catching to the potential reader.
- Related Work
  - The formatting with subsections takes up a lot of space. Consider revising it to remove the subsection headings and use more bolded heading instead. Then the paper would have more room for further explanation or experiments.
- Analysis
  - Table 3 is not an impressive comparison, because DePT has been augmented to include domain-specific and domain-shared parameters while SHOT has not been. Other methods like SHOT could also have domain-specific layers, as done by residual adapters (Rebuffi et al.) for example.
  - Table 4 should include the accuracy without DePT in the caption to help ground the relative improvement of each number of prompts.
- Proofreading
  - ""Second, given only unlabeled target domain data, use what kind of learning objective for optimization."" Should this be a question? Please revise.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666909319278,,3HnIBTjlXTS,ICLR.cc/2023/Conference/Paper2959/-/Official_Review,,[],4,,['everyone'],3HnIBTjlXTS,['ICLR.cc/2023/Conference/Paper2959/Reviewer_zNVW'],1666909319278,,1666949704044,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2959/Reviewer_zNVW']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions two specific papers that are not cited in the original paper:
1. ""Interactive Image Segmentation via Backpropagating Refinement Scheme"" by Jang & Kim, CVPR'19.
2. ""Exploring Visual Prompts for Adapting Large-Scale Models"" by Bahng et al., arXiv'22.

Additionally, the reviewer also mentions other related works such as SHOT++, TENT, and DINO, but it seems that these are already discussed in the original paper. However, the reviewer suggests that the authors should have credited other visual prompting work that has been done, implying that there may be other relevant literature that is not cited in the original paper.",1,"2019, 2022",Interactive Image Segmentation via Backpropagating Refinement Scheme 
x0ZPj9ku30,"Test-time adaptation updates a model to reduce generalization on shifted data.
Such adaptation methods need to choose a loss for adaptation and parameters to update, and this work's main contribution is to introduce visual prompts as a parameterization.
The proposed Data-efficient Prompt Tuning (DePT) method includes additional learnable tokens in a ViT during training, then updates these tokens and the classification token during testing, by entropy minimization through pseudo-labeling and other losses.
The adaptation parameters are limited to the classification head and the proposed layer-wise prompts, which strikes a balance between expressivity and efficiency, where the number of prompts can be varied from just one layer to all layers.
This parameterization is justified by the accuracy achieved (Table 1) and analysis experiments that try alternatives like updating layer normalization parameters (Appendix B.1).
The other losses include a self-supervised regularization scheme derived from DINO, in which projections of the prompts (local representations) and the class token (the global representation) are updated to maximize their similarity.
Experiments evaluate DePT against the prior state-of-the-art AdaContrast, reproduced with the same ViT backbone as the proposed method, and other test-time methods with ResNet backbones (which are therefore not comparable), along with unsupervised domain adaptation methods that require both source and target data.
DePT achieves higher accuracies by offline adaptation on the whole test set and online adaptation on the streaming test set, with a larger margin of improvement for online adaptation.
These results are shown for domain adaptation and generalization datasets, VisDA-C and DomainNet-126, but not corruption or other robustness benchmarks like ImageNet-C or ImageNet-R.
Ablation studies show that each component of the method helps.
Although pseudo-labeling makes the largest difference, and that is already a popular part of test-time adaptation, it is pseudo-labeling as a loss to update the proposed parameterization by prompts that is helping so much.

","*Strengths*:

- This work brings test-time adaptation up-to-date w.r.t. current model architectures and parameters: ViTs are current, strong models and prompting is a timely topic.
  The choice of parameterization is novel for this purpose, low-dimensional for efficiency, and specifically-chosen for ViTs.
- Online adaptation accuracy improves on the prior state-of-the-art AdaContrast by 4 points on VISDA-C and 2 points on DomainNet (which are both standard benchmarks).
  Offline adaptation accuracy is no worse than the state-of-the-art and perhaps slightly better.
- The choice of datasets is sound. VISDA-C is a standard benchmark, and DomainNet is a more recent benchmark that has also seen adoption. The evaluation protocol for these datasets follows prior work.
- There is a self-supervised loss that does not depend on the pseudo-labels, for bottom-up adaptation to the target inputs, which may complement top-down adaptation to the model predictions.
  This loss is closely-related to DINO, which is credited accordingly, but it is somewhat customized to the use of visual prompts in this work.

*Weaknesses*:

- While DePT efficient in its parameter dimension, that does not mean it is efficient in its update computation. For DePT, the prompts are distributed throughout the model, and so optimization has to compute many gradients across layers even if the parameter gradients are themselves much smaller.
  The amount of computation required during testing is not described, nor is it compared against baselines.
- There is only one comparable baseline with the same model architecture (AdaContrast). For thoroughness, it would be useful to adapt a prior method that reported results with ResNets to ViTs for comparison. Tent for instance simply minimizes the entropy of predictions for a given choice of parameters, and could be ported to ViTs by updating the layer norm parameters (for one example).
- There are no results for other common choices of robustness or domain generalization benchmark, such as ImageNet-C (or its other variations ImageNet-V2, ImageNet-R, etc.), or PACS/VLCS/etc. These are included in many prior works such as TENT, SHOT, and TTT.
- There is missing related work on learning and tuning visual prompts, although neither addresses the application of visual prompts to test-time adaptation against shifts, as done in this work.
  - [Interactive Image Segmentation via Backpropagating Refinement Scheme](https://vcg.seas.harvard.edu/publications/interactive-image-segmentation-via-backpropagating-refinement-scheme/paper). Jang & Kim, CVPR'19. This paper prefigures visual prompts by perturbing the input according to a model's own predictions as a kind of self-training to improve accuracy. Citing it would help better trace the history of ideas.
  - [Exploring Visual Prompts for Adapting Large-Scale Models](https://arxiv.org/abs/2203.17274). Bahng et al. arXiv'22. This is highly related to the cited VPT by Jia et al. 2022 and should be considered for reference alongside it.
- The proposed method is not fully test-time because the prompts must be jointly trained with the source model parameters. Its setting should be better identified in the text, and the experiments should compare to the latest source-free methods accordingly, like SHOT++ (Source Data-absent Unsupervised Domain Adaptation through Hypothesis Transfer and Labeling Transfer. Liang et al. PAMI'21.)

","*Clarity*:

- This work can be understood, but it could be more accessible. The exposition would be improved by making introductory statements more definite and citing established vocabulary when it is first used. There are grammatical errors that take time to read through, so proofreading would also make this work more readable.
- The introduction does not fully explain the setting. It should define the test-time adaptation setting and contrast it with source-free adaptation, as the two are sometimes confused, and the established unsupervised domain adaptation setting, which is often synonymous with ""domain adaptation"" in the literature.
- The summarization of the memory bank in Figure 2(B) does not adequately explain the aggregated prompt or how the contents of the memory bank are initialized and updated. The caption should explain the multiple losses, if only briefly, and indicate how the memory bank alters the pseudo-labels.
- Examples are relegated to the appendix, such as appendix B.1, which would better serve the reader as part of the main paper. B.1 in particular has key motivating results for visual prompts as parameters.
- Overall the writing, figures, and tables are adequate for communicating the method and results, with only a few exceptions.

*Quality*:

- The results improve on standard benchmarks, by as much as +4 points when comparing to the prir state-of-the-art and controlling for backbone, which is a larger margin than is sometimes reported in papers on test-time adaptation (where improvements may be +1 or +2 points).
- The method is sensible, as visual prompting has been shown to work elsewhere such as for transfer learning, and the experiments justify this choice of parameterization for the purpose of test-time adaptation.
- The related work could better credit other visual prompting work that has been done (see weaknesses), but at least the application contributed here for test-time adaptation is executed well and delivers an improvement.

*Novelty*:

- Test-time adaptation to shift is a novel application of visual prompts in the form of additional learnable tokens. The experiments on choices of parameterization for test-time adaptation are empirically novel and informative.
- The pseudo-labeling update (Section 3.2.1) is not novel, as moving statistics in the form of exponential moving averages and student-teacher updates are common for self-supervision (MoCo, ODIN, ...) and adaptation (SHOT, DINE, ...).
- The self-supervision update (Section 3.2.2) is essentially DINO, as cited, but it has a few of its own implementation details including a diversity regularizer across the leraned prompts.
- Adapting to smaller amounts of data (Figure 3) is not novel, as claimed in the abstract, because online test-time adaptation methods already only adapt to the data given for testing.
  The significance of its insensitivity to the amount of data for adaptation is also more theoretical than practical, as knowing when to adapt or not is itself a problem, which is why prior methods like TTT or Tent keep adapting.

*Reproducibility*:

The explanation and appendices have a fair amount of detail, but given the number of terms in the loss and variety of update schemes (teacher-student, online memory, EMA, etc.) this work may not be reproducible from the paper alone.
There is no statement about releasing the code.

","In short, the method does work as shown by its evaluation on the standard datasets of VisDA-C and DomainNet.
However, the evaluation could be broader in its datasets and more thorough in its comparable baselines by reproducing more than one method with the ViT backbone.
This is an expectation for work in this area, as many past test-time adaptation papers have evaluated on several datasets of shifts as a way to shown some measure of generality.
All in all, there is value in demonstrating that test-time adaptation applies to ViT models too, and in exploring the right choice of parameterization for this purpose.
While this work could be improved, pushing test-time adaptation to use the most accurate architectures at present may be sufficiently informative to the community.

*For Rebuttal*

1. Please measure the time efficiency of DePT updates, and relate the computation required to update the prompts to the computation required to update normalization layers or other choices of parameters, as used by the compared methods like TENT and SHOT.
2. Please discuss DePT as a source-free (SHOT) vs. test-time (TENT) vs. intermediate (TTT) method. The choice of setting should guide the choice of comparisons.
3. Please report results for ImageNet-C, if possible, because it is a common benchmark for robustness and adaptation to corruption. (It is alright if this is not possible due to computational limitations, but doing it would be a plus.)
4. Please motivate the low data regime reported in the results. Is there a practical deployment that this setting addresses?

*Other Feedback*

- Title
  - Is the method a test-time adaptation method? Test-time methods update during testing, as DePT does, but in some usages ""test-time"" also indicates that the method can update online and that it does not require changes to training. DePT seems to be more like SHOT or TTT in that it alters training.
- Abstract
  - Name the datasets, benchmarks, and settings evaluated. A precise summary of experiments is more eye-catching to the potential reader.
- Related Work
  - The formatting with subsections takes up a lot of space. Consider revising it to remove the subsection headings and use more bolded heading instead. Then the paper would have more room for further explanation or experiments.
- Analysis
  - Table 3 is not an impressive comparison, because DePT has been augmented to include domain-specific and domain-shared parameters while SHOT has not been. Other methods like SHOT could also have domain-specific layers, as done by residual adapters (Rebuffi et al.) for example.
  - Table 4 should include the accuracy without DePT in the caption to help ground the relative improvement of each number of prompts.
- Proofreading
  - ""Second, given only unlabeled target domain data, use what kind of learning objective for optimization."" Should this be a question? Please revise.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666909319278,,3HnIBTjlXTS,ICLR.cc/2023/Conference/Paper2959/-/Official_Review,,[],4,,['everyone'],3HnIBTjlXTS,['ICLR.cc/2023/Conference/Paper2959/Reviewer_zNVW'],1666909319278,,1666949704044,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2959/Reviewer_zNVW']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions two specific papers that are not cited in the original paper:
1. ""Interactive Image Segmentation via Backpropagating Refinement Scheme"" by Jang & Kim, CVPR'19.
2. ""Exploring Visual Prompts for Adapting Large-Scale Models"" by Bahng et al., arXiv'22.

Additionally, the reviewer also mentions other related works such as SHOT++, TENT, and DINO, but it seems that these are already discussed in the original paper. However, the reviewer suggests that the authors should have credited other visual prompting work that has been done, implying that there may be other relevant literature that is not cited in the original paper.",1,"2019, 2022",Exploring Visual Prompts for Adapting Large-Scale Models
CL7_mkdUjpr,This paper proposes a self-supervised skeleton representation learning method based on the BYOL and hyperbolic space. The experimental results show that the proposed HYSP model achieves state-of-the-art performances on three public datasets. ,"Pros:

(1) This is the first self-supervised skeleton representation work on hyperbolic space.

(2) This paper is well written. 


Cons:

(1) The proposal seems to be incremental based on prior works. HYSP is not the first method that uses the hyperbolic space in skeleton action recognition [1*] and uses the BYOL for self-supervised skeleton representation learning [2*]. 

(2) Why only BYOL?: In my opinion, a better way to show the hyperbolic space is useful is to combine the hyperbolic space with multiple self-supervised learning baselines. From the results in Table 3 (w/ neg.), the hyperbolic space seems not to work well on MoCo. 

(3)  The linear evaluation results are not as good as SOTA methods. And the transfer learning results are missing. These two evaluation protocols are more important than the semi-supervised and supervised settings, regarding showing the effectiveness of self-supervised methods, in my opinion.

(4) Table 3: the BYOL-only result (w/o hyper.) may be not reasonable. 


  

[1*] Mix Dimension in Poincar\'{e} Geometry for 3D Skeleton-based Action Recognition. ACM MM 2020. 

[2*] Bootstrapped Representation Learning for Skeleton-Based Action Recognition. CVPRW 2022. ","Clarity: 
This paper is well-written.

Quality:
The paper has minor technical flaws.

Novelty:
The novelty of the proposed method is marginal 

Reproducibility:
The authors have provided the code in the supplemental materials.","As mentioned in the 'Strength And Weaknesses', both the hyperbolic space and BYOL are not new in skeleton action recognition. So I tend to give the 'marginally below the acceptance threshold'.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666426172175,,3Bh6sRPKS3J,ICLR.cc/2023/Conference/Paper2516/-/Official_Review,,[],2,,['everyone'],3Bh6sRPKS3J,['ICLR.cc/2023/Conference/Paper2516/Reviewer_V8KY'],1666426172175,,1670034337310,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2516/Reviewer_V8KY']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers ([1*] and [2*]) that are relevant to the proposed method, implying that the authors should be aware of these works and potentially discuss or compare their method with them. 

The mentioned papers are:
[1*] Mix Dimension in Poincar\'{e} Geometry for 3D Skeleton-based Action Recognition. ACM MM 2020.
[2*] Bootstrapped Representation Learning for Skeleton-Based Action Recognition. CVPRW 2022.

The reviewer uses these references to support their points about the novelty and originality of the proposed method, suggesting that the authors' work may not be entirely new and may build upon existing research.",1,"2020, 2022",Mix Dimension in Poincaré Geometry for 3D Skeleton-based Action Recognition 
CL7_mkdUjpr,This paper proposes a self-supervised skeleton representation learning method based on the BYOL and hyperbolic space. The experimental results show that the proposed HYSP model achieves state-of-the-art performances on three public datasets. ,"Pros:

(1) This is the first self-supervised skeleton representation work on hyperbolic space.

(2) This paper is well written. 


Cons:

(1) The proposal seems to be incremental based on prior works. HYSP is not the first method that uses the hyperbolic space in skeleton action recognition [1*] and uses the BYOL for self-supervised skeleton representation learning [2*]. 

(2) Why only BYOL?: In my opinion, a better way to show the hyperbolic space is useful is to combine the hyperbolic space with multiple self-supervised learning baselines. From the results in Table 3 (w/ neg.), the hyperbolic space seems not to work well on MoCo. 

(3)  The linear evaluation results are not as good as SOTA methods. And the transfer learning results are missing. These two evaluation protocols are more important than the semi-supervised and supervised settings, regarding showing the effectiveness of self-supervised methods, in my opinion.

(4) Table 3: the BYOL-only result (w/o hyper.) may be not reasonable. 


  

[1*] Mix Dimension in Poincar\'{e} Geometry for 3D Skeleton-based Action Recognition. ACM MM 2020. 

[2*] Bootstrapped Representation Learning for Skeleton-Based Action Recognition. CVPRW 2022. ","Clarity: 
This paper is well-written.

Quality:
The paper has minor technical flaws.

Novelty:
The novelty of the proposed method is marginal 

Reproducibility:
The authors have provided the code in the supplemental materials.","As mentioned in the 'Strength And Weaknesses', both the hyperbolic space and BYOL are not new in skeleton action recognition. So I tend to give the 'marginally below the acceptance threshold'.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666426172175,,3Bh6sRPKS3J,ICLR.cc/2023/Conference/Paper2516/-/Official_Review,,[],2,,['everyone'],3Bh6sRPKS3J,['ICLR.cc/2023/Conference/Paper2516/Reviewer_V8KY'],1666426172175,,1670034337310,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2516/Reviewer_V8KY']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers ([1*] and [2*]) that are relevant to the proposed method, implying that the authors should be aware of these works and potentially discuss or compare their method with them. 

The mentioned papers are:
[1*] Mix Dimension in Poincar\'{e} Geometry for 3D Skeleton-based Action Recognition. ACM MM 2020.
[2*] Bootstrapped Representation Learning for Skeleton-Based Action Recognition. CVPRW 2022.

The reviewer uses these references to support their points about the novelty and originality of the proposed method, suggesting that the authors' work may not be entirely new and may build upon existing research.",1,"2020, 2022",Bootstrapped Representation Learning for Skeleton-Based Action Recognition
RoE8SzRwRGj,"The paper proposes a framework (called PRESTO) for simultaneously partitioning the data space and learning a separate model in each partition. The framework relies on solving an optimization problem with both discrete variables (data partitioning) and continuous parameters 
(model parameters) using a constrained optimization method.  ","Stength
 - An interesting problem formulation is presented.
 - Partitioning is obtained through explicit specification of the partitions without resorting to a clustering model.
 - Theoretical bounds are provided.
 - An approximate solution to the constrained optimization problem is proposed.

Weaknesses
  - Method presentation (section 3.2) and algorithm description are not clear, thus they are difficult to comprehend.
  - The method can not directly provide the partition assignments of new examples. Therefore an additional classification 
    model should be trained for the assignment task. 
  - Evaluation is insufficient: PRESTO is not compared to various methods belonging to the Mixture of Experts category.
  - Time complexity is not reported","The paper proposes an interesting formulation and includes approximation results. 
However, it lacks in terms of clarity, especially in what concerns the 'outline of the PRESTO algorithm' (section 3.2) and the corresponding pseudocode (Algorithm 1).
In what concerns reproducibility, the authors provide the code of PRESTO along with details on the used datasets. ","The paper proposes a framework (called PRESTO) for simultaneously partitioning the data space and learning a separate model in each partition. The framework relies on solving an optimization problem with both discrete variables (data partitioning) and continuous parameters (model parameters) using a constrained optimization method.

The idea is interesting and supported by theoretical results. 

However, there are several concerns regarding the manuscript:
1) I think that PRESTO resembles with the Mixture of Experts (MoE) family of methods. Nevertheless, the paper does not include any reference and comparison with MoE.
2) The authors compare only with the typical (trivial) case where partitioning is first obtained using clustering and then a separate model is trained on each partition. Even in this case, the results in Table 1 do not indicate any particular accuracy improvement.
4) The time complexity of the method should be specified and compared to the methods in Table 1. It should be noted that the method must run for various values of K in order for the best number of partitions to be determined through cross-validation.
5) I do not think that the method could be an efficient alternative for k-means of Bergman clustering as mentioned in section 2.3.
6) If mixture of SVMs is a special case, why not implementing PRESTO with SVMs as classifiers and compare with the original mixture of SVMs method?
7) In my opinion PRESTO does not perform mixture modeling. Mixture models are statistical models, PRESTO is a domain decomposition approach.
8) The intuition behind the a_G hyperparameter should be provided.
9) Does PRESTO algorithm converge? What is the role of the 'Iterations' parameter in Algorithm 1.  
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666421667051,,36g8Ept_CCj,ICLR.cc/2023/Conference/Paper5833/-/Official_Review,,[],2,,['everyone'],36g8Ept_CCj,['ICLR.cc/2023/Conference/Paper5833/Reviewer_f3vu'],1666421667051,,1666421667051,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5833/Reviewer_f3vu']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

Specifically, the reviewer mentions the ""Mixture of Experts (MoE) family of methods"" and suggests that the authors should include references and comparisons with MoE, implying that this literature is not already discussed in the original paper. Additionally, the reviewer mentions ""mixture of SVMs method"", which might also be a literature that is not already included in the original paper. 

However, without access to the original paper, it's impossible to confirm whether these literatures are indeed not already discussed. But based on the review, it seems that the reviewer is suggesting the authors to include these references to strengthen their work.",1,,Mixture of Experts 
RoE8SzRwRGj,"The paper proposes a framework (called PRESTO) for simultaneously partitioning the data space and learning a separate model in each partition. The framework relies on solving an optimization problem with both discrete variables (data partitioning) and continuous parameters 
(model parameters) using a constrained optimization method.  ","Stength
 - An interesting problem formulation is presented.
 - Partitioning is obtained through explicit specification of the partitions without resorting to a clustering model.
 - Theoretical bounds are provided.
 - An approximate solution to the constrained optimization problem is proposed.

Weaknesses
  - Method presentation (section 3.2) and algorithm description are not clear, thus they are difficult to comprehend.
  - The method can not directly provide the partition assignments of new examples. Therefore an additional classification 
    model should be trained for the assignment task. 
  - Evaluation is insufficient: PRESTO is not compared to various methods belonging to the Mixture of Experts category.
  - Time complexity is not reported","The paper proposes an interesting formulation and includes approximation results. 
However, it lacks in terms of clarity, especially in what concerns the 'outline of the PRESTO algorithm' (section 3.2) and the corresponding pseudocode (Algorithm 1).
In what concerns reproducibility, the authors provide the code of PRESTO along with details on the used datasets. ","The paper proposes a framework (called PRESTO) for simultaneously partitioning the data space and learning a separate model in each partition. The framework relies on solving an optimization problem with both discrete variables (data partitioning) and continuous parameters (model parameters) using a constrained optimization method.

The idea is interesting and supported by theoretical results. 

However, there are several concerns regarding the manuscript:
1) I think that PRESTO resembles with the Mixture of Experts (MoE) family of methods. Nevertheless, the paper does not include any reference and comparison with MoE.
2) The authors compare only with the typical (trivial) case where partitioning is first obtained using clustering and then a separate model is trained on each partition. Even in this case, the results in Table 1 do not indicate any particular accuracy improvement.
4) The time complexity of the method should be specified and compared to the methods in Table 1. It should be noted that the method must run for various values of K in order for the best number of partitions to be determined through cross-validation.
5) I do not think that the method could be an efficient alternative for k-means of Bergman clustering as mentioned in section 2.3.
6) If mixture of SVMs is a special case, why not implementing PRESTO with SVMs as classifiers and compare with the original mixture of SVMs method?
7) In my opinion PRESTO does not perform mixture modeling. Mixture models are statistical models, PRESTO is a domain decomposition approach.
8) The intuition behind the a_G hyperparameter should be provided.
9) Does PRESTO algorithm converge? What is the role of the 'Iterations' parameter in Algorithm 1.  
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666421667051,,36g8Ept_CCj,ICLR.cc/2023/Conference/Paper5833/-/Official_Review,,[],2,,['everyone'],36g8Ept_CCj,['ICLR.cc/2023/Conference/Paper5833/Reviewer_f3vu'],1666421667051,,1666421667051,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5833/Reviewer_f3vu']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

Specifically, the reviewer mentions the ""Mixture of Experts (MoE) family of methods"" and suggests that the authors should include references and comparisons with MoE, implying that this literature is not already discussed in the original paper. Additionally, the reviewer mentions ""mixture of SVMs method"", which might also be a literature that is not already included in the original paper. 

However, without access to the original paper, it's impossible to confirm whether these literatures are indeed not already discussed. But based on the review, it seems that the reviewer is suggesting the authors to include these references to strengthen their work.",1,,Mixture of SVMs
XliUA0ZiA1x,"This paper proposes a new backdoor attack. Under the proposed threat model, the attacker aims to use semantically similar images to the target class to activate the backdoor. For instance, green cars can be used as semantically similar images to frogs in CIFAR-10. As such, one may re-label those green car images into frogs to create a poisoned data set. The paper argues that under this regime, known as clean-sample backdoor attacks, the attacker requires a weaker set of requirements and can only activate the backdoor without modification of the underlying test samples. This contrasts perturbation-based backdoor attacks that need to attach triggers during test time. To find semantically similar images for a particular target class, the paper uses a score function over the entire set of data that assigns a value between 0 and 1 to any specific input. This score function is supposed to indicate which samples (that come from a non-target class) have similar features to the target class so that they can be used as backdoors. This function is trained in conjunction with the target neural network and is used to select a handful of samples as the backdoor data. Theoretically, the paper shows that as the number of training samples grows, the generalization bounds between the optimal solutions for benign and backdoor empirical risks approach zero. Experimental results over Fashion-MNIST, CIFAR-10, and GTSRB are provided.","### Strengths:
- The proposed method seems interesting. In particular, it is intriguing to see that one can find semantically similar images to the target class and use those samples as the backdoor activation.

- The theoretical analysis seems relevant to the paper.

- The experimental results, especially Figures 3 and 4, beautifully depict the overall idea of the paper.

### Weaknesses:
- A major concern with the current version is the presentation of the paper, especially the proposed method. Many aspects of the proposed method are left to the reader's interpretation, and the paper does not discuss them properly. These include:
   1. In Eq. (1), for the first time the reader faces the objective function for training the scoring mechanism $g_{W}(\boldsymbol{x}_i)$. However, the paper does not properly discuss the objective function's meaning. In particular, what does Eq. (1) imply beyond mathematical terms? Please explain this in accessible language to the readers. Additionally, the symbol $m$ is used in this equation for the first time. However, its definition is left toward the end of Eq. (2) in the next page.

  2. In the definition of $P_{m}(W)$, it takes the reader a while to figure out the equivalency of the minimizer of $P_{m}(W)$ with the constraints in Eq. (2). The paper should mention that  $P_{m}(W)$ is always non-negative, and why is that the case? Discuss how the second term $\sum_{j=1}^n g_W\left(\boldsymbol{x}_j\right)\left(1-g_W\left(\boldsymbol{x}_j\right)\right)$ is always non-negative and remind the reader that you assumed $0 \leq g_W\left(\boldsymbol{x}_j\right) \leq 1$.

  3. A similar pattern can be seen during the theoretical results in Sec. 4.2. While the theory is mathematically presented, it would be better to accompany it with further discussions to let the readers grasp what those theorems mean in simplistic language. Also, could you please elaborate on why Assumption 1 is practical?

- Another critical issue with the current work is its motivations. In Sec. 3.1. it is mentioned that the proposed clean-sample threat model is **weaker** in terms of its requirements. However, the current attack requires being present during training, which makes it easier for the attacker. The previous backdoor poisonings could be used to poison the data only without requiring any control over the training mechanism. As such, the requirements of the current attack are not weaker than existing methods. Besides, there is a great body of literature on *feature collision* attacks that share many characteristics with the current approach, e.g., see [1,2]. However, none of them are mentioned and discussed in the paper.

- Finally, the experimental results need more investigations and baselines. The current method is indeed inherently different from perturbation-based backdoor attacks, but comparing the proposed method against them would help the readers understand the bigger picture. More importantly, a comparison on existing baselines for the detection of backdoor samples could show how much the selected backdoor samples in the paper are successful in circumventing the existing defense mechanisms. Also, investigating the transferability of the backdoor samples between models is another important venue. In other words, what does happen if one generates a poisoned dataset for a model $f_{1}(\cdot)$ but trains a classifier $f_{2}(\cdot)$ with that data? Could the backdoors be transferred between models? Additionally, what are the proposed method's computational burdens compared to vanilla training? Finally, a detailed investigation of the effects of different hyper-parameters on the model's performance over tasks I-III is also missing.

**Minor comment**: The remark mentioned in footnote 1 is inaccurate. One could poison the training data with perturbation-based triggers and maintain the training data size. For instance, see [3].


[1] Shafahi, Ali, et al. ""Poison frogs! targeted clean-label poisoning attacks on neural networks,"" *NeurIPS*, 2018.

[2] Saha, Aniruddha, et al. ""Hidden trigger backdoor attacks,"" *AAAI*, 2020.

[3] Turner, Alexander, et al. ""Label-consistent backdoor attacks,"" *arXiv preprint arXiv:1912.02771* (2019).","Detailed comments are given in the **Strength And Weaknesses**. Below is a summary:

#### **Clarity**: the paper needs some work to explain the proposed approach in an accessible language. For further info, please see the first part of the weaknesses.

#### **Quality**: as mentioned above, the paper requires more straightforward language, better motivation, and further experimentation. These issues place the paper in a moderate quality.

#### **Novelty And Reproducibility**: The paper presents an interesting attack in terms of novelty. However, different aspects of the current work need to be investigated better. Furthermore, in terms of reproducibility, the paper neither provides the code nor a detailed set of hyper-parameters for the results to be re-created. ","Based on my detailed comments above, I feel the paper needs some work. As such, I vote for its rejection at this stage. Meanwhile, I need to read my peers' reviews and wait for the authors' responses to make my final decision.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666598135341,,33daZzvuzY6,ICLR.cc/2023/Conference/Paper4673/-/Official_Review,,[],1,,['everyone'],33daZzvuzY6,['ICLR.cc/2023/Conference/Paper4673/Reviewer_e5gT'],1666598135341,,1666598135341,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4673/Reviewer_e5gT']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions three specific papers ([1], [2], and [3]) that are related to the topic of backdoor attacks and feature collision attacks, and notes that they are not mentioned or discussed in the paper. These papers are:

* Shafahi, Ali, et al. ""Poison frogs! targeted clean-label poisoning attacks on neural networks,"" *NeurIPS*, 2018.
* Saha, Aniruddha, et al. ""Hidden trigger backdoor attacks,"" *AAAI*, 2020.
* Turner, Alexander, et al. ""Label-consistent backdoor attacks,"" *arXiv preprint arXiv:1912.02771* (2019).

The reviewer suggests that the authors should discuss and compare their work with these existing methods to provide a more comprehensive understanding of the topic.",1,"2018, 2020, 2019, 2019",Poison frogs targeted clean-label poisoning attacks on neural networks
XliUA0ZiA1x,"This paper proposes a new backdoor attack. Under the proposed threat model, the attacker aims to use semantically similar images to the target class to activate the backdoor. For instance, green cars can be used as semantically similar images to frogs in CIFAR-10. As such, one may re-label those green car images into frogs to create a poisoned data set. The paper argues that under this regime, known as clean-sample backdoor attacks, the attacker requires a weaker set of requirements and can only activate the backdoor without modification of the underlying test samples. This contrasts perturbation-based backdoor attacks that need to attach triggers during test time. To find semantically similar images for a particular target class, the paper uses a score function over the entire set of data that assigns a value between 0 and 1 to any specific input. This score function is supposed to indicate which samples (that come from a non-target class) have similar features to the target class so that they can be used as backdoors. This function is trained in conjunction with the target neural network and is used to select a handful of samples as the backdoor data. Theoretically, the paper shows that as the number of training samples grows, the generalization bounds between the optimal solutions for benign and backdoor empirical risks approach zero. Experimental results over Fashion-MNIST, CIFAR-10, and GTSRB are provided.","### Strengths:
- The proposed method seems interesting. In particular, it is intriguing to see that one can find semantically similar images to the target class and use those samples as the backdoor activation.

- The theoretical analysis seems relevant to the paper.

- The experimental results, especially Figures 3 and 4, beautifully depict the overall idea of the paper.

### Weaknesses:
- A major concern with the current version is the presentation of the paper, especially the proposed method. Many aspects of the proposed method are left to the reader's interpretation, and the paper does not discuss them properly. These include:
   1. In Eq. (1), for the first time the reader faces the objective function for training the scoring mechanism $g_{W}(\boldsymbol{x}_i)$. However, the paper does not properly discuss the objective function's meaning. In particular, what does Eq. (1) imply beyond mathematical terms? Please explain this in accessible language to the readers. Additionally, the symbol $m$ is used in this equation for the first time. However, its definition is left toward the end of Eq. (2) in the next page.

  2. In the definition of $P_{m}(W)$, it takes the reader a while to figure out the equivalency of the minimizer of $P_{m}(W)$ with the constraints in Eq. (2). The paper should mention that  $P_{m}(W)$ is always non-negative, and why is that the case? Discuss how the second term $\sum_{j=1}^n g_W\left(\boldsymbol{x}_j\right)\left(1-g_W\left(\boldsymbol{x}_j\right)\right)$ is always non-negative and remind the reader that you assumed $0 \leq g_W\left(\boldsymbol{x}_j\right) \leq 1$.

  3. A similar pattern can be seen during the theoretical results in Sec. 4.2. While the theory is mathematically presented, it would be better to accompany it with further discussions to let the readers grasp what those theorems mean in simplistic language. Also, could you please elaborate on why Assumption 1 is practical?

- Another critical issue with the current work is its motivations. In Sec. 3.1. it is mentioned that the proposed clean-sample threat model is **weaker** in terms of its requirements. However, the current attack requires being present during training, which makes it easier for the attacker. The previous backdoor poisonings could be used to poison the data only without requiring any control over the training mechanism. As such, the requirements of the current attack are not weaker than existing methods. Besides, there is a great body of literature on *feature collision* attacks that share many characteristics with the current approach, e.g., see [1,2]. However, none of them are mentioned and discussed in the paper.

- Finally, the experimental results need more investigations and baselines. The current method is indeed inherently different from perturbation-based backdoor attacks, but comparing the proposed method against them would help the readers understand the bigger picture. More importantly, a comparison on existing baselines for the detection of backdoor samples could show how much the selected backdoor samples in the paper are successful in circumventing the existing defense mechanisms. Also, investigating the transferability of the backdoor samples between models is another important venue. In other words, what does happen if one generates a poisoned dataset for a model $f_{1}(\cdot)$ but trains a classifier $f_{2}(\cdot)$ with that data? Could the backdoors be transferred between models? Additionally, what are the proposed method's computational burdens compared to vanilla training? Finally, a detailed investigation of the effects of different hyper-parameters on the model's performance over tasks I-III is also missing.

**Minor comment**: The remark mentioned in footnote 1 is inaccurate. One could poison the training data with perturbation-based triggers and maintain the training data size. For instance, see [3].


[1] Shafahi, Ali, et al. ""Poison frogs! targeted clean-label poisoning attacks on neural networks,"" *NeurIPS*, 2018.

[2] Saha, Aniruddha, et al. ""Hidden trigger backdoor attacks,"" *AAAI*, 2020.

[3] Turner, Alexander, et al. ""Label-consistent backdoor attacks,"" *arXiv preprint arXiv:1912.02771* (2019).","Detailed comments are given in the **Strength And Weaknesses**. Below is a summary:

#### **Clarity**: the paper needs some work to explain the proposed approach in an accessible language. For further info, please see the first part of the weaknesses.

#### **Quality**: as mentioned above, the paper requires more straightforward language, better motivation, and further experimentation. These issues place the paper in a moderate quality.

#### **Novelty And Reproducibility**: The paper presents an interesting attack in terms of novelty. However, different aspects of the current work need to be investigated better. Furthermore, in terms of reproducibility, the paper neither provides the code nor a detailed set of hyper-parameters for the results to be re-created. ","Based on my detailed comments above, I feel the paper needs some work. As such, I vote for its rejection at this stage. Meanwhile, I need to read my peers' reviews and wait for the authors' responses to make my final decision.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666598135341,,33daZzvuzY6,ICLR.cc/2023/Conference/Paper4673/-/Official_Review,,[],1,,['everyone'],33daZzvuzY6,['ICLR.cc/2023/Conference/Paper4673/Reviewer_e5gT'],1666598135341,,1666598135341,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4673/Reviewer_e5gT']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions three specific papers ([1], [2], and [3]) that are related to the topic of backdoor attacks and feature collision attacks, and notes that they are not mentioned or discussed in the paper. These papers are:

* Shafahi, Ali, et al. ""Poison frogs! targeted clean-label poisoning attacks on neural networks,"" *NeurIPS*, 2018.
* Saha, Aniruddha, et al. ""Hidden trigger backdoor attacks,"" *AAAI*, 2020.
* Turner, Alexander, et al. ""Label-consistent backdoor attacks,"" *arXiv preprint arXiv:1912.02771* (2019).

The reviewer suggests that the authors should discuss and compare their work with these existing methods to provide a more comprehensive understanding of the topic.",1,"2018, 2020, 2019, 2019",Hidden trigger backdoor attacks
XliUA0ZiA1x,"This paper proposes a new backdoor attack. Under the proposed threat model, the attacker aims to use semantically similar images to the target class to activate the backdoor. For instance, green cars can be used as semantically similar images to frogs in CIFAR-10. As such, one may re-label those green car images into frogs to create a poisoned data set. The paper argues that under this regime, known as clean-sample backdoor attacks, the attacker requires a weaker set of requirements and can only activate the backdoor without modification of the underlying test samples. This contrasts perturbation-based backdoor attacks that need to attach triggers during test time. To find semantically similar images for a particular target class, the paper uses a score function over the entire set of data that assigns a value between 0 and 1 to any specific input. This score function is supposed to indicate which samples (that come from a non-target class) have similar features to the target class so that they can be used as backdoors. This function is trained in conjunction with the target neural network and is used to select a handful of samples as the backdoor data. Theoretically, the paper shows that as the number of training samples grows, the generalization bounds between the optimal solutions for benign and backdoor empirical risks approach zero. Experimental results over Fashion-MNIST, CIFAR-10, and GTSRB are provided.","### Strengths:
- The proposed method seems interesting. In particular, it is intriguing to see that one can find semantically similar images to the target class and use those samples as the backdoor activation.

- The theoretical analysis seems relevant to the paper.

- The experimental results, especially Figures 3 and 4, beautifully depict the overall idea of the paper.

### Weaknesses:
- A major concern with the current version is the presentation of the paper, especially the proposed method. Many aspects of the proposed method are left to the reader's interpretation, and the paper does not discuss them properly. These include:
   1. In Eq. (1), for the first time the reader faces the objective function for training the scoring mechanism $g_{W}(\boldsymbol{x}_i)$. However, the paper does not properly discuss the objective function's meaning. In particular, what does Eq. (1) imply beyond mathematical terms? Please explain this in accessible language to the readers. Additionally, the symbol $m$ is used in this equation for the first time. However, its definition is left toward the end of Eq. (2) in the next page.

  2. In the definition of $P_{m}(W)$, it takes the reader a while to figure out the equivalency of the minimizer of $P_{m}(W)$ with the constraints in Eq. (2). The paper should mention that  $P_{m}(W)$ is always non-negative, and why is that the case? Discuss how the second term $\sum_{j=1}^n g_W\left(\boldsymbol{x}_j\right)\left(1-g_W\left(\boldsymbol{x}_j\right)\right)$ is always non-negative and remind the reader that you assumed $0 \leq g_W\left(\boldsymbol{x}_j\right) \leq 1$.

  3. A similar pattern can be seen during the theoretical results in Sec. 4.2. While the theory is mathematically presented, it would be better to accompany it with further discussions to let the readers grasp what those theorems mean in simplistic language. Also, could you please elaborate on why Assumption 1 is practical?

- Another critical issue with the current work is its motivations. In Sec. 3.1. it is mentioned that the proposed clean-sample threat model is **weaker** in terms of its requirements. However, the current attack requires being present during training, which makes it easier for the attacker. The previous backdoor poisonings could be used to poison the data only without requiring any control over the training mechanism. As such, the requirements of the current attack are not weaker than existing methods. Besides, there is a great body of literature on *feature collision* attacks that share many characteristics with the current approach, e.g., see [1,2]. However, none of them are mentioned and discussed in the paper.

- Finally, the experimental results need more investigations and baselines. The current method is indeed inherently different from perturbation-based backdoor attacks, but comparing the proposed method against them would help the readers understand the bigger picture. More importantly, a comparison on existing baselines for the detection of backdoor samples could show how much the selected backdoor samples in the paper are successful in circumventing the existing defense mechanisms. Also, investigating the transferability of the backdoor samples between models is another important venue. In other words, what does happen if one generates a poisoned dataset for a model $f_{1}(\cdot)$ but trains a classifier $f_{2}(\cdot)$ with that data? Could the backdoors be transferred between models? Additionally, what are the proposed method's computational burdens compared to vanilla training? Finally, a detailed investigation of the effects of different hyper-parameters on the model's performance over tasks I-III is also missing.

**Minor comment**: The remark mentioned in footnote 1 is inaccurate. One could poison the training data with perturbation-based triggers and maintain the training data size. For instance, see [3].


[1] Shafahi, Ali, et al. ""Poison frogs! targeted clean-label poisoning attacks on neural networks,"" *NeurIPS*, 2018.

[2] Saha, Aniruddha, et al. ""Hidden trigger backdoor attacks,"" *AAAI*, 2020.

[3] Turner, Alexander, et al. ""Label-consistent backdoor attacks,"" *arXiv preprint arXiv:1912.02771* (2019).","Detailed comments are given in the **Strength And Weaknesses**. Below is a summary:

#### **Clarity**: the paper needs some work to explain the proposed approach in an accessible language. For further info, please see the first part of the weaknesses.

#### **Quality**: as mentioned above, the paper requires more straightforward language, better motivation, and further experimentation. These issues place the paper in a moderate quality.

#### **Novelty And Reproducibility**: The paper presents an interesting attack in terms of novelty. However, different aspects of the current work need to be investigated better. Furthermore, in terms of reproducibility, the paper neither provides the code nor a detailed set of hyper-parameters for the results to be re-created. ","Based on my detailed comments above, I feel the paper needs some work. As such, I vote for its rejection at this stage. Meanwhile, I need to read my peers' reviews and wait for the authors' responses to make my final decision.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666598135341,,33daZzvuzY6,ICLR.cc/2023/Conference/Paper4673/-/Official_Review,,[],1,,['everyone'],33daZzvuzY6,['ICLR.cc/2023/Conference/Paper4673/Reviewer_e5gT'],1666598135341,,1666598135341,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4673/Reviewer_e5gT']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions three specific papers ([1], [2], and [3]) that are related to the topic of backdoor attacks and feature collision attacks, and notes that they are not mentioned or discussed in the paper. These papers are:

* Shafahi, Ali, et al. ""Poison frogs! targeted clean-label poisoning attacks on neural networks,"" *NeurIPS*, 2018.
* Saha, Aniruddha, et al. ""Hidden trigger backdoor attacks,"" *AAAI*, 2020.
* Turner, Alexander, et al. ""Label-consistent backdoor attacks,"" *arXiv preprint arXiv:1912.02771* (2019).

The reviewer suggests that the authors should discuss and compare their work with these existing methods to provide a more comprehensive understanding of the topic.",1,"2018, 2020, 2019, 2019",Label-consistent backdoor attacks
Xx3aOGDZdq,"The author proposes a new metric for evaluating the multi-mode image synthesis method, and has tested new metrics. However, method details are limited. In addition, the author's contribution point 1,3 is difficult to capture in the introduction. Besides, the papers investigated are old.","+The author proposes a new metric for evaluating multi-mode image synthesis methods
+The author used a large number of experiments to verify

-Recently, there are many meaningful works for multi-mode image synthesis. I don't know why the author has not conducted more detailed research in the Introduction section. Instead, follow the work of 2020 and 2021.

Dalmaz O, Yurt M, Çukur T. ResViT: residual vision transformers for multimodal medical image synthesis[J]. IEEE Transactions on Medical Imaging, 2022, 41(10): 2598-2614.
Isaac-Medina B K S, Bhowmik N, Willcocks C G, et al. Cross-Modal Image Synthesis Within Dual-Energy X-Ray Security Imagery[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 333-341.
Wu G, Chen X, Shi Z, et al. Convolutional neural network with coarse-to-fine resolution fusion and residual learning structures for cross-modality image synthesis[J]. Biomedical Signal Processing and Control, 2022, 71: 103199.
Lv Z, Li X, Niu Z, et al. Semantic-shape Adaptive Feature Modulation for Semantic Image Synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11214-11223.
Tan Z, Chu Q, Chai M, et al. Semantic Probability Distribution Modeling for Diverse Semantic Image Synthesis[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

-In denying that Measure is an important metric, as Section 7 of [1], I hope that the author can further compare the advantages of the metric and have more proof of the metrics.

[1] Zhou R, Jiang C, Xu Q. A survey on generative adversarial network-based text-to-image synthesis[J]. Neurocomputing, 2021, 451: 316-336.

-In the introduce section, the author introduces MIS-Score, but it is difficult capture the first contribution point and the third contribution point by reading this section.

-The introduction of the method is limited, and it is difficult to capture the details of the method. The author's description of the implementation framework is incomplete and it is difficult to understand the proposed framework.
","The introduction of the method is limited, and it is difficult to capture the details of the method.","The research topic is worth recommending, but the author's description of the method needs to be strengthened.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1666927146757,,2xQVAXKjLdH,ICLR.cc/2023/Conference/Paper672/-/Official_Review,,[],3,,['everyone'],2xQVAXKjLdH,['ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm'],1666927146757,,1666927146757,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers from 2022 that are related to multi-mode image synthesis, such as ResViT, Cross-Modal Image Synthesis, Convolutional neural network with coarse-to-fine resolution fusion, Semantic-shape Adaptive Feature Modulation, and Semantic Probability Distribution Modeling. These papers do not appear to be mentioned in the original paper, as the reviewer notes that the papers investigated in the original paper are old (from 2020 and 2021). The reviewer also mentions a survey paper from 2021, but it is possible that this paper is already included in the original paper, although it is not clear from the review.",1,"2022, 2020, 2021, 2021",ResViT 
Xx3aOGDZdq,"The author proposes a new metric for evaluating the multi-mode image synthesis method, and has tested new metrics. However, method details are limited. In addition, the author's contribution point 1,3 is difficult to capture in the introduction. Besides, the papers investigated are old.","+The author proposes a new metric for evaluating multi-mode image synthesis methods
+The author used a large number of experiments to verify

-Recently, there are many meaningful works for multi-mode image synthesis. I don't know why the author has not conducted more detailed research in the Introduction section. Instead, follow the work of 2020 and 2021.

Dalmaz O, Yurt M, Çukur T. ResViT: residual vision transformers for multimodal medical image synthesis[J]. IEEE Transactions on Medical Imaging, 2022, 41(10): 2598-2614.
Isaac-Medina B K S, Bhowmik N, Willcocks C G, et al. Cross-Modal Image Synthesis Within Dual-Energy X-Ray Security Imagery[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 333-341.
Wu G, Chen X, Shi Z, et al. Convolutional neural network with coarse-to-fine resolution fusion and residual learning structures for cross-modality image synthesis[J]. Biomedical Signal Processing and Control, 2022, 71: 103199.
Lv Z, Li X, Niu Z, et al. Semantic-shape Adaptive Feature Modulation for Semantic Image Synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11214-11223.
Tan Z, Chu Q, Chai M, et al. Semantic Probability Distribution Modeling for Diverse Semantic Image Synthesis[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

-In denying that Measure is an important metric, as Section 7 of [1], I hope that the author can further compare the advantages of the metric and have more proof of the metrics.

[1] Zhou R, Jiang C, Xu Q. A survey on generative adversarial network-based text-to-image synthesis[J]. Neurocomputing, 2021, 451: 316-336.

-In the introduce section, the author introduces MIS-Score, but it is difficult capture the first contribution point and the third contribution point by reading this section.

-The introduction of the method is limited, and it is difficult to capture the details of the method. The author's description of the implementation framework is incomplete and it is difficult to understand the proposed framework.
","The introduction of the method is limited, and it is difficult to capture the details of the method.","The research topic is worth recommending, but the author's description of the method needs to be strengthened.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1666927146757,,2xQVAXKjLdH,ICLR.cc/2023/Conference/Paper672/-/Official_Review,,[],3,,['everyone'],2xQVAXKjLdH,['ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm'],1666927146757,,1666927146757,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers from 2022 that are related to multi-mode image synthesis, such as ResViT, Cross-Modal Image Synthesis, Convolutional neural network with coarse-to-fine resolution fusion, Semantic-shape Adaptive Feature Modulation, and Semantic Probability Distribution Modeling. These papers do not appear to be mentioned in the original paper, as the reviewer notes that the papers investigated in the original paper are old (from 2020 and 2021). The reviewer also mentions a survey paper from 2021, but it is possible that this paper is already included in the original paper, although it is not clear from the review.",1,"2022, 2020, 2021, 2021",Cross-Modal Image Synthesis 
Xx3aOGDZdq,"The author proposes a new metric for evaluating the multi-mode image synthesis method, and has tested new metrics. However, method details are limited. In addition, the author's contribution point 1,3 is difficult to capture in the introduction. Besides, the papers investigated are old.","+The author proposes a new metric for evaluating multi-mode image synthesis methods
+The author used a large number of experiments to verify

-Recently, there are many meaningful works for multi-mode image synthesis. I don't know why the author has not conducted more detailed research in the Introduction section. Instead, follow the work of 2020 and 2021.

Dalmaz O, Yurt M, Çukur T. ResViT: residual vision transformers for multimodal medical image synthesis[J]. IEEE Transactions on Medical Imaging, 2022, 41(10): 2598-2614.
Isaac-Medina B K S, Bhowmik N, Willcocks C G, et al. Cross-Modal Image Synthesis Within Dual-Energy X-Ray Security Imagery[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 333-341.
Wu G, Chen X, Shi Z, et al. Convolutional neural network with coarse-to-fine resolution fusion and residual learning structures for cross-modality image synthesis[J]. Biomedical Signal Processing and Control, 2022, 71: 103199.
Lv Z, Li X, Niu Z, et al. Semantic-shape Adaptive Feature Modulation for Semantic Image Synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11214-11223.
Tan Z, Chu Q, Chai M, et al. Semantic Probability Distribution Modeling for Diverse Semantic Image Synthesis[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

-In denying that Measure is an important metric, as Section 7 of [1], I hope that the author can further compare the advantages of the metric and have more proof of the metrics.

[1] Zhou R, Jiang C, Xu Q. A survey on generative adversarial network-based text-to-image synthesis[J]. Neurocomputing, 2021, 451: 316-336.

-In the introduce section, the author introduces MIS-Score, but it is difficult capture the first contribution point and the third contribution point by reading this section.

-The introduction of the method is limited, and it is difficult to capture the details of the method. The author's description of the implementation framework is incomplete and it is difficult to understand the proposed framework.
","The introduction of the method is limited, and it is difficult to capture the details of the method.","The research topic is worth recommending, but the author's description of the method needs to be strengthened.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1666927146757,,2xQVAXKjLdH,ICLR.cc/2023/Conference/Paper672/-/Official_Review,,[],3,,['everyone'],2xQVAXKjLdH,['ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm'],1666927146757,,1666927146757,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers from 2022 that are related to multi-mode image synthesis, such as ResViT, Cross-Modal Image Synthesis, Convolutional neural network with coarse-to-fine resolution fusion, Semantic-shape Adaptive Feature Modulation, and Semantic Probability Distribution Modeling. These papers do not appear to be mentioned in the original paper, as the reviewer notes that the papers investigated in the original paper are old (from 2020 and 2021). The reviewer also mentions a survey paper from 2021, but it is possible that this paper is already included in the original paper, although it is not clear from the review.",1,"2022, 2020, 2021, 2021",Convolutional neural network with coarse-to-fine resolution fusion 
Xx3aOGDZdq,"The author proposes a new metric for evaluating the multi-mode image synthesis method, and has tested new metrics. However, method details are limited. In addition, the author's contribution point 1,3 is difficult to capture in the introduction. Besides, the papers investigated are old.","+The author proposes a new metric for evaluating multi-mode image synthesis methods
+The author used a large number of experiments to verify

-Recently, there are many meaningful works for multi-mode image synthesis. I don't know why the author has not conducted more detailed research in the Introduction section. Instead, follow the work of 2020 and 2021.

Dalmaz O, Yurt M, Çukur T. ResViT: residual vision transformers for multimodal medical image synthesis[J]. IEEE Transactions on Medical Imaging, 2022, 41(10): 2598-2614.
Isaac-Medina B K S, Bhowmik N, Willcocks C G, et al. Cross-Modal Image Synthesis Within Dual-Energy X-Ray Security Imagery[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 333-341.
Wu G, Chen X, Shi Z, et al. Convolutional neural network with coarse-to-fine resolution fusion and residual learning structures for cross-modality image synthesis[J]. Biomedical Signal Processing and Control, 2022, 71: 103199.
Lv Z, Li X, Niu Z, et al. Semantic-shape Adaptive Feature Modulation for Semantic Image Synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11214-11223.
Tan Z, Chu Q, Chai M, et al. Semantic Probability Distribution Modeling for Diverse Semantic Image Synthesis[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

-In denying that Measure is an important metric, as Section 7 of [1], I hope that the author can further compare the advantages of the metric and have more proof of the metrics.

[1] Zhou R, Jiang C, Xu Q. A survey on generative adversarial network-based text-to-image synthesis[J]. Neurocomputing, 2021, 451: 316-336.

-In the introduce section, the author introduces MIS-Score, but it is difficult capture the first contribution point and the third contribution point by reading this section.

-The introduction of the method is limited, and it is difficult to capture the details of the method. The author's description of the implementation framework is incomplete and it is difficult to understand the proposed framework.
","The introduction of the method is limited, and it is difficult to capture the details of the method.","The research topic is worth recommending, but the author's description of the method needs to be strengthened.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1666927146757,,2xQVAXKjLdH,ICLR.cc/2023/Conference/Paper672/-/Official_Review,,[],3,,['everyone'],2xQVAXKjLdH,['ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm'],1666927146757,,1666927146757,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers from 2022 that are related to multi-mode image synthesis, such as ResViT, Cross-Modal Image Synthesis, Convolutional neural network with coarse-to-fine resolution fusion, Semantic-shape Adaptive Feature Modulation, and Semantic Probability Distribution Modeling. These papers do not appear to be mentioned in the original paper, as the reviewer notes that the papers investigated in the original paper are old (from 2020 and 2021). The reviewer also mentions a survey paper from 2021, but it is possible that this paper is already included in the original paper, although it is not clear from the review.",1,"2022, 2020, 2021, 2021",Semantic-shape Adaptive Feature Modulation 
Xx3aOGDZdq,"The author proposes a new metric for evaluating the multi-mode image synthesis method, and has tested new metrics. However, method details are limited. In addition, the author's contribution point 1,3 is difficult to capture in the introduction. Besides, the papers investigated are old.","+The author proposes a new metric for evaluating multi-mode image synthesis methods
+The author used a large number of experiments to verify

-Recently, there are many meaningful works for multi-mode image synthesis. I don't know why the author has not conducted more detailed research in the Introduction section. Instead, follow the work of 2020 and 2021.

Dalmaz O, Yurt M, Çukur T. ResViT: residual vision transformers for multimodal medical image synthesis[J]. IEEE Transactions on Medical Imaging, 2022, 41(10): 2598-2614.
Isaac-Medina B K S, Bhowmik N, Willcocks C G, et al. Cross-Modal Image Synthesis Within Dual-Energy X-Ray Security Imagery[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 333-341.
Wu G, Chen X, Shi Z, et al. Convolutional neural network with coarse-to-fine resolution fusion and residual learning structures for cross-modality image synthesis[J]. Biomedical Signal Processing and Control, 2022, 71: 103199.
Lv Z, Li X, Niu Z, et al. Semantic-shape Adaptive Feature Modulation for Semantic Image Synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11214-11223.
Tan Z, Chu Q, Chai M, et al. Semantic Probability Distribution Modeling for Diverse Semantic Image Synthesis[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

-In denying that Measure is an important metric, as Section 7 of [1], I hope that the author can further compare the advantages of the metric and have more proof of the metrics.

[1] Zhou R, Jiang C, Xu Q. A survey on generative adversarial network-based text-to-image synthesis[J]. Neurocomputing, 2021, 451: 316-336.

-In the introduce section, the author introduces MIS-Score, but it is difficult capture the first contribution point and the third contribution point by reading this section.

-The introduction of the method is limited, and it is difficult to capture the details of the method. The author's description of the implementation framework is incomplete and it is difficult to understand the proposed framework.
","The introduction of the method is limited, and it is difficult to capture the details of the method.","The research topic is worth recommending, but the author's description of the method needs to be strengthened.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1666927146757,,2xQVAXKjLdH,ICLR.cc/2023/Conference/Paper672/-/Official_Review,,[],3,,['everyone'],2xQVAXKjLdH,['ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm'],1666927146757,,1666927146757,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper672/Reviewer_nMLm']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers from 2022 that are related to multi-mode image synthesis, such as ResViT, Cross-Modal Image Synthesis, Convolutional neural network with coarse-to-fine resolution fusion, Semantic-shape Adaptive Feature Modulation, and Semantic Probability Distribution Modeling. These papers do not appear to be mentioned in the original paper, as the reviewer notes that the papers investigated in the original paper are old (from 2020 and 2021). The reviewer also mentions a survey paper from 2021, but it is possible that this paper is already included in the original paper, although it is not clear from the review.",1,"2022, 2020, 2021, 2021",Semantic Probability Distribution Modeling
tAJjlaTidUd,"The paper considers the problem of causal representation learning from observational data without any form of supervision. The authors assume that the latent causal structure follows a nonlinear additive noise model (ANM) and claim that it is then identifiable. In practice, they use a VAE with a causally structured prior. One novelty in this is that they propose to approximate the KL term in the ELBO by linearizing the causal mechanisms. Their approach is demonstrated on toy image data as well as time series data from a video game.","Strengths:
- The problem of causal representation learning is interesting and potentially very impactful.
- The data regime that the authors tackle – unsupervised observational data – is the holy grail of causal representation learning: a strong identifiability result here could be immediately practically relevant.
- The VAE ansatz with a fully connected graph is very sensible.
- The paper is generally well written. I appreciate the thorough introductions to the background material.

Weaknesses:
- Unfortunately, I believe that the main premise of the paper is flawed. (I hope I am wrong and the authors can correct me!) The authors correctly point out that when the causal variables are given, nonlinear ANMs are identifiable. They then claim that this implies that *causal representations with a latent nonlinear ANM are identifiable*, i.e. one can identify both the map from low-level data to causal variables and the causal graph from data. As far as I understand it, they do this without making assumptions on the decoder (like that being linear). I am afraid this claim of identifiability is not correct.
  - One known counterexample is the special case of the trivial graph. Locatello et al [""Challenging common assumptions in the unsupervised learning of disentangled representations"", ICML 2019] and some other papers show that in this case the variables are not identifiable.
  - Another counterexample can be constructed from any nonlinear ANM with causal variables $z_i$ and noise variables $\epsilon_i$. We can define a second nonlinear ANM that has a trivial graph and causal variables $z'_i = \epsilon_i$ such that the observational data distribution remains the same. (Of course, the map to the data space will then be different between the two causal models.)
  - If I'm missing something here, I kindly ask the authors to add an identifiability argument (for the representations, not just the causal structure once the representations are known) to the paper.
- I don't quite understand the reasoning behind the linearizing approximation. Why not just use a nonlinear density estimator like a normalizing flow for each conditional probability density in the prior? Then we can approximate the KL quite well by sampling. Unlike the linearization, this approach should be unbiased (in some limit) and is well established.
- I find it difficult to draw conclusions from the experiments. It would be better if the results were analyzed more quantitatively, showing that the learned representations and graphs are correct.
- In the related work section, several recent works that use VAEs with causal structure in the latent space are missed, for instance:
  - von Kugelgen et al, ""Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style"", NeurIPS 2021
  - Brehmer et al, ""Weakly supervised causal representation learning""
  - Ahuja et al, ""Interventional Causal Representation Learning""
  - as a review, it may also be useful to point to Schölkopf et al, ""Towards Causal Representation Learning"", IEEE","Quality: In my humble opinion, the paper suffers from a fatal flaw – I think causal representations cannot identifiable from unlabelled observational data alone, at least not without additional assumptions. If this is correct, then unfortunately I believe that there is no reason for the proposed method to work.

Novelty: There are already several papers that propose causal representation learning with a VAE with a causally structured prior. The key novelty of the paper is the claimed identifiability from unsupervised observational data. If that result is correct and well supported, then that is certainly sufficient novelty (and potentially very impactful).

Clarity: The paper is clearly written.
","If the author's claim is true, this would be a great result. Unfortunately, I do not think that the main point – that causal representations can be identified from observational data if we just assume the latent causal structure to be a nonlinear ANM – is correct. I hope the authors can point me to what I'm missing, add an identifiability argument, or provide strong empirical evidence for this claim. Otherwise, I do not think that this paper can be accepted.

UPDATE after rebuttal and discussion: I am grateful to the authors for constructively and quickly engaging with my questions and criticisms and even working out a new identifiability claim. Unfortunately, I am convinced that this claim is incorrect, but even if it were true, it would not solve the main issue with the paper. I am afraid that this paper will need more substantial work before acceptance, and recommend rejection at ICLR.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,1: strong reject,1666604636121,,2xNKMFGPJU5,ICLR.cc/2023/Conference/Paper237/-/Official_Review,,[],3,,['everyone'],2xNKMFGPJU5,['ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c'],1666604636121,,1668770160462,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not included in the related work section of the paper, such as:

* von Kugelgen et al, ""Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style"", NeurIPS 2021
* Brehmer et al, ""Weakly supervised causal representation learning""
* Ahuja et al, ""Interventional Causal Representation Learning""
* Schölkopf et al, ""Towards Causal Representation Learning"", IEEE

The reviewer suggests that including these references could improve the paper's related work section.",1,2021,Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style
tAJjlaTidUd,"The paper considers the problem of causal representation learning from observational data without any form of supervision. The authors assume that the latent causal structure follows a nonlinear additive noise model (ANM) and claim that it is then identifiable. In practice, they use a VAE with a causally structured prior. One novelty in this is that they propose to approximate the KL term in the ELBO by linearizing the causal mechanisms. Their approach is demonstrated on toy image data as well as time series data from a video game.","Strengths:
- The problem of causal representation learning is interesting and potentially very impactful.
- The data regime that the authors tackle – unsupervised observational data – is the holy grail of causal representation learning: a strong identifiability result here could be immediately practically relevant.
- The VAE ansatz with a fully connected graph is very sensible.
- The paper is generally well written. I appreciate the thorough introductions to the background material.

Weaknesses:
- Unfortunately, I believe that the main premise of the paper is flawed. (I hope I am wrong and the authors can correct me!) The authors correctly point out that when the causal variables are given, nonlinear ANMs are identifiable. They then claim that this implies that *causal representations with a latent nonlinear ANM are identifiable*, i.e. one can identify both the map from low-level data to causal variables and the causal graph from data. As far as I understand it, they do this without making assumptions on the decoder (like that being linear). I am afraid this claim of identifiability is not correct.
  - One known counterexample is the special case of the trivial graph. Locatello et al [""Challenging common assumptions in the unsupervised learning of disentangled representations"", ICML 2019] and some other papers show that in this case the variables are not identifiable.
  - Another counterexample can be constructed from any nonlinear ANM with causal variables $z_i$ and noise variables $\epsilon_i$. We can define a second nonlinear ANM that has a trivial graph and causal variables $z'_i = \epsilon_i$ such that the observational data distribution remains the same. (Of course, the map to the data space will then be different between the two causal models.)
  - If I'm missing something here, I kindly ask the authors to add an identifiability argument (for the representations, not just the causal structure once the representations are known) to the paper.
- I don't quite understand the reasoning behind the linearizing approximation. Why not just use a nonlinear density estimator like a normalizing flow for each conditional probability density in the prior? Then we can approximate the KL quite well by sampling. Unlike the linearization, this approach should be unbiased (in some limit) and is well established.
- I find it difficult to draw conclusions from the experiments. It would be better if the results were analyzed more quantitatively, showing that the learned representations and graphs are correct.
- In the related work section, several recent works that use VAEs with causal structure in the latent space are missed, for instance:
  - von Kugelgen et al, ""Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style"", NeurIPS 2021
  - Brehmer et al, ""Weakly supervised causal representation learning""
  - Ahuja et al, ""Interventional Causal Representation Learning""
  - as a review, it may also be useful to point to Schölkopf et al, ""Towards Causal Representation Learning"", IEEE","Quality: In my humble opinion, the paper suffers from a fatal flaw – I think causal representations cannot identifiable from unlabelled observational data alone, at least not without additional assumptions. If this is correct, then unfortunately I believe that there is no reason for the proposed method to work.

Novelty: There are already several papers that propose causal representation learning with a VAE with a causally structured prior. The key novelty of the paper is the claimed identifiability from unsupervised observational data. If that result is correct and well supported, then that is certainly sufficient novelty (and potentially very impactful).

Clarity: The paper is clearly written.
","If the author's claim is true, this would be a great result. Unfortunately, I do not think that the main point – that causal representations can be identified from observational data if we just assume the latent causal structure to be a nonlinear ANM – is correct. I hope the authors can point me to what I'm missing, add an identifiability argument, or provide strong empirical evidence for this claim. Otherwise, I do not think that this paper can be accepted.

UPDATE after rebuttal and discussion: I am grateful to the authors for constructively and quickly engaging with my questions and criticisms and even working out a new identifiability claim. Unfortunately, I am convinced that this claim is incorrect, but even if it were true, it would not solve the main issue with the paper. I am afraid that this paper will need more substantial work before acceptance, and recommend rejection at ICLR.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,1: strong reject,1666604636121,,2xNKMFGPJU5,ICLR.cc/2023/Conference/Paper237/-/Official_Review,,[],3,,['everyone'],2xNKMFGPJU5,['ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c'],1666604636121,,1668770160462,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not included in the related work section of the paper, such as:

* von Kugelgen et al, ""Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style"", NeurIPS 2021
* Brehmer et al, ""Weakly supervised causal representation learning""
* Ahuja et al, ""Interventional Causal Representation Learning""
* Schölkopf et al, ""Towards Causal Representation Learning"", IEEE

The reviewer suggests that including these references could improve the paper's related work section.",1,2021,Weakly supervised causal representation learning
tAJjlaTidUd,"The paper considers the problem of causal representation learning from observational data without any form of supervision. The authors assume that the latent causal structure follows a nonlinear additive noise model (ANM) and claim that it is then identifiable. In practice, they use a VAE with a causally structured prior. One novelty in this is that they propose to approximate the KL term in the ELBO by linearizing the causal mechanisms. Their approach is demonstrated on toy image data as well as time series data from a video game.","Strengths:
- The problem of causal representation learning is interesting and potentially very impactful.
- The data regime that the authors tackle – unsupervised observational data – is the holy grail of causal representation learning: a strong identifiability result here could be immediately practically relevant.
- The VAE ansatz with a fully connected graph is very sensible.
- The paper is generally well written. I appreciate the thorough introductions to the background material.

Weaknesses:
- Unfortunately, I believe that the main premise of the paper is flawed. (I hope I am wrong and the authors can correct me!) The authors correctly point out that when the causal variables are given, nonlinear ANMs are identifiable. They then claim that this implies that *causal representations with a latent nonlinear ANM are identifiable*, i.e. one can identify both the map from low-level data to causal variables and the causal graph from data. As far as I understand it, they do this without making assumptions on the decoder (like that being linear). I am afraid this claim of identifiability is not correct.
  - One known counterexample is the special case of the trivial graph. Locatello et al [""Challenging common assumptions in the unsupervised learning of disentangled representations"", ICML 2019] and some other papers show that in this case the variables are not identifiable.
  - Another counterexample can be constructed from any nonlinear ANM with causal variables $z_i$ and noise variables $\epsilon_i$. We can define a second nonlinear ANM that has a trivial graph and causal variables $z'_i = \epsilon_i$ such that the observational data distribution remains the same. (Of course, the map to the data space will then be different between the two causal models.)
  - If I'm missing something here, I kindly ask the authors to add an identifiability argument (for the representations, not just the causal structure once the representations are known) to the paper.
- I don't quite understand the reasoning behind the linearizing approximation. Why not just use a nonlinear density estimator like a normalizing flow for each conditional probability density in the prior? Then we can approximate the KL quite well by sampling. Unlike the linearization, this approach should be unbiased (in some limit) and is well established.
- I find it difficult to draw conclusions from the experiments. It would be better if the results were analyzed more quantitatively, showing that the learned representations and graphs are correct.
- In the related work section, several recent works that use VAEs with causal structure in the latent space are missed, for instance:
  - von Kugelgen et al, ""Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style"", NeurIPS 2021
  - Brehmer et al, ""Weakly supervised causal representation learning""
  - Ahuja et al, ""Interventional Causal Representation Learning""
  - as a review, it may also be useful to point to Schölkopf et al, ""Towards Causal Representation Learning"", IEEE","Quality: In my humble opinion, the paper suffers from a fatal flaw – I think causal representations cannot identifiable from unlabelled observational data alone, at least not without additional assumptions. If this is correct, then unfortunately I believe that there is no reason for the proposed method to work.

Novelty: There are already several papers that propose causal representation learning with a VAE with a causally structured prior. The key novelty of the paper is the claimed identifiability from unsupervised observational data. If that result is correct and well supported, then that is certainly sufficient novelty (and potentially very impactful).

Clarity: The paper is clearly written.
","If the author's claim is true, this would be a great result. Unfortunately, I do not think that the main point – that causal representations can be identified from observational data if we just assume the latent causal structure to be a nonlinear ANM – is correct. I hope the authors can point me to what I'm missing, add an identifiability argument, or provide strong empirical evidence for this claim. Otherwise, I do not think that this paper can be accepted.

UPDATE after rebuttal and discussion: I am grateful to the authors for constructively and quickly engaging with my questions and criticisms and even working out a new identifiability claim. Unfortunately, I am convinced that this claim is incorrect, but even if it were true, it would not solve the main issue with the paper. I am afraid that this paper will need more substantial work before acceptance, and recommend rejection at ICLR.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,1: strong reject,1666604636121,,2xNKMFGPJU5,ICLR.cc/2023/Conference/Paper237/-/Official_Review,,[],3,,['everyone'],2xNKMFGPJU5,['ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c'],1666604636121,,1668770160462,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not included in the related work section of the paper, such as:

* von Kugelgen et al, ""Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style"", NeurIPS 2021
* Brehmer et al, ""Weakly supervised causal representation learning""
* Ahuja et al, ""Interventional Causal Representation Learning""
* Schölkopf et al, ""Towards Causal Representation Learning"", IEEE

The reviewer suggests that including these references could improve the paper's related work section.",1,2021,Interventional Causal Representation Learning
tAJjlaTidUd,"The paper considers the problem of causal representation learning from observational data without any form of supervision. The authors assume that the latent causal structure follows a nonlinear additive noise model (ANM) and claim that it is then identifiable. In practice, they use a VAE with a causally structured prior. One novelty in this is that they propose to approximate the KL term in the ELBO by linearizing the causal mechanisms. Their approach is demonstrated on toy image data as well as time series data from a video game.","Strengths:
- The problem of causal representation learning is interesting and potentially very impactful.
- The data regime that the authors tackle – unsupervised observational data – is the holy grail of causal representation learning: a strong identifiability result here could be immediately practically relevant.
- The VAE ansatz with a fully connected graph is very sensible.
- The paper is generally well written. I appreciate the thorough introductions to the background material.

Weaknesses:
- Unfortunately, I believe that the main premise of the paper is flawed. (I hope I am wrong and the authors can correct me!) The authors correctly point out that when the causal variables are given, nonlinear ANMs are identifiable. They then claim that this implies that *causal representations with a latent nonlinear ANM are identifiable*, i.e. one can identify both the map from low-level data to causal variables and the causal graph from data. As far as I understand it, they do this without making assumptions on the decoder (like that being linear). I am afraid this claim of identifiability is not correct.
  - One known counterexample is the special case of the trivial graph. Locatello et al [""Challenging common assumptions in the unsupervised learning of disentangled representations"", ICML 2019] and some other papers show that in this case the variables are not identifiable.
  - Another counterexample can be constructed from any nonlinear ANM with causal variables $z_i$ and noise variables $\epsilon_i$. We can define a second nonlinear ANM that has a trivial graph and causal variables $z'_i = \epsilon_i$ such that the observational data distribution remains the same. (Of course, the map to the data space will then be different between the two causal models.)
  - If I'm missing something here, I kindly ask the authors to add an identifiability argument (for the representations, not just the causal structure once the representations are known) to the paper.
- I don't quite understand the reasoning behind the linearizing approximation. Why not just use a nonlinear density estimator like a normalizing flow for each conditional probability density in the prior? Then we can approximate the KL quite well by sampling. Unlike the linearization, this approach should be unbiased (in some limit) and is well established.
- I find it difficult to draw conclusions from the experiments. It would be better if the results were analyzed more quantitatively, showing that the learned representations and graphs are correct.
- In the related work section, several recent works that use VAEs with causal structure in the latent space are missed, for instance:
  - von Kugelgen et al, ""Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style"", NeurIPS 2021
  - Brehmer et al, ""Weakly supervised causal representation learning""
  - Ahuja et al, ""Interventional Causal Representation Learning""
  - as a review, it may also be useful to point to Schölkopf et al, ""Towards Causal Representation Learning"", IEEE","Quality: In my humble opinion, the paper suffers from a fatal flaw – I think causal representations cannot identifiable from unlabelled observational data alone, at least not without additional assumptions. If this is correct, then unfortunately I believe that there is no reason for the proposed method to work.

Novelty: There are already several papers that propose causal representation learning with a VAE with a causally structured prior. The key novelty of the paper is the claimed identifiability from unsupervised observational data. If that result is correct and well supported, then that is certainly sufficient novelty (and potentially very impactful).

Clarity: The paper is clearly written.
","If the author's claim is true, this would be a great result. Unfortunately, I do not think that the main point – that causal representations can be identified from observational data if we just assume the latent causal structure to be a nonlinear ANM – is correct. I hope the authors can point me to what I'm missing, add an identifiability argument, or provide strong empirical evidence for this claim. Otherwise, I do not think that this paper can be accepted.

UPDATE after rebuttal and discussion: I am grateful to the authors for constructively and quickly engaging with my questions and criticisms and even working out a new identifiability claim. Unfortunately, I am convinced that this claim is incorrect, but even if it were true, it would not solve the main issue with the paper. I am afraid that this paper will need more substantial work before acceptance, and recommend rejection at ICLR.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,1: strong reject,1666604636121,,2xNKMFGPJU5,ICLR.cc/2023/Conference/Paper237/-/Official_Review,,[],3,,['everyone'],2xNKMFGPJU5,['ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c'],1666604636121,,1668770160462,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not included in the related work section of the paper, such as:

* von Kugelgen et al, ""Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style"", NeurIPS 2021
* Brehmer et al, ""Weakly supervised causal representation learning""
* Ahuja et al, ""Interventional Causal Representation Learning""
* Schölkopf et al, ""Towards Causal Representation Learning"", IEEE

The reviewer suggests that including these references could improve the paper's related work section.",1,2021,Towards Causal Representation Learning
xteU9GCARjj,"This paper proposes to improve text-supervised segmentation model i.e., GroupVIT by enforcing multi-view semantic consistency. Specifically, a text-to-views consistency module is included to encourage different views to be aligned with the same text input. In addition, there is a cross-view consistency module which enforces features from the different views of the same input image to be closer. The authors conduct extensive experiments on the semantic segmentation benchmarks and show SOTA results.","Strength

-The idea of multi-view consistency is new in visual-language learning. This paper proposes a new method to implement the idea. 

-This paper outperforms GroupVIT consistently on multiple benchmarks.  

Weakness

-A potential related work is missing. The DenseCL [A] paper studies multi-view consistency for self-supervised contrastive learning. The author should discuss the difference.

[A] Dense Contrastive Learning for Self-Supervised Visual Pre-Training. Want et al., CVPR'21","Quality: The paper has a good quality and is evaluated extensively.

Clarity: The paper is written well and easy to understand.

Originality: The proposed method is new.
","Overall, I think this is a good paper with sufficient novelty and good results. I would recommend to accept it.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,4: All of the claims and statements are well-supported and correct.,"4: The contributions are significant, and do not exist in prior works.","4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666702126299,,2XLRBjY46O6,ICLR.cc/2023/Conference/Paper2694/-/Official_Review,,[],1,,['everyone'],2XLRBjY46O6,['ICLR.cc/2023/Conference/Paper2694/Reviewer_kMrx'],1666702126299,,1666702126299,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2694/Reviewer_kMrx']","Yes, this peer review suggests the authors to refer to a specific literature, ""Dense Contrastive Learning for Self-Supervised Visual Pre-Training"" by Want et al., CVPR'21, which is not mentioned as being discussed in the original paper. The reviewer mentions it as a ""potential related work"" that is missing and suggests the authors should discuss the difference between their work and this related work.",1,2021,Dense Contrastive Learning for Self-Supervised Visual Pre-Training
8Ov6dQDBIp,"This paper focuses on text-supervised semantic segmentation by studying a multi-view consistency learning framework. Specifically, a text-to-views consistency and a cross-view segmentation consistency training strategy are proposed and added upon existing work Group VIT. The proposed method tries to construct more training constrains based on correspondence among multiple augmented views to obtain more improvements. Experiments are conducted on multiple datasets for zero-shot segmentation, e.g. PASCAL and COCO., showing improvements upon some existing baselines. 
","
Strength:

+ The motivation of introducing multi-view consistency constrain to text-supervised semantic segmentation is reasonable.

+ Experimental reuslts on multiple datasets show some benifits compared with existing methods. 


Concerns: 

- The general idea of introducing multi-view consistency constrains to vision-language pretraining models is not new. Some existing works, e.g. DeCLIP [a] have explored adding such multi-view self-supervision constrains and validated its effectiveness when learning vision-language models. The differences here are mainly about applying multi-view constrains on segment level rather than whole image when building upon the framework of Group VIT.

[a] Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm, ICLR 2022

-  Cross-view constrains are applied on segments of two augmented views. Since there is no guarantee for location (segments) correspondence between these two augmented views, the proposed strategy in Figure 5 of constructing positive and negative pairs for the corss-view segments seems not reasonable. 

- The performance of the most important baseline GroupViT seems not consistent with the original paper. For example, in Table 1 Comparison with zero-shot baselines on PASCAL VOC 2012, it reports 51.2 mIoU for GroupViT and 52.4 mIoU for the proposed method ViewCo, while in the original paper [b] GroupViT obtains 52.3 mIoU, which is much higher than 51.2 reported in this submission and is only 0.1 lower than the proposed method. Besdies, on PASCAL Context, GroupViT reports 22.4 mIoU, the improvements from ViewCo seems also marginal (23.0 mIoU v.s. 22.4 mIoU). 

[b] GroupViT: Semantic Segmentation Emerges from Text Supervision, CVPR 2022",The paper is easy to follow. The novelty is limited due to the concerns raised in the Strength And Weaknesses section. The code is not provided for reproducibility.,"My current rating is based on concerns for limited technical contributions, unreasonable positive/negative pair selection for cross views, quite limited improvments upon baseline GroupViT. Please find more details in the Strength And Weaknesses section.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667983330983,,2XLRBjY46O6,ICLR.cc/2023/Conference/Paper2694/-/Official_Review,16679833309830.0,[],4,,['everyone'],2XLRBjY46O6,['ICLR.cc/2023/Conference/Paper2694/Reviewer_Dedj'],1667983330983,,1667983330983,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2694/Reviewer_Dedj']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. ""Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm, ICLR 2022"" (referred to as [a]) which introduces DeCLIP, a work that has explored adding multi-view self-supervision constraints to vision-language pretraining models.

2. ""GroupViT: Semantic Segmentation Emerges from Text Supervision, CVPR 2022"" (referred to as [b]) which is the original paper of GroupViT, a baseline method used in the paper being reviewed.

The reviewer uses these references to highlight the limitations of the paper's novelty and to compare the performance of the proposed method with existing methods.",1,"2022, 2022",Supervision exists everywhere A data efficient contrastive language-image pre-training paradigm 
8Ov6dQDBIp,"This paper focuses on text-supervised semantic segmentation by studying a multi-view consistency learning framework. Specifically, a text-to-views consistency and a cross-view segmentation consistency training strategy are proposed and added upon existing work Group VIT. The proposed method tries to construct more training constrains based on correspondence among multiple augmented views to obtain more improvements. Experiments are conducted on multiple datasets for zero-shot segmentation, e.g. PASCAL and COCO., showing improvements upon some existing baselines. 
","
Strength:

+ The motivation of introducing multi-view consistency constrain to text-supervised semantic segmentation is reasonable.

+ Experimental reuslts on multiple datasets show some benifits compared with existing methods. 


Concerns: 

- The general idea of introducing multi-view consistency constrains to vision-language pretraining models is not new. Some existing works, e.g. DeCLIP [a] have explored adding such multi-view self-supervision constrains and validated its effectiveness when learning vision-language models. The differences here are mainly about applying multi-view constrains on segment level rather than whole image when building upon the framework of Group VIT.

[a] Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm, ICLR 2022

-  Cross-view constrains are applied on segments of two augmented views. Since there is no guarantee for location (segments) correspondence between these two augmented views, the proposed strategy in Figure 5 of constructing positive and negative pairs for the corss-view segments seems not reasonable. 

- The performance of the most important baseline GroupViT seems not consistent with the original paper. For example, in Table 1 Comparison with zero-shot baselines on PASCAL VOC 2012, it reports 51.2 mIoU for GroupViT and 52.4 mIoU for the proposed method ViewCo, while in the original paper [b] GroupViT obtains 52.3 mIoU, which is much higher than 51.2 reported in this submission and is only 0.1 lower than the proposed method. Besdies, on PASCAL Context, GroupViT reports 22.4 mIoU, the improvements from ViewCo seems also marginal (23.0 mIoU v.s. 22.4 mIoU). 

[b] GroupViT: Semantic Segmentation Emerges from Text Supervision, CVPR 2022",The paper is easy to follow. The novelty is limited due to the concerns raised in the Strength And Weaknesses section. The code is not provided for reproducibility.,"My current rating is based on concerns for limited technical contributions, unreasonable positive/negative pair selection for cross views, quite limited improvments upon baseline GroupViT. Please find more details in the Strength And Weaknesses section.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667983330983,,2XLRBjY46O6,ICLR.cc/2023/Conference/Paper2694/-/Official_Review,16679833309830.0,[],4,,['everyone'],2XLRBjY46O6,['ICLR.cc/2023/Conference/Paper2694/Reviewer_Dedj'],1667983330983,,1667983330983,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2694/Reviewer_Dedj']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. ""Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm, ICLR 2022"" (referred to as [a]) which introduces DeCLIP, a work that has explored adding multi-view self-supervision constraints to vision-language pretraining models.

2. ""GroupViT: Semantic Segmentation Emerges from Text Supervision, CVPR 2022"" (referred to as [b]) which is the original paper of GroupViT, a baseline method used in the paper being reviewed.

The reviewer uses these references to highlight the limitations of the paper's novelty and to compare the performance of the proposed method with existing methods.",1,"2022, 2022",GroupViT Semantic Segmentation Emerges from Text Supervision
J7L7Qo10bF,"The paper compares two forms of meta-learning, with inner-loop updates based either on modulated Hebbian plasticity, or on gradient descent over a network-generated synthetic loss function. 

Various experiments show that both forms of episodic plasticity (Hebbian and gradient-based) improve performance over using fixed recurrent networks, as in RL^2/L2RL. Network-controlled modulation of plasticity is also shown to be important.

","Strengths:

- The paper tackles an interesting problem (learning to learn without necessarily having access to a supervised or even a reward  signal during each episode, i.e. the inner loop is unsupervised).

- The paper is interesting and may propose one novel method, i.e. network-generated gradient loss, though see below.

- The experiments, such as they are, offer reasonable evidence that the proposed methods are useful over the studied tasks


Weaknesses:

My main concern is about novelty. AFAICT the Hebbian portion of the method is identical to previous proposals that are cited in the paper. 

Similarly, the gradient-based  method seems to be an example of meta-learning a loss function to perform gradient descent on in the inner loop. This approach is clearly not new, see e.g. https://arxiv.org/abs/1802.04821 , https://arxiv.org/abs/1906.05374 , https://arxiv.org/abs/2110.03909 and  the many references therein.  (Disclaimer: I am not affiliated with any of these authors)

Surprisingly, no previous paper on this approach (meta-learning a loss function) seems to be cited in the paper, unless I missed it.

There *may* be some novelty in the proposed approach, because here the synthetic loss  is an output of the learner network itself (as opposed to a separate dedicated network). IIUC this means that the synthetic loss itself is subject to learning and tuning during the inner loop (as  in Baik et al. mentioned above, but in a much more flexible manner). This also gives rise to interesting loops in the learning process (though the precise working of the algorithm is a bit unclear, see below).

If this is correct, and if this is the claim to novelty of the paper, it should be stated explicitly.

Additional possible  problems:
- The paper is somewhat unclear on certain points, which is important to better understand the proposed method. See below. 
- The experiments are interesting, but perhaps a bit limited?
- Not really concerns, but suggestions for future work: the two approaches (Hebbian and gradient-based) seem fully compatible and it might be interesting to see what  happens when a network uses both together; the weight decay parameter, which is now just 1-eta, could include a separate learnable parameter as is Tyulmankov et al. and Rodriguez, Guo && Moraitis ICML 2022 (I am not affiliated with these authors).","Two main points require clarification:

- The description of the network operation is split over several section and somewhat  confusing. Most importantly, exactly which parameters are updated during the inner  vs. outer loop? This should be  specified explicitly.

- The outer loop meta-optimization process should be described more explicitly, especially for the gradient-based case. E.g. do we compute full second-order gradient-of-gradients? Or is there some first-order approximation (like first-order MAML, or REPTL) ? 

For novelty, see  above.
","== Update after reading the authors' response ==

I thank the authors for their clarifications and I have updated my review and score towards acceptance.

== Original review ==

Please cite more existing literature about the concept of meta-learned loss functions; explicitly state the novelty of the work; and clarify the working of the algorithm.

I am willing to increase my score depending on the author's response to the above, especially regarding novelty.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1665875722119,,2WklawyeI08,ICLR.cc/2023/Conference/Paper2942/-/Official_Review,,[],1,,['everyone'],2WklawyeI08,['ICLR.cc/2023/Conference/Paper2942/Reviewer_vDcd'],1665875722119,,1668803052010,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2942/Reviewer_vDcd']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several arXiv papers (https://arxiv.org/abs/1802.04821, https://arxiv.org/abs/1906.05374, https://arxiv.org/abs/2110.03909) and other references that are not cited in the paper, and states that ""Surprisingly, no previous paper on this approach (meta-learning a loss function) seems to be cited in the paper, unless I missed it."" This implies that the reviewer believes the paper is missing relevant citations to existing literature on meta-learned loss functions.",1,"2018, 2019, 2021",Evolved Policy Gradients
J7L7Qo10bF,"The paper compares two forms of meta-learning, with inner-loop updates based either on modulated Hebbian plasticity, or on gradient descent over a network-generated synthetic loss function. 

Various experiments show that both forms of episodic plasticity (Hebbian and gradient-based) improve performance over using fixed recurrent networks, as in RL^2/L2RL. Network-controlled modulation of plasticity is also shown to be important.

","Strengths:

- The paper tackles an interesting problem (learning to learn without necessarily having access to a supervised or even a reward  signal during each episode, i.e. the inner loop is unsupervised).

- The paper is interesting and may propose one novel method, i.e. network-generated gradient loss, though see below.

- The experiments, such as they are, offer reasonable evidence that the proposed methods are useful over the studied tasks


Weaknesses:

My main concern is about novelty. AFAICT the Hebbian portion of the method is identical to previous proposals that are cited in the paper. 

Similarly, the gradient-based  method seems to be an example of meta-learning a loss function to perform gradient descent on in the inner loop. This approach is clearly not new, see e.g. https://arxiv.org/abs/1802.04821 , https://arxiv.org/abs/1906.05374 , https://arxiv.org/abs/2110.03909 and  the many references therein.  (Disclaimer: I am not affiliated with any of these authors)

Surprisingly, no previous paper on this approach (meta-learning a loss function) seems to be cited in the paper, unless I missed it.

There *may* be some novelty in the proposed approach, because here the synthetic loss  is an output of the learner network itself (as opposed to a separate dedicated network). IIUC this means that the synthetic loss itself is subject to learning and tuning during the inner loop (as  in Baik et al. mentioned above, but in a much more flexible manner). This also gives rise to interesting loops in the learning process (though the precise working of the algorithm is a bit unclear, see below).

If this is correct, and if this is the claim to novelty of the paper, it should be stated explicitly.

Additional possible  problems:
- The paper is somewhat unclear on certain points, which is important to better understand the proposed method. See below. 
- The experiments are interesting, but perhaps a bit limited?
- Not really concerns, but suggestions for future work: the two approaches (Hebbian and gradient-based) seem fully compatible and it might be interesting to see what  happens when a network uses both together; the weight decay parameter, which is now just 1-eta, could include a separate learnable parameter as is Tyulmankov et al. and Rodriguez, Guo && Moraitis ICML 2022 (I am not affiliated with these authors).","Two main points require clarification:

- The description of the network operation is split over several section and somewhat  confusing. Most importantly, exactly which parameters are updated during the inner  vs. outer loop? This should be  specified explicitly.

- The outer loop meta-optimization process should be described more explicitly, especially for the gradient-based case. E.g. do we compute full second-order gradient-of-gradients? Or is there some first-order approximation (like first-order MAML, or REPTL) ? 

For novelty, see  above.
","== Update after reading the authors' response ==

I thank the authors for their clarifications and I have updated my review and score towards acceptance.

== Original review ==

Please cite more existing literature about the concept of meta-learned loss functions; explicitly state the novelty of the work; and clarify the working of the algorithm.

I am willing to increase my score depending on the author's response to the above, especially regarding novelty.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1665875722119,,2WklawyeI08,ICLR.cc/2023/Conference/Paper2942/-/Official_Review,,[],1,,['everyone'],2WklawyeI08,['ICLR.cc/2023/Conference/Paper2942/Reviewer_vDcd'],1665875722119,,1668803052010,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2942/Reviewer_vDcd']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several arXiv papers (https://arxiv.org/abs/1802.04821, https://arxiv.org/abs/1906.05374, https://arxiv.org/abs/2110.03909) and other references that are not cited in the paper, and states that ""Surprisingly, no previous paper on this approach (meta-learning a loss function) seems to be cited in the paper, unless I missed it."" This implies that the reviewer believes the paper is missing relevant citations to existing literature on meta-learned loss functions.",1,"2018, 2019, 2021",Meta-Learning via Learned Loss
J7L7Qo10bF,"The paper compares two forms of meta-learning, with inner-loop updates based either on modulated Hebbian plasticity, or on gradient descent over a network-generated synthetic loss function. 

Various experiments show that both forms of episodic plasticity (Hebbian and gradient-based) improve performance over using fixed recurrent networks, as in RL^2/L2RL. Network-controlled modulation of plasticity is also shown to be important.

","Strengths:

- The paper tackles an interesting problem (learning to learn without necessarily having access to a supervised or even a reward  signal during each episode, i.e. the inner loop is unsupervised).

- The paper is interesting and may propose one novel method, i.e. network-generated gradient loss, though see below.

- The experiments, such as they are, offer reasonable evidence that the proposed methods are useful over the studied tasks


Weaknesses:

My main concern is about novelty. AFAICT the Hebbian portion of the method is identical to previous proposals that are cited in the paper. 

Similarly, the gradient-based  method seems to be an example of meta-learning a loss function to perform gradient descent on in the inner loop. This approach is clearly not new, see e.g. https://arxiv.org/abs/1802.04821 , https://arxiv.org/abs/1906.05374 , https://arxiv.org/abs/2110.03909 and  the many references therein.  (Disclaimer: I am not affiliated with any of these authors)

Surprisingly, no previous paper on this approach (meta-learning a loss function) seems to be cited in the paper, unless I missed it.

There *may* be some novelty in the proposed approach, because here the synthetic loss  is an output of the learner network itself (as opposed to a separate dedicated network). IIUC this means that the synthetic loss itself is subject to learning and tuning during the inner loop (as  in Baik et al. mentioned above, but in a much more flexible manner). This also gives rise to interesting loops in the learning process (though the precise working of the algorithm is a bit unclear, see below).

If this is correct, and if this is the claim to novelty of the paper, it should be stated explicitly.

Additional possible  problems:
- The paper is somewhat unclear on certain points, which is important to better understand the proposed method. See below. 
- The experiments are interesting, but perhaps a bit limited?
- Not really concerns, but suggestions for future work: the two approaches (Hebbian and gradient-based) seem fully compatible and it might be interesting to see what  happens when a network uses both together; the weight decay parameter, which is now just 1-eta, could include a separate learnable parameter as is Tyulmankov et al. and Rodriguez, Guo && Moraitis ICML 2022 (I am not affiliated with these authors).","Two main points require clarification:

- The description of the network operation is split over several section and somewhat  confusing. Most importantly, exactly which parameters are updated during the inner  vs. outer loop? This should be  specified explicitly.

- The outer loop meta-optimization process should be described more explicitly, especially for the gradient-based case. E.g. do we compute full second-order gradient-of-gradients? Or is there some first-order approximation (like first-order MAML, or REPTL) ? 

For novelty, see  above.
","== Update after reading the authors' response ==

I thank the authors for their clarifications and I have updated my review and score towards acceptance.

== Original review ==

Please cite more existing literature about the concept of meta-learned loss functions; explicitly state the novelty of the work; and clarify the working of the algorithm.

I am willing to increase my score depending on the author's response to the above, especially regarding novelty.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1665875722119,,2WklawyeI08,ICLR.cc/2023/Conference/Paper2942/-/Official_Review,,[],1,,['everyone'],2WklawyeI08,['ICLR.cc/2023/Conference/Paper2942/Reviewer_vDcd'],1665875722119,,1668803052010,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2942/Reviewer_vDcd']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several arXiv papers (https://arxiv.org/abs/1802.04821, https://arxiv.org/abs/1906.05374, https://arxiv.org/abs/2110.03909) and other references that are not cited in the paper, and states that ""Surprisingly, no previous paper on this approach (meta-learning a loss function) seems to be cited in the paper, unless I missed it."" This implies that the reviewer believes the paper is missing relevant citations to existing literature on meta-learned loss functions.",1,"2018, 2019, 2021",Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning
941P21eQ0ew,"This paper explores how activate learning can help reduce annotation costs in the field of object detection of 3D point clouds. Specifically, to achieve efficient active learning with limited fixed annotation budgets, three selection criteria termed CRB are proposed to learn better sample acquisition of the 3D boxes annotation. Extensive experiments on KITTI and Waymo validate the effectiveness of the proposed method. The paper is well-written and easy to follow. The experimental results demonstrate the effectiveness of the proposed hierarchical active learning scheme in active learning for 3D object detection.
","### Srength: 
- Activate learning in point cloud object detection is interesting and novel. I believe this paper can provide insights and inspiration to the related works in the 3D object detection area.

- The theoretical analysis is sufficient and helpful to understand the method, and also provide support for the final performance validation.

- The experiments are extensive and can validate the effectiveness of the proposed framework.


### Weaknesses: 
- I would suggest the authors clearly clarify the major difference of the paper with existing works on object detection and point clouds processing. It seems that this paper is analyzed in an general optimized view, but it will be better if the author can explicitly illusrate the innovation and the fundamental difference with exisiting techniques.

- For related work, I would suggest that the author add literature on 2D Detection and some of the latest works in 3D point cloud detectors. Just name a few: 

	1. QBox: Partial Transfer Learning With Active Querying for Object Detection [2021, TNNLS]
	2. Towards Dynamic and Scalable Active Learning with Neural Architecture Adaption for Object Detection [2021, BMVC]: Add NAS into the AL loops.
	3. Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint [2022, CVPR]
	4. Weakly Supervised Object Detection Based on Active Learning [2022, NPL]
	5. Active Learning Strategies for Weakly-Supervised Object Detection [2022]
	6. Label-Efficient Point Cloud Semantic Segmentation: An Active Learning Approach [2021, CVPR]
	7. ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation [2021, ICCV]
	8. Active Learning for Point Cloud Semantic Segmentation via Spatial-Structural Diversity Reasoning [2022]

- As for the experimental results in Table 1, it is a little bit counterintuitive: the proposed CRB with 1% labels can perform better than the general full-label training model. In my view, the active learning methods can achieve the performance very close to the full-label training model. The authors are suggested to explicitly clarify why the performance is even better than fully supervised model.

-More experiments on various detectors (e.g., SECOND, PV-RCNN) are required to illustrate the effectiveness of the proposed criteria for general active 3D object detection. Additionally, indoor scenarios such as ScanNet should also be taken into consideration.","This paper is well-written, and seems solid.

There are a number of literatures on active learning in 2D object detection. Referring to the development of general 2D detectors to 3D detectors, this paper seems adapt data based on 3D point cloud data and 3D bounding box labeling, and the novelty is not well-clarified.","I'm not quite familiar with this topic, but this paper looks very comprehensive and solid, so I'm inclined to borderline accept.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666536279626,,2RwXVje1rAh,ICLR.cc/2023/Conference/Paper831/-/Official_Review,,[],2,,['everyone'],2RwXVje1rAh,['ICLR.cc/2023/Conference/Paper831/Reviewer_S6aF'],1666536279626,,1669281635482,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper831/Reviewer_S6aF']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers related to 2D detection and 3D point cloud detectors, such as QBox, Towards Dynamic and Scalable Active Learning with Neural Architecture Adaption for Object Detection, Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint, and others, and suggests that the authors add these literature to the related work section. This implies that these papers are not already cited in the original paper.",1,,QBox
941P21eQ0ew,"This paper explores how activate learning can help reduce annotation costs in the field of object detection of 3D point clouds. Specifically, to achieve efficient active learning with limited fixed annotation budgets, three selection criteria termed CRB are proposed to learn better sample acquisition of the 3D boxes annotation. Extensive experiments on KITTI and Waymo validate the effectiveness of the proposed method. The paper is well-written and easy to follow. The experimental results demonstrate the effectiveness of the proposed hierarchical active learning scheme in active learning for 3D object detection.
","### Srength: 
- Activate learning in point cloud object detection is interesting and novel. I believe this paper can provide insights and inspiration to the related works in the 3D object detection area.

- The theoretical analysis is sufficient and helpful to understand the method, and also provide support for the final performance validation.

- The experiments are extensive and can validate the effectiveness of the proposed framework.


### Weaknesses: 
- I would suggest the authors clearly clarify the major difference of the paper with existing works on object detection and point clouds processing. It seems that this paper is analyzed in an general optimized view, but it will be better if the author can explicitly illusrate the innovation and the fundamental difference with exisiting techniques.

- For related work, I would suggest that the author add literature on 2D Detection and some of the latest works in 3D point cloud detectors. Just name a few: 

	1. QBox: Partial Transfer Learning With Active Querying for Object Detection [2021, TNNLS]
	2. Towards Dynamic and Scalable Active Learning with Neural Architecture Adaption for Object Detection [2021, BMVC]: Add NAS into the AL loops.
	3. Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint [2022, CVPR]
	4. Weakly Supervised Object Detection Based on Active Learning [2022, NPL]
	5. Active Learning Strategies for Weakly-Supervised Object Detection [2022]
	6. Label-Efficient Point Cloud Semantic Segmentation: An Active Learning Approach [2021, CVPR]
	7. ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation [2021, ICCV]
	8. Active Learning for Point Cloud Semantic Segmentation via Spatial-Structural Diversity Reasoning [2022]

- As for the experimental results in Table 1, it is a little bit counterintuitive: the proposed CRB with 1% labels can perform better than the general full-label training model. In my view, the active learning methods can achieve the performance very close to the full-label training model. The authors are suggested to explicitly clarify why the performance is even better than fully supervised model.

-More experiments on various detectors (e.g., SECOND, PV-RCNN) are required to illustrate the effectiveness of the proposed criteria for general active 3D object detection. Additionally, indoor scenarios such as ScanNet should also be taken into consideration.","This paper is well-written, and seems solid.

There are a number of literatures on active learning in 2D object detection. Referring to the development of general 2D detectors to 3D detectors, this paper seems adapt data based on 3D point cloud data and 3D bounding box labeling, and the novelty is not well-clarified.","I'm not quite familiar with this topic, but this paper looks very comprehensive and solid, so I'm inclined to borderline accept.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666536279626,,2RwXVje1rAh,ICLR.cc/2023/Conference/Paper831/-/Official_Review,,[],2,,['everyone'],2RwXVje1rAh,['ICLR.cc/2023/Conference/Paper831/Reviewer_S6aF'],1666536279626,,1669281635482,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper831/Reviewer_S6aF']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers related to 2D detection and 3D point cloud detectors, such as QBox, Towards Dynamic and Scalable Active Learning with Neural Architecture Adaption for Object Detection, Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint, and others, and suggests that the authors add these literature to the related work section. This implies that these papers are not already cited in the original paper.",1,,Towards Dynamic and Scalable Active Learning with Neural Architecture Adaption for Object Detection
941P21eQ0ew,"This paper explores how activate learning can help reduce annotation costs in the field of object detection of 3D point clouds. Specifically, to achieve efficient active learning with limited fixed annotation budgets, three selection criteria termed CRB are proposed to learn better sample acquisition of the 3D boxes annotation. Extensive experiments on KITTI and Waymo validate the effectiveness of the proposed method. The paper is well-written and easy to follow. The experimental results demonstrate the effectiveness of the proposed hierarchical active learning scheme in active learning for 3D object detection.
","### Srength: 
- Activate learning in point cloud object detection is interesting and novel. I believe this paper can provide insights and inspiration to the related works in the 3D object detection area.

- The theoretical analysis is sufficient and helpful to understand the method, and also provide support for the final performance validation.

- The experiments are extensive and can validate the effectiveness of the proposed framework.


### Weaknesses: 
- I would suggest the authors clearly clarify the major difference of the paper with existing works on object detection and point clouds processing. It seems that this paper is analyzed in an general optimized view, but it will be better if the author can explicitly illusrate the innovation and the fundamental difference with exisiting techniques.

- For related work, I would suggest that the author add literature on 2D Detection and some of the latest works in 3D point cloud detectors. Just name a few: 

	1. QBox: Partial Transfer Learning With Active Querying for Object Detection [2021, TNNLS]
	2. Towards Dynamic and Scalable Active Learning with Neural Architecture Adaption for Object Detection [2021, BMVC]: Add NAS into the AL loops.
	3. Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint [2022, CVPR]
	4. Weakly Supervised Object Detection Based on Active Learning [2022, NPL]
	5. Active Learning Strategies for Weakly-Supervised Object Detection [2022]
	6. Label-Efficient Point Cloud Semantic Segmentation: An Active Learning Approach [2021, CVPR]
	7. ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation [2021, ICCV]
	8. Active Learning for Point Cloud Semantic Segmentation via Spatial-Structural Diversity Reasoning [2022]

- As for the experimental results in Table 1, it is a little bit counterintuitive: the proposed CRB with 1% labels can perform better than the general full-label training model. In my view, the active learning methods can achieve the performance very close to the full-label training model. The authors are suggested to explicitly clarify why the performance is even better than fully supervised model.

-More experiments on various detectors (e.g., SECOND, PV-RCNN) are required to illustrate the effectiveness of the proposed criteria for general active 3D object detection. Additionally, indoor scenarios such as ScanNet should also be taken into consideration.","This paper is well-written, and seems solid.

There are a number of literatures on active learning in 2D object detection. Referring to the development of general 2D detectors to 3D detectors, this paper seems adapt data based on 3D point cloud data and 3D bounding box labeling, and the novelty is not well-clarified.","I'm not quite familiar with this topic, but this paper looks very comprehensive and solid, so I'm inclined to borderline accept.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666536279626,,2RwXVje1rAh,ICLR.cc/2023/Conference/Paper831/-/Official_Review,,[],2,,['everyone'],2RwXVje1rAh,['ICLR.cc/2023/Conference/Paper831/Reviewer_S6aF'],1666536279626,,1669281635482,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper831/Reviewer_S6aF']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers related to 2D detection and 3D point cloud detectors, such as QBox, Towards Dynamic and Scalable Active Learning with Neural Architecture Adaption for Object Detection, Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint, and others, and suggests that the authors add these literature to the related work section. This implies that these papers are not already cited in the original paper.",1,,Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint
BMy4Q_LPSJ,"Distributionally robust optimization methods are often overly convservative, while group DRO approaches requires group annotations, which limits its practicality. To address this challenge, they introduce a practical assumption that group shift is captured by high-level features with a simple function. Based on this assumption, the authors propose a BR-DRO, which makes the model to be robust against distribution shifts along group which is realized by simple functions. This helps model not to be overly conservative while being robust to plausible worst cases, resulting in meaningful performance improvement compared to unconstrained DRO.","Strength
- The paper address why restricted adversary for DRO do better than unconstrained one, which is supported by theoretical justification under the assumption of simple group shift.
- The proposed BR-DRO framework is empirically validated by extensive experiments.

Weakness
- Several important unsupervised comparisons are missing. Especially, [George] and [BPA] extend Group DRO in an unsupervised way, where they conduct clustering on the feature space and identify latent subclass (minority group). In other words, they also aim to identify potential groups for worst-case robustness without group annotations.
- The formulation of BR-DRO starts from Assumption 4.2, but what is the motivation of the assumption? Also, I think it may be applied well because most of group robustness datasets has binary (bias) attributes, Does this simple group shift assumption still work well in datasets where the bias attribute has multiple values?

[George] No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems, NeurIPS 2020

[BPA] Unsupervised Learning of Debiased Representations with Pseudo-Attributes, CVPR 2022","- The paper writing is well structured.
- This work nicely bridges the gap between unconstrained DRO and group DRO with a practical assumption in a novel way.
- Source codes are provided for reproducibility.","The authors constrains the distributionally robust optimization method under the assumption of group shift, which prevents the model from being overly conservative and pessimistic. The practical relaxation of BR-DRO is novel and well supported by experimental results, though those are not that strong compared to recent debiasing approaches. Additional discussions with other robust optimization approaches can help for better understanding.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666638366650,,2QzNuaRHn4Z,ICLR.cc/2023/Conference/Paper6129/-/Official_Review,,[],2,,['everyone'],2QzNuaRHn4Z,['ICLR.cc/2023/Conference/Paper6129/Reviewer_BZ25'],1666638366650,,1666853249002,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6129/Reviewer_BZ25']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. [George] ""No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems"" (NeurIPS 2020)
2. [BPA] ""Unsupervised Learning of Debiased Representations with Pseudo-Attributes"" (CVPR 2022)

The reviewer notes that these papers extend Group DRO in an unsupervised way and aim to identify potential groups for worst-case robustness without group annotations, which is relevant to the topic of the paper. The reviewer suggests that comparisons with these papers are missing in the original paper.",1,"2020, 2022",No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems 
BMy4Q_LPSJ,"Distributionally robust optimization methods are often overly convservative, while group DRO approaches requires group annotations, which limits its practicality. To address this challenge, they introduce a practical assumption that group shift is captured by high-level features with a simple function. Based on this assumption, the authors propose a BR-DRO, which makes the model to be robust against distribution shifts along group which is realized by simple functions. This helps model not to be overly conservative while being robust to plausible worst cases, resulting in meaningful performance improvement compared to unconstrained DRO.","Strength
- The paper address why restricted adversary for DRO do better than unconstrained one, which is supported by theoretical justification under the assumption of simple group shift.
- The proposed BR-DRO framework is empirically validated by extensive experiments.

Weakness
- Several important unsupervised comparisons are missing. Especially, [George] and [BPA] extend Group DRO in an unsupervised way, where they conduct clustering on the feature space and identify latent subclass (minority group). In other words, they also aim to identify potential groups for worst-case robustness without group annotations.
- The formulation of BR-DRO starts from Assumption 4.2, but what is the motivation of the assumption? Also, I think it may be applied well because most of group robustness datasets has binary (bias) attributes, Does this simple group shift assumption still work well in datasets where the bias attribute has multiple values?

[George] No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems, NeurIPS 2020

[BPA] Unsupervised Learning of Debiased Representations with Pseudo-Attributes, CVPR 2022","- The paper writing is well structured.
- This work nicely bridges the gap between unconstrained DRO and group DRO with a practical assumption in a novel way.
- Source codes are provided for reproducibility.","The authors constrains the distributionally robust optimization method under the assumption of group shift, which prevents the model from being overly conservative and pessimistic. The practical relaxation of BR-DRO is novel and well supported by experimental results, though those are not that strong compared to recent debiasing approaches. Additional discussions with other robust optimization approaches can help for better understanding.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666638366650,,2QzNuaRHn4Z,ICLR.cc/2023/Conference/Paper6129/-/Official_Review,,[],2,,['everyone'],2QzNuaRHn4Z,['ICLR.cc/2023/Conference/Paper6129/Reviewer_BZ25'],1666638366650,,1666853249002,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6129/Reviewer_BZ25']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. [George] ""No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems"" (NeurIPS 2020)
2. [BPA] ""Unsupervised Learning of Debiased Representations with Pseudo-Attributes"" (CVPR 2022)

The reviewer notes that these papers extend Group DRO in an unsupervised way and aim to identify potential groups for worst-case robustness without group annotations, which is relevant to the topic of the paper. The reviewer suggests that comparisons with these papers are missing in the original paper.",1,"2020, 2022",Unsupervised Learning of Debiased Representations with Pseudo-Attributes
aSuBSSIWDlt,"Collaborative self-supervised learning is considered based on a combination of known split federated learning and self-supervised contrastive leaning strategies. The former bases on model splitting between server and local clients while the latter depends on momentum-based key generation. The proposed combination, called MocoSFL, has some complexity/performance advantages over exiting FL-based solutions. A variation of this idea, based on pretraining (and later freezing) of feature extractors utilizing a small amount of clients' training data, is also proposed to improve communication overhead as well as resistance to MIA attacks.","Strengths: The chosen topic of collaborative SSL is an important and timely topic. The paper is clearly written. Performance is shown to be better than SOTA in some cases.

Weaknesses: 

The novelty is limited as the work represents a combination of known methods. The design choices made during this combination all seem right but are also either already known or easily anticipated (like latent vector concatenation to create a large
batch size for the server-side model, sharing of features and more frequent synchronization). As for the target-aware variation, it is not clear whether the availability of local clients' training data (however small) is justified. Sure, as the authors mention, Bhagoji et al., 2019 considers the possibility of server being able to validate local models, but note that Bhagoji et al. mean to raise the bar for attackers (in an effort to prove that attacks are possible). In contrast, the present authors actually consider the possibility of using the local client data to guard against MIA attacks (and to reduce communication overhead following the feature extractor freezing). My impression is that this is a highly non-standard FL setting, and I would want to see more convincing references. 

Specific questions/issues include:

The obvious baselines FedEMA and FedU are missing from Table 2 and Table 3.
From Table 5, the results for Non-IID cases with Nc=5 or Nc =100 are better than IID cases which is counter-intuitive. Could the authors provide justification for this performance gain?
Can the authors explain why the latent vector size in Table 1 changed from 5000 to 39.1 MB? Is it just because the backward gradients are not included?
In Figure 7, how the GFLOPs for computation are measured? Specifically, does it include the FLOPs needed for backpropagation?
The authors claim this technique to be competitive in cross-silo setting. However, the paper requires having batch size in range of 100-200 at the server-end which seems impractical considering there exists millions of samples/clients. Could the authors comment on this?
Eq (2) finds that having a large batch-size can maintain hardness of negative samples, which seems to contradicts the empirical results in Table 6 where increasing the batch size further to 400 degrades the performance. Reasons?
Did the authors implemented/reproduced the main results of baselines works e.g. FedEMA? It seems like the results are exactly as in the original paper. 
How does this method compared against a FedMoco (Nanqing Dong, Irina Voiculescu: “Federated Contrastive Learning for Decentralized Unlabeled Medical Images”, 2021; arXiv:2109.07504) which also utilized MoCo in federated learning setup? 
Fig 3 and Fig 7 contains computation results per clients only. How much computation burden is added to the server side?
","The proposed algorithm is clear and the paper is well-written, but the novelty is questionable.","The addressed topic - collaborative SSL - is important and timely. But the novelty here is limited as the work represents a combination of known methods. The design choices made during this combination all seem right but are also either already known or easily anticipated. As for the target-aware variation, the availability of local clients' training data at the server needs a stronger justification (is this a realistic/fair assumption? is it actually done in practice somewhere?). There are a number of questions/issues raised in the Weaknesses section above.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666670174720,,2QGJXyMNoPz,ICLR.cc/2023/Conference/Paper537/-/Official_Review,,[],4,,['everyone'],2QGJXyMNoPz,['ICLR.cc/2023/Conference/Paper537/Reviewer_bsEM'],1666670174720,,1670556299105,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper537/Reviewer_bsEM']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions the following literature that may not be already included in the paper:
1. Bhagoji et al., 2019 - Although the authors mention this work, the reviewer is asking for more convincing references to justify the use of local client data to guard against MIA attacks.
2. Nanqing Dong, Irina Voiculescu: “Federated Contrastive Learning for Decentralized Unlabeled Medical Images”, 2021; arXiv:2109.07504 (FedMoco) - The reviewer asks how the proposed method compares to this work, which also utilizes MoCo in a federated learning setup. 

The reviewer is also asking for more justification and references for the assumption of using local client data at the server, which may not be a standard practice in federated learning.",1,"2019, 2021, 2021",Federated Contrastive Learning for Decentralized Unlabeled Medical Images 
XhEW1lxoEsX,"This paper studies the use of self-supervised learning (SSL) techniques for decentralized learning, with the emphasis on the understanding of how and why it is effective. It focuses particularly on the task of decentralized SSL that enables representation learning from unlabeled data that are owned separately by multiple clients, and compares it with other standard setups for decentralized supervised learning. The paper demonstrated that decentralized SSL works pretty well even when client data are highly non-iid (i.e., heterogeneous) both empirically and theoretically. This finding holds also for labeled data settings. Furthermore, the paper presents a new decentralized SSL method named FeatARC, which groups client data into several clusters to train multiple global models, and confirmed that it worked further better than FedAvg.","## Strong points
- Overall, the paper is excellently written and easy to follow. The paper is very well organized, and the connections to related work, proof of theoretical claims, and details of experiments are well documented in the appendix, making the paper complete.
- The proposed work is well motivated. Although enabling SSL in decentralized (or federated) learning setups is a known topic with some existing work, to my knowledge there has been no study at this level of why SSL should work through extensive empirical evaluation and theoretical analysis. This would seem to be a solid technical contribution.
- The fact that SSL works well for representation learning even from highly non-iid and unlabeled data, which was supported both empirically and theoretically, is a nice finding, because collecting labeled data is often identified as difficult in federated learning. 
- Experimental evaluation is thorough. Decentralized SSL is compared against decentralized SL approaches under a variety of conditions, such as multiple datasets, multiple levels of non-iidness, multiple SSL methods (SimCLR, SimSiam, and BYOL), and multiple hyperparameters for federated learning algorithms (FedAvg). 

## Weak points
- Although the experimental result is already extensive, this work could have been further stronger if it confirmed the effectiveness of DecSSL with other federated learning algorithms. Currently, all the results are obtained using FedAvg, a very standard algorithm of federated learning. On the other hand, the performance of FedAvg is known to degrade when data is highly non-iid [a] or when the model includes batch normalization [b]. It remains not perfectly obvious if the current results hold only for FedAvg or various other algorithms such as FedProx and FedBN. This is a critical question to me to judge the significance of this work.
- Another that was not very clear in the paper is the effect of heterogeneity in terms of data size |D_k|. As shown in Eq 2.2, this heterogeneity is considered in the global objective. How does it affect the performance of DecSSL empirically and theoretically? 

[a] Li et al., ""Federated Optimization in Heterogeneous Networks"", MLSys 2020

[b] Li et al., ""FedBN: Federated Learning on Non-IID Features via Local Batch Normalization"", ICLR 2021","- **Clarity**: The paper is very clearly written. The motivation, approaches, and results of the work are all clear.
- **Quality**: Overall, I think the work is of very high quality with excellent presentation and sufficient technical contributions.
- **Novelty**: As far as I know, this level of detailed experimental evaluation and theoretical analysis for how SSL works well under decentralized learning setups is novel.
- **Reproducibility**: Details of experimental setups are presented in the appendix. Code is also submitted. ","I think that this seems to be a solid piece of work with sufficient technical contributions and excellent presentation. Nevertheless, I also believe that this work could become further stronger if other federated learning algorithms (e.g., FedProx, FedBN) are also evaluated. Otherwise it remains unclear if the provided results hold just for a specific algorithm of FedAvg or can work in general.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666158001677,,2L9gzS80tA4,ICLR.cc/2023/Conference/Paper3160/-/Official_Review,,[],1,,['everyone'],2L9gzS80tA4,['ICLR.cc/2023/Conference/Paper3160/Reviewer_fhez'],1666158001677,,1666158060872,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3160/Reviewer_fhez']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Li et al., ""Federated Optimization in Heterogeneous Networks"", MLSys 2020
2. Li et al., ""FedBN: Federated Learning on Non-IID Features via Local Batch Normalization"", ICLR 2021

These papers are mentioned as references [a] and [b] in the review, and the reviewer suggests that the authors should consider the performance of federated learning algorithms other than FedAvg, such as FedProx and FedBN, which are discussed in these papers. This implies that these papers are not already discussed in the original paper.",1,"2020, 2021",Federated Optimization in Heterogeneous Networks 
XhEW1lxoEsX,"This paper studies the use of self-supervised learning (SSL) techniques for decentralized learning, with the emphasis on the understanding of how and why it is effective. It focuses particularly on the task of decentralized SSL that enables representation learning from unlabeled data that are owned separately by multiple clients, and compares it with other standard setups for decentralized supervised learning. The paper demonstrated that decentralized SSL works pretty well even when client data are highly non-iid (i.e., heterogeneous) both empirically and theoretically. This finding holds also for labeled data settings. Furthermore, the paper presents a new decentralized SSL method named FeatARC, which groups client data into several clusters to train multiple global models, and confirmed that it worked further better than FedAvg.","## Strong points
- Overall, the paper is excellently written and easy to follow. The paper is very well organized, and the connections to related work, proof of theoretical claims, and details of experiments are well documented in the appendix, making the paper complete.
- The proposed work is well motivated. Although enabling SSL in decentralized (or federated) learning setups is a known topic with some existing work, to my knowledge there has been no study at this level of why SSL should work through extensive empirical evaluation and theoretical analysis. This would seem to be a solid technical contribution.
- The fact that SSL works well for representation learning even from highly non-iid and unlabeled data, which was supported both empirically and theoretically, is a nice finding, because collecting labeled data is often identified as difficult in federated learning. 
- Experimental evaluation is thorough. Decentralized SSL is compared against decentralized SL approaches under a variety of conditions, such as multiple datasets, multiple levels of non-iidness, multiple SSL methods (SimCLR, SimSiam, and BYOL), and multiple hyperparameters for federated learning algorithms (FedAvg). 

## Weak points
- Although the experimental result is already extensive, this work could have been further stronger if it confirmed the effectiveness of DecSSL with other federated learning algorithms. Currently, all the results are obtained using FedAvg, a very standard algorithm of federated learning. On the other hand, the performance of FedAvg is known to degrade when data is highly non-iid [a] or when the model includes batch normalization [b]. It remains not perfectly obvious if the current results hold only for FedAvg or various other algorithms such as FedProx and FedBN. This is a critical question to me to judge the significance of this work.
- Another that was not very clear in the paper is the effect of heterogeneity in terms of data size |D_k|. As shown in Eq 2.2, this heterogeneity is considered in the global objective. How does it affect the performance of DecSSL empirically and theoretically? 

[a] Li et al., ""Federated Optimization in Heterogeneous Networks"", MLSys 2020

[b] Li et al., ""FedBN: Federated Learning on Non-IID Features via Local Batch Normalization"", ICLR 2021","- **Clarity**: The paper is very clearly written. The motivation, approaches, and results of the work are all clear.
- **Quality**: Overall, I think the work is of very high quality with excellent presentation and sufficient technical contributions.
- **Novelty**: As far as I know, this level of detailed experimental evaluation and theoretical analysis for how SSL works well under decentralized learning setups is novel.
- **Reproducibility**: Details of experimental setups are presented in the appendix. Code is also submitted. ","I think that this seems to be a solid piece of work with sufficient technical contributions and excellent presentation. Nevertheless, I also believe that this work could become further stronger if other federated learning algorithms (e.g., FedProx, FedBN) are also evaluated. Otherwise it remains unclear if the provided results hold just for a specific algorithm of FedAvg or can work in general.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666158001677,,2L9gzS80tA4,ICLR.cc/2023/Conference/Paper3160/-/Official_Review,,[],1,,['everyone'],2L9gzS80tA4,['ICLR.cc/2023/Conference/Paper3160/Reviewer_fhez'],1666158001677,,1666158060872,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3160/Reviewer_fhez']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Li et al., ""Federated Optimization in Heterogeneous Networks"", MLSys 2020
2. Li et al., ""FedBN: Federated Learning on Non-IID Features via Local Batch Normalization"", ICLR 2021

These papers are mentioned as references [a] and [b] in the review, and the reviewer suggests that the authors should consider the performance of federated learning algorithms other than FedAvg, such as FedProx and FedBN, which are discussed in these papers. This implies that these papers are not already discussed in the original paper.",1,"2020, 2021",FedBN Federated Learning on Non-IID Features via Local Batch Normalization
1gJEwS41Y6,"The paper proposes using an empirical Bayesian framework to simultaneously learn an embedding model ($f_\phi$) and a probabilistic model (parameterized w/ $\theta$) for the continual learning setting. To that end, $f_\phi$ is updated using a variant of experience replay through a running memory of samples, where the memory samples are updated with each task such that the KL divergence between the true posterior for $\theta$ and the posterior with the new memory is minimized. While updating the embedding network, the authors use marginal log-likelihood wrt $y$ of a subset of data conditioned on the rest. The paper presents one instance of the proposed framework referred to as DeepCCG which uses conditional Gaussian model as the probabilistic model. Several experiments show the DeepCCG performs better than a few selected baselines in two different (disjoint and shifting window) task incremental settings. ","Strengths

- The proposed DeepCCG seems to do reasonably well in the task-incremental setting compared to some of the considered baselines.
- The experiment design of using a moving window to simulate task overlap has not been explored before.

Weakness

- *Missing related work*: There is an entire class of methods that do expansion-based [1-5] continual learning that have not been discussed in the related work. [1-4] also use a Bayesian framework (though for expansion) for continual learning. These methods also learn the entire model without using any pre-trained/frozen embedding network. This class of methods should be discussed in the related work.
- *Replay Baseline*: Replay based on generative models [5-6] has not been considered as a baseline.
- The writing and the presentation in the paper can be improved significantly. In terms of presentation, section 4 seems like an over-generalization of the methodology presented in section 5. Since the paper only discusses a simple variant of the general framework, which uses a class conditioned Gaussian to define the probabilistic model, section 4 seems a bit redundant. 
- Some of the claims over-state the contribution, for e.g. Pg 5 section 4 (last para): ""our general approach can be used in task-agnostic settings as well"" -- The task-agnostic continual learning is a much more challenging setting than task incremental. Without any empirical evidence, this claim is not well supported. 
- The sample selection strategy based on the KL divergence of the two posteriors in equation 11 assumes that the training data in each task is same across the tasks? From equation 11, it seems that given a new task with large number of samples, the new memory would be biased towards the new task, which could easily cause catastrophic forgetting of previous tasks. However, the paper only assumed settings with equal number of samples across all tasks.
- The paper only considers the task-incremental setting, which is known to be a relatively easier setup than other challenging continual learning settings, including class-incremental and task-free continual learning. 


Other Minor concerns:
- The notation in the paper can be improved:
  - In equation 1, the marginalized likelihood is a function of the embedding network. Making this precise in the notation (e.g. $p_\phi (y|x,t_{x,y})$) would make the discussion that follows in section 4 clearer.
- In section 6.1 under benchmarks: ""in shifting windows ... there is no overlap between any two of the task"" -- Is this a typo? There is overlap with shifting windows, right? 


[1] A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning, (Lee et al., (2020))
[2] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., (2021))
[3] Bayesian structure adaptation for continual learning, (Kumar et al. (2020))
[4] Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning, (Kessler et al, (2020))
[5] Efficient Feature Transformations for Discriminative and Generative Continual Learning, (Kumar et al., (2021))
[6] Deep Generative Replay, (Shin et al. (2017))
","Clarity and Quality: The writing and presentation in the paper can be improved significantly. The section 4 and section 5 can be combined to clarify that the probabilistic model and the embedding network are updated separately.  There are also many grammatical errors that can be fixed.

Novelty: The novelty seems limited given that the probabilistic model is based on a simplified class-conditional Gaussian model (LDA) which was also used in Ostapenko et al. (2020). Moreover, the idea of using a subset of samples to determine the conditional marginal log-likelihood (Equation 3) was also explored in Lofti et al., (2022), although its application in continual learning seems new.

","Based on the above weaknesses and concerns, my recommendation is to reject the paper in its current state. While the idea of using DCCG w/ experience replay is interesting, there are some major concerns in terms of clarity and presentation. The baselines and related work can also be improved significantly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1667549861692,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,,[],3,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD'],1667549861692,,1669410259747,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-6]) that are not discussed in the related work section of the paper, including:

* A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning (Lee et al., 2020)
* Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., 2021)
* Bayesian structure adaptation for continual learning (Kumar et al., 2020)
* Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning (Kessler et al., 2020)
* Efficient Feature Transformations for Discriminative and Generative Continual Learning (Kumar et al., 2021)
* Deep Generative Replay (Shin et al., 2017)

The reviewer also mentions Ostapenko et al. (2020) and Lofti et al. (2022) as relevant works that are not discussed in the paper. The reviewer suggests that the authors should discuss these papers in the related work section to improve the paper's completeness and accuracy.",1,"2020, 2021, 2020, 2020, 2021, 2017, 2020, 2022",A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning 
1gJEwS41Y6,"The paper proposes using an empirical Bayesian framework to simultaneously learn an embedding model ($f_\phi$) and a probabilistic model (parameterized w/ $\theta$) for the continual learning setting. To that end, $f_\phi$ is updated using a variant of experience replay through a running memory of samples, where the memory samples are updated with each task such that the KL divergence between the true posterior for $\theta$ and the posterior with the new memory is minimized. While updating the embedding network, the authors use marginal log-likelihood wrt $y$ of a subset of data conditioned on the rest. The paper presents one instance of the proposed framework referred to as DeepCCG which uses conditional Gaussian model as the probabilistic model. Several experiments show the DeepCCG performs better than a few selected baselines in two different (disjoint and shifting window) task incremental settings. ","Strengths

- The proposed DeepCCG seems to do reasonably well in the task-incremental setting compared to some of the considered baselines.
- The experiment design of using a moving window to simulate task overlap has not been explored before.

Weakness

- *Missing related work*: There is an entire class of methods that do expansion-based [1-5] continual learning that have not been discussed in the related work. [1-4] also use a Bayesian framework (though for expansion) for continual learning. These methods also learn the entire model without using any pre-trained/frozen embedding network. This class of methods should be discussed in the related work.
- *Replay Baseline*: Replay based on generative models [5-6] has not been considered as a baseline.
- The writing and the presentation in the paper can be improved significantly. In terms of presentation, section 4 seems like an over-generalization of the methodology presented in section 5. Since the paper only discusses a simple variant of the general framework, which uses a class conditioned Gaussian to define the probabilistic model, section 4 seems a bit redundant. 
- Some of the claims over-state the contribution, for e.g. Pg 5 section 4 (last para): ""our general approach can be used in task-agnostic settings as well"" -- The task-agnostic continual learning is a much more challenging setting than task incremental. Without any empirical evidence, this claim is not well supported. 
- The sample selection strategy based on the KL divergence of the two posteriors in equation 11 assumes that the training data in each task is same across the tasks? From equation 11, it seems that given a new task with large number of samples, the new memory would be biased towards the new task, which could easily cause catastrophic forgetting of previous tasks. However, the paper only assumed settings with equal number of samples across all tasks.
- The paper only considers the task-incremental setting, which is known to be a relatively easier setup than other challenging continual learning settings, including class-incremental and task-free continual learning. 


Other Minor concerns:
- The notation in the paper can be improved:
  - In equation 1, the marginalized likelihood is a function of the embedding network. Making this precise in the notation (e.g. $p_\phi (y|x,t_{x,y})$) would make the discussion that follows in section 4 clearer.
- In section 6.1 under benchmarks: ""in shifting windows ... there is no overlap between any two of the task"" -- Is this a typo? There is overlap with shifting windows, right? 


[1] A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning, (Lee et al., (2020))
[2] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., (2021))
[3] Bayesian structure adaptation for continual learning, (Kumar et al. (2020))
[4] Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning, (Kessler et al, (2020))
[5] Efficient Feature Transformations for Discriminative and Generative Continual Learning, (Kumar et al., (2021))
[6] Deep Generative Replay, (Shin et al. (2017))
","Clarity and Quality: The writing and presentation in the paper can be improved significantly. The section 4 and section 5 can be combined to clarify that the probabilistic model and the embedding network are updated separately.  There are also many grammatical errors that can be fixed.

Novelty: The novelty seems limited given that the probabilistic model is based on a simplified class-conditional Gaussian model (LDA) which was also used in Ostapenko et al. (2020). Moreover, the idea of using a subset of samples to determine the conditional marginal log-likelihood (Equation 3) was also explored in Lofti et al., (2022), although its application in continual learning seems new.

","Based on the above weaknesses and concerns, my recommendation is to reject the paper in its current state. While the idea of using DCCG w/ experience replay is interesting, there are some major concerns in terms of clarity and presentation. The baselines and related work can also be improved significantly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1667549861692,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,,[],3,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD'],1667549861692,,1669410259747,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-6]) that are not discussed in the related work section of the paper, including:

* A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning (Lee et al., 2020)
* Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., 2021)
* Bayesian structure adaptation for continual learning (Kumar et al., 2020)
* Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning (Kessler et al., 2020)
* Efficient Feature Transformations for Discriminative and Generative Continual Learning (Kumar et al., 2021)
* Deep Generative Replay (Shin et al., 2017)

The reviewer also mentions Ostapenko et al. (2020) and Lofti et al. (2022) as relevant works that are not discussed in the paper. The reviewer suggests that the authors should discuss these papers in the related work section to improve the paper's completeness and accuracy.",1,"2020, 2021, 2020, 2020, 2021, 2017, 2020, 2022",Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors 
1gJEwS41Y6,"The paper proposes using an empirical Bayesian framework to simultaneously learn an embedding model ($f_\phi$) and a probabilistic model (parameterized w/ $\theta$) for the continual learning setting. To that end, $f_\phi$ is updated using a variant of experience replay through a running memory of samples, where the memory samples are updated with each task such that the KL divergence between the true posterior for $\theta$ and the posterior with the new memory is minimized. While updating the embedding network, the authors use marginal log-likelihood wrt $y$ of a subset of data conditioned on the rest. The paper presents one instance of the proposed framework referred to as DeepCCG which uses conditional Gaussian model as the probabilistic model. Several experiments show the DeepCCG performs better than a few selected baselines in two different (disjoint and shifting window) task incremental settings. ","Strengths

- The proposed DeepCCG seems to do reasonably well in the task-incremental setting compared to some of the considered baselines.
- The experiment design of using a moving window to simulate task overlap has not been explored before.

Weakness

- *Missing related work*: There is an entire class of methods that do expansion-based [1-5] continual learning that have not been discussed in the related work. [1-4] also use a Bayesian framework (though for expansion) for continual learning. These methods also learn the entire model without using any pre-trained/frozen embedding network. This class of methods should be discussed in the related work.
- *Replay Baseline*: Replay based on generative models [5-6] has not been considered as a baseline.
- The writing and the presentation in the paper can be improved significantly. In terms of presentation, section 4 seems like an over-generalization of the methodology presented in section 5. Since the paper only discusses a simple variant of the general framework, which uses a class conditioned Gaussian to define the probabilistic model, section 4 seems a bit redundant. 
- Some of the claims over-state the contribution, for e.g. Pg 5 section 4 (last para): ""our general approach can be used in task-agnostic settings as well"" -- The task-agnostic continual learning is a much more challenging setting than task incremental. Without any empirical evidence, this claim is not well supported. 
- The sample selection strategy based on the KL divergence of the two posteriors in equation 11 assumes that the training data in each task is same across the tasks? From equation 11, it seems that given a new task with large number of samples, the new memory would be biased towards the new task, which could easily cause catastrophic forgetting of previous tasks. However, the paper only assumed settings with equal number of samples across all tasks.
- The paper only considers the task-incremental setting, which is known to be a relatively easier setup than other challenging continual learning settings, including class-incremental and task-free continual learning. 


Other Minor concerns:
- The notation in the paper can be improved:
  - In equation 1, the marginalized likelihood is a function of the embedding network. Making this precise in the notation (e.g. $p_\phi (y|x,t_{x,y})$) would make the discussion that follows in section 4 clearer.
- In section 6.1 under benchmarks: ""in shifting windows ... there is no overlap between any two of the task"" -- Is this a typo? There is overlap with shifting windows, right? 


[1] A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning, (Lee et al., (2020))
[2] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., (2021))
[3] Bayesian structure adaptation for continual learning, (Kumar et al. (2020))
[4] Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning, (Kessler et al, (2020))
[5] Efficient Feature Transformations for Discriminative and Generative Continual Learning, (Kumar et al., (2021))
[6] Deep Generative Replay, (Shin et al. (2017))
","Clarity and Quality: The writing and presentation in the paper can be improved significantly. The section 4 and section 5 can be combined to clarify that the probabilistic model and the embedding network are updated separately.  There are also many grammatical errors that can be fixed.

Novelty: The novelty seems limited given that the probabilistic model is based on a simplified class-conditional Gaussian model (LDA) which was also used in Ostapenko et al. (2020). Moreover, the idea of using a subset of samples to determine the conditional marginal log-likelihood (Equation 3) was also explored in Lofti et al., (2022), although its application in continual learning seems new.

","Based on the above weaknesses and concerns, my recommendation is to reject the paper in its current state. While the idea of using DCCG w/ experience replay is interesting, there are some major concerns in terms of clarity and presentation. The baselines and related work can also be improved significantly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1667549861692,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,,[],3,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD'],1667549861692,,1669410259747,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-6]) that are not discussed in the related work section of the paper, including:

* A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning (Lee et al., 2020)
* Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., 2021)
* Bayesian structure adaptation for continual learning (Kumar et al., 2020)
* Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning (Kessler et al., 2020)
* Efficient Feature Transformations for Discriminative and Generative Continual Learning (Kumar et al., 2021)
* Deep Generative Replay (Shin et al., 2017)

The reviewer also mentions Ostapenko et al. (2020) and Lofti et al. (2022) as relevant works that are not discussed in the paper. The reviewer suggests that the authors should discuss these papers in the related work section to improve the paper's completeness and accuracy.",1,"2020, 2021, 2020, 2020, 2021, 2017, 2020, 2022",Bayesian structure adaptation for continual learning 
1gJEwS41Y6,"The paper proposes using an empirical Bayesian framework to simultaneously learn an embedding model ($f_\phi$) and a probabilistic model (parameterized w/ $\theta$) for the continual learning setting. To that end, $f_\phi$ is updated using a variant of experience replay through a running memory of samples, where the memory samples are updated with each task such that the KL divergence between the true posterior for $\theta$ and the posterior with the new memory is minimized. While updating the embedding network, the authors use marginal log-likelihood wrt $y$ of a subset of data conditioned on the rest. The paper presents one instance of the proposed framework referred to as DeepCCG which uses conditional Gaussian model as the probabilistic model. Several experiments show the DeepCCG performs better than a few selected baselines in two different (disjoint and shifting window) task incremental settings. ","Strengths

- The proposed DeepCCG seems to do reasonably well in the task-incremental setting compared to some of the considered baselines.
- The experiment design of using a moving window to simulate task overlap has not been explored before.

Weakness

- *Missing related work*: There is an entire class of methods that do expansion-based [1-5] continual learning that have not been discussed in the related work. [1-4] also use a Bayesian framework (though for expansion) for continual learning. These methods also learn the entire model without using any pre-trained/frozen embedding network. This class of methods should be discussed in the related work.
- *Replay Baseline*: Replay based on generative models [5-6] has not been considered as a baseline.
- The writing and the presentation in the paper can be improved significantly. In terms of presentation, section 4 seems like an over-generalization of the methodology presented in section 5. Since the paper only discusses a simple variant of the general framework, which uses a class conditioned Gaussian to define the probabilistic model, section 4 seems a bit redundant. 
- Some of the claims over-state the contribution, for e.g. Pg 5 section 4 (last para): ""our general approach can be used in task-agnostic settings as well"" -- The task-agnostic continual learning is a much more challenging setting than task incremental. Without any empirical evidence, this claim is not well supported. 
- The sample selection strategy based on the KL divergence of the two posteriors in equation 11 assumes that the training data in each task is same across the tasks? From equation 11, it seems that given a new task with large number of samples, the new memory would be biased towards the new task, which could easily cause catastrophic forgetting of previous tasks. However, the paper only assumed settings with equal number of samples across all tasks.
- The paper only considers the task-incremental setting, which is known to be a relatively easier setup than other challenging continual learning settings, including class-incremental and task-free continual learning. 


Other Minor concerns:
- The notation in the paper can be improved:
  - In equation 1, the marginalized likelihood is a function of the embedding network. Making this precise in the notation (e.g. $p_\phi (y|x,t_{x,y})$) would make the discussion that follows in section 4 clearer.
- In section 6.1 under benchmarks: ""in shifting windows ... there is no overlap between any two of the task"" -- Is this a typo? There is overlap with shifting windows, right? 


[1] A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning, (Lee et al., (2020))
[2] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., (2021))
[3] Bayesian structure adaptation for continual learning, (Kumar et al. (2020))
[4] Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning, (Kessler et al, (2020))
[5] Efficient Feature Transformations for Discriminative and Generative Continual Learning, (Kumar et al., (2021))
[6] Deep Generative Replay, (Shin et al. (2017))
","Clarity and Quality: The writing and presentation in the paper can be improved significantly. The section 4 and section 5 can be combined to clarify that the probabilistic model and the embedding network are updated separately.  There are also many grammatical errors that can be fixed.

Novelty: The novelty seems limited given that the probabilistic model is based on a simplified class-conditional Gaussian model (LDA) which was also used in Ostapenko et al. (2020). Moreover, the idea of using a subset of samples to determine the conditional marginal log-likelihood (Equation 3) was also explored in Lofti et al., (2022), although its application in continual learning seems new.

","Based on the above weaknesses and concerns, my recommendation is to reject the paper in its current state. While the idea of using DCCG w/ experience replay is interesting, there are some major concerns in terms of clarity and presentation. The baselines and related work can also be improved significantly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1667549861692,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,,[],3,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD'],1667549861692,,1669410259747,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-6]) that are not discussed in the related work section of the paper, including:

* A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning (Lee et al., 2020)
* Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., 2021)
* Bayesian structure adaptation for continual learning (Kumar et al., 2020)
* Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning (Kessler et al., 2020)
* Efficient Feature Transformations for Discriminative and Generative Continual Learning (Kumar et al., 2021)
* Deep Generative Replay (Shin et al., 2017)

The reviewer also mentions Ostapenko et al. (2020) and Lofti et al. (2022) as relevant works that are not discussed in the paper. The reviewer suggests that the authors should discuss these papers in the related work section to improve the paper's completeness and accuracy.",1,"2020, 2021, 2020, 2020, 2021, 2017, 2020, 2022",Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning 
1gJEwS41Y6,"The paper proposes using an empirical Bayesian framework to simultaneously learn an embedding model ($f_\phi$) and a probabilistic model (parameterized w/ $\theta$) for the continual learning setting. To that end, $f_\phi$ is updated using a variant of experience replay through a running memory of samples, where the memory samples are updated with each task such that the KL divergence between the true posterior for $\theta$ and the posterior with the new memory is minimized. While updating the embedding network, the authors use marginal log-likelihood wrt $y$ of a subset of data conditioned on the rest. The paper presents one instance of the proposed framework referred to as DeepCCG which uses conditional Gaussian model as the probabilistic model. Several experiments show the DeepCCG performs better than a few selected baselines in two different (disjoint and shifting window) task incremental settings. ","Strengths

- The proposed DeepCCG seems to do reasonably well in the task-incremental setting compared to some of the considered baselines.
- The experiment design of using a moving window to simulate task overlap has not been explored before.

Weakness

- *Missing related work*: There is an entire class of methods that do expansion-based [1-5] continual learning that have not been discussed in the related work. [1-4] also use a Bayesian framework (though for expansion) for continual learning. These methods also learn the entire model without using any pre-trained/frozen embedding network. This class of methods should be discussed in the related work.
- *Replay Baseline*: Replay based on generative models [5-6] has not been considered as a baseline.
- The writing and the presentation in the paper can be improved significantly. In terms of presentation, section 4 seems like an over-generalization of the methodology presented in section 5. Since the paper only discusses a simple variant of the general framework, which uses a class conditioned Gaussian to define the probabilistic model, section 4 seems a bit redundant. 
- Some of the claims over-state the contribution, for e.g. Pg 5 section 4 (last para): ""our general approach can be used in task-agnostic settings as well"" -- The task-agnostic continual learning is a much more challenging setting than task incremental. Without any empirical evidence, this claim is not well supported. 
- The sample selection strategy based on the KL divergence of the two posteriors in equation 11 assumes that the training data in each task is same across the tasks? From equation 11, it seems that given a new task with large number of samples, the new memory would be biased towards the new task, which could easily cause catastrophic forgetting of previous tasks. However, the paper only assumed settings with equal number of samples across all tasks.
- The paper only considers the task-incremental setting, which is known to be a relatively easier setup than other challenging continual learning settings, including class-incremental and task-free continual learning. 


Other Minor concerns:
- The notation in the paper can be improved:
  - In equation 1, the marginalized likelihood is a function of the embedding network. Making this precise in the notation (e.g. $p_\phi (y|x,t_{x,y})$) would make the discussion that follows in section 4 clearer.
- In section 6.1 under benchmarks: ""in shifting windows ... there is no overlap between any two of the task"" -- Is this a typo? There is overlap with shifting windows, right? 


[1] A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning, (Lee et al., (2020))
[2] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., (2021))
[3] Bayesian structure adaptation for continual learning, (Kumar et al. (2020))
[4] Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning, (Kessler et al, (2020))
[5] Efficient Feature Transformations for Discriminative and Generative Continual Learning, (Kumar et al., (2021))
[6] Deep Generative Replay, (Shin et al. (2017))
","Clarity and Quality: The writing and presentation in the paper can be improved significantly. The section 4 and section 5 can be combined to clarify that the probabilistic model and the embedding network are updated separately.  There are also many grammatical errors that can be fixed.

Novelty: The novelty seems limited given that the probabilistic model is based on a simplified class-conditional Gaussian model (LDA) which was also used in Ostapenko et al. (2020). Moreover, the idea of using a subset of samples to determine the conditional marginal log-likelihood (Equation 3) was also explored in Lofti et al., (2022), although its application in continual learning seems new.

","Based on the above weaknesses and concerns, my recommendation is to reject the paper in its current state. While the idea of using DCCG w/ experience replay is interesting, there are some major concerns in terms of clarity and presentation. The baselines and related work can also be improved significantly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1667549861692,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,,[],3,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD'],1667549861692,,1669410259747,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-6]) that are not discussed in the related work section of the paper, including:

* A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning (Lee et al., 2020)
* Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., 2021)
* Bayesian structure adaptation for continual learning (Kumar et al., 2020)
* Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning (Kessler et al., 2020)
* Efficient Feature Transformations for Discriminative and Generative Continual Learning (Kumar et al., 2021)
* Deep Generative Replay (Shin et al., 2017)

The reviewer also mentions Ostapenko et al. (2020) and Lofti et al. (2022) as relevant works that are not discussed in the paper. The reviewer suggests that the authors should discuss these papers in the related work section to improve the paper's completeness and accuracy.",1,"2020, 2021, 2020, 2020, 2021, 2017, 2020, 2022",Efficient Feature Transformations for Discriminative and Generative Continual Learning 
1gJEwS41Y6,"The paper proposes using an empirical Bayesian framework to simultaneously learn an embedding model ($f_\phi$) and a probabilistic model (parameterized w/ $\theta$) for the continual learning setting. To that end, $f_\phi$ is updated using a variant of experience replay through a running memory of samples, where the memory samples are updated with each task such that the KL divergence between the true posterior for $\theta$ and the posterior with the new memory is minimized. While updating the embedding network, the authors use marginal log-likelihood wrt $y$ of a subset of data conditioned on the rest. The paper presents one instance of the proposed framework referred to as DeepCCG which uses conditional Gaussian model as the probabilistic model. Several experiments show the DeepCCG performs better than a few selected baselines in two different (disjoint and shifting window) task incremental settings. ","Strengths

- The proposed DeepCCG seems to do reasonably well in the task-incremental setting compared to some of the considered baselines.
- The experiment design of using a moving window to simulate task overlap has not been explored before.

Weakness

- *Missing related work*: There is an entire class of methods that do expansion-based [1-5] continual learning that have not been discussed in the related work. [1-4] also use a Bayesian framework (though for expansion) for continual learning. These methods also learn the entire model without using any pre-trained/frozen embedding network. This class of methods should be discussed in the related work.
- *Replay Baseline*: Replay based on generative models [5-6] has not been considered as a baseline.
- The writing and the presentation in the paper can be improved significantly. In terms of presentation, section 4 seems like an over-generalization of the methodology presented in section 5. Since the paper only discusses a simple variant of the general framework, which uses a class conditioned Gaussian to define the probabilistic model, section 4 seems a bit redundant. 
- Some of the claims over-state the contribution, for e.g. Pg 5 section 4 (last para): ""our general approach can be used in task-agnostic settings as well"" -- The task-agnostic continual learning is a much more challenging setting than task incremental. Without any empirical evidence, this claim is not well supported. 
- The sample selection strategy based on the KL divergence of the two posteriors in equation 11 assumes that the training data in each task is same across the tasks? From equation 11, it seems that given a new task with large number of samples, the new memory would be biased towards the new task, which could easily cause catastrophic forgetting of previous tasks. However, the paper only assumed settings with equal number of samples across all tasks.
- The paper only considers the task-incremental setting, which is known to be a relatively easier setup than other challenging continual learning settings, including class-incremental and task-free continual learning. 


Other Minor concerns:
- The notation in the paper can be improved:
  - In equation 1, the marginalized likelihood is a function of the embedding network. Making this precise in the notation (e.g. $p_\phi (y|x,t_{x,y})$) would make the discussion that follows in section 4 clearer.
- In section 6.1 under benchmarks: ""in shifting windows ... there is no overlap between any two of the task"" -- Is this a typo? There is overlap with shifting windows, right? 


[1] A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning, (Lee et al., (2020))
[2] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., (2021))
[3] Bayesian structure adaptation for continual learning, (Kumar et al. (2020))
[4] Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning, (Kessler et al, (2020))
[5] Efficient Feature Transformations for Discriminative and Generative Continual Learning, (Kumar et al., (2021))
[6] Deep Generative Replay, (Shin et al. (2017))
","Clarity and Quality: The writing and presentation in the paper can be improved significantly. The section 4 and section 5 can be combined to clarify that the probabilistic model and the embedding network are updated separately.  There are also many grammatical errors that can be fixed.

Novelty: The novelty seems limited given that the probabilistic model is based on a simplified class-conditional Gaussian model (LDA) which was also used in Ostapenko et al. (2020). Moreover, the idea of using a subset of samples to determine the conditional marginal log-likelihood (Equation 3) was also explored in Lofti et al., (2022), although its application in continual learning seems new.

","Based on the above weaknesses and concerns, my recommendation is to reject the paper in its current state. While the idea of using DCCG w/ experience replay is interesting, there are some major concerns in terms of clarity and presentation. The baselines and related work can also be improved significantly.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1667549861692,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,,[],3,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD'],1667549861692,,1669410259747,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_rHsD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-6]) that are not discussed in the related work section of the paper, including:

* A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning (Lee et al., 2020)
* Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors (Mehta et al., 2021)
* Bayesian structure adaptation for continual learning (Kumar et al., 2020)
* Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning (Kessler et al., 2020)
* Efficient Feature Transformations for Discriminative and Generative Continual Learning (Kumar et al., 2021)
* Deep Generative Replay (Shin et al., 2017)

The reviewer also mentions Ostapenko et al. (2020) and Lofti et al. (2022) as relevant works that are not discussed in the paper. The reviewer suggests that the authors should discuss these papers in the related work section to improve the paper's completeness and accuracy.",1,"2020, 2021, 2020, 2020, 2021, 2017, 2020, 2022",Deep Generative Replay 
FPVwRI07-Z,"The paper proposes a deep class conditional model for the online task's incremental learning. The primary motivation of the paper is to learn the feature embedder since a fixed feature extractor is not useful in the dynamic environment. In this work, the model leverages the replay buffer to overcome the catastrophic forgetting and the buffer samples are selected by maximizing the posterior between the subset of samples and task samples. They used jointly the buffer and batch samples to update the embedder network parameter. Finally, the samples are classified by learning the prototype/mean of the class.","Pors:

1: The model is simple and requires a small memory buffer to overcome the catastrophic forgetting. 

2: Easy to follow and ablation for the sample selection strategy w.r.t. reservoir shows the improvement. Sample selection method is intuitive and it may be useful in the other replay based approach. However, practical time complexity may be a bottleneck.

3: For the reproducibility, they provided the code.

Cons:

1: The paper handles the easiest setting of the continual learning (CL) where task id are provided during inference, which is not practically useful. In this setting, a simple expansion based approach [a,b,c,d] without any replay, model achieves the performance near the upper-bound without using the pretrained backbone. In this paper, none of these methods are compared. Replay is required for the comparatively much complex setting, like class incremental learning or task-free continual learning.

2: The model is not end-to-end, it learns the embedder and classifier separately. The embedder learns the class specific cluster representation and class conditional Gaussian learns the prototype classifier. Also, learning only the mean is too simple and it may not handle the complex scenarios. 

3: The sample selection strategy requires finding the subset of samples to maximize the posterior. Is there any optimization based model to select this subset of samples? If not, it’s like a brute-force model and sample selection is too costly.

4: Novelty is limited conditional marginal likelihood was explored in earlier work, sample selection approach seems novel but the complexity of it may be a bottleneck.

5: The baselines are weak and no recent model are included. How the author created the baseline for the previous work and upper-bound? The implementation details of the baseline are missing, do the author did the proper hyperparameter search for the baseline? 

[a] Calibrating CNNs for Lifelong Learning, NeurIPS-20

[b] Ternary Feature Masks: zero-forgetting for task-incremental learning, CVPRW-21

[c] Supermasks in Superposition, NeurIPS-20

[d] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors, AISTATS-21

","The paper has clear writing, novelty is limited and proper baseline of close work and recent works are missing.
The code is provided hence reproducibility may not be an issue.","The paper uses the simplest setting of CL and without replay recent model shows the result close to the upper-bound.

Recent comparison with the similar and recent work is missing 

Paper has some novelty, but that is not significant for the publication. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1670500551654,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,16705005516540.0,[],4,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_U88A'],1670500551654,,1670500551654,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_U88A']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions several papers ([a] Calibrating CNNs for Lifelong Learning, NeurIPS-20; [b] Ternary Feature Masks: zero-forgetting for task-incremental learning, CVPRW-21; [c] Supermasks in Superposition, NeurIPS-20; [d] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors, AISTATS-21) that are not mentioned in the summary of the paper, implying that these papers are relevant to the topic and should be considered by the authors. 

The reviewer criticizes the paper for not comparing with these recent models, which achieve performance near the upper-bound without using replay, and suggests that the authors should include these models as baselines for a more comprehensive evaluation.",1,,Calibrating CNNs for Lifelong Learning 
FPVwRI07-Z,"The paper proposes a deep class conditional model for the online task's incremental learning. The primary motivation of the paper is to learn the feature embedder since a fixed feature extractor is not useful in the dynamic environment. In this work, the model leverages the replay buffer to overcome the catastrophic forgetting and the buffer samples are selected by maximizing the posterior between the subset of samples and task samples. They used jointly the buffer and batch samples to update the embedder network parameter. Finally, the samples are classified by learning the prototype/mean of the class.","Pors:

1: The model is simple and requires a small memory buffer to overcome the catastrophic forgetting. 

2: Easy to follow and ablation for the sample selection strategy w.r.t. reservoir shows the improvement. Sample selection method is intuitive and it may be useful in the other replay based approach. However, practical time complexity may be a bottleneck.

3: For the reproducibility, they provided the code.

Cons:

1: The paper handles the easiest setting of the continual learning (CL) where task id are provided during inference, which is not practically useful. In this setting, a simple expansion based approach [a,b,c,d] without any replay, model achieves the performance near the upper-bound without using the pretrained backbone. In this paper, none of these methods are compared. Replay is required for the comparatively much complex setting, like class incremental learning or task-free continual learning.

2: The model is not end-to-end, it learns the embedder and classifier separately. The embedder learns the class specific cluster representation and class conditional Gaussian learns the prototype classifier. Also, learning only the mean is too simple and it may not handle the complex scenarios. 

3: The sample selection strategy requires finding the subset of samples to maximize the posterior. Is there any optimization based model to select this subset of samples? If not, it’s like a brute-force model and sample selection is too costly.

4: Novelty is limited conditional marginal likelihood was explored in earlier work, sample selection approach seems novel but the complexity of it may be a bottleneck.

5: The baselines are weak and no recent model are included. How the author created the baseline for the previous work and upper-bound? The implementation details of the baseline are missing, do the author did the proper hyperparameter search for the baseline? 

[a] Calibrating CNNs for Lifelong Learning, NeurIPS-20

[b] Ternary Feature Masks: zero-forgetting for task-incremental learning, CVPRW-21

[c] Supermasks in Superposition, NeurIPS-20

[d] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors, AISTATS-21

","The paper has clear writing, novelty is limited and proper baseline of close work and recent works are missing.
The code is provided hence reproducibility may not be an issue.","The paper uses the simplest setting of CL and without replay recent model shows the result close to the upper-bound.

Recent comparison with the similar and recent work is missing 

Paper has some novelty, but that is not significant for the publication. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1670500551654,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,16705005516540.0,[],4,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_U88A'],1670500551654,,1670500551654,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_U88A']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions several papers ([a] Calibrating CNNs for Lifelong Learning, NeurIPS-20; [b] Ternary Feature Masks: zero-forgetting for task-incremental learning, CVPRW-21; [c] Supermasks in Superposition, NeurIPS-20; [d] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors, AISTATS-21) that are not mentioned in the summary of the paper, implying that these papers are relevant to the topic and should be considered by the authors. 

The reviewer criticizes the paper for not comparing with these recent models, which achieve performance near the upper-bound without using replay, and suggests that the authors should include these models as baselines for a more comprehensive evaluation.",1,,Ternary Feature Masks: zero-forgetting for task-incremental learning 
FPVwRI07-Z,"The paper proposes a deep class conditional model for the online task's incremental learning. The primary motivation of the paper is to learn the feature embedder since a fixed feature extractor is not useful in the dynamic environment. In this work, the model leverages the replay buffer to overcome the catastrophic forgetting and the buffer samples are selected by maximizing the posterior between the subset of samples and task samples. They used jointly the buffer and batch samples to update the embedder network parameter. Finally, the samples are classified by learning the prototype/mean of the class.","Pors:

1: The model is simple and requires a small memory buffer to overcome the catastrophic forgetting. 

2: Easy to follow and ablation for the sample selection strategy w.r.t. reservoir shows the improvement. Sample selection method is intuitive and it may be useful in the other replay based approach. However, practical time complexity may be a bottleneck.

3: For the reproducibility, they provided the code.

Cons:

1: The paper handles the easiest setting of the continual learning (CL) where task id are provided during inference, which is not practically useful. In this setting, a simple expansion based approach [a,b,c,d] without any replay, model achieves the performance near the upper-bound without using the pretrained backbone. In this paper, none of these methods are compared. Replay is required for the comparatively much complex setting, like class incremental learning or task-free continual learning.

2: The model is not end-to-end, it learns the embedder and classifier separately. The embedder learns the class specific cluster representation and class conditional Gaussian learns the prototype classifier. Also, learning only the mean is too simple and it may not handle the complex scenarios. 

3: The sample selection strategy requires finding the subset of samples to maximize the posterior. Is there any optimization based model to select this subset of samples? If not, it’s like a brute-force model and sample selection is too costly.

4: Novelty is limited conditional marginal likelihood was explored in earlier work, sample selection approach seems novel but the complexity of it may be a bottleneck.

5: The baselines are weak and no recent model are included. How the author created the baseline for the previous work and upper-bound? The implementation details of the baseline are missing, do the author did the proper hyperparameter search for the baseline? 

[a] Calibrating CNNs for Lifelong Learning, NeurIPS-20

[b] Ternary Feature Masks: zero-forgetting for task-incremental learning, CVPRW-21

[c] Supermasks in Superposition, NeurIPS-20

[d] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors, AISTATS-21

","The paper has clear writing, novelty is limited and proper baseline of close work and recent works are missing.
The code is provided hence reproducibility may not be an issue.","The paper uses the simplest setting of CL and without replay recent model shows the result close to the upper-bound.

Recent comparison with the similar and recent work is missing 

Paper has some novelty, but that is not significant for the publication. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1670500551654,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,16705005516540.0,[],4,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_U88A'],1670500551654,,1670500551654,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_U88A']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions several papers ([a] Calibrating CNNs for Lifelong Learning, NeurIPS-20; [b] Ternary Feature Masks: zero-forgetting for task-incremental learning, CVPRW-21; [c] Supermasks in Superposition, NeurIPS-20; [d] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors, AISTATS-21) that are not mentioned in the summary of the paper, implying that these papers are relevant to the topic and should be considered by the authors. 

The reviewer criticizes the paper for not comparing with these recent models, which achieve performance near the upper-bound without using replay, and suggests that the authors should include these models as baselines for a more comprehensive evaluation.",1,,Supermasks in Superposition 
FPVwRI07-Z,"The paper proposes a deep class conditional model for the online task's incremental learning. The primary motivation of the paper is to learn the feature embedder since a fixed feature extractor is not useful in the dynamic environment. In this work, the model leverages the replay buffer to overcome the catastrophic forgetting and the buffer samples are selected by maximizing the posterior between the subset of samples and task samples. They used jointly the buffer and batch samples to update the embedder network parameter. Finally, the samples are classified by learning the prototype/mean of the class.","Pors:

1: The model is simple and requires a small memory buffer to overcome the catastrophic forgetting. 

2: Easy to follow and ablation for the sample selection strategy w.r.t. reservoir shows the improvement. Sample selection method is intuitive and it may be useful in the other replay based approach. However, practical time complexity may be a bottleneck.

3: For the reproducibility, they provided the code.

Cons:

1: The paper handles the easiest setting of the continual learning (CL) where task id are provided during inference, which is not practically useful. In this setting, a simple expansion based approach [a,b,c,d] without any replay, model achieves the performance near the upper-bound without using the pretrained backbone. In this paper, none of these methods are compared. Replay is required for the comparatively much complex setting, like class incremental learning or task-free continual learning.

2: The model is not end-to-end, it learns the embedder and classifier separately. The embedder learns the class specific cluster representation and class conditional Gaussian learns the prototype classifier. Also, learning only the mean is too simple and it may not handle the complex scenarios. 

3: The sample selection strategy requires finding the subset of samples to maximize the posterior. Is there any optimization based model to select this subset of samples? If not, it’s like a brute-force model and sample selection is too costly.

4: Novelty is limited conditional marginal likelihood was explored in earlier work, sample selection approach seems novel but the complexity of it may be a bottleneck.

5: The baselines are weak and no recent model are included. How the author created the baseline for the previous work and upper-bound? The implementation details of the baseline are missing, do the author did the proper hyperparameter search for the baseline? 

[a] Calibrating CNNs for Lifelong Learning, NeurIPS-20

[b] Ternary Feature Masks: zero-forgetting for task-incremental learning, CVPRW-21

[c] Supermasks in Superposition, NeurIPS-20

[d] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors, AISTATS-21

","The paper has clear writing, novelty is limited and proper baseline of close work and recent works are missing.
The code is provided hence reproducibility may not be an issue.","The paper uses the simplest setting of CL and without replay recent model shows the result close to the upper-bound.

Recent comparison with the similar and recent work is missing 

Paper has some novelty, but that is not significant for the publication. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1670500551654,,2L-nspTvNVC,ICLR.cc/2023/Conference/Paper4279/-/Official_Review,16705005516540.0,[],4,,['everyone'],2L-nspTvNVC,['ICLR.cc/2023/Conference/Paper4279/Reviewer_U88A'],1670500551654,,1670500551654,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4279/Reviewer_U88A']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions several papers ([a] Calibrating CNNs for Lifelong Learning, NeurIPS-20; [b] Ternary Feature Masks: zero-forgetting for task-incremental learning, CVPRW-21; [c] Supermasks in Superposition, NeurIPS-20; [d] Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors, AISTATS-21) that are not mentioned in the summary of the paper, implying that these papers are relevant to the topic and should be considered by the authors. 

The reviewer criticizes the paper for not comparing with these recent models, which achieve performance near the upper-bound without using replay, and suggests that the authors should include these models as baselines for a more comprehensive evaluation.",1,,Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors
iJMHNAFm29,"This paper proposes to use masked inverse folding to pre-train (1) a protein structure-based encoder (MIF) and (2) a structure-based encoder enhanced by protein language model representations (MIF-ST). (1) MIF encodes the information of protein backbone structure and masked protein sequence into node and edge features, and the model is trained to recover the masked residues based on the encoded information. (2) MIF-ST further improves the performance of sequence recovery by feeding protein language model representations into the structure-based encoder. On zero-shot and out-of-domain protein landscape prediction benchmarks, MIF and MIF-ST achieve competitive empirical performance against previous methods. ","Strength:
1. The proposed methods are well motivated. MIF is proposed to improve sequence-only masked language modeling with backbone structural information, and MIF-ST is proposed to further improve MIF by pre-trained protein sequence representations. 
2. The advantages of pre-trained structure-based encoders are demonstrated on a comprehensive set of protein engineering related tasks, and the ablation study analyzes the effect of using experimental and predicted structures for downstream evaluation. 


Weakness:
1. For performance comparison, some important protein structure pre-training baselines are lacked. In a closely related work [a], several structure-based encoders pre-trained by contrastive learning and self-prediction methods are proposed, and the source codes are released. It will be nice to compare with these baseline models on protein engineering tasks. 
2. Some ablation studies are lacked. According to Table 2, ESM-1b and ESM-1v are superior protein language models against CARP-640M. Why not using their representations as inputs of MIF? In this way, we can expect stronger MIF-ST models than the current one based on CARP-640M representations. 
3. Some notations are not consistent across figures and texts. In Figure 2, authors use $\varphi$ and $\theta$, while $\phi$ and $\psi$ are used in the equations (3) and (4).


[a] Zhang, Zuobai, et al. ""Protein representation learning by geometric structure pretraining."" arXiv preprint arXiv:2203.06125 (2022).
","Quality:

In general, this paper is well-written and well-motivated, except for some notation issues.


Novelty:

The technical novelty of this work is plain. The structure-based encoder is proposed by a previous work, and the masked inverse folding pre-training scheme share great similarity with the ""structure-based residue type prediction"" objective proposed by [a].


Reproducibility:

Authors submitted source codes for reproducing the results. The codes look well-organized according to my brief check. 
","To summarize, this paper shows some merits of boosting future research on structure-based pre-training for protein engineering, while more comprehensive performance comparisons and ablation studies are required to make it a more solid contribution. I think it is currently on the border and expect authors' efforts during the response period. ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1665740063853,,2EO8eQ2vySB,ICLR.cc/2023/Conference/Paper1987/-/Official_Review,,[],1,,['everyone'],2EO8eQ2vySB,['ICLR.cc/2023/Conference/Paper1987/Reviewer_mxA7'],1665740063853,,1665740063853,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1987/Reviewer_mxA7']","Yes, this peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer mentions a closely related work [a] ""Zhang, Zuobai, et al. 'Protein representation learning by geometric structure pretraining.' arXiv preprint arXiv:2203.06125 (2022)"" and suggests that the authors compare their methods with the baseline models proposed in this work. This implies that the original paper does not already discuss or compare with this specific literature.",1,"2022, 2022",Protein representation learning by geometric structure pretraining
XVPa2Cl8yW,"In IR, this paper generalizes some neural interaction models like e.g. ColBERT, ME-BERT or COIL, by ""learning"" the alignement, as well as pruning the multi-vector representations. The model is shown to have a few-sample adaptability on the BEIR dataset.
","
Strengths:
- Learning the alignement allows to improve results of already successful models such as ColBERT.
- Few-shot learning based on selecting the right alignment improve the results
- Interesting sparsity regularizer with a target ratio (or number? This is not really clear) of tokens kept for the index

Weaknesses:
- The alignement is not learned
- The sparsity is not really learned since during indexing the ratio is different ($\beta$ vs $\alpha$) and more importantly it is fixed - why this discrepancy?
- Novelty is low (pruning has been proposed in ColBERTer)
- The experimental comparisons lack rigor (compared models have 60x times less parameters, ColBERTer is not included in the experiments, some models could have been tested with the alignement adaptation)
- No complexity analysis (comparing it to ColBERTv2 or ColBERTer)
","
The paper is mostly clear and well organized. There is some novelty although sparsifying is largely studied in IR.

Section 3.2 (sparsity regularizer) is not well motivated - why not using a L1 loss (like in ColBERTer)? The authors should discuss and motivate much more this aspect since it could interesting for other works (but like it is, the approach is not really convincing). Also, why using two pruning ratios (for training and inference)? How were those ratios determined?

In the experiments,

1) ColBERTer and/or ColBERTv2 should be compared with the approach (or at least, your entropy-based regularization should be compared with a simpler L1 one). Some related works cited in the ColBERTer paper are purely ignored, but quite related to what you are doing (e.g. Multi-Vector Attention Models for Deep Re-ranking, )
2) An even simpler pruning strategy, random pruning, has not been experimented with
3) Some extra-care should be taken to compare comparable models (in terms of parameters at least, a 60x ratio is not acceptable)

Other issues:
- Foonote 4: $H$ is not a point-wise entropy if defined that way - looks like the standard entropy (but $\lambda_i$ does not define a probability distribution)
- ""We further check"" (p. 7): this paragraph is not self-contained and cannot be understood without reading the appendix","This paper proposes to (1) use different alignment patterns for few-shot learning (2) sparsify the set of vectors used to compute a document. There are some unclear parts and a lack of motivation (in particular for (2)), and experimental results lack some rigor in the comparison. The alignment few-shot learning could have been more interesting and innovative by looking at various patterns (and motivating them better).
",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666706505900,,2EFQ_QlcPs8,ICLR.cc/2023/Conference/Paper3816/-/Official_Review,,[],3,,['everyone'],2EFQ_QlcPs8,['ICLR.cc/2023/Conference/Paper3816/Reviewer_sjAr'],1666706505900,,1669390082184,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3816/Reviewer_sjAr']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions ""Multi-Vector Attention Models for Deep Re-ranking"" as a related work that is ""purely ignored"" in the paper, implying that it is not cited or discussed in the original paper. This suggests that the reviewer believes the authors should be aware of and engage with this literature in their work.

Additionally, the reviewer mentions ColBERTer and ColBERTv2, which may be already discussed in the paper, but the reviewer is asking for a more thorough comparison with these models, implying that the authors should delve deeper into the literature on these models. 

However, it is not clear if ""Multi-Vector Attention Models for Deep Re-ranking"" is actually not cited in the paper, as the review does not provide the list of references of the paper. But based on the text, it seems that the reviewer is suggesting the authors to look into this specific literature.",1,,Multi-Vector Attention Models for Deep Re-ranking
1ziDHVa6PY,"The authors cast the multi-vector retrieval problem as sparse alignment between query and document tokens. They propose ALIGNER, a novel multi-vector retrieval model that learns sparsified pairwise alignments between query and document tokens (e.g. ‘dog’ vs. ‘puppy’) and per-token unary saliences reflecting their relative importance for retrieval. They show that controlling the sparsity of pairwise token alignments often brings significant performance gains. In a zero-shot setting, ALIGNER scores 51.1 points nDCG@10, achieving a new retriever-only state-of-the-art on 13 tasks in the BEIR benchmark.","Strength
1. The authors proposed a new way to learn sparsified pairwise alignments between query and document tokens.
2. The unary saliences can significantly reduce the document token representations minimal performance loss.
3. The method can achieve good performance on BEIR benchmark under the zero-shot setting.

Weakness:
1. No analysis on retrieval speed and index space.
2. The size of the datasets seem not large enough. Most of the datasets have been pre-processed by some existing IR tools. It would be great if the authors could build index from scratch and compare to the method of ""Dense Passage Retrieval for Open-Domain Question Answering"".
",What's the retrieval speed and index space? How the hyper parameters influence the speed and space?,"I think this is an interesting paper by introducing a new method of sparsified pairwise alignments, and achieve solid performance on benchmark IR dataset. It would be better if the author can discuss more details about the cost on speed/space and further compare with the setting in dense passage retrieval work.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666720026004,,2EFQ_QlcPs8,ICLR.cc/2023/Conference/Paper3816/-/Official_Review,,[],4,,['everyone'],2EFQ_QlcPs8,['ICLR.cc/2023/Conference/Paper3816/Reviewer_waGp'],1666720026004,,1666720026004,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3816/Reviewer_waGp']","Yes, this peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper, specifically the method of ""Dense Passage Retrieval for Open-Domain Question Answering"". The reviewer mentions that it would be great if the authors could compare their method to this approach, implying that it is not already discussed in the paper.",1,,Dense Passage Retrieval for Open-Domain Question Answering
2uxRS7V9fV,"This paper studies the problem of global and personalized federated learning, aiming to improve both at the same time. This paper proposes a method, FedHKD, that leverages hyper-knowledge distillation to improve both global and local learning. Specifically, hyper-knowledge is defined as the class-wise mean representations and mean logit predictions. The server aggregates meta-knowledge from all clients, and sends the aggregated hyper-knowledge back for clients to learn. Compared with other works which use knowledge distillation to improve FL, FedHKD ahas the advantage of not requiring public datasets, and does not require training generative models. The authors present theoretical analysis on the convergence of FedHKD. Empirically, FedHKD outperforms state-of-the-art federated learning methods in both local and global accuracy. ","Strengths: 
- The proposed FedHKD seems efficient and general. FedHKD does not require local datasets, and does not require training generative models, which makes it more applicable in practice. 
- The paper is clearly organized and easy to follow. The main advantages and motivations of FedHKD are clearly stated. The method is clearly described. 

Weaknesses and Questions. 
- Missing an important related work. I appreciate the authors' attempts to bridge local and global federated learning. However, as the authors study class distribution heterogeneity only, I think FedROD (Chen and Chao, 2022) should be discussed and compared, as FedROD also aims to bridge local and global federated learning under class distribution heterogeneity. 
- Experiment results do not comply with existing works. For example, with ResNet18 on CIFAR-10, with Dirichlet label distribution $\beta=0.5$ and 20 clients, FedHKD achieve global accuracy of 0.5735. However, as reported in Fed-ROD, with a simple CNN and $\beta=0.3$ (larger distribution heterogeneity), 20 clients, Fed-ROD achieves 0.768 (even FedAvg achieves 0.686). These numbers differ significantly, and the authors should justify the difference. 
- ""Local Acc"" is not clearly described. First, the evaluated model is not clearly stated (I assume that the models evaluated are the local models before aggregation). Second, it has been studied that FedAvg+local fine-tuning (Cheng et al. 2021) is a powerful baseline in personalized FL. I think is is more appropriate and fair to evaluate local accuracy after some local fine-tuning. 
- DP on hyper-knowledge seems not necessary. As aggregating the hyper-knowledge requires only addition, is it possible to leverage secure aggregation (Bonawitz et al. 2017) instead of DP, which adds no noise? 
- Minor points: It is suggested to replace \cite with \citep in some places, such as Section 4.1. Also, in conclusion, there is a 'FedHDK' which should be a typo. 

H. Chen and W. Chao, ON BRIDGING GENERIC AND PERSONALIZED FEDERATED LEARNING FOR IMAGE CLASSIFICATION. ICLR 2022

K. Bonawitz et al. Practical Secure Aggregation for Privacy-Preserving Machine Learning, CCS 2017

Gary Cheng et al. Federated Asymptotics: a model to compare federated learning algorithms. https://arxiv.org/abs/2108.07313
","Clarity: The clarity of this work is good. I can follow the paper without much effort. 

Novelty: The novelty of this work is good. I appreciate the attempts to use KD without public datasets or generative model. 

Quality: I have some doubts on the experimental results of this paper. Some justifications are needed to improve its technical quality. ","My main concerns of this paper are 1) missing a related work, and 2) experimental results require more justification. I recommend a weak reject at this stage. I am open to recommending accept if the authors can clarify my doubts. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1665913419463,,29V3AWjVAFi,ICLR.cc/2023/Conference/Paper5862/-/Official_Review,,[],1,,['everyone'],29V3AWjVAFi,['ICLR.cc/2023/Conference/Paper5862/Reviewer_y1Yi'],1665913419463,,1668568459918,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5862/Reviewer_y1Yi']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are not included in the original paper:

1. FedROD (Chen and Chao, 2022) - The reviewer thinks this paper should be discussed and compared as it also aims to bridge local and global federated learning under class distribution heterogeneity.
2. Practical Secure Aggregation for Privacy-Preserving Machine Learning (Bonawitz et al., 2017) - The reviewer suggests using secure aggregation instead of differential privacy (DP) for aggregating hyper-knowledge.
3. Federated Asymptotics (Cheng et al., 2021) - The reviewer mentions this paper as a powerful baseline in personalized FL and thinks it would be more appropriate to evaluate local accuracy after some local fine-tuning.

These papers are not mentioned in the original paper, and the reviewer is suggesting that the authors should consider including them in their discussion to improve the paper's quality and novelty.",1,"2022, 2017, 2021",FedROD 
2uxRS7V9fV,"This paper studies the problem of global and personalized federated learning, aiming to improve both at the same time. This paper proposes a method, FedHKD, that leverages hyper-knowledge distillation to improve both global and local learning. Specifically, hyper-knowledge is defined as the class-wise mean representations and mean logit predictions. The server aggregates meta-knowledge from all clients, and sends the aggregated hyper-knowledge back for clients to learn. Compared with other works which use knowledge distillation to improve FL, FedHKD ahas the advantage of not requiring public datasets, and does not require training generative models. The authors present theoretical analysis on the convergence of FedHKD. Empirically, FedHKD outperforms state-of-the-art federated learning methods in both local and global accuracy. ","Strengths: 
- The proposed FedHKD seems efficient and general. FedHKD does not require local datasets, and does not require training generative models, which makes it more applicable in practice. 
- The paper is clearly organized and easy to follow. The main advantages and motivations of FedHKD are clearly stated. The method is clearly described. 

Weaknesses and Questions. 
- Missing an important related work. I appreciate the authors' attempts to bridge local and global federated learning. However, as the authors study class distribution heterogeneity only, I think FedROD (Chen and Chao, 2022) should be discussed and compared, as FedROD also aims to bridge local and global federated learning under class distribution heterogeneity. 
- Experiment results do not comply with existing works. For example, with ResNet18 on CIFAR-10, with Dirichlet label distribution $\beta=0.5$ and 20 clients, FedHKD achieve global accuracy of 0.5735. However, as reported in Fed-ROD, with a simple CNN and $\beta=0.3$ (larger distribution heterogeneity), 20 clients, Fed-ROD achieves 0.768 (even FedAvg achieves 0.686). These numbers differ significantly, and the authors should justify the difference. 
- ""Local Acc"" is not clearly described. First, the evaluated model is not clearly stated (I assume that the models evaluated are the local models before aggregation). Second, it has been studied that FedAvg+local fine-tuning (Cheng et al. 2021) is a powerful baseline in personalized FL. I think is is more appropriate and fair to evaluate local accuracy after some local fine-tuning. 
- DP on hyper-knowledge seems not necessary. As aggregating the hyper-knowledge requires only addition, is it possible to leverage secure aggregation (Bonawitz et al. 2017) instead of DP, which adds no noise? 
- Minor points: It is suggested to replace \cite with \citep in some places, such as Section 4.1. Also, in conclusion, there is a 'FedHDK' which should be a typo. 

H. Chen and W. Chao, ON BRIDGING GENERIC AND PERSONALIZED FEDERATED LEARNING FOR IMAGE CLASSIFICATION. ICLR 2022

K. Bonawitz et al. Practical Secure Aggregation for Privacy-Preserving Machine Learning, CCS 2017

Gary Cheng et al. Federated Asymptotics: a model to compare federated learning algorithms. https://arxiv.org/abs/2108.07313
","Clarity: The clarity of this work is good. I can follow the paper without much effort. 

Novelty: The novelty of this work is good. I appreciate the attempts to use KD without public datasets or generative model. 

Quality: I have some doubts on the experimental results of this paper. Some justifications are needed to improve its technical quality. ","My main concerns of this paper are 1) missing a related work, and 2) experimental results require more justification. I recommend a weak reject at this stage. I am open to recommending accept if the authors can clarify my doubts. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1665913419463,,29V3AWjVAFi,ICLR.cc/2023/Conference/Paper5862/-/Official_Review,,[],1,,['everyone'],29V3AWjVAFi,['ICLR.cc/2023/Conference/Paper5862/Reviewer_y1Yi'],1665913419463,,1668568459918,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5862/Reviewer_y1Yi']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are not included in the original paper:

1. FedROD (Chen and Chao, 2022) - The reviewer thinks this paper should be discussed and compared as it also aims to bridge local and global federated learning under class distribution heterogeneity.
2. Practical Secure Aggregation for Privacy-Preserving Machine Learning (Bonawitz et al., 2017) - The reviewer suggests using secure aggregation instead of differential privacy (DP) for aggregating hyper-knowledge.
3. Federated Asymptotics (Cheng et al., 2021) - The reviewer mentions this paper as a powerful baseline in personalized FL and thinks it would be more appropriate to evaluate local accuracy after some local fine-tuning.

These papers are not mentioned in the original paper, and the reviewer is suggesting that the authors should consider including them in their discussion to improve the paper's quality and novelty.",1,"2022, 2017, 2021",Practical Secure Aggregation for Privacy-Preserving Machine Learning 
2uxRS7V9fV,"This paper studies the problem of global and personalized federated learning, aiming to improve both at the same time. This paper proposes a method, FedHKD, that leverages hyper-knowledge distillation to improve both global and local learning. Specifically, hyper-knowledge is defined as the class-wise mean representations and mean logit predictions. The server aggregates meta-knowledge from all clients, and sends the aggregated hyper-knowledge back for clients to learn. Compared with other works which use knowledge distillation to improve FL, FedHKD ahas the advantage of not requiring public datasets, and does not require training generative models. The authors present theoretical analysis on the convergence of FedHKD. Empirically, FedHKD outperforms state-of-the-art federated learning methods in both local and global accuracy. ","Strengths: 
- The proposed FedHKD seems efficient and general. FedHKD does not require local datasets, and does not require training generative models, which makes it more applicable in practice. 
- The paper is clearly organized and easy to follow. The main advantages and motivations of FedHKD are clearly stated. The method is clearly described. 

Weaknesses and Questions. 
- Missing an important related work. I appreciate the authors' attempts to bridge local and global federated learning. However, as the authors study class distribution heterogeneity only, I think FedROD (Chen and Chao, 2022) should be discussed and compared, as FedROD also aims to bridge local and global federated learning under class distribution heterogeneity. 
- Experiment results do not comply with existing works. For example, with ResNet18 on CIFAR-10, with Dirichlet label distribution $\beta=0.5$ and 20 clients, FedHKD achieve global accuracy of 0.5735. However, as reported in Fed-ROD, with a simple CNN and $\beta=0.3$ (larger distribution heterogeneity), 20 clients, Fed-ROD achieves 0.768 (even FedAvg achieves 0.686). These numbers differ significantly, and the authors should justify the difference. 
- ""Local Acc"" is not clearly described. First, the evaluated model is not clearly stated (I assume that the models evaluated are the local models before aggregation). Second, it has been studied that FedAvg+local fine-tuning (Cheng et al. 2021) is a powerful baseline in personalized FL. I think is is more appropriate and fair to evaluate local accuracy after some local fine-tuning. 
- DP on hyper-knowledge seems not necessary. As aggregating the hyper-knowledge requires only addition, is it possible to leverage secure aggregation (Bonawitz et al. 2017) instead of DP, which adds no noise? 
- Minor points: It is suggested to replace \cite with \citep in some places, such as Section 4.1. Also, in conclusion, there is a 'FedHDK' which should be a typo. 

H. Chen and W. Chao, ON BRIDGING GENERIC AND PERSONALIZED FEDERATED LEARNING FOR IMAGE CLASSIFICATION. ICLR 2022

K. Bonawitz et al. Practical Secure Aggregation for Privacy-Preserving Machine Learning, CCS 2017

Gary Cheng et al. Federated Asymptotics: a model to compare federated learning algorithms. https://arxiv.org/abs/2108.07313
","Clarity: The clarity of this work is good. I can follow the paper without much effort. 

Novelty: The novelty of this work is good. I appreciate the attempts to use KD without public datasets or generative model. 

Quality: I have some doubts on the experimental results of this paper. Some justifications are needed to improve its technical quality. ","My main concerns of this paper are 1) missing a related work, and 2) experimental results require more justification. I recommend a weak reject at this stage. I am open to recommending accept if the authors can clarify my doubts. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1665913419463,,29V3AWjVAFi,ICLR.cc/2023/Conference/Paper5862/-/Official_Review,,[],1,,['everyone'],29V3AWjVAFi,['ICLR.cc/2023/Conference/Paper5862/Reviewer_y1Yi'],1665913419463,,1668568459918,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5862/Reviewer_y1Yi']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are not included in the original paper:

1. FedROD (Chen and Chao, 2022) - The reviewer thinks this paper should be discussed and compared as it also aims to bridge local and global federated learning under class distribution heterogeneity.
2. Practical Secure Aggregation for Privacy-Preserving Machine Learning (Bonawitz et al., 2017) - The reviewer suggests using secure aggregation instead of differential privacy (DP) for aggregating hyper-knowledge.
3. Federated Asymptotics (Cheng et al., 2021) - The reviewer mentions this paper as a powerful baseline in personalized FL and thinks it would be more appropriate to evaluate local accuracy after some local fine-tuning.

These papers are not mentioned in the original paper, and the reviewer is suggesting that the authors should consider including them in their discussion to improve the paper's quality and novelty.",1,"2022, 2017, 2021",Federated Asymptotics
PTnnAr9w8G,"In this work, the authors propose a pretraining diffusion model for defending against 3D adversarial point clouds. They also shows the limitation of robust training for defense due to gradient obfuscation. The results have shown better qualitative performance than SOTA methods. ","Pros: 
+) The idea is clear and easy to follow. They show that adversarial training does not work well on 3D point cloud recognition task. 
+) They deploy diffusion model for recovering/purifying 3D point clouds, which further increase the correct detection ability. 

Cons:
-) The methodology is straightford with limited technical contributions.
-) Diffusion model itself is a powerful generative model, capable of denoising. Limited novelty on technical contribution on purifying adversarial 3D point clouds. The authors are suggested to clarify their contributions. 
-) A lot of missing experiments, e.g. [1, 2]. It is suggested to clarify the advantage of the method with more experimental results. 
-) Missing evaluations on larger distortions, e.g. perturbation is 0.1. 
-) typo error: P3: Donget al.->Dong et al.

[1] Kaidong Li et al, Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients, CVPR22
[2] Hang Zhou et al, DUP-Net: Denoiser and upsampler network for 3d adversarial point clouds defense, ICCV19","+) The paper is well written.
+) The idea and experiments are clear.
+) The first diffusion model based 3D adversarial point cloud defense. ","The paper has shown potential of deploying diffusion model for 3D adversarial point cloud defense. The major problem is the lack of novely on designing their diffusion models, as well as experiments on the recent works. Therefore, it is difficult to justify their performance improvements.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,"Yes, Privacy, security and safety",5: marginally below the acceptance threshold,1666340558406,,293zPCqNqe,ICLR.cc/2023/Conference/Paper609/-/Official_Review,,[],1,,['everyone'],293zPCqNqe,['ICLR.cc/2023/Conference/Paper609/Reviewer_TFZv'],1666340558406,,1670925958118,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper609/Reviewer_TFZv']","Yes, the peer review suggests the authors of the paper to refer to specific literature that may not be already discussed in the original paper. The reviewer mentions two specific papers:

1. Kaidong Li et al, ""Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients"", CVPR22
2. Hang Zhou et al, ""DUP-Net: Denoiser and upsampler network for 3d adversarial point clouds defense"", ICCV19

The reviewer suggests that the authors should clarify the advantage of their method with more experimental results, including comparisons with these papers. This implies that the reviewer is not aware of these papers being discussed in the original paper, and is recommending that the authors consider them in their research.",1,,Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients 
PTnnAr9w8G,"In this work, the authors propose a pretraining diffusion model for defending against 3D adversarial point clouds. They also shows the limitation of robust training for defense due to gradient obfuscation. The results have shown better qualitative performance than SOTA methods. ","Pros: 
+) The idea is clear and easy to follow. They show that adversarial training does not work well on 3D point cloud recognition task. 
+) They deploy diffusion model for recovering/purifying 3D point clouds, which further increase the correct detection ability. 

Cons:
-) The methodology is straightford with limited technical contributions.
-) Diffusion model itself is a powerful generative model, capable of denoising. Limited novelty on technical contribution on purifying adversarial 3D point clouds. The authors are suggested to clarify their contributions. 
-) A lot of missing experiments, e.g. [1, 2]. It is suggested to clarify the advantage of the method with more experimental results. 
-) Missing evaluations on larger distortions, e.g. perturbation is 0.1. 
-) typo error: P3: Donget al.->Dong et al.

[1] Kaidong Li et al, Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients, CVPR22
[2] Hang Zhou et al, DUP-Net: Denoiser and upsampler network for 3d adversarial point clouds defense, ICCV19","+) The paper is well written.
+) The idea and experiments are clear.
+) The first diffusion model based 3D adversarial point cloud defense. ","The paper has shown potential of deploying diffusion model for 3D adversarial point cloud defense. The major problem is the lack of novely on designing their diffusion models, as well as experiments on the recent works. Therefore, it is difficult to justify their performance improvements.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,"Yes, Privacy, security and safety",5: marginally below the acceptance threshold,1666340558406,,293zPCqNqe,ICLR.cc/2023/Conference/Paper609/-/Official_Review,,[],1,,['everyone'],293zPCqNqe,['ICLR.cc/2023/Conference/Paper609/Reviewer_TFZv'],1666340558406,,1670925958118,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper609/Reviewer_TFZv']","Yes, the peer review suggests the authors of the paper to refer to specific literature that may not be already discussed in the original paper. The reviewer mentions two specific papers:

1. Kaidong Li et al, ""Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients"", CVPR22
2. Hang Zhou et al, ""DUP-Net: Denoiser and upsampler network for 3d adversarial point clouds defense"", ICCV19

The reviewer suggests that the authors should clarify the advantage of their method with more experimental results, including comparisons with these papers. This implies that the reviewer is not aware of these papers being discussed in the original paper, and is recommending that the authors consider them in their research.",1,,DUP-Net: Denoiser and upsampler network for 3d adversarial point clouds defense
YQb5bpPmek,"This paper is intended to formally introduce the online continual learning problem with progressive distribution shift, and also presented four benchmark datasets and 12 algorithms (adapted from existing algorithms) for this setting, and finally presented empirical results of these algorithms on these datasets. ","* The paper tries to be practical and bridge the gap between academic and industry, but I don’t really see anything novel. Missing citation: Cai, Sener, Koltun, “Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data”, ICCV 2021. 
* An important contribution is the benchmark datasets, and a 3-step procedure is described, but no examples are shown in the main text how these 3 steps are executed, and how the distribution is shifting with some metrics to demonstrate. I’m not very convinced these benchmark dataset are good. 
* Most of the results are presented in the format of scatter plot of “worst reg perf” vs  “avg online perf”, but it’s not well motivated why this is a good way of demonstrating the benchmarks are good.
","* Clarity: overall well written, but as a paper introducing benchmark datasets, the procedure on how the datasets are constructed is only briefly discussed, and the experiments part can be improved, for example, figure titles are not informative at all. 
* Quality: fair
* novelty: the novelty is limited, the problem is not not new, datasets are not new, algorithms are not new, missing citation of similar existing work  
* reproducibility: appears to be good 
","This paper attempts to establish some benchmark datasets and algorithms for online continual learning under progressive distribution shift. While a practitioners’ perspective is emphasized, I failed to see much novelty of this perspective, and the empirical results are not presented in a convincing enough way to show the benchmarks are good enough for wide use for the community. 

I would recommend the authors to check out this paper and explain the differences: Cai, Sener, Koltun, “Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data”, ICCV 2021. 
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666548193920,,23jfQBSUh4x,ICLR.cc/2023/Conference/Paper966/-/Official_Review,,[],2,,['everyone'],23jfQBSUh4x,['ICLR.cc/2023/Conference/Paper966/Reviewer_6758'],1666548193920,,1666548193920,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper966/Reviewer_6758']","Yes, this peer review suggests that the authors of the paper refer to a specific literature that is not already discussed in the original paper. The reviewer mentions the paper ""Cai, Sener, Koltun, “Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data”, ICCV 2021"" and notes that it is a ""missing citation"", implying that it is not already included in the original paper. The reviewer recommends that the authors check out this paper and explain the differences, suggesting that it is relevant to the topic and should be considered by the authors.",1,2021,Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data
UubSCn7T5V,"Authors introduce the novel OCL-PDS problem - Online Continual Learning for Progressive Distribution Shift. Authors contrast their problem with that of continual learning and domain adaptation. Authors build 4 new benchmarks and implement 12 algorithms to test these benchmarks.
","Strengths:
1) Paper is clearly written and the authors point to practical issues with the current approaches. 
2) Authors give very detailed justification of their choices, give details of experiments which will be helpful for other researchers to build on. 

Weaknesses: 
1) The proposed approach still has some practical issues (e.g. how does one decide the divergence threshold, how does one make sure that 3-step procedure benchmark OCL-PDS is applicable to real world problems? 
2) The final takeaway is not clear. Authors should discuss some of the real world problems and give some conclusions based on it. It is not clear what part of the discussion is helpful if a real world practitioner wants to use any of the knowledge in the paper. 
",Paper is clearly written. There is novelty but author's claim that their method is more practical is not justified. ,"
1) From Appendix B, one can understand why divergence should be asymmetric. But from the example given, it looks like one needs structured asymmetry. Is just having asymmetry enough? 

2) Figure 1: It is not clear what distribution shift we are talking about in ""FMoW-WPDS benchmark."" Are most images in 2002 of prison? Are most images in 2009 of Helipads? 

3) Section 2.3: Work is also very related to domain generalization (DG) which is a harder problem compared to domain adaptation (DA) [1]. If one wants to be truly ""online"" then one DG might be a better setting than DA? Authors should add more discussion comparing and contrasting with DG too. 

4) In general, real world datasets do not have shift continuity. Or there could be too much non-stationarity. How would ODD check or shift continuity check work in that case?

5) How does one decide threshold on Div (D_t || D_t+1)? In the real world dataset, this could be even harder to determine. 

6) I checked the hyperparameters considered In Table 14(in appendix). But the list of hyperparameters tuned does not look complete. The training data generation (domains, dist shift and threshold on div for continual shift) should be treated as hyperparameters too. Can authors comment more on this? 




[1] Blanchard, Gilles, Aniket Anand Deshmukh, Ürun Dogan, Gyemin Lee, and Clayton Scott. ""Domain generalization by marginal transfer learning."" The Journal of Machine Learning Research 22, no. 1 (2021): 46-100.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666930319327,,23jfQBSUh4x,ICLR.cc/2023/Conference/Paper966/-/Official_Review,,[],4,,['everyone'],23jfQBSUh4x,['ICLR.cc/2023/Conference/Paper966/Reviewer_FxrK'],1666930319327,,1666930319327,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper966/Reviewer_FxrK']","Yes, the peer review suggests that the authors of the paper refer to specific literature that is not already discussed in the original paper. Specifically, the reviewer mentions a paper by Blanchard et al. (2021) on ""Domain generalization by marginal transfer learning"" and suggests that the authors should discuss and compare their work with domain generalization (DG), which is considered a harder problem than domain adaptation (DA). This implies that the reviewer believes the authors should engage with this literature to strengthen their paper.",1,2021,Domain generalization by marginal transfer learning
jMYh1pBmZN,"The paper proposes an approach to provide a probabilistic forecast on top of any point forecast. Two post-processing approaches in particular are investigated, one fits a Gaussian noise by learning a condition heteroskedastic variance on top of an existing point forecast, the other fits a quantile-regression model. Experiments are conducted by applying this post-processing to an existing method (NHits) on 7 datasets against several baselines with an emphasis on long-term forecasting performance.","The main strength of the paper are that it is clearly written and has clear motivation since providing accurate probabilistic forecast on top of point forecast has many relevant applications.

The main weaknesses are as follow.

The approach is very related to conformal prediction, see [Romano 2019] for instance who proposes to fit a quantile regression model that can be guaranteed to be calibrated or [Hasson2021] who already showed the benefit of the approach by demonstrating how the top point forecast of M5 could be made a top competitor to the uncertainty track with a post-processing. Sadly, those approaches are not mentioned nor compared while being directly similar to the method proposed.

In addition, the method proposed does not distinguish itself clearly from just fitting a Gaussian/Quantile regression on top of an existing architecture. Indeed, the main improvement reported in the experiments seems to come from using an architecture better suited for long term predictions (NHiTS). However, one could also fit the same model with a Gaussian likelihood or a quantile regression as the loss function of NHiTS and likely obtain similar results in term of CRPS and calibration. The main difference of the approach proposed is the two step estimation but it is not compared to the simpler approach consisting on just fitting a distribution on top of an existing model and also have drawbacks that are not mentioned (two training must be done and latency will likely be twice higher).

To clearly justify their contribution, the authors should probably show that their methods outperforms both non conformal prediction approaches and fitting Gaussian/quantile regression approaches (the first one because it is state-of-the-art and the standard approach, the second one because it is arguably simpler and faster than the proposed method by the authors). 

[Hasson 2021] Probabilistic Forecasting: A Level-Set Approach. Hasson et al. Neurips 2021.
[Romano 2019] Conformalized Quantile Regression. Romano et al. Neurips 2019.

","The paper is very clearly written. The novelty is poor given the similarity to quantile conformal regression which is not mentioned in this work or even just fitting simple likelihood on top of existing models. The reproducibility is good, the author provided code that should allow one to rerun the benchmarks (which I did not check nor try to run).
","Given the lack of novelty and lack of comparison with relevant approaches (conformal quantile prediction, likelihood directly fitted on top of previous models, [Hasson2021]), I recommend rejecting the paper in its current state.

Additional details:
* p3: Salinas et al. (2020) does not consider only Gaussian noise model but also other likelihood (neg-binomial for instance)
* p4: ""To train this model, we use maximum likelihood in a two-step manner, first training the mean model ... using MSE loss, and then ... maximizing the likelihood of the Gaussian observation."" This seems to be the claimed novelty but a clear baseline would to fit both mean/variance or have a quantile regression on top of an existing architecture. This would have twice lower training runtime and latency and should probably be added in the comparisons (in addition to conformal prediction).
* Figure 7: this figure is hard to read since you are plotting samples for some methods and confidence intervals for others. It would be much better to report calibration to allow to quantify the calibration of the probabilistic predictions as you are doing for wind/nasdaq
* why reporting calibration only on wind/nasdaq on not on other datasets (electricity/traffic/etc)? I would recommend reporting calibration plots on all datasets, calibration errors would also be important to provide quantitative metrics
* Figure 9: small comment calibration are typically plotted with equal axis as it is then easier to assess the quality of the fit
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666016808092,,22h1XSEiN0,ICLR.cc/2023/Conference/Paper4522/-/Official_Review,,[],2,,['everyone'],22h1XSEiN0,['ICLR.cc/2023/Conference/Paper4522/Reviewer_RuZ4'],1666016808092,,1666016808092,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4522/Reviewer_RuZ4']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are not mentioned in the original paper:

1. [Romano 2019] - Conformalized Quantile Regression
2. [Hasson2021] - Probabilistic Forecasting: A Level-Set Approach

The reviewer suggests that the authors should compare their approach with these conformal prediction approaches and also with simpler approaches such as fitting a Gaussian/quantile regression on top of an existing architecture. This implies that the reviewer believes these references are relevant to the paper and should be discussed.",1,2019,Conformalized Quantile Regression 
jMYh1pBmZN,"The paper proposes an approach to provide a probabilistic forecast on top of any point forecast. Two post-processing approaches in particular are investigated, one fits a Gaussian noise by learning a condition heteroskedastic variance on top of an existing point forecast, the other fits a quantile-regression model. Experiments are conducted by applying this post-processing to an existing method (NHits) on 7 datasets against several baselines with an emphasis on long-term forecasting performance.","The main strength of the paper are that it is clearly written and has clear motivation since providing accurate probabilistic forecast on top of point forecast has many relevant applications.

The main weaknesses are as follow.

The approach is very related to conformal prediction, see [Romano 2019] for instance who proposes to fit a quantile regression model that can be guaranteed to be calibrated or [Hasson2021] who already showed the benefit of the approach by demonstrating how the top point forecast of M5 could be made a top competitor to the uncertainty track with a post-processing. Sadly, those approaches are not mentioned nor compared while being directly similar to the method proposed.

In addition, the method proposed does not distinguish itself clearly from just fitting a Gaussian/Quantile regression on top of an existing architecture. Indeed, the main improvement reported in the experiments seems to come from using an architecture better suited for long term predictions (NHiTS). However, one could also fit the same model with a Gaussian likelihood or a quantile regression as the loss function of NHiTS and likely obtain similar results in term of CRPS and calibration. The main difference of the approach proposed is the two step estimation but it is not compared to the simpler approach consisting on just fitting a distribution on top of an existing model and also have drawbacks that are not mentioned (two training must be done and latency will likely be twice higher).

To clearly justify their contribution, the authors should probably show that their methods outperforms both non conformal prediction approaches and fitting Gaussian/quantile regression approaches (the first one because it is state-of-the-art and the standard approach, the second one because it is arguably simpler and faster than the proposed method by the authors). 

[Hasson 2021] Probabilistic Forecasting: A Level-Set Approach. Hasson et al. Neurips 2021.
[Romano 2019] Conformalized Quantile Regression. Romano et al. Neurips 2019.

","The paper is very clearly written. The novelty is poor given the similarity to quantile conformal regression which is not mentioned in this work or even just fitting simple likelihood on top of existing models. The reproducibility is good, the author provided code that should allow one to rerun the benchmarks (which I did not check nor try to run).
","Given the lack of novelty and lack of comparison with relevant approaches (conformal quantile prediction, likelihood directly fitted on top of previous models, [Hasson2021]), I recommend rejecting the paper in its current state.

Additional details:
* p3: Salinas et al. (2020) does not consider only Gaussian noise model but also other likelihood (neg-binomial for instance)
* p4: ""To train this model, we use maximum likelihood in a two-step manner, first training the mean model ... using MSE loss, and then ... maximizing the likelihood of the Gaussian observation."" This seems to be the claimed novelty but a clear baseline would to fit both mean/variance or have a quantile regression on top of an existing architecture. This would have twice lower training runtime and latency and should probably be added in the comparisons (in addition to conformal prediction).
* Figure 7: this figure is hard to read since you are plotting samples for some methods and confidence intervals for others. It would be much better to report calibration to allow to quantify the calibration of the probabilistic predictions as you are doing for wind/nasdaq
* why reporting calibration only on wind/nasdaq on not on other datasets (electricity/traffic/etc)? I would recommend reporting calibration plots on all datasets, calibration errors would also be important to provide quantitative metrics
* Figure 9: small comment calibration are typically plotted with equal axis as it is then easier to assess the quality of the fit
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666016808092,,22h1XSEiN0,ICLR.cc/2023/Conference/Paper4522/-/Official_Review,,[],2,,['everyone'],22h1XSEiN0,['ICLR.cc/2023/Conference/Paper4522/Reviewer_RuZ4'],1666016808092,,1666016808092,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4522/Reviewer_RuZ4']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are not mentioned in the original paper:

1. [Romano 2019] - Conformalized Quantile Regression
2. [Hasson2021] - Probabilistic Forecasting: A Level-Set Approach

The reviewer suggests that the authors should compare their approach with these conformal prediction approaches and also with simpler approaches such as fitting a Gaussian/quantile regression on top of an existing architecture. This implies that the reviewer believes these references are relevant to the paper and should be discussed.",1,2019,Probabilistic Forecasting: A Level-Set Approach
ufzU_hTfqtO,"The paper proposes a way to transfer semantic information from Imagenet1K to train a 3D detector on point clouds to detect classes for which there are no 3D labels at all. 

The method trains a 2D detector (DETR) and a 3D detector (3DETR) in two stages. 
1) The 2D and 3D detector are co-trained, in line with Detic (Zhou 2022), with joint losses. The 3D labels (Sun/ScanNet) train the 3D detector, as well as 2D detector on 2D boxes derived from the 3D GT. The Imagenet labels train the DETR classifier head (applied on the largest detected box in the image). 
2) In Phase 2, an augmented dataset is created by running 3D detector to find ""unseen classes"". The 2D crops obtained from the detections are classifier by the 2D detector. Training continues also incorporating this augmented dataset, but adding contrastive training losses for the 2D crops, that take into account ImageNet classification results (crops with same class are considered positives). 
The process of generating the augmented dataset is iterated several times, taking more and more positive examples from the classifier. 

The method is evaluated on Sun and Scannet 3D datasets on a number of withheld classes. 



","Strengths: 
+ Seemingly novel use case of transferring 2D classification knowledge to a point cloud 3D detector. The choice of Imagenet1K seems suitable as a source of supervision, as the classes in question are present in Imagenet. 
+ The method is shown to ourperform several reasonable baselines on Sun and Scannet for a number of classes, in some cases by significant margins.  
+ Suitable ablations and studies of the effects of iterating the process are done. 

Potential weaknesses: 
- The start of related work section seems missing. It also claims ""to the best of our knowledge, there has been no work on open-set 3D object detection"". There is some related work that should be cited. For example Cen, J., Yun, P., Cai, J., Wang, M.Y., Liu, M.: Open-set 3d object detection. 3DV, 2021. Or Wong, K., Wang, S., Ren, M., Liang, M., Urtasun, R.: Identifying unknown instances for autonomous driving. In: CoRL. PMLR (2020). These are not quite the same, as they don't transfer class labels from a trained image classifier, however. 
- It is unclear how well the method does compared to full supervision. It would help to share how well DETR3 does on the classes when there is actually supervision. 
- Ablation shows that the first step of generating pseudolabels (mostly following the Detic paper) yields most of the gains. The application of ""class-unbiased"" contrastive training, or ""distance-aware"" temperature, which are technical details of some minor novelty, yield gains that seems quite small. 
- Sun RGB-D dataset contains also images, yet the transfer there is done purely on the 3D point cloud data. It would have been interesting to see how well we can do when also using the RGB data. ","The paper is very clear, the supplement contains additional results and code has been provided, so the work is reproducible. 

In terms of novelty, the idea to transfer ImageNet1K label knowledge to a 3D detector seems novel. The approach itself is mostly a combination of existing approaches (DETR, DETR3, Detic, Contrastive learning) but the combination makes sense. 

Some things could be made clearer: 
- Description of phases and related notation is a bit cumbersome. 
- A little more clarity on how you generate 3D boxes for unseen classes in phase 2 would be helpful. If you trained a 3D detector classifier head for the seen classes in scene1, do you just take any objects where classification is anything other than 'background'? 
- ""max-size proposal fmaxsize "" is not particularly clear without having the Detic paper context. 

There are a few minor language issues: 
“Real world owns a cornucopia of classes” 
“Or using 2D detection dataset” 
“Ote that” 

","The application is interesting and novel. The system is a combination of mostly known components, but they are put together reasonably well. Related work section could be improved. I think it's important to show the gap between the OS3-DETIC system and full supervision. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666793461290,,1yclzf1DWsf,ICLR.cc/2023/Conference/Paper5723/-/Official_Review,,[],3,,['everyone'],1yclzf1DWsf,['ICLR.cc/2023/Conference/Paper5723/Reviewer_fcKh'],1666793461290,,1666793461290,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5723/Reviewer_fcKh']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions two specific papers that are not cited in the original paper: 

1. Cen, J., Yun, P., Cai, J., Wang, M.Y., Liu, M.: Open-set 3d object detection. 3DV, 2021.
2. Wong, K., Wang, S., Ren, M., Liang, M., Urtasun, R.: Identifying unknown instances for autonomous driving. In: CoRL. PMLR (2020).

The reviewer states that these papers are related to open-set 3D object detection, but notes that they do not transfer class labels from a trained image classifier, which is a key aspect of the original paper. The reviewer suggests that these papers should be cited in the related work section to provide a more comprehensive overview of the field.",1,"2021, 2020",Open-set 3d object detection 
ufzU_hTfqtO,"The paper proposes a way to transfer semantic information from Imagenet1K to train a 3D detector on point clouds to detect classes for which there are no 3D labels at all. 

The method trains a 2D detector (DETR) and a 3D detector (3DETR) in two stages. 
1) The 2D and 3D detector are co-trained, in line with Detic (Zhou 2022), with joint losses. The 3D labels (Sun/ScanNet) train the 3D detector, as well as 2D detector on 2D boxes derived from the 3D GT. The Imagenet labels train the DETR classifier head (applied on the largest detected box in the image). 
2) In Phase 2, an augmented dataset is created by running 3D detector to find ""unseen classes"". The 2D crops obtained from the detections are classifier by the 2D detector. Training continues also incorporating this augmented dataset, but adding contrastive training losses for the 2D crops, that take into account ImageNet classification results (crops with same class are considered positives). 
The process of generating the augmented dataset is iterated several times, taking more and more positive examples from the classifier. 

The method is evaluated on Sun and Scannet 3D datasets on a number of withheld classes. 



","Strengths: 
+ Seemingly novel use case of transferring 2D classification knowledge to a point cloud 3D detector. The choice of Imagenet1K seems suitable as a source of supervision, as the classes in question are present in Imagenet. 
+ The method is shown to ourperform several reasonable baselines on Sun and Scannet for a number of classes, in some cases by significant margins.  
+ Suitable ablations and studies of the effects of iterating the process are done. 

Potential weaknesses: 
- The start of related work section seems missing. It also claims ""to the best of our knowledge, there has been no work on open-set 3D object detection"". There is some related work that should be cited. For example Cen, J., Yun, P., Cai, J., Wang, M.Y., Liu, M.: Open-set 3d object detection. 3DV, 2021. Or Wong, K., Wang, S., Ren, M., Liang, M., Urtasun, R.: Identifying unknown instances for autonomous driving. In: CoRL. PMLR (2020). These are not quite the same, as they don't transfer class labels from a trained image classifier, however. 
- It is unclear how well the method does compared to full supervision. It would help to share how well DETR3 does on the classes when there is actually supervision. 
- Ablation shows that the first step of generating pseudolabels (mostly following the Detic paper) yields most of the gains. The application of ""class-unbiased"" contrastive training, or ""distance-aware"" temperature, which are technical details of some minor novelty, yield gains that seems quite small. 
- Sun RGB-D dataset contains also images, yet the transfer there is done purely on the 3D point cloud data. It would have been interesting to see how well we can do when also using the RGB data. ","The paper is very clear, the supplement contains additional results and code has been provided, so the work is reproducible. 

In terms of novelty, the idea to transfer ImageNet1K label knowledge to a 3D detector seems novel. The approach itself is mostly a combination of existing approaches (DETR, DETR3, Detic, Contrastive learning) but the combination makes sense. 

Some things could be made clearer: 
- Description of phases and related notation is a bit cumbersome. 
- A little more clarity on how you generate 3D boxes for unseen classes in phase 2 would be helpful. If you trained a 3D detector classifier head for the seen classes in scene1, do you just take any objects where classification is anything other than 'background'? 
- ""max-size proposal fmaxsize "" is not particularly clear without having the Detic paper context. 

There are a few minor language issues: 
“Real world owns a cornucopia of classes” 
“Or using 2D detection dataset” 
“Ote that” 

","The application is interesting and novel. The system is a combination of mostly known components, but they are put together reasonably well. Related work section could be improved. I think it's important to show the gap between the OS3-DETIC system and full supervision. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666793461290,,1yclzf1DWsf,ICLR.cc/2023/Conference/Paper5723/-/Official_Review,,[],3,,['everyone'],1yclzf1DWsf,['ICLR.cc/2023/Conference/Paper5723/Reviewer_fcKh'],1666793461290,,1666793461290,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5723/Reviewer_fcKh']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions two specific papers that are not cited in the original paper: 

1. Cen, J., Yun, P., Cai, J., Wang, M.Y., Liu, M.: Open-set 3d object detection. 3DV, 2021.
2. Wong, K., Wang, S., Ren, M., Liang, M., Urtasun, R.: Identifying unknown instances for autonomous driving. In: CoRL. PMLR (2020).

The reviewer states that these papers are related to open-set 3D object detection, but notes that they do not transfer class labels from a trained image classifier, which is a key aspect of the original paper. The reviewer suggests that these papers should be cited in the related work section to provide a more comprehensive overview of the field.",1,"2021, 2020",Identifying unknown instances for autonomous driving
WK2GOZaggZ,"This paper considers the robustness of deep classification models, noting that out-of-the-box models can yield arbitrarily high-confidence predictions on out-of-distribution data. As a proposed remedy, “total activation classifiers” (TAC) are introduced to capture “class-dependent patterns” which are claimed to offer a reliable way to identify test data that do not conform to such patterns. Notably, the approach does not require access to out-of-distribution data at training time.

In more detail, TAC consists of a predictor mapping data onto the unit cube. The features are extracted from the neural network by taking “slices” from all intermediate activations which are then reduced (via summation) to 2-dimensional objects (e.g., spatial dimensions in convolutional architectures for images). The goal is to “make it so that groups of high-level features fire-up more strongly depending on the class of the input.” The sequence of all such slices is called the “activation profile” whose distance is measured to the classes. The class labels are represented as distinct binary codes; the set of binary codes is chosen in this way to maximise the “discriminability of its elements.” To train TAC, the binary-cross entropy loss is used to obtain class activation profiles which minimise the coordinate-wise distance to the corresponding class; variations on this loss are explored to improve training stability.

The overall TAC procedure can be applied to both “frozen” pre-trained models to obtain a secondary confidence score as well as trained “from scratch,” at a slight drop in accuracy (A.1). Thus, the recommended approach is to apply TAC to frozen models.","Overall, this paper tackles an important problem and the experimental evaluations on robustness to adversarial attacks, ability to abstain, and out-of-distribution detection seem fairly convincing overall. However, I have a few concerns about motivation for the proposed approach as well as some lacking discussion (and perhaps experiments) of related prior work.

The approach is not well-motivated. In the introduction, the question is posed, “can we enforce this property rather than hope it emerges?”, but the reader is left wondering what property is actually under discussion. It would be helpful to include further discussion of the property of interest to motivate why it’s sufficient to target this to obtain “models that know that they don’t know.” 

The use of binary codes is motivated by the need to have well-separated targets for optimization. However, in S2.4 it is stated that the codes are randomly selected with coordinates being independent binary random variables. Do the codes actually need to be binary? ","Although there are notable differences that make the proposed approach unique and interesting, there is more than a passing similarity to https://arxiv.org/pdf/2102.12967.pdf from ICLR 2022. This prior approach also involves extracting summary statistics from all layers of the neural network, and additionally has the benefit of proposing a principled statistical hypothesis testing framework for OOD detection.","Overall, this paper tackles an important problem and the experimental evaluations on robustness to adversarial attacks, ability to abstain, and out-of-distribution detection seem fairly convincing overall. However, I have a few concerns about motivation for the proposed approach and there is an important omission in the discussion of related work.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666549530677,,1w_Amtk67X,ICLR.cc/2023/Conference/Paper3259/-/Official_Review,,[],2,,['everyone'],1w_Amtk67X,['ICLR.cc/2023/Conference/Paper3259/Reviewer_eB98'],1666549530677,,1669005517556,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3259/Reviewer_eB98']","Yes, the peer review suggests that the authors of the paper refer to specific literature that is not already discussed in the original paper. The reviewer mentions a specific paper from ICLR 2022 (https://arxiv.org/pdf/2102.12967.pdf) that has a similar approach and proposes a principled statistical hypothesis testing framework for OOD detection, implying that the authors should be aware of and discuss this related work. This paper is not mentioned in the summary of the paper provided, suggesting that it is not already discussed in the original paper.",1,"2022, 2021",A Principled Statistical Hypothesis Testing Framework for OOD Detection
atevwlcCxcC,"The paper proposes a closed form update for offline RL. A few major assumptions and steps of derivations apply: (1) policy is Gaussian or mixture of Gaussians; (2) apply Taylor expansion to the optimization objective. The paper shows some theoretical properties of the proposed method, and empirical improvements over prior work baselines.","=== Strength ===

The paper provides an application of Taylor expansion based objective + behavior regularization in the offline RL setting, and has shown empirical improvements over baseline algorithms. This might motivate carrying out analytic updates (based on Taylor expansion derived objective) in future investigation on offline RL.

=== Weakness ===

The theoretical and conceptual contributions of the paper are a bit weak. Though the contribution of deriving analytic updates based on Taylor expansion + Gaussian policy assumption is technically solid, it is not clear why carrying out such updates should be useful in the offline RL setting in the first place. Is this due to statistical efficiency? Does it help stabilize the update? The theory front of the paper is weak in that much of the results come from technical derivation, or Thm 3.1 is a fairly straightforward application of PDL (admittedly, certain adaptations are needed since in this case the action is being expanded in the first order, but not the policy as in the original PDL case). ","=== Clarity ===

The paper is written pretty clearly.

=== Quality ===

The contribution of the paper is not strong enough, both in terms of novelty and theoretical statements. Otherwise, the paper provides some empirical evidence of the merits of the new method.

=== Novelty ===

The application of Taylor expansion based approach to offline RL is novel, but the method itself is not very novel in the general RL setting.

=== Reproduce ===

The results should arguably be reproducible because the source code is provided.","A few main questions.

=== **Taylor expansion based approach to RL** ===

Using Taylor expansion is not novel in the RL community in general. As the authors may have noted, PDL [1] itself arises from the application of Taylor expansion to the value function objective. Renowned algorithms such as TRPO and PPO have applied the results to trust region setting, where there is a regularization from the behavior policy (or any regularized policy chosen by the algo designers) [2]. Lately, Tang et al. [3] has developed second order Taylor expansion approach to a similar setup in online RL.

Admittedly, there is some difference between the setup in the current paper and aforementioned work, most notably, here the Taylor expansion is applied to the action directly (mainly because the policy is deterministic) whereas prior work mostly handles stochastic policy. In prior work, the expansion takes place in the policy distribution space; here, the expansion takes place in the action space. Such technical differences aside, I think it is worth discussing current work's relation with such prior work.

[1] Approximately optimal approximate RL, Kakade et al, 2007
[2] Proximal policy optimization, Schulman et al, 2015
[3] Taylor expansion policy optimization, Tang et al, 2020

=== **Why closed form update is useful in offline RL case** ===

Though experiments have showed performance gains, I think it helps the paper can be greatly improved to clarify more on the motivational question: why is using closed form update useful in the offline RL case. Intuitively, I think this might make sense because compared to usual SGD based updates, closed form update may reduce the stochastic jittering a bit and hence make the update more stable. Can we characterize the effect in theory? Or can we have some clean simple setup to illustrate this point? This question I think is quite fundamentally interesting overall but is a bit lacking in the current paper.

=== **Theory is a bit weak** ===

The flagship theoretical result in the paper is Thm 1, where they show an one-step improvement bound on the objective. On the technical side, I think showing the theory does require adaptations from the original PDL, due to the difference between expanding on the action space or policy space, but overall the adaptation is pretty straightforward. 

The main issue I have is that the connection between theory and practice is very weak. The efficiency of any offline RL algorithms over baseline methods, should be due to its statistical efficiency, i.e., given finite amount of offline RL data, can the algorithm achieve good performance. This setup underlies the challenge of offline RL where no new data can be collected. However, Thm 3.1 deals with the ""expectation"" behavior of the algorithm, i.e., how the algorithm behaves when there is infinite amount of data such that finite sample bounds turn into expected improvements. This is not very insightful because by studying the expected behavior of the algorithm, we bypass much of the challenge that underlies offline RL itself.

There should arguably, be comparison to sample complexity bound mentioned in [4]. If we obtain the finite sample bound version of Thm 3.1, does it improve over the bound derived in CQL? How do they compare? They have different dependencies on diff hyper-parameters on the env, what do they mean in practice?

[4] Conservative Q-learning for offline RL, Kumar et al, 2020

=== **Experiment** ===

The experiments show that the new algo obtains benefits in certain envs over the SOTA baselines, but understandably the improvements are not uniform and not always. This is expected because different algorithms behave differently under the same env, and we may not expect the same algo to outperform all other SOTA at the same time. I think what's valuable is to elucidate why the new algo performs better than the previous SOTA, e.g., IQL in M and ME dataset, but not as much in the MR dataset (in Table 1). Such a comparison would be valuable to understand relative strengths of diff methods.

As a side note, I think adding a ""total"" row in the table is not very informative, because aggregating statistics across all games does not help understand the performance much in this specific case.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666610621666,,1usJZBGNrZ,ICLR.cc/2023/Conference/Paper1188/-/Official_Review,,[],1,,['everyone'],1usJZBGNrZ,['ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5'],1666610621666,,1666610621666,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are relevant to the topic, including:

* ""Approximately optimal approximate RL"" by Kakade et al. (2007) [1]
* ""Proximal policy optimization"" by Schulman et al. (2015) [2]
* ""Taylor expansion policy optimization"" by Tang et al. (2020) [3]
* ""Conservative Q-learning for offline RL"" by Kumar et al. (2020) [4]

The reviewer suggests that the authors should discuss the relation of their work to these prior works, and also compare their theoretical results to the sample complexity bound mentioned in [4]. This implies that these references are not already included in the original paper, and the reviewer is suggesting that the authors should incorporate them into their discussion.",1,"2007, 2015, 2020, 2020",Approximately optimal approximate RL 
atevwlcCxcC,"The paper proposes a closed form update for offline RL. A few major assumptions and steps of derivations apply: (1) policy is Gaussian or mixture of Gaussians; (2) apply Taylor expansion to the optimization objective. The paper shows some theoretical properties of the proposed method, and empirical improvements over prior work baselines.","=== Strength ===

The paper provides an application of Taylor expansion based objective + behavior regularization in the offline RL setting, and has shown empirical improvements over baseline algorithms. This might motivate carrying out analytic updates (based on Taylor expansion derived objective) in future investigation on offline RL.

=== Weakness ===

The theoretical and conceptual contributions of the paper are a bit weak. Though the contribution of deriving analytic updates based on Taylor expansion + Gaussian policy assumption is technically solid, it is not clear why carrying out such updates should be useful in the offline RL setting in the first place. Is this due to statistical efficiency? Does it help stabilize the update? The theory front of the paper is weak in that much of the results come from technical derivation, or Thm 3.1 is a fairly straightforward application of PDL (admittedly, certain adaptations are needed since in this case the action is being expanded in the first order, but not the policy as in the original PDL case). ","=== Clarity ===

The paper is written pretty clearly.

=== Quality ===

The contribution of the paper is not strong enough, both in terms of novelty and theoretical statements. Otherwise, the paper provides some empirical evidence of the merits of the new method.

=== Novelty ===

The application of Taylor expansion based approach to offline RL is novel, but the method itself is not very novel in the general RL setting.

=== Reproduce ===

The results should arguably be reproducible because the source code is provided.","A few main questions.

=== **Taylor expansion based approach to RL** ===

Using Taylor expansion is not novel in the RL community in general. As the authors may have noted, PDL [1] itself arises from the application of Taylor expansion to the value function objective. Renowned algorithms such as TRPO and PPO have applied the results to trust region setting, where there is a regularization from the behavior policy (or any regularized policy chosen by the algo designers) [2]. Lately, Tang et al. [3] has developed second order Taylor expansion approach to a similar setup in online RL.

Admittedly, there is some difference between the setup in the current paper and aforementioned work, most notably, here the Taylor expansion is applied to the action directly (mainly because the policy is deterministic) whereas prior work mostly handles stochastic policy. In prior work, the expansion takes place in the policy distribution space; here, the expansion takes place in the action space. Such technical differences aside, I think it is worth discussing current work's relation with such prior work.

[1] Approximately optimal approximate RL, Kakade et al, 2007
[2] Proximal policy optimization, Schulman et al, 2015
[3] Taylor expansion policy optimization, Tang et al, 2020

=== **Why closed form update is useful in offline RL case** ===

Though experiments have showed performance gains, I think it helps the paper can be greatly improved to clarify more on the motivational question: why is using closed form update useful in the offline RL case. Intuitively, I think this might make sense because compared to usual SGD based updates, closed form update may reduce the stochastic jittering a bit and hence make the update more stable. Can we characterize the effect in theory? Or can we have some clean simple setup to illustrate this point? This question I think is quite fundamentally interesting overall but is a bit lacking in the current paper.

=== **Theory is a bit weak** ===

The flagship theoretical result in the paper is Thm 1, where they show an one-step improvement bound on the objective. On the technical side, I think showing the theory does require adaptations from the original PDL, due to the difference between expanding on the action space or policy space, but overall the adaptation is pretty straightforward. 

The main issue I have is that the connection between theory and practice is very weak. The efficiency of any offline RL algorithms over baseline methods, should be due to its statistical efficiency, i.e., given finite amount of offline RL data, can the algorithm achieve good performance. This setup underlies the challenge of offline RL where no new data can be collected. However, Thm 3.1 deals with the ""expectation"" behavior of the algorithm, i.e., how the algorithm behaves when there is infinite amount of data such that finite sample bounds turn into expected improvements. This is not very insightful because by studying the expected behavior of the algorithm, we bypass much of the challenge that underlies offline RL itself.

There should arguably, be comparison to sample complexity bound mentioned in [4]. If we obtain the finite sample bound version of Thm 3.1, does it improve over the bound derived in CQL? How do they compare? They have different dependencies on diff hyper-parameters on the env, what do they mean in practice?

[4] Conservative Q-learning for offline RL, Kumar et al, 2020

=== **Experiment** ===

The experiments show that the new algo obtains benefits in certain envs over the SOTA baselines, but understandably the improvements are not uniform and not always. This is expected because different algorithms behave differently under the same env, and we may not expect the same algo to outperform all other SOTA at the same time. I think what's valuable is to elucidate why the new algo performs better than the previous SOTA, e.g., IQL in M and ME dataset, but not as much in the MR dataset (in Table 1). Such a comparison would be valuable to understand relative strengths of diff methods.

As a side note, I think adding a ""total"" row in the table is not very informative, because aggregating statistics across all games does not help understand the performance much in this specific case.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666610621666,,1usJZBGNrZ,ICLR.cc/2023/Conference/Paper1188/-/Official_Review,,[],1,,['everyone'],1usJZBGNrZ,['ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5'],1666610621666,,1666610621666,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are relevant to the topic, including:

* ""Approximately optimal approximate RL"" by Kakade et al. (2007) [1]
* ""Proximal policy optimization"" by Schulman et al. (2015) [2]
* ""Taylor expansion policy optimization"" by Tang et al. (2020) [3]
* ""Conservative Q-learning for offline RL"" by Kumar et al. (2020) [4]

The reviewer suggests that the authors should discuss the relation of their work to these prior works, and also compare their theoretical results to the sample complexity bound mentioned in [4]. This implies that these references are not already included in the original paper, and the reviewer is suggesting that the authors should incorporate them into their discussion.",1,"2007, 2015, 2020, 2020",Proximal policy optimization 
atevwlcCxcC,"The paper proposes a closed form update for offline RL. A few major assumptions and steps of derivations apply: (1) policy is Gaussian or mixture of Gaussians; (2) apply Taylor expansion to the optimization objective. The paper shows some theoretical properties of the proposed method, and empirical improvements over prior work baselines.","=== Strength ===

The paper provides an application of Taylor expansion based objective + behavior regularization in the offline RL setting, and has shown empirical improvements over baseline algorithms. This might motivate carrying out analytic updates (based on Taylor expansion derived objective) in future investigation on offline RL.

=== Weakness ===

The theoretical and conceptual contributions of the paper are a bit weak. Though the contribution of deriving analytic updates based on Taylor expansion + Gaussian policy assumption is technically solid, it is not clear why carrying out such updates should be useful in the offline RL setting in the first place. Is this due to statistical efficiency? Does it help stabilize the update? The theory front of the paper is weak in that much of the results come from technical derivation, or Thm 3.1 is a fairly straightforward application of PDL (admittedly, certain adaptations are needed since in this case the action is being expanded in the first order, but not the policy as in the original PDL case). ","=== Clarity ===

The paper is written pretty clearly.

=== Quality ===

The contribution of the paper is not strong enough, both in terms of novelty and theoretical statements. Otherwise, the paper provides some empirical evidence of the merits of the new method.

=== Novelty ===

The application of Taylor expansion based approach to offline RL is novel, but the method itself is not very novel in the general RL setting.

=== Reproduce ===

The results should arguably be reproducible because the source code is provided.","A few main questions.

=== **Taylor expansion based approach to RL** ===

Using Taylor expansion is not novel in the RL community in general. As the authors may have noted, PDL [1] itself arises from the application of Taylor expansion to the value function objective. Renowned algorithms such as TRPO and PPO have applied the results to trust region setting, where there is a regularization from the behavior policy (or any regularized policy chosen by the algo designers) [2]. Lately, Tang et al. [3] has developed second order Taylor expansion approach to a similar setup in online RL.

Admittedly, there is some difference between the setup in the current paper and aforementioned work, most notably, here the Taylor expansion is applied to the action directly (mainly because the policy is deterministic) whereas prior work mostly handles stochastic policy. In prior work, the expansion takes place in the policy distribution space; here, the expansion takes place in the action space. Such technical differences aside, I think it is worth discussing current work's relation with such prior work.

[1] Approximately optimal approximate RL, Kakade et al, 2007
[2] Proximal policy optimization, Schulman et al, 2015
[3] Taylor expansion policy optimization, Tang et al, 2020

=== **Why closed form update is useful in offline RL case** ===

Though experiments have showed performance gains, I think it helps the paper can be greatly improved to clarify more on the motivational question: why is using closed form update useful in the offline RL case. Intuitively, I think this might make sense because compared to usual SGD based updates, closed form update may reduce the stochastic jittering a bit and hence make the update more stable. Can we characterize the effect in theory? Or can we have some clean simple setup to illustrate this point? This question I think is quite fundamentally interesting overall but is a bit lacking in the current paper.

=== **Theory is a bit weak** ===

The flagship theoretical result in the paper is Thm 1, where they show an one-step improvement bound on the objective. On the technical side, I think showing the theory does require adaptations from the original PDL, due to the difference between expanding on the action space or policy space, but overall the adaptation is pretty straightforward. 

The main issue I have is that the connection between theory and practice is very weak. The efficiency of any offline RL algorithms over baseline methods, should be due to its statistical efficiency, i.e., given finite amount of offline RL data, can the algorithm achieve good performance. This setup underlies the challenge of offline RL where no new data can be collected. However, Thm 3.1 deals with the ""expectation"" behavior of the algorithm, i.e., how the algorithm behaves when there is infinite amount of data such that finite sample bounds turn into expected improvements. This is not very insightful because by studying the expected behavior of the algorithm, we bypass much of the challenge that underlies offline RL itself.

There should arguably, be comparison to sample complexity bound mentioned in [4]. If we obtain the finite sample bound version of Thm 3.1, does it improve over the bound derived in CQL? How do they compare? They have different dependencies on diff hyper-parameters on the env, what do they mean in practice?

[4] Conservative Q-learning for offline RL, Kumar et al, 2020

=== **Experiment** ===

The experiments show that the new algo obtains benefits in certain envs over the SOTA baselines, but understandably the improvements are not uniform and not always. This is expected because different algorithms behave differently under the same env, and we may not expect the same algo to outperform all other SOTA at the same time. I think what's valuable is to elucidate why the new algo performs better than the previous SOTA, e.g., IQL in M and ME dataset, but not as much in the MR dataset (in Table 1). Such a comparison would be valuable to understand relative strengths of diff methods.

As a side note, I think adding a ""total"" row in the table is not very informative, because aggregating statistics across all games does not help understand the performance much in this specific case.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666610621666,,1usJZBGNrZ,ICLR.cc/2023/Conference/Paper1188/-/Official_Review,,[],1,,['everyone'],1usJZBGNrZ,['ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5'],1666610621666,,1666610621666,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are relevant to the topic, including:

* ""Approximately optimal approximate RL"" by Kakade et al. (2007) [1]
* ""Proximal policy optimization"" by Schulman et al. (2015) [2]
* ""Taylor expansion policy optimization"" by Tang et al. (2020) [3]
* ""Conservative Q-learning for offline RL"" by Kumar et al. (2020) [4]

The reviewer suggests that the authors should discuss the relation of their work to these prior works, and also compare their theoretical results to the sample complexity bound mentioned in [4]. This implies that these references are not already included in the original paper, and the reviewer is suggesting that the authors should incorporate them into their discussion.",1,"2007, 2015, 2020, 2020",Taylor expansion policy optimization 
atevwlcCxcC,"The paper proposes a closed form update for offline RL. A few major assumptions and steps of derivations apply: (1) policy is Gaussian or mixture of Gaussians; (2) apply Taylor expansion to the optimization objective. The paper shows some theoretical properties of the proposed method, and empirical improvements over prior work baselines.","=== Strength ===

The paper provides an application of Taylor expansion based objective + behavior regularization in the offline RL setting, and has shown empirical improvements over baseline algorithms. This might motivate carrying out analytic updates (based on Taylor expansion derived objective) in future investigation on offline RL.

=== Weakness ===

The theoretical and conceptual contributions of the paper are a bit weak. Though the contribution of deriving analytic updates based on Taylor expansion + Gaussian policy assumption is technically solid, it is not clear why carrying out such updates should be useful in the offline RL setting in the first place. Is this due to statistical efficiency? Does it help stabilize the update? The theory front of the paper is weak in that much of the results come from technical derivation, or Thm 3.1 is a fairly straightforward application of PDL (admittedly, certain adaptations are needed since in this case the action is being expanded in the first order, but not the policy as in the original PDL case). ","=== Clarity ===

The paper is written pretty clearly.

=== Quality ===

The contribution of the paper is not strong enough, both in terms of novelty and theoretical statements. Otherwise, the paper provides some empirical evidence of the merits of the new method.

=== Novelty ===

The application of Taylor expansion based approach to offline RL is novel, but the method itself is not very novel in the general RL setting.

=== Reproduce ===

The results should arguably be reproducible because the source code is provided.","A few main questions.

=== **Taylor expansion based approach to RL** ===

Using Taylor expansion is not novel in the RL community in general. As the authors may have noted, PDL [1] itself arises from the application of Taylor expansion to the value function objective. Renowned algorithms such as TRPO and PPO have applied the results to trust region setting, where there is a regularization from the behavior policy (or any regularized policy chosen by the algo designers) [2]. Lately, Tang et al. [3] has developed second order Taylor expansion approach to a similar setup in online RL.

Admittedly, there is some difference between the setup in the current paper and aforementioned work, most notably, here the Taylor expansion is applied to the action directly (mainly because the policy is deterministic) whereas prior work mostly handles stochastic policy. In prior work, the expansion takes place in the policy distribution space; here, the expansion takes place in the action space. Such technical differences aside, I think it is worth discussing current work's relation with such prior work.

[1] Approximately optimal approximate RL, Kakade et al, 2007
[2] Proximal policy optimization, Schulman et al, 2015
[3] Taylor expansion policy optimization, Tang et al, 2020

=== **Why closed form update is useful in offline RL case** ===

Though experiments have showed performance gains, I think it helps the paper can be greatly improved to clarify more on the motivational question: why is using closed form update useful in the offline RL case. Intuitively, I think this might make sense because compared to usual SGD based updates, closed form update may reduce the stochastic jittering a bit and hence make the update more stable. Can we characterize the effect in theory? Or can we have some clean simple setup to illustrate this point? This question I think is quite fundamentally interesting overall but is a bit lacking in the current paper.

=== **Theory is a bit weak** ===

The flagship theoretical result in the paper is Thm 1, where they show an one-step improvement bound on the objective. On the technical side, I think showing the theory does require adaptations from the original PDL, due to the difference between expanding on the action space or policy space, but overall the adaptation is pretty straightforward. 

The main issue I have is that the connection between theory and practice is very weak. The efficiency of any offline RL algorithms over baseline methods, should be due to its statistical efficiency, i.e., given finite amount of offline RL data, can the algorithm achieve good performance. This setup underlies the challenge of offline RL where no new data can be collected. However, Thm 3.1 deals with the ""expectation"" behavior of the algorithm, i.e., how the algorithm behaves when there is infinite amount of data such that finite sample bounds turn into expected improvements. This is not very insightful because by studying the expected behavior of the algorithm, we bypass much of the challenge that underlies offline RL itself.

There should arguably, be comparison to sample complexity bound mentioned in [4]. If we obtain the finite sample bound version of Thm 3.1, does it improve over the bound derived in CQL? How do they compare? They have different dependencies on diff hyper-parameters on the env, what do they mean in practice?

[4] Conservative Q-learning for offline RL, Kumar et al, 2020

=== **Experiment** ===

The experiments show that the new algo obtains benefits in certain envs over the SOTA baselines, but understandably the improvements are not uniform and not always. This is expected because different algorithms behave differently under the same env, and we may not expect the same algo to outperform all other SOTA at the same time. I think what's valuable is to elucidate why the new algo performs better than the previous SOTA, e.g., IQL in M and ME dataset, but not as much in the MR dataset (in Table 1). Such a comparison would be valuable to understand relative strengths of diff methods.

As a side note, I think adding a ""total"" row in the table is not very informative, because aggregating statistics across all games does not help understand the performance much in this specific case.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666610621666,,1usJZBGNrZ,ICLR.cc/2023/Conference/Paper1188/-/Official_Review,,[],1,,['everyone'],1usJZBGNrZ,['ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5'],1666610621666,,1666610621666,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are relevant to the topic, including:

* ""Approximately optimal approximate RL"" by Kakade et al. (2007) [1]
* ""Proximal policy optimization"" by Schulman et al. (2015) [2]
* ""Taylor expansion policy optimization"" by Tang et al. (2020) [3]
* ""Conservative Q-learning for offline RL"" by Kumar et al. (2020) [4]

The reviewer suggests that the authors should discuss the relation of their work to these prior works, and also compare their theoretical results to the sample complexity bound mentioned in [4]. This implies that these references are not already included in the original paper, and the reviewer is suggesting that the authors should incorporate them into their discussion.",1,"2007, 2015, 2020, 2020",Conservative Q-learning for offline RL
Pdkvn3F3Le,"The paper introduces a new posterior sampling algorithm (RWPSP) for the horizon-free finite state tabular MDP setting in RL. The algorithm is part of the posterior sampling family of approaches (e.g. a la Thompson sampling), but differentiates itself from past results by:
- not attempting to sample an estimate the transition model and optimize a policy out of that
- but instead building directly a posterior on policies based on an estimate transition model, and then sample a policy out of that

This different approach results in a different regret bound of O(\Gamma T^1/2 S^-2) which the authors show is always smaller or equal than the current best bounds of O(D(SAT)^1/2).

Finally, preliminary results on toy MDPs are provided to highlight the improved performance in terms of regret of the proposed method.","The main strength of the method are matching (and potentially improving) the current best rates for this setting, the novelty of the approach to the posterior sampling family, and the seemingly complete lack of propabilistic statements in the regret proof.

The weaknesses are:
- The claim of optimality of O(D(SAT)^1/2) regret only holds for posterior sampling approaches (the paper ""Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function"" Zihan Zhang, Xiangyang Ji, NeurIPS2019 seems to report a O((DSAT)^1/2) bound under comparable assumptions). I am not sure if since 2019 no other posterior sampling approach also matched this bound.
- Could the authors clarify the proof of lemma 5.8? From Lemma A.6 in the appendix we have equation (33) proving \Gamma \leq D by application of Assumption 3.2. At this point further bounding D with DS^2(SA)^1/2 seems pointless. A finite MDP cannot have less than 1 state, so a direct application of \Gamma \leq D would yield a O(DT^1/2/S^2) regret, much better than existing results.
- Lemma A.6 seems to rely on Assumption 3.2 heavily. However I do not see why it would be natural to assume that the optimal policy would reduce the diameter between *any two states* in the MDP compared to any of the policies encountered during optimization. In particular the MDP reward (and therefore the optimal policy) does not usually care about arbitrary transition times, but only about transitioning to a good state as quickly as possible. On the other hand a sub-optimal policy might be well aligned (due to sheer randomness) with the explorative goal of tansversing the whole space as efficiently as possible.
- Assumption 3.2 does not define what would the max be over.
- I am not fully familiar with the setting, but the complete lack of any probabilistic statement in any of the results (i.e. none of the theorem hold only with some probability, and the regret bounds seem to rely only on the assumptions). The only statistical comment is a brief reference in section 5.2. This seems very unlikely to be rigorous considering posterior sampling is a randomized algorithm. If this was indeed the case it would be a significant aspect of the contribution and should be highlighted in the text.
- The exposition of the whole paper is well structured, but clarity falls apart at a more careful inspection. Most of the maxes are missing the variable that are being maxed on. Section 3.1 contains broken phrases, including crucially the one that introduces \theta. p_\theta is never introduced. the span is mentioned once and never again. Lemma 5.3 uses a different notation in the main paper and in the appendix. ","The paper seems well structured at a high level, but once I started looking into the details it is severly lacking. I made a fair attempt at understanding the proof, but due to incomplete notation and very barebones derivations in some places I would not be confident in its meaningfulness.

The quality of the paper is negatively impacted by the lack of clarity.

Similar, if there are novel contributions beyond the algorithmic one they should be appropriately highlighted in the proof. Which part of the derivation is made possible by sampling directly posterior policies rather than full transition models?

The reproducibility of the theoretical part is not very good due to lack of clarity. The experimental part is not reproducible, as the authors do not report any of the relevant hyperparameters to run RWPSP, nor include code.

 ","Overall I feel the paper introduces a nice idea (side-stepping the problem of estimating and sampling the transition model), but I could not understand from the theoretical derivation how this change unlocks new solutions in the regret analysis. Due to this lack of clarity and confusion in the soundness of the result I would not reccomend acceptance for the paper in its current state.

However, despite having a background in theoretical RL, I also recognize that the field is extremely specialized and I might be missing necessary context and conventions given for granted in the proof derivation, and remain open to re-evaluate my position if the authors can clarify their work.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667521930323,,1uPo_IrEp8,ICLR.cc/2023/Conference/Paper3796/-/Official_Review,,[],4,,['everyone'],1uPo_IrEp8,['ICLR.cc/2023/Conference/Paper3796/Reviewer_j9Gs'],1667521930323,,1667521930323,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3796/Reviewer_j9Gs']","Yes, the peer review suggests the authors of the paper to refer to a specific literature that is not already discussed in the original paper. The reviewer mentions the paper ""Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function"" by Zihan Zhang, Xiangyang Ji, NeurIPS2019, which reports a O((DSAT)^1/2) bound under comparable assumptions. This paper is not mentioned in the original summary of the paper, suggesting that the reviewer is introducing new literature that the authors may not be aware of.",1,,Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function
u2COOjvug1,"This paper studies safe reinforcement learning (RL) where safety violation must be bounded during training. They propose a new approach that balances the trade-off between efficient progress in exploration and safety guarantee. Specifically, the proposed approach updates Dirichlet-Categorical models of the state transition probabilities that describes the agent's behavior within the environment via Bayesian inference. They then approximate the agent's belief in terms of risks. They provide theoretical guarantees on the convergence on as well as empirically showing the performance of the proposed approach.","### Strength
**Problem Settings.** The problem settings are important and interesting.

**Theoretical Analysis** I think Theorem 3.1 is indeed a nice theorem though it is not clear to me how novel this theorem is.

### Weakness
**Paper writing.** First of all, the presentation of this paper should be improved in terms of notations. For example, $P$ or $Q$ are used in two meanings
+ P: Transition probability (in Section 2) and lowest risk level (in Section 3.3)
+ Q: finite set of states (in Section 2) and Q-function (in Section 3.4).

Such inconsistency makes this paper quite hard to follow. Also, the notations are far from standard as an RL paper, which gives the reader unnecessary and unessential burden. As an instance, I personally want the authors to avoid using $Q$ in other meanings than Q-function. State space is typically denoted as $S$ or $X$ in RL papers.

**Related work.** It would be better to write related work section in more organized manner. The current related work section is just a list of relevant papers, and it is hard to follow the story of the related work.

**Missing citations.** As a particularly relevant papers, I would like to list the following paper. I think it may be better to compare the authors' method with this existing work (the source code is released).
- As, Yarden, et al. ""Constrained Policy Optimization via Bayesian World Models."" International Conference on Learning Representations. 2021.

**Experiments.**  I don't think the experiment has been fully conducted. The benchmark task is very simple (i.e., grid world and PacMan), and the baseline methods are vanilla Q-learning. There are a lot of existing safe RL algorithms or benchmark tasks, so I think the authors should have compared with recent ones. For example, representatives of the benchmark tasks for safe RL is Safety-Gym or MuJoCo, and the notable safe RL baselines are PPO-Lagrangian (Here are just examples). At least, I do not think that the authors' claims are supported by the current experiments.

### Minor comments
- Since Moldovan and Abbeel (2012) is on safe exploration without ergodicity assumption, position of its citation seems weird to me. It would be more reasonable to cite it in the end of the next sentence.","**Clarity.** This paper is hard to follow due to inconsistency of mathematical notations. Also, related work section should be rewritten in an organized manner.

**Quality.** The quality of the theoretical analysis seems ok to me (though I am not fully confident), but that of experiments should be improved in terms of the selection of both benchmark problems and baselines.

**Novelty.** As far as I know, the proposed method is new. However, compared to recent work, I am not convinced whether the proposed method is better than existing ones. I don't think this paper has not supported the advantages of the proposed method neither empirically nor theoretically

**Reproducibility.** The source code is not attached and experimental conditions are not fully written in the paper (including appendix), I need to say that the reproducibility is low for now.","Though the problem setting and proposed method is interesting, I have several concerns as I listed in the Weakness. Hence, I recommend rejection for now.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666340586294,,1tfGKiwnJRJ,ICLR.cc/2023/Conference/Paper2952/-/Official_Review,,[],1,,['everyone'],1tfGKiwnJRJ,['ICLR.cc/2023/Conference/Paper2952/Reviewer_BCMN'],1666340586294,,1666574983569,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2952/Reviewer_BCMN']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the paper ""Constrained Policy Optimization via Bayesian World Models"" by As, Yarden, et al. (2021) as a relevant work that the authors should compare their method with, and also mentions other benchmark tasks and baseline methods such as Safety-Gym, MuJoCo, and PPO-Lagrangian that are not mentioned in the original paper.",1,2021,Constrained Policy Optimization via Bayesian World Models
klAhycf8G0H,"This paper presents a neural image compression models based on lattice vector quantization (LVQ).

The authors note two deficiencies in existing work: (1) most models use scalar quantization (SQ), which is typically less powerful than vector quantization (VQ) if a large enough codebook is used, and (2) models that do use VQ do not use a hyperprior or end-to-end optimization. The authors address these problems by using LVQ, which has a large, though constrained, codebook, and they show how to jointly optimize a distribution over the lattice (the entropy model) with a hyperprior and spatially autoregressive model.

Two other important contributions are the evaluation of predicting a full covariance matrix for each LVQ group, rather than a diagonal covariance matrix, and using Monte Carlo integration to more quickly calculate (an approximation of) the likelihood of each feature vector.

The result is, as far as I know, state of the art rate-distortion (RD) performance for lossy image compression. The evaluation shows an 18% rate savings over VTM, one of the best standard (hand-engineered) image codecs, as well as gains over other learning-based (neural) codecs (Fig 5 and Table 2). The paper also compares different lattice dimensionalities and the effect of predicting a full covariance matrix (Fig 4 and Table 1).
","The primary strength of this paper is the achieving SOTA rate-distortion performance. As far as I know, now other papers or standard codecs achieve as good a compression rate when using PSNR as the quality metric.

The primary weakness of the paper is a lack of runtime data. I suspect this method is extremely slow since spatially AR context models are known to be slow due to the difficulty of parallelizing the decoding process, and the prediction of a full covariance matrix leads to a large number of output parameters (if I understand the setup correctly, 192 channels using \Lambda_24 leads to 8 groups of 24D vectors and thus 8 * (24^2 + 24) = 4800 parameters per spatial location in the latent image representation). I suspect this leads to decode times of multiple seconds, even on fast hardware, when a deployable model needs to decode kodak-size images in a handful of milliseconds. To the best of my knowledge, no neural methods achieve decode speeds on par with standard codecs like VTM (or BPG or JPEG, etc.), but other models are probably two orders of magnitude faster than the method in this paper while only losing 5-10% in terms of rate.

The other weakness is a lack of direct comparison against stronger neural methods. Balle 2018, Minnen 2018, and Cheng 2020 are all relatively old methods. In particular, part of the motivation of this paper is the lack of modeling the dependencies between channels (SQ doesn't do this, VQ does). But existing papers adopt other methods for (partially) modeling these dependencies, for example:

Channel-wise Autoregressive Entropy Models for Learned Image Compression
David Minnen, Saurabh Singh
(ICIP 2020) https://arxiv.org/abs/2007.08739

ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding
Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, Yan Wang
(CVPR 2022) https://arxiv.org/abs/2203.10886

Transformer-based Transform Coding
Yinhao Zhu, Yang Yang, Taco Cohen
(ICLR 2022) https://openreview.net/forum?id=IDwN6xjHnK8

The channel-wise AR approach used by these papers should be mentioned and compared to the LVQ approach adopted here since they both address the same underlying deficiency. I believe the RD performance of the LVQ approach is better than what is reported in these papers. However, the runtime is vastly different (ELIC in particular is focused on balancing RD performance with runtime), and the preferable trade-off probably falls to ELIC over the proposed LVQ method, i.e. I wouldn't accept 10-100x slower decoding to save 5-10% on file sizes.

Finally, I'd like to better understand how much of the RD performance is due to the LVQ vs. the full covariance matrix. Fig 4 and Table 1 partially answer the question by showing that for several lattice dimensionalities, predicting a full covariance matrix helps (as expected). Another interesting comparison would be to use a trivial lattice (hyper-cubes) but still group the channels into 24D vectors and predict a full covariance matrix. I expect \Lambda_24 is better (the underlying theory predicts this for mse) but the size of the gap would make it clear whether or not the added complexity of \Lambda_24 is warranted.
","The overall quality of the paper is quite good.

Novelty is medium. While the results are SOTA and I agree with the authors' claim that previous work has not used LVQ with a hyperprior and end-to-end optimization, all of the components do already exist.

Reproducibility seems fairly difficult so releasing code to set up the lattice and approximate the likelihoods would help significantly (the other components already exist in opensource projects like TensorFlow-Compression and CompressAI).
","As far as I know, this paper presents state-of-the-art rate-distortion results for lossy image compression. That's a significant achievement given the existing work on this problem.

That said, the improvement in rate-savings is only moderate compared to the best existing methods (see references above), and the other methods appear to support much faster decode speeds, which is crucial for deployment.

So my recommendation is ""5: marginally below the acceptance threshold"" since it's not entirely clear how much of the performance gain comes from LVQ vs. predicting a full covariance matrix within each VQ group vs. the spatial autoregressive model.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666642281286,,1pGmKJvneD7,ICLR.cc/2023/Conference/Paper3340/-/Official_Review,,[],4,,['everyone'],1pGmKJvneD7,['ICLR.cc/2023/Conference/Paper3340/Reviewer_SVp9'],1666642281286,,1666642281286,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3340/Reviewer_SVp9']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. Specifically, the reviewer mentions the following papers that are not mentioned in the original paper:

1. ""Channel-wise Autoregressive Entropy Models for Learned Image Compression"" by David Minnen and Saurabh Singh (ICIP 2020)
2. ""ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding"" by Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang (CVPR 2022)
3. ""Transformer-based Transform Coding"" by Yinhao Zhu, Yang Yang, and Taco Cohen (ICLR 2022)

The reviewer suggests that the authors should compare their work with these papers, which also address the issue of modeling dependencies between channels, and discuss the trade-offs between rate-distortion performance and decoding speed.",1,"2020, 2022, 2022",Channel-wise Autoregressive Entropy Models for Learned Image Compression
klAhycf8G0H,"This paper presents a neural image compression models based on lattice vector quantization (LVQ).

The authors note two deficiencies in existing work: (1) most models use scalar quantization (SQ), which is typically less powerful than vector quantization (VQ) if a large enough codebook is used, and (2) models that do use VQ do not use a hyperprior or end-to-end optimization. The authors address these problems by using LVQ, which has a large, though constrained, codebook, and they show how to jointly optimize a distribution over the lattice (the entropy model) with a hyperprior and spatially autoregressive model.

Two other important contributions are the evaluation of predicting a full covariance matrix for each LVQ group, rather than a diagonal covariance matrix, and using Monte Carlo integration to more quickly calculate (an approximation of) the likelihood of each feature vector.

The result is, as far as I know, state of the art rate-distortion (RD) performance for lossy image compression. The evaluation shows an 18% rate savings over VTM, one of the best standard (hand-engineered) image codecs, as well as gains over other learning-based (neural) codecs (Fig 5 and Table 2). The paper also compares different lattice dimensionalities and the effect of predicting a full covariance matrix (Fig 4 and Table 1).
","The primary strength of this paper is the achieving SOTA rate-distortion performance. As far as I know, now other papers or standard codecs achieve as good a compression rate when using PSNR as the quality metric.

The primary weakness of the paper is a lack of runtime data. I suspect this method is extremely slow since spatially AR context models are known to be slow due to the difficulty of parallelizing the decoding process, and the prediction of a full covariance matrix leads to a large number of output parameters (if I understand the setup correctly, 192 channels using \Lambda_24 leads to 8 groups of 24D vectors and thus 8 * (24^2 + 24) = 4800 parameters per spatial location in the latent image representation). I suspect this leads to decode times of multiple seconds, even on fast hardware, when a deployable model needs to decode kodak-size images in a handful of milliseconds. To the best of my knowledge, no neural methods achieve decode speeds on par with standard codecs like VTM (or BPG or JPEG, etc.), but other models are probably two orders of magnitude faster than the method in this paper while only losing 5-10% in terms of rate.

The other weakness is a lack of direct comparison against stronger neural methods. Balle 2018, Minnen 2018, and Cheng 2020 are all relatively old methods. In particular, part of the motivation of this paper is the lack of modeling the dependencies between channels (SQ doesn't do this, VQ does). But existing papers adopt other methods for (partially) modeling these dependencies, for example:

Channel-wise Autoregressive Entropy Models for Learned Image Compression
David Minnen, Saurabh Singh
(ICIP 2020) https://arxiv.org/abs/2007.08739

ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding
Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, Yan Wang
(CVPR 2022) https://arxiv.org/abs/2203.10886

Transformer-based Transform Coding
Yinhao Zhu, Yang Yang, Taco Cohen
(ICLR 2022) https://openreview.net/forum?id=IDwN6xjHnK8

The channel-wise AR approach used by these papers should be mentioned and compared to the LVQ approach adopted here since they both address the same underlying deficiency. I believe the RD performance of the LVQ approach is better than what is reported in these papers. However, the runtime is vastly different (ELIC in particular is focused on balancing RD performance with runtime), and the preferable trade-off probably falls to ELIC over the proposed LVQ method, i.e. I wouldn't accept 10-100x slower decoding to save 5-10% on file sizes.

Finally, I'd like to better understand how much of the RD performance is due to the LVQ vs. the full covariance matrix. Fig 4 and Table 1 partially answer the question by showing that for several lattice dimensionalities, predicting a full covariance matrix helps (as expected). Another interesting comparison would be to use a trivial lattice (hyper-cubes) but still group the channels into 24D vectors and predict a full covariance matrix. I expect \Lambda_24 is better (the underlying theory predicts this for mse) but the size of the gap would make it clear whether or not the added complexity of \Lambda_24 is warranted.
","The overall quality of the paper is quite good.

Novelty is medium. While the results are SOTA and I agree with the authors' claim that previous work has not used LVQ with a hyperprior and end-to-end optimization, all of the components do already exist.

Reproducibility seems fairly difficult so releasing code to set up the lattice and approximate the likelihoods would help significantly (the other components already exist in opensource projects like TensorFlow-Compression and CompressAI).
","As far as I know, this paper presents state-of-the-art rate-distortion results for lossy image compression. That's a significant achievement given the existing work on this problem.

That said, the improvement in rate-savings is only moderate compared to the best existing methods (see references above), and the other methods appear to support much faster decode speeds, which is crucial for deployment.

So my recommendation is ""5: marginally below the acceptance threshold"" since it's not entirely clear how much of the performance gain comes from LVQ vs. predicting a full covariance matrix within each VQ group vs. the spatial autoregressive model.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666642281286,,1pGmKJvneD7,ICLR.cc/2023/Conference/Paper3340/-/Official_Review,,[],4,,['everyone'],1pGmKJvneD7,['ICLR.cc/2023/Conference/Paper3340/Reviewer_SVp9'],1666642281286,,1666642281286,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3340/Reviewer_SVp9']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. Specifically, the reviewer mentions the following papers that are not mentioned in the original paper:

1. ""Channel-wise Autoregressive Entropy Models for Learned Image Compression"" by David Minnen and Saurabh Singh (ICIP 2020)
2. ""ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding"" by Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang (CVPR 2022)
3. ""Transformer-based Transform Coding"" by Yinhao Zhu, Yang Yang, and Taco Cohen (ICLR 2022)

The reviewer suggests that the authors should compare their work with these papers, which also address the issue of modeling dependencies between channels, and discuss the trade-offs between rate-distortion performance and decoding speed.",1,"2020, 2022, 2022",ELIC Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding
klAhycf8G0H,"This paper presents a neural image compression models based on lattice vector quantization (LVQ).

The authors note two deficiencies in existing work: (1) most models use scalar quantization (SQ), which is typically less powerful than vector quantization (VQ) if a large enough codebook is used, and (2) models that do use VQ do not use a hyperprior or end-to-end optimization. The authors address these problems by using LVQ, which has a large, though constrained, codebook, and they show how to jointly optimize a distribution over the lattice (the entropy model) with a hyperprior and spatially autoregressive model.

Two other important contributions are the evaluation of predicting a full covariance matrix for each LVQ group, rather than a diagonal covariance matrix, and using Monte Carlo integration to more quickly calculate (an approximation of) the likelihood of each feature vector.

The result is, as far as I know, state of the art rate-distortion (RD) performance for lossy image compression. The evaluation shows an 18% rate savings over VTM, one of the best standard (hand-engineered) image codecs, as well as gains over other learning-based (neural) codecs (Fig 5 and Table 2). The paper also compares different lattice dimensionalities and the effect of predicting a full covariance matrix (Fig 4 and Table 1).
","The primary strength of this paper is the achieving SOTA rate-distortion performance. As far as I know, now other papers or standard codecs achieve as good a compression rate when using PSNR as the quality metric.

The primary weakness of the paper is a lack of runtime data. I suspect this method is extremely slow since spatially AR context models are known to be slow due to the difficulty of parallelizing the decoding process, and the prediction of a full covariance matrix leads to a large number of output parameters (if I understand the setup correctly, 192 channels using \Lambda_24 leads to 8 groups of 24D vectors and thus 8 * (24^2 + 24) = 4800 parameters per spatial location in the latent image representation). I suspect this leads to decode times of multiple seconds, even on fast hardware, when a deployable model needs to decode kodak-size images in a handful of milliseconds. To the best of my knowledge, no neural methods achieve decode speeds on par with standard codecs like VTM (or BPG or JPEG, etc.), but other models are probably two orders of magnitude faster than the method in this paper while only losing 5-10% in terms of rate.

The other weakness is a lack of direct comparison against stronger neural methods. Balle 2018, Minnen 2018, and Cheng 2020 are all relatively old methods. In particular, part of the motivation of this paper is the lack of modeling the dependencies between channels (SQ doesn't do this, VQ does). But existing papers adopt other methods for (partially) modeling these dependencies, for example:

Channel-wise Autoregressive Entropy Models for Learned Image Compression
David Minnen, Saurabh Singh
(ICIP 2020) https://arxiv.org/abs/2007.08739

ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding
Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, Yan Wang
(CVPR 2022) https://arxiv.org/abs/2203.10886

Transformer-based Transform Coding
Yinhao Zhu, Yang Yang, Taco Cohen
(ICLR 2022) https://openreview.net/forum?id=IDwN6xjHnK8

The channel-wise AR approach used by these papers should be mentioned and compared to the LVQ approach adopted here since they both address the same underlying deficiency. I believe the RD performance of the LVQ approach is better than what is reported in these papers. However, the runtime is vastly different (ELIC in particular is focused on balancing RD performance with runtime), and the preferable trade-off probably falls to ELIC over the proposed LVQ method, i.e. I wouldn't accept 10-100x slower decoding to save 5-10% on file sizes.

Finally, I'd like to better understand how much of the RD performance is due to the LVQ vs. the full covariance matrix. Fig 4 and Table 1 partially answer the question by showing that for several lattice dimensionalities, predicting a full covariance matrix helps (as expected). Another interesting comparison would be to use a trivial lattice (hyper-cubes) but still group the channels into 24D vectors and predict a full covariance matrix. I expect \Lambda_24 is better (the underlying theory predicts this for mse) but the size of the gap would make it clear whether or not the added complexity of \Lambda_24 is warranted.
","The overall quality of the paper is quite good.

Novelty is medium. While the results are SOTA and I agree with the authors' claim that previous work has not used LVQ with a hyperprior and end-to-end optimization, all of the components do already exist.

Reproducibility seems fairly difficult so releasing code to set up the lattice and approximate the likelihoods would help significantly (the other components already exist in opensource projects like TensorFlow-Compression and CompressAI).
","As far as I know, this paper presents state-of-the-art rate-distortion results for lossy image compression. That's a significant achievement given the existing work on this problem.

That said, the improvement in rate-savings is only moderate compared to the best existing methods (see references above), and the other methods appear to support much faster decode speeds, which is crucial for deployment.

So my recommendation is ""5: marginally below the acceptance threshold"" since it's not entirely clear how much of the performance gain comes from LVQ vs. predicting a full covariance matrix within each VQ group vs. the spatial autoregressive model.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666642281286,,1pGmKJvneD7,ICLR.cc/2023/Conference/Paper3340/-/Official_Review,,[],4,,['everyone'],1pGmKJvneD7,['ICLR.cc/2023/Conference/Paper3340/Reviewer_SVp9'],1666642281286,,1666642281286,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3340/Reviewer_SVp9']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. Specifically, the reviewer mentions the following papers that are not mentioned in the original paper:

1. ""Channel-wise Autoregressive Entropy Models for Learned Image Compression"" by David Minnen and Saurabh Singh (ICIP 2020)
2. ""ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding"" by Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang (CVPR 2022)
3. ""Transformer-based Transform Coding"" by Yinhao Zhu, Yang Yang, and Taco Cohen (ICLR 2022)

The reviewer suggests that the authors should compare their work with these papers, which also address the issue of modeling dependencies between channels, and discuss the trade-offs between rate-distortion performance and decoding speed.",1,"2020, 2022, 2022",Transformer-based Transform Coding
l-FMgGOe3U,"The paper aims to efficiently replace the text encoder of the text-to-image generation model to improve the generation quality (or to make a multilingual text-to-image generative model). To this end, the authors proposed Model Translation Network (MTN) in which training loss is motivated by domain adaptation and cross-domain alignment. The proposed method shows that the performance gain is consistent over the recent latent diffusion model (LDM) and can make LDM a multilingual text-to-image generative model. ","**Strength**

(1) The research direction itself is quite interesting, i.e., replacing the text encoder for a better (or multilingual) generative model.

(2) The presentation and organization are clear.

(3) The author provides comprehensive generated results (in the Supplementary).

----
**Weakness**

(1) Ablation study is missing. Only the quantitative results are given in Appendix (A.3). To claim the effectiveness of each component, the qualitative results should be given.

(2) The choice of using domain adaptation is somewhat unclear. Since authors have to run an additional discriminator, it may increase the computation, which is slightly against the motivation. Does this bring a significant improvement?

(3) The performance gain is somewhat marginal (in Table 1). Since the LDM is not properly reproduced, the gain seems to be about -0.5 FID or -1.5 FID, which is slightly marginal. Can the author try a bigger language model to achieve more gain? E.g., providing encoder size and FID results like in Figure 4 of [1].

(4) Comparison with a naive approach for a multilingual generation. One can simply use a state-of-the-art machine translation model (e.g., CONT [2]) on top of LDM (or SDM) and fine-tune it. Does the proposed approach outperform such a naive approach?

(5) (minor) Some analysis of the hyper-parameter will be great (e.g., hyper-parameter sensitivity)

[1] Saharia et al., Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, NeurIPS 2022\
[2] An et al., CONT: Contrastive Neural Text Generation, NeurIPS 2022","**Clarity**

The writing and the presentation are clear. If the author can provide more clear mathematical notation, in Section 4.1, it will be much better, e.g., some problem setups in a mathematical formula.

**Quality**

The overall quality is fine.

**Novelty**

The method itself is a combination of existing works, but the research direction is novel.

**Reproducibility**

Since the authors did not provide the code and the hyper-parameter, I believe it is hard to be reproduced.","I recommend weak acceptance. I believe the target problem is quite interesting and has the strength to be accepted. However, some concerns (mentioned in the weakness part) make me choose the weak acceptance.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666611325184,,1n1c7cHl3Zc,ICLR.cc/2023/Conference/Paper5567/-/Official_Review,,[],2,,['everyone'],1n1c7cHl3Zc,['ICLR.cc/2023/Conference/Paper5567/Reviewer_2Z7M'],1666611325184,,1666611325184,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5567/Reviewer_2Z7M']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. Saharia et al., Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, NeurIPS 2022
2. An et al., CONT: Contrastive Neural Text Generation, NeurIPS 2022

These papers are used as a reference to suggest potential improvements or comparisons to the authors' work, implying that they are not already discussed in the original paper.",1,"2022, 2022",Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding 
l-FMgGOe3U,"The paper aims to efficiently replace the text encoder of the text-to-image generation model to improve the generation quality (or to make a multilingual text-to-image generative model). To this end, the authors proposed Model Translation Network (MTN) in which training loss is motivated by domain adaptation and cross-domain alignment. The proposed method shows that the performance gain is consistent over the recent latent diffusion model (LDM) and can make LDM a multilingual text-to-image generative model. ","**Strength**

(1) The research direction itself is quite interesting, i.e., replacing the text encoder for a better (or multilingual) generative model.

(2) The presentation and organization are clear.

(3) The author provides comprehensive generated results (in the Supplementary).

----
**Weakness**

(1) Ablation study is missing. Only the quantitative results are given in Appendix (A.3). To claim the effectiveness of each component, the qualitative results should be given.

(2) The choice of using domain adaptation is somewhat unclear. Since authors have to run an additional discriminator, it may increase the computation, which is slightly against the motivation. Does this bring a significant improvement?

(3) The performance gain is somewhat marginal (in Table 1). Since the LDM is not properly reproduced, the gain seems to be about -0.5 FID or -1.5 FID, which is slightly marginal. Can the author try a bigger language model to achieve more gain? E.g., providing encoder size and FID results like in Figure 4 of [1].

(4) Comparison with a naive approach for a multilingual generation. One can simply use a state-of-the-art machine translation model (e.g., CONT [2]) on top of LDM (or SDM) and fine-tune it. Does the proposed approach outperform such a naive approach?

(5) (minor) Some analysis of the hyper-parameter will be great (e.g., hyper-parameter sensitivity)

[1] Saharia et al., Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, NeurIPS 2022\
[2] An et al., CONT: Contrastive Neural Text Generation, NeurIPS 2022","**Clarity**

The writing and the presentation are clear. If the author can provide more clear mathematical notation, in Section 4.1, it will be much better, e.g., some problem setups in a mathematical formula.

**Quality**

The overall quality is fine.

**Novelty**

The method itself is a combination of existing works, but the research direction is novel.

**Reproducibility**

Since the authors did not provide the code and the hyper-parameter, I believe it is hard to be reproduced.","I recommend weak acceptance. I believe the target problem is quite interesting and has the strength to be accepted. However, some concerns (mentioned in the weakness part) make me choose the weak acceptance.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666611325184,,1n1c7cHl3Zc,ICLR.cc/2023/Conference/Paper5567/-/Official_Review,,[],2,,['everyone'],1n1c7cHl3Zc,['ICLR.cc/2023/Conference/Paper5567/Reviewer_2Z7M'],1666611325184,,1666611325184,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5567/Reviewer_2Z7M']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. Saharia et al., Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, NeurIPS 2022
2. An et al., CONT: Contrastive Neural Text Generation, NeurIPS 2022

These papers are used as a reference to suggest potential improvements or comparisons to the authors' work, implying that they are not already discussed in the original paper.",1,"2022, 2022",CONT Contrastive Neural Text Generation
_xUgaNVY5do,"In this paper, the authors proposed to address the problem of replacing a text encoder in a text-to-image model without re-training the whole pipeline from scratch. In doing so, it is proposed that MTN (Model Translation Network) projects the output of the new text encoder to the old one's output space, enabling the replacement. Experiments show both qualitatively and quantitatively the proposed baseline is capable of doing a replacement.
","# Strength

The problem this paper addresses is a novel and very interesting one per se, and is well-motivated. If fully fulfilled, this would lead to a large number of applications.

The authors make a reasonable effort to show the performance of this method. The effort in qualitative experiments is great in that lots of examples are shown for comparison. The method would be welcomed by the community.

# Weakness

With the strength said, I'm not fully convinced of the proposed method, as detailed below:

Proposed algorithm for alignment: The proposed method projects the new text-encoder's output space to the old encoder's space, thus can serving as a wrapper over new text encoder. However, several issues are presenting:

1. The training loss, which consists of three losses (eq 3,4,5), are complex and the dynamic is unclear. No ablation study is presented to provide the justification of such a design.
2. It is unclear if the architecture of MTN could handle fixed length representation or the variant length ones.
3. The technical contribution of the proposed MTN is unclear. Related works in bridging the latent space of different language models are not discussed. Just to name a few, in machine translation:
    - Kulshreshtha et al. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study
    - Cao et al. Multilingual Alignment of Contextual Word Representations
4. The performance contribution, or how good the MTN works, is unclear. MSE loss seems to be okay as shown in the experiment, however there is not a grounding of exactly how well MTN works.  We have no idea if the qualitative results come from the MTN being just okay but the generative model for images is powerful enough to compensate for MTN. Ideally, a good ablation is to have two runs of the same language models with different initial parameters and using the proposed technique to replace one with another.
5. One argument for this work is that a new language model's ability could be leveraged. However if it's demonstrated through an language model in another language, a justification should be provided through comparing it with simply translating the prompt.

# Questions for Authors

1. How much computation is required for the proposed MTN?
","The paper is a clear one. However it suffers issues in quality and novelty as mentioned above.
","While this paper addresses an interesting problem and is well-motivated, the issues in the proposed method and the experiment's design makes it hard to be presented in the conference in its current form. I would thus suggest a rejection of this manuscript based on the current understanding, but I'm happy to change my rating if my concerns and questions are addressed in the discussion.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666643921037,,1n1c7cHl3Zc,ICLR.cc/2023/Conference/Paper5567/-/Official_Review,,[],3,,['everyone'],1n1c7cHl3Zc,['ICLR.cc/2023/Conference/Paper5567/Reviewer_5yb4'],1666643921037,,1669345465896,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5567/Reviewer_5yb4']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Kulshreshtha et al. ""Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study""
2. Cao et al. ""Multilingual Alignment of Contextual Word Representations""

The reviewer states that related works in bridging the latent space of different language models, such as these two papers, are not discussed in the original paper. This implies that the reviewer believes these papers are relevant to the topic and should be referenced and considered in the authors' work.",1,,Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study
_xUgaNVY5do,"In this paper, the authors proposed to address the problem of replacing a text encoder in a text-to-image model without re-training the whole pipeline from scratch. In doing so, it is proposed that MTN (Model Translation Network) projects the output of the new text encoder to the old one's output space, enabling the replacement. Experiments show both qualitatively and quantitatively the proposed baseline is capable of doing a replacement.
","# Strength

The problem this paper addresses is a novel and very interesting one per se, and is well-motivated. If fully fulfilled, this would lead to a large number of applications.

The authors make a reasonable effort to show the performance of this method. The effort in qualitative experiments is great in that lots of examples are shown for comparison. The method would be welcomed by the community.

# Weakness

With the strength said, I'm not fully convinced of the proposed method, as detailed below:

Proposed algorithm for alignment: The proposed method projects the new text-encoder's output space to the old encoder's space, thus can serving as a wrapper over new text encoder. However, several issues are presenting:

1. The training loss, which consists of three losses (eq 3,4,5), are complex and the dynamic is unclear. No ablation study is presented to provide the justification of such a design.
2. It is unclear if the architecture of MTN could handle fixed length representation or the variant length ones.
3. The technical contribution of the proposed MTN is unclear. Related works in bridging the latent space of different language models are not discussed. Just to name a few, in machine translation:
    - Kulshreshtha et al. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study
    - Cao et al. Multilingual Alignment of Contextual Word Representations
4. The performance contribution, or how good the MTN works, is unclear. MSE loss seems to be okay as shown in the experiment, however there is not a grounding of exactly how well MTN works.  We have no idea if the qualitative results come from the MTN being just okay but the generative model for images is powerful enough to compensate for MTN. Ideally, a good ablation is to have two runs of the same language models with different initial parameters and using the proposed technique to replace one with another.
5. One argument for this work is that a new language model's ability could be leveraged. However if it's demonstrated through an language model in another language, a justification should be provided through comparing it with simply translating the prompt.

# Questions for Authors

1. How much computation is required for the proposed MTN?
","The paper is a clear one. However it suffers issues in quality and novelty as mentioned above.
","While this paper addresses an interesting problem and is well-motivated, the issues in the proposed method and the experiment's design makes it hard to be presented in the conference in its current form. I would thus suggest a rejection of this manuscript based on the current understanding, but I'm happy to change my rating if my concerns and questions are addressed in the discussion.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666643921037,,1n1c7cHl3Zc,ICLR.cc/2023/Conference/Paper5567/-/Official_Review,,[],3,,['everyone'],1n1c7cHl3Zc,['ICLR.cc/2023/Conference/Paper5567/Reviewer_5yb4'],1666643921037,,1669345465896,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5567/Reviewer_5yb4']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Kulshreshtha et al. ""Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study""
2. Cao et al. ""Multilingual Alignment of Contextual Word Representations""

The reviewer states that related works in bridging the latent space of different language models, such as these two papers, are not discussed in the original paper. This implies that the reviewer believes these papers are relevant to the topic and should be referenced and considered in the authors' work.",1,,Multilingual Alignment of Contextual Word Representations
ASTxCP8kV3,"This paper proposes to combine contrastive learning with masked image modeling in order to provide strong pre-text tasks. As opposed to conventional contrastive learning and masked image modeling (MIM) approaches, the authors cast masked image modeling as denoising contrastive learning for an effective vision dictionary look-up. It was validated that the proposed method, ConMIM, competes favorably on downstream tasks (e.g., image classification, semantic segmentation).","Strengths:
- Quantitative results are promising. The authors validated the proposed method on the models of various sizes, clearly showing the superior performance to prior approaches.
- The method is simple yet effective. It does not leverage extra training stages of image tokenizer.

Weaknesses:
- The authors miss some important reference in the field of MIM. For instance, ‘SimMIM: A Simple Framework for Masked Image Modeling (Z.Xie et al., CVPR 2022)’ was not referenced in this paper. Though SimMIM is remarkably simple in that it applies a simple one-layer prediction head decoder to MIM, it records competitive performance compared to ConMIM. Thus, the effectiveness of the proposed method compared to this simple approach seems rather questionable.

- Similarly, there are various attempts to combine contrastive learning and MIM. For example,

(1) IBOT: Image BERT pre-training with online tokenizer (J.Zhou et al)

(2) Siamese image modeling for self-supervised vision representation learning (C. Tao et al)

(3) Contrastive Masked Autoencoders are Stronger Vision Learners (Z.Huang et al)

The novelty of the proposed method should be clarified with respect to these methods. It was briefly mentioned in the related work section, but it seems that the basic idea itself of the proposed framework (combining the contrastive learning and MIM) is very similar to the above-mentioned methods, and the only difference is what kind of contrastive learning module (or what kind of MIM module) is used in the proposed framework. Namely, it seems that in terms of combining the two approaches, the proposed framework is essentially very similar to the existing methods.

- The asymmetric augmentation used in the proposed method needs more clarification. To the best of my knowledge, applying weak augmentation to the input of the target encoder (slowly progressing ViT in Figure 2) and strong augmentation to the input of the source encoder is known to be suitable for the field of contrastive learning. In contrast, the proposed method contradicts this common wisdom. Do you have any explanation for this?

- Since the proposed method importantly leverages contrastive learning, it seems that the result of combining a variant of contrastive learning approaches (e.g.. DINO, Byol, Simsiam) and MIM should be provided as an ablation study.
","This work attempts to combine the two popular approaches, contrastive learning and MIM, for achieving better performance, but it is hard to catch up the key difference with recent approaches that try to use the contrastive learning and MIM.","The authors proposed a reasonable approach to take the advantage of the two pre-training approaches, but there are some concerns related to novelty, evaluation and method details, as mentioned in ‘Strength and Weaknesses’. Nevertheless, if these comments are well-addressed, the initial rating can be adjusted.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666675678302,,1fZd4owfJP6,ICLR.cc/2023/Conference/Paper1611/-/Official_Review,,[],2,,['everyone'],1fZd4owfJP6,['ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22'],1666675678302,,1666676653088,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the original paper, including:

1. ""SimMIM: A Simple Framework for Masked Image Modeling"" by Z. Xie et al. (CVPR 2022)
2. ""IBOT: Image BERT pre-training with online tokenizer"" by J. Zhou et al.
3. ""Siamese image modeling for self-supervised vision representation learning"" by C. Tao et al.
4. ""Contrastive Masked Autoencoders are Stronger Vision Learners"" by Z. Huang et al.
5. DINO, Byol, and Simsiam ( variants of contrastive learning approaches)

The reviewer suggests that the authors should clarify the novelty of their proposed method with respect to these existing methods, and also provide an ablation study to compare the results of combining different contrastive learning approaches with MIM.",1,2022,SimMIM A Simple Framework for Masked Image Modeling 
ASTxCP8kV3,"This paper proposes to combine contrastive learning with masked image modeling in order to provide strong pre-text tasks. As opposed to conventional contrastive learning and masked image modeling (MIM) approaches, the authors cast masked image modeling as denoising contrastive learning for an effective vision dictionary look-up. It was validated that the proposed method, ConMIM, competes favorably on downstream tasks (e.g., image classification, semantic segmentation).","Strengths:
- Quantitative results are promising. The authors validated the proposed method on the models of various sizes, clearly showing the superior performance to prior approaches.
- The method is simple yet effective. It does not leverage extra training stages of image tokenizer.

Weaknesses:
- The authors miss some important reference in the field of MIM. For instance, ‘SimMIM: A Simple Framework for Masked Image Modeling (Z.Xie et al., CVPR 2022)’ was not referenced in this paper. Though SimMIM is remarkably simple in that it applies a simple one-layer prediction head decoder to MIM, it records competitive performance compared to ConMIM. Thus, the effectiveness of the proposed method compared to this simple approach seems rather questionable.

- Similarly, there are various attempts to combine contrastive learning and MIM. For example,

(1) IBOT: Image BERT pre-training with online tokenizer (J.Zhou et al)

(2) Siamese image modeling for self-supervised vision representation learning (C. Tao et al)

(3) Contrastive Masked Autoencoders are Stronger Vision Learners (Z.Huang et al)

The novelty of the proposed method should be clarified with respect to these methods. It was briefly mentioned in the related work section, but it seems that the basic idea itself of the proposed framework (combining the contrastive learning and MIM) is very similar to the above-mentioned methods, and the only difference is what kind of contrastive learning module (or what kind of MIM module) is used in the proposed framework. Namely, it seems that in terms of combining the two approaches, the proposed framework is essentially very similar to the existing methods.

- The asymmetric augmentation used in the proposed method needs more clarification. To the best of my knowledge, applying weak augmentation to the input of the target encoder (slowly progressing ViT in Figure 2) and strong augmentation to the input of the source encoder is known to be suitable for the field of contrastive learning. In contrast, the proposed method contradicts this common wisdom. Do you have any explanation for this?

- Since the proposed method importantly leverages contrastive learning, it seems that the result of combining a variant of contrastive learning approaches (e.g.. DINO, Byol, Simsiam) and MIM should be provided as an ablation study.
","This work attempts to combine the two popular approaches, contrastive learning and MIM, for achieving better performance, but it is hard to catch up the key difference with recent approaches that try to use the contrastive learning and MIM.","The authors proposed a reasonable approach to take the advantage of the two pre-training approaches, but there are some concerns related to novelty, evaluation and method details, as mentioned in ‘Strength and Weaknesses’. Nevertheless, if these comments are well-addressed, the initial rating can be adjusted.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666675678302,,1fZd4owfJP6,ICLR.cc/2023/Conference/Paper1611/-/Official_Review,,[],2,,['everyone'],1fZd4owfJP6,['ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22'],1666675678302,,1666676653088,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the original paper, including:

1. ""SimMIM: A Simple Framework for Masked Image Modeling"" by Z. Xie et al. (CVPR 2022)
2. ""IBOT: Image BERT pre-training with online tokenizer"" by J. Zhou et al.
3. ""Siamese image modeling for self-supervised vision representation learning"" by C. Tao et al.
4. ""Contrastive Masked Autoencoders are Stronger Vision Learners"" by Z. Huang et al.
5. DINO, Byol, and Simsiam ( variants of contrastive learning approaches)

The reviewer suggests that the authors should clarify the novelty of their proposed method with respect to these existing methods, and also provide an ablation study to compare the results of combining different contrastive learning approaches with MIM.",1,2022,IBOT Image BERT pre-training with online tokenizer 
ASTxCP8kV3,"This paper proposes to combine contrastive learning with masked image modeling in order to provide strong pre-text tasks. As opposed to conventional contrastive learning and masked image modeling (MIM) approaches, the authors cast masked image modeling as denoising contrastive learning for an effective vision dictionary look-up. It was validated that the proposed method, ConMIM, competes favorably on downstream tasks (e.g., image classification, semantic segmentation).","Strengths:
- Quantitative results are promising. The authors validated the proposed method on the models of various sizes, clearly showing the superior performance to prior approaches.
- The method is simple yet effective. It does not leverage extra training stages of image tokenizer.

Weaknesses:
- The authors miss some important reference in the field of MIM. For instance, ‘SimMIM: A Simple Framework for Masked Image Modeling (Z.Xie et al., CVPR 2022)’ was not referenced in this paper. Though SimMIM is remarkably simple in that it applies a simple one-layer prediction head decoder to MIM, it records competitive performance compared to ConMIM. Thus, the effectiveness of the proposed method compared to this simple approach seems rather questionable.

- Similarly, there are various attempts to combine contrastive learning and MIM. For example,

(1) IBOT: Image BERT pre-training with online tokenizer (J.Zhou et al)

(2) Siamese image modeling for self-supervised vision representation learning (C. Tao et al)

(3) Contrastive Masked Autoencoders are Stronger Vision Learners (Z.Huang et al)

The novelty of the proposed method should be clarified with respect to these methods. It was briefly mentioned in the related work section, but it seems that the basic idea itself of the proposed framework (combining the contrastive learning and MIM) is very similar to the above-mentioned methods, and the only difference is what kind of contrastive learning module (or what kind of MIM module) is used in the proposed framework. Namely, it seems that in terms of combining the two approaches, the proposed framework is essentially very similar to the existing methods.

- The asymmetric augmentation used in the proposed method needs more clarification. To the best of my knowledge, applying weak augmentation to the input of the target encoder (slowly progressing ViT in Figure 2) and strong augmentation to the input of the source encoder is known to be suitable for the field of contrastive learning. In contrast, the proposed method contradicts this common wisdom. Do you have any explanation for this?

- Since the proposed method importantly leverages contrastive learning, it seems that the result of combining a variant of contrastive learning approaches (e.g.. DINO, Byol, Simsiam) and MIM should be provided as an ablation study.
","This work attempts to combine the two popular approaches, contrastive learning and MIM, for achieving better performance, but it is hard to catch up the key difference with recent approaches that try to use the contrastive learning and MIM.","The authors proposed a reasonable approach to take the advantage of the two pre-training approaches, but there are some concerns related to novelty, evaluation and method details, as mentioned in ‘Strength and Weaknesses’. Nevertheless, if these comments are well-addressed, the initial rating can be adjusted.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666675678302,,1fZd4owfJP6,ICLR.cc/2023/Conference/Paper1611/-/Official_Review,,[],2,,['everyone'],1fZd4owfJP6,['ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22'],1666675678302,,1666676653088,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the original paper, including:

1. ""SimMIM: A Simple Framework for Masked Image Modeling"" by Z. Xie et al. (CVPR 2022)
2. ""IBOT: Image BERT pre-training with online tokenizer"" by J. Zhou et al.
3. ""Siamese image modeling for self-supervised vision representation learning"" by C. Tao et al.
4. ""Contrastive Masked Autoencoders are Stronger Vision Learners"" by Z. Huang et al.
5. DINO, Byol, and Simsiam ( variants of contrastive learning approaches)

The reviewer suggests that the authors should clarify the novelty of their proposed method with respect to these existing methods, and also provide an ablation study to compare the results of combining different contrastive learning approaches with MIM.",1,2022,Siamese image modeling for self-supervised vision representation learning 
ASTxCP8kV3,"This paper proposes to combine contrastive learning with masked image modeling in order to provide strong pre-text tasks. As opposed to conventional contrastive learning and masked image modeling (MIM) approaches, the authors cast masked image modeling as denoising contrastive learning for an effective vision dictionary look-up. It was validated that the proposed method, ConMIM, competes favorably on downstream tasks (e.g., image classification, semantic segmentation).","Strengths:
- Quantitative results are promising. The authors validated the proposed method on the models of various sizes, clearly showing the superior performance to prior approaches.
- The method is simple yet effective. It does not leverage extra training stages of image tokenizer.

Weaknesses:
- The authors miss some important reference in the field of MIM. For instance, ‘SimMIM: A Simple Framework for Masked Image Modeling (Z.Xie et al., CVPR 2022)’ was not referenced in this paper. Though SimMIM is remarkably simple in that it applies a simple one-layer prediction head decoder to MIM, it records competitive performance compared to ConMIM. Thus, the effectiveness of the proposed method compared to this simple approach seems rather questionable.

- Similarly, there are various attempts to combine contrastive learning and MIM. For example,

(1) IBOT: Image BERT pre-training with online tokenizer (J.Zhou et al)

(2) Siamese image modeling for self-supervised vision representation learning (C. Tao et al)

(3) Contrastive Masked Autoencoders are Stronger Vision Learners (Z.Huang et al)

The novelty of the proposed method should be clarified with respect to these methods. It was briefly mentioned in the related work section, but it seems that the basic idea itself of the proposed framework (combining the contrastive learning and MIM) is very similar to the above-mentioned methods, and the only difference is what kind of contrastive learning module (or what kind of MIM module) is used in the proposed framework. Namely, it seems that in terms of combining the two approaches, the proposed framework is essentially very similar to the existing methods.

- The asymmetric augmentation used in the proposed method needs more clarification. To the best of my knowledge, applying weak augmentation to the input of the target encoder (slowly progressing ViT in Figure 2) and strong augmentation to the input of the source encoder is known to be suitable for the field of contrastive learning. In contrast, the proposed method contradicts this common wisdom. Do you have any explanation for this?

- Since the proposed method importantly leverages contrastive learning, it seems that the result of combining a variant of contrastive learning approaches (e.g.. DINO, Byol, Simsiam) and MIM should be provided as an ablation study.
","This work attempts to combine the two popular approaches, contrastive learning and MIM, for achieving better performance, but it is hard to catch up the key difference with recent approaches that try to use the contrastive learning and MIM.","The authors proposed a reasonable approach to take the advantage of the two pre-training approaches, but there are some concerns related to novelty, evaluation and method details, as mentioned in ‘Strength and Weaknesses’. Nevertheless, if these comments are well-addressed, the initial rating can be adjusted.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666675678302,,1fZd4owfJP6,ICLR.cc/2023/Conference/Paper1611/-/Official_Review,,[],2,,['everyone'],1fZd4owfJP6,['ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22'],1666675678302,,1666676653088,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the original paper, including:

1. ""SimMIM: A Simple Framework for Masked Image Modeling"" by Z. Xie et al. (CVPR 2022)
2. ""IBOT: Image BERT pre-training with online tokenizer"" by J. Zhou et al.
3. ""Siamese image modeling for self-supervised vision representation learning"" by C. Tao et al.
4. ""Contrastive Masked Autoencoders are Stronger Vision Learners"" by Z. Huang et al.
5. DINO, Byol, and Simsiam ( variants of contrastive learning approaches)

The reviewer suggests that the authors should clarify the novelty of their proposed method with respect to these existing methods, and also provide an ablation study to compare the results of combining different contrastive learning approaches with MIM.",1,2022,Contrastive Masked Autoencoders are Stronger Vision Learners
jScqu5EU8h,"This paper studies the problem of learning a decision function when the output might be corrupted by the fact that the sensor in charge of collecting it has failed to record it properly, underestimating it. Without a debiasing procedure, the decision function naturally underestimate the magnitude of the event. Under the assumption that the loss function does not depend on the prediction (when the latter is greater than the output), the authors propose an estimator of the gradient that is unbiased. Experiments complement the paper.","**Strengths**
- the paper is globally clear and well written
- the problem studied is of interest and the proposed approach is natural

**Weaknesses**
- it seems to me that the interest of the approach is completely shortcut by the choice of loss functions, which do not depend on the label. This is a very strong limitation of the approach to me
- in the same vein, could the authors think about other examples where to apply a similar method? This could make the contribution of greater interest
- the experiments are pretty rudimentary, especially on real data. For instance how would a model with censorship behave, when given the information if the output has been altered or not?
- I am also pointing out the literature on Median-of-Means (MoM)-based methods for robust regression, see in particular [1], that do not assume the outliers to be symmetric and could be interesting to benchmark

[1] Robust classification via MOM minimization, Lecué et al. 2020","Gloablly good, except for the following points
- the way conditional expectation are presented is confusing to me
- in Eq. (9), (10), shouldn't it be $y'$ instead of $y$?
- Lem 3.4: shouldn't $\eta$ be $1/2$ since the noise is symmetric?","Overall, I feel the contribution of this paper might not be enough to warrant acceptance. In particular, the restricted choice of loss functions is shortcuting the interest of the approach. The experiments need strengthening. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666820092051,,1ehuYMrigt,ICLR.cc/2023/Conference/Paper3966/-/Official_Review,,[],4,,['everyone'],1ehuYMrigt,['ICLR.cc/2023/Conference/Paper3966/Reviewer_Sduq'],1666820092051,,1666820092051,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3966/Reviewer_Sduq']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the literature on Median-of-Means (MoM)-based methods for robust regression, specifically citing ""Robust classification via MOM minimization, Lecué et al. 2020"" as a relevant reference. This suggests that the reviewer believes the authors should be aware of and consider this literature in their work, implying that it is not already discussed in the original paper.",1,2020,Robust classification via MOM minimization
XsbK100ZqV,"The paper addresses the problem of unsupervised semantic segmentation from images, which is relatively new in the literature. The methodology is based on saliency detection, clustering of self-supervised features (e.g., DINO) within the salient regions to obtain pseudo-masks, followed by a few iterations of self-training. This method achieves state-of-the-art results on unsupervised semantic segmentation on PASCAL-VOC and MS COCO.  ","
**Strengths**

The proposed method is simple, yet effective as it achieves over 10 points mIoU above prior work. Results are also shown for the first time on the challenging MS COCO dataset (80 classes).   

The method directly builds on pre-trained representations, with optional self-training. As such, and in contrast to some other approaches, it does not require costly self-supervised pre-training at object level (i.e., dense representations) to achieve a pixel-wise task, such as semantic segmentation.  


**Weaknesses**

(1) One major issue with the paper is its close resemblance to DeepSpectral (Melas-Kyriazi et al., 2022), although the authors do not discuss this explicitly. DeepSpectral uses a similar pipeline for (unsupervised) semantic segmentation: unsupervised extraction of image regions, clustering for categorizing these regions, and self-training. The most notable difference appears to be the choice of method for the extraction of relevant image regions, that is DeepUSPS for this paper and spectral clustering for DeepSpectral. The rest of the pipeline seems to be quite similar with minor changes (spectral vs k-means clustering, or the number of iterations during self-training). This limits the technical contribution of the paper, despite its strong performance. Therefore, the authors should provide an extensive discussion of the differences to DeepSpectral and highlight their significance. 

(2) The approach relies on a saliency detection method (DeepUSPS/BASNet) to get an initial set of object masks. Despite common belief, I would argue that DeepUSPS may not be considered *truly* unsupervised since:  
 - it distills an ensemble of *handcrafted* priors
 - it uses *supervised* pretraining (e.g., on ImageNet or Cityscapes according to (Nguyen et al., 2019)) 

This fact gives an inherent advantage to DeepUSPS vs fully unsupervised approaches for saliency detection. This is also clear from Table 7, where DeepUSPS performs significantly better than approaches such as LOST or DINOSeg which only rely on self-supervised features to estimate saliency. This in turn gives a clear advantage to the object proposal masks used by the rest of the method. 

Notably, DeepSpectral also relies on self-supervised features (e.g., DINO) for object proposal masks. Therefore, the question that arises is whether the biggest part of the performance boost of this paper could be simply attributed to the advantage of DeepUSPS over DINO-based saliency. The authors could investigate this further, e.g., by swapping DeepUSPS for the object proposal part of DeepSpectral (namely, spectral decomposition). 

(3) In most figures, it is notable that this method still struggles with images containing more than one semantic category. This is due to the initial assumption that there is only a single object per image (i.e., assigning a single cluster ID to the entire salient region in an image). Despite self-training, the model does not seem to fully recover from this assumption, especially in PASCAL VOC where the majority of images do indeed contain a single object, so there is little training signal. This likely can be measured, e.g., by measuring the performance and predicted object count on images containing more than one object 

(4) The authors could also consider running experiments on COCO-Stuff, to facilitate comparisons with other state-of-the-art methods, such as STEGO (Hamilton et al., 2022) and PiCIE (Cho et al., 2021).  

(5) The authors should also consider citing, discussing, and (where possible) comparing to the following related methods:

[1] Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)

[2] Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)

[3] Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
","Regarding clarity, the paper is well-structured, straightforward, and easy to follow. Enough details are provided that make the paper mostly reproducible. However, due to the reasons outlined above, the proposed idea is not very original and thus the paper lacks in terms of novelty. ","Although this paper achieves state-of-the-art results on the task of unsupervised semantic segmentation by a large margin, these results may be (to a large degree) due to the superior saliency estimation method that is used in this paper in contrast to prior work. The rest of the pipeline is very similar to that of (Melas-Kyriazi et al., 2022). As a result, I find that the novelty of this paper is not significant enough to warrant acceptance to ICLR.  ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666590369295,,1_jFneF07YC,ICLR.cc/2023/Conference/Paper3900/-/Official_Review,,[],1,,['everyone'],1_jFneF07YC,['ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH'],1666590369295,,1673093379278,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that the authors should consider citing, discussing, and comparing to:

1. Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)
2. Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)
3. Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
4. Hamilton et al., ""STEGO"" (2022)
5. Cho et al., ""PiCIE"" (2021)

The reviewer also suggests running experiments on COCO-Stuff to facilitate comparisons with other state-of-the-art methods, such as STEGO and PiCIE.",1,"2022, 2022, 2022, 2022, 2021",Self-Supervised Learning of Object Parts for Semantic Segmentation
XsbK100ZqV,"The paper addresses the problem of unsupervised semantic segmentation from images, which is relatively new in the literature. The methodology is based on saliency detection, clustering of self-supervised features (e.g., DINO) within the salient regions to obtain pseudo-masks, followed by a few iterations of self-training. This method achieves state-of-the-art results on unsupervised semantic segmentation on PASCAL-VOC and MS COCO.  ","
**Strengths**

The proposed method is simple, yet effective as it achieves over 10 points mIoU above prior work. Results are also shown for the first time on the challenging MS COCO dataset (80 classes).   

The method directly builds on pre-trained representations, with optional self-training. As such, and in contrast to some other approaches, it does not require costly self-supervised pre-training at object level (i.e., dense representations) to achieve a pixel-wise task, such as semantic segmentation.  


**Weaknesses**

(1) One major issue with the paper is its close resemblance to DeepSpectral (Melas-Kyriazi et al., 2022), although the authors do not discuss this explicitly. DeepSpectral uses a similar pipeline for (unsupervised) semantic segmentation: unsupervised extraction of image regions, clustering for categorizing these regions, and self-training. The most notable difference appears to be the choice of method for the extraction of relevant image regions, that is DeepUSPS for this paper and spectral clustering for DeepSpectral. The rest of the pipeline seems to be quite similar with minor changes (spectral vs k-means clustering, or the number of iterations during self-training). This limits the technical contribution of the paper, despite its strong performance. Therefore, the authors should provide an extensive discussion of the differences to DeepSpectral and highlight their significance. 

(2) The approach relies on a saliency detection method (DeepUSPS/BASNet) to get an initial set of object masks. Despite common belief, I would argue that DeepUSPS may not be considered *truly* unsupervised since:  
 - it distills an ensemble of *handcrafted* priors
 - it uses *supervised* pretraining (e.g., on ImageNet or Cityscapes according to (Nguyen et al., 2019)) 

This fact gives an inherent advantage to DeepUSPS vs fully unsupervised approaches for saliency detection. This is also clear from Table 7, where DeepUSPS performs significantly better than approaches such as LOST or DINOSeg which only rely on self-supervised features to estimate saliency. This in turn gives a clear advantage to the object proposal masks used by the rest of the method. 

Notably, DeepSpectral also relies on self-supervised features (e.g., DINO) for object proposal masks. Therefore, the question that arises is whether the biggest part of the performance boost of this paper could be simply attributed to the advantage of DeepUSPS over DINO-based saliency. The authors could investigate this further, e.g., by swapping DeepUSPS for the object proposal part of DeepSpectral (namely, spectral decomposition). 

(3) In most figures, it is notable that this method still struggles with images containing more than one semantic category. This is due to the initial assumption that there is only a single object per image (i.e., assigning a single cluster ID to the entire salient region in an image). Despite self-training, the model does not seem to fully recover from this assumption, especially in PASCAL VOC where the majority of images do indeed contain a single object, so there is little training signal. This likely can be measured, e.g., by measuring the performance and predicted object count on images containing more than one object 

(4) The authors could also consider running experiments on COCO-Stuff, to facilitate comparisons with other state-of-the-art methods, such as STEGO (Hamilton et al., 2022) and PiCIE (Cho et al., 2021).  

(5) The authors should also consider citing, discussing, and (where possible) comparing to the following related methods:

[1] Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)

[2] Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)

[3] Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
","Regarding clarity, the paper is well-structured, straightforward, and easy to follow. Enough details are provided that make the paper mostly reproducible. However, due to the reasons outlined above, the proposed idea is not very original and thus the paper lacks in terms of novelty. ","Although this paper achieves state-of-the-art results on the task of unsupervised semantic segmentation by a large margin, these results may be (to a large degree) due to the superior saliency estimation method that is used in this paper in contrast to prior work. The rest of the pipeline is very similar to that of (Melas-Kyriazi et al., 2022). As a result, I find that the novelty of this paper is not significant enough to warrant acceptance to ICLR.  ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666590369295,,1_jFneF07YC,ICLR.cc/2023/Conference/Paper3900/-/Official_Review,,[],1,,['everyone'],1_jFneF07YC,['ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH'],1666590369295,,1673093379278,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that the authors should consider citing, discussing, and comparing to:

1. Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)
2. Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)
3. Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
4. Hamilton et al., ""STEGO"" (2022)
5. Cho et al., ""PiCIE"" (2021)

The reviewer also suggests running experiments on COCO-Stuff to facilitate comparisons with other state-of-the-art methods, such as STEGO and PiCIE.",1,"2022, 2022, 2022, 2022, 2021",Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers
XsbK100ZqV,"The paper addresses the problem of unsupervised semantic segmentation from images, which is relatively new in the literature. The methodology is based on saliency detection, clustering of self-supervised features (e.g., DINO) within the salient regions to obtain pseudo-masks, followed by a few iterations of self-training. This method achieves state-of-the-art results on unsupervised semantic segmentation on PASCAL-VOC and MS COCO.  ","
**Strengths**

The proposed method is simple, yet effective as it achieves over 10 points mIoU above prior work. Results are also shown for the first time on the challenging MS COCO dataset (80 classes).   

The method directly builds on pre-trained representations, with optional self-training. As such, and in contrast to some other approaches, it does not require costly self-supervised pre-training at object level (i.e., dense representations) to achieve a pixel-wise task, such as semantic segmentation.  


**Weaknesses**

(1) One major issue with the paper is its close resemblance to DeepSpectral (Melas-Kyriazi et al., 2022), although the authors do not discuss this explicitly. DeepSpectral uses a similar pipeline for (unsupervised) semantic segmentation: unsupervised extraction of image regions, clustering for categorizing these regions, and self-training. The most notable difference appears to be the choice of method for the extraction of relevant image regions, that is DeepUSPS for this paper and spectral clustering for DeepSpectral. The rest of the pipeline seems to be quite similar with minor changes (spectral vs k-means clustering, or the number of iterations during self-training). This limits the technical contribution of the paper, despite its strong performance. Therefore, the authors should provide an extensive discussion of the differences to DeepSpectral and highlight their significance. 

(2) The approach relies on a saliency detection method (DeepUSPS/BASNet) to get an initial set of object masks. Despite common belief, I would argue that DeepUSPS may not be considered *truly* unsupervised since:  
 - it distills an ensemble of *handcrafted* priors
 - it uses *supervised* pretraining (e.g., on ImageNet or Cityscapes according to (Nguyen et al., 2019)) 

This fact gives an inherent advantage to DeepUSPS vs fully unsupervised approaches for saliency detection. This is also clear from Table 7, where DeepUSPS performs significantly better than approaches such as LOST or DINOSeg which only rely on self-supervised features to estimate saliency. This in turn gives a clear advantage to the object proposal masks used by the rest of the method. 

Notably, DeepSpectral also relies on self-supervised features (e.g., DINO) for object proposal masks. Therefore, the question that arises is whether the biggest part of the performance boost of this paper could be simply attributed to the advantage of DeepUSPS over DINO-based saliency. The authors could investigate this further, e.g., by swapping DeepUSPS for the object proposal part of DeepSpectral (namely, spectral decomposition). 

(3) In most figures, it is notable that this method still struggles with images containing more than one semantic category. This is due to the initial assumption that there is only a single object per image (i.e., assigning a single cluster ID to the entire salient region in an image). Despite self-training, the model does not seem to fully recover from this assumption, especially in PASCAL VOC where the majority of images do indeed contain a single object, so there is little training signal. This likely can be measured, e.g., by measuring the performance and predicted object count on images containing more than one object 

(4) The authors could also consider running experiments on COCO-Stuff, to facilitate comparisons with other state-of-the-art methods, such as STEGO (Hamilton et al., 2022) and PiCIE (Cho et al., 2021).  

(5) The authors should also consider citing, discussing, and (where possible) comparing to the following related methods:

[1] Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)

[2] Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)

[3] Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
","Regarding clarity, the paper is well-structured, straightforward, and easy to follow. Enough details are provided that make the paper mostly reproducible. However, due to the reasons outlined above, the proposed idea is not very original and thus the paper lacks in terms of novelty. ","Although this paper achieves state-of-the-art results on the task of unsupervised semantic segmentation by a large margin, these results may be (to a large degree) due to the superior saliency estimation method that is used in this paper in contrast to prior work. The rest of the pipeline is very similar to that of (Melas-Kyriazi et al., 2022). As a result, I find that the novelty of this paper is not significant enough to warrant acceptance to ICLR.  ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666590369295,,1_jFneF07YC,ICLR.cc/2023/Conference/Paper3900/-/Official_Review,,[],1,,['everyone'],1_jFneF07YC,['ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH'],1666590369295,,1673093379278,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that the authors should consider citing, discussing, and comparing to:

1. Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)
2. Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)
3. Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
4. Hamilton et al., ""STEGO"" (2022)
5. Cho et al., ""PiCIE"" (2021)

The reviewer also suggests running experiments on COCO-Stuff to facilitate comparisons with other state-of-the-art methods, such as STEGO and PiCIE.",1,"2022, 2022, 2022, 2022, 2021",Object discovery and representation networks
XsbK100ZqV,"The paper addresses the problem of unsupervised semantic segmentation from images, which is relatively new in the literature. The methodology is based on saliency detection, clustering of self-supervised features (e.g., DINO) within the salient regions to obtain pseudo-masks, followed by a few iterations of self-training. This method achieves state-of-the-art results on unsupervised semantic segmentation on PASCAL-VOC and MS COCO.  ","
**Strengths**

The proposed method is simple, yet effective as it achieves over 10 points mIoU above prior work. Results are also shown for the first time on the challenging MS COCO dataset (80 classes).   

The method directly builds on pre-trained representations, with optional self-training. As such, and in contrast to some other approaches, it does not require costly self-supervised pre-training at object level (i.e., dense representations) to achieve a pixel-wise task, such as semantic segmentation.  


**Weaknesses**

(1) One major issue with the paper is its close resemblance to DeepSpectral (Melas-Kyriazi et al., 2022), although the authors do not discuss this explicitly. DeepSpectral uses a similar pipeline for (unsupervised) semantic segmentation: unsupervised extraction of image regions, clustering for categorizing these regions, and self-training. The most notable difference appears to be the choice of method for the extraction of relevant image regions, that is DeepUSPS for this paper and spectral clustering for DeepSpectral. The rest of the pipeline seems to be quite similar with minor changes (spectral vs k-means clustering, or the number of iterations during self-training). This limits the technical contribution of the paper, despite its strong performance. Therefore, the authors should provide an extensive discussion of the differences to DeepSpectral and highlight their significance. 

(2) The approach relies on a saliency detection method (DeepUSPS/BASNet) to get an initial set of object masks. Despite common belief, I would argue that DeepUSPS may not be considered *truly* unsupervised since:  
 - it distills an ensemble of *handcrafted* priors
 - it uses *supervised* pretraining (e.g., on ImageNet or Cityscapes according to (Nguyen et al., 2019)) 

This fact gives an inherent advantage to DeepUSPS vs fully unsupervised approaches for saliency detection. This is also clear from Table 7, where DeepUSPS performs significantly better than approaches such as LOST or DINOSeg which only rely on self-supervised features to estimate saliency. This in turn gives a clear advantage to the object proposal masks used by the rest of the method. 

Notably, DeepSpectral also relies on self-supervised features (e.g., DINO) for object proposal masks. Therefore, the question that arises is whether the biggest part of the performance boost of this paper could be simply attributed to the advantage of DeepUSPS over DINO-based saliency. The authors could investigate this further, e.g., by swapping DeepUSPS for the object proposal part of DeepSpectral (namely, spectral decomposition). 

(3) In most figures, it is notable that this method still struggles with images containing more than one semantic category. This is due to the initial assumption that there is only a single object per image (i.e., assigning a single cluster ID to the entire salient region in an image). Despite self-training, the model does not seem to fully recover from this assumption, especially in PASCAL VOC where the majority of images do indeed contain a single object, so there is little training signal. This likely can be measured, e.g., by measuring the performance and predicted object count on images containing more than one object 

(4) The authors could also consider running experiments on COCO-Stuff, to facilitate comparisons with other state-of-the-art methods, such as STEGO (Hamilton et al., 2022) and PiCIE (Cho et al., 2021).  

(5) The authors should also consider citing, discussing, and (where possible) comparing to the following related methods:

[1] Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)

[2] Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)

[3] Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
","Regarding clarity, the paper is well-structured, straightforward, and easy to follow. Enough details are provided that make the paper mostly reproducible. However, due to the reasons outlined above, the proposed idea is not very original and thus the paper lacks in terms of novelty. ","Although this paper achieves state-of-the-art results on the task of unsupervised semantic segmentation by a large margin, these results may be (to a large degree) due to the superior saliency estimation method that is used in this paper in contrast to prior work. The rest of the pipeline is very similar to that of (Melas-Kyriazi et al., 2022). As a result, I find that the novelty of this paper is not significant enough to warrant acceptance to ICLR.  ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666590369295,,1_jFneF07YC,ICLR.cc/2023/Conference/Paper3900/-/Official_Review,,[],1,,['everyone'],1_jFneF07YC,['ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH'],1666590369295,,1673093379278,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that the authors should consider citing, discussing, and comparing to:

1. Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)
2. Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)
3. Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
4. Hamilton et al., ""STEGO"" (2022)
5. Cho et al., ""PiCIE"" (2021)

The reviewer also suggests running experiments on COCO-Stuff to facilitate comparisons with other state-of-the-art methods, such as STEGO and PiCIE.",1,"2022, 2022, 2022, 2022, 2021",STEGO
XsbK100ZqV,"The paper addresses the problem of unsupervised semantic segmentation from images, which is relatively new in the literature. The methodology is based on saliency detection, clustering of self-supervised features (e.g., DINO) within the salient regions to obtain pseudo-masks, followed by a few iterations of self-training. This method achieves state-of-the-art results on unsupervised semantic segmentation on PASCAL-VOC and MS COCO.  ","
**Strengths**

The proposed method is simple, yet effective as it achieves over 10 points mIoU above prior work. Results are also shown for the first time on the challenging MS COCO dataset (80 classes).   

The method directly builds on pre-trained representations, with optional self-training. As such, and in contrast to some other approaches, it does not require costly self-supervised pre-training at object level (i.e., dense representations) to achieve a pixel-wise task, such as semantic segmentation.  


**Weaknesses**

(1) One major issue with the paper is its close resemblance to DeepSpectral (Melas-Kyriazi et al., 2022), although the authors do not discuss this explicitly. DeepSpectral uses a similar pipeline for (unsupervised) semantic segmentation: unsupervised extraction of image regions, clustering for categorizing these regions, and self-training. The most notable difference appears to be the choice of method for the extraction of relevant image regions, that is DeepUSPS for this paper and spectral clustering for DeepSpectral. The rest of the pipeline seems to be quite similar with minor changes (spectral vs k-means clustering, or the number of iterations during self-training). This limits the technical contribution of the paper, despite its strong performance. Therefore, the authors should provide an extensive discussion of the differences to DeepSpectral and highlight their significance. 

(2) The approach relies on a saliency detection method (DeepUSPS/BASNet) to get an initial set of object masks. Despite common belief, I would argue that DeepUSPS may not be considered *truly* unsupervised since:  
 - it distills an ensemble of *handcrafted* priors
 - it uses *supervised* pretraining (e.g., on ImageNet or Cityscapes according to (Nguyen et al., 2019)) 

This fact gives an inherent advantage to DeepUSPS vs fully unsupervised approaches for saliency detection. This is also clear from Table 7, where DeepUSPS performs significantly better than approaches such as LOST or DINOSeg which only rely on self-supervised features to estimate saliency. This in turn gives a clear advantage to the object proposal masks used by the rest of the method. 

Notably, DeepSpectral also relies on self-supervised features (e.g., DINO) for object proposal masks. Therefore, the question that arises is whether the biggest part of the performance boost of this paper could be simply attributed to the advantage of DeepUSPS over DINO-based saliency. The authors could investigate this further, e.g., by swapping DeepUSPS for the object proposal part of DeepSpectral (namely, spectral decomposition). 

(3) In most figures, it is notable that this method still struggles with images containing more than one semantic category. This is due to the initial assumption that there is only a single object per image (i.e., assigning a single cluster ID to the entire salient region in an image). Despite self-training, the model does not seem to fully recover from this assumption, especially in PASCAL VOC where the majority of images do indeed contain a single object, so there is little training signal. This likely can be measured, e.g., by measuring the performance and predicted object count on images containing more than one object 

(4) The authors could also consider running experiments on COCO-Stuff, to facilitate comparisons with other state-of-the-art methods, such as STEGO (Hamilton et al., 2022) and PiCIE (Cho et al., 2021).  

(5) The authors should also consider citing, discussing, and (where possible) comparing to the following related methods:

[1] Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)

[2] Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)

[3] Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
","Regarding clarity, the paper is well-structured, straightforward, and easy to follow. Enough details are provided that make the paper mostly reproducible. However, due to the reasons outlined above, the proposed idea is not very original and thus the paper lacks in terms of novelty. ","Although this paper achieves state-of-the-art results on the task of unsupervised semantic segmentation by a large margin, these results may be (to a large degree) due to the superior saliency estimation method that is used in this paper in contrast to prior work. The rest of the pipeline is very similar to that of (Melas-Kyriazi et al., 2022). As a result, I find that the novelty of this paper is not significant enough to warrant acceptance to ICLR.  ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666590369295,,1_jFneF07YC,ICLR.cc/2023/Conference/Paper3900/-/Official_Review,,[],1,,['everyone'],1_jFneF07YC,['ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH'],1666590369295,,1673093379278,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3900/Reviewer_6jsH']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that the authors should consider citing, discussing, and comparing to:

1. Ziegler and Asano, “Self-Supervised Learning of Object Parts for Semantic Segmentation.” (CVPR 2022)
2. Ke et al. ""Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers."" (CVPR 2022)
3. Henaff et al., “Object discovery and representation networks.” (ECCV 2022)
4. Hamilton et al., ""STEGO"" (2022)
5. Cho et al., ""PiCIE"" (2021)

The reviewer also suggests running experiments on COCO-Stuff to facilitate comparisons with other state-of-the-art methods, such as STEGO and PiCIE.",1,"2022, 2022, 2022, 2022, 2021",PiCIE
d24YNN63vmX,"The authors proposed a robust GAN-inversion method to cope with restoring images with unknown corruption and detecting unknown defects. They addressed the problem by involving the mask in the optimization process, and encouraging it to be sparse. Besides, to further close the GAN gap, they proposed to finetune the GAN generators while adapting new datasets. The authors showed the experimental results on both mask-free inpainting and unsupervised defect detection tasks, and claim it outperforms SOTA, and can be applied to other SOTA framework easily. ","Strength:
- The paper is well-written and easy to follow. The two tasks are closely related and the proposed methods make sense.
- The two-stage optimization further closes the GAN gaps. The generalization of the method is intuitive.

Weaknesses:
- The idea is simple, effective and intuitive, but the problem setting is not that practical. 
    (1) Center block is too easy to be overfitted, and it cannot reveal the advantages of the mask learning. 
    (2) Random missing is also not practical enough, and a simple denoising method may be more effective in resolving the problem.
    (3) Synthetic defects are limited, and cannot be proven to be generalized to real tasks. 
- The sparsity assumption is limited. Similar ideas can also be found in paper [1], while the authors of that inpainting paper proposed to jointly optimize the mask and reconstruction loss during training.  
- It would be interesting to study whether we can learn the regional GAN-inversion by matching partial generated images with the ground truth. For example, whether we can optimize the inversion network to only extract the foreground objects or salient objects. It may be an interesting extension of this work. 
- Finetunning the generator may be not that practical for real applications. 

[1] Zeng, Yu, et al. ""High-resolution image inpainting with iterative confidence feedback and guided upsampling."" European conference on computer vision. Springer, Cham, 2020.",The writing quality is good. ,"The major concerns of this paper:
- The synthetic dataset (for both tasks) fail to convince the reviewer that the proposed method is effective enough to recover the unknown corruption in the wild. If the author claimed only synthetic dataset can be used due to the lack of clean images for training GANs, then there are additional concerns on how the proposed methods can be generalized to more real-world tasks. Or the problems are not about the synthetic dataset, but the synthetic data (center block / random pixel missing) in this paper is not that realistic and diverse to reveal the advantages of the proposed method.
- The evaluation is poor, especially for the mask-free inpainting tasks. Baselines are weak, and the metrics are not advanced to demonstrate the visual quality. For GAN-based generation, at least LPIPS/FID should also be reported. 
- More interesting topics like partial GAN-inversion should be explored to prove the robustness. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666635725485,,1UbNwQC89a,ICLR.cc/2023/Conference/Paper2203/-/Official_Review,,[],4,,['everyone'],1UbNwQC89a,['ICLR.cc/2023/Conference/Paper2203/Reviewer_fpcW'],1666635725485,,1669715771657,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2203/Reviewer_fpcW']","Yes, the peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer mentions the paper ""High-resolution image inpainting with iterative confidence feedback and guided upsampling"" by Zeng et al. (2020), which is not mentioned in the original paper. The reviewer uses this reference to point out that similar ideas to the authors' sparsity assumption can be found in this paper, and that the authors of that paper proposed to jointly optimize the mask and reconstruction loss during training.",1,2020,High-resolution image inpainting with iterative confidence feedback and guided upsampling
0n6LT8cjLA9,"This paper presents GMM-based robot policy optimization formulated as a Wasserstein gradient flow, resulting in constraining policy updates for a stable optimization process. The proposed method is compared against two baselines PPO-based GMM update and SAC-GMM. The evaluations are performed in three tasks with a toy robot arm to reach a target, avoid collision during trajectory tracking, and reach multiple targets. The results demonstrate that their proposed approach (WGF) outperforms PPO and SAC in all three tasks. ","*Strength*
+ A novel Gradient flow approach for GMM optimization
+ Algothrim seems sound with theoretical backing.
+ The results demonstrate better performance than PPO and SAC

*Weaknesses*
- Experiment section is relatively weak. The robot tasks are too simple. Although WGF outperforms PPO and SAC on those tasks, a more cluttered scenario with a realistic robot arm (URDFs of UR5, Panda, etc.) would exhibit the applicability of the proposed approach to practical environments. Perhaps a relevant baseline for simulation environment setup could be [1].

[1] Continuous-time Gaussian process motion planning via probabilistic inference. The International Journal of Robotics Research, 37(11), 1319-1340.     ","Updated: Overall paper is well-written. The approach is novel for optimizing GMM-based robot policies. However, the experiments need significant improvement to validate the proposed approach under complex collision avoidance constraints.  

","This paper presents a stable approach to optimize GMM policies for robot control which also outperforms prior methods such as PPO and SAC. However, the experiments could have more complex environments to highlight the proposed approach's scalability better. Even if it does not scale, those evaluations will highlight a limitation to address in future work. 

Updated: The paper presents an interesting idea but lacks experiments that could validate the proposed approach under standard collision avoidance constraints for motion planning problems.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"3: reject, not good enough",1666630735417,,1UBSvnGHFxK,ICLR.cc/2023/Conference/Paper3839/-/Official_Review,,[],1,,['everyone'],1UBSvnGHFxK,['ICLR.cc/2023/Conference/Paper3839/Reviewer_1kyr'],1666630735417,,1669042738904,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3839/Reviewer_1kyr']","Yes, the peer review suggests the authors of the paper to refer to a specific literature that is not already discussed in the original paper. The reviewer mentions ""[1] Continuous-time Gaussian process motion planning via probabilistic inference. The International Journal of Robotics Research, 37(11), 1319-1340"" as a relevant baseline for simulation environment setup, implying that this reference is not already included in the original paper.",1,,Continuous-time Gaussian process motion planning via probabilistic inference
Q4CawzzQrGt,"The paper describes a novel diffusion model extending the concept to modelling irregularly sampled time series by utilizing correlated noise obtained from noise processes to produce continuous samples. The paper provide an elegant framework to combine forecasting, and imputation tasks.","One of the main advantage of the method is the elegant framework to combine all kinds of inference tasks for possibly sporadically observed time series. Forecasting (prediction in temporal causal direction), smoothing (prediction in both direction,) and imputation tasks can be formalized as conditional generation tasks in the framework.

One of the weakness is that little guidance is provided on how to select the noise process, and that not many benchmark method is provided, see later.","The paper is quite well written, and easy to follow. The method is clearly novel, generalizing the diffusion models to time series is a nontrivial task. In one hand the proper noise process need to be used, and algorithms for interpolation, imputation and forward prediction need to be given.

Can you please give the used definition of continuity in case of stochastic process samples. What is the exact constraint you have on the noise process?

Do you have any idea on the effect of the stochastic process kernel on the model? We see the difference between OU and GP in the results. Do you have any observations you can share with the reader on the effect of the shape of the noise curve, effect of gamma parameter etc. ?

It would be useful to compare with more methods as it is not clear that the Latent ODE (Rubanova et al. 2019)  is the best alternative method. There is a concurrently published work (De Brouwer et al. 2019) that applies Bayesian filtering in the NODE framework, and a later paper (Kideger et al 2020) using the formulation of controlled differential equations and in some situation outperforms the previous two,
It is clearly a biased list given I am familiar with the NODE literature more than other similar methods.

(De Brouwer et al. 2019) De Brouwer, Edward, et al. ""GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series."" Advances in neural information processing systems 32 (2019).
(Kideger et al 2020)  Kidger, Patrick, et al. ""Neural controlled differential equations for irregular time series."" Advances in Neural Information Processing Systems 33 (2020): 6696-6707.

Note that I feel the work is novel enough that it is useful for the community even if it will not over-perform every single baseline, it would still be quite useful to see a bit more benchmark, even for just a selected dataset.
","The paper is quite well written and the method is novel. A little bit more detailed comparison would benefit the paper, but in general I find the paper a valuable addition to state of the art.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,"4: The contributions are significant, and do not exist in prior works.",2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666715018873,,1TxMUE7cF6_,ICLR.cc/2023/Conference/Paper4257/-/Official_Review,,[],3,,['everyone'],1TxMUE7cF6_,['ICLR.cc/2023/Conference/Paper4257/Reviewer_ienA'],1666715018873,,1666715018873,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4257/Reviewer_ienA']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. De Brouwer et al. (2019) - ""GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series.""
2. Kidger et al. (2020) - ""Neural controlled differential equations for irregular time series.""

The reviewer also mentions another paper, Rubanova et al. (2019) - ""Latent ODE"", which might already be discussed in the original paper, but it's not clear from the review.

The reviewer suggests that comparing the proposed method with these additional papers would be beneficial for the paper, implying that they are not already included in the original paper.",1,"2019, 2020, 2019",GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series
Q4CawzzQrGt,"The paper describes a novel diffusion model extending the concept to modelling irregularly sampled time series by utilizing correlated noise obtained from noise processes to produce continuous samples. The paper provide an elegant framework to combine forecasting, and imputation tasks.","One of the main advantage of the method is the elegant framework to combine all kinds of inference tasks for possibly sporadically observed time series. Forecasting (prediction in temporal causal direction), smoothing (prediction in both direction,) and imputation tasks can be formalized as conditional generation tasks in the framework.

One of the weakness is that little guidance is provided on how to select the noise process, and that not many benchmark method is provided, see later.","The paper is quite well written, and easy to follow. The method is clearly novel, generalizing the diffusion models to time series is a nontrivial task. In one hand the proper noise process need to be used, and algorithms for interpolation, imputation and forward prediction need to be given.

Can you please give the used definition of continuity in case of stochastic process samples. What is the exact constraint you have on the noise process?

Do you have any idea on the effect of the stochastic process kernel on the model? We see the difference between OU and GP in the results. Do you have any observations you can share with the reader on the effect of the shape of the noise curve, effect of gamma parameter etc. ?

It would be useful to compare with more methods as it is not clear that the Latent ODE (Rubanova et al. 2019)  is the best alternative method. There is a concurrently published work (De Brouwer et al. 2019) that applies Bayesian filtering in the NODE framework, and a later paper (Kideger et al 2020) using the formulation of controlled differential equations and in some situation outperforms the previous two,
It is clearly a biased list given I am familiar with the NODE literature more than other similar methods.

(De Brouwer et al. 2019) De Brouwer, Edward, et al. ""GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series."" Advances in neural information processing systems 32 (2019).
(Kideger et al 2020)  Kidger, Patrick, et al. ""Neural controlled differential equations for irregular time series."" Advances in Neural Information Processing Systems 33 (2020): 6696-6707.

Note that I feel the work is novel enough that it is useful for the community even if it will not over-perform every single baseline, it would still be quite useful to see a bit more benchmark, even for just a selected dataset.
","The paper is quite well written and the method is novel. A little bit more detailed comparison would benefit the paper, but in general I find the paper a valuable addition to state of the art.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,"4: The contributions are significant, and do not exist in prior works.",2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666715018873,,1TxMUE7cF6_,ICLR.cc/2023/Conference/Paper4257/-/Official_Review,,[],3,,['everyone'],1TxMUE7cF6_,['ICLR.cc/2023/Conference/Paper4257/Reviewer_ienA'],1666715018873,,1666715018873,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4257/Reviewer_ienA']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. De Brouwer et al. (2019) - ""GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series.""
2. Kidger et al. (2020) - ""Neural controlled differential equations for irregular time series.""

The reviewer also mentions another paper, Rubanova et al. (2019) - ""Latent ODE"", which might already be discussed in the original paper, but it's not clear from the review.

The reviewer suggests that comparing the proposed method with these additional papers would be beneficial for the paper, implying that they are not already included in the original paper.",1,"2019, 2020, 2019",Neural controlled differential equations for irregular time series
pne0s6lk0lh,"The study provides evidence that a network architecture plays a significant role in contrastive SSL, by utilizing 116 variants of ResNet and MobileNet architecture, which were evaluated across 11 downstream tasks in the contrastive SSL setting.
It showed that no one architecture demonstrated a consistently good result, thus suggesting future researchers focus on learning architecture as well as the weights of the network in the SSL setting. They conducted two experiments: 1) network variation experiment on downstream tasks with observation of correlation between the models in downstream performance, and 2) applying NAS algorithm to the SSL setting to search for the optimal architectures on unlabeled pretraining dataset via contrastive learning.
","[Strengths]

S1: The study found out the implicit underlying assumption of the current literature and pointed out that it can be incorrect.

S2: The results have shown that the SSL architecture outperforms the handcrafted architectures, and have included concrete experiments regarding the distributional shift.

[Weaknesses]

W1: Only one optimization objective (SimCLR) has been used. They could have checked if the proposing method works across many different learning objectives.

W2: Idea of combing NAS and SSL is not novel.

W3: In the downstream transfer experiment, the searched architecture did not show promising performance on out-of-distribution dataset, except for the comparison of MobileNetV2, and this results are not surprising.

W4: Although 116 variations of network were experimented, there are just two backbone network architectures.","[Clarity]
1. Clarified the assumptions of the current research.
2. Clarified the main objective of the study is to show that the choice of network is highly impactful in SSL and handcrafting the architecture is very hard.
3. Authors raised research questions in a clear context.
4. Clearly states the three main contributions
5. Clearly explains the reason behind the choice of the NAS algorithm and dataset


[Quality]
1. Appropriate references were made.
2. Tables, figures, and appendix are effectively supporting the arguments made.


[Novelty]
This work possesses novelty to a certain degree in that the author(s) tried to use NAS in aid of SSL, and conducted a large scale experiments on the variant of networks. However, the novelty is not substantial (limited) as there are a number of work that have similar approach in combining NAS and SSL. A preprint titled ‘CSNAS: Contrastive Self-supervised Learning Neural Architecture Search via Sequential Model-Based Optimization’ (https://arxiv.org/abs/2102.10557) is one of the examples. Despite the fact, as author(s) mentioned, in the beginning of Section 4, there is a distinction between their idea and prior work.


[Reproducibility]
1. Provided the details of 1) how they varied the architecture of each network they used , 2) the number of epochs and batch size when pretraining with the machine they used for training.
2. Provided the metric used (Spearman’s rank correlation coefficient) when evaluating the correlation between ImageNet and downstream performance.","From the extensive study, readers can agree to the statement that there is no one architecture that performs well across different downstream tasks in SSL, and that NAS+SSL can be a solution for mitigating the problem, where researchers have to handcraft the network architecture. However, as mentioned in the section above, the novelty of this idea is limited in that the combination of NAS and SSL has been experimented widely in the literature.
However, this study opens up many possible research questions regarding the effectiveness on architecture search for SSL, thus can be act as a ground research of future studies.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1667849363410,,1KaSx3GrBBm,ICLR.cc/2023/Conference/Paper2775/-/Official_Review,,[],3,,['everyone'],1KaSx3GrBBm,['ICLR.cc/2023/Conference/Paper2775/Reviewer_6dkS'],1667849363410,,1667849729192,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2775/Reviewer_6dkS']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions a preprint titled 'CSNAS: Contrastive Self-supervised Learning Neural Architecture Search via Sequential Model-Based Optimization' (https://arxiv.org/abs/2102.10557) as an example of prior work that has a similar approach to combining NAS and SSL. This suggests that the reviewer is aware of this literature and believes it is relevant to the paper, but it is not clear if the authors have already discussed it in their paper.",1,2021,CSNAS: Contrastive Self-supervised Learning Neural Architecture Search via Sequential Model-Based Optimization
0sIVFi0b3ED,"The paper focuses on analyzing the prompt learning paradigm in CLIP. The authors discuss some observations on the use of various types of hand-crafted prompts including class names, basic prompts (e.g., ""a photo of a {CLASS}""), negative prompts (e.g., “this is not a photo of a {CLASS}”), and random prompts on 11 benchmark datasets. The highlighted observations include no major drop in performance using negative prompts. Additionally, the paper analyzes classifier fine-tuning as a faster alternative to prompt learning and shows superior performance compared to prompt learning. Experiments on few-shot classification, robustness to distribution shifts, and generalization to unseen classes are included using the CLIP model as the backbone.","**Strengths**:

- The paper is well written and easy to follow.
- The paper has tried towards analyzing the effect of different compositional handcrafted prompts and studying the performance behavior on several downstream classification datasets.

**Weaknesses**:

- Although the paper states “Vision-Language Models” in its title, the experiments are only performed on CLIP models. It would be great to see similar findings for other vision-language models like DeCLIP, FILIP, CLOOB, CyCLIP, etc. Given that the paper is more of an analysis paper instead of a methodology paper, I would expect authors to verify their claims on other CLIP models like CLIP ViT variants beside CLIP ResNet. Does the size of models effect the conclusions presented in this paper? A more thorough comparison and anlysis should be included in the paper.
- It is interesting to observe that the four types of handcrafted templates used for zero-shot classification provide accuracies in a similar range, but this requires further extensive analysis to draw concrete conclusions. The {CLASS} token is very much important in classifying the images and is present in all the templates which provides the major information to the text encoder. The negative prompt has all the words the same as the revised prompt except “not”, obtaining accuracies in the similar range is expected as the text encoder is a language model and they lack compositionality which is well studied in NLP [a], and also the way CLIP has been pre-trained using the contrastive loss doesn’t guarantee compositionality. For the random prompts using random tokens and embeddings, the same argument of the presence of the {CLASS} token applies. Thus, a fair comparison between different prompts would be to remove the class token and then compare their performance on the downstream tasks. It might require some modifications at the output as done in ""Learning to Decompose Visual Features with Latent Textual Prompts"".
- The paper proposes classifier fine-tuning as a faster alternative to prompt learning and shows an average improvement of around 1.5%, but there is no parameter analysis (i.e., number of tunable parameters) provided for this experiment. It is known from the CoOp paper [b] that ensembling improves the performance, I assume that the authors fine-tuned the final projection layer of the text encoder which is of dimension 512x1024 for CLIP-ResNet-50, which has almost 64 times more parameters than learning 16 prompt tokens of 512 dimension. Therefore, it is very much essential to have a fair comparison with CoOp wrt the number of parameters each method uses. A fair comparison would be to use 64 learnable prompts and run CoOp for it. But, I believe from observing the results provided in CoOp [b], only 8 learnable prompts would be able to perform as good as the classifier fine-tuning. This argument applies to all the results in Figure-1 and Table-4.
- Additionally, the very motivation to learn continuous prompts for large vision and language models was to efficiently adapt the knowledge in the models to downstream tasks, which makes classifier fine-tuning contrasting to the motivation of prompt learning. I would like the authors to discuss on how classifier finetuning is a better alternative to prompt tuning for parameter efficient adaptation of large vision-language models for diverse downstream tasks.
- It would be good to add the average values across the datasets in table-2.
- No experimental comparison with other parameter efficient adaptation methods for CLIP, like CLIP-adapter [c], Tip-Adapter [d], UPL [e], PDL [f] etc. has been made in the paper. Authors should compare with these methods to verify the effectiveness of the proposed method over existing methods.
- Figure-2 is not clear. What are the epoch values corresponding to the points for CoOp?
- I feel that the section on optimality-generalization trade-off is very subjective and lacks proper experimental evaluation and theoretical support. In the beginning the authors ask the question - “why would the prompt learning methods or the improved conditional prompt learning methods have strong generalization ability?”, but get an answer - “Our assumption is that, due to the changed architecture, the improved prompt learning method maybe actually trying to find a better optimality-generalization trade-off.” I am confused on what it means. Additionally, I am not sure how the experiment of training CoOp for multiple epochs helps support the claims. Isn’t it known that training for a higher number of epochs makes networks susceptible to overfitting and is expected to lose generalizability? Also, why was only CoOp run for multiple epochs and not CoCoOp?

[a] Evaluating Compositionality of Sentence Representation Models: https://aclanthology.org/2020.repl4nlp-1.22.pdf

[b] Learning to Prompt for Vision-Language Models: https://arxiv.org/pdf/2109.01134.pdf

[c] CLIP-Adapter: Better Vision-Language Models with Feature Adapters: https://arxiv.org/pdf/2110.04544.pdf

[d] Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling: https://arxiv.org/pdf/2207.09519.pdf

[e] Unsupervised Prompt Learning for Vision-Language Models: https://arxiv.org/pdf/2204.03649.pdf

[f] Prompt Distribution Learning: https://arxiv.org/pdf/2205.03340.pdf



","Although the paper is well-written, the paper lacks clarity on what exactly it tries to study and mitigate. The experimental analysis is not extensive and lacks proper justification to them. ",I think the identified problem is important but I’d like to rate the current submission as a clear rejection due to limited technical contributions and lack of convincing experiments. The paper needs significant changes including new experiments and possibly methodological improvements before being accepted to any major conference.,5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666819736943,,1FsdIfRngtw,ICLR.cc/2023/Conference/Paper6510/-/Official_Review,,[],2,,['everyone'],1FsdIfRngtw,['ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8'],1666819736943,,1666819736943,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the provided summary of the paper, including:

1. ""Evaluating Compositionality of Sentence Representation Models"" ([a])
2. ""Learning to Prompt for Vision-Language Models"" ([b])
3. ""CLIP-Adapter: Better Vision-Language Models with Feature Adapters"" ([c])
4. ""Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling"" ([d])
5. ""Unsupervised Prompt Learning for Vision-Language Models"" ([e])
6. ""Prompt Distribution Learning"" ([f])

These papers are mentioned as relevant to the topic and the reviewer suggests that the authors should compare their method with the methods presented in these papers, particularly in terms of parameter efficiency and performance.",1,,Evaluating Compositionality of Sentence Representation Models
0sIVFi0b3ED,"The paper focuses on analyzing the prompt learning paradigm in CLIP. The authors discuss some observations on the use of various types of hand-crafted prompts including class names, basic prompts (e.g., ""a photo of a {CLASS}""), negative prompts (e.g., “this is not a photo of a {CLASS}”), and random prompts on 11 benchmark datasets. The highlighted observations include no major drop in performance using negative prompts. Additionally, the paper analyzes classifier fine-tuning as a faster alternative to prompt learning and shows superior performance compared to prompt learning. Experiments on few-shot classification, robustness to distribution shifts, and generalization to unseen classes are included using the CLIP model as the backbone.","**Strengths**:

- The paper is well written and easy to follow.
- The paper has tried towards analyzing the effect of different compositional handcrafted prompts and studying the performance behavior on several downstream classification datasets.

**Weaknesses**:

- Although the paper states “Vision-Language Models” in its title, the experiments are only performed on CLIP models. It would be great to see similar findings for other vision-language models like DeCLIP, FILIP, CLOOB, CyCLIP, etc. Given that the paper is more of an analysis paper instead of a methodology paper, I would expect authors to verify their claims on other CLIP models like CLIP ViT variants beside CLIP ResNet. Does the size of models effect the conclusions presented in this paper? A more thorough comparison and anlysis should be included in the paper.
- It is interesting to observe that the four types of handcrafted templates used for zero-shot classification provide accuracies in a similar range, but this requires further extensive analysis to draw concrete conclusions. The {CLASS} token is very much important in classifying the images and is present in all the templates which provides the major information to the text encoder. The negative prompt has all the words the same as the revised prompt except “not”, obtaining accuracies in the similar range is expected as the text encoder is a language model and they lack compositionality which is well studied in NLP [a], and also the way CLIP has been pre-trained using the contrastive loss doesn’t guarantee compositionality. For the random prompts using random tokens and embeddings, the same argument of the presence of the {CLASS} token applies. Thus, a fair comparison between different prompts would be to remove the class token and then compare their performance on the downstream tasks. It might require some modifications at the output as done in ""Learning to Decompose Visual Features with Latent Textual Prompts"".
- The paper proposes classifier fine-tuning as a faster alternative to prompt learning and shows an average improvement of around 1.5%, but there is no parameter analysis (i.e., number of tunable parameters) provided for this experiment. It is known from the CoOp paper [b] that ensembling improves the performance, I assume that the authors fine-tuned the final projection layer of the text encoder which is of dimension 512x1024 for CLIP-ResNet-50, which has almost 64 times more parameters than learning 16 prompt tokens of 512 dimension. Therefore, it is very much essential to have a fair comparison with CoOp wrt the number of parameters each method uses. A fair comparison would be to use 64 learnable prompts and run CoOp for it. But, I believe from observing the results provided in CoOp [b], only 8 learnable prompts would be able to perform as good as the classifier fine-tuning. This argument applies to all the results in Figure-1 and Table-4.
- Additionally, the very motivation to learn continuous prompts for large vision and language models was to efficiently adapt the knowledge in the models to downstream tasks, which makes classifier fine-tuning contrasting to the motivation of prompt learning. I would like the authors to discuss on how classifier finetuning is a better alternative to prompt tuning for parameter efficient adaptation of large vision-language models for diverse downstream tasks.
- It would be good to add the average values across the datasets in table-2.
- No experimental comparison with other parameter efficient adaptation methods for CLIP, like CLIP-adapter [c], Tip-Adapter [d], UPL [e], PDL [f] etc. has been made in the paper. Authors should compare with these methods to verify the effectiveness of the proposed method over existing methods.
- Figure-2 is not clear. What are the epoch values corresponding to the points for CoOp?
- I feel that the section on optimality-generalization trade-off is very subjective and lacks proper experimental evaluation and theoretical support. In the beginning the authors ask the question - “why would the prompt learning methods or the improved conditional prompt learning methods have strong generalization ability?”, but get an answer - “Our assumption is that, due to the changed architecture, the improved prompt learning method maybe actually trying to find a better optimality-generalization trade-off.” I am confused on what it means. Additionally, I am not sure how the experiment of training CoOp for multiple epochs helps support the claims. Isn’t it known that training for a higher number of epochs makes networks susceptible to overfitting and is expected to lose generalizability? Also, why was only CoOp run for multiple epochs and not CoCoOp?

[a] Evaluating Compositionality of Sentence Representation Models: https://aclanthology.org/2020.repl4nlp-1.22.pdf

[b] Learning to Prompt for Vision-Language Models: https://arxiv.org/pdf/2109.01134.pdf

[c] CLIP-Adapter: Better Vision-Language Models with Feature Adapters: https://arxiv.org/pdf/2110.04544.pdf

[d] Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling: https://arxiv.org/pdf/2207.09519.pdf

[e] Unsupervised Prompt Learning for Vision-Language Models: https://arxiv.org/pdf/2204.03649.pdf

[f] Prompt Distribution Learning: https://arxiv.org/pdf/2205.03340.pdf



","Although the paper is well-written, the paper lacks clarity on what exactly it tries to study and mitigate. The experimental analysis is not extensive and lacks proper justification to them. ",I think the identified problem is important but I’d like to rate the current submission as a clear rejection due to limited technical contributions and lack of convincing experiments. The paper needs significant changes including new experiments and possibly methodological improvements before being accepted to any major conference.,5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666819736943,,1FsdIfRngtw,ICLR.cc/2023/Conference/Paper6510/-/Official_Review,,[],2,,['everyone'],1FsdIfRngtw,['ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8'],1666819736943,,1666819736943,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the provided summary of the paper, including:

1. ""Evaluating Compositionality of Sentence Representation Models"" ([a])
2. ""Learning to Prompt for Vision-Language Models"" ([b])
3. ""CLIP-Adapter: Better Vision-Language Models with Feature Adapters"" ([c])
4. ""Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling"" ([d])
5. ""Unsupervised Prompt Learning for Vision-Language Models"" ([e])
6. ""Prompt Distribution Learning"" ([f])

These papers are mentioned as relevant to the topic and the reviewer suggests that the authors should compare their method with the methods presented in these papers, particularly in terms of parameter efficiency and performance.",1,,Learning to Prompt for Vision-Language Models
0sIVFi0b3ED,"The paper focuses on analyzing the prompt learning paradigm in CLIP. The authors discuss some observations on the use of various types of hand-crafted prompts including class names, basic prompts (e.g., ""a photo of a {CLASS}""), negative prompts (e.g., “this is not a photo of a {CLASS}”), and random prompts on 11 benchmark datasets. The highlighted observations include no major drop in performance using negative prompts. Additionally, the paper analyzes classifier fine-tuning as a faster alternative to prompt learning and shows superior performance compared to prompt learning. Experiments on few-shot classification, robustness to distribution shifts, and generalization to unseen classes are included using the CLIP model as the backbone.","**Strengths**:

- The paper is well written and easy to follow.
- The paper has tried towards analyzing the effect of different compositional handcrafted prompts and studying the performance behavior on several downstream classification datasets.

**Weaknesses**:

- Although the paper states “Vision-Language Models” in its title, the experiments are only performed on CLIP models. It would be great to see similar findings for other vision-language models like DeCLIP, FILIP, CLOOB, CyCLIP, etc. Given that the paper is more of an analysis paper instead of a methodology paper, I would expect authors to verify their claims on other CLIP models like CLIP ViT variants beside CLIP ResNet. Does the size of models effect the conclusions presented in this paper? A more thorough comparison and anlysis should be included in the paper.
- It is interesting to observe that the four types of handcrafted templates used for zero-shot classification provide accuracies in a similar range, but this requires further extensive analysis to draw concrete conclusions. The {CLASS} token is very much important in classifying the images and is present in all the templates which provides the major information to the text encoder. The negative prompt has all the words the same as the revised prompt except “not”, obtaining accuracies in the similar range is expected as the text encoder is a language model and they lack compositionality which is well studied in NLP [a], and also the way CLIP has been pre-trained using the contrastive loss doesn’t guarantee compositionality. For the random prompts using random tokens and embeddings, the same argument of the presence of the {CLASS} token applies. Thus, a fair comparison between different prompts would be to remove the class token and then compare their performance on the downstream tasks. It might require some modifications at the output as done in ""Learning to Decompose Visual Features with Latent Textual Prompts"".
- The paper proposes classifier fine-tuning as a faster alternative to prompt learning and shows an average improvement of around 1.5%, but there is no parameter analysis (i.e., number of tunable parameters) provided for this experiment. It is known from the CoOp paper [b] that ensembling improves the performance, I assume that the authors fine-tuned the final projection layer of the text encoder which is of dimension 512x1024 for CLIP-ResNet-50, which has almost 64 times more parameters than learning 16 prompt tokens of 512 dimension. Therefore, it is very much essential to have a fair comparison with CoOp wrt the number of parameters each method uses. A fair comparison would be to use 64 learnable prompts and run CoOp for it. But, I believe from observing the results provided in CoOp [b], only 8 learnable prompts would be able to perform as good as the classifier fine-tuning. This argument applies to all the results in Figure-1 and Table-4.
- Additionally, the very motivation to learn continuous prompts for large vision and language models was to efficiently adapt the knowledge in the models to downstream tasks, which makes classifier fine-tuning contrasting to the motivation of prompt learning. I would like the authors to discuss on how classifier finetuning is a better alternative to prompt tuning for parameter efficient adaptation of large vision-language models for diverse downstream tasks.
- It would be good to add the average values across the datasets in table-2.
- No experimental comparison with other parameter efficient adaptation methods for CLIP, like CLIP-adapter [c], Tip-Adapter [d], UPL [e], PDL [f] etc. has been made in the paper. Authors should compare with these methods to verify the effectiveness of the proposed method over existing methods.
- Figure-2 is not clear. What are the epoch values corresponding to the points for CoOp?
- I feel that the section on optimality-generalization trade-off is very subjective and lacks proper experimental evaluation and theoretical support. In the beginning the authors ask the question - “why would the prompt learning methods or the improved conditional prompt learning methods have strong generalization ability?”, but get an answer - “Our assumption is that, due to the changed architecture, the improved prompt learning method maybe actually trying to find a better optimality-generalization trade-off.” I am confused on what it means. Additionally, I am not sure how the experiment of training CoOp for multiple epochs helps support the claims. Isn’t it known that training for a higher number of epochs makes networks susceptible to overfitting and is expected to lose generalizability? Also, why was only CoOp run for multiple epochs and not CoCoOp?

[a] Evaluating Compositionality of Sentence Representation Models: https://aclanthology.org/2020.repl4nlp-1.22.pdf

[b] Learning to Prompt for Vision-Language Models: https://arxiv.org/pdf/2109.01134.pdf

[c] CLIP-Adapter: Better Vision-Language Models with Feature Adapters: https://arxiv.org/pdf/2110.04544.pdf

[d] Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling: https://arxiv.org/pdf/2207.09519.pdf

[e] Unsupervised Prompt Learning for Vision-Language Models: https://arxiv.org/pdf/2204.03649.pdf

[f] Prompt Distribution Learning: https://arxiv.org/pdf/2205.03340.pdf



","Although the paper is well-written, the paper lacks clarity on what exactly it tries to study and mitigate. The experimental analysis is not extensive and lacks proper justification to them. ",I think the identified problem is important but I’d like to rate the current submission as a clear rejection due to limited technical contributions and lack of convincing experiments. The paper needs significant changes including new experiments and possibly methodological improvements before being accepted to any major conference.,5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666819736943,,1FsdIfRngtw,ICLR.cc/2023/Conference/Paper6510/-/Official_Review,,[],2,,['everyone'],1FsdIfRngtw,['ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8'],1666819736943,,1666819736943,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the provided summary of the paper, including:

1. ""Evaluating Compositionality of Sentence Representation Models"" ([a])
2. ""Learning to Prompt for Vision-Language Models"" ([b])
3. ""CLIP-Adapter: Better Vision-Language Models with Feature Adapters"" ([c])
4. ""Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling"" ([d])
5. ""Unsupervised Prompt Learning for Vision-Language Models"" ([e])
6. ""Prompt Distribution Learning"" ([f])

These papers are mentioned as relevant to the topic and the reviewer suggests that the authors should compare their method with the methods presented in these papers, particularly in terms of parameter efficiency and performance.",1,,CLIP-Adapter Better Vision-Language Models with Feature Adapters
0sIVFi0b3ED,"The paper focuses on analyzing the prompt learning paradigm in CLIP. The authors discuss some observations on the use of various types of hand-crafted prompts including class names, basic prompts (e.g., ""a photo of a {CLASS}""), negative prompts (e.g., “this is not a photo of a {CLASS}”), and random prompts on 11 benchmark datasets. The highlighted observations include no major drop in performance using negative prompts. Additionally, the paper analyzes classifier fine-tuning as a faster alternative to prompt learning and shows superior performance compared to prompt learning. Experiments on few-shot classification, robustness to distribution shifts, and generalization to unseen classes are included using the CLIP model as the backbone.","**Strengths**:

- The paper is well written and easy to follow.
- The paper has tried towards analyzing the effect of different compositional handcrafted prompts and studying the performance behavior on several downstream classification datasets.

**Weaknesses**:

- Although the paper states “Vision-Language Models” in its title, the experiments are only performed on CLIP models. It would be great to see similar findings for other vision-language models like DeCLIP, FILIP, CLOOB, CyCLIP, etc. Given that the paper is more of an analysis paper instead of a methodology paper, I would expect authors to verify their claims on other CLIP models like CLIP ViT variants beside CLIP ResNet. Does the size of models effect the conclusions presented in this paper? A more thorough comparison and anlysis should be included in the paper.
- It is interesting to observe that the four types of handcrafted templates used for zero-shot classification provide accuracies in a similar range, but this requires further extensive analysis to draw concrete conclusions. The {CLASS} token is very much important in classifying the images and is present in all the templates which provides the major information to the text encoder. The negative prompt has all the words the same as the revised prompt except “not”, obtaining accuracies in the similar range is expected as the text encoder is a language model and they lack compositionality which is well studied in NLP [a], and also the way CLIP has been pre-trained using the contrastive loss doesn’t guarantee compositionality. For the random prompts using random tokens and embeddings, the same argument of the presence of the {CLASS} token applies. Thus, a fair comparison between different prompts would be to remove the class token and then compare their performance on the downstream tasks. It might require some modifications at the output as done in ""Learning to Decompose Visual Features with Latent Textual Prompts"".
- The paper proposes classifier fine-tuning as a faster alternative to prompt learning and shows an average improvement of around 1.5%, but there is no parameter analysis (i.e., number of tunable parameters) provided for this experiment. It is known from the CoOp paper [b] that ensembling improves the performance, I assume that the authors fine-tuned the final projection layer of the text encoder which is of dimension 512x1024 for CLIP-ResNet-50, which has almost 64 times more parameters than learning 16 prompt tokens of 512 dimension. Therefore, it is very much essential to have a fair comparison with CoOp wrt the number of parameters each method uses. A fair comparison would be to use 64 learnable prompts and run CoOp for it. But, I believe from observing the results provided in CoOp [b], only 8 learnable prompts would be able to perform as good as the classifier fine-tuning. This argument applies to all the results in Figure-1 and Table-4.
- Additionally, the very motivation to learn continuous prompts for large vision and language models was to efficiently adapt the knowledge in the models to downstream tasks, which makes classifier fine-tuning contrasting to the motivation of prompt learning. I would like the authors to discuss on how classifier finetuning is a better alternative to prompt tuning for parameter efficient adaptation of large vision-language models for diverse downstream tasks.
- It would be good to add the average values across the datasets in table-2.
- No experimental comparison with other parameter efficient adaptation methods for CLIP, like CLIP-adapter [c], Tip-Adapter [d], UPL [e], PDL [f] etc. has been made in the paper. Authors should compare with these methods to verify the effectiveness of the proposed method over existing methods.
- Figure-2 is not clear. What are the epoch values corresponding to the points for CoOp?
- I feel that the section on optimality-generalization trade-off is very subjective and lacks proper experimental evaluation and theoretical support. In the beginning the authors ask the question - “why would the prompt learning methods or the improved conditional prompt learning methods have strong generalization ability?”, but get an answer - “Our assumption is that, due to the changed architecture, the improved prompt learning method maybe actually trying to find a better optimality-generalization trade-off.” I am confused on what it means. Additionally, I am not sure how the experiment of training CoOp for multiple epochs helps support the claims. Isn’t it known that training for a higher number of epochs makes networks susceptible to overfitting and is expected to lose generalizability? Also, why was only CoOp run for multiple epochs and not CoCoOp?

[a] Evaluating Compositionality of Sentence Representation Models: https://aclanthology.org/2020.repl4nlp-1.22.pdf

[b] Learning to Prompt for Vision-Language Models: https://arxiv.org/pdf/2109.01134.pdf

[c] CLIP-Adapter: Better Vision-Language Models with Feature Adapters: https://arxiv.org/pdf/2110.04544.pdf

[d] Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling: https://arxiv.org/pdf/2207.09519.pdf

[e] Unsupervised Prompt Learning for Vision-Language Models: https://arxiv.org/pdf/2204.03649.pdf

[f] Prompt Distribution Learning: https://arxiv.org/pdf/2205.03340.pdf



","Although the paper is well-written, the paper lacks clarity on what exactly it tries to study and mitigate. The experimental analysis is not extensive and lacks proper justification to them. ",I think the identified problem is important but I’d like to rate the current submission as a clear rejection due to limited technical contributions and lack of convincing experiments. The paper needs significant changes including new experiments and possibly methodological improvements before being accepted to any major conference.,5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666819736943,,1FsdIfRngtw,ICLR.cc/2023/Conference/Paper6510/-/Official_Review,,[],2,,['everyone'],1FsdIfRngtw,['ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8'],1666819736943,,1666819736943,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the provided summary of the paper, including:

1. ""Evaluating Compositionality of Sentence Representation Models"" ([a])
2. ""Learning to Prompt for Vision-Language Models"" ([b])
3. ""CLIP-Adapter: Better Vision-Language Models with Feature Adapters"" ([c])
4. ""Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling"" ([d])
5. ""Unsupervised Prompt Learning for Vision-Language Models"" ([e])
6. ""Prompt Distribution Learning"" ([f])

These papers are mentioned as relevant to the topic and the reviewer suggests that the authors should compare their method with the methods presented in these papers, particularly in terms of parameter efficiency and performance.",1,,Tip-Adapter Training-free CLIP-Adapter for Better Vision-Language Modeling
0sIVFi0b3ED,"The paper focuses on analyzing the prompt learning paradigm in CLIP. The authors discuss some observations on the use of various types of hand-crafted prompts including class names, basic prompts (e.g., ""a photo of a {CLASS}""), negative prompts (e.g., “this is not a photo of a {CLASS}”), and random prompts on 11 benchmark datasets. The highlighted observations include no major drop in performance using negative prompts. Additionally, the paper analyzes classifier fine-tuning as a faster alternative to prompt learning and shows superior performance compared to prompt learning. Experiments on few-shot classification, robustness to distribution shifts, and generalization to unseen classes are included using the CLIP model as the backbone.","**Strengths**:

- The paper is well written and easy to follow.
- The paper has tried towards analyzing the effect of different compositional handcrafted prompts and studying the performance behavior on several downstream classification datasets.

**Weaknesses**:

- Although the paper states “Vision-Language Models” in its title, the experiments are only performed on CLIP models. It would be great to see similar findings for other vision-language models like DeCLIP, FILIP, CLOOB, CyCLIP, etc. Given that the paper is more of an analysis paper instead of a methodology paper, I would expect authors to verify their claims on other CLIP models like CLIP ViT variants beside CLIP ResNet. Does the size of models effect the conclusions presented in this paper? A more thorough comparison and anlysis should be included in the paper.
- It is interesting to observe that the four types of handcrafted templates used for zero-shot classification provide accuracies in a similar range, but this requires further extensive analysis to draw concrete conclusions. The {CLASS} token is very much important in classifying the images and is present in all the templates which provides the major information to the text encoder. The negative prompt has all the words the same as the revised prompt except “not”, obtaining accuracies in the similar range is expected as the text encoder is a language model and they lack compositionality which is well studied in NLP [a], and also the way CLIP has been pre-trained using the contrastive loss doesn’t guarantee compositionality. For the random prompts using random tokens and embeddings, the same argument of the presence of the {CLASS} token applies. Thus, a fair comparison between different prompts would be to remove the class token and then compare their performance on the downstream tasks. It might require some modifications at the output as done in ""Learning to Decompose Visual Features with Latent Textual Prompts"".
- The paper proposes classifier fine-tuning as a faster alternative to prompt learning and shows an average improvement of around 1.5%, but there is no parameter analysis (i.e., number of tunable parameters) provided for this experiment. It is known from the CoOp paper [b] that ensembling improves the performance, I assume that the authors fine-tuned the final projection layer of the text encoder which is of dimension 512x1024 for CLIP-ResNet-50, which has almost 64 times more parameters than learning 16 prompt tokens of 512 dimension. Therefore, it is very much essential to have a fair comparison with CoOp wrt the number of parameters each method uses. A fair comparison would be to use 64 learnable prompts and run CoOp for it. But, I believe from observing the results provided in CoOp [b], only 8 learnable prompts would be able to perform as good as the classifier fine-tuning. This argument applies to all the results in Figure-1 and Table-4.
- Additionally, the very motivation to learn continuous prompts for large vision and language models was to efficiently adapt the knowledge in the models to downstream tasks, which makes classifier fine-tuning contrasting to the motivation of prompt learning. I would like the authors to discuss on how classifier finetuning is a better alternative to prompt tuning for parameter efficient adaptation of large vision-language models for diverse downstream tasks.
- It would be good to add the average values across the datasets in table-2.
- No experimental comparison with other parameter efficient adaptation methods for CLIP, like CLIP-adapter [c], Tip-Adapter [d], UPL [e], PDL [f] etc. has been made in the paper. Authors should compare with these methods to verify the effectiveness of the proposed method over existing methods.
- Figure-2 is not clear. What are the epoch values corresponding to the points for CoOp?
- I feel that the section on optimality-generalization trade-off is very subjective and lacks proper experimental evaluation and theoretical support. In the beginning the authors ask the question - “why would the prompt learning methods or the improved conditional prompt learning methods have strong generalization ability?”, but get an answer - “Our assumption is that, due to the changed architecture, the improved prompt learning method maybe actually trying to find a better optimality-generalization trade-off.” I am confused on what it means. Additionally, I am not sure how the experiment of training CoOp for multiple epochs helps support the claims. Isn’t it known that training for a higher number of epochs makes networks susceptible to overfitting and is expected to lose generalizability? Also, why was only CoOp run for multiple epochs and not CoCoOp?

[a] Evaluating Compositionality of Sentence Representation Models: https://aclanthology.org/2020.repl4nlp-1.22.pdf

[b] Learning to Prompt for Vision-Language Models: https://arxiv.org/pdf/2109.01134.pdf

[c] CLIP-Adapter: Better Vision-Language Models with Feature Adapters: https://arxiv.org/pdf/2110.04544.pdf

[d] Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling: https://arxiv.org/pdf/2207.09519.pdf

[e] Unsupervised Prompt Learning for Vision-Language Models: https://arxiv.org/pdf/2204.03649.pdf

[f] Prompt Distribution Learning: https://arxiv.org/pdf/2205.03340.pdf



","Although the paper is well-written, the paper lacks clarity on what exactly it tries to study and mitigate. The experimental analysis is not extensive and lacks proper justification to them. ",I think the identified problem is important but I’d like to rate the current submission as a clear rejection due to limited technical contributions and lack of convincing experiments. The paper needs significant changes including new experiments and possibly methodological improvements before being accepted to any major conference.,5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666819736943,,1FsdIfRngtw,ICLR.cc/2023/Conference/Paper6510/-/Official_Review,,[],2,,['everyone'],1FsdIfRngtw,['ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8'],1666819736943,,1666819736943,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the provided summary of the paper, including:

1. ""Evaluating Compositionality of Sentence Representation Models"" ([a])
2. ""Learning to Prompt for Vision-Language Models"" ([b])
3. ""CLIP-Adapter: Better Vision-Language Models with Feature Adapters"" ([c])
4. ""Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling"" ([d])
5. ""Unsupervised Prompt Learning for Vision-Language Models"" ([e])
6. ""Prompt Distribution Learning"" ([f])

These papers are mentioned as relevant to the topic and the reviewer suggests that the authors should compare their method with the methods presented in these papers, particularly in terms of parameter efficiency and performance.",1,,Unsupervised Prompt Learning for Vision-Language Models
0sIVFi0b3ED,"The paper focuses on analyzing the prompt learning paradigm in CLIP. The authors discuss some observations on the use of various types of hand-crafted prompts including class names, basic prompts (e.g., ""a photo of a {CLASS}""), negative prompts (e.g., “this is not a photo of a {CLASS}”), and random prompts on 11 benchmark datasets. The highlighted observations include no major drop in performance using negative prompts. Additionally, the paper analyzes classifier fine-tuning as a faster alternative to prompt learning and shows superior performance compared to prompt learning. Experiments on few-shot classification, robustness to distribution shifts, and generalization to unseen classes are included using the CLIP model as the backbone.","**Strengths**:

- The paper is well written and easy to follow.
- The paper has tried towards analyzing the effect of different compositional handcrafted prompts and studying the performance behavior on several downstream classification datasets.

**Weaknesses**:

- Although the paper states “Vision-Language Models” in its title, the experiments are only performed on CLIP models. It would be great to see similar findings for other vision-language models like DeCLIP, FILIP, CLOOB, CyCLIP, etc. Given that the paper is more of an analysis paper instead of a methodology paper, I would expect authors to verify their claims on other CLIP models like CLIP ViT variants beside CLIP ResNet. Does the size of models effect the conclusions presented in this paper? A more thorough comparison and anlysis should be included in the paper.
- It is interesting to observe that the four types of handcrafted templates used for zero-shot classification provide accuracies in a similar range, but this requires further extensive analysis to draw concrete conclusions. The {CLASS} token is very much important in classifying the images and is present in all the templates which provides the major information to the text encoder. The negative prompt has all the words the same as the revised prompt except “not”, obtaining accuracies in the similar range is expected as the text encoder is a language model and they lack compositionality which is well studied in NLP [a], and also the way CLIP has been pre-trained using the contrastive loss doesn’t guarantee compositionality. For the random prompts using random tokens and embeddings, the same argument of the presence of the {CLASS} token applies. Thus, a fair comparison between different prompts would be to remove the class token and then compare their performance on the downstream tasks. It might require some modifications at the output as done in ""Learning to Decompose Visual Features with Latent Textual Prompts"".
- The paper proposes classifier fine-tuning as a faster alternative to prompt learning and shows an average improvement of around 1.5%, but there is no parameter analysis (i.e., number of tunable parameters) provided for this experiment. It is known from the CoOp paper [b] that ensembling improves the performance, I assume that the authors fine-tuned the final projection layer of the text encoder which is of dimension 512x1024 for CLIP-ResNet-50, which has almost 64 times more parameters than learning 16 prompt tokens of 512 dimension. Therefore, it is very much essential to have a fair comparison with CoOp wrt the number of parameters each method uses. A fair comparison would be to use 64 learnable prompts and run CoOp for it. But, I believe from observing the results provided in CoOp [b], only 8 learnable prompts would be able to perform as good as the classifier fine-tuning. This argument applies to all the results in Figure-1 and Table-4.
- Additionally, the very motivation to learn continuous prompts for large vision and language models was to efficiently adapt the knowledge in the models to downstream tasks, which makes classifier fine-tuning contrasting to the motivation of prompt learning. I would like the authors to discuss on how classifier finetuning is a better alternative to prompt tuning for parameter efficient adaptation of large vision-language models for diverse downstream tasks.
- It would be good to add the average values across the datasets in table-2.
- No experimental comparison with other parameter efficient adaptation methods for CLIP, like CLIP-adapter [c], Tip-Adapter [d], UPL [e], PDL [f] etc. has been made in the paper. Authors should compare with these methods to verify the effectiveness of the proposed method over existing methods.
- Figure-2 is not clear. What are the epoch values corresponding to the points for CoOp?
- I feel that the section on optimality-generalization trade-off is very subjective and lacks proper experimental evaluation and theoretical support. In the beginning the authors ask the question - “why would the prompt learning methods or the improved conditional prompt learning methods have strong generalization ability?”, but get an answer - “Our assumption is that, due to the changed architecture, the improved prompt learning method maybe actually trying to find a better optimality-generalization trade-off.” I am confused on what it means. Additionally, I am not sure how the experiment of training CoOp for multiple epochs helps support the claims. Isn’t it known that training for a higher number of epochs makes networks susceptible to overfitting and is expected to lose generalizability? Also, why was only CoOp run for multiple epochs and not CoCoOp?

[a] Evaluating Compositionality of Sentence Representation Models: https://aclanthology.org/2020.repl4nlp-1.22.pdf

[b] Learning to Prompt for Vision-Language Models: https://arxiv.org/pdf/2109.01134.pdf

[c] CLIP-Adapter: Better Vision-Language Models with Feature Adapters: https://arxiv.org/pdf/2110.04544.pdf

[d] Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling: https://arxiv.org/pdf/2207.09519.pdf

[e] Unsupervised Prompt Learning for Vision-Language Models: https://arxiv.org/pdf/2204.03649.pdf

[f] Prompt Distribution Learning: https://arxiv.org/pdf/2205.03340.pdf



","Although the paper is well-written, the paper lacks clarity on what exactly it tries to study and mitigate. The experimental analysis is not extensive and lacks proper justification to them. ",I think the identified problem is important but I’d like to rate the current submission as a clear rejection due to limited technical contributions and lack of convincing experiments. The paper needs significant changes including new experiments and possibly methodological improvements before being accepted to any major conference.,5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666819736943,,1FsdIfRngtw,ICLR.cc/2023/Conference/Paper6510/-/Official_Review,,[],2,,['everyone'],1FsdIfRngtw,['ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8'],1666819736943,,1666819736943,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6510/Reviewer_Tvm8']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not referenced in the provided summary of the paper, including:

1. ""Evaluating Compositionality of Sentence Representation Models"" ([a])
2. ""Learning to Prompt for Vision-Language Models"" ([b])
3. ""CLIP-Adapter: Better Vision-Language Models with Feature Adapters"" ([c])
4. ""Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling"" ([d])
5. ""Unsupervised Prompt Learning for Vision-Language Models"" ([e])
6. ""Prompt Distribution Learning"" ([f])

These papers are mentioned as relevant to the topic and the reviewer suggests that the authors should compare their method with the methods presented in these papers, particularly in terms of parameter efficiency and performance.",1,,Prompt Distribution Learning
DEiIvTSzHPM,"The authors propose a new task, music-to-text synaesthesia, in which audio features from musical tracks are extracted and interpreted as textual descriptions.  The authors collect a dataset composed of classical recordings and a set of manually annotated textual descriptions, and evaluate a multi-modal encoder-decoder model on the task.  They propose a topology preserving loss to propogate some learning signal from the text (the similarities between the text descriptions of different examples) to guide the learning of the music encoder, and show this loss improves the quality of text descriptions (measured in BLEU) over the baseline contrastive or triplet loss.
","The authors begin by laying out an awkward case, defining translating information between modalities as synaesthesia, and therefore claiming that speech recognition and image captioning as forms of synaesthesia.  I don't want to argue the particular semantics of this, only to say that if synaesthesia here is going to be applied so broadly, it loses any important meaning and misleads the reader.  Being familiar with the definitions of synaesthesia as a -perceptual- phenomenon, I certainly thought there would be deeper cognitive connections, so felt a bit mislead by the title. Alternatively, in terms of modeling, I would have expected something more along the lines of [2], where more emphasis is put on a shared latent space (not unlike earlier sorts of image captioning - Socher, etc.).  

But if image captioning is a form of synaesthesia, then there is another name for such research that springs to mind: music captioning.  Looking down this line of research, a number of related work pops up [1-4].  And in terms of music labeling vs. music ""synaesthesia"", if the only difference between these is whether an output vector is mapped to one label from a set of many, or decoded into many words, it seems a weak point from which to pitch this research as having an important distinction from other related audio->text tasks.  So while I think the general idea of learning to predict textual descriptions from raw music audio is interesting and should be pursued, the presentation of this paper falls short in terms of recognizing existing work, or establishing a novel task.

It terms of the dataset, it seems useful but is limited in both size and the domain of being a specific set of classical music pieces.  On the text side, the descriptions (as viewed through the generating text descriptions) do not seem that diverse nor bring something by virtue of being full sentences that could not have been conveyed by bag of words.

The experimental setup is reasonable given the lack of established baselines for this type of task, yet, they also seem weak enough or inherently disadvantaged that they serve little competitive function.  The two experiments that are more interesting are with/without the topographic loss modifications.  The GTP loss itself seems well-motivated and may be only applicable to datasets of this sort, but stands out as one of the novel contributions.  However, when we see the generated text descriptions in Table 2, it led me to believe the learning problem on this data is quite simple, and might be the classification of just a few categories, strung together with text, and much less of a newly established task where the text descriptions themselves offer something qualitatively more informative than a multi-label classification task.


[1]
@article{Choi2016TowardsMC,
  title={Towards Music Captioning: Generating Music Playlist Descriptions},
  author={Keunwoo Choi and Gy{\""o}rgy Fazekas and Mark B. Sandler},
  journal={ArXiv},
  year={2016},
  volume={abs/1608.04868}
}

[2]
@article{Manco2022ContrastiveAL,
  title={Contrastive Audio-Language Learning for Music},
  author={Ilaria Manco and Emmanouil Benetos and Elio Quinton and Gy{\""o}rgy Fazekas},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.12208}
}

[3]
@article{Manco2021MusCapsGC,
  title={MusCaps: Generating Captions for Music Audio},
  author={Ilaria Manco and Emmanouil Benetos and Elio Quinton and Gy{\""o}rgy Fazekas},
  journal={2021 International Joint Conference on Neural Networks (IJCNN)},
  year={2021},
  pages={1-8}
}

[4]
And arguably:
@article{Gao2022MusicQA,
  title={Music Question Answering:Cognize and Perceive Music},
  author={Wenhao Gao and Xiaobing Li and Cong Jin and Tie Yun},
  journal={2022 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
  year={2022},
  pages={1-6}
}


Equation (4), BLEU vs. later BLUE.",The presentation was reasonably clear and there would not be too much difficulty in reproducing most of the results.  However it is a bit lacking in terms of novelty and substance.,"In favor of this paper, a new dataset of paired classical music pieces and text descriptions is collected, and a number of models are evaluated on it.  It's reasonable clear in most of its presentation, and the evaluation is probably sufficiently thorough.

On the other hand, it is questionable how novel this task really is, as defined here and on this dataset, over previous music captioning and music labeling work.  There is also minimal novelty on the modeling side, really boiling down to the topology preserving loss function.  Subjectively, studying the generated descriptions did not make a compelling case for the descriptions being more informative than having a small set of text labels (albeit necessary to have more than the single labels gleaned from the music categories).  I found the general motivation of the task confusing and overlooking previous related work.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1667043567922,,1FsLDqHivn4,ICLR.cc/2023/Conference/Paper1869/-/Official_Review,,[],2,,['everyone'],1FsLDqHivn4,['ICLR.cc/2023/Conference/Paper1869/Reviewer_DjXC'],1667043567922,,1667043567922,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1869/Reviewer_DjXC']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-4]) that are related to music captioning and music labeling, which they believe the authors should be aware of and discuss in their paper. These papers are:

* Choi et al. (2016) - ""Towards Music Captioning: Generating Music Playlist Descriptions""
* Manco et al. (2022) - ""Contrastive Audio-Language Learning for Music""
* Manco et al. (2021) - ""MusCaps: Generating Captions for Music Audio""
* Gao et al. (2022) - ""Music Question Answering: Cognize and Perceive Music""

The reviewer believes that these papers are relevant to the task proposed in the paper and that the authors should discuss them to establish the novelty and significance of their work.",1,"2016, 2022, 2021, 2022",Towards Music Captioning Generating Music Playlist Descriptions
DEiIvTSzHPM,"The authors propose a new task, music-to-text synaesthesia, in which audio features from musical tracks are extracted and interpreted as textual descriptions.  The authors collect a dataset composed of classical recordings and a set of manually annotated textual descriptions, and evaluate a multi-modal encoder-decoder model on the task.  They propose a topology preserving loss to propogate some learning signal from the text (the similarities between the text descriptions of different examples) to guide the learning of the music encoder, and show this loss improves the quality of text descriptions (measured in BLEU) over the baseline contrastive or triplet loss.
","The authors begin by laying out an awkward case, defining translating information between modalities as synaesthesia, and therefore claiming that speech recognition and image captioning as forms of synaesthesia.  I don't want to argue the particular semantics of this, only to say that if synaesthesia here is going to be applied so broadly, it loses any important meaning and misleads the reader.  Being familiar with the definitions of synaesthesia as a -perceptual- phenomenon, I certainly thought there would be deeper cognitive connections, so felt a bit mislead by the title. Alternatively, in terms of modeling, I would have expected something more along the lines of [2], where more emphasis is put on a shared latent space (not unlike earlier sorts of image captioning - Socher, etc.).  

But if image captioning is a form of synaesthesia, then there is another name for such research that springs to mind: music captioning.  Looking down this line of research, a number of related work pops up [1-4].  And in terms of music labeling vs. music ""synaesthesia"", if the only difference between these is whether an output vector is mapped to one label from a set of many, or decoded into many words, it seems a weak point from which to pitch this research as having an important distinction from other related audio->text tasks.  So while I think the general idea of learning to predict textual descriptions from raw music audio is interesting and should be pursued, the presentation of this paper falls short in terms of recognizing existing work, or establishing a novel task.

It terms of the dataset, it seems useful but is limited in both size and the domain of being a specific set of classical music pieces.  On the text side, the descriptions (as viewed through the generating text descriptions) do not seem that diverse nor bring something by virtue of being full sentences that could not have been conveyed by bag of words.

The experimental setup is reasonable given the lack of established baselines for this type of task, yet, they also seem weak enough or inherently disadvantaged that they serve little competitive function.  The two experiments that are more interesting are with/without the topographic loss modifications.  The GTP loss itself seems well-motivated and may be only applicable to datasets of this sort, but stands out as one of the novel contributions.  However, when we see the generated text descriptions in Table 2, it led me to believe the learning problem on this data is quite simple, and might be the classification of just a few categories, strung together with text, and much less of a newly established task where the text descriptions themselves offer something qualitatively more informative than a multi-label classification task.


[1]
@article{Choi2016TowardsMC,
  title={Towards Music Captioning: Generating Music Playlist Descriptions},
  author={Keunwoo Choi and Gy{\""o}rgy Fazekas and Mark B. Sandler},
  journal={ArXiv},
  year={2016},
  volume={abs/1608.04868}
}

[2]
@article{Manco2022ContrastiveAL,
  title={Contrastive Audio-Language Learning for Music},
  author={Ilaria Manco and Emmanouil Benetos and Elio Quinton and Gy{\""o}rgy Fazekas},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.12208}
}

[3]
@article{Manco2021MusCapsGC,
  title={MusCaps: Generating Captions for Music Audio},
  author={Ilaria Manco and Emmanouil Benetos and Elio Quinton and Gy{\""o}rgy Fazekas},
  journal={2021 International Joint Conference on Neural Networks (IJCNN)},
  year={2021},
  pages={1-8}
}

[4]
And arguably:
@article{Gao2022MusicQA,
  title={Music Question Answering:Cognize and Perceive Music},
  author={Wenhao Gao and Xiaobing Li and Cong Jin and Tie Yun},
  journal={2022 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
  year={2022},
  pages={1-6}
}


Equation (4), BLEU vs. later BLUE.",The presentation was reasonably clear and there would not be too much difficulty in reproducing most of the results.  However it is a bit lacking in terms of novelty and substance.,"In favor of this paper, a new dataset of paired classical music pieces and text descriptions is collected, and a number of models are evaluated on it.  It's reasonable clear in most of its presentation, and the evaluation is probably sufficiently thorough.

On the other hand, it is questionable how novel this task really is, as defined here and on this dataset, over previous music captioning and music labeling work.  There is also minimal novelty on the modeling side, really boiling down to the topology preserving loss function.  Subjectively, studying the generated descriptions did not make a compelling case for the descriptions being more informative than having a small set of text labels (albeit necessary to have more than the single labels gleaned from the music categories).  I found the general motivation of the task confusing and overlooking previous related work.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1667043567922,,1FsLDqHivn4,ICLR.cc/2023/Conference/Paper1869/-/Official_Review,,[],2,,['everyone'],1FsLDqHivn4,['ICLR.cc/2023/Conference/Paper1869/Reviewer_DjXC'],1667043567922,,1667043567922,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1869/Reviewer_DjXC']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-4]) that are related to music captioning and music labeling, which they believe the authors should be aware of and discuss in their paper. These papers are:

* Choi et al. (2016) - ""Towards Music Captioning: Generating Music Playlist Descriptions""
* Manco et al. (2022) - ""Contrastive Audio-Language Learning for Music""
* Manco et al. (2021) - ""MusCaps: Generating Captions for Music Audio""
* Gao et al. (2022) - ""Music Question Answering: Cognize and Perceive Music""

The reviewer believes that these papers are relevant to the task proposed in the paper and that the authors should discuss them to establish the novelty and significance of their work.",1,"2016, 2022, 2021, 2022",Contrastive Audio-Language Learning for Music
DEiIvTSzHPM,"The authors propose a new task, music-to-text synaesthesia, in which audio features from musical tracks are extracted and interpreted as textual descriptions.  The authors collect a dataset composed of classical recordings and a set of manually annotated textual descriptions, and evaluate a multi-modal encoder-decoder model on the task.  They propose a topology preserving loss to propogate some learning signal from the text (the similarities between the text descriptions of different examples) to guide the learning of the music encoder, and show this loss improves the quality of text descriptions (measured in BLEU) over the baseline contrastive or triplet loss.
","The authors begin by laying out an awkward case, defining translating information between modalities as synaesthesia, and therefore claiming that speech recognition and image captioning as forms of synaesthesia.  I don't want to argue the particular semantics of this, only to say that if synaesthesia here is going to be applied so broadly, it loses any important meaning and misleads the reader.  Being familiar with the definitions of synaesthesia as a -perceptual- phenomenon, I certainly thought there would be deeper cognitive connections, so felt a bit mislead by the title. Alternatively, in terms of modeling, I would have expected something more along the lines of [2], where more emphasis is put on a shared latent space (not unlike earlier sorts of image captioning - Socher, etc.).  

But if image captioning is a form of synaesthesia, then there is another name for such research that springs to mind: music captioning.  Looking down this line of research, a number of related work pops up [1-4].  And in terms of music labeling vs. music ""synaesthesia"", if the only difference between these is whether an output vector is mapped to one label from a set of many, or decoded into many words, it seems a weak point from which to pitch this research as having an important distinction from other related audio->text tasks.  So while I think the general idea of learning to predict textual descriptions from raw music audio is interesting and should be pursued, the presentation of this paper falls short in terms of recognizing existing work, or establishing a novel task.

It terms of the dataset, it seems useful but is limited in both size and the domain of being a specific set of classical music pieces.  On the text side, the descriptions (as viewed through the generating text descriptions) do not seem that diverse nor bring something by virtue of being full sentences that could not have been conveyed by bag of words.

The experimental setup is reasonable given the lack of established baselines for this type of task, yet, they also seem weak enough or inherently disadvantaged that they serve little competitive function.  The two experiments that are more interesting are with/without the topographic loss modifications.  The GTP loss itself seems well-motivated and may be only applicable to datasets of this sort, but stands out as one of the novel contributions.  However, when we see the generated text descriptions in Table 2, it led me to believe the learning problem on this data is quite simple, and might be the classification of just a few categories, strung together with text, and much less of a newly established task where the text descriptions themselves offer something qualitatively more informative than a multi-label classification task.


[1]
@article{Choi2016TowardsMC,
  title={Towards Music Captioning: Generating Music Playlist Descriptions},
  author={Keunwoo Choi and Gy{\""o}rgy Fazekas and Mark B. Sandler},
  journal={ArXiv},
  year={2016},
  volume={abs/1608.04868}
}

[2]
@article{Manco2022ContrastiveAL,
  title={Contrastive Audio-Language Learning for Music},
  author={Ilaria Manco and Emmanouil Benetos and Elio Quinton and Gy{\""o}rgy Fazekas},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.12208}
}

[3]
@article{Manco2021MusCapsGC,
  title={MusCaps: Generating Captions for Music Audio},
  author={Ilaria Manco and Emmanouil Benetos and Elio Quinton and Gy{\""o}rgy Fazekas},
  journal={2021 International Joint Conference on Neural Networks (IJCNN)},
  year={2021},
  pages={1-8}
}

[4]
And arguably:
@article{Gao2022MusicQA,
  title={Music Question Answering:Cognize and Perceive Music},
  author={Wenhao Gao and Xiaobing Li and Cong Jin and Tie Yun},
  journal={2022 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
  year={2022},
  pages={1-6}
}


Equation (4), BLEU vs. later BLUE.",The presentation was reasonably clear and there would not be too much difficulty in reproducing most of the results.  However it is a bit lacking in terms of novelty and substance.,"In favor of this paper, a new dataset of paired classical music pieces and text descriptions is collected, and a number of models are evaluated on it.  It's reasonable clear in most of its presentation, and the evaluation is probably sufficiently thorough.

On the other hand, it is questionable how novel this task really is, as defined here and on this dataset, over previous music captioning and music labeling work.  There is also minimal novelty on the modeling side, really boiling down to the topology preserving loss function.  Subjectively, studying the generated descriptions did not make a compelling case for the descriptions being more informative than having a small set of text labels (albeit necessary to have more than the single labels gleaned from the music categories).  I found the general motivation of the task confusing and overlooking previous related work.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1667043567922,,1FsLDqHivn4,ICLR.cc/2023/Conference/Paper1869/-/Official_Review,,[],2,,['everyone'],1FsLDqHivn4,['ICLR.cc/2023/Conference/Paper1869/Reviewer_DjXC'],1667043567922,,1667043567922,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1869/Reviewer_DjXC']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-4]) that are related to music captioning and music labeling, which they believe the authors should be aware of and discuss in their paper. These papers are:

* Choi et al. (2016) - ""Towards Music Captioning: Generating Music Playlist Descriptions""
* Manco et al. (2022) - ""Contrastive Audio-Language Learning for Music""
* Manco et al. (2021) - ""MusCaps: Generating Captions for Music Audio""
* Gao et al. (2022) - ""Music Question Answering: Cognize and Perceive Music""

The reviewer believes that these papers are relevant to the task proposed in the paper and that the authors should discuss them to establish the novelty and significance of their work.",1,"2016, 2022, 2021, 2022",MusCaps Generating Captions for Music Audio
DEiIvTSzHPM,"The authors propose a new task, music-to-text synaesthesia, in which audio features from musical tracks are extracted and interpreted as textual descriptions.  The authors collect a dataset composed of classical recordings and a set of manually annotated textual descriptions, and evaluate a multi-modal encoder-decoder model on the task.  They propose a topology preserving loss to propogate some learning signal from the text (the similarities between the text descriptions of different examples) to guide the learning of the music encoder, and show this loss improves the quality of text descriptions (measured in BLEU) over the baseline contrastive or triplet loss.
","The authors begin by laying out an awkward case, defining translating information between modalities as synaesthesia, and therefore claiming that speech recognition and image captioning as forms of synaesthesia.  I don't want to argue the particular semantics of this, only to say that if synaesthesia here is going to be applied so broadly, it loses any important meaning and misleads the reader.  Being familiar with the definitions of synaesthesia as a -perceptual- phenomenon, I certainly thought there would be deeper cognitive connections, so felt a bit mislead by the title. Alternatively, in terms of modeling, I would have expected something more along the lines of [2], where more emphasis is put on a shared latent space (not unlike earlier sorts of image captioning - Socher, etc.).  

But if image captioning is a form of synaesthesia, then there is another name for such research that springs to mind: music captioning.  Looking down this line of research, a number of related work pops up [1-4].  And in terms of music labeling vs. music ""synaesthesia"", if the only difference between these is whether an output vector is mapped to one label from a set of many, or decoded into many words, it seems a weak point from which to pitch this research as having an important distinction from other related audio->text tasks.  So while I think the general idea of learning to predict textual descriptions from raw music audio is interesting and should be pursued, the presentation of this paper falls short in terms of recognizing existing work, or establishing a novel task.

It terms of the dataset, it seems useful but is limited in both size and the domain of being a specific set of classical music pieces.  On the text side, the descriptions (as viewed through the generating text descriptions) do not seem that diverse nor bring something by virtue of being full sentences that could not have been conveyed by bag of words.

The experimental setup is reasonable given the lack of established baselines for this type of task, yet, they also seem weak enough or inherently disadvantaged that they serve little competitive function.  The two experiments that are more interesting are with/without the topographic loss modifications.  The GTP loss itself seems well-motivated and may be only applicable to datasets of this sort, but stands out as one of the novel contributions.  However, when we see the generated text descriptions in Table 2, it led me to believe the learning problem on this data is quite simple, and might be the classification of just a few categories, strung together with text, and much less of a newly established task where the text descriptions themselves offer something qualitatively more informative than a multi-label classification task.


[1]
@article{Choi2016TowardsMC,
  title={Towards Music Captioning: Generating Music Playlist Descriptions},
  author={Keunwoo Choi and Gy{\""o}rgy Fazekas and Mark B. Sandler},
  journal={ArXiv},
  year={2016},
  volume={abs/1608.04868}
}

[2]
@article{Manco2022ContrastiveAL,
  title={Contrastive Audio-Language Learning for Music},
  author={Ilaria Manco and Emmanouil Benetos and Elio Quinton and Gy{\""o}rgy Fazekas},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.12208}
}

[3]
@article{Manco2021MusCapsGC,
  title={MusCaps: Generating Captions for Music Audio},
  author={Ilaria Manco and Emmanouil Benetos and Elio Quinton and Gy{\""o}rgy Fazekas},
  journal={2021 International Joint Conference on Neural Networks (IJCNN)},
  year={2021},
  pages={1-8}
}

[4]
And arguably:
@article{Gao2022MusicQA,
  title={Music Question Answering:Cognize and Perceive Music},
  author={Wenhao Gao and Xiaobing Li and Cong Jin and Tie Yun},
  journal={2022 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
  year={2022},
  pages={1-6}
}


Equation (4), BLEU vs. later BLUE.",The presentation was reasonably clear and there would not be too much difficulty in reproducing most of the results.  However it is a bit lacking in terms of novelty and substance.,"In favor of this paper, a new dataset of paired classical music pieces and text descriptions is collected, and a number of models are evaluated on it.  It's reasonable clear in most of its presentation, and the evaluation is probably sufficiently thorough.

On the other hand, it is questionable how novel this task really is, as defined here and on this dataset, over previous music captioning and music labeling work.  There is also minimal novelty on the modeling side, really boiling down to the topology preserving loss function.  Subjectively, studying the generated descriptions did not make a compelling case for the descriptions being more informative than having a small set of text labels (albeit necessary to have more than the single labels gleaned from the music categories).  I found the general motivation of the task confusing and overlooking previous related work.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1667043567922,,1FsLDqHivn4,ICLR.cc/2023/Conference/Paper1869/-/Official_Review,,[],2,,['everyone'],1FsLDqHivn4,['ICLR.cc/2023/Conference/Paper1869/Reviewer_DjXC'],1667043567922,,1667043567922,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1869/Reviewer_DjXC']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1-4]) that are related to music captioning and music labeling, which they believe the authors should be aware of and discuss in their paper. These papers are:

* Choi et al. (2016) - ""Towards Music Captioning: Generating Music Playlist Descriptions""
* Manco et al. (2022) - ""Contrastive Audio-Language Learning for Music""
* Manco et al. (2021) - ""MusCaps: Generating Captions for Music Audio""
* Gao et al. (2022) - ""Music Question Answering: Cognize and Perceive Music""

The reviewer believes that these papers are relevant to the task proposed in the paper and that the authors should discuss them to establish the novelty and significance of their work.",1,"2016, 2022, 2021, 2022",Music Question Answering Cognize and Perceive Music
8lp9oA2iRa,"This paper studied the unbalancing problem in data generation based fairness, i.e, the generated data should be balanced or adjusted to control the prediction/fair performance with raw data. Based on this idea, an bi-level optimization (through implicit function) is proposed to learn the balancing coefficient $\lambda$, $\mu$. Empirical results on CelebA/ImageNet demonstrated its effectiveness. 
","### Pros
- This paper considered a meaningful problem in data-augmentation based fairness, i.e, rebalancing the training loss of generated and real data. Furthermore a bi-level objective is proposed to automatically balance the sensitive attribute and label balancing weights. The idea sounds reasonable and empirically evaluations are conducted. 

### Cons:
- This paper focused on a very narrow improvement in technical fair learning without proper motivation in social aspects. Thus I would say the paper scope and significance are quite modest. 
- The bi-level objective is not well-motivated. The framework could also be realized by DRO (distribution robust optimization).  
- Lack of clear discussion on previous work.

Overall I would say, this paper has merits, while they are rather modest in terms of technical part and understanding fairness.

### Comments on cons

1. [Paper scope] This paper focused on the specific aspect in data augmentation based fairness. I.e, data generation to improve unfair predictions. However, data generation through generative models is generally difficult. Thus I would be a bit unsure about the importance of fair generative models in data-augmented fair learning or general fair learning. 
2. [Technical motivation] The key contribution is to introduce the label and sensitive attribute reweighted loss to balance different factors. While I think this framework could also be achieved by the DRO framework by properly adjusting the weights of different groups/labels. Why not DRO? Is it necessarily to formulate the problem as a bi-level objective?
3. [Related work/Technical novelty] The bi-level objective in fair learning or data-augmentation are not entirely new. Discussions on related work seem quite lacking. For example, fair-mixup [1] as a typical data augmentation approach. As for the bi-level in learning fairness is not new, for example fair-path [2] indeed introduced bi-level objective and implicit function to learn fair representation. 
4. [Impact on real-world fairness] This paper focused on a nich technical point in learning the balancing coefficient. However, this paper does not consider the real-world fairness impact. Please take note that algorithmic fairness is not merely a method to improve state-of-the-art. An important aspect is to understand the social impact of proposed fair analysis. Unfortunately, this part is completely missing in the paper, making the contribution in fairness (non technical) quite modest. 
5. [Experiments, Impact on real-world fairness] The experiments are fine to validate the improved performance on benchmark but are not well-motivated to support the real-world fairness. We could always have better results on some benchmarks while the real-world discrimination still is not addressed. 
6. [Technical correctness] In Sec 2.1 and Sec 2.2 the loss constraint is not equivalent to enforce EO. They could be equivalent only in certains specific assumptions such as the loss is calibrated. This part is not correct for me. 
7. [Claims on achieving fair and accuracy] In many parts, this paper claimed the fair and accuracy could be both improved, this is quite a big claim. In fact, this paper did not provide rigorous justifications on why/when/how these could satisfy. I would think there is an over-claim of the contribution.  

Reference 
[1] Fair mixup: Fairness via interpolation. ICLR 2021
[2] Fair Representation Learning through Implicit Path Alignment. ICML 2022
","In general this paper is clearly written and easy to follow. The reproducibility seems high. 
","[ I would recommend that *the paper is generally fine, but not good enough* (4/10). While it seems that no such a score is provided. Thus I would currently recommend 3.]

This paper studied the unbalancing problem in data generation based fairness, i.e, the generated data should be balanced or adjusted to control the prediction/fair performance with raw data. Based on this idea, an bi-level optimization (through implicit function) is proposed. 

This paper has merits, while they are rather modest in technical parts (see 1-3, 6 in detailed comments) and understanding fairness (see 4-5, 7 in detailed comments).
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",1: The contributions are neither significant nor novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666639466794,,1FVv8PS8LYW,ICLR.cc/2023/Conference/Paper2270/-/Official_Review,,[],3,,['everyone'],1FVv8PS8LYW,['ICLR.cc/2023/Conference/Paper2270/Reviewer_ZWfo'],1666639466794,,1668784139738,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2270/Reviewer_ZWfo']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. ""Fair mixup: Fairness via interpolation"" (ICLR 2021) - [1]
2. ""Fair Representation Learning through Implicit Path Alignment"" (ICML 2022) - [2]

The reviewer uses these references to point out that the bi-level objective in fair learning or data-augmentation is not entirely new and that the paper lacks discussion on related work. This implies that the reviewer believes these papers are relevant to the topic and should be referenced and discussed in the original paper.",1,"2021, 2022",Fair mixup: Fairness via interpolation 
8lp9oA2iRa,"This paper studied the unbalancing problem in data generation based fairness, i.e, the generated data should be balanced or adjusted to control the prediction/fair performance with raw data. Based on this idea, an bi-level optimization (through implicit function) is proposed to learn the balancing coefficient $\lambda$, $\mu$. Empirical results on CelebA/ImageNet demonstrated its effectiveness. 
","### Pros
- This paper considered a meaningful problem in data-augmentation based fairness, i.e, rebalancing the training loss of generated and real data. Furthermore a bi-level objective is proposed to automatically balance the sensitive attribute and label balancing weights. The idea sounds reasonable and empirically evaluations are conducted. 

### Cons:
- This paper focused on a very narrow improvement in technical fair learning without proper motivation in social aspects. Thus I would say the paper scope and significance are quite modest. 
- The bi-level objective is not well-motivated. The framework could also be realized by DRO (distribution robust optimization).  
- Lack of clear discussion on previous work.

Overall I would say, this paper has merits, while they are rather modest in terms of technical part and understanding fairness.

### Comments on cons

1. [Paper scope] This paper focused on the specific aspect in data augmentation based fairness. I.e, data generation to improve unfair predictions. However, data generation through generative models is generally difficult. Thus I would be a bit unsure about the importance of fair generative models in data-augmented fair learning or general fair learning. 
2. [Technical motivation] The key contribution is to introduce the label and sensitive attribute reweighted loss to balance different factors. While I think this framework could also be achieved by the DRO framework by properly adjusting the weights of different groups/labels. Why not DRO? Is it necessarily to formulate the problem as a bi-level objective?
3. [Related work/Technical novelty] The bi-level objective in fair learning or data-augmentation are not entirely new. Discussions on related work seem quite lacking. For example, fair-mixup [1] as a typical data augmentation approach. As for the bi-level in learning fairness is not new, for example fair-path [2] indeed introduced bi-level objective and implicit function to learn fair representation. 
4. [Impact on real-world fairness] This paper focused on a nich technical point in learning the balancing coefficient. However, this paper does not consider the real-world fairness impact. Please take note that algorithmic fairness is not merely a method to improve state-of-the-art. An important aspect is to understand the social impact of proposed fair analysis. Unfortunately, this part is completely missing in the paper, making the contribution in fairness (non technical) quite modest. 
5. [Experiments, Impact on real-world fairness] The experiments are fine to validate the improved performance on benchmark but are not well-motivated to support the real-world fairness. We could always have better results on some benchmarks while the real-world discrimination still is not addressed. 
6. [Technical correctness] In Sec 2.1 and Sec 2.2 the loss constraint is not equivalent to enforce EO. They could be equivalent only in certains specific assumptions such as the loss is calibrated. This part is not correct for me. 
7. [Claims on achieving fair and accuracy] In many parts, this paper claimed the fair and accuracy could be both improved, this is quite a big claim. In fact, this paper did not provide rigorous justifications on why/when/how these could satisfy. I would think there is an over-claim of the contribution.  

Reference 
[1] Fair mixup: Fairness via interpolation. ICLR 2021
[2] Fair Representation Learning through Implicit Path Alignment. ICML 2022
","In general this paper is clearly written and easy to follow. The reproducibility seems high. 
","[ I would recommend that *the paper is generally fine, but not good enough* (4/10). While it seems that no such a score is provided. Thus I would currently recommend 3.]

This paper studied the unbalancing problem in data generation based fairness, i.e, the generated data should be balanced or adjusted to control the prediction/fair performance with raw data. Based on this idea, an bi-level optimization (through implicit function) is proposed. 

This paper has merits, while they are rather modest in technical parts (see 1-3, 6 in detailed comments) and understanding fairness (see 4-5, 7 in detailed comments).
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",1: The contributions are neither significant nor novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666639466794,,1FVv8PS8LYW,ICLR.cc/2023/Conference/Paper2270/-/Official_Review,,[],3,,['everyone'],1FVv8PS8LYW,['ICLR.cc/2023/Conference/Paper2270/Reviewer_ZWfo'],1666639466794,,1668784139738,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2270/Reviewer_ZWfo']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. ""Fair mixup: Fairness via interpolation"" (ICLR 2021) - [1]
2. ""Fair Representation Learning through Implicit Path Alignment"" (ICML 2022) - [2]

The reviewer uses these references to point out that the bi-level objective in fair learning or data-augmentation is not entirely new and that the paper lacks discussion on related work. This implies that the reviewer believes these papers are relevant to the topic and should be referenced and discussed in the original paper.",1,"2021, 2022",Fair Representation Learning through Implicit Path Alignment
yntP_LO3Y6,This paper presents a method to generate images from a collage of patches representing objects. Each patch is fed to a feature encoder and the resulting features are spatially concatenated such that their layout corresponds that of a collage where each object occupies a specific bounding box. The aggregated feature map is then fed to a decoder that is trained adversarially such that both the full image and each bounding box corresponding to the object are realistic.,"
Overall, the idea is nice and is very practical in terms of applications in the sense that it seems more intuitive to copy/paste objects in a scene to represent the target image that you had in mind rather than describing it with a long text (that can be cumbersome and not precise enough) or drawing a segmentation mask. The idea stems from image editing, but here the goal is different as you certainly do not want the resulting image to be a smooth blending of the input collage which would not be diverse enough as a creation tool.

There are weaknesses in the experimental setup and in the overall positionning. For the experiments, the results are not extremely convincing due to the lack of comparision with methods that can do the same. Only IC-GAN is tested, but there are many more GAN inversion methods that could take the image collage as input encode it to the GAN latent space and then decode it, like:
- Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans. arXiv:2103.10426, 2021.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.
- Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, 2020.
- Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing. arXiv:2111.15264, 2021
- Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models. arXiv:2210.08573, 2022. (this one came out after the submission deadline, so of course it cannot be compared to)

Additionally, the experiment of fitting the training set is not convincing at all. Memorizing the training set is essentially what it takes to obtain a good score in this test and it does not tell anything about generalization capabilities. Why not just reserve a validation set and perform this metric on anything else than training data?
Overall, the visual results are not super impressive. Maybe it is because the task is very hard, but comparing with editing methods (even back to blending methods that do not use deep learning), would have shown the difficulty of trade-off between fidelity and diversity.",The paper is clear enough.,"It's a good idea but the experimental validation is a bit lacking.

---
 The authors made efforts to address my concerns","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666678697247,,1CHhsUY32a,ICLR.cc/2023/Conference/Paper1116/-/Official_Review,,[],1,,['everyone'],1CHhsUY32a,['ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57'],1666678697247,,1671176196090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not compared to in the original paper, such as:

* Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans.
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space?
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images?
* Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing.
* Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing.
* Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models.

The reviewer suggests that the authors should compare their method to these existing methods to demonstrate its effectiveness.",1,,Using latent space regression to analyze and leverage compositionality in gans
yntP_LO3Y6,This paper presents a method to generate images from a collage of patches representing objects. Each patch is fed to a feature encoder and the resulting features are spatially concatenated such that their layout corresponds that of a collage where each object occupies a specific bounding box. The aggregated feature map is then fed to a decoder that is trained adversarially such that both the full image and each bounding box corresponding to the object are realistic.,"
Overall, the idea is nice and is very practical in terms of applications in the sense that it seems more intuitive to copy/paste objects in a scene to represent the target image that you had in mind rather than describing it with a long text (that can be cumbersome and not precise enough) or drawing a segmentation mask. The idea stems from image editing, but here the goal is different as you certainly do not want the resulting image to be a smooth blending of the input collage which would not be diverse enough as a creation tool.

There are weaknesses in the experimental setup and in the overall positionning. For the experiments, the results are not extremely convincing due to the lack of comparision with methods that can do the same. Only IC-GAN is tested, but there are many more GAN inversion methods that could take the image collage as input encode it to the GAN latent space and then decode it, like:
- Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans. arXiv:2103.10426, 2021.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.
- Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, 2020.
- Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing. arXiv:2111.15264, 2021
- Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models. arXiv:2210.08573, 2022. (this one came out after the submission deadline, so of course it cannot be compared to)

Additionally, the experiment of fitting the training set is not convincing at all. Memorizing the training set is essentially what it takes to obtain a good score in this test and it does not tell anything about generalization capabilities. Why not just reserve a validation set and perform this metric on anything else than training data?
Overall, the visual results are not super impressive. Maybe it is because the task is very hard, but comparing with editing methods (even back to blending methods that do not use deep learning), would have shown the difficulty of trade-off between fidelity and diversity.",The paper is clear enough.,"It's a good idea but the experimental validation is a bit lacking.

---
 The authors made efforts to address my concerns","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666678697247,,1CHhsUY32a,ICLR.cc/2023/Conference/Paper1116/-/Official_Review,,[],1,,['everyone'],1CHhsUY32a,['ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57'],1666678697247,,1671176196090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not compared to in the original paper, such as:

* Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans.
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space?
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images?
* Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing.
* Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing.
* Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models.

The reviewer suggests that the authors should compare their method to these existing methods to demonstrate its effectiveness.",1,,Image2stylegan How to embed images into the stylegan latent space
yntP_LO3Y6,This paper presents a method to generate images from a collage of patches representing objects. Each patch is fed to a feature encoder and the resulting features are spatially concatenated such that their layout corresponds that of a collage where each object occupies a specific bounding box. The aggregated feature map is then fed to a decoder that is trained adversarially such that both the full image and each bounding box corresponding to the object are realistic.,"
Overall, the idea is nice and is very practical in terms of applications in the sense that it seems more intuitive to copy/paste objects in a scene to represent the target image that you had in mind rather than describing it with a long text (that can be cumbersome and not precise enough) or drawing a segmentation mask. The idea stems from image editing, but here the goal is different as you certainly do not want the resulting image to be a smooth blending of the input collage which would not be diverse enough as a creation tool.

There are weaknesses in the experimental setup and in the overall positionning. For the experiments, the results are not extremely convincing due to the lack of comparision with methods that can do the same. Only IC-GAN is tested, but there are many more GAN inversion methods that could take the image collage as input encode it to the GAN latent space and then decode it, like:
- Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans. arXiv:2103.10426, 2021.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.
- Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, 2020.
- Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing. arXiv:2111.15264, 2021
- Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models. arXiv:2210.08573, 2022. (this one came out after the submission deadline, so of course it cannot be compared to)

Additionally, the experiment of fitting the training set is not convincing at all. Memorizing the training set is essentially what it takes to obtain a good score in this test and it does not tell anything about generalization capabilities. Why not just reserve a validation set and perform this metric on anything else than training data?
Overall, the visual results are not super impressive. Maybe it is because the task is very hard, but comparing with editing methods (even back to blending methods that do not use deep learning), would have shown the difficulty of trade-off between fidelity and diversity.",The paper is clear enough.,"It's a good idea but the experimental validation is a bit lacking.

---
 The authors made efforts to address my concerns","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666678697247,,1CHhsUY32a,ICLR.cc/2023/Conference/Paper1116/-/Official_Review,,[],1,,['everyone'],1CHhsUY32a,['ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57'],1666678697247,,1671176196090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not compared to in the original paper, such as:

* Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans.
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space?
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images?
* Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing.
* Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing.
* Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models.

The reviewer suggests that the authors should compare their method to these existing methods to demonstrate its effectiveness.",1,,Image2stylegan++ How to edit the embedded images
yntP_LO3Y6,This paper presents a method to generate images from a collage of patches representing objects. Each patch is fed to a feature encoder and the resulting features are spatially concatenated such that their layout corresponds that of a collage where each object occupies a specific bounding box. The aggregated feature map is then fed to a decoder that is trained adversarially such that both the full image and each bounding box corresponding to the object are realistic.,"
Overall, the idea is nice and is very practical in terms of applications in the sense that it seems more intuitive to copy/paste objects in a scene to represent the target image that you had in mind rather than describing it with a long text (that can be cumbersome and not precise enough) or drawing a segmentation mask. The idea stems from image editing, but here the goal is different as you certainly do not want the resulting image to be a smooth blending of the input collage which would not be diverse enough as a creation tool.

There are weaknesses in the experimental setup and in the overall positionning. For the experiments, the results are not extremely convincing due to the lack of comparision with methods that can do the same. Only IC-GAN is tested, but there are many more GAN inversion methods that could take the image collage as input encode it to the GAN latent space and then decode it, like:
- Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans. arXiv:2103.10426, 2021.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.
- Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, 2020.
- Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing. arXiv:2111.15264, 2021
- Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models. arXiv:2210.08573, 2022. (this one came out after the submission deadline, so of course it cannot be compared to)

Additionally, the experiment of fitting the training set is not convincing at all. Memorizing the training set is essentially what it takes to obtain a good score in this test and it does not tell anything about generalization capabilities. Why not just reserve a validation set and perform this metric on anything else than training data?
Overall, the visual results are not super impressive. Maybe it is because the task is very hard, but comparing with editing methods (even back to blending methods that do not use deep learning), would have shown the difficulty of trade-off between fidelity and diversity.",The paper is clear enough.,"It's a good idea but the experimental validation is a bit lacking.

---
 The authors made efforts to address my concerns","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666678697247,,1CHhsUY32a,ICLR.cc/2023/Conference/Paper1116/-/Official_Review,,[],1,,['everyone'],1CHhsUY32a,['ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57'],1666678697247,,1671176196090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not compared to in the original paper, such as:

* Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans.
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space?
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images?
* Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing.
* Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing.
* Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models.

The reviewer suggests that the authors should compare their method to these existing methods to demonstrate its effectiveness.",1,,In-domain gan inversion for real image editing
yntP_LO3Y6,This paper presents a method to generate images from a collage of patches representing objects. Each patch is fed to a feature encoder and the resulting features are spatially concatenated such that their layout corresponds that of a collage where each object occupies a specific bounding box. The aggregated feature map is then fed to a decoder that is trained adversarially such that both the full image and each bounding box corresponding to the object are realistic.,"
Overall, the idea is nice and is very practical in terms of applications in the sense that it seems more intuitive to copy/paste objects in a scene to represent the target image that you had in mind rather than describing it with a long text (that can be cumbersome and not precise enough) or drawing a segmentation mask. The idea stems from image editing, but here the goal is different as you certainly do not want the resulting image to be a smooth blending of the input collage which would not be diverse enough as a creation tool.

There are weaknesses in the experimental setup and in the overall positionning. For the experiments, the results are not extremely convincing due to the lack of comparision with methods that can do the same. Only IC-GAN is tested, but there are many more GAN inversion methods that could take the image collage as input encode it to the GAN latent space and then decode it, like:
- Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans. arXiv:2103.10426, 2021.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.
- Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, 2020.
- Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing. arXiv:2111.15264, 2021
- Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models. arXiv:2210.08573, 2022. (this one came out after the submission deadline, so of course it cannot be compared to)

Additionally, the experiment of fitting the training set is not convincing at all. Memorizing the training set is essentially what it takes to obtain a good score in this test and it does not tell anything about generalization capabilities. Why not just reserve a validation set and perform this metric on anything else than training data?
Overall, the visual results are not super impressive. Maybe it is because the task is very hard, but comparing with editing methods (even back to blending methods that do not use deep learning), would have shown the difficulty of trade-off between fidelity and diversity.",The paper is clear enough.,"It's a good idea but the experimental validation is a bit lacking.

---
 The authors made efforts to address my concerns","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666678697247,,1CHhsUY32a,ICLR.cc/2023/Conference/Paper1116/-/Official_Review,,[],1,,['everyone'],1CHhsUY32a,['ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57'],1666678697247,,1671176196090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not compared to in the original paper, such as:

* Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans.
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space?
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images?
* Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing.
* Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing.
* Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models.

The reviewer suggests that the authors should compare their method to these existing methods to demonstrate its effectiveness.",1,,EdiBERT a generative model for image editing
yntP_LO3Y6,This paper presents a method to generate images from a collage of patches representing objects. Each patch is fed to a feature encoder and the resulting features are spatially concatenated such that their layout corresponds that of a collage where each object occupies a specific bounding box. The aggregated feature map is then fed to a decoder that is trained adversarially such that both the full image and each bounding box corresponding to the object are realistic.,"
Overall, the idea is nice and is very practical in terms of applications in the sense that it seems more intuitive to copy/paste objects in a scene to represent the target image that you had in mind rather than describing it with a long text (that can be cumbersome and not precise enough) or drawing a segmentation mask. The idea stems from image editing, but here the goal is different as you certainly do not want the resulting image to be a smooth blending of the input collage which would not be diverse enough as a creation tool.

There are weaknesses in the experimental setup and in the overall positionning. For the experiments, the results are not extremely convincing due to the lack of comparision with methods that can do the same. Only IC-GAN is tested, but there are many more GAN inversion methods that could take the image collage as input encode it to the GAN latent space and then decode it, like:
- Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans. arXiv:2103.10426, 2021.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.
- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.
- Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, 2020.
- Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing. arXiv:2111.15264, 2021
- Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models. arXiv:2210.08573, 2022. (this one came out after the submission deadline, so of course it cannot be compared to)

Additionally, the experiment of fitting the training set is not convincing at all. Memorizing the training set is essentially what it takes to obtain a good score in this test and it does not tell anything about generalization capabilities. Why not just reserve a validation set and perform this metric on anything else than training data?
Overall, the visual results are not super impressive. Maybe it is because the task is very hard, but comparing with editing methods (even back to blending methods that do not use deep learning), would have shown the difficulty of trade-off between fidelity and diversity.",The paper is clear enough.,"It's a good idea but the experimental validation is a bit lacking.

---
 The authors made efforts to address my concerns","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666678697247,,1CHhsUY32a,ICLR.cc/2023/Conference/Paper1116/-/Official_Review,,[],1,,['everyone'],1CHhsUY32a,['ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57'],1666678697247,,1671176196090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not compared to in the original paper, such as:

* Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans.
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space?
* Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images?
* Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing.
* Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard. EdiBERT, a generative model for image editing.
* Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models.

The reviewer suggests that the authors should compare their method to these existing methods to demonstrate its effectiveness.",1,,DiffGAR Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models
fP0vENMk4W,"The paper studies experience replay mechanisms in a deep reinforcement learning (DRL) setting. Notably, it proposes a modification to the existing reverse experience replay method. The paper claims that purely prioritizing experiences according to TD errors and a naive (uniform) ER method may suffer from sub-optimal convergence and potentially large bias, respectively. Then the paper proposes the following method. First, the algorithm picks up k “pivot points” from a large buffer according to the TD error-based sampling distribution. Second, get the corresponding k batches of data where each batch ends with a pivot point sampled in the previous step. Then those mini-batches are used to update the training parameters. Extensive experiments on simple discrete domains, mujoco domains, and Atari games are conducted to show the effectiveness of the algorithms. 
","Strength: 

1. The topic of ER mechanism is interesting and important; 
2. The paper presents its mean idea very clear;
3. The proposed sampling method seems to be novel;
4. The paper conduct extensive experiments on a variety of domains. 

Weaknesses: 

---------------------------
Algorithm design. The presented algorithm 1 has to collect data episode by episode. This turns the algorithm into an offline algorithm, restricting its utility. But it seems the sampling method can be done in an online manner, why not propose that? This is important because 1) an online method is a closer competitor to PER which runs online (update parameters at each environment time step) and 2) it is clearer how the two algorithms are compared. 

Figure 3 is not persuasive. The TD errors can change as the parameters get updated. I do not see a clear correlation between reward magnitude and TD error magnitude. Also, if this correlation is true and is beneficial, shouldn't PER perform very well in the sparse reward setting? 

---------------------------
Concerns about experiments. The empirical results are extensive but not persuasive. 

1. Many figures (Fig 4-6) in the experiments section include learning curves with very large variances/standard errors, where one cannot really identify the proposed method to be better than others. Furthermore, it is better to study the hyper-parameter sensitivity of the proposed algorithm. The algorithm seems to have a large reliance on the size of the ER buffer. 

2. Missing at least two intuitive baselines to make the proposed method more persuasive:
1). Uniformly sample the pivot points, and the rest is the same as the proposed method: this can verify the usefulness of the claimed ""reverse replay.""
2). Prioritized sampling of the pivot points and then uniform sampling of the rest of datapoints in each batch: this can further validate the reverse sampling is important

3. One critical question about the experiments. The PER/UER can update parameters at each time step, while RER++ needs to wait until the end of an episode. How do you conduct the comparison? Do you use the same amount of real environment data or the same computation power for all algorithms? 

4. Missing details of the PER. PER has a mechanism to anneal the sampling bias. Since I see the proposed method used a ""mixed replay"" method to mitigate bias, it is important to report if there any effort (tuning the hyper-parameter) of the PER baseline has been made to anneal the bias. 


---------------------------
Missing related work. The paper belongs to the broad subarea of the sampling distribution of experiences, and there are many more papers in this category that should be discussed. I name a few highly relevant works here: 
[1] An equivalence between loss functions and non-uniform sampling in experience replay by Scott Fujimoto et al. 
[2] Remember and forget for experience replay by Guido Novati et al.
[3] Understanding and mitigating the limitations of PER by Yangchen Pan et al.
[4] regret minimization ER in off-policy RL et al. 

All these papers discuss the pros and cons of PER/ER methods, and some of them shed light on the theoretical mechanisms behind why a sampling method should be beneficial or what a good sampling distribution should be. 

----------------------
Presentation issue (I consider this not critical, but it can be significantly improved). The proposed sampling approach is not well-motivated. In the abstract, it says PER and UER may suffer from large bias and sub-optimal convergence, respectively. However, there is no evidence in the paper showing the proposed IER method is optimal or has a small bias. In fact, PER does address the biased sampling issue by using an important ratio, as introduced in the original paper (section 3.4). In contrast, the proposed method does not even have a sound method to anneal the sampling bias.
Furthermore, the paper attempts to use RER to motivate their approach too. However, the RER theory (from a system identification setting or linear MDP setting) cited by the authors does not really apply to general RL settings. I do not mean the authors need to provide a new/strong theory to motivate their method. Still, it would be a plus if the authors specify which theorems from existing work motivate their algorithmic design. 
",I comment on each of these in the above section. ,"The paper studies an important topic and presents a new ER method with extensive empirical results. However, I think the drawbacks of the algorithmic and experimental design currently outweigh the advantages. I will adjust my score based on reading the author's response and other reviewers' opinions. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666644072677,,15fiz99C8B,ICLR.cc/2023/Conference/Paper574/-/Official_Review,,[],1,,['everyone'],15fiz99C8B,['ICLR.cc/2023/Conference/Paper574/Reviewer_HzYs'],1666644072677,,1670498490221,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper574/Reviewer_HzYs']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are ""highly relevant"" and should be discussed:

1. ""An equivalence between loss functions and non-uniform sampling in experience replay"" by Scott Fujimoto et al.
2. ""Remember and forget for experience replay"" by Guido Novati et al.
3. ""Understanding and mitigating the limitations of PER"" by Yangchen Pan et al.
4. ""Regret minimization ER in off-policy RL"" (no specific authors mentioned)

These papers are not mentioned as being already included in the original paper, and the reviewer believes they are relevant to the topic and should be discussed to provide a more comprehensive overview of the area.",1,,An equivalence between loss functions and non-uniform sampling in experience replay 
fP0vENMk4W,"The paper studies experience replay mechanisms in a deep reinforcement learning (DRL) setting. Notably, it proposes a modification to the existing reverse experience replay method. The paper claims that purely prioritizing experiences according to TD errors and a naive (uniform) ER method may suffer from sub-optimal convergence and potentially large bias, respectively. Then the paper proposes the following method. First, the algorithm picks up k “pivot points” from a large buffer according to the TD error-based sampling distribution. Second, get the corresponding k batches of data where each batch ends with a pivot point sampled in the previous step. Then those mini-batches are used to update the training parameters. Extensive experiments on simple discrete domains, mujoco domains, and Atari games are conducted to show the effectiveness of the algorithms. 
","Strength: 

1. The topic of ER mechanism is interesting and important; 
2. The paper presents its mean idea very clear;
3. The proposed sampling method seems to be novel;
4. The paper conduct extensive experiments on a variety of domains. 

Weaknesses: 

---------------------------
Algorithm design. The presented algorithm 1 has to collect data episode by episode. This turns the algorithm into an offline algorithm, restricting its utility. But it seems the sampling method can be done in an online manner, why not propose that? This is important because 1) an online method is a closer competitor to PER which runs online (update parameters at each environment time step) and 2) it is clearer how the two algorithms are compared. 

Figure 3 is not persuasive. The TD errors can change as the parameters get updated. I do not see a clear correlation between reward magnitude and TD error magnitude. Also, if this correlation is true and is beneficial, shouldn't PER perform very well in the sparse reward setting? 

---------------------------
Concerns about experiments. The empirical results are extensive but not persuasive. 

1. Many figures (Fig 4-6) in the experiments section include learning curves with very large variances/standard errors, where one cannot really identify the proposed method to be better than others. Furthermore, it is better to study the hyper-parameter sensitivity of the proposed algorithm. The algorithm seems to have a large reliance on the size of the ER buffer. 

2. Missing at least two intuitive baselines to make the proposed method more persuasive:
1). Uniformly sample the pivot points, and the rest is the same as the proposed method: this can verify the usefulness of the claimed ""reverse replay.""
2). Prioritized sampling of the pivot points and then uniform sampling of the rest of datapoints in each batch: this can further validate the reverse sampling is important

3. One critical question about the experiments. The PER/UER can update parameters at each time step, while RER++ needs to wait until the end of an episode. How do you conduct the comparison? Do you use the same amount of real environment data or the same computation power for all algorithms? 

4. Missing details of the PER. PER has a mechanism to anneal the sampling bias. Since I see the proposed method used a ""mixed replay"" method to mitigate bias, it is important to report if there any effort (tuning the hyper-parameter) of the PER baseline has been made to anneal the bias. 


---------------------------
Missing related work. The paper belongs to the broad subarea of the sampling distribution of experiences, and there are many more papers in this category that should be discussed. I name a few highly relevant works here: 
[1] An equivalence between loss functions and non-uniform sampling in experience replay by Scott Fujimoto et al. 
[2] Remember and forget for experience replay by Guido Novati et al.
[3] Understanding and mitigating the limitations of PER by Yangchen Pan et al.
[4] regret minimization ER in off-policy RL et al. 

All these papers discuss the pros and cons of PER/ER methods, and some of them shed light on the theoretical mechanisms behind why a sampling method should be beneficial or what a good sampling distribution should be. 

----------------------
Presentation issue (I consider this not critical, but it can be significantly improved). The proposed sampling approach is not well-motivated. In the abstract, it says PER and UER may suffer from large bias and sub-optimal convergence, respectively. However, there is no evidence in the paper showing the proposed IER method is optimal or has a small bias. In fact, PER does address the biased sampling issue by using an important ratio, as introduced in the original paper (section 3.4). In contrast, the proposed method does not even have a sound method to anneal the sampling bias.
Furthermore, the paper attempts to use RER to motivate their approach too. However, the RER theory (from a system identification setting or linear MDP setting) cited by the authors does not really apply to general RL settings. I do not mean the authors need to provide a new/strong theory to motivate their method. Still, it would be a plus if the authors specify which theorems from existing work motivate their algorithmic design. 
",I comment on each of these in the above section. ,"The paper studies an important topic and presents a new ER method with extensive empirical results. However, I think the drawbacks of the algorithmic and experimental design currently outweigh the advantages. I will adjust my score based on reading the author's response and other reviewers' opinions. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666644072677,,15fiz99C8B,ICLR.cc/2023/Conference/Paper574/-/Official_Review,,[],1,,['everyone'],15fiz99C8B,['ICLR.cc/2023/Conference/Paper574/Reviewer_HzYs'],1666644072677,,1670498490221,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper574/Reviewer_HzYs']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are ""highly relevant"" and should be discussed:

1. ""An equivalence between loss functions and non-uniform sampling in experience replay"" by Scott Fujimoto et al.
2. ""Remember and forget for experience replay"" by Guido Novati et al.
3. ""Understanding and mitigating the limitations of PER"" by Yangchen Pan et al.
4. ""Regret minimization ER in off-policy RL"" (no specific authors mentioned)

These papers are not mentioned as being already included in the original paper, and the reviewer believes they are relevant to the topic and should be discussed to provide a more comprehensive overview of the area.",1,,Remember and forget for experience replay 
fP0vENMk4W,"The paper studies experience replay mechanisms in a deep reinforcement learning (DRL) setting. Notably, it proposes a modification to the existing reverse experience replay method. The paper claims that purely prioritizing experiences according to TD errors and a naive (uniform) ER method may suffer from sub-optimal convergence and potentially large bias, respectively. Then the paper proposes the following method. First, the algorithm picks up k “pivot points” from a large buffer according to the TD error-based sampling distribution. Second, get the corresponding k batches of data where each batch ends with a pivot point sampled in the previous step. Then those mini-batches are used to update the training parameters. Extensive experiments on simple discrete domains, mujoco domains, and Atari games are conducted to show the effectiveness of the algorithms. 
","Strength: 

1. The topic of ER mechanism is interesting and important; 
2. The paper presents its mean idea very clear;
3. The proposed sampling method seems to be novel;
4. The paper conduct extensive experiments on a variety of domains. 

Weaknesses: 

---------------------------
Algorithm design. The presented algorithm 1 has to collect data episode by episode. This turns the algorithm into an offline algorithm, restricting its utility. But it seems the sampling method can be done in an online manner, why not propose that? This is important because 1) an online method is a closer competitor to PER which runs online (update parameters at each environment time step) and 2) it is clearer how the two algorithms are compared. 

Figure 3 is not persuasive. The TD errors can change as the parameters get updated. I do not see a clear correlation between reward magnitude and TD error magnitude. Also, if this correlation is true and is beneficial, shouldn't PER perform very well in the sparse reward setting? 

---------------------------
Concerns about experiments. The empirical results are extensive but not persuasive. 

1. Many figures (Fig 4-6) in the experiments section include learning curves with very large variances/standard errors, where one cannot really identify the proposed method to be better than others. Furthermore, it is better to study the hyper-parameter sensitivity of the proposed algorithm. The algorithm seems to have a large reliance on the size of the ER buffer. 

2. Missing at least two intuitive baselines to make the proposed method more persuasive:
1). Uniformly sample the pivot points, and the rest is the same as the proposed method: this can verify the usefulness of the claimed ""reverse replay.""
2). Prioritized sampling of the pivot points and then uniform sampling of the rest of datapoints in each batch: this can further validate the reverse sampling is important

3. One critical question about the experiments. The PER/UER can update parameters at each time step, while RER++ needs to wait until the end of an episode. How do you conduct the comparison? Do you use the same amount of real environment data or the same computation power for all algorithms? 

4. Missing details of the PER. PER has a mechanism to anneal the sampling bias. Since I see the proposed method used a ""mixed replay"" method to mitigate bias, it is important to report if there any effort (tuning the hyper-parameter) of the PER baseline has been made to anneal the bias. 


---------------------------
Missing related work. The paper belongs to the broad subarea of the sampling distribution of experiences, and there are many more papers in this category that should be discussed. I name a few highly relevant works here: 
[1] An equivalence between loss functions and non-uniform sampling in experience replay by Scott Fujimoto et al. 
[2] Remember and forget for experience replay by Guido Novati et al.
[3] Understanding and mitigating the limitations of PER by Yangchen Pan et al.
[4] regret minimization ER in off-policy RL et al. 

All these papers discuss the pros and cons of PER/ER methods, and some of them shed light on the theoretical mechanisms behind why a sampling method should be beneficial or what a good sampling distribution should be. 

----------------------
Presentation issue (I consider this not critical, but it can be significantly improved). The proposed sampling approach is not well-motivated. In the abstract, it says PER and UER may suffer from large bias and sub-optimal convergence, respectively. However, there is no evidence in the paper showing the proposed IER method is optimal or has a small bias. In fact, PER does address the biased sampling issue by using an important ratio, as introduced in the original paper (section 3.4). In contrast, the proposed method does not even have a sound method to anneal the sampling bias.
Furthermore, the paper attempts to use RER to motivate their approach too. However, the RER theory (from a system identification setting or linear MDP setting) cited by the authors does not really apply to general RL settings. I do not mean the authors need to provide a new/strong theory to motivate their method. Still, it would be a plus if the authors specify which theorems from existing work motivate their algorithmic design. 
",I comment on each of these in the above section. ,"The paper studies an important topic and presents a new ER method with extensive empirical results. However, I think the drawbacks of the algorithmic and experimental design currently outweigh the advantages. I will adjust my score based on reading the author's response and other reviewers' opinions. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666644072677,,15fiz99C8B,ICLR.cc/2023/Conference/Paper574/-/Official_Review,,[],1,,['everyone'],15fiz99C8B,['ICLR.cc/2023/Conference/Paper574/Reviewer_HzYs'],1666644072677,,1670498490221,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper574/Reviewer_HzYs']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are ""highly relevant"" and should be discussed:

1. ""An equivalence between loss functions and non-uniform sampling in experience replay"" by Scott Fujimoto et al.
2. ""Remember and forget for experience replay"" by Guido Novati et al.
3. ""Understanding and mitigating the limitations of PER"" by Yangchen Pan et al.
4. ""Regret minimization ER in off-policy RL"" (no specific authors mentioned)

These papers are not mentioned as being already included in the original paper, and the reviewer believes they are relevant to the topic and should be discussed to provide a more comprehensive overview of the area.",1,,Understanding and mitigating the limitations of PER 
fP0vENMk4W,"The paper studies experience replay mechanisms in a deep reinforcement learning (DRL) setting. Notably, it proposes a modification to the existing reverse experience replay method. The paper claims that purely prioritizing experiences according to TD errors and a naive (uniform) ER method may suffer from sub-optimal convergence and potentially large bias, respectively. Then the paper proposes the following method. First, the algorithm picks up k “pivot points” from a large buffer according to the TD error-based sampling distribution. Second, get the corresponding k batches of data where each batch ends with a pivot point sampled in the previous step. Then those mini-batches are used to update the training parameters. Extensive experiments on simple discrete domains, mujoco domains, and Atari games are conducted to show the effectiveness of the algorithms. 
","Strength: 

1. The topic of ER mechanism is interesting and important; 
2. The paper presents its mean idea very clear;
3. The proposed sampling method seems to be novel;
4. The paper conduct extensive experiments on a variety of domains. 

Weaknesses: 

---------------------------
Algorithm design. The presented algorithm 1 has to collect data episode by episode. This turns the algorithm into an offline algorithm, restricting its utility. But it seems the sampling method can be done in an online manner, why not propose that? This is important because 1) an online method is a closer competitor to PER which runs online (update parameters at each environment time step) and 2) it is clearer how the two algorithms are compared. 

Figure 3 is not persuasive. The TD errors can change as the parameters get updated. I do not see a clear correlation between reward magnitude and TD error magnitude. Also, if this correlation is true and is beneficial, shouldn't PER perform very well in the sparse reward setting? 

---------------------------
Concerns about experiments. The empirical results are extensive but not persuasive. 

1. Many figures (Fig 4-6) in the experiments section include learning curves with very large variances/standard errors, where one cannot really identify the proposed method to be better than others. Furthermore, it is better to study the hyper-parameter sensitivity of the proposed algorithm. The algorithm seems to have a large reliance on the size of the ER buffer. 

2. Missing at least two intuitive baselines to make the proposed method more persuasive:
1). Uniformly sample the pivot points, and the rest is the same as the proposed method: this can verify the usefulness of the claimed ""reverse replay.""
2). Prioritized sampling of the pivot points and then uniform sampling of the rest of datapoints in each batch: this can further validate the reverse sampling is important

3. One critical question about the experiments. The PER/UER can update parameters at each time step, while RER++ needs to wait until the end of an episode. How do you conduct the comparison? Do you use the same amount of real environment data or the same computation power for all algorithms? 

4. Missing details of the PER. PER has a mechanism to anneal the sampling bias. Since I see the proposed method used a ""mixed replay"" method to mitigate bias, it is important to report if there any effort (tuning the hyper-parameter) of the PER baseline has been made to anneal the bias. 


---------------------------
Missing related work. The paper belongs to the broad subarea of the sampling distribution of experiences, and there are many more papers in this category that should be discussed. I name a few highly relevant works here: 
[1] An equivalence between loss functions and non-uniform sampling in experience replay by Scott Fujimoto et al. 
[2] Remember and forget for experience replay by Guido Novati et al.
[3] Understanding and mitigating the limitations of PER by Yangchen Pan et al.
[4] regret minimization ER in off-policy RL et al. 

All these papers discuss the pros and cons of PER/ER methods, and some of them shed light on the theoretical mechanisms behind why a sampling method should be beneficial or what a good sampling distribution should be. 

----------------------
Presentation issue (I consider this not critical, but it can be significantly improved). The proposed sampling approach is not well-motivated. In the abstract, it says PER and UER may suffer from large bias and sub-optimal convergence, respectively. However, there is no evidence in the paper showing the proposed IER method is optimal or has a small bias. In fact, PER does address the biased sampling issue by using an important ratio, as introduced in the original paper (section 3.4). In contrast, the proposed method does not even have a sound method to anneal the sampling bias.
Furthermore, the paper attempts to use RER to motivate their approach too. However, the RER theory (from a system identification setting or linear MDP setting) cited by the authors does not really apply to general RL settings. I do not mean the authors need to provide a new/strong theory to motivate their method. Still, it would be a plus if the authors specify which theorems from existing work motivate their algorithmic design. 
",I comment on each of these in the above section. ,"The paper studies an important topic and presents a new ER method with extensive empirical results. However, I think the drawbacks of the algorithmic and experimental design currently outweigh the advantages. I will adjust my score based on reading the author's response and other reviewers' opinions. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666644072677,,15fiz99C8B,ICLR.cc/2023/Conference/Paper574/-/Official_Review,,[],1,,['everyone'],15fiz99C8B,['ICLR.cc/2023/Conference/Paper574/Reviewer_HzYs'],1666644072677,,1670498490221,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper574/Reviewer_HzYs']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are ""highly relevant"" and should be discussed:

1. ""An equivalence between loss functions and non-uniform sampling in experience replay"" by Scott Fujimoto et al.
2. ""Remember and forget for experience replay"" by Guido Novati et al.
3. ""Understanding and mitigating the limitations of PER"" by Yangchen Pan et al.
4. ""Regret minimization ER in off-policy RL"" (no specific authors mentioned)

These papers are not mentioned as being already included in the original paper, and the reviewer believes they are relevant to the topic and should be discussed to provide a more comprehensive overview of the area.",1,,Regret minimization ER in off-policy RL
1MTr8EYT4O,"This paper proposes an experience replay method for reinforcement learning. The paper argued that previous methods are sub-optimal and have bias. The proposed method picks some pivot points at first. Then it selects transitions before these pivot points. 

The intuition of the paper is that an agent should select transitions that associate with outcomes. 

In the experiments, the proposed method compares UER, PER, HER with multiple environments. The paper uses the top-k seeds moving average return as the evaluation metric and 3 seeds. The paper also compares IER forward and IER reverse. The results show that the proposed method works better for the most of the dataset.
","> Strength

The problem is important and very interesting. 

Considering the pivot points and selecting transitions before outcomes seems to be new for experience replay.

The experiments show the proposed method works better than others.

>Weaknesses

The paper argues that the previous methods have bias. Does the proposed have bias too? The paper needs to provide an analysis about it.  

The surprised pivots are one of key components of the method. How to pick pivots seems to be not new. The paper uses TD error for selecting pivots. What is the difference between the proposed method and PER about the idea?

Related works are not enough. For example, for the experience replay, the paper does not mention CER (Competitive experience replay), CHER (DHER: Hindsight experience replay for dynamic goals), DHER (Curriculum-guided hindsight experience replay), and so on. 

For the important function, it uses the magnitude of the TD error, what does magnitude mean? What exactly is the important function? It is not clear.

It is better to improve the writing of the paper. For example, in Figure 1, the paper uses colors but does not provide explanation. 

In Table 2, why does not  IER work better in Pong?

In Table 2 and Table 3, why do IER and Reverse have different results?

The baselines used in the paper are a little confusing. What is OER? Why does it not appear in Table 2? It is better to provide more details. 

Why does not the paper provide learning curves for comparing different methods? Using curves is common to present results. 
",The quality and clarity are good. The paper also proposes a new method.,"The paper argues the previous methods have bias. However, the paper does not provide enough support to show their method does not have bias. The proposed method uses TD for selecting pivots. It might be similar to previous works. The paper also needs to provide more details about related works and techniques. ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,Not applicable,NO.,6: marginally above the acceptance threshold,1666720355060,,15fiz99C8B,ICLR.cc/2023/Conference/Paper574/-/Official_Review,,[],2,,['everyone'],15fiz99C8B,['ICLR.cc/2023/Conference/Paper574/Reviewer_nF51'],1666720355060,,1669851543327,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper574/Reviewer_nF51']","Yes, this peer review suggests that the authors of the paper refer to specific literature that is not already discussed in the original paper. The reviewer mentions several related works that are not cited in the paper, such as:

* CER (Competitive experience replay)
* CHER (DHER: Hindsight experience replay for dynamic goals)
* DHER (Curriculum-guided hindsight experience replay)

The reviewer notes that the paper does not mention these works, implying that the authors should include them in the related works section to provide a more comprehensive overview of the field.",1,,Competitive experience replay 
1MTr8EYT4O,"This paper proposes an experience replay method for reinforcement learning. The paper argued that previous methods are sub-optimal and have bias. The proposed method picks some pivot points at first. Then it selects transitions before these pivot points. 

The intuition of the paper is that an agent should select transitions that associate with outcomes. 

In the experiments, the proposed method compares UER, PER, HER with multiple environments. The paper uses the top-k seeds moving average return as the evaluation metric and 3 seeds. The paper also compares IER forward and IER reverse. The results show that the proposed method works better for the most of the dataset.
","> Strength

The problem is important and very interesting. 

Considering the pivot points and selecting transitions before outcomes seems to be new for experience replay.

The experiments show the proposed method works better than others.

>Weaknesses

The paper argues that the previous methods have bias. Does the proposed have bias too? The paper needs to provide an analysis about it.  

The surprised pivots are one of key components of the method. How to pick pivots seems to be not new. The paper uses TD error for selecting pivots. What is the difference between the proposed method and PER about the idea?

Related works are not enough. For example, for the experience replay, the paper does not mention CER (Competitive experience replay), CHER (DHER: Hindsight experience replay for dynamic goals), DHER (Curriculum-guided hindsight experience replay), and so on. 

For the important function, it uses the magnitude of the TD error, what does magnitude mean? What exactly is the important function? It is not clear.

It is better to improve the writing of the paper. For example, in Figure 1, the paper uses colors but does not provide explanation. 

In Table 2, why does not  IER work better in Pong?

In Table 2 and Table 3, why do IER and Reverse have different results?

The baselines used in the paper are a little confusing. What is OER? Why does it not appear in Table 2? It is better to provide more details. 

Why does not the paper provide learning curves for comparing different methods? Using curves is common to present results. 
",The quality and clarity are good. The paper also proposes a new method.,"The paper argues the previous methods have bias. However, the paper does not provide enough support to show their method does not have bias. The proposed method uses TD for selecting pivots. It might be similar to previous works. The paper also needs to provide more details about related works and techniques. ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,Not applicable,NO.,6: marginally above the acceptance threshold,1666720355060,,15fiz99C8B,ICLR.cc/2023/Conference/Paper574/-/Official_Review,,[],2,,['everyone'],15fiz99C8B,['ICLR.cc/2023/Conference/Paper574/Reviewer_nF51'],1666720355060,,1669851543327,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper574/Reviewer_nF51']","Yes, this peer review suggests that the authors of the paper refer to specific literature that is not already discussed in the original paper. The reviewer mentions several related works that are not cited in the paper, such as:

* CER (Competitive experience replay)
* CHER (DHER: Hindsight experience replay for dynamic goals)
* DHER (Curriculum-guided hindsight experience replay)

The reviewer notes that the paper does not mention these works, implying that the authors should include them in the related works section to provide a more comprehensive overview of the field.",1,,Hindsight experience replay for dynamic goals 
1MTr8EYT4O,"This paper proposes an experience replay method for reinforcement learning. The paper argued that previous methods are sub-optimal and have bias. The proposed method picks some pivot points at first. Then it selects transitions before these pivot points. 

The intuition of the paper is that an agent should select transitions that associate with outcomes. 

In the experiments, the proposed method compares UER, PER, HER with multiple environments. The paper uses the top-k seeds moving average return as the evaluation metric and 3 seeds. The paper also compares IER forward and IER reverse. The results show that the proposed method works better for the most of the dataset.
","> Strength

The problem is important and very interesting. 

Considering the pivot points and selecting transitions before outcomes seems to be new for experience replay.

The experiments show the proposed method works better than others.

>Weaknesses

The paper argues that the previous methods have bias. Does the proposed have bias too? The paper needs to provide an analysis about it.  

The surprised pivots are one of key components of the method. How to pick pivots seems to be not new. The paper uses TD error for selecting pivots. What is the difference between the proposed method and PER about the idea?

Related works are not enough. For example, for the experience replay, the paper does not mention CER (Competitive experience replay), CHER (DHER: Hindsight experience replay for dynamic goals), DHER (Curriculum-guided hindsight experience replay), and so on. 

For the important function, it uses the magnitude of the TD error, what does magnitude mean? What exactly is the important function? It is not clear.

It is better to improve the writing of the paper. For example, in Figure 1, the paper uses colors but does not provide explanation. 

In Table 2, why does not  IER work better in Pong?

In Table 2 and Table 3, why do IER and Reverse have different results?

The baselines used in the paper are a little confusing. What is OER? Why does it not appear in Table 2? It is better to provide more details. 

Why does not the paper provide learning curves for comparing different methods? Using curves is common to present results. 
",The quality and clarity are good. The paper also proposes a new method.,"The paper argues the previous methods have bias. However, the paper does not provide enough support to show their method does not have bias. The proposed method uses TD for selecting pivots. It might be similar to previous works. The paper also needs to provide more details about related works and techniques. ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,Not applicable,NO.,6: marginally above the acceptance threshold,1666720355060,,15fiz99C8B,ICLR.cc/2023/Conference/Paper574/-/Official_Review,,[],2,,['everyone'],15fiz99C8B,['ICLR.cc/2023/Conference/Paper574/Reviewer_nF51'],1666720355060,,1669851543327,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper574/Reviewer_nF51']","Yes, this peer review suggests that the authors of the paper refer to specific literature that is not already discussed in the original paper. The reviewer mentions several related works that are not cited in the paper, such as:

* CER (Competitive experience replay)
* CHER (DHER: Hindsight experience replay for dynamic goals)
* DHER (Curriculum-guided hindsight experience replay)

The reviewer notes that the paper does not mention these works, implying that the authors should include them in the related works section to provide a more comprehensive overview of the field.",1,,Curriculum-guided hindsight experience replay
WWK33a1m0NA,"This paper presents a meta-learning formulation for learning to adapt a deep time-index model to the look-back window. INR was the choice of time-index models, and meta-learning was formulated by using samples from look-back window as context set and forecasting horizon as query set. The goal of the meta-learning was mainly motivated for dealing with non-stationary time series. Experiments were conducted on synthetic as well as real datasets demonstrating performance improvement of forecasting over selected baselines.
","Strengths:

1. Addressing non-stationary time-series by learning to adapt the time-series model is an important and interesting direction of research. 
2. The use of INR for the deep time-index model is interesting. 
3. The results demonstrated improvement over a good number of baselines used.


Weakness:

1. The presented work is heavily related to switching state-space models [1] and sequential neural processes [2]. In particular, it seems that it can be formulated as a special case of the SNP for learning y = f(t) where Bayesian meta-learning is learned to adapt f to observed frames from the lookback window for prediction in the forecasting horizon. Neither of these two works were discussed in the paper. Relations with these two works, and comparisons in terms of performance, should be provided. 

2. By formulating the meta-learning with optimization-based approaches, the model needs to be optimized to the query set before being used for forecasting. To mitigate this, the paper opted to restrict the optimization to only the last layer of the INR. The effect of this restriction should be demonstrated, empirically at least. In an ideal setting (where resource or time is not a constraint) where the full INR can be optimized to the lookback window each time, how does the performance look like, and how does it compare with the assumption of restricting the optimization to the last layer? Note that, with SNP, this was avoided by formulating the meta-learning with feedforward based approaches. This again stresses the need to compare with SNP.

3. Details on meta-training is needed. It is not clear how the current tasks are defined -- by treating each pair of lookback window and forecasting horizon as a task, it means that the meta-training is looking at a large number of tasks and each task has only one set of context and query sets? Is the task boundary known assumed to be known ahead of time? If it is, it seems to be an unrealistic assumption. It it is not, this seems to be an unconventional setting of meta-learning -- Does the training follow the typical episodic training then? Further, how does the method apply if either the look-back window or the forecasting horizon falls within the transition of tasks boundaries? 

4. Experimental details are missing. It is not clear how the baseline models are trained. Since most of them do not use any meta-learning formulation, are they trained on the meta-training set? In that case, the comparison may not be fair as the presented model -- at meta-test time, actually is optimized to the look-back window (while the baseline models are simply applied to the look-back window without optimization). A fair comparison would be to fine-tune the baseline models (after training on the meta-training set) to the same samples (look-back window) used at meta-test time. If the argument is that these models cannot be fine-tuned this way (since they are history-value based models), at least in ablation study, the ""-RR"" version need to be fine-tuned at meta-test time.

5. Since the look-back window represents context set in meta-learning, its size may have an important effect in ""optimizing"" the base model. The value of L in synthetic experiments seems to be large, and was not specified in real-data. Please add such details, and provide experimental evidence about the effect of the size of the context set L on meta-learning.

6. While the paper was heavily motivated for better learning non-stationary time series, the experimental evaluation is limited in demonstrating how or whether the proposed solutions achieved the stated goal. In synthetic experiments, there lack details on how many tasks were used to meta-train the model and how many tasks were used in meta-testing. Since tasks/segments are generated with random sampling of the parameters, it'd be good to get a sense of the distance between the meta-train and meta-testing tasks. Finally, it was stated that ""A total of 400 time steps are sampled, with a lookback window length of 200 and forecast horizon of 200."" In Appendix D, it was then stated that ""each function/task consists of 400 evenly spaced points"". So assuming each context-target set pairs are 400 time points, it was not clear how many such samples of length 400 were used. It was also not clear how does the forecasting work in such segment of 400. Does the model take 200 context samples and forecast for 200 context samples, and then it moves to the next window of 400? i.e., the task boundary is assumed to be known?

In real data experiments, it is understandable that the truth about the ""non-stationary"" nature of the data is not always available, but to the extent it's possible, it'd be desirable to say some level of analyses that link the model performance with the ""non-stationary"" nature of the underlying time series. 



7. The methodology is presented in a general fashion for forecasting m-dimensional observations. It'd be good to understand to what extend the value of m could be, i.e., what types of observations can be modeled by the presented method. Are we looking at multivariate data with relatively lower number of dimensions, or are we looking at image series?






[1] Variational Learning for Switching State-Space Models, Zoubin Ghahramani and Geoffrey E. Hinton, Neural Computation 12, 831–864 (2000)
[2] Sequential Neural Processes, Gautam Singh, Jaesik Yoon, Youngsung Son, Sungjin Ahn, NeurIPS 2019

","The writing was overall clear and relatively easy to follow, although the methodology and experiment descriptions lack many critical details (as detailed above) that make it difficult to assess or reproduce the method. I have questions on the novelty of the work due to its unclear and not-discussed relation with switching systems and more importantly, SNP. 

Code is submitted and will be released for reproducibility.","This paper tackles an important problem of time-series forecasting (i..e, non-stationary series) with an interesting solution (meta-learning of time-index models). The novelty however is unclear with respect to some existing works especially sequential neural processes. The choice or benefit of meta-learning method and the simplification of optimizing only the last layer of the INR, in comparison to alternative meta-learning method such as feed-forward model based method used in SNP that can bypass such simplifications, is not clear. The writing lacks many critical details on methodology and experiments, such as how are tasks handled during meta-training, whether the task boundary needs to be known, and how to address lookback window or forecasting horizon that includes task boundaries. It is also not clear how large a L is needed as the context set, and how baseline models utilized these context data at meta-test time. Overall it is an interesting idea, but can be improved in clarification of novelty and many methodological/experimental details. 

","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666661873092,,13rQhx37o3u,ICLR.cc/2023/Conference/Paper2304/-/Official_Review,,[],3,,['everyone'],13rQhx37o3u,['ICLR.cc/2023/Conference/Paper2304/Reviewer_i4vc'],1666661873092,,1666661873092,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2304/Reviewer_i4vc']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. ""Variational Learning for Switching State-Space Models"" by Zoubin Ghahramani and Geoffrey E. Hinton (2000)
2. ""Sequential Neural Processes"" by Gautam Singh, Jaesik Yoon, Youngsung Son, and Sungjin Ahn (NeurIPS 2019)

The reviewer states that the presented work is heavily related to these two papers, and that the authors should discuss and compare their work with these papers, particularly in terms of performance. This implies that these papers are not already referenced in the original paper.",1,"2000, 2019",Variational Learning for Switching State-Space Models 
WWK33a1m0NA,"This paper presents a meta-learning formulation for learning to adapt a deep time-index model to the look-back window. INR was the choice of time-index models, and meta-learning was formulated by using samples from look-back window as context set and forecasting horizon as query set. The goal of the meta-learning was mainly motivated for dealing with non-stationary time series. Experiments were conducted on synthetic as well as real datasets demonstrating performance improvement of forecasting over selected baselines.
","Strengths:

1. Addressing non-stationary time-series by learning to adapt the time-series model is an important and interesting direction of research. 
2. The use of INR for the deep time-index model is interesting. 
3. The results demonstrated improvement over a good number of baselines used.


Weakness:

1. The presented work is heavily related to switching state-space models [1] and sequential neural processes [2]. In particular, it seems that it can be formulated as a special case of the SNP for learning y = f(t) where Bayesian meta-learning is learned to adapt f to observed frames from the lookback window for prediction in the forecasting horizon. Neither of these two works were discussed in the paper. Relations with these two works, and comparisons in terms of performance, should be provided. 

2. By formulating the meta-learning with optimization-based approaches, the model needs to be optimized to the query set before being used for forecasting. To mitigate this, the paper opted to restrict the optimization to only the last layer of the INR. The effect of this restriction should be demonstrated, empirically at least. In an ideal setting (where resource or time is not a constraint) where the full INR can be optimized to the lookback window each time, how does the performance look like, and how does it compare with the assumption of restricting the optimization to the last layer? Note that, with SNP, this was avoided by formulating the meta-learning with feedforward based approaches. This again stresses the need to compare with SNP.

3. Details on meta-training is needed. It is not clear how the current tasks are defined -- by treating each pair of lookback window and forecasting horizon as a task, it means that the meta-training is looking at a large number of tasks and each task has only one set of context and query sets? Is the task boundary known assumed to be known ahead of time? If it is, it seems to be an unrealistic assumption. It it is not, this seems to be an unconventional setting of meta-learning -- Does the training follow the typical episodic training then? Further, how does the method apply if either the look-back window or the forecasting horizon falls within the transition of tasks boundaries? 

4. Experimental details are missing. It is not clear how the baseline models are trained. Since most of them do not use any meta-learning formulation, are they trained on the meta-training set? In that case, the comparison may not be fair as the presented model -- at meta-test time, actually is optimized to the look-back window (while the baseline models are simply applied to the look-back window without optimization). A fair comparison would be to fine-tune the baseline models (after training on the meta-training set) to the same samples (look-back window) used at meta-test time. If the argument is that these models cannot be fine-tuned this way (since they are history-value based models), at least in ablation study, the ""-RR"" version need to be fine-tuned at meta-test time.

5. Since the look-back window represents context set in meta-learning, its size may have an important effect in ""optimizing"" the base model. The value of L in synthetic experiments seems to be large, and was not specified in real-data. Please add such details, and provide experimental evidence about the effect of the size of the context set L on meta-learning.

6. While the paper was heavily motivated for better learning non-stationary time series, the experimental evaluation is limited in demonstrating how or whether the proposed solutions achieved the stated goal. In synthetic experiments, there lack details on how many tasks were used to meta-train the model and how many tasks were used in meta-testing. Since tasks/segments are generated with random sampling of the parameters, it'd be good to get a sense of the distance between the meta-train and meta-testing tasks. Finally, it was stated that ""A total of 400 time steps are sampled, with a lookback window length of 200 and forecast horizon of 200."" In Appendix D, it was then stated that ""each function/task consists of 400 evenly spaced points"". So assuming each context-target set pairs are 400 time points, it was not clear how many such samples of length 400 were used. It was also not clear how does the forecasting work in such segment of 400. Does the model take 200 context samples and forecast for 200 context samples, and then it moves to the next window of 400? i.e., the task boundary is assumed to be known?

In real data experiments, it is understandable that the truth about the ""non-stationary"" nature of the data is not always available, but to the extent it's possible, it'd be desirable to say some level of analyses that link the model performance with the ""non-stationary"" nature of the underlying time series. 



7. The methodology is presented in a general fashion for forecasting m-dimensional observations. It'd be good to understand to what extend the value of m could be, i.e., what types of observations can be modeled by the presented method. Are we looking at multivariate data with relatively lower number of dimensions, or are we looking at image series?






[1] Variational Learning for Switching State-Space Models, Zoubin Ghahramani and Geoffrey E. Hinton, Neural Computation 12, 831–864 (2000)
[2] Sequential Neural Processes, Gautam Singh, Jaesik Yoon, Youngsung Son, Sungjin Ahn, NeurIPS 2019

","The writing was overall clear and relatively easy to follow, although the methodology and experiment descriptions lack many critical details (as detailed above) that make it difficult to assess or reproduce the method. I have questions on the novelty of the work due to its unclear and not-discussed relation with switching systems and more importantly, SNP. 

Code is submitted and will be released for reproducibility.","This paper tackles an important problem of time-series forecasting (i..e, non-stationary series) with an interesting solution (meta-learning of time-index models). The novelty however is unclear with respect to some existing works especially sequential neural processes. The choice or benefit of meta-learning method and the simplification of optimizing only the last layer of the INR, in comparison to alternative meta-learning method such as feed-forward model based method used in SNP that can bypass such simplifications, is not clear. The writing lacks many critical details on methodology and experiments, such as how are tasks handled during meta-training, whether the task boundary needs to be known, and how to address lookback window or forecasting horizon that includes task boundaries. It is also not clear how large a L is needed as the context set, and how baseline models utilized these context data at meta-test time. Overall it is an interesting idea, but can be improved in clarification of novelty and many methodological/experimental details. 

","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666661873092,,13rQhx37o3u,ICLR.cc/2023/Conference/Paper2304/-/Official_Review,,[],3,,['everyone'],13rQhx37o3u,['ICLR.cc/2023/Conference/Paper2304/Reviewer_i4vc'],1666661873092,,1666661873092,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2304/Reviewer_i4vc']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. ""Variational Learning for Switching State-Space Models"" by Zoubin Ghahramani and Geoffrey E. Hinton (2000)
2. ""Sequential Neural Processes"" by Gautam Singh, Jaesik Yoon, Youngsung Son, and Sungjin Ahn (NeurIPS 2019)

The reviewer states that the presented work is heavily related to these two papers, and that the authors should discuss and compare their work with these papers, particularly in terms of performance. This implies that these papers are not already referenced in the original paper.",1,"2000, 2019",Sequential Neural Processes
mxp8FFEaIPL,"The paper ""Reward Design with Language Models"" proposes to study to effectiveness of pre-trained large language models for designing rewards in RL tasks (tasks implying language). It shows that, somehow unsurprisingly given recent literature, leveraging these models to drive agents with textual prompts is possible in few-shot or zero-shot settings. ","Strengths: 
- Using textual prompts to drive RL agents is an emerging topic with promising outcomes

- Promising results for controling RL agents with language

Weakness: 
    
- Using pre-trained LLMs in zero-shot settings is clearly not new, many studies already exist on that topic. Not on the specific tasks authors suggest, but the transferability of their embedded knowledge with well specified prompts is well known. Prompt tuning is an emerging field following that observation. 

- Shaping rewards with LLMs is not new either, as many works recently proposed to leverage them to split difficult tasks into textual subgoals, or to drive agents by asking questions for instances, in a curriculum RL fashion. These works should at least be mentioned in the related work.   

- Writing is not always very clear. Readers have to go in many various places again and again during reading to get things together. Very difficult to understand without appendix. Many settings not sufficiently formally specified to well analyse results (e.g., the supervised learning of SL, the definition of function g, etc.). 

- Environments look quite simple (short trajectories, few actions, deterministic transitions, full observability). It would be helpful to see more classical complex tasks of RL, in environments such as TextWorld, Alfred or BabyAI for instances. ","My major concern about this paper is its novelty. While it may be quite interesting for the community, I feel that its findings are not sufficient for the level of ICLR.   ","  - Could authors position their work w.r.t literature on prompt tuning, zero-shot learning with pre-trained LLMs (in particular, the last reference I give below looks very connected to the presented work), and also w.r.t. reward shaping with language? 
   - The global learning setting is not fully clear to me. How is defined or learned the function g that maps the textual outcome of the LLM to an int reward ? It is a language model too ? It is learned from binary true rewards ? If it is manually defined, how do authors manage variability of llm outputs ? (for expected answers such as yes or no, it is possible that the llm outputs something different right ? or do authors restrict its possible outcomes ?) 
   - In fig 8 right, the prompt finishes with ""let think step by step :"". So the LLM is supposed to propose a step by step forecasting of reasoning ?  
   




Multitask Prompted Training Enables Zero-Shot Task Generalization. ICLR 2022

Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.

Ella: Exploration through learned language abstraction. In Advances in Neural Information Processing Systems (NeurIPS), 2021

EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. CoRR abs/2206.09674 (2022)

Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. (2022)
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666374240442,,10uNUgI5Kl,ICLR.cc/2023/Conference/Paper5175/-/Official_Review,,[],3,,['everyone'],10uNUgI5Kl,['ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1'],1666374240442,,1669976419378,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1']","Yes, the peer review suggests that the authors of the paper should refer to specific literature that may not be already discussed in the original paper. The reviewer mentions several papers, including:

1. ""Multitask Prompted Training Enables Zero-Shot Task Generalization"" (ICLR 2022)
2. ""Finetuned language models are zero-shot learners"" (arXiv preprint arXiv:2109.01652)
3. ""Ella: Exploration through learned language abstraction"" (NeurIPS 2021)
4. ""EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL"" (CoRR abs/2206.09674, 2022)
5. ""Do as i can, not as i say: Grounding language in robotic affordances"" (arXiv preprint arXiv:2204.01691, 2022)

The reviewer specifically asks the authors to position their work with respect to literature on prompt tuning, zero-shot learning with pre-trained LLMs, and reward shaping with language, suggesting that these references may not be adequately addressed in the original paper.",1,"2022, 2021, 2021, 2022, 2022, 2022, 2022",Multitask Prompted Training Enables Zero-Shot Task Generalization 
mxp8FFEaIPL,"The paper ""Reward Design with Language Models"" proposes to study to effectiveness of pre-trained large language models for designing rewards in RL tasks (tasks implying language). It shows that, somehow unsurprisingly given recent literature, leveraging these models to drive agents with textual prompts is possible in few-shot or zero-shot settings. ","Strengths: 
- Using textual prompts to drive RL agents is an emerging topic with promising outcomes

- Promising results for controling RL agents with language

Weakness: 
    
- Using pre-trained LLMs in zero-shot settings is clearly not new, many studies already exist on that topic. Not on the specific tasks authors suggest, but the transferability of their embedded knowledge with well specified prompts is well known. Prompt tuning is an emerging field following that observation. 

- Shaping rewards with LLMs is not new either, as many works recently proposed to leverage them to split difficult tasks into textual subgoals, or to drive agents by asking questions for instances, in a curriculum RL fashion. These works should at least be mentioned in the related work.   

- Writing is not always very clear. Readers have to go in many various places again and again during reading to get things together. Very difficult to understand without appendix. Many settings not sufficiently formally specified to well analyse results (e.g., the supervised learning of SL, the definition of function g, etc.). 

- Environments look quite simple (short trajectories, few actions, deterministic transitions, full observability). It would be helpful to see more classical complex tasks of RL, in environments such as TextWorld, Alfred or BabyAI for instances. ","My major concern about this paper is its novelty. While it may be quite interesting for the community, I feel that its findings are not sufficient for the level of ICLR.   ","  - Could authors position their work w.r.t literature on prompt tuning, zero-shot learning with pre-trained LLMs (in particular, the last reference I give below looks very connected to the presented work), and also w.r.t. reward shaping with language? 
   - The global learning setting is not fully clear to me. How is defined or learned the function g that maps the textual outcome of the LLM to an int reward ? It is a language model too ? It is learned from binary true rewards ? If it is manually defined, how do authors manage variability of llm outputs ? (for expected answers such as yes or no, it is possible that the llm outputs something different right ? or do authors restrict its possible outcomes ?) 
   - In fig 8 right, the prompt finishes with ""let think step by step :"". So the LLM is supposed to propose a step by step forecasting of reasoning ?  
   




Multitask Prompted Training Enables Zero-Shot Task Generalization. ICLR 2022

Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.

Ella: Exploration through learned language abstraction. In Advances in Neural Information Processing Systems (NeurIPS), 2021

EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. CoRR abs/2206.09674 (2022)

Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. (2022)
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666374240442,,10uNUgI5Kl,ICLR.cc/2023/Conference/Paper5175/-/Official_Review,,[],3,,['everyone'],10uNUgI5Kl,['ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1'],1666374240442,,1669976419378,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1']","Yes, the peer review suggests that the authors of the paper should refer to specific literature that may not be already discussed in the original paper. The reviewer mentions several papers, including:

1. ""Multitask Prompted Training Enables Zero-Shot Task Generalization"" (ICLR 2022)
2. ""Finetuned language models are zero-shot learners"" (arXiv preprint arXiv:2109.01652)
3. ""Ella: Exploration through learned language abstraction"" (NeurIPS 2021)
4. ""EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL"" (CoRR abs/2206.09674, 2022)
5. ""Do as i can, not as i say: Grounding language in robotic affordances"" (arXiv preprint arXiv:2204.01691, 2022)

The reviewer specifically asks the authors to position their work with respect to literature on prompt tuning, zero-shot learning with pre-trained LLMs, and reward shaping with language, suggesting that these references may not be adequately addressed in the original paper.",1,"2022, 2021, 2021, 2022, 2022, 2022, 2022",Finetuned language models are zero-shot learners 
mxp8FFEaIPL,"The paper ""Reward Design with Language Models"" proposes to study to effectiveness of pre-trained large language models for designing rewards in RL tasks (tasks implying language). It shows that, somehow unsurprisingly given recent literature, leveraging these models to drive agents with textual prompts is possible in few-shot or zero-shot settings. ","Strengths: 
- Using textual prompts to drive RL agents is an emerging topic with promising outcomes

- Promising results for controling RL agents with language

Weakness: 
    
- Using pre-trained LLMs in zero-shot settings is clearly not new, many studies already exist on that topic. Not on the specific tasks authors suggest, but the transferability of their embedded knowledge with well specified prompts is well known. Prompt tuning is an emerging field following that observation. 

- Shaping rewards with LLMs is not new either, as many works recently proposed to leverage them to split difficult tasks into textual subgoals, or to drive agents by asking questions for instances, in a curriculum RL fashion. These works should at least be mentioned in the related work.   

- Writing is not always very clear. Readers have to go in many various places again and again during reading to get things together. Very difficult to understand without appendix. Many settings not sufficiently formally specified to well analyse results (e.g., the supervised learning of SL, the definition of function g, etc.). 

- Environments look quite simple (short trajectories, few actions, deterministic transitions, full observability). It would be helpful to see more classical complex tasks of RL, in environments such as TextWorld, Alfred or BabyAI for instances. ","My major concern about this paper is its novelty. While it may be quite interesting for the community, I feel that its findings are not sufficient for the level of ICLR.   ","  - Could authors position their work w.r.t literature on prompt tuning, zero-shot learning with pre-trained LLMs (in particular, the last reference I give below looks very connected to the presented work), and also w.r.t. reward shaping with language? 
   - The global learning setting is not fully clear to me. How is defined or learned the function g that maps the textual outcome of the LLM to an int reward ? It is a language model too ? It is learned from binary true rewards ? If it is manually defined, how do authors manage variability of llm outputs ? (for expected answers such as yes or no, it is possible that the llm outputs something different right ? or do authors restrict its possible outcomes ?) 
   - In fig 8 right, the prompt finishes with ""let think step by step :"". So the LLM is supposed to propose a step by step forecasting of reasoning ?  
   




Multitask Prompted Training Enables Zero-Shot Task Generalization. ICLR 2022

Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.

Ella: Exploration through learned language abstraction. In Advances in Neural Information Processing Systems (NeurIPS), 2021

EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. CoRR abs/2206.09674 (2022)

Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. (2022)
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666374240442,,10uNUgI5Kl,ICLR.cc/2023/Conference/Paper5175/-/Official_Review,,[],3,,['everyone'],10uNUgI5Kl,['ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1'],1666374240442,,1669976419378,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1']","Yes, the peer review suggests that the authors of the paper should refer to specific literature that may not be already discussed in the original paper. The reviewer mentions several papers, including:

1. ""Multitask Prompted Training Enables Zero-Shot Task Generalization"" (ICLR 2022)
2. ""Finetuned language models are zero-shot learners"" (arXiv preprint arXiv:2109.01652)
3. ""Ella: Exploration through learned language abstraction"" (NeurIPS 2021)
4. ""EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL"" (CoRR abs/2206.09674, 2022)
5. ""Do as i can, not as i say: Grounding language in robotic affordances"" (arXiv preprint arXiv:2204.01691, 2022)

The reviewer specifically asks the authors to position their work with respect to literature on prompt tuning, zero-shot learning with pre-trained LLMs, and reward shaping with language, suggesting that these references may not be adequately addressed in the original paper.",1,"2022, 2021, 2021, 2022, 2022, 2022, 2022",Ella Exploration through learned language abstraction 
mxp8FFEaIPL,"The paper ""Reward Design with Language Models"" proposes to study to effectiveness of pre-trained large language models for designing rewards in RL tasks (tasks implying language). It shows that, somehow unsurprisingly given recent literature, leveraging these models to drive agents with textual prompts is possible in few-shot or zero-shot settings. ","Strengths: 
- Using textual prompts to drive RL agents is an emerging topic with promising outcomes

- Promising results for controling RL agents with language

Weakness: 
    
- Using pre-trained LLMs in zero-shot settings is clearly not new, many studies already exist on that topic. Not on the specific tasks authors suggest, but the transferability of their embedded knowledge with well specified prompts is well known. Prompt tuning is an emerging field following that observation. 

- Shaping rewards with LLMs is not new either, as many works recently proposed to leverage them to split difficult tasks into textual subgoals, or to drive agents by asking questions for instances, in a curriculum RL fashion. These works should at least be mentioned in the related work.   

- Writing is not always very clear. Readers have to go in many various places again and again during reading to get things together. Very difficult to understand without appendix. Many settings not sufficiently formally specified to well analyse results (e.g., the supervised learning of SL, the definition of function g, etc.). 

- Environments look quite simple (short trajectories, few actions, deterministic transitions, full observability). It would be helpful to see more classical complex tasks of RL, in environments such as TextWorld, Alfred or BabyAI for instances. ","My major concern about this paper is its novelty. While it may be quite interesting for the community, I feel that its findings are not sufficient for the level of ICLR.   ","  - Could authors position their work w.r.t literature on prompt tuning, zero-shot learning with pre-trained LLMs (in particular, the last reference I give below looks very connected to the presented work), and also w.r.t. reward shaping with language? 
   - The global learning setting is not fully clear to me. How is defined or learned the function g that maps the textual outcome of the LLM to an int reward ? It is a language model too ? It is learned from binary true rewards ? If it is manually defined, how do authors manage variability of llm outputs ? (for expected answers such as yes or no, it is possible that the llm outputs something different right ? or do authors restrict its possible outcomes ?) 
   - In fig 8 right, the prompt finishes with ""let think step by step :"". So the LLM is supposed to propose a step by step forecasting of reasoning ?  
   




Multitask Prompted Training Enables Zero-Shot Task Generalization. ICLR 2022

Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.

Ella: Exploration through learned language abstraction. In Advances in Neural Information Processing Systems (NeurIPS), 2021

EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. CoRR abs/2206.09674 (2022)

Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. (2022)
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666374240442,,10uNUgI5Kl,ICLR.cc/2023/Conference/Paper5175/-/Official_Review,,[],3,,['everyone'],10uNUgI5Kl,['ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1'],1666374240442,,1669976419378,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1']","Yes, the peer review suggests that the authors of the paper should refer to specific literature that may not be already discussed in the original paper. The reviewer mentions several papers, including:

1. ""Multitask Prompted Training Enables Zero-Shot Task Generalization"" (ICLR 2022)
2. ""Finetuned language models are zero-shot learners"" (arXiv preprint arXiv:2109.01652)
3. ""Ella: Exploration through learned language abstraction"" (NeurIPS 2021)
4. ""EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL"" (CoRR abs/2206.09674, 2022)
5. ""Do as i can, not as i say: Grounding language in robotic affordances"" (arXiv preprint arXiv:2204.01691, 2022)

The reviewer specifically asks the authors to position their work with respect to literature on prompt tuning, zero-shot learning with pre-trained LLMs, and reward shaping with language, suggesting that these references may not be adequately addressed in the original paper.",1,"2022, 2021, 2021, 2022, 2022, 2022, 2022",EAGER Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL 
mxp8FFEaIPL,"The paper ""Reward Design with Language Models"" proposes to study to effectiveness of pre-trained large language models for designing rewards in RL tasks (tasks implying language). It shows that, somehow unsurprisingly given recent literature, leveraging these models to drive agents with textual prompts is possible in few-shot or zero-shot settings. ","Strengths: 
- Using textual prompts to drive RL agents is an emerging topic with promising outcomes

- Promising results for controling RL agents with language

Weakness: 
    
- Using pre-trained LLMs in zero-shot settings is clearly not new, many studies already exist on that topic. Not on the specific tasks authors suggest, but the transferability of their embedded knowledge with well specified prompts is well known. Prompt tuning is an emerging field following that observation. 

- Shaping rewards with LLMs is not new either, as many works recently proposed to leverage them to split difficult tasks into textual subgoals, or to drive agents by asking questions for instances, in a curriculum RL fashion. These works should at least be mentioned in the related work.   

- Writing is not always very clear. Readers have to go in many various places again and again during reading to get things together. Very difficult to understand without appendix. Many settings not sufficiently formally specified to well analyse results (e.g., the supervised learning of SL, the definition of function g, etc.). 

- Environments look quite simple (short trajectories, few actions, deterministic transitions, full observability). It would be helpful to see more classical complex tasks of RL, in environments such as TextWorld, Alfred or BabyAI for instances. ","My major concern about this paper is its novelty. While it may be quite interesting for the community, I feel that its findings are not sufficient for the level of ICLR.   ","  - Could authors position their work w.r.t literature on prompt tuning, zero-shot learning with pre-trained LLMs (in particular, the last reference I give below looks very connected to the presented work), and also w.r.t. reward shaping with language? 
   - The global learning setting is not fully clear to me. How is defined or learned the function g that maps the textual outcome of the LLM to an int reward ? It is a language model too ? It is learned from binary true rewards ? If it is manually defined, how do authors manage variability of llm outputs ? (for expected answers such as yes or no, it is possible that the llm outputs something different right ? or do authors restrict its possible outcomes ?) 
   - In fig 8 right, the prompt finishes with ""let think step by step :"". So the LLM is supposed to propose a step by step forecasting of reasoning ?  
   




Multitask Prompted Training Enables Zero-Shot Task Generalization. ICLR 2022

Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.

Ella: Exploration through learned language abstraction. In Advances in Neural Information Processing Systems (NeurIPS), 2021

EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. CoRR abs/2206.09674 (2022)

Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. (2022)
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666374240442,,10uNUgI5Kl,ICLR.cc/2023/Conference/Paper5175/-/Official_Review,,[],3,,['everyone'],10uNUgI5Kl,['ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1'],1666374240442,,1669976419378,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1']","Yes, the peer review suggests that the authors of the paper should refer to specific literature that may not be already discussed in the original paper. The reviewer mentions several papers, including:

1. ""Multitask Prompted Training Enables Zero-Shot Task Generalization"" (ICLR 2022)
2. ""Finetuned language models are zero-shot learners"" (arXiv preprint arXiv:2109.01652)
3. ""Ella: Exploration through learned language abstraction"" (NeurIPS 2021)
4. ""EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL"" (CoRR abs/2206.09674, 2022)
5. ""Do as i can, not as i say: Grounding language in robotic affordances"" (arXiv preprint arXiv:2204.01691, 2022)

The reviewer specifically asks the authors to position their work with respect to literature on prompt tuning, zero-shot learning with pre-trained LLMs, and reward shaping with language, suggesting that these references may not be adequately addressed in the original paper.",1,"2022, 2021, 2021, 2022, 2022, 2022, 2022",Do as i can not as i say Grounding language in robotic affordances
KjPayidV98n,"The authors propose to address the shortcomings in image captioning models by generating richer captions with additional object attribute information. They pretrain vision language based transformer models using an additional masked attribute loss and match the object information to the corresponding attributes using Hungarian matching algorithm. They further finetune the transformer to generate informative captions conditioned on previous words, object labels , attributes and region based visual features.","Strengths - The motivation of the paper to generate more descriptive and well grounded captions for an image is well inspired from the limitations of existing methods. They  integrate an additional loss based on object attributes to the pre-training of Vision language transformers. 

Weaknesses - #

In the pretraining stage in Fig 4, a very useful ablation / simpler modeling strategy would be to combine the object tags + attributes and then mask that information randomly for token loss prediction.

Due to the absence of masked region modeling, the model might fail to draw associations between attributes / tags and regions and hence fail to generate novel combinations of attributes + tags (black hat - > black dog) during caption generation.

A study of attention maps from the transformers could help to see what muli-modal attention is being learned by the model. 

The results are far below the baselines and are not justified well by using human evaluation, I recommend the authors to compare using some human evaluation. 

Another metric - SPICE - U [1] could be more useful in measuring such descriptive captions. 

The contributions are very limited compared to VIVO (baseline) and the results worse. 

[1]Wang, Zeyu, et al. ""Towards unique and informative captioning of images."" European Conference on Computer Vision. Springer, Cham, 2020.
",The paper is original and well written. The figures are small and not very clear and should be improved. ,"The contributions and experimental outcomes of the paper are marginal and not well supported by ablations, human studies or architectural changes. I propose major improvements both in the method and evaluation for a good submission. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,Not applicable,NO.,"3: reject, not good enough",1666539720210,,0uHNy9jmR7z,ICLR.cc/2023/Conference/Paper2956/-/Official_Review,,[],1,,['everyone'],0uHNy9jmR7z,['ICLR.cc/2023/Conference/Paper2956/Reviewer_4nF4'],1666539720210,,1666539720210,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2956/Reviewer_4nF4']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions a specific paper by Wang et al. (2020) titled ""Towards unique and informative captioning of images"" and suggests using the SPICE-U metric, which is introduced in that paper, to measure the descriptiveness of captions. This indicates that the reviewer believes the authors may not be aware of this paper or metric, and that it could be relevant to their work.",1,2020,Towards unique and informative captioning of images
XuGBrgGKSX,"
The authors study the loss landscape of multilayer convolutional neural
networks, and in particular consider the construction of spurious minimizers of
networks trained on regression/classification tasks. They consider networks
with fully connected, convolutional, max-pooling, and average pooling layers,
as well as ReLU nonlinearities. Their main results (1) construct spurious
minimizers in a network assuming that a subnetwork construction is satisfied
and a certain nondegeneracy assumption holds at the local minimizer obtained
from this subnetwork construction; (2) show in a specific setting (slightly
nonstandard architecture) that a subnetwork can explicitly be constructed to
give a local minimizer. 
","## Strengths

The authors argue that their approach to constructing local minima is modular,
in that the task is reduced to studying a local minimum of a subnetwork (which
can then be embedded into larger networks). This may be useful for obtaining
general conclusions. They mention on page 5 that this is new in the context of
CNNs (standard for feedforward nets).

The analysis seems highly technical and intricate, involving detailed
index-level manipulations of the filters in the network. (This also makes it
challenging to verify). The result considers fairly general CNN architectures
and various loss functions, including regression and classification losses. 

## Weaknesses

The claims of novelty in the introduction (end of first paragraph, end of third
paragraph) are not correct: the related work section even discusses a work of
Du et al. on one-layer CNNs that shows spurious minima (and see similar works
not cited, such as https://arxiv.org/abs/1909.03172). This is not hard to fix:
you can claim the first result on spurious minimizers in deep CNNs / CNNs with
several channels, etc. I would also recommend an additional literature search
for possible related references in this context, since the one I mention here
was missed.

Claim about argmax at bottom of page 3 and top of page 4: I do not think this
is correct in general -- consider the case where there are two distinct indices
$i$ which achieve the argmax corresponding to different filters, and suppose
the coordinates $o_p^{l-1}(i)$ are open mappings (is there any reason for them
not to be?). Then every neighborhood of the parameters $W, b$ contains points
at which the argmax is taken uniquely on a distinct coordinate involved in the
tie; it implies that there is no locally-valid way to define a *constant* matrix
that selects the right element of the max-pooling operation. The assumption that the 
coordinate maps are open is only sufficient; it should be possible to construct
counterexamples under more general conditions specific to the definition of the
feature maps.

The result relies on a technical assumption (Assumption 1) that seems to
significantly limit the scope of the theory (or even make it vacuous).
Assumption 1 is a nondegeneracy assumption on the network features at a local
minimizer (this is used to prove that the minimizer is spurious). As a result
of the dependence on this assumption, it does not seem that the theory makes
any predictions about the existence or non-existence of spurious minimizers in
any actual network architectures. Based on the way Section 4 and Theorem 2-3
are written, I understand that even in the authors' construction of a concrete
local minimum, it is still necessary to make assumption 1 in order to conclude
that the local minimum is spurious. In the rebuttal, I would appreciate if the
authors could enlighten me on whether assumptions like this are standard in
other works that study spurious minimizers in neural networks (say, in the
well-studied feedforward setting). From my perspective, I would find it hard to
understand why the natural extension from the results mentioned in the related
work section by Du et al. and the one I mentioned above for concrete spurious
minimizers non-overlapping-receptive-field and one-layer Convnets would not be
something for single-layer and overlapping receptive field networks, or
multi-layer and non-overlapping receptive field networks, rather than the
extremely general setting the authors consider here (given that there are no
concrete conclusions in this setting in the presented work). 

## Minor / Questions

Bottom of page 1 and elsewhere: ""perturbated"" -> ""perturbed"" (""perturbate"" does
not sound correct)
","The paper is very technical and written in a nonintuitive way -- for example,
when the authors describe their results and techniques in the introduction of
the paper, it reads like a direct translation of some of the proof techniques
into natural language rather than as an elucidation of the ideas behind the
proof (why such a result is true, how one might arrive at it, etc.). This makes
the effort required to penetrate the paper for a nonspecialist unduly high.

In general (following comments in the previous section), the preliminaries
section 2 is written in an unclear and disorganized style that is overly
general and unnecessarily confusing.  For example, the definition of the max
pooling operation (3) is not clearly made with respect to its interaction with
distinct filters $T_{\ell}$ in a layer; the use of a matrix $W^{\ell}$ for all
operations does not accurately show the distinct structures of each operation
(average pool; convolution; fully connected) as functions of the actual weight
parameters of the network and architecture parameters, and hence makes it seem
unnecessary to define such intricate notation here (just put it in an appendix
instead, if it is not essential to understanding the main body of the paper?);
the discussion of feature map padding is done vaguely in an unhelpful way (one
wants to know here **what restrictions does this place on the various
parameters of the network**? In (2), the input feature map dimension is 5 and
the output dimension is 6. If I pad the input by 1, now the input dimension is
6 and the output dimension is 8. It does not seem to be possible to pad the
input feature map to make the input and output sizes equal!). I would recommend
the authors polish the writing and only introduce notation that will be
necessary to discuss the main results that are presented in the subsequent
sections of the main paper.


The technical presentation of the argument reads as though it is poorly
organized, as well. Lemma 1 is a technical result that seems rather abstruse,
and it also applies to a very limited setting (only Conv + ReLU layers); the
lemma has unnatural technical assumptions on nondegeneracy, and after
presenting it the authors then state that ""the conclusion in Lemma 1 still
holds"" for more general settings without any proof or connection to a result in
the appendices. Why not present this result informally, with the necessary
links to the fully general versions in the appendices made? The second
paragraph of section 3.1 says ""for ease of presentation"", but this seems to be
off to me -- the result presented is still very technical, and the
simplifications seem to rather have the effect of making it unclear what
networks the authors' result actually applies to, rather than the stated
intention. The same could be suggested for the following material that leads to
Theorem 1, although here the technicality may be essential.

","
The strength of the assumptions made in the theory seems to significantly limit
its scope. As a result, although the work is technically impressive, it does
not seem to shed much novel light on when and why spurious minimizers exist in
convolutional neural networks trained on various practical tasks. The notation
and writing could do with additional polishing.
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,Not applicable,NO.,5: marginally below the acceptance threshold,1667237234017,,0sjwFxqLHw3,ICLR.cc/2023/Conference/Paper501/-/Official_Review,,[],4,,['everyone'],0sjwFxqLHw3,['ICLR.cc/2023/Conference/Paper501/Reviewer_L56V'],1667237234017,,1667237234017,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper501/Reviewer_L56V']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions a specific work (https://arxiv.org/abs/1909.03172) that is not cited in the paper and suggests that the authors should conduct an additional literature search to find other related references. This implies that the reviewer is aware of existing literature that is relevant to the topic, but is not included in the original paper.",1,2019,Towards Understanding the Importance of Noise in Training Neural Networks
CkcXL0_TWT,"The paper compares the ReLU activation function to its many smooth variants that converge to it in the limit of low temperatures. The paper proposes to investigate the role of these two sub-classes of activation functions by looking at their gradient propagation at initialization time. In particular, prior work has shown that there are some initializations that produce the known phenomenon of exploding/vanishing gradients. In addition, there are specific initializations where the magnitude of gradients stays constant on average throughout the network. When the latter is possible the paper defines it as a criticality. In this work this property is studied for a number of variants of smoothed ReLUs and how it changes with temperature. The paper proves the sufficient and necessary conditions for a smoothed ReLU kind of function to have a critical initialization scheme, and then examines how it manifests for the various activations, including whether this property is stable or not (stable for small pertubations). The paper continues with examining the various activation function through the lens of Neural Tanget Kernel, and use that to argue when smooth ReLU follows similar properties as the non-smooth ReLU. Finally, the paper examines how these activation functions affect the training dynamics, showing a very different behavior of the smooth vs. the non-smooth case.","Strengths:
* With an evergrowing zoo of activation functions, often very similar to each other, there is a growing need for a crisp theory to understand their inherent properties, and when one should prefer a specific one. The family of ReLU-like activation functions is vast, and this paper proposes a method to study a large set of them if they can be represented as $f(z, T) = z \cdot a(\frac{z}{T})$ for some sigmoid function $a(z)$ that converges to the Heaviside function for $T \to 0$. Such a general result could have a major impact on how activation functions are used and how new ones could be designed.
* The paper also examines 3 novel kinds of smooth ReLUs based on this generic definition, and examines their properties. This demonstrates the breadth of the theoretical framework.
* As a side note, I suggest the authors to consider adding to their table the Softplus function (the ""original"" smooth alternative to ReLU). It too can be put into the studied form by rewriting it as $Softplus(z) = z \cdot a(z/T)$ for $a(x) = \frac{\ln(1+\exp(x))}{x}$, and it can be shown that $a(x)$ converges to the Heaviside function as the temperature goes to zero, but $a(x)$ is not a sigmoid function. It is unclear in your analysis if $a(x)$ being a sigmoid is critical to your results or not, and having a function that does not follow this pattern could shed more light.

Weaknesses:
* The paper has serious clarity issues, starting with not giving a clear definition of the core expressions in its analysis. For example, a very loose definition of $< \cdot >$ is given in the text, and the definition for $< \cdot >_K$ is completely absent. One has to read one of the cited works [1] to understand what it means, and the paper doesn't even bother directing the reader to do so. Moreover, after examining the other paper that defines these terms, it raised even more questions. This is because only at the end of section 2.1 is it apparent that the prior definitions were for the limit of infinite width/depth. A much more in-depth background is needed to explain how this limit is taken. Similarly, NTK is poorly introduced, giving no insight into its definitions and why the reader should care. This paper essentially requires you to be well versed in the literature on NTK and the works on the limit of infinite NN as Gaussian processes.
* Given this point, I had a hard time understanding and verifying the correctness of this work. Therefore, I cannot judge whether the analysis is accurate or evaluate its limitations.
* Furthermore, due to the various clarity issues, many of the results are essentially left unexplained and unmotivated. This is too bad, as beyond the point on criticality, which I understood, I cannot really say I understood the point of the later sections that are based on NTK, which is a shame as I'd like to believe there might be interesting insights there.
* Of a minor note, it appears that the style file was modified. I'm not taking this into account in my rating, but simply pointing this out for the AC for their consideration.

[1] - B. Hanin. Correlation functions in random fully connected neural networks at finite width. arXiv preprint arXiv:2204.01058, 2022.","Clarity: A lot to be desired. See weakneeses above for details.

Quality: Difficult to judge given many clarity issues. Of the little I could verify, I didn't find any mistakes.

Novelty: The results and analysis build on prior works (NTK and NN as Gaussian processes), but the specific application on smooth ReLUs is novel and could have significant impact on practical usage.

Reproducibility: Again, clarity issues make this not very reproducible.","Due to the severe clarity issues, I cannot recommend acceptance. It could be that a reviewer that is more versed in the prior works this work builds upon could evaluate the merits of this submission independently of its presentation, but I am (sadly) unable to do so. I hope the authors will take this criticism and revise their manuscript to be accessible to a broader readership, as what I did understand I liked.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666678857147,,0qnryNf6XwR,ICLR.cc/2023/Conference/Paper2686/-/Official_Review,,[],2,,['everyone'],0qnryNf6XwR,['ICLR.cc/2023/Conference/Paper2686/Reviewer_AkRp'],1666678857147,,1666678857147,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2686/Reviewer_AkRp']","Yes, the peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer mentions that they had to read one of the cited works ([1] - B. Hanin. Correlation functions in random fully connected neural networks at finite width. arXiv preprint arXiv:2204.01058, 2022) to understand the definition of certain terms, and even then, it raised more questions. The reviewer also implies that the paper assumes a high level of background knowledge in the literature on NTK and the works on the limit of infinite NN as Gaussian processes, which may not be accessible to all readers. Additionally, the reviewer suggests adding the Softplus function to the analysis, which is not mentioned in the original paper.",1,"2022, 2022",Correlation functions in random fully connected neural networks at finite width
C80-6o1yeor,"This paper suggests dynamic adversarial contrastive learning which gradually anneals from a strong augmentation to a weak augmentation. Further, the authors propose fast post-processing stage for adapting it to classification tasks which boost the robustness. From this simple and effective strategy, DynACL reduces the gap between supervised AT and selfsup AT. ","**Strength**

- This paper proposes a simple and effective scheduling method for selfsup-AT which could surprisingly improve the robustness.
- The paper is well-written and shows convincing motivations.
- This paper suggests novel aspects of the relationship between augmentation and adversarial training in selfsup-AT.
- This paper demonstrates extensive experimental results including diverse ablation experiments and semi-supervised settings.

**Concerns and Questions**
- I hope the authors describe the difference between the DynACL++ and AdvCL post-processing which pseudo labels the unlabeled examples with k mean clustering. I think it is overemphasized claims that fast post-processing is a novel idea.
- Does proposed technique also can be adapted to RoCL? And also boost the performance of this method too?
- Does dynamic scheduling also benefit the performance of vanilla simCLR?
- The proposed dynamic scheduling which gradually decreases the strength of the augmentation seems a somewhat empirical approach seems bit simple approach without any convincing motivation or evidence. It is understandable that we should control the strength of augmentation but I am not sure proposed method is the most effective design of data augmentation. Because if there is some standard to find the adequate strength of the augmentation at a certain training stage of self-AT (i.e., classwise distance, or MMD), an adaptive strength controller could show better robustness than the gradual scheduling.
- I am not quite sure this approach could be generalized to different architectures, or different frameworks. Since the approach is a naive approach, hyperparameter search may be always needed for optimal performance. And the current gradual scheduling could not be the best option for some circumstances (where may need a longer step size). I think adaptive data augmentation scheduling [1,2] could also be applied to self-AT which could improve the current approach more intuitively and make the approach to be more universal to diverse circumstances.

[1] ADAAUG: LEARNING CLASS- AND INSTANCE- ADAPTIVE DATA AUGMENTATION POLICIES, ICLR 2022

[2] MetaAugment: Sample-Aware Data Augmentation Policy Learning, AAAI 2021","**Clarity:** The paper is easy to understand and well-written.

**Quality:** The presentation of the paper is good and well-organized.

**Novelty:** The dynamic gradual design of the data augmentation seems not very novel to me but it is simple and effective.

**Reproducibility:** The paper has well reproducibility which elaborates well on the details.","Overall, I recommend acceptance to this paper. This is well written and has intuitive motivation with empirical results.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,Not applicable,NO.,"8: accept, good paper",1666701074621,,0qmwFNJyxCL,ICLR.cc/2023/Conference/Paper756/-/Official_Review,,[],3,,['everyone'],0qmwFNJyxCL,['ICLR.cc/2023/Conference/Paper756/Reviewer_umnn'],1666701074621,,1668500238792,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper756/Reviewer_umnn']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. ADAAUG: LEARNING CLASS- AND INSTANCE- ADAPTIVE DATA AUGMENTATION POLICIES, ICLR 2022
2. MetaAugment: Sample-Aware Data Augmentation Policy Learning, AAAI 2021

These papers are related to adaptive data augmentation scheduling, which the reviewer thinks could be applied to self-AT and improve the current approach. The reviewer mentions that these papers could make the approach more universal to diverse circumstances, implying that they are not already discussed in the original paper.",1,"2022, 2021",ADAAUG: LEARNING CLASS- AND INSTANCE- ADAPTIVE DATA AUGMENTATION POLICIES 
C80-6o1yeor,"This paper suggests dynamic adversarial contrastive learning which gradually anneals from a strong augmentation to a weak augmentation. Further, the authors propose fast post-processing stage for adapting it to classification tasks which boost the robustness. From this simple and effective strategy, DynACL reduces the gap between supervised AT and selfsup AT. ","**Strength**

- This paper proposes a simple and effective scheduling method for selfsup-AT which could surprisingly improve the robustness.
- The paper is well-written and shows convincing motivations.
- This paper suggests novel aspects of the relationship between augmentation and adversarial training in selfsup-AT.
- This paper demonstrates extensive experimental results including diverse ablation experiments and semi-supervised settings.

**Concerns and Questions**
- I hope the authors describe the difference between the DynACL++ and AdvCL post-processing which pseudo labels the unlabeled examples with k mean clustering. I think it is overemphasized claims that fast post-processing is a novel idea.
- Does proposed technique also can be adapted to RoCL? And also boost the performance of this method too?
- Does dynamic scheduling also benefit the performance of vanilla simCLR?
- The proposed dynamic scheduling which gradually decreases the strength of the augmentation seems a somewhat empirical approach seems bit simple approach without any convincing motivation or evidence. It is understandable that we should control the strength of augmentation but I am not sure proposed method is the most effective design of data augmentation. Because if there is some standard to find the adequate strength of the augmentation at a certain training stage of self-AT (i.e., classwise distance, or MMD), an adaptive strength controller could show better robustness than the gradual scheduling.
- I am not quite sure this approach could be generalized to different architectures, or different frameworks. Since the approach is a naive approach, hyperparameter search may be always needed for optimal performance. And the current gradual scheduling could not be the best option for some circumstances (where may need a longer step size). I think adaptive data augmentation scheduling [1,2] could also be applied to self-AT which could improve the current approach more intuitively and make the approach to be more universal to diverse circumstances.

[1] ADAAUG: LEARNING CLASS- AND INSTANCE- ADAPTIVE DATA AUGMENTATION POLICIES, ICLR 2022

[2] MetaAugment: Sample-Aware Data Augmentation Policy Learning, AAAI 2021","**Clarity:** The paper is easy to understand and well-written.

**Quality:** The presentation of the paper is good and well-organized.

**Novelty:** The dynamic gradual design of the data augmentation seems not very novel to me but it is simple and effective.

**Reproducibility:** The paper has well reproducibility which elaborates well on the details.","Overall, I recommend acceptance to this paper. This is well written and has intuitive motivation with empirical results.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,Not applicable,NO.,"8: accept, good paper",1666701074621,,0qmwFNJyxCL,ICLR.cc/2023/Conference/Paper756/-/Official_Review,,[],3,,['everyone'],0qmwFNJyxCL,['ICLR.cc/2023/Conference/Paper756/Reviewer_umnn'],1666701074621,,1668500238792,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper756/Reviewer_umnn']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. ADAAUG: LEARNING CLASS- AND INSTANCE- ADAPTIVE DATA AUGMENTATION POLICIES, ICLR 2022
2. MetaAugment: Sample-Aware Data Augmentation Policy Learning, AAAI 2021

These papers are related to adaptive data augmentation scheduling, which the reviewer thinks could be applied to self-AT and improve the current approach. The reviewer mentions that these papers could make the approach more universal to diverse circumstances, implying that they are not already discussed in the original paper.",1,"2022, 2021",MetaAugment: Sample-Aware Data Augmentation Policy Learning
Rjg5ylzOXTK,"Remarking that language models can assign high probability to low “quality” outputs, this paper proposes “sequence likelihood calibration” (SLiC) as a solution. With SLiC, “decoding heuristics become unnecessary” and “quality significantly improves.” The claims are empirically supported by experiments on abstractive summarization, question generation, abstractive question answering, and data-to-text generation, with results that “exceed or match SOTA results.”

In more detail, the high-level idea proceeds in two stages. In the first stage, for each training instance, an initial model is used to sample m candidates (a variety of decoding methods are compared, including beam search and nucleus sampling). In the second stage, the initial model is further optimised using a combination of two losses:

A “calibration” loss which aims to align the sequence likelihood with the similarity to the reference target sequence. Four such losses are considered, including a ranking loss, margin loss, listwise ranking loss, and expected reward. All losses rely on a notion of similarity of a candidate sequence to the reference sequence, for which a variation of BERTScore is proposed (S2.1).
A “regularization” loss which penalizes deviations from the token-level confidences from the stage-one model. The cross-entropy to the reference labels and the KL divergence from the stage-one model predictive distribution are both considered.

Overall, beam search using the rank “calibration” loss with KL divergence “regularization” loss are found to work best.","Overall, it is notable that the proposed approach yields models that do not appear to require decoding heuristics; this suggests that the calibration loss is doing something useful. Figure 3 is also fairly convincing in this respect, as it shows a distinct difference between the vanilla model and the calibrated one when further decoding samples are used (consistent improvement for the calibrated model). However, the experiments overall are not entirely convincing, since in several cases the degree of improvement over the baselines is marginal, while the proposed model is larger than the baselines in terms of # of parameters (2B). So it’s not clear that it’s the new calibration scheme that’s responsible for the improvement in these cases.

The scoring function (S2.1) is a key element of the proposal. It’s stated that “compared to using external metrics, [...] it differs from the metrics we evaluate the generation systems with and mitigates the risk of directly optimising towards imperfect metrics.” However, the proposed similarity is related to both ROUGE and BERTScore, and so it’s not clear there isn’t some bias towards better performance on the target metrics.

Finally, I would have liked to see probabilistic calibration addressed somewhere. Does the proposed calibration loss actually lead to more meaningful probability estimates? This could be measured at the token-level or on other events (cf https://papers.nips.cc/paper/2015/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf)","Regarding the presentation, although the approach itself is quite straightforward, I found it difficult to follow the narrative of the paper. At various points, such as the introduction of the calibration losses, many options are considered without proper motivation or discussion of relative merits to justify their inclusion in the experiments. I found the use of “TL;DR;” blocks at the end of certain chapters to be out-of-place; it would have been better to provide this kind of information at the beginning of the sections, ideally framed as research questions.

On related work, the approach is conceptually similar to knowledge distillation, and would benefit from comparing and contrasting both the original Hinton paper (when it comes to the KL regularization loss) and the sequence-level extension (https://arxiv.org/abs/1606.07947) when it comes to the use of sequence-level training. Some other relevant work is not cited, such as papers in non-autoregressive decoding where it is common to train on decoded outputs from an initial “teacher” model.

On reproducibility, the appendix provides many details about hyperparameter choices. On the other hand, there’s no obvious intent to release code, and some of the model sizes considered are prohibitive in terms of compute costs for many researchers.

In Figure 3, what is the “score” in the y-axis?",There are some good ideas here but the presentation could be improved and the experimental results are lackluster. ,"4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666504695289,,0qSOodKmJaN,ICLR.cc/2023/Conference/Paper1988/-/Official_Review,,[],3,,['everyone'],0qSOodKmJaN,['ICLR.cc/2023/Conference/Paper1988/Reviewer_YBaL'],1666504695289,,1669040767287,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1988/Reviewer_YBaL']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

Specifically, the reviewer mentions the following papers that may not be already included in the original paper:
1. The original Hinton paper on knowledge distillation (when it comes to the KL regularization loss).
2. The sequence-level extension of knowledge distillation (https://arxiv.org/abs/1606.07947) when it comes to the use of sequence-level training.
3. Papers in non-autoregressive decoding where it is common to train on decoded outputs from an initial “teacher” model.
4. A paper from NIPS 2015 (https://papers.nips.cc/paper/2015/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf) on probabilistic calibration. 

These literature references are suggested to improve the paper by comparing and contrasting the proposed approach with existing related work, and to address probabilistic calibration which is not discussed in the original paper.",1,"2016, 2015, 2015",Distilling the Knowledge in a Neural Network 
Rjg5ylzOXTK,"Remarking that language models can assign high probability to low “quality” outputs, this paper proposes “sequence likelihood calibration” (SLiC) as a solution. With SLiC, “decoding heuristics become unnecessary” and “quality significantly improves.” The claims are empirically supported by experiments on abstractive summarization, question generation, abstractive question answering, and data-to-text generation, with results that “exceed or match SOTA results.”

In more detail, the high-level idea proceeds in two stages. In the first stage, for each training instance, an initial model is used to sample m candidates (a variety of decoding methods are compared, including beam search and nucleus sampling). In the second stage, the initial model is further optimised using a combination of two losses:

A “calibration” loss which aims to align the sequence likelihood with the similarity to the reference target sequence. Four such losses are considered, including a ranking loss, margin loss, listwise ranking loss, and expected reward. All losses rely on a notion of similarity of a candidate sequence to the reference sequence, for which a variation of BERTScore is proposed (S2.1).
A “regularization” loss which penalizes deviations from the token-level confidences from the stage-one model. The cross-entropy to the reference labels and the KL divergence from the stage-one model predictive distribution are both considered.

Overall, beam search using the rank “calibration” loss with KL divergence “regularization” loss are found to work best.","Overall, it is notable that the proposed approach yields models that do not appear to require decoding heuristics; this suggests that the calibration loss is doing something useful. Figure 3 is also fairly convincing in this respect, as it shows a distinct difference between the vanilla model and the calibrated one when further decoding samples are used (consistent improvement for the calibrated model). However, the experiments overall are not entirely convincing, since in several cases the degree of improvement over the baselines is marginal, while the proposed model is larger than the baselines in terms of # of parameters (2B). So it’s not clear that it’s the new calibration scheme that’s responsible for the improvement in these cases.

The scoring function (S2.1) is a key element of the proposal. It’s stated that “compared to using external metrics, [...] it differs from the metrics we evaluate the generation systems with and mitigates the risk of directly optimising towards imperfect metrics.” However, the proposed similarity is related to both ROUGE and BERTScore, and so it’s not clear there isn’t some bias towards better performance on the target metrics.

Finally, I would have liked to see probabilistic calibration addressed somewhere. Does the proposed calibration loss actually lead to more meaningful probability estimates? This could be measured at the token-level or on other events (cf https://papers.nips.cc/paper/2015/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf)","Regarding the presentation, although the approach itself is quite straightforward, I found it difficult to follow the narrative of the paper. At various points, such as the introduction of the calibration losses, many options are considered without proper motivation or discussion of relative merits to justify their inclusion in the experiments. I found the use of “TL;DR;” blocks at the end of certain chapters to be out-of-place; it would have been better to provide this kind of information at the beginning of the sections, ideally framed as research questions.

On related work, the approach is conceptually similar to knowledge distillation, and would benefit from comparing and contrasting both the original Hinton paper (when it comes to the KL regularization loss) and the sequence-level extension (https://arxiv.org/abs/1606.07947) when it comes to the use of sequence-level training. Some other relevant work is not cited, such as papers in non-autoregressive decoding where it is common to train on decoded outputs from an initial “teacher” model.

On reproducibility, the appendix provides many details about hyperparameter choices. On the other hand, there’s no obvious intent to release code, and some of the model sizes considered are prohibitive in terms of compute costs for many researchers.

In Figure 3, what is the “score” in the y-axis?",There are some good ideas here but the presentation could be improved and the experimental results are lackluster. ,"4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666504695289,,0qSOodKmJaN,ICLR.cc/2023/Conference/Paper1988/-/Official_Review,,[],3,,['everyone'],0qSOodKmJaN,['ICLR.cc/2023/Conference/Paper1988/Reviewer_YBaL'],1666504695289,,1669040767287,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1988/Reviewer_YBaL']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

Specifically, the reviewer mentions the following papers that may not be already included in the original paper:
1. The original Hinton paper on knowledge distillation (when it comes to the KL regularization loss).
2. The sequence-level extension of knowledge distillation (https://arxiv.org/abs/1606.07947) when it comes to the use of sequence-level training.
3. Papers in non-autoregressive decoding where it is common to train on decoded outputs from an initial “teacher” model.
4. A paper from NIPS 2015 (https://papers.nips.cc/paper/2015/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf) on probabilistic calibration. 

These literature references are suggested to improve the paper by comparing and contrasting the proposed approach with existing related work, and to address probabilistic calibration which is not discussed in the original paper.",1,"2016, 2015, 2015",Sequence-Level Knowledge Distillation 
Rjg5ylzOXTK,"Remarking that language models can assign high probability to low “quality” outputs, this paper proposes “sequence likelihood calibration” (SLiC) as a solution. With SLiC, “decoding heuristics become unnecessary” and “quality significantly improves.” The claims are empirically supported by experiments on abstractive summarization, question generation, abstractive question answering, and data-to-text generation, with results that “exceed or match SOTA results.”

In more detail, the high-level idea proceeds in two stages. In the first stage, for each training instance, an initial model is used to sample m candidates (a variety of decoding methods are compared, including beam search and nucleus sampling). In the second stage, the initial model is further optimised using a combination of two losses:

A “calibration” loss which aims to align the sequence likelihood with the similarity to the reference target sequence. Four such losses are considered, including a ranking loss, margin loss, listwise ranking loss, and expected reward. All losses rely on a notion of similarity of a candidate sequence to the reference sequence, for which a variation of BERTScore is proposed (S2.1).
A “regularization” loss which penalizes deviations from the token-level confidences from the stage-one model. The cross-entropy to the reference labels and the KL divergence from the stage-one model predictive distribution are both considered.

Overall, beam search using the rank “calibration” loss with KL divergence “regularization” loss are found to work best.","Overall, it is notable that the proposed approach yields models that do not appear to require decoding heuristics; this suggests that the calibration loss is doing something useful. Figure 3 is also fairly convincing in this respect, as it shows a distinct difference between the vanilla model and the calibrated one when further decoding samples are used (consistent improvement for the calibrated model). However, the experiments overall are not entirely convincing, since in several cases the degree of improvement over the baselines is marginal, while the proposed model is larger than the baselines in terms of # of parameters (2B). So it’s not clear that it’s the new calibration scheme that’s responsible for the improvement in these cases.

The scoring function (S2.1) is a key element of the proposal. It’s stated that “compared to using external metrics, [...] it differs from the metrics we evaluate the generation systems with and mitigates the risk of directly optimising towards imperfect metrics.” However, the proposed similarity is related to both ROUGE and BERTScore, and so it’s not clear there isn’t some bias towards better performance on the target metrics.

Finally, I would have liked to see probabilistic calibration addressed somewhere. Does the proposed calibration loss actually lead to more meaningful probability estimates? This could be measured at the token-level or on other events (cf https://papers.nips.cc/paper/2015/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf)","Regarding the presentation, although the approach itself is quite straightforward, I found it difficult to follow the narrative of the paper. At various points, such as the introduction of the calibration losses, many options are considered without proper motivation or discussion of relative merits to justify their inclusion in the experiments. I found the use of “TL;DR;” blocks at the end of certain chapters to be out-of-place; it would have been better to provide this kind of information at the beginning of the sections, ideally framed as research questions.

On related work, the approach is conceptually similar to knowledge distillation, and would benefit from comparing and contrasting both the original Hinton paper (when it comes to the KL regularization loss) and the sequence-level extension (https://arxiv.org/abs/1606.07947) when it comes to the use of sequence-level training. Some other relevant work is not cited, such as papers in non-autoregressive decoding where it is common to train on decoded outputs from an initial “teacher” model.

On reproducibility, the appendix provides many details about hyperparameter choices. On the other hand, there’s no obvious intent to release code, and some of the model sizes considered are prohibitive in terms of compute costs for many researchers.

In Figure 3, what is the “score” in the y-axis?",There are some good ideas here but the presentation could be improved and the experimental results are lackluster. ,"4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666504695289,,0qSOodKmJaN,ICLR.cc/2023/Conference/Paper1988/-/Official_Review,,[],3,,['everyone'],0qSOodKmJaN,['ICLR.cc/2023/Conference/Paper1988/Reviewer_YBaL'],1666504695289,,1669040767287,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1988/Reviewer_YBaL']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

Specifically, the reviewer mentions the following papers that may not be already included in the original paper:
1. The original Hinton paper on knowledge distillation (when it comes to the KL regularization loss).
2. The sequence-level extension of knowledge distillation (https://arxiv.org/abs/1606.07947) when it comes to the use of sequence-level training.
3. Papers in non-autoregressive decoding where it is common to train on decoded outputs from an initial “teacher” model.
4. A paper from NIPS 2015 (https://papers.nips.cc/paper/2015/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf) on probabilistic calibration. 

These literature references are suggested to improve the paper by comparing and contrasting the proposed approach with existing related work, and to address probabilistic calibration which is not discussed in the original paper.",1,"2016, 2015, 2015",Probabilistic Calibration of Neural Networks
nPygH3H9rYk,"This paper proposes Specformer, a Transformer-based graph spectral filter that captures the magnitudes and relative dependencies of all Laplacian eigenvalues. Specformer is permutation equivariant and can perform non-local graph convolutions. Extensive experiments on the node-level and graph-level datasets demonstrate Specformer's promising performance.","Strengths：

1)	The motivation for using self-attention in the spectral domain is clear and inspiring.

2)	The authors' theoretical comparisons of Specformer to Polynomial GNNs, MPNNs, and Graph Transformers indicate Specformer's flexibility and universality, which looks sound to me.

3)	Experiments demonstrate that Specformer can learn a flexible and expressive filter and outperform baselines on node-level and graph-level tasks, particularly on heterophilic graph datasets.

4)	This paper is well-written and easy to follow.

Weaknesses:

1)	The scalability of Specformer is seriously limited by its quadratic time complexity.

2)	It would be helpful to empirically compare Specformer with baselines in terms of time and space overhead, even as the Specformer's time complexity is theoretically analyzed.

3)	Some baseline experimental results are missing, and no explanation. For example, the results of the Graphormer on MolHIV and MolPCBA are not displayed in Table 3 but are reported in the Graphormer's paper.

4)	The experimental datasets are small and a bit old. Using large and latest datasets (such as ogbn-arxiv in OGB and Penn94 in LINKX [1]) may significantly enhance the paper's quality.

[1] Lim, Derek, et al. ""Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods."" In NeurIPS 2021.
","Clarity: The paper is overall reader-friendly. 

Quality and Novelty: This paper has a clear motivation, and the proposed method is marginally novel.

Reproducibility: There is no available code.
","The motivation of this paper is clear, and the proposed method is promising. However, Specformer's quadratic time complexity is a serious limitation, and the experimental results are incomplete since some baseline results are missing. Due to the negative aspects of this work, I am slightly inclined to recommend rejection, but I would encourage the authors to address some of the issues raised in the comments in the rebuttal.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666783591646,,0pdSt3oyJa1,ICLR.cc/2023/Conference/Paper5968/-/Official_Review,,[],4,,['everyone'],0pdSt3oyJa1,['ICLR.cc/2023/Conference/Paper5968/Reviewer_Ge61'],1666783591646,,1666833204624,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5968/Reviewer_Ge61']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the paper ""Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods"" by Lim et al. (2021) and suggests using large and latest datasets such as ogbn-arxiv in OGB and Penn94 in LINKX, which are referenced in this paper. This implies that these datasets and the paper by Lim et al. are not already discussed in the original paper.",1,2021,Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods
eUNK2hsnG0P,"This article proposes a new algorithm (BAPG) to estimate the GW distance between two discrete measured metric spaces $(X,d_X,\mu)$ and $(Y, d_Y, \nu)$. That is, it consider the following optimization problem  :
$$\min_{\pi} -Tr(D_X \pi D_Y \pi^T)$$
where $D_X = (d_X(x_i,x_j))_{ij}$ (and similarly for $D_Y$) and $\pi$ is constraint to have $\mu,\nu$ as marginals. 

The main idea is to consider a relaxation of the bilinear problem that naturally arise from this _quadratic_ optimization problem by minimizing instead 

$$F_\rho (\pi,w) = - Tr(D_X \pi D_Y w^T) + \rho D_h(\pi, w)$$

where $\pi$ is constrained to have $\mu$ as first marginal, $w$ is constrained to have $\nu$ as second marginal, $\rho$ is a regularization parameter and $D_h$ is the Bregman divergence associated to the convex map $h$ (in practice, the entropy, so that $D_h$ is the KL divergence, or the quadratic loss so that $D_h$ is the Euclidean distance). 

Akin to the usual derivations of the Sinkhorn algorithm as Iterative Bregman Projections [Benamou et al., 2015], authors propose an iterative algorithm (BAPG) to obtained minimizers of the proposed functional. 

Of crucial importance, authors show that : 
- [Theorem 3.6] Any accumulation point of the sequence $(\pi^k,w^k)_k$ must belong to a set of fixed-points for BAPG. 
- [Proposition 3.3] Any such fixed-point must satisfy $|\pi^\star - w^\star| = O(1/\rho)$, so that in particular in the regime $\rho \to \infty$, one expect to have $\pi^\star = w^\star$, and the middle-point $(\pi^\star + w^\star) / 2$ is close (as a $O(1/\rho)$) to an optimal solution of the original GW problem. 

Eventually, authors showcase their approach on a variety of numerical experiments, achieving good performances in terms of computational efficiency (running times) and utility (matching accuracy). ","## Strength

- Introduces an interesting algorithm to tackle an important problem in Computational OT and related fields. 
- Address both theoretical and numerical aspects of the problem. 
- Well-written paper. 

## Weaknesses 

- There are still few theoretical points that remain to be investigated/detailed. For instance: 
   - does Eq. (9) enables to get a convergence rate toward the fixed-point $(\pi^\star, w^\star)$? (if so, and provided a rounding scheme is valid there (see below), I think this would yield an overall approximation-rate for GW which may be of interest and could be compared with other benchmarks).
    - Why is it clear that a limit point $(\pi^\infty, w^\infty)$ exists? I guess this follow from some compactness arguments (i.e. the sequence shall be bounded thanks to $f$ being coercive), but this is not striking in the proof as far as I can tell.  
- [More a suggestion than a weakness] Experimental evaluation could be slightly extended as they suggest some interesting behavior. In particular, in the light of Fig 2. (b), it seems there is a (natural) tradeoff between convergence speed and the infeasibility error wrt the parameter $\rho$, so a possibly useful idea would be to start with a fairly high $\rho$ and progressively reduce it (as a sort of warm start). 
- I think some references are missing, in particular the work of _The unbalanced Gromov Wasserstein distance: Conic formulation and relaxation_ by Séjourné et al., 2021, which introduces a similar relaxation of the GW problem, and proves under specific condition that minimizers of $(\pi, w) \mapsto F(\pi,w)$ are the same as those of $\pi \mapsto F(\pi,\pi)$. 

## Other remarks/questions/suggestion
- If my understanding is correct, the output of BAPG is (approximately) a plan of the form $(\pi^\star + w^\star)/2$, which, if I am correct, does not exactly have the expected marginals $(\mu,\nu)$. Assuming this would be critical, could someone apply a `rounding` scheme in the vein of [Altschuler et al., 2018] ? 
- [suggestion for clarity] I think that in propositions statements, it may serve clarity to add quantifers ""$\forall x,y$"", or ""$\forall \pi \in \mathbb{R}^{n \times m}$, and so on, to make clear on which parameter does the constant $M,\tau_1,\tau_2$ depend (or not). 
- [typo] In Remark 3.7, the ref to prop 3.4 should be 3.5 I think. 
- [typo] In proof of prop 3.1, $D_x$ should be $D_X$. ","# Clarity

Well written work, quite clear overall. 

# Quality

From my understanding, this is a good paper that introduce a possibly impactful algorithm supported by relevant theory and appropriate experiments. 

# Novelty

To some extend, one could argue that BAPG is quite similar to standard techniques (in particular the Iterative Bregman Iterations interpretation of the Sinkhorn algorithm used in regular computational OT), but the proposed algorithm remains new to the best of my knowledge. 

# Reproducibility

Proofs have been (non-extensively) checked and no major flaw was identified (they are pretty well-written). Code to reproduce experiments has been provided with the paper (not tested) and it seems well organized at first sight.",I think this is a good paper that introduce an interesting approach to address a difficult and important problem. It is supported by both theoretical and numerical claims. ,"4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666633655994,,0jxPyVWmiiF,ICLR.cc/2023/Conference/Paper2482/-/Official_Review,,[],3,,['everyone'],0jxPyVWmiiF,['ICLR.cc/2023/Conference/Paper2482/Reviewer_h6Kq'],1666633655994,,1666633655994,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2482/Reviewer_h6Kq']","Yes, the peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. Specifically, the reviewer mentions the work of Séjourné et al., 2021, ""The unbalanced Gromov Wasserstein distance: Conic formulation and relaxation"", which introduces a similar relaxation of the GW problem, and proves under specific conditions that minimizers of $(\pi, w) \mapsto F(\pi,w)$ are the same as those of $\pi \mapsto F(\pi,\pi)$. The reviewer notes that this reference is missing from the original paper. Additionally, the reviewer mentions the work of Altschuler et al., 2018, in the context of applying a ""rounding"" scheme, but it is not clear if this reference is already included in the original paper.",1,"2021, 2018",The unbalanced Gromov Wasserstein distance: Conic formulation and relaxation 
qYySvJPjQDt,"The paper makes the observation that classic attention mechanisms do not take advantage in the hierarchical structure implicit within language and vision. 
The propose augmenting the attention operators within transformers with a hierarchy-aware module that progressively discovers ""semantic hierarchies"" between the tokens. 
The new module is applied within the scope of vision and language pre-training (CLIP) and evaluated primarily on CLIP and DeCLIP (a method that combines CLIP with several other self-supervised tasks) with some comparisons to SLIP (CLIP + SimCLR) and FILIP (A variant of CLIP that computes image-caption similarity on the tokens instead of cosine similarity between global vectors). 
The proposed attention can improve performance of both CLIP and DeCLIP on downstream visual discrimination and vision and language tasks. Furthermore, augmenting CLIP with hierarchical attention can sometimes improve performance more than adding other SSL tasks; HiCLIP > DeCLIP for some vision and language tasks. 
Furthermore, a few qualitative examples are provided that show that the learned hierarchy is meaningful. Several ablations are conducted to evaluate the impact of dataset size (more data helps), backbone parameters (smaller patches resulted in performance improvements), and hierarchy on each domain (hierarchy-attention seems to benefit language encoders more than visual encoders, and using both helps even more). ","**Strengths**:
- The idea of taking advantage of the hierarchy implicit within visual and language data is very interesting, and it's nice to see such an observation translated to performance improvements. 
- the details of implementing attention seem to be well-though of. I specifically liked how the authors explicitly listed the hierarchy aggregation priors. 
- I appreciated how the authors went beyond the typical evaluation sets towards some reasoning tasks and analyzed their importance with respect to their modeling contributions. 
- The improvements are impressive, especially in cases where HA outperforms the use of SSL tasks in DeCLIP. 

**Weaknesses:**
- The missing numbers for SLIP and FILIP in Table 1 are problematic. It seems that the numbers match the DeCLIP github repo, however, both models are capable of doing the 0-shot tasks. SLIP only reported numbers for VIT-B/16 which is different from the backbone used in table 1, so one cannot compare numbers across tables. I want to note that I do not think that SLIP is a very important comparison since it's implicit within DeCLIP, however, FILIP is an important comparison. FILIP does a comparison between tokens, and hence, allows the model to implicitly learn features that allow a mapping between both modality-specific tokens. This is a different way to dealing with the structure between tokens: CLIP aggregates all tokens with equal weight, HICLIP learns hierarchical aggregation, FILIP aggregates the loss/similarity through basically though a form of ""cross-modal attention."" Understanding how it performs would be helpful to understand what aggregation matters for training. Finally, since the paper reports numbers on ViT-B/16 in table 3, evaluating the pre-trained FILIP weights from DeCLIP should be easy as they seem to have the same training setup. 
- The authors do not mention GroupViT by Xu et al. (CVPR 2022). This paper changes the aggregation function through a learned tokenization. As far as I understand, their aggregation is also non-splittable, however, unlike HA-Attention, their grouping block is global rather than local and their computation of affinities are different. GroupViT also uses the same training task and loss as CLIP, so there's a lot of overlap. I think it would be very important for this paper to discuss GroupViT as well as SlotAttention by Locatello et al (NeurIPS 2020). There has been some additional work in that area, so there might be additional work to compare to that I am not aware of, but I think GroupViT is an important comparison and SlotAttention and GroupViT are both important works to discuss as they propose augmentations to attention or transformers with the goal of understanding the composition of the scene to improve learning. 
- The paper makes many claims about how previous approaches in vision overlook hierarchies (first line of second paragraph in intro), however, this is not true. First, convolutional networks were initially proposed to capture hierarchies of features through progressively larger receptive fields. This was shown by many papers and notably discussed by Olah et al (2017). Furthermore, vision datasets are often collected via hierarchies with ImageNet collected using WordNet. Pretrained models also often implicitly learn this hierarchy which can show in confusion patterns as shown by Alsallakh et al (2017). Work on scene graphs and scene compositions defines problems that tries to very explicitly capture different hierarchies. Finally, the paper used to discuss how humans perceive the world hierarchically (Kuzovkin et al) compares brain activations with neural network activations and show they correlate. I do not think any of this requires comparison, but the current tone of the introduction greatly overclaims how hierarchy has been overlooked within vision and multimodal research. 
- While I liked the idea of showing examples of hierarchy in Fig 3, 4, 5 ... I think it would be nice to show more. I am also not sure if there's a way to quantitatively evaluate this via segmentation (similar to GroupViT) or comparing to parse trees for language. This is more of a suggestion than a weakness. 
- I find it very difficult to understand how C is used or computed. Here's my understanding and points I found confusing. I would greatly appreciate some clarification and I think the paper would greatly benefit from an illustrated example of those values (or at least just C) for a small graph (C for the hierarchy of a 4 or 5 word sentence as a toy example would be sufficient I think)
	- For N tokens, you compute affinity values. I am assuming equation 3 can be used for $s_{i, i-1}$ otherwise, it's unclear how one gets those values for equation 4. Furthermore, it's unclear how to compute the similarity for edge tokens; I can imagine using start and end tokens in language, but how do you do that for image tokens? 
	- Equation 5 shows how affinities are updated between layers to ensure that affinities are monotonically non-decreasing, however, that doesn't mean that the order of affinities will not change with time (eg, $a_{0,1} = 0.5 \text{ and }a_{1,2} = 0.4$ at $t_0$ but , $a_{0,1} = 0.55 \text{ and }a_{1,2} = 0.6$ at $t_1$). How would that affect the model? is there something that I am missing that prohibits that from happening? If the ordering changes, does that affect the implied hierarchy. 
	- Most importantly C appears to only be computed for $j>i$ , is this true? if not, I think you need to update the notation to explain that as well as explain how to set the values $a_{i,i}$. Additionally, C appears to be a set of values between 0 and 1 which is multiplied by the QK component of the attention. It does not appear to create a strict hierarchy with definitive edges that only connect parents and children.


 **References:**  
- Xu, J., De Mello, S., Liu, S., Byeon, W., Breuel, T., Kautz, J., & Wang, X. (2022). GroupViT: Semantic Segmentation Emerges from Text Supervision. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 18134-18144).
- Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., ... & Kipf, T. (2020). Object-centric learning with slot attention. _Advances in Neural Information Processing Systems_, _33_, 11525-11538.
- Olah, et al., ""Feature Visualization"", Distill, 2017. https://distill.pub/2017/feature-visualization/ 
- Bilal, Alsallakh, et al. ""Do convolutional neural networks learn class hierarchy?."" _IEEE transactions on visualization and computer graphics_ 24.1 (2017): 152-162.

	  ","**Clarity** 
Overall, the paper was easy to read. One very important exception was the second half of Sec 3.1.2: I found it difficult to understand the second portion of HA as noted in the weaknesses. There are also many minor typos and grammatical mistakes which I point out some of them below.

**Reproducibility:**
- What's the batch size? 
- Do you plan on releasing code? While the method is explained well, I don't think it would be easy to implement this or reproducing it without access to code or pseudocode of the method. 

**Minor suggestions/typos:** I noticed several typos while reading the paper. I list some below, but this list is not comprehensive. 
- Abstract: ""multimodality content understanding"" -> ""multimodal content understanding""
- The second sentence in the abstract was very confusing. I encourage the authors to rephrase it. 
- Abstract: ""CLIP features can hardly reflect the hierarchy nature"" -> ""CLIP features hardly reflect the hierarchical nature"". Furthermore, I think this claim is unsupported. There are several hierarchies that exist in the word. This paper appears to capture the syntactic trees within language and objects masks within images. However, one could imagine a hierarchy over types similar to WordNet (dogs are a superclass of poodle and bulldog) or graph structure depicting scenes (room has chairs and tables) or part-structure (chair has legs). While the paper shows that adding an explicit attentional mechanism to capture one form of structure improves performance, it does not show that CLIP's feature do not capture any hierarchy. 
- Sec 3, first paragraph, first line: ""share a hierarchy nature"" -> ""share a hierarchical nature"" second line: ""The lower level hierarchy"" -> ""the lower level of the hierarchy"" ","Overall, I think the paper presents an interesting idea with a good motivation.  The results are compelling and show strong improvements over prior methods. However, I found the explanation of how hierarchical attention operated very confusing, with some unclear notation. Furthermore, there are some missing comparisons. I tentatively set the rating as marginally above acceptance, but I am happy to raise my rating if the points below are adequately addressed. 

Below, I summarize the major weaknesses and clarifications in order of importance that I hope the authors will address:
- The hierarchical attention explanation is a bit confusing, especially how C is computed and used. I would appreciate some clarification in the discussion and I believe the paper would greatly benefit from some additional explanation and a toy example clarifying how C is computed and used.  
- A discussion of GroupViT and SlotAttention would be important to add, as well as a comparison to GroupViT as an alternative approach for aggregating information and a relevant baseline. 
- Table 1 is incomplete for no clear reason. As I noted, there are published numbers that share the setup using a ViT B/16 which was trained by the authors are reported in Table 3. While the comparison against SLIP is not crucial, I think comparing against FILIP and DeFILIP would be very helpful.
- I think the claims about hierarchy being overlooked in computer vision and multimodal representation learning should be adjusted or better contextualized. 

----------------------
**Update (Nov 17th):** I updated my recommendation from 6 to 8 as the authors have responded to my major concerns. 
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666912667881,,0eTTKOOOQkV,ICLR.cc/2023/Conference/Paper5856/-/Official_Review,,[],3,,['everyone'],0eTTKOOOQkV,['ICLR.cc/2023/Conference/Paper5856/Reviewer_WpY2'],1666912667881,,1668704924846,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5856/Reviewer_WpY2']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not cited in the original paper, including:

1. Xu et al. (2022) - GroupViT: Semantic Segmentation Emerges from Text Supervision
2. Locatello et al. (2020) - Object-centric learning with slot attention
3. Olah et al. (2017) - Feature Visualization
4. Alsallakh et al. (2017) - Do convolutional neural networks learn class hierarchy?

The reviewer suggests that the authors should discuss these papers, particularly GroupViT and SlotAttention, as they propose alternative approaches to aggregating information and are relevant to the topic of hierarchical attention. The reviewer also mentions that a comparison to GroupViT would be an important addition to the paper.",1,"2022, 2020, 2017, 2017",GroupViT Semantic Segmentation Emerges from Text Supervision 
qYySvJPjQDt,"The paper makes the observation that classic attention mechanisms do not take advantage in the hierarchical structure implicit within language and vision. 
The propose augmenting the attention operators within transformers with a hierarchy-aware module that progressively discovers ""semantic hierarchies"" between the tokens. 
The new module is applied within the scope of vision and language pre-training (CLIP) and evaluated primarily on CLIP and DeCLIP (a method that combines CLIP with several other self-supervised tasks) with some comparisons to SLIP (CLIP + SimCLR) and FILIP (A variant of CLIP that computes image-caption similarity on the tokens instead of cosine similarity between global vectors). 
The proposed attention can improve performance of both CLIP and DeCLIP on downstream visual discrimination and vision and language tasks. Furthermore, augmenting CLIP with hierarchical attention can sometimes improve performance more than adding other SSL tasks; HiCLIP > DeCLIP for some vision and language tasks. 
Furthermore, a few qualitative examples are provided that show that the learned hierarchy is meaningful. Several ablations are conducted to evaluate the impact of dataset size (more data helps), backbone parameters (smaller patches resulted in performance improvements), and hierarchy on each domain (hierarchy-attention seems to benefit language encoders more than visual encoders, and using both helps even more). ","**Strengths**:
- The idea of taking advantage of the hierarchy implicit within visual and language data is very interesting, and it's nice to see such an observation translated to performance improvements. 
- the details of implementing attention seem to be well-though of. I specifically liked how the authors explicitly listed the hierarchy aggregation priors. 
- I appreciated how the authors went beyond the typical evaluation sets towards some reasoning tasks and analyzed their importance with respect to their modeling contributions. 
- The improvements are impressive, especially in cases where HA outperforms the use of SSL tasks in DeCLIP. 

**Weaknesses:**
- The missing numbers for SLIP and FILIP in Table 1 are problematic. It seems that the numbers match the DeCLIP github repo, however, both models are capable of doing the 0-shot tasks. SLIP only reported numbers for VIT-B/16 which is different from the backbone used in table 1, so one cannot compare numbers across tables. I want to note that I do not think that SLIP is a very important comparison since it's implicit within DeCLIP, however, FILIP is an important comparison. FILIP does a comparison between tokens, and hence, allows the model to implicitly learn features that allow a mapping between both modality-specific tokens. This is a different way to dealing with the structure between tokens: CLIP aggregates all tokens with equal weight, HICLIP learns hierarchical aggregation, FILIP aggregates the loss/similarity through basically though a form of ""cross-modal attention."" Understanding how it performs would be helpful to understand what aggregation matters for training. Finally, since the paper reports numbers on ViT-B/16 in table 3, evaluating the pre-trained FILIP weights from DeCLIP should be easy as they seem to have the same training setup. 
- The authors do not mention GroupViT by Xu et al. (CVPR 2022). This paper changes the aggregation function through a learned tokenization. As far as I understand, their aggregation is also non-splittable, however, unlike HA-Attention, their grouping block is global rather than local and their computation of affinities are different. GroupViT also uses the same training task and loss as CLIP, so there's a lot of overlap. I think it would be very important for this paper to discuss GroupViT as well as SlotAttention by Locatello et al (NeurIPS 2020). There has been some additional work in that area, so there might be additional work to compare to that I am not aware of, but I think GroupViT is an important comparison and SlotAttention and GroupViT are both important works to discuss as they propose augmentations to attention or transformers with the goal of understanding the composition of the scene to improve learning. 
- The paper makes many claims about how previous approaches in vision overlook hierarchies (first line of second paragraph in intro), however, this is not true. First, convolutional networks were initially proposed to capture hierarchies of features through progressively larger receptive fields. This was shown by many papers and notably discussed by Olah et al (2017). Furthermore, vision datasets are often collected via hierarchies with ImageNet collected using WordNet. Pretrained models also often implicitly learn this hierarchy which can show in confusion patterns as shown by Alsallakh et al (2017). Work on scene graphs and scene compositions defines problems that tries to very explicitly capture different hierarchies. Finally, the paper used to discuss how humans perceive the world hierarchically (Kuzovkin et al) compares brain activations with neural network activations and show they correlate. I do not think any of this requires comparison, but the current tone of the introduction greatly overclaims how hierarchy has been overlooked within vision and multimodal research. 
- While I liked the idea of showing examples of hierarchy in Fig 3, 4, 5 ... I think it would be nice to show more. I am also not sure if there's a way to quantitatively evaluate this via segmentation (similar to GroupViT) or comparing to parse trees for language. This is more of a suggestion than a weakness. 
- I find it very difficult to understand how C is used or computed. Here's my understanding and points I found confusing. I would greatly appreciate some clarification and I think the paper would greatly benefit from an illustrated example of those values (or at least just C) for a small graph (C for the hierarchy of a 4 or 5 word sentence as a toy example would be sufficient I think)
	- For N tokens, you compute affinity values. I am assuming equation 3 can be used for $s_{i, i-1}$ otherwise, it's unclear how one gets those values for equation 4. Furthermore, it's unclear how to compute the similarity for edge tokens; I can imagine using start and end tokens in language, but how do you do that for image tokens? 
	- Equation 5 shows how affinities are updated between layers to ensure that affinities are monotonically non-decreasing, however, that doesn't mean that the order of affinities will not change with time (eg, $a_{0,1} = 0.5 \text{ and }a_{1,2} = 0.4$ at $t_0$ but , $a_{0,1} = 0.55 \text{ and }a_{1,2} = 0.6$ at $t_1$). How would that affect the model? is there something that I am missing that prohibits that from happening? If the ordering changes, does that affect the implied hierarchy. 
	- Most importantly C appears to only be computed for $j>i$ , is this true? if not, I think you need to update the notation to explain that as well as explain how to set the values $a_{i,i}$. Additionally, C appears to be a set of values between 0 and 1 which is multiplied by the QK component of the attention. It does not appear to create a strict hierarchy with definitive edges that only connect parents and children.


 **References:**  
- Xu, J., De Mello, S., Liu, S., Byeon, W., Breuel, T., Kautz, J., & Wang, X. (2022). GroupViT: Semantic Segmentation Emerges from Text Supervision. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 18134-18144).
- Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., ... & Kipf, T. (2020). Object-centric learning with slot attention. _Advances in Neural Information Processing Systems_, _33_, 11525-11538.
- Olah, et al., ""Feature Visualization"", Distill, 2017. https://distill.pub/2017/feature-visualization/ 
- Bilal, Alsallakh, et al. ""Do convolutional neural networks learn class hierarchy?."" _IEEE transactions on visualization and computer graphics_ 24.1 (2017): 152-162.

	  ","**Clarity** 
Overall, the paper was easy to read. One very important exception was the second half of Sec 3.1.2: I found it difficult to understand the second portion of HA as noted in the weaknesses. There are also many minor typos and grammatical mistakes which I point out some of them below.

**Reproducibility:**
- What's the batch size? 
- Do you plan on releasing code? While the method is explained well, I don't think it would be easy to implement this or reproducing it without access to code or pseudocode of the method. 

**Minor suggestions/typos:** I noticed several typos while reading the paper. I list some below, but this list is not comprehensive. 
- Abstract: ""multimodality content understanding"" -> ""multimodal content understanding""
- The second sentence in the abstract was very confusing. I encourage the authors to rephrase it. 
- Abstract: ""CLIP features can hardly reflect the hierarchy nature"" -> ""CLIP features hardly reflect the hierarchical nature"". Furthermore, I think this claim is unsupported. There are several hierarchies that exist in the word. This paper appears to capture the syntactic trees within language and objects masks within images. However, one could imagine a hierarchy over types similar to WordNet (dogs are a superclass of poodle and bulldog) or graph structure depicting scenes (room has chairs and tables) or part-structure (chair has legs). While the paper shows that adding an explicit attentional mechanism to capture one form of structure improves performance, it does not show that CLIP's feature do not capture any hierarchy. 
- Sec 3, first paragraph, first line: ""share a hierarchy nature"" -> ""share a hierarchical nature"" second line: ""The lower level hierarchy"" -> ""the lower level of the hierarchy"" ","Overall, I think the paper presents an interesting idea with a good motivation.  The results are compelling and show strong improvements over prior methods. However, I found the explanation of how hierarchical attention operated very confusing, with some unclear notation. Furthermore, there are some missing comparisons. I tentatively set the rating as marginally above acceptance, but I am happy to raise my rating if the points below are adequately addressed. 

Below, I summarize the major weaknesses and clarifications in order of importance that I hope the authors will address:
- The hierarchical attention explanation is a bit confusing, especially how C is computed and used. I would appreciate some clarification in the discussion and I believe the paper would greatly benefit from some additional explanation and a toy example clarifying how C is computed and used.  
- A discussion of GroupViT and SlotAttention would be important to add, as well as a comparison to GroupViT as an alternative approach for aggregating information and a relevant baseline. 
- Table 1 is incomplete for no clear reason. As I noted, there are published numbers that share the setup using a ViT B/16 which was trained by the authors are reported in Table 3. While the comparison against SLIP is not crucial, I think comparing against FILIP and DeFILIP would be very helpful.
- I think the claims about hierarchy being overlooked in computer vision and multimodal representation learning should be adjusted or better contextualized. 

----------------------
**Update (Nov 17th):** I updated my recommendation from 6 to 8 as the authors have responded to my major concerns. 
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666912667881,,0eTTKOOOQkV,ICLR.cc/2023/Conference/Paper5856/-/Official_Review,,[],3,,['everyone'],0eTTKOOOQkV,['ICLR.cc/2023/Conference/Paper5856/Reviewer_WpY2'],1666912667881,,1668704924846,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5856/Reviewer_WpY2']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not cited in the original paper, including:

1. Xu et al. (2022) - GroupViT: Semantic Segmentation Emerges from Text Supervision
2. Locatello et al. (2020) - Object-centric learning with slot attention
3. Olah et al. (2017) - Feature Visualization
4. Alsallakh et al. (2017) - Do convolutional neural networks learn class hierarchy?

The reviewer suggests that the authors should discuss these papers, particularly GroupViT and SlotAttention, as they propose alternative approaches to aggregating information and are relevant to the topic of hierarchical attention. The reviewer also mentions that a comparison to GroupViT would be an important addition to the paper.",1,"2022, 2020, 2017, 2017",Object-centric learning with slot attention 
qYySvJPjQDt,"The paper makes the observation that classic attention mechanisms do not take advantage in the hierarchical structure implicit within language and vision. 
The propose augmenting the attention operators within transformers with a hierarchy-aware module that progressively discovers ""semantic hierarchies"" between the tokens. 
The new module is applied within the scope of vision and language pre-training (CLIP) and evaluated primarily on CLIP and DeCLIP (a method that combines CLIP with several other self-supervised tasks) with some comparisons to SLIP (CLIP + SimCLR) and FILIP (A variant of CLIP that computes image-caption similarity on the tokens instead of cosine similarity between global vectors). 
The proposed attention can improve performance of both CLIP and DeCLIP on downstream visual discrimination and vision and language tasks. Furthermore, augmenting CLIP with hierarchical attention can sometimes improve performance more than adding other SSL tasks; HiCLIP > DeCLIP for some vision and language tasks. 
Furthermore, a few qualitative examples are provided that show that the learned hierarchy is meaningful. Several ablations are conducted to evaluate the impact of dataset size (more data helps), backbone parameters (smaller patches resulted in performance improvements), and hierarchy on each domain (hierarchy-attention seems to benefit language encoders more than visual encoders, and using both helps even more). ","**Strengths**:
- The idea of taking advantage of the hierarchy implicit within visual and language data is very interesting, and it's nice to see such an observation translated to performance improvements. 
- the details of implementing attention seem to be well-though of. I specifically liked how the authors explicitly listed the hierarchy aggregation priors. 
- I appreciated how the authors went beyond the typical evaluation sets towards some reasoning tasks and analyzed their importance with respect to their modeling contributions. 
- The improvements are impressive, especially in cases where HA outperforms the use of SSL tasks in DeCLIP. 

**Weaknesses:**
- The missing numbers for SLIP and FILIP in Table 1 are problematic. It seems that the numbers match the DeCLIP github repo, however, both models are capable of doing the 0-shot tasks. SLIP only reported numbers for VIT-B/16 which is different from the backbone used in table 1, so one cannot compare numbers across tables. I want to note that I do not think that SLIP is a very important comparison since it's implicit within DeCLIP, however, FILIP is an important comparison. FILIP does a comparison between tokens, and hence, allows the model to implicitly learn features that allow a mapping between both modality-specific tokens. This is a different way to dealing with the structure between tokens: CLIP aggregates all tokens with equal weight, HICLIP learns hierarchical aggregation, FILIP aggregates the loss/similarity through basically though a form of ""cross-modal attention."" Understanding how it performs would be helpful to understand what aggregation matters for training. Finally, since the paper reports numbers on ViT-B/16 in table 3, evaluating the pre-trained FILIP weights from DeCLIP should be easy as they seem to have the same training setup. 
- The authors do not mention GroupViT by Xu et al. (CVPR 2022). This paper changes the aggregation function through a learned tokenization. As far as I understand, their aggregation is also non-splittable, however, unlike HA-Attention, their grouping block is global rather than local and their computation of affinities are different. GroupViT also uses the same training task and loss as CLIP, so there's a lot of overlap. I think it would be very important for this paper to discuss GroupViT as well as SlotAttention by Locatello et al (NeurIPS 2020). There has been some additional work in that area, so there might be additional work to compare to that I am not aware of, but I think GroupViT is an important comparison and SlotAttention and GroupViT are both important works to discuss as they propose augmentations to attention or transformers with the goal of understanding the composition of the scene to improve learning. 
- The paper makes many claims about how previous approaches in vision overlook hierarchies (first line of second paragraph in intro), however, this is not true. First, convolutional networks were initially proposed to capture hierarchies of features through progressively larger receptive fields. This was shown by many papers and notably discussed by Olah et al (2017). Furthermore, vision datasets are often collected via hierarchies with ImageNet collected using WordNet. Pretrained models also often implicitly learn this hierarchy which can show in confusion patterns as shown by Alsallakh et al (2017). Work on scene graphs and scene compositions defines problems that tries to very explicitly capture different hierarchies. Finally, the paper used to discuss how humans perceive the world hierarchically (Kuzovkin et al) compares brain activations with neural network activations and show they correlate. I do not think any of this requires comparison, but the current tone of the introduction greatly overclaims how hierarchy has been overlooked within vision and multimodal research. 
- While I liked the idea of showing examples of hierarchy in Fig 3, 4, 5 ... I think it would be nice to show more. I am also not sure if there's a way to quantitatively evaluate this via segmentation (similar to GroupViT) or comparing to parse trees for language. This is more of a suggestion than a weakness. 
- I find it very difficult to understand how C is used or computed. Here's my understanding and points I found confusing. I would greatly appreciate some clarification and I think the paper would greatly benefit from an illustrated example of those values (or at least just C) for a small graph (C for the hierarchy of a 4 or 5 word sentence as a toy example would be sufficient I think)
	- For N tokens, you compute affinity values. I am assuming equation 3 can be used for $s_{i, i-1}$ otherwise, it's unclear how one gets those values for equation 4. Furthermore, it's unclear how to compute the similarity for edge tokens; I can imagine using start and end tokens in language, but how do you do that for image tokens? 
	- Equation 5 shows how affinities are updated between layers to ensure that affinities are monotonically non-decreasing, however, that doesn't mean that the order of affinities will not change with time (eg, $a_{0,1} = 0.5 \text{ and }a_{1,2} = 0.4$ at $t_0$ but , $a_{0,1} = 0.55 \text{ and }a_{1,2} = 0.6$ at $t_1$). How would that affect the model? is there something that I am missing that prohibits that from happening? If the ordering changes, does that affect the implied hierarchy. 
	- Most importantly C appears to only be computed for $j>i$ , is this true? if not, I think you need to update the notation to explain that as well as explain how to set the values $a_{i,i}$. Additionally, C appears to be a set of values between 0 and 1 which is multiplied by the QK component of the attention. It does not appear to create a strict hierarchy with definitive edges that only connect parents and children.


 **References:**  
- Xu, J., De Mello, S., Liu, S., Byeon, W., Breuel, T., Kautz, J., & Wang, X. (2022). GroupViT: Semantic Segmentation Emerges from Text Supervision. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 18134-18144).
- Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., ... & Kipf, T. (2020). Object-centric learning with slot attention. _Advances in Neural Information Processing Systems_, _33_, 11525-11538.
- Olah, et al., ""Feature Visualization"", Distill, 2017. https://distill.pub/2017/feature-visualization/ 
- Bilal, Alsallakh, et al. ""Do convolutional neural networks learn class hierarchy?."" _IEEE transactions on visualization and computer graphics_ 24.1 (2017): 152-162.

	  ","**Clarity** 
Overall, the paper was easy to read. One very important exception was the second half of Sec 3.1.2: I found it difficult to understand the second portion of HA as noted in the weaknesses. There are also many minor typos and grammatical mistakes which I point out some of them below.

**Reproducibility:**
- What's the batch size? 
- Do you plan on releasing code? While the method is explained well, I don't think it would be easy to implement this or reproducing it without access to code or pseudocode of the method. 

**Minor suggestions/typos:** I noticed several typos while reading the paper. I list some below, but this list is not comprehensive. 
- Abstract: ""multimodality content understanding"" -> ""multimodal content understanding""
- The second sentence in the abstract was very confusing. I encourage the authors to rephrase it. 
- Abstract: ""CLIP features can hardly reflect the hierarchy nature"" -> ""CLIP features hardly reflect the hierarchical nature"". Furthermore, I think this claim is unsupported. There are several hierarchies that exist in the word. This paper appears to capture the syntactic trees within language and objects masks within images. However, one could imagine a hierarchy over types similar to WordNet (dogs are a superclass of poodle and bulldog) or graph structure depicting scenes (room has chairs and tables) or part-structure (chair has legs). While the paper shows that adding an explicit attentional mechanism to capture one form of structure improves performance, it does not show that CLIP's feature do not capture any hierarchy. 
- Sec 3, first paragraph, first line: ""share a hierarchy nature"" -> ""share a hierarchical nature"" second line: ""The lower level hierarchy"" -> ""the lower level of the hierarchy"" ","Overall, I think the paper presents an interesting idea with a good motivation.  The results are compelling and show strong improvements over prior methods. However, I found the explanation of how hierarchical attention operated very confusing, with some unclear notation. Furthermore, there are some missing comparisons. I tentatively set the rating as marginally above acceptance, but I am happy to raise my rating if the points below are adequately addressed. 

Below, I summarize the major weaknesses and clarifications in order of importance that I hope the authors will address:
- The hierarchical attention explanation is a bit confusing, especially how C is computed and used. I would appreciate some clarification in the discussion and I believe the paper would greatly benefit from some additional explanation and a toy example clarifying how C is computed and used.  
- A discussion of GroupViT and SlotAttention would be important to add, as well as a comparison to GroupViT as an alternative approach for aggregating information and a relevant baseline. 
- Table 1 is incomplete for no clear reason. As I noted, there are published numbers that share the setup using a ViT B/16 which was trained by the authors are reported in Table 3. While the comparison against SLIP is not crucial, I think comparing against FILIP and DeFILIP would be very helpful.
- I think the claims about hierarchy being overlooked in computer vision and multimodal representation learning should be adjusted or better contextualized. 

----------------------
**Update (Nov 17th):** I updated my recommendation from 6 to 8 as the authors have responded to my major concerns. 
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666912667881,,0eTTKOOOQkV,ICLR.cc/2023/Conference/Paper5856/-/Official_Review,,[],3,,['everyone'],0eTTKOOOQkV,['ICLR.cc/2023/Conference/Paper5856/Reviewer_WpY2'],1666912667881,,1668704924846,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5856/Reviewer_WpY2']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not cited in the original paper, including:

1. Xu et al. (2022) - GroupViT: Semantic Segmentation Emerges from Text Supervision
2. Locatello et al. (2020) - Object-centric learning with slot attention
3. Olah et al. (2017) - Feature Visualization
4. Alsallakh et al. (2017) - Do convolutional neural networks learn class hierarchy?

The reviewer suggests that the authors should discuss these papers, particularly GroupViT and SlotAttention, as they propose alternative approaches to aggregating information and are relevant to the topic of hierarchical attention. The reviewer also mentions that a comparison to GroupViT would be an important addition to the paper.",1,"2022, 2020, 2017, 2017",Feature Visualization 
qYySvJPjQDt,"The paper makes the observation that classic attention mechanisms do not take advantage in the hierarchical structure implicit within language and vision. 
The propose augmenting the attention operators within transformers with a hierarchy-aware module that progressively discovers ""semantic hierarchies"" between the tokens. 
The new module is applied within the scope of vision and language pre-training (CLIP) and evaluated primarily on CLIP and DeCLIP (a method that combines CLIP with several other self-supervised tasks) with some comparisons to SLIP (CLIP + SimCLR) and FILIP (A variant of CLIP that computes image-caption similarity on the tokens instead of cosine similarity between global vectors). 
The proposed attention can improve performance of both CLIP and DeCLIP on downstream visual discrimination and vision and language tasks. Furthermore, augmenting CLIP with hierarchical attention can sometimes improve performance more than adding other SSL tasks; HiCLIP > DeCLIP for some vision and language tasks. 
Furthermore, a few qualitative examples are provided that show that the learned hierarchy is meaningful. Several ablations are conducted to evaluate the impact of dataset size (more data helps), backbone parameters (smaller patches resulted in performance improvements), and hierarchy on each domain (hierarchy-attention seems to benefit language encoders more than visual encoders, and using both helps even more). ","**Strengths**:
- The idea of taking advantage of the hierarchy implicit within visual and language data is very interesting, and it's nice to see such an observation translated to performance improvements. 
- the details of implementing attention seem to be well-though of. I specifically liked how the authors explicitly listed the hierarchy aggregation priors. 
- I appreciated how the authors went beyond the typical evaluation sets towards some reasoning tasks and analyzed their importance with respect to their modeling contributions. 
- The improvements are impressive, especially in cases where HA outperforms the use of SSL tasks in DeCLIP. 

**Weaknesses:**
- The missing numbers for SLIP and FILIP in Table 1 are problematic. It seems that the numbers match the DeCLIP github repo, however, both models are capable of doing the 0-shot tasks. SLIP only reported numbers for VIT-B/16 which is different from the backbone used in table 1, so one cannot compare numbers across tables. I want to note that I do not think that SLIP is a very important comparison since it's implicit within DeCLIP, however, FILIP is an important comparison. FILIP does a comparison between tokens, and hence, allows the model to implicitly learn features that allow a mapping between both modality-specific tokens. This is a different way to dealing with the structure between tokens: CLIP aggregates all tokens with equal weight, HICLIP learns hierarchical aggregation, FILIP aggregates the loss/similarity through basically though a form of ""cross-modal attention."" Understanding how it performs would be helpful to understand what aggregation matters for training. Finally, since the paper reports numbers on ViT-B/16 in table 3, evaluating the pre-trained FILIP weights from DeCLIP should be easy as they seem to have the same training setup. 
- The authors do not mention GroupViT by Xu et al. (CVPR 2022). This paper changes the aggregation function through a learned tokenization. As far as I understand, their aggregation is also non-splittable, however, unlike HA-Attention, their grouping block is global rather than local and their computation of affinities are different. GroupViT also uses the same training task and loss as CLIP, so there's a lot of overlap. I think it would be very important for this paper to discuss GroupViT as well as SlotAttention by Locatello et al (NeurIPS 2020). There has been some additional work in that area, so there might be additional work to compare to that I am not aware of, but I think GroupViT is an important comparison and SlotAttention and GroupViT are both important works to discuss as they propose augmentations to attention or transformers with the goal of understanding the composition of the scene to improve learning. 
- The paper makes many claims about how previous approaches in vision overlook hierarchies (first line of second paragraph in intro), however, this is not true. First, convolutional networks were initially proposed to capture hierarchies of features through progressively larger receptive fields. This was shown by many papers and notably discussed by Olah et al (2017). Furthermore, vision datasets are often collected via hierarchies with ImageNet collected using WordNet. Pretrained models also often implicitly learn this hierarchy which can show in confusion patterns as shown by Alsallakh et al (2017). Work on scene graphs and scene compositions defines problems that tries to very explicitly capture different hierarchies. Finally, the paper used to discuss how humans perceive the world hierarchically (Kuzovkin et al) compares brain activations with neural network activations and show they correlate. I do not think any of this requires comparison, but the current tone of the introduction greatly overclaims how hierarchy has been overlooked within vision and multimodal research. 
- While I liked the idea of showing examples of hierarchy in Fig 3, 4, 5 ... I think it would be nice to show more. I am also not sure if there's a way to quantitatively evaluate this via segmentation (similar to GroupViT) or comparing to parse trees for language. This is more of a suggestion than a weakness. 
- I find it very difficult to understand how C is used or computed. Here's my understanding and points I found confusing. I would greatly appreciate some clarification and I think the paper would greatly benefit from an illustrated example of those values (or at least just C) for a small graph (C for the hierarchy of a 4 or 5 word sentence as a toy example would be sufficient I think)
	- For N tokens, you compute affinity values. I am assuming equation 3 can be used for $s_{i, i-1}$ otherwise, it's unclear how one gets those values for equation 4. Furthermore, it's unclear how to compute the similarity for edge tokens; I can imagine using start and end tokens in language, but how do you do that for image tokens? 
	- Equation 5 shows how affinities are updated between layers to ensure that affinities are monotonically non-decreasing, however, that doesn't mean that the order of affinities will not change with time (eg, $a_{0,1} = 0.5 \text{ and }a_{1,2} = 0.4$ at $t_0$ but , $a_{0,1} = 0.55 \text{ and }a_{1,2} = 0.6$ at $t_1$). How would that affect the model? is there something that I am missing that prohibits that from happening? If the ordering changes, does that affect the implied hierarchy. 
	- Most importantly C appears to only be computed for $j>i$ , is this true? if not, I think you need to update the notation to explain that as well as explain how to set the values $a_{i,i}$. Additionally, C appears to be a set of values between 0 and 1 which is multiplied by the QK component of the attention. It does not appear to create a strict hierarchy with definitive edges that only connect parents and children.


 **References:**  
- Xu, J., De Mello, S., Liu, S., Byeon, W., Breuel, T., Kautz, J., & Wang, X. (2022). GroupViT: Semantic Segmentation Emerges from Text Supervision. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 18134-18144).
- Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., ... & Kipf, T. (2020). Object-centric learning with slot attention. _Advances in Neural Information Processing Systems_, _33_, 11525-11538.
- Olah, et al., ""Feature Visualization"", Distill, 2017. https://distill.pub/2017/feature-visualization/ 
- Bilal, Alsallakh, et al. ""Do convolutional neural networks learn class hierarchy?."" _IEEE transactions on visualization and computer graphics_ 24.1 (2017): 152-162.

	  ","**Clarity** 
Overall, the paper was easy to read. One very important exception was the second half of Sec 3.1.2: I found it difficult to understand the second portion of HA as noted in the weaknesses. There are also many minor typos and grammatical mistakes which I point out some of them below.

**Reproducibility:**
- What's the batch size? 
- Do you plan on releasing code? While the method is explained well, I don't think it would be easy to implement this or reproducing it without access to code or pseudocode of the method. 

**Minor suggestions/typos:** I noticed several typos while reading the paper. I list some below, but this list is not comprehensive. 
- Abstract: ""multimodality content understanding"" -> ""multimodal content understanding""
- The second sentence in the abstract was very confusing. I encourage the authors to rephrase it. 
- Abstract: ""CLIP features can hardly reflect the hierarchy nature"" -> ""CLIP features hardly reflect the hierarchical nature"". Furthermore, I think this claim is unsupported. There are several hierarchies that exist in the word. This paper appears to capture the syntactic trees within language and objects masks within images. However, one could imagine a hierarchy over types similar to WordNet (dogs are a superclass of poodle and bulldog) or graph structure depicting scenes (room has chairs and tables) or part-structure (chair has legs). While the paper shows that adding an explicit attentional mechanism to capture one form of structure improves performance, it does not show that CLIP's feature do not capture any hierarchy. 
- Sec 3, first paragraph, first line: ""share a hierarchy nature"" -> ""share a hierarchical nature"" second line: ""The lower level hierarchy"" -> ""the lower level of the hierarchy"" ","Overall, I think the paper presents an interesting idea with a good motivation.  The results are compelling and show strong improvements over prior methods. However, I found the explanation of how hierarchical attention operated very confusing, with some unclear notation. Furthermore, there are some missing comparisons. I tentatively set the rating as marginally above acceptance, but I am happy to raise my rating if the points below are adequately addressed. 

Below, I summarize the major weaknesses and clarifications in order of importance that I hope the authors will address:
- The hierarchical attention explanation is a bit confusing, especially how C is computed and used. I would appreciate some clarification in the discussion and I believe the paper would greatly benefit from some additional explanation and a toy example clarifying how C is computed and used.  
- A discussion of GroupViT and SlotAttention would be important to add, as well as a comparison to GroupViT as an alternative approach for aggregating information and a relevant baseline. 
- Table 1 is incomplete for no clear reason. As I noted, there are published numbers that share the setup using a ViT B/16 which was trained by the authors are reported in Table 3. While the comparison against SLIP is not crucial, I think comparing against FILIP and DeFILIP would be very helpful.
- I think the claims about hierarchy being overlooked in computer vision and multimodal representation learning should be adjusted or better contextualized. 

----------------------
**Update (Nov 17th):** I updated my recommendation from 6 to 8 as the authors have responded to my major concerns. 
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666912667881,,0eTTKOOOQkV,ICLR.cc/2023/Conference/Paper5856/-/Official_Review,,[],3,,['everyone'],0eTTKOOOQkV,['ICLR.cc/2023/Conference/Paper5856/Reviewer_WpY2'],1666912667881,,1668704924846,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5856/Reviewer_WpY2']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers that are not cited in the original paper, including:

1. Xu et al. (2022) - GroupViT: Semantic Segmentation Emerges from Text Supervision
2. Locatello et al. (2020) - Object-centric learning with slot attention
3. Olah et al. (2017) - Feature Visualization
4. Alsallakh et al. (2017) - Do convolutional neural networks learn class hierarchy?

The reviewer suggests that the authors should discuss these papers, particularly GroupViT and SlotAttention, as they propose alternative approaches to aggregating information and are relevant to the topic of hierarchical attention. The reviewer also mentions that a comparison to GroupViT would be an important addition to the paper.",1,"2022, 2020, 2017, 2017",Do convolutional neural networks learn class hierarchy
UwGn1ht8TrF,"This paper presents MeshDiffusion, a method for generating 3D meshes. The authors use the deformable tetrahedral grid as the 3D representation, and train a diffusion model on this parameterization. The authors demonstrated the effectiveness of the proposed model on multiple generative tasks, including unconditional and conditional generation.","+ The tetrahedral grid representation is able to generate fine details of 3D shapes with arbitrary topology.

+ The mesh output can be easily used in graphics pipelines.

- The main problem is: There's a recently published paper  ""GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images"" in NeurIPS 2022. This paper is released on Sep. 22, which is before the paper deadline of ICLR 2023 (Sep. 28). So it should be considered as ""previous work"". The GET3D paper uses a similar idea (tetrahedral representation) to perform a similar task. However, in the submission the authors didn't mention, review, or compare with GET3D.

- Multiple loss functions are used for training the network. However, the ablation study does not show the necessity of each loss function.","Clarity: The mathematical formulations in this paper needs more explanations. For the training objective equation in Sec. 4.2, it would be better if the symbols are explained right in this section, because it is hard for readers to find the meaning of the symbols in Sec. 3. Besides, the training objective equation is not numbered.

Quality: The result is better than compared methods. However, it is unclear whether it outperforms GET3D.

Novelty: The novelty is questionable since it is very similar to GET3D.

Reproducibility: It should be reproducible given the implementation details.

","The paper proposes a mesh generation model based on tetrahedral grid representation and diffusion model. The results show that the proposed method outperforms the baseline methods. However, the main issue is that one main state-of-the-art method ""GET3D"" was not mentioned or compared in this paper. Thus I would suggest the authors add this comparison and re-submit the paper.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1665950954198,,0cpM2ApF9p6,ICLR.cc/2023/Conference/Paper3314/-/Official_Review,,[],1,,['everyone'],0cpM2ApF9p6,['ICLR.cc/2023/Conference/Paper3314/Reviewer_Mo7X'],1665950954198,,1670633889576,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3314/Reviewer_Mo7X']","Yes, this peer review suggests that the authors of the paper refer to specific literature that is not already discussed in the original paper, specifically the ""GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images"" paper published in NeurIPS 2022. The reviewer mentions that this paper was not mentioned, reviewed, or compared in the original submission, and recommends that the authors add this comparison and re-submit the paper.",1,2022,GET3D A Generative Model of High Quality 3D Textured Shapes Learned from Images
3OqWai7yGUB,"The paper proposed a diffusion model-based mesh generator. 
It adopts the representation from the previous `deep marching tetrahedral`.
The method works in two steps by first learning to reconstruct 3D meshes and then training a diffusion model on the grid points using a 3D U-Net. 
","Strength:
- To my knowledge, it is the first diffusion model on 3D **mesh**. 
- On unconditional generation evaluation, it truly beats one of the latest SOTA(SDF-StyleGAN).

Weakness:
- Not particularly novel. The paper is more like a combination of previous tetrahedral generation work and the diffusion model, especially when I see that the method has two training steps. 

- Misses evaluation on SVR. Obviously, the first step can reconstruct shapes from 2D images. However, I could not find any evaluation on it neither in the paper nor in the appendix. 

- Miss closest shape results when showing generated shapes. Can you show the nearest shapes in training set to validate the generation capability of your generative model? 

- I saw claims like `extremely easy and stable to train.` and `MeshDiffusion is also very stable to train without bells and whistles.`. Is there any evidence to prove this?

- Holes seem still exist on the generated meshes. Are there any quantitative results that can support your point `REDUCING NOISE-SENSITIVITY OF MARCHING TETRAHEDRA`? 

- Why FID metric of the car category is so high(even worse than IM-GAN)? 

- Misses comparison with GET3D. 

Misses references:
point cloud diffusion model:
LION: Latent Point Diffusion Models for 3D Shape Generation

recent tetrahedral mesh generation method:
GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images

Typos:
`including unconditional generation, conditional generation and interpolation` -> 
`including unconditional generation, conditional generation**,** and interpolation`","The paper is well-structured and gives as many details as possible. 
Architecture and hyperparameters are given. I think the paper is reproducible. ","The paper proposes the first diffusion model on 3D meshes. 
However, due to its limited evaluation, I have doubts on its generation capacity and its claims on stable training and reducing noise. 
In addition, quantitative comparisons in FID metrics with baseline methods can not show its superiority.
The paper also misses a comparison with recent GET3D. 
So I am slightly leaning toward rejection. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666621943299,,0cpM2ApF9p6,ICLR.cc/2023/Conference/Paper3314/-/Official_Review,,[],2,,['everyone'],0cpM2ApF9p6,['ICLR.cc/2023/Conference/Paper3314/Reviewer_NSt1'],1666621943299,,1670421167327,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3314/Reviewer_NSt1']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers that are not referenced in the original paper:

1. ""LION: Latent Point Diffusion Models for 3D Shape Generation"" (a point cloud diffusion model)
2. ""GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images"" (a recent tetrahedral mesh generation method)

The reviewer explicitly states ""Misses references"" and lists these two papers, indicating that they believe these papers are relevant to the topic and should be included in the discussion.",1,,LION Latent Point Diffusion Models for 3D Shape Generation
3OqWai7yGUB,"The paper proposed a diffusion model-based mesh generator. 
It adopts the representation from the previous `deep marching tetrahedral`.
The method works in two steps by first learning to reconstruct 3D meshes and then training a diffusion model on the grid points using a 3D U-Net. 
","Strength:
- To my knowledge, it is the first diffusion model on 3D **mesh**. 
- On unconditional generation evaluation, it truly beats one of the latest SOTA(SDF-StyleGAN).

Weakness:
- Not particularly novel. The paper is more like a combination of previous tetrahedral generation work and the diffusion model, especially when I see that the method has two training steps. 

- Misses evaluation on SVR. Obviously, the first step can reconstruct shapes from 2D images. However, I could not find any evaluation on it neither in the paper nor in the appendix. 

- Miss closest shape results when showing generated shapes. Can you show the nearest shapes in training set to validate the generation capability of your generative model? 

- I saw claims like `extremely easy and stable to train.` and `MeshDiffusion is also very stable to train without bells and whistles.`. Is there any evidence to prove this?

- Holes seem still exist on the generated meshes. Are there any quantitative results that can support your point `REDUCING NOISE-SENSITIVITY OF MARCHING TETRAHEDRA`? 

- Why FID metric of the car category is so high(even worse than IM-GAN)? 

- Misses comparison with GET3D. 

Misses references:
point cloud diffusion model:
LION: Latent Point Diffusion Models for 3D Shape Generation

recent tetrahedral mesh generation method:
GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images

Typos:
`including unconditional generation, conditional generation and interpolation` -> 
`including unconditional generation, conditional generation**,** and interpolation`","The paper is well-structured and gives as many details as possible. 
Architecture and hyperparameters are given. I think the paper is reproducible. ","The paper proposes the first diffusion model on 3D meshes. 
However, due to its limited evaluation, I have doubts on its generation capacity and its claims on stable training and reducing noise. 
In addition, quantitative comparisons in FID metrics with baseline methods can not show its superiority.
The paper also misses a comparison with recent GET3D. 
So I am slightly leaning toward rejection. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666621943299,,0cpM2ApF9p6,ICLR.cc/2023/Conference/Paper3314/-/Official_Review,,[],2,,['everyone'],0cpM2ApF9p6,['ICLR.cc/2023/Conference/Paper3314/Reviewer_NSt1'],1666621943299,,1670421167327,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3314/Reviewer_NSt1']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers that are not referenced in the original paper:

1. ""LION: Latent Point Diffusion Models for 3D Shape Generation"" (a point cloud diffusion model)
2. ""GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images"" (a recent tetrahedral mesh generation method)

The reviewer explicitly states ""Misses references"" and lists these two papers, indicating that they believe these papers are relevant to the topic and should be included in the discussion.",1,,GET3D A Generative Model of High Quality 3D Textured Shapes Learned from Images
w8PgKxbl0Dx,"This paper studies the forward pass of a convolutional decoder in the frequency domain. It finds that the CNN layer forward propagates each frequency component in the
spectrum map independently to other frequency components.
It also finds that the CNN operations make a convolutional decoder network more likely to weaken high-frequency components.
","Strengths:

—-------------------

The paper has many strengths and is tackling an important problem. For the sake of time  I am only writing about weaknesses in this review, since those are the ones that should be actioned upon.


Weaknesses (W)

—-------------------

W: As far as I understand the analysis for the deep network is done on an idealized frequency domain network that does not correspond to a CNN with ReLUs and finite size filters. If this is true, this is the biggest drawback of the current paper. Furthermore, in real world networks, very small filters (e.g., 3x3 or even 2x2) usually provide the best results.

W: Keeping in mind the previous comment, the paper’s language might be too strong. It talks about proving various aspects of the forward propagation of the CNN, but maybe the proofs only apply to the idealized network?

W: Regarding zero-padding, there are previous works that try to compensate for the resulting border effects, e.g., by learning different filters to the borders or using circular or other types of padding and convolutions. Could the authors discuss how zero padding vs. these methods affect the end-results of the CNN decoder and whether it would be beneficial not to zero-pad.

W: The paper makes the claim that decoder CNN is biased towards learning low-frequency content. What is missing is the analysis on when this is actually harmful and whether it could be beneficial in some settings. Also, in cases where this is harmful, it would be interesting if the authors could provide a suggestion for improvement.

W: The analysis is done for randomly initialized networks. While this is an interesting point of analysis, it is lacking the effect of training altogether. I would suggest the authors to extend their work towards the training as well.

W: Based on Figure 3, ReLU seems to have a big effect when comparing the freq domain version of the network with the original one. A big downside of the analysis is the lack of analysing the effect of ReLU, since, of course, the activation function is a key component in enabling nonlinearity in the neural networks.

W: The authors should bring the related work section from the appendix to the main paper and also build a connection with related works on freq domain neural networks, for example. Pan, H., Chen, Y., Niu, X., Zhou, W., and Li, D., “Learning Convolutional Neural Networks in the Frequency Domain”, arXiv e-prints, 2022. https://arxiv.org/abs/2204.06718 , and many others.

W: The authors note that their work is different from the “F-Principle” e.g., https://arxiv.org/abs/1807.01251 because the current paper focuses on a “fully different type of frequency”, i.e., the frequency w.r.t. the DFT on an input image or a feature map. To me this distinction is not so clear and both works seem to be related. I would suggest the authors to build a better bridge between the works, since even the authors seem to be doing similar experiments, e.g., in “C.1 VERIFYING THAT A NEURAL NETWORK USUALLY LEARNED LOW-FREQUENT COMPONENTS FIRST.”



Minor issues and spelling mistakes:

B, page 26: “DNN first qucikly” -> DNN first quickly

Page 27: “DNNs tipically” -> DNNs typically

","The writing clarity and quality is ok. There are many minor grammar mistakes though. There seems to be novelty in the works, and it should be reproducible.
","Decent work with some issues that would warrant a new revision before acceptance.
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666450501899,,0cm8HroIxJV,ICLR.cc/2023/Conference/Paper1318/-/Official_Review,,[],2,,['everyone'],0cm8HroIxJV,['ICLR.cc/2023/Conference/Paper1318/Reviewer_V1PA'],1666450501899,,1666450544088,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1318/Reviewer_V1PA']","Yes, this peer review suggests the authors of the paper to refer to specific literature that may not be already discussed in the original paper. 

The reviewer mentions two specific papers:
1. Pan, H., Chen, Y., Niu, X., Zhou, W., and Li, D., “Learning Convolutional Neural Networks in the Frequency Domain”, arXiv e-prints, 2022. https://arxiv.org/abs/2204.06718
2. The “F-Principle” paper, https://arxiv.org/abs/1807.01251

The reviewer suggests that the authors should build a connection with these related works on frequency domain neural networks and clarify the distinction between their work and the ""F-Principle"" paper. Additionally, the reviewer mentions ""and many others"", implying that there may be other relevant literature that the authors should consider referencing.",1,"2022, 2022, 2018",Learning Convolutional Neural Networks in the Frequency Domain 
w8PgKxbl0Dx,"This paper studies the forward pass of a convolutional decoder in the frequency domain. It finds that the CNN layer forward propagates each frequency component in the
spectrum map independently to other frequency components.
It also finds that the CNN operations make a convolutional decoder network more likely to weaken high-frequency components.
","Strengths:

—-------------------

The paper has many strengths and is tackling an important problem. For the sake of time  I am only writing about weaknesses in this review, since those are the ones that should be actioned upon.


Weaknesses (W)

—-------------------

W: As far as I understand the analysis for the deep network is done on an idealized frequency domain network that does not correspond to a CNN with ReLUs and finite size filters. If this is true, this is the biggest drawback of the current paper. Furthermore, in real world networks, very small filters (e.g., 3x3 or even 2x2) usually provide the best results.

W: Keeping in mind the previous comment, the paper’s language might be too strong. It talks about proving various aspects of the forward propagation of the CNN, but maybe the proofs only apply to the idealized network?

W: Regarding zero-padding, there are previous works that try to compensate for the resulting border effects, e.g., by learning different filters to the borders or using circular or other types of padding and convolutions. Could the authors discuss how zero padding vs. these methods affect the end-results of the CNN decoder and whether it would be beneficial not to zero-pad.

W: The paper makes the claim that decoder CNN is biased towards learning low-frequency content. What is missing is the analysis on when this is actually harmful and whether it could be beneficial in some settings. Also, in cases where this is harmful, it would be interesting if the authors could provide a suggestion for improvement.

W: The analysis is done for randomly initialized networks. While this is an interesting point of analysis, it is lacking the effect of training altogether. I would suggest the authors to extend their work towards the training as well.

W: Based on Figure 3, ReLU seems to have a big effect when comparing the freq domain version of the network with the original one. A big downside of the analysis is the lack of analysing the effect of ReLU, since, of course, the activation function is a key component in enabling nonlinearity in the neural networks.

W: The authors should bring the related work section from the appendix to the main paper and also build a connection with related works on freq domain neural networks, for example. Pan, H., Chen, Y., Niu, X., Zhou, W., and Li, D., “Learning Convolutional Neural Networks in the Frequency Domain”, arXiv e-prints, 2022. https://arxiv.org/abs/2204.06718 , and many others.

W: The authors note that their work is different from the “F-Principle” e.g., https://arxiv.org/abs/1807.01251 because the current paper focuses on a “fully different type of frequency”, i.e., the frequency w.r.t. the DFT on an input image or a feature map. To me this distinction is not so clear and both works seem to be related. I would suggest the authors to build a better bridge between the works, since even the authors seem to be doing similar experiments, e.g., in “C.1 VERIFYING THAT A NEURAL NETWORK USUALLY LEARNED LOW-FREQUENT COMPONENTS FIRST.”



Minor issues and spelling mistakes:

B, page 26: “DNN first qucikly” -> DNN first quickly

Page 27: “DNNs tipically” -> DNNs typically

","The writing clarity and quality is ok. There are many minor grammar mistakes though. There seems to be novelty in the works, and it should be reproducible.
","Decent work with some issues that would warrant a new revision before acceptance.
",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666450501899,,0cm8HroIxJV,ICLR.cc/2023/Conference/Paper1318/-/Official_Review,,[],2,,['everyone'],0cm8HroIxJV,['ICLR.cc/2023/Conference/Paper1318/Reviewer_V1PA'],1666450501899,,1666450544088,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1318/Reviewer_V1PA']","Yes, this peer review suggests the authors of the paper to refer to specific literature that may not be already discussed in the original paper. 

The reviewer mentions two specific papers:
1. Pan, H., Chen, Y., Niu, X., Zhou, W., and Li, D., “Learning Convolutional Neural Networks in the Frequency Domain”, arXiv e-prints, 2022. https://arxiv.org/abs/2204.06718
2. The “F-Principle” paper, https://arxiv.org/abs/1807.01251

The reviewer suggests that the authors should build a connection with these related works on frequency domain neural networks and clarify the distinction between their work and the ""F-Principle"" paper. Additionally, the reviewer mentions ""and many others"", implying that there may be other relevant literature that the authors should consider referencing.",1,"2022, 2022, 2018",Training behavior of deep neural network in frequency domain
yPuFUNfNb94,"This paper proposes a higher order Brownian Motion Controller (BMC) for BrGANs to stabilize GANs' training process. Starting with the prototypical case of Dirac-GANs, the authors design a BMC and propose Dirac-BrGANs that retrieve exactly the same but reachable optimal equilibrium regardless of GANs' framework. The authors also provide corresponding proof and experiments, although the experimental results show poor performance.
","Strengths:
- The idea of taking GAN training as a dynamic system and applying Brownian Motion Controller on it sounds interesting.

Weaknesses:
- My biggest concern is about the experimental results. Only CIFAR-10 (32x32 resolution) and CelebA (64x64 resolution) are evaluated and the FID score is not state-of-the-art (actually there is a very big gap between the provided results and SOTA, eg. IS 5.42 on CIFAR-10 v.s. 9.18 in [1]). I truly understand the architecture may be different, but it would be better to see whether the proposed method can continually improve the performance on top of the best architecture. Given there is plenty of theory works studying GAN training stability issues, it is hard to trust this work can indeed help improve the GAN training process since this one considering its poor performance.
- Meanwhile, only two datasets are evaluated, and both of them are low-resolution. It would be better to show more results on LSUN, FFHQ, with their high-resolution version.
- The claim of ""the training process of Dirac-BrGANs achieves exponential stability almost surely"" sounds like an overclaim to me. To verify its correctness, I think some additional experiments are needed: e.g., using various hyperparameters (training G after training D every $N$ times), optimizers, batch size, and demonstrating that the proposed methods can help GAN training robust to different settings.
References:
[1] Differentiable Augmentation for Data-Efficient GAN Training",The experimental results are less satisfying and the analysis is not enough.,"This work propose a new way to stabilize the training process of GAN, using Brownian Motion Controller. Although the motivation is interesting, the results are less satisfactory and the quality of the work is lower than the ICLR's bar.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666929547246,,0YYQ_KKsIZ,ICLR.cc/2023/Conference/Paper2518/-/Official_Review,,[],4,,['everyone'],0YYQ_KKsIZ,['ICLR.cc/2023/Conference/Paper2518/Reviewer_sHwP'],1666929547246,,1666929547246,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2518/Reviewer_sHwP']","Yes, the peer review suggests the authors to refer to specific literature that may not be already discussed in the original paper. The reviewer mentions ""[1] Differentiable Augmentation for Data-Efficient GAN Training"" as a reference point for state-of-the-art results on CIFAR-10, implying that the authors should be aware of this work and compare their results to it. However, it is unclear whether this reference was already included in the original paper. 

The reviewer also mentions other datasets (LSUN, FFHQ) and their high-resolution versions, which may imply that the authors should be aware of literature related to these datasets, but it does not explicitly mention specific papers that the authors should refer to.",1,,Differentiable Augmentation for Data-Efficient GAN Training
5WWAfCA70Yj,"This paper for the first time adopts motif extraction as a pre-process step to generate the candidate set for explanation, and employs an attention-based method to identify the most important motifs as explanations.
","**Strength**
- Using motifs as explanations is a new and motivating idea
- The proposed algorithm is simple and effective
- The writing is easy to follow

**Weaknesses**
- The method heavily relies on the quality of motif extraction rules, whose complexity should also be discussed
- Some details need to be explained as commented in my summary
","The writing and presentation is in general clear and easy to follow. The novelty lies in the using motifs to provide explanations, which triggers a simple (with limited technical contribution) yet effective method. The codes are provided for reproducibility.
","This paper provides a new way to explain GNNs via motifs. The method is simple, practical and effective, while the idea is straight-forward with limited technical contribution. I also have the following questions.
- The method requires a motif extraction step, which heavily relies on domain knowledge. As a fair comparison, what if directly applying the general motif extraction algorithm for all datasets, including the molecule datasets?
- The complexity of the motif extraction algorithm should be discussed, especially when the graph is large.
- There is a contradiction in the method description: $y$ is the predicted label based on Figure 1, but in Algorithm 1 it represents the ground truth label of the graph. Which one is adopted in the actual design?
- How large is the motif list for different datasets, compared with the candidate set of SubgraphX?
- How are the datasets split for training and testing?
- The fidelity metric defined in Eq. (10) is unclear: the output of the feature extractor $\Psi$ should be the embedding, then what is the meaning of the subscript $y_i$? Presumably, it should be based on the prediction accuracy or probability (e.g. output of the classifier $\xi$) as in [1].
- The reported evaluation metric in Table 1 should be explained in the title to avoid confusion: for BA-2Motif and BA-Shape datasets, the metric is accuracy.
- In section 2, “donate” -> “denote”

[1] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic survey. arXiv preprint arXiv:2012.15445, 2020b.

","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1667451687532,,0YXmOFLb1wQ,ICLR.cc/2023/Conference/Paper1107/-/Official_Review,,[],5,,['everyone'],0YXmOFLb1wQ,['ICLR.cc/2023/Conference/Paper1107/Reviewer_b3v8'],1667451687532,,1667451687532,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1107/Reviewer_b3v8']","Yes, the peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer mentions a reference [1] ""Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic survey. arXiv preprint arXiv:2012.15445, 2020b."" and implies that the authors should consider the definition of fidelity metric used in this reference, suggesting that it might be more suitable for their work. This reference is not mentioned in the provided context of the original paper, indicating that it is an external suggestion by the reviewer.",1,2020,Explainability in graph neural networks: A taxonomic survey
JJFeriOVLRH,This paper introduced the Entity-Factored Markov Decision Process (EFMDP) for modeling the entity-based compositional structure in controlling tasks. The authors studied several structured policy architectures that can utilize the factorized discrete entities on a suite of manipulation tasks. Experimental results showed that structured policy architectures have faster learning speed in general when compared to MLP policy. Robust extrapolation and OOD generalization at the skill-level can also be observed.,"### Strength
- The experiments are well designed, and the results support the author's statement well.

### Weakness
- Major:
  - As the authors mentioned also, the current model takes vector state as input. However, a model that can handle visual input is more desired. A related work, SMORL[1], has shown promising results given visual input. Similar results can also be obtained from there.


-------------After rebuttal-----------------------

Thanks for the author's responses. I decide to increase my score to 5. 
1)I still think the proposed work shares some common motivations with SMORL, such as the reduced size of effective state space. The authors did investigate more architecture than the SMORL work. However, the experiment's only finding is that utilizing the factorized entity-based structure improves the performance, which does not have much novelty. Similar findings have been demonstrated in prior work like SMORL. 2)Though given the ground-truth entity states and goal state as input, the proposed work is solving a much simpler task than SMORL, I'm convinced that the OOD experiments are meaningful, and I'd like to increase my score to 5. However, I am leaning toward rejection based on the current experimental results.


[1]: Andrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Self-supervised visual reinforcement learning with object-centric representations.","The paper is well-written and easy to follow. The results can be reproduced with reasonable effort.
","This paper introduced the Entity-Factored Markov Decision Process (EFMDP) for modeling the entity-based compositional structure in controlling tasks. Experiments showed that structured policy outperforms the MLP policy. However, similar results have been obtained from related work already.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666680148621,,0W1TQ_hoMFN,ICLR.cc/2023/Conference/Paper2082/-/Official_Review,,[],4,,['everyone'],0W1TQ_hoMFN,['ICLR.cc/2023/Conference/Paper2082/Reviewer_Et3S'],1666680148621,,1669870581922,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2082/Reviewer_Et3S']","Yes, this peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer mentions ""SMORL"" (Self-supervised visual reinforcement learning with object-centric representations) by Andrii Zadaianchuk, Maximilian Seitzer, and Georg Martius, which is not mentioned in the original paper's summary. The reviewer compares the authors' work to SMORL, indicating that SMORL has already demonstrated similar findings and suggests that the authors should be aware of this related work.",1,,Self-supervised visual reinforcement learning with object-centric representations
K2J72DDcq9,"This paper proposes an approach for the estimation of human body 3D pose and shape from monocular input (single image or video). The core idea is to use learnable independent tokens for the joint rotations, the camera parameters (the weak-perspective camera model is used) and the body shape, as expressed in the used SMPL model. This allows for an extension for the case of video, with the temporal information first captured per joint by the respective token, and then decoded by a suitable temporal transformer. This approach implicitly imposes temporal smoothness, yielding temporally consistent results with minimized (but not eliminated) jitter.","The manuscript clearly presents the core idea and provides intuitive justification on why the proposed approach is beneficial for the task at hand. The literature overview is comprehensive within the space limitations, the method description is clear, and the experimental results are extensive and convincing. Apart from a few syntax and grammar errors (and also the use of the word ""achieve"" instead of ""compute"", please fix this) this is a well-written paper overall.

I don't have any major points of criticism, rather some thoughts for potential improvement: Recent advances in human pose estimation using transformers indicate that it is beneficial to use more queries than the total number of estimated keypoints (see eg Li et al. ""Pose Recognition With Cascade Transformers"" 2021 -- a good reference to add to the literature overview by the way, despite the fact that it only regresses 2D keypoints). It is not clear whether such an approach would also be beneficial with the proposed approach, or even possible. Please consider commenting on this, it is unclear to me whether it also makes sense to experiment on this.","In my opinion the clarity of presentation is fine given the space constraints. To the best of my knowledge, the presented approach is novel for the task at hand. Transformer architectures have been applied before for the problem, but this specific approach allows for state-of-the-art results and temporal consistency. Similarly, independent tokens have been proposed before, but not in the given context. Furthermore, the use of these tokens allows for a straightforward temporal extension, which constitutes a novel contribution to the best of my knowledge. Lastly, reproducibility is not an issue here to the best of my understanding.","Overall I think that, despite the limited novelty, it is still a well presented idea that achieves state-of-the-art results in the problem that is worth accepting in ICLR.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666637781674,,0Vv4H4Ch0la,ICLR.cc/2023/Conference/Paper3994/-/Official_Review,,[],3,,['everyone'],0Vv4H4Ch0la,['ICLR.cc/2023/Conference/Paper3994/Reviewer_EHtd'],1666637781674,,1666637781674,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3994/Reviewer_EHtd']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions ""Li et al. 'Pose Recognition With Cascade Transformers' 2021"" as a good reference to add to the literature overview, implying that it is not already included in the paper.",1,2021,Pose Recognition With Cascade Transformers
s8kuRRrdfZ,"This paper investigates the streaming property in ST and propose new streaming methods.
The two proposed methods: FINE-mask and FINE-wait are proven to be effective and achieve better trade-offs between translation quality and latency.
Furthermore, these methods can effectively alleviate the mismatch problem between offline training and online inference.","Strength:
The paper propose two different kinds of method and good results are obtained.


Weaknesses:
Although the investigation may draw a lot of interest, the conclusion is not surprising.
In 3.1, the investigation shows that the representations at the end position has a higher gap with the full input case. This is not surprising as   frames near the boundary has no future context comparing with the full input case.
By the way, if the end position is actually the end position of the whole sentence, the representations should have no difference with the full input case.

I also think the authors should compare their methods with the idea in ""Low latency end-to-end streaming speech recognition with a scout network""

The FINE-wait method proposed by the paper is not new.","Clarity: The paper is clearly written.

Quality: Standard.

Novelty: Marginal. The novelty is mainly contributed by the FINE-mask method. For the FINE-wait method, it is not new and is just slightly modified from existing wait-k method.

Reproducibility: The codes are provided in the paper and others can replicate the results.

","This paper is motived by an observation that representations at the end position are usually very different from the full input case.
The authors try to solve this problem by proposing two different methods and good results are obtained.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666704221689,,0VhwJYrZew,ICLR.cc/2023/Conference/Paper2109/-/Official_Review,,[],1,,['everyone'],0VhwJYrZew,['ICLR.cc/2023/Conference/Paper2109/Reviewer_JSnA'],1666704221689,,1666840085937,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2109/Reviewer_JSnA']","Yes, the peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer mentions ""Low latency end-to-end streaming speech recognition with a scout network"" as an idea that the authors should compare their methods with, implying that this work is not cited or discussed in the original paper.",1,,Low latency end-to-end streaming speech recognition with a scout network
A4eBv7FSV6,"The paper analyzes parallel neural networks for nonparametric regression (including approximation error and regression error analysis). Suppose the regression function has nonhomogeneous regularities, e.g., Besov functions or functions of bounded variation. Parallel neural networks trained with weight decay achieves near optimal estimation, indicating the adaptability of parallel neural networks.","============= Strength =============

The paper is clearly written with both theoretical and empirical results. The theoretical results are correct and empirical results are supportive.

The adaptability of neural networks is studied in many recent works, including its adaptability to data geometry and adaptability to structures in function spaces. This paper considers parallel neural networks and uses weight decay for controlling the complexity of the network, which circumvents sparsity constraints in many existing network approximation results.

============= Weakness =============

There is no related work section. Some of papers share similar ideas to parallel neural networks, for example, ""Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks"" and ""Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks"" both rely on implementing block sparse feedforward networks. The resulting (residual) convolutional network does not need a sparsity constraint in training.

The obtained rate of estimation is slightly slower than the optimal rate, which is attainable (up to log factors) in many recent works on using ReLU neural networks for nonparametric regression.

From experiments, parallel neural networks seem to achieve comparable performance to neural networks and conventional methods.
  ","Some of the technical contributions need highlighting. For example, the conversion of weight decay objective to the constraint form in Equation (5) can be highlighted as a contribution -- if it does not appear in Parhi & Nowak (2021a). Another example is the decomposition of \mathcal{F}_{\parallel} and \mathcal{F}_{\perp}.

A discussion on why parallel neural networks can only achieve the optimal rate in the asymptotic sense ($L \to \infty$) is helpful. Is it because the weight decay introduces additional bias?

In Theorem 1, it is good to state how to choose weight decay $\lambda$.

Lemma 6 is relative difficult to understand, mainly because the first sentence begins with an equation.

Overall, the paper is well written. There are some typos and typeset issues, for example, ""errorand"" --> ""error and"" above Section 4.2","I think the paper is interesting. I am currently giving a slightly negative rating, due to some concerns of the results and missing references. I am happy to raise to a positive rating after paper revision and an effective author feedback.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666677431707,,0Q9H_Pgx132,ICLR.cc/2023/Conference/Paper1123/-/Official_Review,,[],2,,['everyone'],0Q9H_Pgx132,['ICLR.cc/2023/Conference/Paper1123/Reviewer_wKTh'],1666677431707,,1668802036440,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1123/Reviewer_wKTh']","Yes, the peer review suggests that the authors of the paper refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: ""Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks"" and ""Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks"", which indicate that the reviewer thinks these papers share similar ideas to parallel neural networks and should be included in the related work section. Additionally, the reviewer mentions Parhi & Nowak (2021a), which may be already included in the original paper, but it's not clear from the review. The reviewer also mentions that there is no related work section in the paper, which implies that the authors should include a discussion of relevant literature, including the mentioned papers.",1,,Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks
A4eBv7FSV6,"The paper analyzes parallel neural networks for nonparametric regression (including approximation error and regression error analysis). Suppose the regression function has nonhomogeneous regularities, e.g., Besov functions or functions of bounded variation. Parallel neural networks trained with weight decay achieves near optimal estimation, indicating the adaptability of parallel neural networks.","============= Strength =============

The paper is clearly written with both theoretical and empirical results. The theoretical results are correct and empirical results are supportive.

The adaptability of neural networks is studied in many recent works, including its adaptability to data geometry and adaptability to structures in function spaces. This paper considers parallel neural networks and uses weight decay for controlling the complexity of the network, which circumvents sparsity constraints in many existing network approximation results.

============= Weakness =============

There is no related work section. Some of papers share similar ideas to parallel neural networks, for example, ""Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks"" and ""Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks"" both rely on implementing block sparse feedforward networks. The resulting (residual) convolutional network does not need a sparsity constraint in training.

The obtained rate of estimation is slightly slower than the optimal rate, which is attainable (up to log factors) in many recent works on using ReLU neural networks for nonparametric regression.

From experiments, parallel neural networks seem to achieve comparable performance to neural networks and conventional methods.
  ","Some of the technical contributions need highlighting. For example, the conversion of weight decay objective to the constraint form in Equation (5) can be highlighted as a contribution -- if it does not appear in Parhi & Nowak (2021a). Another example is the decomposition of \mathcal{F}_{\parallel} and \mathcal{F}_{\perp}.

A discussion on why parallel neural networks can only achieve the optimal rate in the asymptotic sense ($L \to \infty$) is helpful. Is it because the weight decay introduces additional bias?

In Theorem 1, it is good to state how to choose weight decay $\lambda$.

Lemma 6 is relative difficult to understand, mainly because the first sentence begins with an equation.

Overall, the paper is well written. There are some typos and typeset issues, for example, ""errorand"" --> ""error and"" above Section 4.2","I think the paper is interesting. I am currently giving a slightly negative rating, due to some concerns of the results and missing references. I am happy to raise to a positive rating after paper revision and an effective author feedback.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666677431707,,0Q9H_Pgx132,ICLR.cc/2023/Conference/Paper1123/-/Official_Review,,[],2,,['everyone'],0Q9H_Pgx132,['ICLR.cc/2023/Conference/Paper1123/Reviewer_wKTh'],1666677431707,,1668802036440,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1123/Reviewer_wKTh']","Yes, the peer review suggests that the authors of the paper refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: ""Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks"" and ""Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks"", which indicate that the reviewer thinks these papers share similar ideas to parallel neural networks and should be included in the related work section. Additionally, the reviewer mentions Parhi & Nowak (2021a), which may be already included in the original paper, but it's not clear from the review. The reviewer also mentions that there is no related work section in the paper, which implies that the authors should include a discussion of relevant literature, including the mentioned papers.",1,,Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks
p3Tymla7Zwt,"This work aims to study how to effectively achieve compact bilinear representations. To this end, the authors proposed a low-rank factorized bilinear pooling method. Specifically, as shown in Eq. (10), $k$ parallel 1x1 convolutions are used to reduce feature dimension before bilinear pooling, and then $k$ bilinear representations are summed following by a fully-connected layer. As such, size of bilinear representations could be controlled by 1x1 convolution and fully-connected layer. The experiments are conducted on several small-scale fine-grained image benchmarks.","Strength:

+: The idea on parallel 1x1 convolutions for reducing feature dimension before bilinear pooling seems bring clear improvement (Fig.5 and Table 1), which may be a feasible solution to balance performance and model complexity for bilinear pooling methods.

+: On several small-scale fine-grained image benchmarks, the proposed method achieves competitive results with a relatively low representation dimension.

+: The proposed method is clearly written and is easy to implement.

Weaknesses:

-: Could $\mathbf{U}$ and $\mathbf{V}$ in Eq. (10) be shared?

-: The idea on combination of 1x1 convolutions and fully-connected layer for obtaining compact covariance representations appeared in [r1] (Section 3.5 and Table 4), where sequential 1x1 convolutions (not parallel) are used to reduce feature dimension before covariance pooling, and then a fully-connected layer is adopted after covariance pooling. It would better discuss relationship between the proposed method and [r1].

[r1] Deep CNNs Meet Global Covariance Pooling: Better Representation and Generalization. IEEE T-PAMI, 2021.

-: The concerns on experiments.

(1)	Are hyper-parameters $k$ and $h$ sensitive to various datasets? Particularly, could the proposed method be flexibly adopted to large-species classification (e.g., iNat2017 [r2]) with the same hyper-parameters?

[r2] The iNaturalist species classification and detection dataset. CVPR, 2018.

(2)	I wonder what meaning of ‘Param’ in Table 2. Particularly, why Param of iSQRT-COV with backbone of ResNet-50 is 312M? To our best knowledge, iSQRT-COV is parameter-free itself. For dimension reduction, a 1x1 convolution (2048, 256) is used and contain about 0.5M parameters. If ‘Param’ in Table 2 contains one of classifier, it clearly varies for different datasets, while RK-FBP (2+) and RK-FBP (2+,3+) should have different parameters.

(3)	Besides parameters, FLOPs and running time are important metric to evaluate model complexity.

(4)	I would like to know how about performance of the proposed method under training-from-scratch setting (e.g., ImageNet-1K).
",The paper is generally very written. The idea on parallel 1x1 convolutions for reducing feature dimension before bilinear pooling may be a feasible solution. The proposed method is easy to implement. More evaluation and discussion seem be necessary.  ,"I have reviewed this work in previous conference, and the current version achieves clear improvement on writing and experiments. However, discussion on previous works and experimental evaluation could be further strengthened. Particularly, the authors should clarify meaning of ‘Param’ in Table 2. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666415633158,,0PH-P_FIqGD,ICLR.cc/2023/Conference/Paper1432/-/Official_Review,,[],1,,['everyone'],0PH-P_FIqGD,['ICLR.cc/2023/Conference/Paper1432/Reviewer_MKth'],1666415633158,,1666415633158,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1432/Reviewer_MKth']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific references, [r1] (""Deep CNNs Meet Global Covariance Pooling: Better Representation and Generalization. IEEE T-PAMI, 2021"") and [r2] (""The iNaturalist species classification and detection dataset. CVPR, 2018""), which are likely not already included in the original paper. The reviewer suggests that the authors should discuss the relationship between their proposed method and [r1], and also consider evaluating their method on the dataset mentioned in [r2].",1,"2021, 2018",Deep CNNs Meet Global Covariance Pooling Better Representation and Generalization 
p3Tymla7Zwt,"This work aims to study how to effectively achieve compact bilinear representations. To this end, the authors proposed a low-rank factorized bilinear pooling method. Specifically, as shown in Eq. (10), $k$ parallel 1x1 convolutions are used to reduce feature dimension before bilinear pooling, and then $k$ bilinear representations are summed following by a fully-connected layer. As such, size of bilinear representations could be controlled by 1x1 convolution and fully-connected layer. The experiments are conducted on several small-scale fine-grained image benchmarks.","Strength:

+: The idea on parallel 1x1 convolutions for reducing feature dimension before bilinear pooling seems bring clear improvement (Fig.5 and Table 1), which may be a feasible solution to balance performance and model complexity for bilinear pooling methods.

+: On several small-scale fine-grained image benchmarks, the proposed method achieves competitive results with a relatively low representation dimension.

+: The proposed method is clearly written and is easy to implement.

Weaknesses:

-: Could $\mathbf{U}$ and $\mathbf{V}$ in Eq. (10) be shared?

-: The idea on combination of 1x1 convolutions and fully-connected layer for obtaining compact covariance representations appeared in [r1] (Section 3.5 and Table 4), where sequential 1x1 convolutions (not parallel) are used to reduce feature dimension before covariance pooling, and then a fully-connected layer is adopted after covariance pooling. It would better discuss relationship between the proposed method and [r1].

[r1] Deep CNNs Meet Global Covariance Pooling: Better Representation and Generalization. IEEE T-PAMI, 2021.

-: The concerns on experiments.

(1)	Are hyper-parameters $k$ and $h$ sensitive to various datasets? Particularly, could the proposed method be flexibly adopted to large-species classification (e.g., iNat2017 [r2]) with the same hyper-parameters?

[r2] The iNaturalist species classification and detection dataset. CVPR, 2018.

(2)	I wonder what meaning of ‘Param’ in Table 2. Particularly, why Param of iSQRT-COV with backbone of ResNet-50 is 312M? To our best knowledge, iSQRT-COV is parameter-free itself. For dimension reduction, a 1x1 convolution (2048, 256) is used and contain about 0.5M parameters. If ‘Param’ in Table 2 contains one of classifier, it clearly varies for different datasets, while RK-FBP (2+) and RK-FBP (2+,3+) should have different parameters.

(3)	Besides parameters, FLOPs and running time are important metric to evaluate model complexity.

(4)	I would like to know how about performance of the proposed method under training-from-scratch setting (e.g., ImageNet-1K).
",The paper is generally very written. The idea on parallel 1x1 convolutions for reducing feature dimension before bilinear pooling may be a feasible solution. The proposed method is easy to implement. More evaluation and discussion seem be necessary.  ,"I have reviewed this work in previous conference, and the current version achieves clear improvement on writing and experiments. However, discussion on previous works and experimental evaluation could be further strengthened. Particularly, the authors should clarify meaning of ‘Param’ in Table 2. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666415633158,,0PH-P_FIqGD,ICLR.cc/2023/Conference/Paper1432/-/Official_Review,,[],1,,['everyone'],0PH-P_FIqGD,['ICLR.cc/2023/Conference/Paper1432/Reviewer_MKth'],1666415633158,,1666415633158,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1432/Reviewer_MKth']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific references, [r1] (""Deep CNNs Meet Global Covariance Pooling: Better Representation and Generalization. IEEE T-PAMI, 2021"") and [r2] (""The iNaturalist species classification and detection dataset. CVPR, 2018""), which are likely not already included in the original paper. The reviewer suggests that the authors should discuss the relationship between their proposed method and [r1], and also consider evaluating their method on the dataset mentioned in [r2].",1,"2021, 2018",The iNaturalist species classification and detection dataset
Dx9Omd0_4u,This paper tackles the problem of Out-of-distribution detection. A pre-trained object-detection network is firstly used to build a semantic graph. Then the graph embedding is further used to detect OOD samples. The experiment is conducted on the far-OOD and near-OOD setting.,"
Strength: 

	1. The idea of building a semantic map to detect OOD is novel. It points out one promising direction for the interpretable OOD detection, which can tell ""why the sample is an OOD"". 

	2. The comparison of different graph embedding algorithms is comprehensive. 

Weakness: 

	1. The proposed method relies on a pretrained object detection network that contains the sufficient semantic information for the in-distribution data. When the semantic of in-distribution data like medical images is not covered by the object detection network (pre-trained on natural image), the built semantic graph can be incomplete or even erroneous. If we use additional annotations to train a sufficiently strong object detection network, the effort will be extremely expensive comparing the existing methods. 

	2. The paper is not polished and not ready to publish, with missing details in related work / experiment / writing. See more in ""Clarity, Quality, Novelty And Reproducibility"". 
","
1. Experimental section is far from enough to validate the effectiveness of the introduced method. Missing experiment includes: 

 a). Comparison of other feature-based OOD detection method like:

     [A] Lee, K., Lee, K., Lee, H., & Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31.

     [B] Sehwag, V., Chiang, M., & Mittal, P. (2020, September). SSD: A Unified Framework for Self-Supervised Outlier Detection. In International Conference on Learning Representations.

 b). Comparison of other commonly used in-distribution data like CIFAR-10/100 and ImageNet. The goal is to verify if the pre-trained object detector build semantic map on more types of in-distribution data. 

 c). Ablation study of the hyperparameters (e.g. $\epsilon$). 

 d). Interpretability of the OOD detection by semantic graph is not presented. 


2. The paper is poorly written with lots of glitches and missing details:

 - Page 2: [Zhu et al. (2022) missing right square bracket.
 - Z is given without explanation in Problem Setup.
 - Figure 1 is poorly presented by missing explanatory details of digits and colors. 
 - Page 3: ”graph-feature”, quoting marks have the same direction. 
 - Explanation of Section 4 is insufficient for others to reproduce the work. A complete inference pipeline is not formally presented. 

3. The related work is poorly discussed. A large part of OOD detection methods with distance-based score and logit-based score is missing. ","This paper provides an interesting idea that uses the pre-trained object detector to build semantic graph and then detect OOD samples by the graph embedding. Firstly, the assumption of having an oracle object detector for all in-distribution data is unrealistic. Secondly, the idea is poorly presented by the insufficient details and the experimental results are limited. Overall, the paper is far from ready to be published in this venue. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1665612470904,,0OlEBibFa_g,ICLR.cc/2023/Conference/Paper6532/-/Official_Review,,[],1,,['everyone'],0OlEBibFa_g,['ICLR.cc/2023/Conference/Paper6532/Reviewer_qAw3'],1665612470904,,1666368539105,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6532/Reviewer_qAw3']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Lee, K., Lee, K., Lee, H., & Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31.
2. Sehwag, V., Chiang, M., & Mittal, P. (2020, September). SSD: A Unified Framework for Self-Supervised Outlier Detection. In International Conference on Learning Representations.

The reviewer suggests that the authors should compare their method with these feature-based OOD detection methods, implying that they are not already discussed in the original paper. Additionally, the reviewer mentions that a large part of OOD detection methods with distance-based score and logit-based score is missing from the related work, suggesting that the authors should discuss these methods as well.",1,"2018, 2020",A simple unified framework for detecting out-of-distribution samples and adversarial attacks 
Dx9Omd0_4u,This paper tackles the problem of Out-of-distribution detection. A pre-trained object-detection network is firstly used to build a semantic graph. Then the graph embedding is further used to detect OOD samples. The experiment is conducted on the far-OOD and near-OOD setting.,"
Strength: 

	1. The idea of building a semantic map to detect OOD is novel. It points out one promising direction for the interpretable OOD detection, which can tell ""why the sample is an OOD"". 

	2. The comparison of different graph embedding algorithms is comprehensive. 

Weakness: 

	1. The proposed method relies on a pretrained object detection network that contains the sufficient semantic information for the in-distribution data. When the semantic of in-distribution data like medical images is not covered by the object detection network (pre-trained on natural image), the built semantic graph can be incomplete or even erroneous. If we use additional annotations to train a sufficiently strong object detection network, the effort will be extremely expensive comparing the existing methods. 

	2. The paper is not polished and not ready to publish, with missing details in related work / experiment / writing. See more in ""Clarity, Quality, Novelty And Reproducibility"". 
","
1. Experimental section is far from enough to validate the effectiveness of the introduced method. Missing experiment includes: 

 a). Comparison of other feature-based OOD detection method like:

     [A] Lee, K., Lee, K., Lee, H., & Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31.

     [B] Sehwag, V., Chiang, M., & Mittal, P. (2020, September). SSD: A Unified Framework for Self-Supervised Outlier Detection. In International Conference on Learning Representations.

 b). Comparison of other commonly used in-distribution data like CIFAR-10/100 and ImageNet. The goal is to verify if the pre-trained object detector build semantic map on more types of in-distribution data. 

 c). Ablation study of the hyperparameters (e.g. $\epsilon$). 

 d). Interpretability of the OOD detection by semantic graph is not presented. 


2. The paper is poorly written with lots of glitches and missing details:

 - Page 2: [Zhu et al. (2022) missing right square bracket.
 - Z is given without explanation in Problem Setup.
 - Figure 1 is poorly presented by missing explanatory details of digits and colors. 
 - Page 3: ”graph-feature”, quoting marks have the same direction. 
 - Explanation of Section 4 is insufficient for others to reproduce the work. A complete inference pipeline is not formally presented. 

3. The related work is poorly discussed. A large part of OOD detection methods with distance-based score and logit-based score is missing. ","This paper provides an interesting idea that uses the pre-trained object detector to build semantic graph and then detect OOD samples by the graph embedding. Firstly, the assumption of having an oracle object detector for all in-distribution data is unrealistic. Secondly, the idea is poorly presented by the insufficient details and the experimental results are limited. Overall, the paper is far from ready to be published in this venue. ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1665612470904,,0OlEBibFa_g,ICLR.cc/2023/Conference/Paper6532/-/Official_Review,,[],1,,['everyone'],0OlEBibFa_g,['ICLR.cc/2023/Conference/Paper6532/Reviewer_qAw3'],1665612470904,,1666368539105,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6532/Reviewer_qAw3']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Lee, K., Lee, K., Lee, H., & Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31.
2. Sehwag, V., Chiang, M., & Mittal, P. (2020, September). SSD: A Unified Framework for Self-Supervised Outlier Detection. In International Conference on Learning Representations.

The reviewer suggests that the authors should compare their method with these feature-based OOD detection methods, implying that they are not already discussed in the original paper. Additionally, the reviewer mentions that a large part of OOD detection methods with distance-based score and logit-based score is missing from the related work, suggesting that the authors should discuss these methods as well.",1,"2018, 2020",SSD A Unified Framework for Self-Supervised Outlier Detection
1yeZ8W7dpP,"In this paper, the authors focus on the OOD detection task, where they consider both near OOD and Far OOD. To this end, they aim to exploit graph structures and topological properties to improve this task. Specifically, they propose a semi-supervised geometric-learning-based framework that operates on human-interpretable concepts. Finally, the authors conduct experiments on the LSUN dataset to verify the performance of the proposed method.","Strength

This work is not well-prepared so that I cannot give points of strength.

Weakness

1. This paper writing should be improved. First, the notations in this work are confused, such as ""Near OOD"" and ""Far OOD"". Their notations in the introduction seems different from those in the Table 1. Second, the motivation and the proposed method are not clearly described, so I cannot give a judgement for its novelty. Third, this work totally ignores the literature of OOD detection, which is generally introduced in the survey [1].

2. The empirical results are not convincing. The proposed method is not compared to existing state-of-the-art methods for OOD detection, which can be found in [1]. Instead, the authors compare the proposed method to some graph embedding algorithms, which is confused for me. Moreover, they just conduct experiments on the LSUN dataset, which is not extensive.


[1] Yang, Jingkang, et al. ""Generalized out-of-distribution detection: A survey."" arXiv preprint arXiv:2110.11334 (2021).","- This work is not well-prepared, so I cannot give a judgement for its novelty.
- The paper writing is poor and should be improved as described above.
- The empirical evaluation is not in a proper setting and also not extensive.","This work is not well-prepared and also not conducted in a way of scientific research. So, I recommend a strong reject.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.,1: The contributions are neither significant nor novel.,1: The contributions are neither significant nor novel.,NO.,1: strong reject,1667425650445,,0OlEBibFa_g,ICLR.cc/2023/Conference/Paper6532/-/Official_Review,,[],3,,['everyone'],0OlEBibFa_g,['ICLR.cc/2023/Conference/Paper6532/Reviewer_ox4H'],1667425650445,,1667425650445,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6532/Reviewer_ox4H']","Yes, this peer review suggests the authors of the paper to refer to specific literature that may not be already discussed in the original paper. The reviewer mentions a survey paper [1] ""Generalized out-of-distribution detection: A survey"" by Yang et al., which is not mentioned in the summary of the paper. The reviewer recommends that the authors compare their method to existing state-of-the-art methods for OOD detection, which can be found in this survey, implying that these methods may not be discussed in the original paper.",1,,Generalized out-of-distribution detection A survey 
aOP7Ietqlyt,"The paper studies trust region multi-agent policy gradients. The core contribution is a normalization term when calculating the advantage function. The authors compute the partial derivatives of global Q functions with respect to local utility functions to identify actions subject to the IGM condition. Only those ""IGM actions"" will be used to calculate advantage functions.","The reviewer is mainly concerned about the soundness of the proposed method. The use of partial derivatives in masking out action in the calculation of advantage functions is not well supported. In practice, the sign of the derivatives may change abruptly and is vulnerable to gradient noise, which may render the learning unstable.

The background section is clearly written, but the method section has various issues (discussed in the following sections) and is somewhat difficult to follow. The reviewer had to ","__Quality__

1. The derivation of the constrained advantage seems to lack theoretical support or sound analysis. How does the authors guarantee that a 0-1 alpha assignment depending on the sign of partial derivatives (Equation 15) satisfies Equation 13, noting that the sign of these derivatives are changing during the learning process.

2. After giving Equation 13, the authors motivate their method by saying that the decomposition cannot guarantee monotonic improvement.  However, although the monotonic improvement is not guaranteed, a linear decomposition guarantees convergence to local optima [Wang et al. 2021, Off-Policy Multi-Agent Decomposed Policy Gradients]. This raises a possible problem: non-monotonic improvement may bring some benefits similar to simulated annealing. Is monotonic improvement a good property in the aspect of increasing the probability of finding global optima?

3. Many claims in the paper are problematic. Some errors occur to very basic knowledge in the MARL field.

> ""similar to QMIX, the estimated joint-action advantage function can be represented by the summation of the local advantage function""

In QMIX, the joint-action value function is not a summation of local advantage functions, but a learnable monotonic combination.

> ""by imposing a monotonic constraint on the relationship between $A_\pi(s, a)$ and $A(s, a_i)$, the global arg max on joint-action yields the same results as a set of arg max individual action as follows""

The correctness of Equation (12) does not depend on the monotonic constraint. Actually, this holds whenever the IGM holds, which is obvious by subtracting $V$ from $Q$.","The paper studies an important problems, but there are many issues about the proposed method.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666642956090,,0LJRS7B3r4_,ICLR.cc/2023/Conference/Paper4978/-/Official_Review,,[],3,,['everyone'],0LJRS7B3r4_,['ICLR.cc/2023/Conference/Paper4978/Reviewer_ghg2'],1666642956090,,1666642956090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4978/Reviewer_ghg2']","Yes, the peer review suggests the authors to refer to specific literature that may not be already discussed in the original paper. The reviewer mentions ""Wang et al. 2021, Off-Policy Multi-Agent Decomposed Policy Gradients"", which is not mentioned in the provided context of the paper. This suggests that the reviewer is introducing new literature that the authors may not be aware of, in order to support their criticism of the paper's method.",1,2021,Off-Policy Multi-Agent Decomposed Policy Gradients
s54Lne8iaJX,"The authors propose a new method to train spiking neural networks (SNNs) with only one timestep. The method is based on a new regularization term (""Hoyer""), and a new threshold adjustment method (""Hoyer spike"", Eq 4).

","STRENGTHS:

The accuracy they reach with only one timestep is impressive, better than the previous state-of-the-art.

WEAKNESSES:

The theory is unclear:

* ""we estimate the value of the Hoyer extremum as He(ul) =""
I think what follows is not the value of H but the value of the element of u_l at the extremum.

* a SNN with one timestep is actually a somewhat degenerated case because there is no temporal integration. It boils down to a vanilla feed-forward artificial neural network, with Heaviside as the activation function. This also corresponds to the first neuron model proposed by McCulloch and Pitts and 1943, also known as threshold gates. This should be discussed.

* according to Table 5 what matters is the Hoyer spike, not the Hoyer regularization. This raises a question: could Hoyer spike be used alone, without the Hoyer regularization?

* does the approach extend to multistep SNNs?

","* Will the authors share their code?

* Eq 1 is misleading, there is only one type step here.

* |u_l| is described in the text before it's actually used in the equations

* what are the blue areas on Fig 1?

* what is the dataset in Table 1?

* https://arxiv.org/abs/2102.04159 and https://arxiv.org/abs/2007.05785 should be included in Table 4

","A potentially promising method, but some aspects are unclear.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666600396684,,0L8tuglXJaW,ICLR.cc/2023/Conference/Paper1297/-/Official_Review,,[],2,,['everyone'],0L8tuglXJaW,['ICLR.cc/2023/Conference/Paper1297/Reviewer_qbCE'],1666600396684,,1668775651055,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1297/Reviewer_qbCE']","Yes, the peer review suggests that the authors refer to specific literature that may not be already discussed in the original paper. The reviewer mentions two specific arXiv papers (https://arxiv.org/abs/2102.04159 and https://arxiv.org/abs/2007.05785) that should be included in Table 4, implying that they are relevant to the topic and may not be already cited in the original paper. Additionally, the reviewer mentions the work of McCulloch and Pitts (1943), which may also not be already discussed in the original paper.",1,"2021, 2020, 1943",Deep Residual Learning in Spiking Neural Networks
s54Lne8iaJX,"The authors propose a new method to train spiking neural networks (SNNs) with only one timestep. The method is based on a new regularization term (""Hoyer""), and a new threshold adjustment method (""Hoyer spike"", Eq 4).

","STRENGTHS:

The accuracy they reach with only one timestep is impressive, better than the previous state-of-the-art.

WEAKNESSES:

The theory is unclear:

* ""we estimate the value of the Hoyer extremum as He(ul) =""
I think what follows is not the value of H but the value of the element of u_l at the extremum.

* a SNN with one timestep is actually a somewhat degenerated case because there is no temporal integration. It boils down to a vanilla feed-forward artificial neural network, with Heaviside as the activation function. This also corresponds to the first neuron model proposed by McCulloch and Pitts and 1943, also known as threshold gates. This should be discussed.

* according to Table 5 what matters is the Hoyer spike, not the Hoyer regularization. This raises a question: could Hoyer spike be used alone, without the Hoyer regularization?

* does the approach extend to multistep SNNs?

","* Will the authors share their code?

* Eq 1 is misleading, there is only one type step here.

* |u_l| is described in the text before it's actually used in the equations

* what are the blue areas on Fig 1?

* what is the dataset in Table 1?

* https://arxiv.org/abs/2102.04159 and https://arxiv.org/abs/2007.05785 should be included in Table 4

","A potentially promising method, but some aspects are unclear.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666600396684,,0L8tuglXJaW,ICLR.cc/2023/Conference/Paper1297/-/Official_Review,,[],2,,['everyone'],0L8tuglXJaW,['ICLR.cc/2023/Conference/Paper1297/Reviewer_qbCE'],1666600396684,,1668775651055,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1297/Reviewer_qbCE']","Yes, the peer review suggests that the authors refer to specific literature that may not be already discussed in the original paper. The reviewer mentions two specific arXiv papers (https://arxiv.org/abs/2102.04159 and https://arxiv.org/abs/2007.05785) that should be included in Table 4, implying that they are relevant to the topic and may not be already cited in the original paper. Additionally, the reviewer mentions the work of McCulloch and Pitts (1943), which may also not be already discussed in the original paper.",1,"2021, 2020, 1943",Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks
s54Lne8iaJX,"The authors propose a new method to train spiking neural networks (SNNs) with only one timestep. The method is based on a new regularization term (""Hoyer""), and a new threshold adjustment method (""Hoyer spike"", Eq 4).

","STRENGTHS:

The accuracy they reach with only one timestep is impressive, better than the previous state-of-the-art.

WEAKNESSES:

The theory is unclear:

* ""we estimate the value of the Hoyer extremum as He(ul) =""
I think what follows is not the value of H but the value of the element of u_l at the extremum.

* a SNN with one timestep is actually a somewhat degenerated case because there is no temporal integration. It boils down to a vanilla feed-forward artificial neural network, with Heaviside as the activation function. This also corresponds to the first neuron model proposed by McCulloch and Pitts and 1943, also known as threshold gates. This should be discussed.

* according to Table 5 what matters is the Hoyer spike, not the Hoyer regularization. This raises a question: could Hoyer spike be used alone, without the Hoyer regularization?

* does the approach extend to multistep SNNs?

","* Will the authors share their code?

* Eq 1 is misleading, there is only one type step here.

* |u_l| is described in the text before it's actually used in the equations

* what are the blue areas on Fig 1?

* what is the dataset in Table 1?

* https://arxiv.org/abs/2102.04159 and https://arxiv.org/abs/2007.05785 should be included in Table 4

","A potentially promising method, but some aspects are unclear.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666600396684,,0L8tuglXJaW,ICLR.cc/2023/Conference/Paper1297/-/Official_Review,,[],2,,['everyone'],0L8tuglXJaW,['ICLR.cc/2023/Conference/Paper1297/Reviewer_qbCE'],1666600396684,,1668775651055,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1297/Reviewer_qbCE']","Yes, the peer review suggests that the authors refer to specific literature that may not be already discussed in the original paper. The reviewer mentions two specific arXiv papers (https://arxiv.org/abs/2102.04159 and https://arxiv.org/abs/2007.05785) that should be included in Table 4, implying that they are relevant to the topic and may not be already cited in the original paper. Additionally, the reviewer mentions the work of McCulloch and Pitts (1943), which may also not be already discussed in the original paper.",1,"2021, 2020, 1943",A logical calculus of the ideas immanent in nervous activity
RU94kcAg0A8,"This manuscript proposes Corrupted Image Modeling, which is a self-supervised learning framework for generic architectures, e.g., CNN and ViT. Specifically, the proposed method reconstructs or predicts the original image from a generated image whose are partially reconstructed from [MASK] tokens. The extensive experiments showed the effectiveness of the proposed method on various vision tasks, such as image classification and semantic segmentation.","Strengths
- The writing is clear and easy to understand.
- The proposed method is a generic self-supervised learning scheme in a Masked Image Modeling manner, which can be applied to CNN and ViT.
- Extensive experimental results demonstrate the effectiveness of the proposed method.

Weaknesses
- The proposed method has a weakness in that a pre-trained model is necessarily required in any form (e.g., Dall-E models) as a part of the generator.
- In the case of the ViT model, I am not sure if the proposed method is a better way than the existing BeiT or MAE-style MIM approaches. Also, related explanations and comparisons are lacking. For example, the comparison with MAE (83.1% in Table 1) will be fair when the MAE generator is used instead of external Dall-E models (82.6% in Table 10). In this case, MAE shows better performance than the proposed method.
- The explanation of the need for a generator is somewhat lacking. Is it not possible to replace this by applying strong augmentations (e.g., cutout, ColorJitter, etc.) only to random patches?
- I think the main novelty comes from that made MIM pre-training in CNN. Is there any unique advantage of MIM pre-training in CNN?
- Missing comparison; BeiT and MAE have also known for their superior performances on ADE20K semantic segmentation. However, the authors only report the ADE20K performances for CNNs. Could the proposed method achieve better performances on the ADE20K than BeiT or MAE?","Clarity
- The writing is clear. 

Quality
- It requires more explanations and comparisons to emphasize the strengths compared to existing methodologies to improve, as I mentioned above.

Novelty
- The technical contribution is incremental, but the introduced approach is worth sharing. 

Reproducibility
- The code is provided, but some components are missing, e.g., model architecture for the proposed method (""CNNForMaskedImageModeling"")","Overall, I think the paper is worth sharing, although I have several concerns, as mentioned above. I hope the authors will address the issues and questions raised.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666666058237,,09hVcSDkea,ICLR.cc/2023/Conference/Paper2355/-/Official_Review,,[],1,,['everyone'],09hVcSDkea,['ICLR.cc/2023/Conference/Paper2355/Reviewer_wVPU'],1666666058237,,1666666058237,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2355/Reviewer_wVPU']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several existing methodologies and papers, such as BeiT, MAE, and Dall-E models, and suggests that the authors should provide more explanations and comparisons with these existing approaches to emphasize the strengths of their proposed method. Specifically, the reviewer mentions that the comparison with MAE would be fair if the MAE generator is used instead of external Dall-E models, and that the authors should report the ADE20K performances for ViT models, similar to BeiT and MAE, which implies that these comparisons are not already included in the original paper.",1,,BeiT 
RU94kcAg0A8,"This manuscript proposes Corrupted Image Modeling, which is a self-supervised learning framework for generic architectures, e.g., CNN and ViT. Specifically, the proposed method reconstructs or predicts the original image from a generated image whose are partially reconstructed from [MASK] tokens. The extensive experiments showed the effectiveness of the proposed method on various vision tasks, such as image classification and semantic segmentation.","Strengths
- The writing is clear and easy to understand.
- The proposed method is a generic self-supervised learning scheme in a Masked Image Modeling manner, which can be applied to CNN and ViT.
- Extensive experimental results demonstrate the effectiveness of the proposed method.

Weaknesses
- The proposed method has a weakness in that a pre-trained model is necessarily required in any form (e.g., Dall-E models) as a part of the generator.
- In the case of the ViT model, I am not sure if the proposed method is a better way than the existing BeiT or MAE-style MIM approaches. Also, related explanations and comparisons are lacking. For example, the comparison with MAE (83.1% in Table 1) will be fair when the MAE generator is used instead of external Dall-E models (82.6% in Table 10). In this case, MAE shows better performance than the proposed method.
- The explanation of the need for a generator is somewhat lacking. Is it not possible to replace this by applying strong augmentations (e.g., cutout, ColorJitter, etc.) only to random patches?
- I think the main novelty comes from that made MIM pre-training in CNN. Is there any unique advantage of MIM pre-training in CNN?
- Missing comparison; BeiT and MAE have also known for their superior performances on ADE20K semantic segmentation. However, the authors only report the ADE20K performances for CNNs. Could the proposed method achieve better performances on the ADE20K than BeiT or MAE?","Clarity
- The writing is clear. 

Quality
- It requires more explanations and comparisons to emphasize the strengths compared to existing methodologies to improve, as I mentioned above.

Novelty
- The technical contribution is incremental, but the introduced approach is worth sharing. 

Reproducibility
- The code is provided, but some components are missing, e.g., model architecture for the proposed method (""CNNForMaskedImageModeling"")","Overall, I think the paper is worth sharing, although I have several concerns, as mentioned above. I hope the authors will address the issues and questions raised.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666666058237,,09hVcSDkea,ICLR.cc/2023/Conference/Paper2355/-/Official_Review,,[],1,,['everyone'],09hVcSDkea,['ICLR.cc/2023/Conference/Paper2355/Reviewer_wVPU'],1666666058237,,1666666058237,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2355/Reviewer_wVPU']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several existing methodologies and papers, such as BeiT, MAE, and Dall-E models, and suggests that the authors should provide more explanations and comparisons with these existing approaches to emphasize the strengths of their proposed method. Specifically, the reviewer mentions that the comparison with MAE would be fair if the MAE generator is used instead of external Dall-E models, and that the authors should report the ADE20K performances for ViT models, similar to BeiT and MAE, which implies that these comparisons are not already included in the original paper.",1,,MAE 
RU94kcAg0A8,"This manuscript proposes Corrupted Image Modeling, which is a self-supervised learning framework for generic architectures, e.g., CNN and ViT. Specifically, the proposed method reconstructs or predicts the original image from a generated image whose are partially reconstructed from [MASK] tokens. The extensive experiments showed the effectiveness of the proposed method on various vision tasks, such as image classification and semantic segmentation.","Strengths
- The writing is clear and easy to understand.
- The proposed method is a generic self-supervised learning scheme in a Masked Image Modeling manner, which can be applied to CNN and ViT.
- Extensive experimental results demonstrate the effectiveness of the proposed method.

Weaknesses
- The proposed method has a weakness in that a pre-trained model is necessarily required in any form (e.g., Dall-E models) as a part of the generator.
- In the case of the ViT model, I am not sure if the proposed method is a better way than the existing BeiT or MAE-style MIM approaches. Also, related explanations and comparisons are lacking. For example, the comparison with MAE (83.1% in Table 1) will be fair when the MAE generator is used instead of external Dall-E models (82.6% in Table 10). In this case, MAE shows better performance than the proposed method.
- The explanation of the need for a generator is somewhat lacking. Is it not possible to replace this by applying strong augmentations (e.g., cutout, ColorJitter, etc.) only to random patches?
- I think the main novelty comes from that made MIM pre-training in CNN. Is there any unique advantage of MIM pre-training in CNN?
- Missing comparison; BeiT and MAE have also known for their superior performances on ADE20K semantic segmentation. However, the authors only report the ADE20K performances for CNNs. Could the proposed method achieve better performances on the ADE20K than BeiT or MAE?","Clarity
- The writing is clear. 

Quality
- It requires more explanations and comparisons to emphasize the strengths compared to existing methodologies to improve, as I mentioned above.

Novelty
- The technical contribution is incremental, but the introduced approach is worth sharing. 

Reproducibility
- The code is provided, but some components are missing, e.g., model architecture for the proposed method (""CNNForMaskedImageModeling"")","Overall, I think the paper is worth sharing, although I have several concerns, as mentioned above. I hope the authors will address the issues and questions raised.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666666058237,,09hVcSDkea,ICLR.cc/2023/Conference/Paper2355/-/Official_Review,,[],1,,['everyone'],09hVcSDkea,['ICLR.cc/2023/Conference/Paper2355/Reviewer_wVPU'],1666666058237,,1666666058237,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2355/Reviewer_wVPU']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several existing methodologies and papers, such as BeiT, MAE, and Dall-E models, and suggests that the authors should provide more explanations and comparisons with these existing approaches to emphasize the strengths of their proposed method. Specifically, the reviewer mentions that the comparison with MAE would be fair if the MAE generator is used instead of external Dall-E models, and that the authors should report the ADE20K performances for ViT models, similar to BeiT and MAE, which implies that these comparisons are not already included in the original paper.",1,,Dall-E
MkR2_iN4swk,"This paper studies class imbalance and proposes that we should model the imbalance based on the semantic scale per class instead of the number of samples per class. Following this, this paper conducts a series of empirical studies to verify this idea and proposes a new method to handle class imbalance. Empirical results demonstrate the effectiveness of the proposed method.","Strengths:
1. The problem of class imbalance is important for practical applications.
2. It is interesting and reasonable to rethink class imbalance beyond just the sample number per class.
3. The formulated semantic scale is new to me.

Weaknesses:
1. The designed DSB loss relies not only on the semantic scale but also the inter-class interference. Therefore, it is unclear how much performance gains are from the semantic scale and how much is from the inter-class interference. Since the main innovation is the semantic scale, it is better to show its empirical effectiveness.
2. In Sec 4.1, this paper argues that ""all methods that rebalance the loss function or adjust the sampling rate based on the sample number can be improved with the semantic scale, since both are natural measures and they are not model-dependent."" This argument is not clear enough. First of all, the evaluation of the semantic scale and the inter-class interference are based on model features, so they are model-dependent. Moreover, in my view, combining the proposed reweighting loss and existing class re-balancing is not that easy, since they are competing with each other in reversing imbalance to some degree. Therefore, the combination pipeline should be carefully designed and the hyper-parameters should be carefully tuned; otherwise, the model performance may not be improved and even become worse.
3. There are many re-weighting long-tailed approaches that do not rely on the number of samples, like Uncertainty-based margin learning [A], LOCE [B] and Domain balancing [C]. Please discuss some of them if you have not done so and also compare the proposed method with them to show superiority.
[A] Striking the right balance with uncertainty. In CVPR 2019.
[B] Exploring classification equilibrium in long-tailed object detection. In ICCV 2021.
[C] Domain balancing: Face recognition on long-tailed domains. In CVPR 2020.
4. Can the proposed semantic scale be used to guide re-sampling for long-tailed data? Whether is it better guidance than the sample number per class for class-balanced re-sampling?
5. The performance of BS [39] on ImageNet-LT is not that bad. Many recent studies have verified this. Please recheck your implementation in the experiment.
6. In Section 5.2, can you order the semantic scale of different classes, and group them into three groups? I wonder how many accuracies of different groups are improved. Whether are the classes with small semantic scales improved more?","- The paper is easy to follow. 
- The quality is overall good but there are some issues should be resolved. 
- The novelty is fair. 
- The reproducibility: the source code is expected to be released.","This paper is overall good, but there are several problems that need to be resolved. 

*******************
Post-rebuttal: Thanks for the response and modifications. The quality of this paper is further improved now. However,  I still think the proposed method is a mix of different ideas, considering that inter-class interference is not that important. Empirical analysis and suggestions regarding inter-class interference are good, but it is not that elegant when designing them into one method. Even so, I appreciate the efforts of the authors and increase my rate to 6.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1665993543615,,07tc5kKRIo,ICLR.cc/2023/Conference/Paper42/-/Official_Review,,[],1,,['everyone'],07tc5kKRIo,['ICLR.cc/2023/Conference/Paper42/Reviewer_bNtG'],1665993543615,,1668999105896,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper42/Reviewer_bNtG']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions three specific papers:

1. ""Striking the right balance with uncertainty"" (CVPR 2019)
2. ""Exploring classification equilibrium in long-tailed object detection"" (ICCV 2021)
3. ""Domain balancing: Face recognition on long-tailed domains"" (CVPR 2020)

The reviewer asks the authors to discuss these papers and compare the proposed method with them to show superiority, implying that they are not already included in the original paper.",1,"2019, 2021, 2020",Striking the right balance with uncertainty 
MkR2_iN4swk,"This paper studies class imbalance and proposes that we should model the imbalance based on the semantic scale per class instead of the number of samples per class. Following this, this paper conducts a series of empirical studies to verify this idea and proposes a new method to handle class imbalance. Empirical results demonstrate the effectiveness of the proposed method.","Strengths:
1. The problem of class imbalance is important for practical applications.
2. It is interesting and reasonable to rethink class imbalance beyond just the sample number per class.
3. The formulated semantic scale is new to me.

Weaknesses:
1. The designed DSB loss relies not only on the semantic scale but also the inter-class interference. Therefore, it is unclear how much performance gains are from the semantic scale and how much is from the inter-class interference. Since the main innovation is the semantic scale, it is better to show its empirical effectiveness.
2. In Sec 4.1, this paper argues that ""all methods that rebalance the loss function or adjust the sampling rate based on the sample number can be improved with the semantic scale, since both are natural measures and they are not model-dependent."" This argument is not clear enough. First of all, the evaluation of the semantic scale and the inter-class interference are based on model features, so they are model-dependent. Moreover, in my view, combining the proposed reweighting loss and existing class re-balancing is not that easy, since they are competing with each other in reversing imbalance to some degree. Therefore, the combination pipeline should be carefully designed and the hyper-parameters should be carefully tuned; otherwise, the model performance may not be improved and even become worse.
3. There are many re-weighting long-tailed approaches that do not rely on the number of samples, like Uncertainty-based margin learning [A], LOCE [B] and Domain balancing [C]. Please discuss some of them if you have not done so and also compare the proposed method with them to show superiority.
[A] Striking the right balance with uncertainty. In CVPR 2019.
[B] Exploring classification equilibrium in long-tailed object detection. In ICCV 2021.
[C] Domain balancing: Face recognition on long-tailed domains. In CVPR 2020.
4. Can the proposed semantic scale be used to guide re-sampling for long-tailed data? Whether is it better guidance than the sample number per class for class-balanced re-sampling?
5. The performance of BS [39] on ImageNet-LT is not that bad. Many recent studies have verified this. Please recheck your implementation in the experiment.
6. In Section 5.2, can you order the semantic scale of different classes, and group them into three groups? I wonder how many accuracies of different groups are improved. Whether are the classes with small semantic scales improved more?","- The paper is easy to follow. 
- The quality is overall good but there are some issues should be resolved. 
- The novelty is fair. 
- The reproducibility: the source code is expected to be released.","This paper is overall good, but there are several problems that need to be resolved. 

*******************
Post-rebuttal: Thanks for the response and modifications. The quality of this paper is further improved now. However,  I still think the proposed method is a mix of different ideas, considering that inter-class interference is not that important. Empirical analysis and suggestions regarding inter-class interference are good, but it is not that elegant when designing them into one method. Even so, I appreciate the efforts of the authors and increase my rate to 6.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1665993543615,,07tc5kKRIo,ICLR.cc/2023/Conference/Paper42/-/Official_Review,,[],1,,['everyone'],07tc5kKRIo,['ICLR.cc/2023/Conference/Paper42/Reviewer_bNtG'],1665993543615,,1668999105896,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper42/Reviewer_bNtG']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions three specific papers:

1. ""Striking the right balance with uncertainty"" (CVPR 2019)
2. ""Exploring classification equilibrium in long-tailed object detection"" (ICCV 2021)
3. ""Domain balancing: Face recognition on long-tailed domains"" (CVPR 2020)

The reviewer asks the authors to discuss these papers and compare the proposed method with them to show superiority, implying that they are not already included in the original paper.",1,"2019, 2021, 2020",Exploring classification equilibrium in long-tailed object detection 
MkR2_iN4swk,"This paper studies class imbalance and proposes that we should model the imbalance based on the semantic scale per class instead of the number of samples per class. Following this, this paper conducts a series of empirical studies to verify this idea and proposes a new method to handle class imbalance. Empirical results demonstrate the effectiveness of the proposed method.","Strengths:
1. The problem of class imbalance is important for practical applications.
2. It is interesting and reasonable to rethink class imbalance beyond just the sample number per class.
3. The formulated semantic scale is new to me.

Weaknesses:
1. The designed DSB loss relies not only on the semantic scale but also the inter-class interference. Therefore, it is unclear how much performance gains are from the semantic scale and how much is from the inter-class interference. Since the main innovation is the semantic scale, it is better to show its empirical effectiveness.
2. In Sec 4.1, this paper argues that ""all methods that rebalance the loss function or adjust the sampling rate based on the sample number can be improved with the semantic scale, since both are natural measures and they are not model-dependent."" This argument is not clear enough. First of all, the evaluation of the semantic scale and the inter-class interference are based on model features, so they are model-dependent. Moreover, in my view, combining the proposed reweighting loss and existing class re-balancing is not that easy, since they are competing with each other in reversing imbalance to some degree. Therefore, the combination pipeline should be carefully designed and the hyper-parameters should be carefully tuned; otherwise, the model performance may not be improved and even become worse.
3. There are many re-weighting long-tailed approaches that do not rely on the number of samples, like Uncertainty-based margin learning [A], LOCE [B] and Domain balancing [C]. Please discuss some of them if you have not done so and also compare the proposed method with them to show superiority.
[A] Striking the right balance with uncertainty. In CVPR 2019.
[B] Exploring classification equilibrium in long-tailed object detection. In ICCV 2021.
[C] Domain balancing: Face recognition on long-tailed domains. In CVPR 2020.
4. Can the proposed semantic scale be used to guide re-sampling for long-tailed data? Whether is it better guidance than the sample number per class for class-balanced re-sampling?
5. The performance of BS [39] on ImageNet-LT is not that bad. Many recent studies have verified this. Please recheck your implementation in the experiment.
6. In Section 5.2, can you order the semantic scale of different classes, and group them into three groups? I wonder how many accuracies of different groups are improved. Whether are the classes with small semantic scales improved more?","- The paper is easy to follow. 
- The quality is overall good but there are some issues should be resolved. 
- The novelty is fair. 
- The reproducibility: the source code is expected to be released.","This paper is overall good, but there are several problems that need to be resolved. 

*******************
Post-rebuttal: Thanks for the response and modifications. The quality of this paper is further improved now. However,  I still think the proposed method is a mix of different ideas, considering that inter-class interference is not that important. Empirical analysis and suggestions regarding inter-class interference are good, but it is not that elegant when designing them into one method. Even so, I appreciate the efforts of the authors and increase my rate to 6.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1665993543615,,07tc5kKRIo,ICLR.cc/2023/Conference/Paper42/-/Official_Review,,[],1,,['everyone'],07tc5kKRIo,['ICLR.cc/2023/Conference/Paper42/Reviewer_bNtG'],1665993543615,,1668999105896,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper42/Reviewer_bNtG']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions three specific papers:

1. ""Striking the right balance with uncertainty"" (CVPR 2019)
2. ""Exploring classification equilibrium in long-tailed object detection"" (ICCV 2021)
3. ""Domain balancing: Face recognition on long-tailed domains"" (CVPR 2020)

The reviewer asks the authors to discuss these papers and compare the proposed method with them to show superiority, implying that they are not already included in the original paper.",1,"2019, 2021, 2020",Domain balancing Face recognition on long-tailed domains
wu7teKUhB3,"This work studies long-tailed learning in the semantic level, which goes deeper beyond the conventional quantity bias. Specially, the authors explore to explain multiple phenomena by defining the semantic level imbalance and propose a measure by volume of manifold to reweight the learning objective, which has been demonstrate to improve the performance effectively.","In summary, there are some positive points regarding this work, summarized as follows,

(1) The topic is relatively novel and meaningful in the perspective of long-tailed learning, since the quantity bias is not the essential factor to decide the performance of imbalance learning. 

(2) The proposed measure for imbalance degree is effective, which is demonstrated by a range of validation on uniform datasets and long-tailed datasets. Specially, the trend of the curve is approximately similar to the final performance, which is heuristic to future extensions.

(3) The proposed semantic-scale-based balanced loss has been demonstrated useful to a range of previous methods to improve the performance in both class-imbalanced and class-balanced cases.

However, there are also some clear flaws in terms of the current submission, which needs to be further improved.

(1) It is confusing that the authors discuss both the marginal effect and semantic-scale imbalance. Especially, the marginal effect is not the unique characteristic of imbalance learning, and also exists in the absolutely balanced learning. When the samples are sufficient to make the model reach the optimal, more samples might not bring gains. 

(2) In terms of the semantic-scale-based imbalance learning, it is the approximately same topic with the recent work termed as the generalized long-tailed learning [1], which takes into the fine-grained pattern imbalancedness into account. The authors missed this work and did not discuss their difference as well as the corresponding comparison in the experimental part. Especially, they have released a benchmark in the github repository, which can be used in this work.

(3) The evaluation in the quantity-uniform datasets like CIFAR10 and CIFAR100 is ill-posed, since both the training and the test are IID. The improvement is not convincing to attribute to the imbalance learning, and conversely it is more possible to be explained as the effect of the hard negative mining by the re-weighting. Here, the authors need to carefully discuss the improvement in the IID instead of the Non-IID case about the training and the test. 

(4) There are some concerns about the experiments, since it seems that the authors do not demonstrate the model can achieve the better improvement on the basis of the state-of-the-art method like PaCO and LA, which have adjusted the model according to the quantity imbalance. Note that, RIDE is a multi-branch ensemble model, and it is meaningless to show the improvement on RIDE achieves the state-of-the-art, since PaCO can also integrate this mechanism to achieve better performance. It will be better to conduct some experiments to compared LA or PaCO and your method in the same backbone, which can show the gap of the improvement between the explicit quantity bias and the implicit semantic-scale bias.  Again, it is better to have some comparison in the benchmark of [1].

[1] Invariant Feature Learning for Generalized Long-Tailed Classification. ECCV 2022.","The topic is relative novel, since the work from the same topic is just conducted in the same year and in a few months ago.

[Posthoc Response] 
The code of this work can be open to promote the further explorations in this direction.","In summary, the current work has both pros and cons. I advise the authors to carefully consider the advices in the review and I can consider to raise the score if the concerns have been well solved.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666180248264,,07tc5kKRIo,ICLR.cc/2023/Conference/Paper42/-/Official_Review,,[],2,,['everyone'],07tc5kKRIo,['ICLR.cc/2023/Conference/Paper42/Reviewer_DeDZ'],1666180248264,,1669900008727,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper42/Reviewer_DeDZ']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions a specific work, ""Invariant Feature Learning for Generalized Long-Tailed Classification"" from ECCV 2022, which is referred to as [1] in the review. The reviewer notes that the authors missed this work and did not discuss their differences or compare their results in the experimental part. The reviewer also suggests that the authors use a benchmark from the GitHub repository associated with this work to compare their results. This suggests that the reviewer believes the authors should be aware of and engage with this literature in their work.",1,2022,Invariant Feature Learning for Generalized Long-Tailed Classification
yGItNI61cCd,"The paper proposes a new concept semantic scale, which conceptually captures the richness or diversity of the feature space, e.g., “Bird” features should be richer than “Swan” features. This quantity is quantified by the volume of the learned feature space, proportional to the determinant of feature covariance matrix. Using this new concept, the paper attempts to explain 1) the change of performance improvement w.r.t the number of training data and 2) implicit semantic bias in classification when classes are balanced. Finally, based on the discrepancy in semantic scales, the paper proposes a dynamically weighted loss function to show improvement on classification under balanced and imbalanced settings. ","Pros:

1, The concept of semantic scale is novel and well-motivated mathematically. 

2, The usage of sphere packing method to control the scale of semantic scale is a very neat idea.  

Cons: 

**1, Strong correlation with model performance weakens the contribution of the proposed loss function.**  The paper highlights the correlation between semantic scale and model performance. There exits method such as hard-class mining [1] and recall loss [2] that rely on the model performance to re-weight losses. This strong correlation makes one wonder why a loss function weighted by semantic scale would be better than one weighted by performance. 

**2, Adding inter-class interference seems ad-hoc and obscures the contribution of semantic scale.** In sec 3.4.1, an inter-class interference term is added to the semantic scale metric. This combination seems ad-hoc. Even though the performance of the combined metric seems to work well, it becomes less clear how much contribution the vanilla semantic scale has. 

**3, Marginal effects of semantic scale and model performance w.r.t the amount of data represent correlation not causation.**  In the sec.3.3, the authors observe a strong correlation between the marginal effects of semantic scale and model performance w.r.t the amount of data. This is an interesting correlation, however, which cannot be used to explain why classification gain is marginal when data are sufficient or why performance drops quickly when data are scarce, as claimed in the abstract. Because the observation only establishes a correlation not a causation between the two events. 

**4, Semantic scale is model-dependent.** Even though the authors claim that semantic scale is a natural measure that is model-dependent, it is calculated from the output (feature space) of a trained model and therefore should be model-dependent. The same dataset, with the same class statistics, will lead to different semantic scales if models are different. 

[1] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based object detectors with online hard example mining,” in Proceedings
of the IEEE conference on computer vision and pattern recognition, 2016, pp. 761–769.

[2] Tian, Junjiao, et al. ""Striking the Right Balance: Recall Loss for Semantic Segmentation."" 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022.","The paper is relatively clear in writing and has extensive experiments. The method is novel. Values of hyperparameters are given for reproducibility. However, no code is provided. ","The paper presents a novel idea to quantify implicit bias in classification. However, questions regarding claims and some less well explained components in the method make the overall message less convincing. ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666279783776,,07tc5kKRIo,ICLR.cc/2023/Conference/Paper42/-/Official_Review,,[],3,,['everyone'],07tc5kKRIo,['ICLR.cc/2023/Conference/Paper42/Reviewer_kLsB'],1666279783776,,1669993441475,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper42/Reviewer_kLsB']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. ""Training region-based object detectors with online hard example mining"" by Shrivastava et al. (2016)
2. ""Striking the Right Balance: Recall Loss for Semantic Segmentation"" by Tian et al. (2022)

These papers are cited as examples of methods that rely on model performance to re-weight losses, and are used to question the contribution of the proposed loss function in the paper. It appears that these papers are not already discussed in the original paper, as the reviewer introduces them as external references to support their critique.",1,"2016, 2022",Training region-based object detectors with online hard example mining 
yGItNI61cCd,"The paper proposes a new concept semantic scale, which conceptually captures the richness or diversity of the feature space, e.g., “Bird” features should be richer than “Swan” features. This quantity is quantified by the volume of the learned feature space, proportional to the determinant of feature covariance matrix. Using this new concept, the paper attempts to explain 1) the change of performance improvement w.r.t the number of training data and 2) implicit semantic bias in classification when classes are balanced. Finally, based on the discrepancy in semantic scales, the paper proposes a dynamically weighted loss function to show improvement on classification under balanced and imbalanced settings. ","Pros:

1, The concept of semantic scale is novel and well-motivated mathematically. 

2, The usage of sphere packing method to control the scale of semantic scale is a very neat idea.  

Cons: 

**1, Strong correlation with model performance weakens the contribution of the proposed loss function.**  The paper highlights the correlation between semantic scale and model performance. There exits method such as hard-class mining [1] and recall loss [2] that rely on the model performance to re-weight losses. This strong correlation makes one wonder why a loss function weighted by semantic scale would be better than one weighted by performance. 

**2, Adding inter-class interference seems ad-hoc and obscures the contribution of semantic scale.** In sec 3.4.1, an inter-class interference term is added to the semantic scale metric. This combination seems ad-hoc. Even though the performance of the combined metric seems to work well, it becomes less clear how much contribution the vanilla semantic scale has. 

**3, Marginal effects of semantic scale and model performance w.r.t the amount of data represent correlation not causation.**  In the sec.3.3, the authors observe a strong correlation between the marginal effects of semantic scale and model performance w.r.t the amount of data. This is an interesting correlation, however, which cannot be used to explain why classification gain is marginal when data are sufficient or why performance drops quickly when data are scarce, as claimed in the abstract. Because the observation only establishes a correlation not a causation between the two events. 

**4, Semantic scale is model-dependent.** Even though the authors claim that semantic scale is a natural measure that is model-dependent, it is calculated from the output (feature space) of a trained model and therefore should be model-dependent. The same dataset, with the same class statistics, will lead to different semantic scales if models are different. 

[1] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based object detectors with online hard example mining,” in Proceedings
of the IEEE conference on computer vision and pattern recognition, 2016, pp. 761–769.

[2] Tian, Junjiao, et al. ""Striking the Right Balance: Recall Loss for Semantic Segmentation."" 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022.","The paper is relatively clear in writing and has extensive experiments. The method is novel. Values of hyperparameters are given for reproducibility. However, no code is provided. ","The paper presents a novel idea to quantify implicit bias in classification. However, questions regarding claims and some less well explained components in the method make the overall message less convincing. ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666279783776,,07tc5kKRIo,ICLR.cc/2023/Conference/Paper42/-/Official_Review,,[],3,,['everyone'],07tc5kKRIo,['ICLR.cc/2023/Conference/Paper42/Reviewer_kLsB'],1666279783776,,1669993441475,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper42/Reviewer_kLsB']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. ""Training region-based object detectors with online hard example mining"" by Shrivastava et al. (2016)
2. ""Striking the Right Balance: Recall Loss for Semantic Segmentation"" by Tian et al. (2022)

These papers are cited as examples of methods that rely on model performance to re-weight losses, and are used to question the contribution of the proposed loss function in the paper. It appears that these papers are not already discussed in the original paper, as the reviewer introduces them as external references to support their critique.",1,"2016, 2022",Striking the Right Balance: Recall Loss for Semantic Segmentation
CZxGLSnlRp,"This paper presents a surrogate model that predicts the accuracy of an architecture.  The surrogate model has access to the architectures and their corresponding accuracies from the source search space. It is tasked with predicting the accuracies of architectures that are sampled from the target search space. It uses adversarial training for unsupervised domain adaptation. A Graph convolution network is used to extract the latent features from the architectures and serves as a generator. The discriminator in turn tries to differentiate the domain of the architecture.  A fundament layer F models the entire feature space.  A bridge layer B captures the domain specific features. The difference between the $F(x_{i})$ and $B(x_{i})$ gives the domain invariant features. The domain invariant features are fed to a predictor layer which in turn predicts the accuracy. 

The overall loss function is a combination of the domain loss from the discriminator, the bridge layer loss and the regression loss from the predict layer. The input to the surrogate model is an adjacency matrix. It learns an embedding for the operations of the architecture. ","**Strengths**:
1. The paper is able to learn a surrogate model on the target search space without any training data

**Questions**:
1. Could you evaluate the quality of the domain invariant features that are learnt? For example, use the source domain as nasbench-201 and target as Darts, source domain as nasbench-101 and target as nasbench-201 and compute the distance (KL-Divergence) or similarity (cosine similarity) between the learnt domain invariant features using source domain 1 and source domain 2.
2. Could you also include a table for the regression error and the domain classification  accuracy?
3. Is USPP DANN + bridge layer. If so, in the ablation study, when the bridge layer is removed, why is the kendall tau different from DANN? For table 5, are you using operation embeddings for all the baselines?
4. For table 5, rather than training the architectures for darts search space, why not use NDS-Darts [1] benchmark? Evaluate it on 100 architectures sampled from that search space.  Similarly, for table 2, if you could actually use NDS-Darts to find the best architecture and report the numbers for other algorithms also from the same benchmark, it would be a fair comparison.
5. Please include darts-pt [2] also in table 1 and 2.

[1] On Network Design Spaces for Visual Recognition, Radosavovic et al.
[2] Rethinking Architecture Selection in Differentiable NAS, Wang et al.","This paper is written very clearly. But the novelty of this paper is limited. The idea is heavily borrowed from the paper ""Gradually Vanishing Bridge for Adversarial Domain Adaptation"" by Cui et al. That paper uses a bridge layer for both generator and discriminator and use the domain invariant features to compute the classification loss and adversarial loss.

","Domain adaptation to learn a surrogate model for NAS is very useful for practical purposes. But the paper is mainly adapted from ""Gradually Vanishing Bridge for Adversarial Domain Adaptation"". Using operations for embeddings has also been done in the past. So the novelty is limited.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666388002498,,074e7Rojdj,ICLR.cc/2023/Conference/Paper3941/-/Official_Review,,[],1,,['everyone'],074e7Rojdj,['ICLR.cc/2023/Conference/Paper3941/Reviewer_4ZdJ'],1666388002498,,1666834574535,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3941/Reviewer_4ZdJ']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. ""On Network Design Spaces for Visual Recognition"" by Radosavovic et al. (referred to as [1] in the review), which introduces the NDS-Darts benchmark.
2. ""Rethinking Architecture Selection in Differentiable NAS"" by Wang et al. (referred to as [2] in the review), which introduces the darts-pt method.

The reviewer suggests using the NDS-Darts benchmark for evaluation and including darts-pt in tables 1 and 2, implying that these references are not already discussed in the original paper.",1,,On Network Design Spaces for Visual Recognition
CZxGLSnlRp,"This paper presents a surrogate model that predicts the accuracy of an architecture.  The surrogate model has access to the architectures and their corresponding accuracies from the source search space. It is tasked with predicting the accuracies of architectures that are sampled from the target search space. It uses adversarial training for unsupervised domain adaptation. A Graph convolution network is used to extract the latent features from the architectures and serves as a generator. The discriminator in turn tries to differentiate the domain of the architecture.  A fundament layer F models the entire feature space.  A bridge layer B captures the domain specific features. The difference between the $F(x_{i})$ and $B(x_{i})$ gives the domain invariant features. The domain invariant features are fed to a predictor layer which in turn predicts the accuracy. 

The overall loss function is a combination of the domain loss from the discriminator, the bridge layer loss and the regression loss from the predict layer. The input to the surrogate model is an adjacency matrix. It learns an embedding for the operations of the architecture. ","**Strengths**:
1. The paper is able to learn a surrogate model on the target search space without any training data

**Questions**:
1. Could you evaluate the quality of the domain invariant features that are learnt? For example, use the source domain as nasbench-201 and target as Darts, source domain as nasbench-101 and target as nasbench-201 and compute the distance (KL-Divergence) or similarity (cosine similarity) between the learnt domain invariant features using source domain 1 and source domain 2.
2. Could you also include a table for the regression error and the domain classification  accuracy?
3. Is USPP DANN + bridge layer. If so, in the ablation study, when the bridge layer is removed, why is the kendall tau different from DANN? For table 5, are you using operation embeddings for all the baselines?
4. For table 5, rather than training the architectures for darts search space, why not use NDS-Darts [1] benchmark? Evaluate it on 100 architectures sampled from that search space.  Similarly, for table 2, if you could actually use NDS-Darts to find the best architecture and report the numbers for other algorithms also from the same benchmark, it would be a fair comparison.
5. Please include darts-pt [2] also in table 1 and 2.

[1] On Network Design Spaces for Visual Recognition, Radosavovic et al.
[2] Rethinking Architecture Selection in Differentiable NAS, Wang et al.","This paper is written very clearly. But the novelty of this paper is limited. The idea is heavily borrowed from the paper ""Gradually Vanishing Bridge for Adversarial Domain Adaptation"" by Cui et al. That paper uses a bridge layer for both generator and discriminator and use the domain invariant features to compute the classification loss and adversarial loss.

","Domain adaptation to learn a surrogate model for NAS is very useful for practical purposes. But the paper is mainly adapted from ""Gradually Vanishing Bridge for Adversarial Domain Adaptation"". Using operations for embeddings has also been done in the past. So the novelty is limited.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666388002498,,074e7Rojdj,ICLR.cc/2023/Conference/Paper3941/-/Official_Review,,[],1,,['everyone'],074e7Rojdj,['ICLR.cc/2023/Conference/Paper3941/Reviewer_4ZdJ'],1666388002498,,1666834574535,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3941/Reviewer_4ZdJ']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers: 

1. ""On Network Design Spaces for Visual Recognition"" by Radosavovic et al. (referred to as [1] in the review), which introduces the NDS-Darts benchmark.
2. ""Rethinking Architecture Selection in Differentiable NAS"" by Wang et al. (referred to as [2] in the review), which introduces the darts-pt method.

The reviewer suggests using the NDS-Darts benchmark for evaluation and including darts-pt in tables 1 and 2, implying that these references are not already discussed in the original paper.",1,,Rethinking Architecture Selection in Differentiable NAS
pGn88oRvyta,"This paper proposes a method for learning human locomotion policies from human motion data using an off-the-shelf differentiable physics simulator (DPS). Specifically, the method uses the Brax simulator (Freeman et al., 2021) to enable gradient computation of the forward dynamics. A simple joint and angle-based motion matching loss is used to supervise the training. To further improve the stability of training, the paper proposes a technique called demonstration replay, which resets the rollout state to the reference state when it deviates too much. Experiments on human locomotions and acrobatics show the method converges faster than RL-based methods while achieving similar motion quality.","**Strength:**

- The method achieves faster convergence by using a differentiable physics simulator compared to RL-based approaches.
- The paper is generally well-written and easy to follow.

**Weakness:**

- The novelty of the paper is limited. The idea of using a differentiable physics simulator to model human motions has been explored in prior work [1, 2] and in more complex settings, human pose estimation, navigation, path following, etc. There is no new method proposed in this paper besides using a differentiable physics simulator. The demonstration replay seems like an incremental technical contribution, and its benefit is not very significant given the mean pose error stays the same thus traditional approaches perform similarly over the whole motion.
- From Table 1, we can see that DeepMimic actually outperforms the proposed method in at least 6 motions (Back-Flip, Dance, Run, etc.) where in some cases DeepMimic actually beats the method a lot (0.038 vs 0.070 for Dance), so this method overall has only slightly better performance than an RL-based approach like DeepMimic. Even though the proposed method learns faster due to the use of simulation, achieving better motion quality consistently in the end is more important for motion imitation.
- From the visualization on the website, the “jog” motion produced by this method seems to be rather unnatural with the character limping, this does not happen for DeepMimic or AMP. There is also some foot sliding in the “roll” motion. Are the artifacts caused by the use of a differentiable physics simulator, which might not enforce the contact constraints as well?

[1] Gärtner, Erik, et al. ""Differentiable Dynamics for Articulated 3d Human Motion Reconstruction."" CVPR. 2022.

[2] Won, Jungdam, Deepak Gopinath, and Jessica Hodgins. ""Physics-based character controllers using conditional VAEs."" SIGGRAPH 2022.","- Clarity & Quality: The paper is generally clear and easy to follow. The accompanying visualization through the interactive website is very helpful.
- Novelty: As I said above, the paper lacks novelty since prior work [1, 2] has already explored applying a differentiable physics simulator to model human motions in much more complex settings. Technical innovation is also incremental.
- Reproducibility: I believe the paper can be reproduced relatively easily for people familiar with motion and control.","The paper shows improved efficiency of motion imitation with a differentiable physics simulator. However, the motion quality improvement is not huge, and the paper didn’t enable learning new motions that RL-based methods can’t. A main concern is also the paper’s lack of novelty. Therefore, I lean towards reject at this point.

--- update ---
After the rebuttal, I'm slightly more positive about this paper. I don't mind seeing the paper accepted. I still think the motion quality needs to be improved (e.g., obvious foot sliding). It is possible the lower quality is due to DPS's approximation of contact to enable differentiability, and it could be an inherent limitation of using DPS.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666410884274,,06mk-epSwZ,ICLR.cc/2023/Conference/Paper582/-/Official_Review,,[],1,,['everyone'],06mk-epSwZ,['ICLR.cc/2023/Conference/Paper582/Reviewer_8KfC'],1666410884274,,1668966586921,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper582/Reviewer_8KfC']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Gärtner, Erik, et al. ""Differentiable Dynamics for Articulated 3d Human Motion Reconstruction."" CVPR. 2022.
2. Won, Jungdam, Deepak Gopinath, and Jessica Hodgins. ""Physics-based character controllers using conditional VAEs."" SIGGRAPH 2022.

The reviewer implies that these papers have already explored the idea of using a differentiable physics simulator to model human motions, which is the main contribution of the paper being reviewed. The reviewer suggests that the paper lacks novelty because it does not significantly build upon or improve upon the work presented in these prior papers.",1,"2022, 2022",Differentiable Dynamics for Articulated 3d Human Motion Reconstruction 
pGn88oRvyta,"This paper proposes a method for learning human locomotion policies from human motion data using an off-the-shelf differentiable physics simulator (DPS). Specifically, the method uses the Brax simulator (Freeman et al., 2021) to enable gradient computation of the forward dynamics. A simple joint and angle-based motion matching loss is used to supervise the training. To further improve the stability of training, the paper proposes a technique called demonstration replay, which resets the rollout state to the reference state when it deviates too much. Experiments on human locomotions and acrobatics show the method converges faster than RL-based methods while achieving similar motion quality.","**Strength:**

- The method achieves faster convergence by using a differentiable physics simulator compared to RL-based approaches.
- The paper is generally well-written and easy to follow.

**Weakness:**

- The novelty of the paper is limited. The idea of using a differentiable physics simulator to model human motions has been explored in prior work [1, 2] and in more complex settings, human pose estimation, navigation, path following, etc. There is no new method proposed in this paper besides using a differentiable physics simulator. The demonstration replay seems like an incremental technical contribution, and its benefit is not very significant given the mean pose error stays the same thus traditional approaches perform similarly over the whole motion.
- From Table 1, we can see that DeepMimic actually outperforms the proposed method in at least 6 motions (Back-Flip, Dance, Run, etc.) where in some cases DeepMimic actually beats the method a lot (0.038 vs 0.070 for Dance), so this method overall has only slightly better performance than an RL-based approach like DeepMimic. Even though the proposed method learns faster due to the use of simulation, achieving better motion quality consistently in the end is more important for motion imitation.
- From the visualization on the website, the “jog” motion produced by this method seems to be rather unnatural with the character limping, this does not happen for DeepMimic or AMP. There is also some foot sliding in the “roll” motion. Are the artifacts caused by the use of a differentiable physics simulator, which might not enforce the contact constraints as well?

[1] Gärtner, Erik, et al. ""Differentiable Dynamics for Articulated 3d Human Motion Reconstruction."" CVPR. 2022.

[2] Won, Jungdam, Deepak Gopinath, and Jessica Hodgins. ""Physics-based character controllers using conditional VAEs."" SIGGRAPH 2022.","- Clarity & Quality: The paper is generally clear and easy to follow. The accompanying visualization through the interactive website is very helpful.
- Novelty: As I said above, the paper lacks novelty since prior work [1, 2] has already explored applying a differentiable physics simulator to model human motions in much more complex settings. Technical innovation is also incremental.
- Reproducibility: I believe the paper can be reproduced relatively easily for people familiar with motion and control.","The paper shows improved efficiency of motion imitation with a differentiable physics simulator. However, the motion quality improvement is not huge, and the paper didn’t enable learning new motions that RL-based methods can’t. A main concern is also the paper’s lack of novelty. Therefore, I lean towards reject at this point.

--- update ---
After the rebuttal, I'm slightly more positive about this paper. I don't mind seeing the paper accepted. I still think the motion quality needs to be improved (e.g., obvious foot sliding). It is possible the lower quality is due to DPS's approximation of contact to enable differentiability, and it could be an inherent limitation of using DPS.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666410884274,,06mk-epSwZ,ICLR.cc/2023/Conference/Paper582/-/Official_Review,,[],1,,['everyone'],06mk-epSwZ,['ICLR.cc/2023/Conference/Paper582/Reviewer_8KfC'],1666410884274,,1668966586921,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper582/Reviewer_8KfC']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Gärtner, Erik, et al. ""Differentiable Dynamics for Articulated 3d Human Motion Reconstruction."" CVPR. 2022.
2. Won, Jungdam, Deepak Gopinath, and Jessica Hodgins. ""Physics-based character controllers using conditional VAEs."" SIGGRAPH 2022.

The reviewer implies that these papers have already explored the idea of using a differentiable physics simulator to model human motions, which is the main contribution of the paper being reviewed. The reviewer suggests that the paper lacks novelty because it does not significantly build upon or improve upon the work presented in these prior papers.",1,"2022, 2022",Physics-based character controllers using conditional VAEs
T7_SxU4mPq3,"A differentiable physics simulator (DPS, Brax in this case), is used to learn an RL policy that
imitates a given reference motion, as done in DeepMimic. The contributions include (a) better sample
efficiency, and therefore faster learning, as compared to DeepMimic; (b) the use of ""demonstration
replay"" when the motion deviates too far from the reference motions.  This is a type of reset that
is critical to provide stable gradients and to avoid local minima.
","Strengths: 
- speeds up learning for an important class of problem (physics-based motion imitation)
- multiple useful practical insights on window length and resets, i.e., (demonstration replay) that are needed
  to get DPS-based methods working with complex human motions. This is the first time that I have
  seen Brax successfully used for this kind of scenario, although [Xu et al 2022] demonstrate results
  of similar complexity with a simulator of their own.

Weaknesses:
- mentions of closely related work, but with no deeper discussion on comparing the approaches, 
  or direct experimental comparisons 
  [Fussell et al 2021, Xu et al 2022, Mora et al 2021]
  as well as ""A Scalable Approach to Control Diverse Behaviors for Physically Simulated Characters""
  There are differences of the current work with respect to all of these, but much of the value of the paper
  lies in discussing these differences. I.e., Xu et al 2022 come up with a different solution to the
  basic challenges, i.e., exploding gradients, local minima.
","The paper is reasonable well written. Some minor english fixes need to be made. 
The code is available on GitHub.  Further details might nevertheless be useful, so that readers don't need to go to the code to understand particular hyperparameters, ","I see the value of this paper in terms of the pragmatic details that are needed to overcome the basic issues of exploiting differentiable simulators to directly learn policies. Getting these details right is important, as we also see from [Xu et al 2022].  However, the discussion could really be improved. 

The paper claims that DeepMimic rewards are heavily engineered. I believe that the 
DeepMimic imitation reward is quite similar to the loss function used here. 

DeepMimic introduces both early termination (ET) and reference state initialization (RSI) 
precisely to escape local minima of the type described here.  The idea of using a deviation threshold
to dynamically reset back to the trajectory could also be seen as a combination of ET and RSI.

Section 2, first para, missing bibtex reference e.g., ""?]""

The existing work that makes use of differentiable simulators is mentioned, but then more-or-less ignored. 
I'm not clear why. E.g.,   [Fussell et al 2021, Xu et al 2022, Mora et al 2021].
The following work also takes advantage of transfer learning when learning to imitate a wide range of motions,
and to therefore to immediately generalize to a large percentage of new motions:
""A Scalable Approach to Control Diverse Behaviors for Physically Simulated Characters""
In the long term, this may be more useful than speeding up the single-motion case.

Fig 4 shows 3 runs for some methods and only one run for DiffMimic.
DiffMimic presumably not deterministic, i.e., it still uses a stochastic action policy, 
to help enable encourage robustness. Or is that incorrect?

text below Figure 4:
""Dynamic Time Warping is applied to sync the simulated motion and the reference motion following the convention""
Which convention?  I don't believe that DeepMimic uses DTW.

Table 2:
The results might be better stated as a fraction of the DeepMimic time.

How does the robustness of the learned policies compare to those learned with DeepMimic?
Is the exploration noise comparable?
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666578252026,,06mk-epSwZ,ICLR.cc/2023/Conference/Paper582/-/Official_Review,,[],3,,['everyone'],06mk-epSwZ,['ICLR.cc/2023/Conference/Paper582/Reviewer_s4rT'],1666578252026,,1670102422620,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper582/Reviewer_s4rT']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers, including ""A Scalable Approach to Control Diverse Behaviors for Physically Simulated Characters"", which is not mentioned in the original paper. The reviewer also mentions that the paper could benefit from a deeper discussion on comparing the approaches and direct experimental comparisons with other related works, such as [Fussell et al 2021, Xu et al 2022, Mora et al 2021]. This implies that the reviewer thinks the authors should engage more with the existing literature and provide a more thorough comparison with other relevant works.",1,"2021, 2022, 2021",A Scalable Approach to Control Diverse Behaviors for Physically Simulated Characters 
_Eiorqgb5N,"This paper focuses on the NPG in softmax policy with linear function approximation. By proposing a new updating strategy, in which the stepsize increases at a designed rate, this paper is able to show that the NPG enjoys a per-iteration geometry convergence rate w.r.t a factor $1-1/\nu_\mu$. Since previous works of NPG in the similar setting mostly focus on establishing sublinear convergence rate, the result established in this paper is interesting.","Strength:
(1) This paper is well-written and easy to follow
(2) The technique proof is solid and clear
(3) The result established in this paper is different from all previous works in the same setting, which is interesting.

Weakness:
(1) The major technique contribution of this paper is in exploring how the incremental increasing stepsize with the special RL optimization nature can provide a linear convergence rate, which is interesting but not strong enough.
(2) The linear convergence rate in this paper may not be very useful in most of the case. The major reason is that the contraction factor $(1-1/\nu_\mu)$ is significantly larger than \gamma (or very close to 1 as $\nu_\mu$ is usually very large. With a given sample complexity $\epsilon$, if $\nu_\mu$ scale as $\mathcal{O}(\exp(1/\epsilon))$ then the sample complexity in this paper may actually looser than standard NPG complexity results established in many previous works. Thus, the linear convergence rate established in this paper only improves the real sample complexity of NPG within a very limited scenario. ","The author may need to discuss in what scenario their established result really outperforms the pervious SOTA results of NPG. After doing that, readers are able to justify the real contribution of this paper.","Overall this paper provide a very interesting result of NPG. However, the result is more like provide a different formulation of the sample complexity result of NPG instead of establishing a more tighter bound (for detail see my comment in weakness in previous section).


============ post rebuttal ============


After reviewing author's response, I will raise my score to 6. The contribution in this paper is not strong enough given the state-of-the-art. The linear convergence rate of PG without entropy regularization has been explored in previous work. For example, in [1], when adopting l2 regularization in their mirror descent update, the result in [1] naturally implies a linear convergence rate for PG algorithm. However, since this paper explore a new technique for analyzing the PG type algorithm, it might be interesting to the community.

[1] Lan, G. (2022). Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. Mathematical programming, 1-48.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1666489396343,,03sXXjL1um3,ICLR.cc/2023/Conference/Paper2896/-/Official_Review,,[],1,,['everyone'],03sXXjL1um3,['ICLR.cc/2023/Conference/Paper2896/Reviewer_vAhQ'],1666489396343,,1671004473607,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2896/Reviewer_vAhQ']","Yes, this peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer mentions a paper by Lan (2022) titled ""Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes"" and notes that the result in this paper implies a linear convergence rate for the PG algorithm, which is relevant to the authors' work. The reviewer implies that the authors may not be aware of this work, as they mention it as an example of previous work that has explored the linear convergence rate of PG without entropy regularization.",1,2022,Policy mirror descent for reinforcement learning Linear convergence new sampling complexity and generalized problem classes
G7RVDuNwzA3,"The paper considers the problem of learning fair representations containing little or no information about the protected variable. In the absence of prior knowledge about downstream tasks, it requires the learned representations to be both fair and discriminative enough, which turns out to be a trade-off and may incur instability when applying adversarial training.

The main contribution of this paper is to propose a new algorithm called ALFR-DS that retrofits the existing adversarial learning framework ALFR [1] with dampening and stacking. Specifically, the algorithm uses an adaptive parameter (dampening) to stabilize the gradients of the actor and the adversary when the adversary is too weak and combines it with stacked encoders (stacking) [2] to improve censoring. Experimental results on MNIST and the AIF360 benchmark show that ALFR-DS achieves better censoring and reconstruction performance than several baselines.

[1] Edwards, Harrison, and Amos Storkey. ""Censoring representations with an adversary."" arXiv preprint arXiv:1511.05897 (2015).

[2] Kenfack, Patrik Joslin, et al. ""Adversarial Stacked Auto-Encoders for Fair Representation Learning."" arXiv preprint arXiv:2107.12826 (2021).","Pros:

1. The paper considers an important problem of how to learn fair representations without knowing downstream tasks.

2. The paper is well-structured.

Cons:

1. The novelty of this work is limited. Compared with ALFR, the main novelty brought by ALFR-DS is the use of dampening together with some other tricks discussed in Section 4.3 to tame the instability of adversarial training. Given that ALFR was proposed 7 years ago and a variety of studies has explored similar issues in a more principled way [1, 2, 3], the contributions made in terms of methodology are inadequate.

2. The proposed dampening mechanism needs more support. In its current version, the paper does not provide sufficient theoretical grounding nor empirical analysis to verify that dampening does prevent instability. The only effort made is plotting the curve of the reconstruction error (Figure 1), which seems too coarse-grained to prove the effectiveness of dampening.

3. The writing is not clear enough. In Section 3 the problem formulation is entangled with the methodology. It is also hard to find a proper mathematical definition of “good” fair representations.

4. I have some questions regarding important details that are not well specified. For example, it is unclear how to select threshold $T$ and what value is used exactly for experiments. It also seems inexplicit why to exclude results with BA less than 60% for the AIF360 Benchmark.

[1] Roth, Kevin, et al. ""Stabilizing training of generative adversarial networks through regularization."" Advances in neural information processing systems 30 (2017).

[2] Yadav, Abhay, et al. ""Stabilizing Adversarial Nets with Prediction Methods."" International Conference on Learning Representations. 2018.

[3] Jenni, Simon, and Paolo Favaro. ""On stabilizing generative adversarial training with noise."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.","The clarity and originality of this paper are fair, but the quality is relatively low. Please see my comments in Strength And Weaknesses.","Overall, I think this paper does not present significant contributions and would recommend rejection. Please see my comments in Strength And Weaknesses.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1666617326945,,-z911HH4RFv,ICLR.cc/2023/Conference/Paper1236/-/Official_Review,,[],1,,['everyone'],-z911HH4RFv,['ICLR.cc/2023/Conference/Paper1236/Reviewer_kwp8'],1666617326945,,1666617326945,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1236/Reviewer_kwp8']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several studies ([1], [2], [3] in the ""Cons"" section) that have explored similar issues in a more principled way, which are not mentioned in the original paper as references [1], [2] in the summary of the paper. These additional references are:

[1] Roth, Kevin, et al. ""Stabilizing training of generative adversarial networks through regularization."" Advances in neural information processing systems 30 (2017).
[2] Yadav, Abhay, et al. ""Stabilizing Adversarial Nets with Prediction Methods."" International Conference on Learning Representations. 2018.
[3] Jenni, Simon, and Paolo Favaro. ""On stabilizing generative adversarial training with noise."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

The reviewer implies that the authors should be aware of these studies and potentially incorporate their ideas or methodologies to improve the paper.",1,"2017, 2018, 2019",Stabilizing training of generative adversarial networks through regularization
G7RVDuNwzA3,"The paper considers the problem of learning fair representations containing little or no information about the protected variable. In the absence of prior knowledge about downstream tasks, it requires the learned representations to be both fair and discriminative enough, which turns out to be a trade-off and may incur instability when applying adversarial training.

The main contribution of this paper is to propose a new algorithm called ALFR-DS that retrofits the existing adversarial learning framework ALFR [1] with dampening and stacking. Specifically, the algorithm uses an adaptive parameter (dampening) to stabilize the gradients of the actor and the adversary when the adversary is too weak and combines it with stacked encoders (stacking) [2] to improve censoring. Experimental results on MNIST and the AIF360 benchmark show that ALFR-DS achieves better censoring and reconstruction performance than several baselines.

[1] Edwards, Harrison, and Amos Storkey. ""Censoring representations with an adversary."" arXiv preprint arXiv:1511.05897 (2015).

[2] Kenfack, Patrik Joslin, et al. ""Adversarial Stacked Auto-Encoders for Fair Representation Learning."" arXiv preprint arXiv:2107.12826 (2021).","Pros:

1. The paper considers an important problem of how to learn fair representations without knowing downstream tasks.

2. The paper is well-structured.

Cons:

1. The novelty of this work is limited. Compared with ALFR, the main novelty brought by ALFR-DS is the use of dampening together with some other tricks discussed in Section 4.3 to tame the instability of adversarial training. Given that ALFR was proposed 7 years ago and a variety of studies has explored similar issues in a more principled way [1, 2, 3], the contributions made in terms of methodology are inadequate.

2. The proposed dampening mechanism needs more support. In its current version, the paper does not provide sufficient theoretical grounding nor empirical analysis to verify that dampening does prevent instability. The only effort made is plotting the curve of the reconstruction error (Figure 1), which seems too coarse-grained to prove the effectiveness of dampening.

3. The writing is not clear enough. In Section 3 the problem formulation is entangled with the methodology. It is also hard to find a proper mathematical definition of “good” fair representations.

4. I have some questions regarding important details that are not well specified. For example, it is unclear how to select threshold $T$ and what value is used exactly for experiments. It also seems inexplicit why to exclude results with BA less than 60% for the AIF360 Benchmark.

[1] Roth, Kevin, et al. ""Stabilizing training of generative adversarial networks through regularization."" Advances in neural information processing systems 30 (2017).

[2] Yadav, Abhay, et al. ""Stabilizing Adversarial Nets with Prediction Methods."" International Conference on Learning Representations. 2018.

[3] Jenni, Simon, and Paolo Favaro. ""On stabilizing generative adversarial training with noise."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.","The clarity and originality of this paper are fair, but the quality is relatively low. Please see my comments in Strength And Weaknesses.","Overall, I think this paper does not present significant contributions and would recommend rejection. Please see my comments in Strength And Weaknesses.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1666617326945,,-z911HH4RFv,ICLR.cc/2023/Conference/Paper1236/-/Official_Review,,[],1,,['everyone'],-z911HH4RFv,['ICLR.cc/2023/Conference/Paper1236/Reviewer_kwp8'],1666617326945,,1666617326945,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1236/Reviewer_kwp8']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several studies ([1], [2], [3] in the ""Cons"" section) that have explored similar issues in a more principled way, which are not mentioned in the original paper as references [1], [2] in the summary of the paper. These additional references are:

[1] Roth, Kevin, et al. ""Stabilizing training of generative adversarial networks through regularization."" Advances in neural information processing systems 30 (2017).
[2] Yadav, Abhay, et al. ""Stabilizing Adversarial Nets with Prediction Methods."" International Conference on Learning Representations. 2018.
[3] Jenni, Simon, and Paolo Favaro. ""On stabilizing generative adversarial training with noise."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

The reviewer implies that the authors should be aware of these studies and potentially incorporate their ideas or methodologies to improve the paper.",1,"2017, 2018, 2019",Stabilizing Adversarial Nets with Prediction Methods
G7RVDuNwzA3,"The paper considers the problem of learning fair representations containing little or no information about the protected variable. In the absence of prior knowledge about downstream tasks, it requires the learned representations to be both fair and discriminative enough, which turns out to be a trade-off and may incur instability when applying adversarial training.

The main contribution of this paper is to propose a new algorithm called ALFR-DS that retrofits the existing adversarial learning framework ALFR [1] with dampening and stacking. Specifically, the algorithm uses an adaptive parameter (dampening) to stabilize the gradients of the actor and the adversary when the adversary is too weak and combines it with stacked encoders (stacking) [2] to improve censoring. Experimental results on MNIST and the AIF360 benchmark show that ALFR-DS achieves better censoring and reconstruction performance than several baselines.

[1] Edwards, Harrison, and Amos Storkey. ""Censoring representations with an adversary."" arXiv preprint arXiv:1511.05897 (2015).

[2] Kenfack, Patrik Joslin, et al. ""Adversarial Stacked Auto-Encoders for Fair Representation Learning."" arXiv preprint arXiv:2107.12826 (2021).","Pros:

1. The paper considers an important problem of how to learn fair representations without knowing downstream tasks.

2. The paper is well-structured.

Cons:

1. The novelty of this work is limited. Compared with ALFR, the main novelty brought by ALFR-DS is the use of dampening together with some other tricks discussed in Section 4.3 to tame the instability of adversarial training. Given that ALFR was proposed 7 years ago and a variety of studies has explored similar issues in a more principled way [1, 2, 3], the contributions made in terms of methodology are inadequate.

2. The proposed dampening mechanism needs more support. In its current version, the paper does not provide sufficient theoretical grounding nor empirical analysis to verify that dampening does prevent instability. The only effort made is plotting the curve of the reconstruction error (Figure 1), which seems too coarse-grained to prove the effectiveness of dampening.

3. The writing is not clear enough. In Section 3 the problem formulation is entangled with the methodology. It is also hard to find a proper mathematical definition of “good” fair representations.

4. I have some questions regarding important details that are not well specified. For example, it is unclear how to select threshold $T$ and what value is used exactly for experiments. It also seems inexplicit why to exclude results with BA less than 60% for the AIF360 Benchmark.

[1] Roth, Kevin, et al. ""Stabilizing training of generative adversarial networks through regularization."" Advances in neural information processing systems 30 (2017).

[2] Yadav, Abhay, et al. ""Stabilizing Adversarial Nets with Prediction Methods."" International Conference on Learning Representations. 2018.

[3] Jenni, Simon, and Paolo Favaro. ""On stabilizing generative adversarial training with noise."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.","The clarity and originality of this paper are fair, but the quality is relatively low. Please see my comments in Strength And Weaknesses.","Overall, I think this paper does not present significant contributions and would recommend rejection. Please see my comments in Strength And Weaknesses.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,1: The contributions are neither significant nor novel.,NO.,"3: reject, not good enough",1666617326945,,-z911HH4RFv,ICLR.cc/2023/Conference/Paper1236/-/Official_Review,,[],1,,['everyone'],-z911HH4RFv,['ICLR.cc/2023/Conference/Paper1236/Reviewer_kwp8'],1666617326945,,1666617326945,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1236/Reviewer_kwp8']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several studies ([1], [2], [3] in the ""Cons"" section) that have explored similar issues in a more principled way, which are not mentioned in the original paper as references [1], [2] in the summary of the paper. These additional references are:

[1] Roth, Kevin, et al. ""Stabilizing training of generative adversarial networks through regularization."" Advances in neural information processing systems 30 (2017).
[2] Yadav, Abhay, et al. ""Stabilizing Adversarial Nets with Prediction Methods."" International Conference on Learning Representations. 2018.
[3] Jenni, Simon, and Paolo Favaro. ""On stabilizing generative adversarial training with noise."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

The reviewer implies that the authors should be aware of these studies and potentially incorporate their ideas or methodologies to improve the paper.",1,"2017, 2018, 2019",On stabilizing generative adversarial training with noise
qBcZQlBIkDX,This paper proposes using Wasserstein gradient flow of KL dviergence  to construct the flow model. ,"First of all, the paper has a bad definition of notations. For example, the definition of V (is the potential of the equilibrium density) appears in the preliminaries. I think the paper should organize a problem-setting section to make all things clear.

However, in objective (8) there is a V (is the potential of the equilibrium density) included. How to get the V for general distribution? V includes fitting a score, which is very similar to yang's ICLR paper. 

The main concern is the realtionship to paper [1,2]. [1] consider the gradient flow of f-divergence and gives out a flow with velocity field f''*nabla (p/p_t). Thrid, how this paper differs from [3]. The only difference is the objective for training but not the gradient flow aiming to approximate. ( If the author claim their objective is better, I need to see evidence)
[1] https://arxiv.org/abs/2012.06094
[3] Johnson R, Zhang T. A framework of composite functional gradient methods for generative adversarial models[J]. IEEE transactions on pattern analysis and machine intelligence, 2019, 43(1): 17-32.

In terms of the literature review, this paper only discusses how they are different in terms of motivation (or how to derive the model), but have no discussion of how the derived model are different from each other. 

Missing experiments on CelebA. Missing baseline using diffusion model.

Missing reference:
[1] Liutkus A, Simsekli U, Majewski S, et al. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions[C]//International Conference on Machine Learning. PMLR, 2019: 4104-4113.
[2] Zhang L, Wang L. Monge-amp\ere flow for generative modeling[J]. arXiv preprint arXiv:1809.10188, 2018.",This paper did a bad job in literature review and notation. ,See above,5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667198976963,,-z7O7fk_Cs,ICLR.cc/2023/Conference/Paper3140/-/Official_Review,,[],3,,['everyone'],-z7O7fk_Cs,['ICLR.cc/2023/Conference/Paper3140/Reviewer_tb5k'],1667198976963,,1667198976963,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3140/Reviewer_tb5k']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several references, including:

* [1] Liutkus A, Simsekli U, Majewski S, et al. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions
* [2] Zhang L, Wang L. Monge-amp\ere flow for generative modeling

These references are explicitly mentioned as ""Missing reference"", indicating that they are not already discussed in the original paper. The reviewer suggests that the authors should include these references in their literature review.",1,,Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions 
qBcZQlBIkDX,This paper proposes using Wasserstein gradient flow of KL dviergence  to construct the flow model. ,"First of all, the paper has a bad definition of notations. For example, the definition of V (is the potential of the equilibrium density) appears in the preliminaries. I think the paper should organize a problem-setting section to make all things clear.

However, in objective (8) there is a V (is the potential of the equilibrium density) included. How to get the V for general distribution? V includes fitting a score, which is very similar to yang's ICLR paper. 

The main concern is the realtionship to paper [1,2]. [1] consider the gradient flow of f-divergence and gives out a flow with velocity field f''*nabla (p/p_t). Thrid, how this paper differs from [3]. The only difference is the objective for training but not the gradient flow aiming to approximate. ( If the author claim their objective is better, I need to see evidence)
[1] https://arxiv.org/abs/2012.06094
[3] Johnson R, Zhang T. A framework of composite functional gradient methods for generative adversarial models[J]. IEEE transactions on pattern analysis and machine intelligence, 2019, 43(1): 17-32.

In terms of the literature review, this paper only discusses how they are different in terms of motivation (or how to derive the model), but have no discussion of how the derived model are different from each other. 

Missing experiments on CelebA. Missing baseline using diffusion model.

Missing reference:
[1] Liutkus A, Simsekli U, Majewski S, et al. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions[C]//International Conference on Machine Learning. PMLR, 2019: 4104-4113.
[2] Zhang L, Wang L. Monge-amp\ere flow for generative modeling[J]. arXiv preprint arXiv:1809.10188, 2018.",This paper did a bad job in literature review and notation. ,See above,5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667198976963,,-z7O7fk_Cs,ICLR.cc/2023/Conference/Paper3140/-/Official_Review,,[],3,,['everyone'],-z7O7fk_Cs,['ICLR.cc/2023/Conference/Paper3140/Reviewer_tb5k'],1667198976963,,1667198976963,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3140/Reviewer_tb5k']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several references, including:

* [1] Liutkus A, Simsekli U, Majewski S, et al. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions
* [2] Zhang L, Wang L. Monge-amp\ere flow for generative modeling

These references are explicitly mentioned as ""Missing reference"", indicating that they are not already discussed in the original paper. The reviewer suggests that the authors should include these references in their literature review.",1,,Monge-ampere flow for generative modeling
Q8n6PPMqYS,"This current paper proposes a normalizing flow algorithm that implements the JKO scheme using neural ODE flow blocks. At each time interval, an optimization over a vector field parameterized as ResNet is done to obtain the velocity field that can be integrated to obtain samples at the next step. The normalizing flow is invertible and the objective functional of the JKO scheme is taken to be the KL divergence with respect to a standard Gaussian so that by inverting the flow we can generate the data distribution. The proposed method appears to be more efficient than the alternatives as demonstrated by the experiments.
","## Strengths:
* Optimizing the velocity field of the ODE for JKO instead of transport maps that are more common in the existing literature is novel.
* The idea of using one block for each JKO step is interesting and it seems to have resulted in efficiency.


## Weaknesses:
* I found the novelty of the proposed method somewhat limited. The only difference from [Alvarez-Meliset et al. 2021], [Mokrov et al. 2021] is that instead of parameterizing the pushforward map as ICNN (note in either work it's not necessary to use ICNN; arbitrary networks can also be used, following the same reasoning at Lemma A.1), the current work uses neural ODE. I'm not convinced by the superiority of using neural ODE over a pushforward map, which the current work does not compare against. In my opinion, a more severe problem is not addressed: namely at step $k$ all these algorithms need to push initial samples by $k$ steps to obtain samples for the current iteration, which scales overall quadratically in $k$.
* The section 2.2 is not new to my knowledge and references are missing. See a similar derivation in Theorem 3.1 of Liu et al. ""Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm"".
* I appreciate that two heuristic enhancements are proposed in Section 3.2. However, at first glance, these two enhancements seem contradictory. On the one hand, trajectory reparameterization wants to remove blocks, but on the other hand, progressive refinement requires adding intermediate blocks for large time steps. It would be great to have some ablation studies illustrating the effectiveness of either enhancement. 
* Since the target application is generative modeling, and the overall idea follows diffusion models (namely mapping the data distribution to a standard Gaussian, and then inverting the process), I think the authors should compare with diffusion models as well. 
* The writing is a bit sloppy overall. There is almost no assumption stated for Lemma A.1 or Proposition 2.1. At the very least we probably need $p$ and $q$ to have a finite second moment along with differentiability assumptions. The proof of 2.1 does not seem novel to me. It is essentially a direct application of the instantaneous change of variable from [Chen et al. 2018] (which is by itself just rewriting the continuity equation). There is missing reference on this formula (see the equation under (17)), and moreover as written this formula is not correct. The derivative needs to be a total derivative, i.e., it should be $d/dt (\log \rho(x(t), t)) = -\nabla \cdot f(x(t), t)$. The sloppiness of the writing is also manifested in the many handwavy sentences in the main text. To give a few examples:
    - Above (4), ""Under generic conditions"" --- what conditions?
    - In the second paragraph of 2.1, ""The solution of (1) ... gives a one-to-one mapping"" this should only be true if the time interval is small enough, by Picard–Lindelöf theorem
    - In the last paragraph of page 5, ""... it will also have bounded Lipschitz constant"". How is this true? There is no assumption on $\rho_t$ having bounded the Lipschitz constant.
    - At the end of page 5, ""The analysis is postponed here"", where more details could have been given
* For the conditional generation experiments, more details will be helpful. I don't understand what it means by ""we evaluate $V$ for a Gaussian mixture $H | Y$"".
","The clarity of the paper is okay, although more details and more precise language can further improve the clarity. The source code is given but I did not run it.
","Overall I think the paper lacks novelty and needs more comparison with other methods (other JKO methods, diffusion models) to demonstrate its effectiveness. The writing can also be improved.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1667238346710,,-z7O7fk_Cs,ICLR.cc/2023/Conference/Paper3140/-/Official_Review,,[],4,,['everyone'],-z7O7fk_Cs,['ICLR.cc/2023/Conference/Paper3140/Reviewer_ffZu'],1667238346710,,1667238346710,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3140/Reviewer_ffZu']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following references that may not be included in the original paper:

1. Liu et al. ""Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm"" (specifically, Theorem 3.1)
2. Chen et al. 2018 (related to the instantaneous change of variable formula)

The reviewer also mentions that the writing is sloppy and lacks references, and suggests that the authors should provide more precise language and assumptions, but does not explicitly mention that these references are not already included in the paper. However, based on the context, it appears that the reviewer is suggesting that the authors should include these references to support their claims and provide more clarity.",1,2018,Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm 
3G3w7s6cvB,"This paper proposes a new approach named FlexRound to improve the post-training quantization of deep neural networks. Previous methods primarily rely on element-wise addition for the rounding function. The proposed FlexRound, instead, uses element-wise division, which can exploit the parameter magnitude to update the quantization scale. The authors validate the efficacy of FlexRound over various CV and NLP tasks, together with a variety of network architectures.","Strengths:

- The authors propose FlexRound, a new approach that applies element-wise division to exploit the parameter magnitude to update the quantization scale. Figure 3 and Figure 4 provide good explanations on the advantages of FlexRound. 

- The authors conduct comprehensive empirical studies over the image classification, natural language understanding and natural language generation tasks. Moreover, the authors testify various network architectures across CV and NLP, and especially, GPT-Neo and OPT, for post-training quantization. 

- The writing is clean.

Weakness:

- Lack of comparisons with existing approaches, especially BRECQ and Q-Drop. According to the published results of BRECQ and Q-Drop, the results in this paper seem to be worse than these two baselines (e.g., W3-A3 Q-Drop for MobileNet-v2 is 57.98%, but only 41.51% in this paper), even under the same configuration. More comparisons and explanations should be presented.

- While BRECQ and AdaRound rely on element-wise addition for quantization, it is still unclear what is their disadvantage according to the paper.

- The proposed approach seems to be only applicable to weight quantization, yet this is hardly mentioned. For Table-3, it should be made clear that what type of activation quantization is adopted. 



Detailed comments:

- Despite ablated, it is still not clear the benefits of introducing $\mathbf{s}_3$ and $\mathbf{s}_4$ in Equation 2. Intuitively, both factors can be absorbed into $\mathbf{S}_2$. 

- What is the initialization method for $\mathbf{S}$? This should be important as PTQ does not allow intensive training like those in LSQ.

- Provide more derivations (in appendix) for the gradient w.r.t. the scaling factor $\mathbf{S}^{'}$.

- As a minor suggestion, a loss curve or accuracy curve to compare FlexRound with BRECQ during the PTQ iterations could help illustrate FlexRound. This is based on the intuition that parameters with larger magnitude tend to move faster and thus converge better given the limited training iterations and samples.

- Some recent research on PTQ for your reference.

  - Liu Z, Wang Y, Han K, et al. Post-training quantization for vision transformer. NeurIPS 2021.

  - Bai H, Hou L, Shang L, et al. Towards efficient post-training quantization of pre-trained language models. NeurIPS 2022.

  - Park G, Park B, Kwon S J, et al. nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models. arXiv preprint arXiv:2206.09557, 2022.




",See the above.,"Despite the methodological change is minor, the authors provide a reasonable explanation why FlexRound can exploit the importance of pre-trained weights to better calibrate the quantized model (Figure 3 and Figure 4). Experimentally, I appreciate the authors provide comprehensive studies across computer vision, natural language understanding and natural language generation tasks, and results demonstrate the success. Nonetheless, there are still issues with the experimental designs and comparisons, as mentioned above. There are also concerns w.r.t. the methodology, e.g., if FlexRound can be applied to activation quantization; and the necessity in introducing multiple scaling factors $\mathbf{s}3$, $\mathbf{s}4$, e.t.c..","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666626366292,,-tYCaP0phY_,ICLR.cc/2023/Conference/Paper3598/-/Official_Review,,[],3,,['everyone'],-tYCaP0phY_,['ICLR.cc/2023/Conference/Paper3598/Reviewer_mQzD'],1666626366292,,1666681498658,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3598/Reviewer_mQzD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions three specific research papers:

1. Liu Z, Wang Y, Han K, et al. Post-training quantization for vision transformer. NeurIPS 2021.
2. Bai H, Hou L, Shang L, et al. Towards efficient post-training quantization of pre-trained language models. NeurIPS 2022.
3. Park G, Park B, Kwon S J, et al. nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models. arXiv preprint arXiv:2206.09557, 2022.

These papers are provided as a reference for the authors, implying that they are not already discussed in the original paper. The reviewer is suggesting that the authors may find these papers relevant to their work and may want to consider citing them or discussing their findings in relation to the authors' own research.",1,"2021, 2022, 2022, 2022",Post-training quantization for vision transformer 
3G3w7s6cvB,"This paper proposes a new approach named FlexRound to improve the post-training quantization of deep neural networks. Previous methods primarily rely on element-wise addition for the rounding function. The proposed FlexRound, instead, uses element-wise division, which can exploit the parameter magnitude to update the quantization scale. The authors validate the efficacy of FlexRound over various CV and NLP tasks, together with a variety of network architectures.","Strengths:

- The authors propose FlexRound, a new approach that applies element-wise division to exploit the parameter magnitude to update the quantization scale. Figure 3 and Figure 4 provide good explanations on the advantages of FlexRound. 

- The authors conduct comprehensive empirical studies over the image classification, natural language understanding and natural language generation tasks. Moreover, the authors testify various network architectures across CV and NLP, and especially, GPT-Neo and OPT, for post-training quantization. 

- The writing is clean.

Weakness:

- Lack of comparisons with existing approaches, especially BRECQ and Q-Drop. According to the published results of BRECQ and Q-Drop, the results in this paper seem to be worse than these two baselines (e.g., W3-A3 Q-Drop for MobileNet-v2 is 57.98%, but only 41.51% in this paper), even under the same configuration. More comparisons and explanations should be presented.

- While BRECQ and AdaRound rely on element-wise addition for quantization, it is still unclear what is their disadvantage according to the paper.

- The proposed approach seems to be only applicable to weight quantization, yet this is hardly mentioned. For Table-3, it should be made clear that what type of activation quantization is adopted. 



Detailed comments:

- Despite ablated, it is still not clear the benefits of introducing $\mathbf{s}_3$ and $\mathbf{s}_4$ in Equation 2. Intuitively, both factors can be absorbed into $\mathbf{S}_2$. 

- What is the initialization method for $\mathbf{S}$? This should be important as PTQ does not allow intensive training like those in LSQ.

- Provide more derivations (in appendix) for the gradient w.r.t. the scaling factor $\mathbf{S}^{'}$.

- As a minor suggestion, a loss curve or accuracy curve to compare FlexRound with BRECQ during the PTQ iterations could help illustrate FlexRound. This is based on the intuition that parameters with larger magnitude tend to move faster and thus converge better given the limited training iterations and samples.

- Some recent research on PTQ for your reference.

  - Liu Z, Wang Y, Han K, et al. Post-training quantization for vision transformer. NeurIPS 2021.

  - Bai H, Hou L, Shang L, et al. Towards efficient post-training quantization of pre-trained language models. NeurIPS 2022.

  - Park G, Park B, Kwon S J, et al. nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models. arXiv preprint arXiv:2206.09557, 2022.




",See the above.,"Despite the methodological change is minor, the authors provide a reasonable explanation why FlexRound can exploit the importance of pre-trained weights to better calibrate the quantized model (Figure 3 and Figure 4). Experimentally, I appreciate the authors provide comprehensive studies across computer vision, natural language understanding and natural language generation tasks, and results demonstrate the success. Nonetheless, there are still issues with the experimental designs and comparisons, as mentioned above. There are also concerns w.r.t. the methodology, e.g., if FlexRound can be applied to activation quantization; and the necessity in introducing multiple scaling factors $\mathbf{s}3$, $\mathbf{s}4$, e.t.c..","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666626366292,,-tYCaP0phY_,ICLR.cc/2023/Conference/Paper3598/-/Official_Review,,[],3,,['everyone'],-tYCaP0phY_,['ICLR.cc/2023/Conference/Paper3598/Reviewer_mQzD'],1666626366292,,1666681498658,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3598/Reviewer_mQzD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions three specific research papers:

1. Liu Z, Wang Y, Han K, et al. Post-training quantization for vision transformer. NeurIPS 2021.
2. Bai H, Hou L, Shang L, et al. Towards efficient post-training quantization of pre-trained language models. NeurIPS 2022.
3. Park G, Park B, Kwon S J, et al. nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models. arXiv preprint arXiv:2206.09557, 2022.

These papers are provided as a reference for the authors, implying that they are not already discussed in the original paper. The reviewer is suggesting that the authors may find these papers relevant to their work and may want to consider citing them or discussing their findings in relation to the authors' own research.",1,"2021, 2022, 2022, 2022",Towards efficient post-training quantization of pre-trained language models 
3G3w7s6cvB,"This paper proposes a new approach named FlexRound to improve the post-training quantization of deep neural networks. Previous methods primarily rely on element-wise addition for the rounding function. The proposed FlexRound, instead, uses element-wise division, which can exploit the parameter magnitude to update the quantization scale. The authors validate the efficacy of FlexRound over various CV and NLP tasks, together with a variety of network architectures.","Strengths:

- The authors propose FlexRound, a new approach that applies element-wise division to exploit the parameter magnitude to update the quantization scale. Figure 3 and Figure 4 provide good explanations on the advantages of FlexRound. 

- The authors conduct comprehensive empirical studies over the image classification, natural language understanding and natural language generation tasks. Moreover, the authors testify various network architectures across CV and NLP, and especially, GPT-Neo and OPT, for post-training quantization. 

- The writing is clean.

Weakness:

- Lack of comparisons with existing approaches, especially BRECQ and Q-Drop. According to the published results of BRECQ and Q-Drop, the results in this paper seem to be worse than these two baselines (e.g., W3-A3 Q-Drop for MobileNet-v2 is 57.98%, but only 41.51% in this paper), even under the same configuration. More comparisons and explanations should be presented.

- While BRECQ and AdaRound rely on element-wise addition for quantization, it is still unclear what is their disadvantage according to the paper.

- The proposed approach seems to be only applicable to weight quantization, yet this is hardly mentioned. For Table-3, it should be made clear that what type of activation quantization is adopted. 



Detailed comments:

- Despite ablated, it is still not clear the benefits of introducing $\mathbf{s}_3$ and $\mathbf{s}_4$ in Equation 2. Intuitively, both factors can be absorbed into $\mathbf{S}_2$. 

- What is the initialization method for $\mathbf{S}$? This should be important as PTQ does not allow intensive training like those in LSQ.

- Provide more derivations (in appendix) for the gradient w.r.t. the scaling factor $\mathbf{S}^{'}$.

- As a minor suggestion, a loss curve or accuracy curve to compare FlexRound with BRECQ during the PTQ iterations could help illustrate FlexRound. This is based on the intuition that parameters with larger magnitude tend to move faster and thus converge better given the limited training iterations and samples.

- Some recent research on PTQ for your reference.

  - Liu Z, Wang Y, Han K, et al. Post-training quantization for vision transformer. NeurIPS 2021.

  - Bai H, Hou L, Shang L, et al. Towards efficient post-training quantization of pre-trained language models. NeurIPS 2022.

  - Park G, Park B, Kwon S J, et al. nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models. arXiv preprint arXiv:2206.09557, 2022.




",See the above.,"Despite the methodological change is minor, the authors provide a reasonable explanation why FlexRound can exploit the importance of pre-trained weights to better calibrate the quantized model (Figure 3 and Figure 4). Experimentally, I appreciate the authors provide comprehensive studies across computer vision, natural language understanding and natural language generation tasks, and results demonstrate the success. Nonetheless, there are still issues with the experimental designs and comparisons, as mentioned above. There are also concerns w.r.t. the methodology, e.g., if FlexRound can be applied to activation quantization; and the necessity in introducing multiple scaling factors $\mathbf{s}3$, $\mathbf{s}4$, e.t.c..","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666626366292,,-tYCaP0phY_,ICLR.cc/2023/Conference/Paper3598/-/Official_Review,,[],3,,['everyone'],-tYCaP0phY_,['ICLR.cc/2023/Conference/Paper3598/Reviewer_mQzD'],1666626366292,,1666681498658,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3598/Reviewer_mQzD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions three specific research papers:

1. Liu Z, Wang Y, Han K, et al. Post-training quantization for vision transformer. NeurIPS 2021.
2. Bai H, Hou L, Shang L, et al. Towards efficient post-training quantization of pre-trained language models. NeurIPS 2022.
3. Park G, Park B, Kwon S J, et al. nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models. arXiv preprint arXiv:2206.09557, 2022.

These papers are provided as a reference for the authors, implying that they are not already discussed in the original paper. The reviewer is suggesting that the authors may find these papers relevant to their work and may want to consider citing them or discussing their findings in relation to the authors' own research.",1,"2021, 2022, 2022, 2022",nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models
hewA-xqteKS,"This paper presents two techniques for speeding up convolutional neural network inference in homomorphic encryption settings.
Conceptually, the underlying RNS-CKKS homomorphic encryption provides the functionality to securely evaluate functions over a vector of encrypted data in a SIMD manner. In this specific setting, this paper proposes a novel data encoding scheme and a HE-specific pruning technique (sub-block pruning) that substantially reduce the number of homomorphic operations.","Strength:
They considered the problem very common and important in building homomorphic encrypted neural network applications, especially the data encoding one. Also, their solutions are simple and very effective. 

Weakness:
- In the experiment setup, they didn't explicitly mention how they deal with level-zero ciphertexts. I assume that they didn't use bootstrapping and do the re-encryption.
- Same as above, usually in applications, bootstrapping will dominate in the computational cost. Which will make this work less important than it is justified to be. However, these are two independent directions of improvement.
- Except for bootstrapping, the choice of activation function is also questionable. Beyond quadratic polynomials, other works already adopt high-precision approximation of the non-linear ReLU function for higher accuracy (e.g. https://arxiv.org/abs/2105.10879 ). Though, it's also independent of the proposed techniques. But it's better to check in experiments if they could be combined.","The result of this paper looks convincing to me, and their group-interleaved data encoding scheme is very interesting.

For clarity, I think the data encoding scheme deserves a better ( and more formal ) explanation. It's hard to comprehend from just a figure and a short paragraph.","Overall, I think the problem they considered is very important, and the proposed solutions are simple and effective and have a very wide application in secure neural network applications. But this paper's contribution is also limited by the lack of consideration of other aspects like bootstrapping and approximation of non-linear activation function.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,"4: The contributions are significant, and do not exist in prior works.",NO.,5: marginally below the acceptance threshold,1666584317654,,-syx4GzWdTM,ICLR.cc/2023/Conference/Paper5475/-/Official_Review,,[],1,,['everyone'],-syx4GzWdTM,['ICLR.cc/2023/Conference/Paper5475/Reviewer_WT91'],1666584317654,,1666658967480,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5475/Reviewer_WT91']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. Specifically, the reviewer mentions a paper (https://arxiv.org/abs/2105.10879) that adopts high-precision approximation of the non-linear ReLU function for higher accuracy, which is not mentioned in the original paper. The reviewer suggests that it would be better to check in experiments if the proposed techniques can be combined with this approach.",1,2021,High-precision approximation of the non-linear ReLU function
Bdl_EiSjEYO,"The paper seeks to improve the computational efficiency of convolution layers in FHE. Since homomorphic rotations are the primary computational bottleneck of convolutions in FHE, the paper seeks to reduce the number of rotations. This is achieved in two steps, 1) adopting group convolutions, which reduces out-level rotations, and 2) weight pruning, which reduces inner-level rotations.

The efficiency of the proposed convolution is evaluated on *shallow* CNNs designed for MNIST and CIFAR-10. The proposed approach shows appreciable speed-up over *naive* implementations of convolutions in FHE.","Strengths:
- The paper rightly identifies the main bottleneck of *naive* implementations of convolution in FHE, namely rotations. As such, efforts to improve efficiency of convolution is necessary. Leveraging alternative convolutions, such as group convolutions or in the extreme depth-wise convolutions is interesting.
- Adopting pruning for sparsifying the convolution and optimizing the sparsity pattern for reducing number of homomorphic rotations.

Weaknesses:
- The main drawback of the paper is the lack of comparisons to prior work that improve efficiency of convolutions. These include multiplexed convolutions [1], mobile networks explored HEMET [2].
- The main premise of the paper is that rotations in convolutional layers are the main computational bottleneck of networks in FHE. So the paper considers shallow networks only, which are not likely to be practically useful. For instance 85% accuracy on CIFAR-10 is quite poor by the standards of the best plaintext models which achieve ~99% accuracy.
- As networks become deeper, the main accuracy bottleneck is low-degree polynomial approximations of non-linear functions like ReLU and the main computational bottleneck is the bootstrapping operations required for evaluating high-multiplicative depth circuits. So improving efficiency of convolutional layers does not benefit deeper networks since convolution is not the main bottleneck for such networks.

Other Clarification Questions:
- The paper does not mention how pooling operations or strided convolutions are handled. Strided convolutions result in wasted slots. How  does that affect the proposed convolutions?

[1] HEMET: A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture, ICML 2021
[2] Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions, ICML 2022
","- The paper is clear for the most part. The figures are a bit challenging to understand, but becomes clear with the description in the text.

- The quality of the paper is good for the most part, however there are important baselines that are missing. Such methods have not been cited, discussed, or compared against.

- The proposed method is fairly novel. Most existing CNN implementations in FHE using standard dense convolutional layers. This paper proposes to use group convolutions which are more HE friendly. Sparsity pattern is also optimized for minimizing rotations as opposed to other criterion used in standard networks.

- The proposed approach is not reproducible based on the descriptions in the paper. There is missing information, hyper-parameters etc. And the paper does not provide code, nor do the authors promise to release code publicly later on.","The paper proposed to use group convolutions and a weight pruning to mitigate the computational bottlenecks of convolutional layers, namely homomorphic rotations. The paper, however, does not compare to or discuss existing attempts toward HE-friendly CNNs. Furthermore, the experiments are conducted on shallow networks. The proposed approach will not provide much computational benefit for deeper networks since convolution is not the main bottleneck for such networks. Reproducibility is also limited.

Overall, the paper has good ideas, but an evaluation, comparison, and discussion of the broader utility of the proposed approach are missing.

**Update After Rebuttal:** The author's rebuttal does not adequately address the comments from the initial review. In theory, the method may have promise in achieving the claims in the rebuttal, but I do not believe it is straightforward and needs to be demonstrated. I will maintain the original rating.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1667066605423,,-syx4GzWdTM,ICLR.cc/2023/Conference/Paper5475/-/Official_Review,,[],3,,['everyone'],-syx4GzWdTM,['ICLR.cc/2023/Conference/Paper5475/Reviewer_AXmR'],1667066605423,,1670705757393,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5475/Reviewer_AXmR']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. ""HEMET: A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture, ICML 2021"" ([1])
2. ""Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions, ICML 2022"" ([2])

The reviewer states that these papers, which propose ""multiplexed convolutions"" and ""mobile networks explored HEMET"", are not cited, discussed, or compared against in the original paper, and suggests that the authors should have included them as baselines for comparison. This implies that these papers are not already referenced in the original paper.",1,"2021, 2022",HEMET A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture 
Bdl_EiSjEYO,"The paper seeks to improve the computational efficiency of convolution layers in FHE. Since homomorphic rotations are the primary computational bottleneck of convolutions in FHE, the paper seeks to reduce the number of rotations. This is achieved in two steps, 1) adopting group convolutions, which reduces out-level rotations, and 2) weight pruning, which reduces inner-level rotations.

The efficiency of the proposed convolution is evaluated on *shallow* CNNs designed for MNIST and CIFAR-10. The proposed approach shows appreciable speed-up over *naive* implementations of convolutions in FHE.","Strengths:
- The paper rightly identifies the main bottleneck of *naive* implementations of convolution in FHE, namely rotations. As such, efforts to improve efficiency of convolution is necessary. Leveraging alternative convolutions, such as group convolutions or in the extreme depth-wise convolutions is interesting.
- Adopting pruning for sparsifying the convolution and optimizing the sparsity pattern for reducing number of homomorphic rotations.

Weaknesses:
- The main drawback of the paper is the lack of comparisons to prior work that improve efficiency of convolutions. These include multiplexed convolutions [1], mobile networks explored HEMET [2].
- The main premise of the paper is that rotations in convolutional layers are the main computational bottleneck of networks in FHE. So the paper considers shallow networks only, which are not likely to be practically useful. For instance 85% accuracy on CIFAR-10 is quite poor by the standards of the best plaintext models which achieve ~99% accuracy.
- As networks become deeper, the main accuracy bottleneck is low-degree polynomial approximations of non-linear functions like ReLU and the main computational bottleneck is the bootstrapping operations required for evaluating high-multiplicative depth circuits. So improving efficiency of convolutional layers does not benefit deeper networks since convolution is not the main bottleneck for such networks.

Other Clarification Questions:
- The paper does not mention how pooling operations or strided convolutions are handled. Strided convolutions result in wasted slots. How  does that affect the proposed convolutions?

[1] HEMET: A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture, ICML 2021
[2] Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions, ICML 2022
","- The paper is clear for the most part. The figures are a bit challenging to understand, but becomes clear with the description in the text.

- The quality of the paper is good for the most part, however there are important baselines that are missing. Such methods have not been cited, discussed, or compared against.

- The proposed method is fairly novel. Most existing CNN implementations in FHE using standard dense convolutional layers. This paper proposes to use group convolutions which are more HE friendly. Sparsity pattern is also optimized for minimizing rotations as opposed to other criterion used in standard networks.

- The proposed approach is not reproducible based on the descriptions in the paper. There is missing information, hyper-parameters etc. And the paper does not provide code, nor do the authors promise to release code publicly later on.","The paper proposed to use group convolutions and a weight pruning to mitigate the computational bottlenecks of convolutional layers, namely homomorphic rotations. The paper, however, does not compare to or discuss existing attempts toward HE-friendly CNNs. Furthermore, the experiments are conducted on shallow networks. The proposed approach will not provide much computational benefit for deeper networks since convolution is not the main bottleneck for such networks. Reproducibility is also limited.

Overall, the paper has good ideas, but an evaluation, comparison, and discussion of the broader utility of the proposed approach are missing.

**Update After Rebuttal:** The author's rebuttal does not adequately address the comments from the initial review. In theory, the method may have promise in achieving the claims in the rebuttal, but I do not believe it is straightforward and needs to be demonstrated. I will maintain the original rating.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1667066605423,,-syx4GzWdTM,ICLR.cc/2023/Conference/Paper5475/-/Official_Review,,[],3,,['everyone'],-syx4GzWdTM,['ICLR.cc/2023/Conference/Paper5475/Reviewer_AXmR'],1667066605423,,1670705757393,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5475/Reviewer_AXmR']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. ""HEMET: A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture, ICML 2021"" ([1])
2. ""Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions, ICML 2022"" ([2])

The reviewer states that these papers, which propose ""multiplexed convolutions"" and ""mobile networks explored HEMET"", are not cited, discussed, or compared against in the original paper, and suggests that the authors should have included them as baselines for comparison. This implies that these papers are not already referenced in the original paper.",1,"2021, 2022",Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions
4Jb2ac6zFbe,"The paper presents a class-distribution estimation based framework for federated learning. The method incorporates loss reweighting scheme for handling global class imbalance. The paper is completely based on the assumption that after certain rounds of training with imbalanced class, the class probability returned by any sample will be equal to the global distribution of the classes. E.g. if global distribution of a 3-class problem is 5%, 10% and 85%, the returned softmax for each sample will be close to 0.05, 0.10, and 0.85. This is shown with a 2-class example experiment (page 4) and stated as Theorem 1 (page 5).","Unfortunately, I noted a number of issues regarding the technical quality of the paper and these are elaborated below.

1) Paragraph below Theorem 1 (page 5) states that the noise arising out of any violation of above assumption can be eliminated by adding more number of layers. This is demonstrated by some results in fig 4, but no proof or theoretical discussion has been provided.

2) In results, accuracy for minority class in all SOTA algorithms are shown to be ABSOLUTELY zero, whereas the proposed algorithm reaches considerable accuracy (except for a few rows for CIFAR10 dataset where it is single digit). It should be investigated if they have implemented the SOTA algorithms correctly.

3) The brief survey on methods for handling class imbalance in the Introduction section is a bit outdated. Given the problem is quite popular there are many majors developments that took place in recent years. Can you please explain a bit more on why data level imbalance handling techniques cannot be applied locally? If possible can you please cite a reference as you did for the case of cost sensitive learning. 

4) While it is somewhat clear to me what the paper is aiming to propose the motivation remains a bit vague. A bit more clarity on motivation and a glimpse of the algorithm preferably with an illustrative example will only help to highlight the contributions and novelty. 

5) The simulation study I felt is too simple. A more complex example may be using an imbalanced version of MNIST or Fashion-MNIST using a deeper CNN and a high imbalance ratio may be more beneficial. 

6) Section 3.2 introduces three new hyper-parameters. Unfortunately,  we do not know how the plateau region behaves with the three hyper-parameters or if its behaviour changes in any way in practice. Also I did not understand how the other cited stopping criteria can aid us or why they are mentioned if not in the scope of the current article.

7) The getLossReweight is a key ingredient of the proposed algorithm and deserves its own place in the paper. Also no intuitive explanation behind the heuristic of converting the estimated priors to corresponding weights is given. Is there any ablation study on this using different choices of functions?

8) Why Accuracy is used in an imbalanced setting given we know it fails to properly evaluate the performance is such cases? Especially, for Table 3 we have only the worst performing minority class performance to compare on. That Minority class may change over algorithms and if that happens the comparison may become unfair to an extent. Also what happens to the second minority class remains a mystery. Can you use some indices that provides an unbiased evaluation of the classifier in presence of class imbalance (see for example, https://doi.org/10.1016/j.patcog.2020.107197)? I also felt the experimental setup is somewhat simple. May be you can order the classes and select different number of samples from each class, so that 1. Pairwise imbalance ratios will vary and 2. The overall imbalance ratio can be made really high like in the range of 50-100.You can also check how the estimated class priors match to that of the real one. 

9) One problem of cost sensitive learning is that it can overcompensate. In effect this may bring down the TPRs for relatively majority classes. Can you please confirm if such a case is happening? 

10) The algorithm assumes we can generate synthetic data that is balanced, or some real-life balanced dataset is publicly available to server. Balanced auxiliary dataset is mentioned as input to Algorithm 2 and used in line 8 of that algorithm. How much practically relevant is that?

","The paper is more or less clearly written, although the linguistic quality could improve,

The novelty is moderate as global class reweighting for imbalanced classification task has been used frequently before in mainstream machine learning. 

Technical quality and experimental validation part is poor as I already elaborated before. Consequantly the reproducibility is also not up to the expected level. ","The paper addresses an important issue in federated machine learning. However, the current version is far from being matched to the high standards expected for a conference like ICLR.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667018227280,,-qjmJkacGv,ICLR.cc/2023/Conference/Paper4480/-/Official_Review,,[],3,,['everyone'],-qjmJkacGv,['ICLR.cc/2023/Conference/Paper4480/Reviewer_oRX1'],1667018227280,,1667018227280,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4480/Reviewer_oRX1']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

For example, the reviewer mentions that the brief survey on methods for handling class imbalance in the Introduction section is a bit outdated and suggests that there have been many major developments in recent years, implying that the authors should consider including more recent literature.

Additionally, the reviewer provides a specific reference (https://doi.org/10.1016/j.patcog.2020.107197) for using indices that provide an unbiased evaluation of the classifier in the presence of class imbalance, which may not be included in the original paper. 

Therefore, the answer is yes, the peer review does suggest the authors to refer to specific literature that are not already discussed in the original paper.",1,2020,Appropriateness of performance indices for imbalanced data classification: An analysis
dXa7X8xGvik,"The paper presented a new mobile ViT architecture (SeaFormer) for semantic segmentation. The authors improved the computational cost of ViT architecture by proposing an efficient attention block, and light segmentation head. They showed solid results on ADE20K, Pascal Context, and COCO-stuff segmentation datasets.

Post rebuttal:

Thanks for clarifying the cityscapes results, and ImageNet. My rating will stay the same. ","Strength:

* The paper presents a new architecture for mobile semantic segmentation using ViT. Such dense prediction task is very challenging, and usually requires global attention blocks in ViTs to model long range dependencies. The paper presented an efficient attention block utilizing axial attention to tackle segmentation task.

* The paper presented solid numbers on 3 different segmentation datasets: ADE20K, Pascal Context, and COCO-Stuff. They also showed promising results on ImageNet for the image classification task.

Weaknesses:

* The paper proposed an efficient model for segmentation, but they didn't show any results on CityScapes to show the efficiency on dataset with large input size.
* The paper forgot to compare to some of recent work (e.g. EfficientFormer [1]).
* The paper listed a whole subsection (Shared STEM) in the method section which can imply it is a new work, instead it is based on previous work.

[1] Li, Y., Yuan, G., Wen, Y., Hu, E., Evangelidis, G., Tulyakov, S., Wang, Y. and Ren, J., 2022. EfficientFormer: Vision Transformers at MobileNet Speed. arXiv preprint arXiv:2206.01191.
","The paper is written very clearly with enough empirical evidence to support their claims. The work is based on axial attention, but there is enough originality to the method from an architecture design perspective.","The paper presented SeaFormer: a new mobile-ViT model for semantic segmentation. The authors proposed an efficient architecture that model spatial and global information utilizing convnets, and an efficient axial attention block. Empirically, they showed solid results on ADE20K, Pascal Context, and COCO-stuff. This work benefit the effort to push ViT based models to be more mobile friendly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666662205526,,-qg8MQNrxZw,ICLR.cc/2023/Conference/Paper1110/-/Official_Review,,[],3,,['everyone'],-qg8MQNrxZw,['ICLR.cc/2023/Conference/Paper1110/Reviewer_sRfJ'],1666662205526,,1669237607827,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1110/Reviewer_sRfJ']","Yes, this peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer mentions ""EfficientFormer [1]"" as a recent work that the authors should compare their results to, and provides a reference to the paper ""Li, Y., Yuan, G., Wen, Y., Hu, E., Evangelidis, G., Tulyakov, S., Wang, Y. and Ren, J., 2022. EfficientFormer: Vision Transformers at MobileNet Speed. arXiv preprint arXiv:2206.01191."" This implies that the reviewer believes the authors should be aware of and discuss this work in their paper, and that it is not already included in the original paper.",1,"2022, 2022",EfficientFormer Vision Transformers at MobileNet Speed
kKnsd4t01YR,"The paper proposes a new type of recurrent neural network that operates over continuous time and continuous depth.
The continuous depth is achieved through an auxiliary variable t' that is integrated in each step from zero to a certain Tmax. The paper also proposes a heat equation PDE model to decode both time variables.","# Strenghts
- The paper presents an interesting idea on a relevant topic (time-continuous sequential data).
- The paper presents a novel PDE model for resolving the problem of having two different time variables.

# Weaknesses
The experimental evaluation of the paper is insufficient for ICLR.
First of all, the paper lacks discussion and comparison with significant related works.
For instance, [1,2,3,4,5,6,7] propose models for processing irregularly sampled time series. All of these models have demonstrated significant improvements in continuous-time modeling in the past two years alone.

Moreover, the selection of benchmark datasets is insufficient. Particularly, the paper only evaluates the model on two datasets. Additionally, both evaluations seem to be sourced from (Lechner & Hasani, 2020), which did not perform any hyperparameter tuning, evaluated on five datasets, and does not include any method published after 2020.

[1] Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.  
[2] Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.  
[3] Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.  
[4] Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.  
[5] Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.  
[6] Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.  
[7] Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.  
","Regarding novelty, the combination of ODE in time and depth is not a too novel contribution. The main novelties lie in the heat PDE model I belive.

The writing could be sharpened. Specifically, the use of the word ""labeling"" to describe the processing of data by a neural network is inconsistent with the literature on ML. The common understanding in ML is that labeling describes the process of obtaining targets (i.e., ""labels"") for the training of the network. Moreover, the use of ""and sequence data"" in the very first sentence is unnecessary here as ""in sequence processing tasks"" already require sequence data. More minor things like that make reading the paper unpleasant. 
","Overall, minor but interesting contribution with a bit of unconventional writing and insufficient experimental evaluation.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666358129951,,-p5ZEVGtojQ,ICLR.cc/2023/Conference/Paper4980/-/Official_Review,,[],2,,['everyone'],-p5ZEVGtojQ,['ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc'],1666358129951,,1666418080890,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1,2,3,4,5,6,7]) that propose models for processing irregularly sampled time series and have demonstrated significant improvements in continuous-time modeling, implying that the authors should discuss and compare their work with these related works. These papers are:

* Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.
* Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.
* Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.
* Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.
* Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.
* Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.
* Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.

The reviewer also mentions Lechner & Hasani (2020), which is likely already included in the original paper, as the evaluations in the paper seem to be sourced from it.",1,"2022, 2022, 2020, 2020, 2020, 2021, 2021, 2020",CKConv Continuous Kernel Convolution For Sequential Data
kKnsd4t01YR,"The paper proposes a new type of recurrent neural network that operates over continuous time and continuous depth.
The continuous depth is achieved through an auxiliary variable t' that is integrated in each step from zero to a certain Tmax. The paper also proposes a heat equation PDE model to decode both time variables.","# Strenghts
- The paper presents an interesting idea on a relevant topic (time-continuous sequential data).
- The paper presents a novel PDE model for resolving the problem of having two different time variables.

# Weaknesses
The experimental evaluation of the paper is insufficient for ICLR.
First of all, the paper lacks discussion and comparison with significant related works.
For instance, [1,2,3,4,5,6,7] propose models for processing irregularly sampled time series. All of these models have demonstrated significant improvements in continuous-time modeling in the past two years alone.

Moreover, the selection of benchmark datasets is insufficient. Particularly, the paper only evaluates the model on two datasets. Additionally, both evaluations seem to be sourced from (Lechner & Hasani, 2020), which did not perform any hyperparameter tuning, evaluated on five datasets, and does not include any method published after 2020.

[1] Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.  
[2] Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.  
[3] Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.  
[4] Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.  
[5] Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.  
[6] Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.  
[7] Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.  
","Regarding novelty, the combination of ODE in time and depth is not a too novel contribution. The main novelties lie in the heat PDE model I belive.

The writing could be sharpened. Specifically, the use of the word ""labeling"" to describe the processing of data by a neural network is inconsistent with the literature on ML. The common understanding in ML is that labeling describes the process of obtaining targets (i.e., ""labels"") for the training of the network. Moreover, the use of ""and sequence data"" in the very first sentence is unnecessary here as ""in sequence processing tasks"" already require sequence data. More minor things like that make reading the paper unpleasant. 
","Overall, minor but interesting contribution with a bit of unconventional writing and insufficient experimental evaluation.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666358129951,,-p5ZEVGtojQ,ICLR.cc/2023/Conference/Paper4980/-/Official_Review,,[],2,,['everyone'],-p5ZEVGtojQ,['ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc'],1666358129951,,1666418080890,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1,2,3,4,5,6,7]) that propose models for processing irregularly sampled time series and have demonstrated significant improvements in continuous-time modeling, implying that the authors should discuss and compare their work with these related works. These papers are:

* Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.
* Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.
* Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.
* Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.
* Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.
* Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.
* Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.

The reviewer also mentions Lechner & Hasani (2020), which is likely already included in the original paper, as the evaluations in the paper seem to be sourced from it.",1,"2022, 2022, 2020, 2020, 2020, 2021, 2021, 2020",Efficiently Modeling Long Sequences with Structured State Spaces
kKnsd4t01YR,"The paper proposes a new type of recurrent neural network that operates over continuous time and continuous depth.
The continuous depth is achieved through an auxiliary variable t' that is integrated in each step from zero to a certain Tmax. The paper also proposes a heat equation PDE model to decode both time variables.","# Strenghts
- The paper presents an interesting idea on a relevant topic (time-continuous sequential data).
- The paper presents a novel PDE model for resolving the problem of having two different time variables.

# Weaknesses
The experimental evaluation of the paper is insufficient for ICLR.
First of all, the paper lacks discussion and comparison with significant related works.
For instance, [1,2,3,4,5,6,7] propose models for processing irregularly sampled time series. All of these models have demonstrated significant improvements in continuous-time modeling in the past two years alone.

Moreover, the selection of benchmark datasets is insufficient. Particularly, the paper only evaluates the model on two datasets. Additionally, both evaluations seem to be sourced from (Lechner & Hasani, 2020), which did not perform any hyperparameter tuning, evaluated on five datasets, and does not include any method published after 2020.

[1] Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.  
[2] Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.  
[3] Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.  
[4] Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.  
[5] Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.  
[6] Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.  
[7] Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.  
","Regarding novelty, the combination of ODE in time and depth is not a too novel contribution. The main novelties lie in the heat PDE model I belive.

The writing could be sharpened. Specifically, the use of the word ""labeling"" to describe the processing of data by a neural network is inconsistent with the literature on ML. The common understanding in ML is that labeling describes the process of obtaining targets (i.e., ""labels"") for the training of the network. Moreover, the use of ""and sequence data"" in the very first sentence is unnecessary here as ""in sequence processing tasks"" already require sequence data. More minor things like that make reading the paper unpleasant. 
","Overall, minor but interesting contribution with a bit of unconventional writing and insufficient experimental evaluation.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666358129951,,-p5ZEVGtojQ,ICLR.cc/2023/Conference/Paper4980/-/Official_Review,,[],2,,['everyone'],-p5ZEVGtojQ,['ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc'],1666358129951,,1666418080890,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1,2,3,4,5,6,7]) that propose models for processing irregularly sampled time series and have demonstrated significant improvements in continuous-time modeling, implying that the authors should discuss and compare their work with these related works. These papers are:

* Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.
* Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.
* Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.
* Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.
* Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.
* Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.
* Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.

The reviewer also mentions Lechner & Hasani (2020), which is likely already included in the original paper, as the evaluations in the paper seem to be sourced from it.",1,"2022, 2022, 2020, 2020, 2020, 2021, 2021, 2020",Neural Controlled Differential Equations for Irregular Time Series
kKnsd4t01YR,"The paper proposes a new type of recurrent neural network that operates over continuous time and continuous depth.
The continuous depth is achieved through an auxiliary variable t' that is integrated in each step from zero to a certain Tmax. The paper also proposes a heat equation PDE model to decode both time variables.","# Strenghts
- The paper presents an interesting idea on a relevant topic (time-continuous sequential data).
- The paper presents a novel PDE model for resolving the problem of having two different time variables.

# Weaknesses
The experimental evaluation of the paper is insufficient for ICLR.
First of all, the paper lacks discussion and comparison with significant related works.
For instance, [1,2,3,4,5,6,7] propose models for processing irregularly sampled time series. All of these models have demonstrated significant improvements in continuous-time modeling in the past two years alone.

Moreover, the selection of benchmark datasets is insufficient. Particularly, the paper only evaluates the model on two datasets. Additionally, both evaluations seem to be sourced from (Lechner & Hasani, 2020), which did not perform any hyperparameter tuning, evaluated on five datasets, and does not include any method published after 2020.

[1] Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.  
[2] Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.  
[3] Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.  
[4] Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.  
[5] Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.  
[6] Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.  
[7] Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.  
","Regarding novelty, the combination of ODE in time and depth is not a too novel contribution. The main novelties lie in the heat PDE model I belive.

The writing could be sharpened. Specifically, the use of the word ""labeling"" to describe the processing of data by a neural network is inconsistent with the literature on ML. The common understanding in ML is that labeling describes the process of obtaining targets (i.e., ""labels"") for the training of the network. Moreover, the use of ""and sequence data"" in the very first sentence is unnecessary here as ""in sequence processing tasks"" already require sequence data. More minor things like that make reading the paper unpleasant. 
","Overall, minor but interesting contribution with a bit of unconventional writing and insufficient experimental evaluation.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666358129951,,-p5ZEVGtojQ,ICLR.cc/2023/Conference/Paper4980/-/Official_Review,,[],2,,['everyone'],-p5ZEVGtojQ,['ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc'],1666358129951,,1666418080890,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1,2,3,4,5,6,7]) that propose models for processing irregularly sampled time series and have demonstrated significant improvements in continuous-time modeling, implying that the authors should discuss and compare their work with these related works. These papers are:

* Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.
* Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.
* Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.
* Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.
* Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.
* Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.
* Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.

The reviewer also mentions Lechner & Hasani (2020), which is likely already included in the original paper, as the evaluations in the paper seem to be sourced from it.",1,"2022, 2022, 2020, 2020, 2020, 2021, 2021, 2020",Neural Rough Differential Equations for Long Time Series
kKnsd4t01YR,"The paper proposes a new type of recurrent neural network that operates over continuous time and continuous depth.
The continuous depth is achieved through an auxiliary variable t' that is integrated in each step from zero to a certain Tmax. The paper also proposes a heat equation PDE model to decode both time variables.","# Strenghts
- The paper presents an interesting idea on a relevant topic (time-continuous sequential data).
- The paper presents a novel PDE model for resolving the problem of having two different time variables.

# Weaknesses
The experimental evaluation of the paper is insufficient for ICLR.
First of all, the paper lacks discussion and comparison with significant related works.
For instance, [1,2,3,4,5,6,7] propose models for processing irregularly sampled time series. All of these models have demonstrated significant improvements in continuous-time modeling in the past two years alone.

Moreover, the selection of benchmark datasets is insufficient. Particularly, the paper only evaluates the model on two datasets. Additionally, both evaluations seem to be sourced from (Lechner & Hasani, 2020), which did not perform any hyperparameter tuning, evaluated on five datasets, and does not include any method published after 2020.

[1] Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.  
[2] Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.  
[3] Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.  
[4] Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.  
[5] Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.  
[6] Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.  
[7] Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.  
","Regarding novelty, the combination of ODE in time and depth is not a too novel contribution. The main novelties lie in the heat PDE model I belive.

The writing could be sharpened. Specifically, the use of the word ""labeling"" to describe the processing of data by a neural network is inconsistent with the literature on ML. The common understanding in ML is that labeling describes the process of obtaining targets (i.e., ""labels"") for the training of the network. Moreover, the use of ""and sequence data"" in the very first sentence is unnecessary here as ""in sequence processing tasks"" already require sequence data. More minor things like that make reading the paper unpleasant. 
","Overall, minor but interesting contribution with a bit of unconventional writing and insufficient experimental evaluation.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666358129951,,-p5ZEVGtojQ,ICLR.cc/2023/Conference/Paper4980/-/Official_Review,,[],2,,['everyone'],-p5ZEVGtojQ,['ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc'],1666358129951,,1666418080890,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1,2,3,4,5,6,7]) that propose models for processing irregularly sampled time series and have demonstrated significant improvements in continuous-time modeling, implying that the authors should discuss and compare their work with these related works. These papers are:

* Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.
* Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.
* Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.
* Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.
* Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.
* Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.
* Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.

The reviewer also mentions Lechner & Hasani (2020), which is likely already included in the original paper, as the evaluations in the paper seem to be sourced from it.",1,"2022, 2022, 2020, 2020, 2020, 2021, 2021, 2020",HiPPO Recurrent Memory with Optimal Polynomial Projections
kKnsd4t01YR,"The paper proposes a new type of recurrent neural network that operates over continuous time and continuous depth.
The continuous depth is achieved through an auxiliary variable t' that is integrated in each step from zero to a certain Tmax. The paper also proposes a heat equation PDE model to decode both time variables.","# Strenghts
- The paper presents an interesting idea on a relevant topic (time-continuous sequential data).
- The paper presents a novel PDE model for resolving the problem of having two different time variables.

# Weaknesses
The experimental evaluation of the paper is insufficient for ICLR.
First of all, the paper lacks discussion and comparison with significant related works.
For instance, [1,2,3,4,5,6,7] propose models for processing irregularly sampled time series. All of these models have demonstrated significant improvements in continuous-time modeling in the past two years alone.

Moreover, the selection of benchmark datasets is insufficient. Particularly, the paper only evaluates the model on two datasets. Additionally, both evaluations seem to be sourced from (Lechner & Hasani, 2020), which did not perform any hyperparameter tuning, evaluated on five datasets, and does not include any method published after 2020.

[1] Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.  
[2] Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.  
[3] Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.  
[4] Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.  
[5] Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.  
[6] Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.  
[7] Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.  
","Regarding novelty, the combination of ODE in time and depth is not a too novel contribution. The main novelties lie in the heat PDE model I belive.

The writing could be sharpened. Specifically, the use of the word ""labeling"" to describe the processing of data by a neural network is inconsistent with the literature on ML. The common understanding in ML is that labeling describes the process of obtaining targets (i.e., ""labels"") for the training of the network. Moreover, the use of ""and sequence data"" in the very first sentence is unnecessary here as ""in sequence processing tasks"" already require sequence data. More minor things like that make reading the paper unpleasant. 
","Overall, minor but interesting contribution with a bit of unconventional writing and insufficient experimental evaluation.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666358129951,,-p5ZEVGtojQ,ICLR.cc/2023/Conference/Paper4980/-/Official_Review,,[],2,,['everyone'],-p5ZEVGtojQ,['ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc'],1666358129951,,1666418080890,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1,2,3,4,5,6,7]) that propose models for processing irregularly sampled time series and have demonstrated significant improvements in continuous-time modeling, implying that the authors should discuss and compare their work with these related works. These papers are:

* Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.
* Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.
* Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.
* Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.
* Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.
* Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.
* Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.

The reviewer also mentions Lechner & Hasani (2020), which is likely already included in the original paper, as the evaluations in the paper seem to be sourced from it.",1,"2022, 2022, 2020, 2020, 2020, 2021, 2021, 2020",Efficient and Accurate Gradients for Neural SDEs
kKnsd4t01YR,"The paper proposes a new type of recurrent neural network that operates over continuous time and continuous depth.
The continuous depth is achieved through an auxiliary variable t' that is integrated in each step from zero to a certain Tmax. The paper also proposes a heat equation PDE model to decode both time variables.","# Strenghts
- The paper presents an interesting idea on a relevant topic (time-continuous sequential data).
- The paper presents a novel PDE model for resolving the problem of having two different time variables.

# Weaknesses
The experimental evaluation of the paper is insufficient for ICLR.
First of all, the paper lacks discussion and comparison with significant related works.
For instance, [1,2,3,4,5,6,7] propose models for processing irregularly sampled time series. All of these models have demonstrated significant improvements in continuous-time modeling in the past two years alone.

Moreover, the selection of benchmark datasets is insufficient. Particularly, the paper only evaluates the model on two datasets. Additionally, both evaluations seem to be sourced from (Lechner & Hasani, 2020), which did not perform any hyperparameter tuning, evaluated on five datasets, and does not include any method published after 2020.

[1] Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.  
[2] Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.  
[3] Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.  
[4] Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.  
[5] Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.  
[6] Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.  
[7] Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.  
","Regarding novelty, the combination of ODE in time and depth is not a too novel contribution. The main novelties lie in the heat PDE model I belive.

The writing could be sharpened. Specifically, the use of the word ""labeling"" to describe the processing of data by a neural network is inconsistent with the literature on ML. The common understanding in ML is that labeling describes the process of obtaining targets (i.e., ""labels"") for the training of the network. Moreover, the use of ""and sequence data"" in the very first sentence is unnecessary here as ""in sequence processing tasks"" already require sequence data. More minor things like that make reading the paper unpleasant. 
","Overall, minor but interesting contribution with a bit of unconventional writing and insufficient experimental evaluation.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666358129951,,-p5ZEVGtojQ,ICLR.cc/2023/Conference/Paper4980/-/Official_Review,,[],2,,['everyone'],-p5ZEVGtojQ,['ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc'],1666358129951,,1666418080890,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1,2,3,4,5,6,7]) that propose models for processing irregularly sampled time series and have demonstrated significant improvements in continuous-time modeling, implying that the authors should discuss and compare their work with these related works. These papers are:

* Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.
* Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.
* Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.
* Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.
* Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.
* Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.
* Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.

The reviewer also mentions Lechner & Hasani (2020), which is likely already included in the original paper, as the evaluations in the paper seem to be sourced from it.",1,"2022, 2022, 2020, 2020, 2020, 2021, 2021, 2020",Multi-Time Attention Networks for Irregularly Sampled Time Series
NzAxzhaF93,"The paper presents an approach to modeling PDE solutions using a trainable hyper-network that generates the parameters of an FNO network. The hyper-network gets as an input only the desired continuous time t (>0) and the FNO transforms an initial condition under the dynamics specified by the output of the hyper-network. While the performance of this method at the discrete training time steps already improves on FNO, the main innovation is allowing for interpolated solutions at any continuous time. ","The main strength of the paper is the pursuit to learn the underlying dynamics over the continuous time domain. As the authors mention in the discussion section (Sec. 6), this might indeed be very useful as a reduced-order model e.g. of the NSE. 

Comments:
- Given that the main comparison baseline is FNO and FNO is also a major building block in this architecture, I would have expected a brief introduction to the used FNO formalism. 
- Where are CNNs and GNNs? Given the recent popularity of GNNs for the solution of PDEs (e.g. https://arxiv.org/abs/2202.03376, https://arxiv.org/abs/2002.09405) as well as CNNs (e.g. https://arxiv.org/abs/1810.08217), one should at least mention these lines of research in the ""Related Work"" section and justify why they are not (or are) relevant here. 
- I see the intention to learn meaningful interpolated solutions of PDEs via the hyper-network, but given the lack of results on extrapolation over longer horizons, e.g. t=10...100, I would speculate that the results are not promising. Any comment on that?  Given the practical relevance of extrapolating over longer times, this aspect should be addressed somewhere. And this regime is something that has already been studied by the so-called direct-time methods, e.g. in this workshop paper by Meyer et al., 2021 using GNNs: https://arxiv.org/abs/2112.10296. I understand that all the PDEs you consider are dissipative, thus too long trajectories wouldn't make sense, but why not look at some inviscid equations (e.g. Burgers, Euler) and discuss e.g. the survival time as done in https://arxiv.org/abs/2202.03376.
- I like the loss terms defined in the paper quite a lot: they incorporate all possible types of interpolation. However, I know other papers that do similar things (ignoring L_final and L_initial for a second): The construction of L_comp^(P) is very closely related to the ""temporal bundling"" presented here https://arxiv.org/abs/2202.03376, and L_inter reminds me of the ""pushforward training"" from the same paper. I'm sure that there are more analogies from other papers.

Two minor problems I detected are:
1. some formatting issues with the equations leading to ""Equation equation [...]""
2. ""Miltiple Graph Neural Operator"" -> Multipole Graph Neural Operator""","The paper is well-written and easy to digest. As mentioned in the main review, some more information on FNO could be useful.

I haven't had the chance to work with FNO-based approaches yet, thus I cannot evaluate the figures and tables more than what I see in the paper. The presented results look ok to me and the improvement over vanilla FNO in the interpolation regime seems quite clear.

Regarding novelty, the paper seems to introduce the combination of hyper-networks and FNO for the first time. And although the hyper-network and FNO networks seem to be ""off the shelf"", given the rather specialized loss makes for a novel approach.

The supplementary material seems to do the job in terms of reproducibility. The 1D data is downloadable, and the 2D and 3D data can be generated using PhiFlow. I successfully started a training run on the 1D data.","The idea of the paper is very interesting, but out of the 21 mentioned references, I didn't find a single one from 2022. This suggests that the authors lack knowledge of the recent literature, which includes e.g. GNNs. This issue and the weak justification of the chosen methods and experiments (see above) are the reason why I believe that this paper is not yet ready for publication.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1667474388855,,-i73LPWa3bD,ICLR.cc/2023/Conference/Paper1284/-/Official_Review,,[],5,,['everyone'],-i73LPWa3bD,['ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD'],1667474388855,,1667474388855,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers from 2022 that are not included in the original paper's references, such as:

* https://arxiv.org/abs/2202.03376
* https://arxiv.org/abs/2002.09405
* https://arxiv.org/abs/1810.08217
* https://arxiv.org/abs/2112.10296

These papers are related to the use of Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) for solving Partial Differential Equations (PDEs), which is the topic of the original paper. The reviewer suggests that the authors should at least mention these lines of research in the ""Related Work"" section and justify why they are not relevant to the paper.",1,"2022, 2022, 2020, 2018, 2021",Message Passing Neural PDE Solvers
NzAxzhaF93,"The paper presents an approach to modeling PDE solutions using a trainable hyper-network that generates the parameters of an FNO network. The hyper-network gets as an input only the desired continuous time t (>0) and the FNO transforms an initial condition under the dynamics specified by the output of the hyper-network. While the performance of this method at the discrete training time steps already improves on FNO, the main innovation is allowing for interpolated solutions at any continuous time. ","The main strength of the paper is the pursuit to learn the underlying dynamics over the continuous time domain. As the authors mention in the discussion section (Sec. 6), this might indeed be very useful as a reduced-order model e.g. of the NSE. 

Comments:
- Given that the main comparison baseline is FNO and FNO is also a major building block in this architecture, I would have expected a brief introduction to the used FNO formalism. 
- Where are CNNs and GNNs? Given the recent popularity of GNNs for the solution of PDEs (e.g. https://arxiv.org/abs/2202.03376, https://arxiv.org/abs/2002.09405) as well as CNNs (e.g. https://arxiv.org/abs/1810.08217), one should at least mention these lines of research in the ""Related Work"" section and justify why they are not (or are) relevant here. 
- I see the intention to learn meaningful interpolated solutions of PDEs via the hyper-network, but given the lack of results on extrapolation over longer horizons, e.g. t=10...100, I would speculate that the results are not promising. Any comment on that?  Given the practical relevance of extrapolating over longer times, this aspect should be addressed somewhere. And this regime is something that has already been studied by the so-called direct-time methods, e.g. in this workshop paper by Meyer et al., 2021 using GNNs: https://arxiv.org/abs/2112.10296. I understand that all the PDEs you consider are dissipative, thus too long trajectories wouldn't make sense, but why not look at some inviscid equations (e.g. Burgers, Euler) and discuss e.g. the survival time as done in https://arxiv.org/abs/2202.03376.
- I like the loss terms defined in the paper quite a lot: they incorporate all possible types of interpolation. However, I know other papers that do similar things (ignoring L_final and L_initial for a second): The construction of L_comp^(P) is very closely related to the ""temporal bundling"" presented here https://arxiv.org/abs/2202.03376, and L_inter reminds me of the ""pushforward training"" from the same paper. I'm sure that there are more analogies from other papers.

Two minor problems I detected are:
1. some formatting issues with the equations leading to ""Equation equation [...]""
2. ""Miltiple Graph Neural Operator"" -> Multipole Graph Neural Operator""","The paper is well-written and easy to digest. As mentioned in the main review, some more information on FNO could be useful.

I haven't had the chance to work with FNO-based approaches yet, thus I cannot evaluate the figures and tables more than what I see in the paper. The presented results look ok to me and the improvement over vanilla FNO in the interpolation regime seems quite clear.

Regarding novelty, the paper seems to introduce the combination of hyper-networks and FNO for the first time. And although the hyper-network and FNO networks seem to be ""off the shelf"", given the rather specialized loss makes for a novel approach.

The supplementary material seems to do the job in terms of reproducibility. The 1D data is downloadable, and the 2D and 3D data can be generated using PhiFlow. I successfully started a training run on the 1D data.","The idea of the paper is very interesting, but out of the 21 mentioned references, I didn't find a single one from 2022. This suggests that the authors lack knowledge of the recent literature, which includes e.g. GNNs. This issue and the weak justification of the chosen methods and experiments (see above) are the reason why I believe that this paper is not yet ready for publication.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1667474388855,,-i73LPWa3bD,ICLR.cc/2023/Conference/Paper1284/-/Official_Review,,[],5,,['everyone'],-i73LPWa3bD,['ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD'],1667474388855,,1667474388855,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers from 2022 that are not included in the original paper's references, such as:

* https://arxiv.org/abs/2202.03376
* https://arxiv.org/abs/2002.09405
* https://arxiv.org/abs/1810.08217
* https://arxiv.org/abs/2112.10296

These papers are related to the use of Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) for solving Partial Differential Equations (PDEs), which is the topic of the original paper. The reviewer suggests that the authors should at least mention these lines of research in the ""Related Work"" section and justify why they are not relevant to the paper.",1,"2022, 2022, 2020, 2018, 2021",Learning to Simulate Complex Physics with Graph Networks
NzAxzhaF93,"The paper presents an approach to modeling PDE solutions using a trainable hyper-network that generates the parameters of an FNO network. The hyper-network gets as an input only the desired continuous time t (>0) and the FNO transforms an initial condition under the dynamics specified by the output of the hyper-network. While the performance of this method at the discrete training time steps already improves on FNO, the main innovation is allowing for interpolated solutions at any continuous time. ","The main strength of the paper is the pursuit to learn the underlying dynamics over the continuous time domain. As the authors mention in the discussion section (Sec. 6), this might indeed be very useful as a reduced-order model e.g. of the NSE. 

Comments:
- Given that the main comparison baseline is FNO and FNO is also a major building block in this architecture, I would have expected a brief introduction to the used FNO formalism. 
- Where are CNNs and GNNs? Given the recent popularity of GNNs for the solution of PDEs (e.g. https://arxiv.org/abs/2202.03376, https://arxiv.org/abs/2002.09405) as well as CNNs (e.g. https://arxiv.org/abs/1810.08217), one should at least mention these lines of research in the ""Related Work"" section and justify why they are not (or are) relevant here. 
- I see the intention to learn meaningful interpolated solutions of PDEs via the hyper-network, but given the lack of results on extrapolation over longer horizons, e.g. t=10...100, I would speculate that the results are not promising. Any comment on that?  Given the practical relevance of extrapolating over longer times, this aspect should be addressed somewhere. And this regime is something that has already been studied by the so-called direct-time methods, e.g. in this workshop paper by Meyer et al., 2021 using GNNs: https://arxiv.org/abs/2112.10296. I understand that all the PDEs you consider are dissipative, thus too long trajectories wouldn't make sense, but why not look at some inviscid equations (e.g. Burgers, Euler) and discuss e.g. the survival time as done in https://arxiv.org/abs/2202.03376.
- I like the loss terms defined in the paper quite a lot: they incorporate all possible types of interpolation. However, I know other papers that do similar things (ignoring L_final and L_initial for a second): The construction of L_comp^(P) is very closely related to the ""temporal bundling"" presented here https://arxiv.org/abs/2202.03376, and L_inter reminds me of the ""pushforward training"" from the same paper. I'm sure that there are more analogies from other papers.

Two minor problems I detected are:
1. some formatting issues with the equations leading to ""Equation equation [...]""
2. ""Miltiple Graph Neural Operator"" -> Multipole Graph Neural Operator""","The paper is well-written and easy to digest. As mentioned in the main review, some more information on FNO could be useful.

I haven't had the chance to work with FNO-based approaches yet, thus I cannot evaluate the figures and tables more than what I see in the paper. The presented results look ok to me and the improvement over vanilla FNO in the interpolation regime seems quite clear.

Regarding novelty, the paper seems to introduce the combination of hyper-networks and FNO for the first time. And although the hyper-network and FNO networks seem to be ""off the shelf"", given the rather specialized loss makes for a novel approach.

The supplementary material seems to do the job in terms of reproducibility. The 1D data is downloadable, and the 2D and 3D data can be generated using PhiFlow. I successfully started a training run on the 1D data.","The idea of the paper is very interesting, but out of the 21 mentioned references, I didn't find a single one from 2022. This suggests that the authors lack knowledge of the recent literature, which includes e.g. GNNs. This issue and the weak justification of the chosen methods and experiments (see above) are the reason why I believe that this paper is not yet ready for publication.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1667474388855,,-i73LPWa3bD,ICLR.cc/2023/Conference/Paper1284/-/Official_Review,,[],5,,['everyone'],-i73LPWa3bD,['ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD'],1667474388855,,1667474388855,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers from 2022 that are not included in the original paper's references, such as:

* https://arxiv.org/abs/2202.03376
* https://arxiv.org/abs/2002.09405
* https://arxiv.org/abs/1810.08217
* https://arxiv.org/abs/2112.10296

These papers are related to the use of Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) for solving Partial Differential Equations (PDEs), which is the topic of the original paper. The reviewer suggests that the authors should at least mention these lines of research in the ""Related Work"" section and justify why they are not relevant to the paper.",1,"2022, 2022, 2020, 2018, 2021",Deep Learning Methods for Reynolds-Averaged Navier-Stokes Simulations of Airfoil Flows
NzAxzhaF93,"The paper presents an approach to modeling PDE solutions using a trainable hyper-network that generates the parameters of an FNO network. The hyper-network gets as an input only the desired continuous time t (>0) and the FNO transforms an initial condition under the dynamics specified by the output of the hyper-network. While the performance of this method at the discrete training time steps already improves on FNO, the main innovation is allowing for interpolated solutions at any continuous time. ","The main strength of the paper is the pursuit to learn the underlying dynamics over the continuous time domain. As the authors mention in the discussion section (Sec. 6), this might indeed be very useful as a reduced-order model e.g. of the NSE. 

Comments:
- Given that the main comparison baseline is FNO and FNO is also a major building block in this architecture, I would have expected a brief introduction to the used FNO formalism. 
- Where are CNNs and GNNs? Given the recent popularity of GNNs for the solution of PDEs (e.g. https://arxiv.org/abs/2202.03376, https://arxiv.org/abs/2002.09405) as well as CNNs (e.g. https://arxiv.org/abs/1810.08217), one should at least mention these lines of research in the ""Related Work"" section and justify why they are not (or are) relevant here. 
- I see the intention to learn meaningful interpolated solutions of PDEs via the hyper-network, but given the lack of results on extrapolation over longer horizons, e.g. t=10...100, I would speculate that the results are not promising. Any comment on that?  Given the practical relevance of extrapolating over longer times, this aspect should be addressed somewhere. And this regime is something that has already been studied by the so-called direct-time methods, e.g. in this workshop paper by Meyer et al., 2021 using GNNs: https://arxiv.org/abs/2112.10296. I understand that all the PDEs you consider are dissipative, thus too long trajectories wouldn't make sense, but why not look at some inviscid equations (e.g. Burgers, Euler) and discuss e.g. the survival time as done in https://arxiv.org/abs/2202.03376.
- I like the loss terms defined in the paper quite a lot: they incorporate all possible types of interpolation. However, I know other papers that do similar things (ignoring L_final and L_initial for a second): The construction of L_comp^(P) is very closely related to the ""temporal bundling"" presented here https://arxiv.org/abs/2202.03376, and L_inter reminds me of the ""pushforward training"" from the same paper. I'm sure that there are more analogies from other papers.

Two minor problems I detected are:
1. some formatting issues with the equations leading to ""Equation equation [...]""
2. ""Miltiple Graph Neural Operator"" -> Multipole Graph Neural Operator""","The paper is well-written and easy to digest. As mentioned in the main review, some more information on FNO could be useful.

I haven't had the chance to work with FNO-based approaches yet, thus I cannot evaluate the figures and tables more than what I see in the paper. The presented results look ok to me and the improvement over vanilla FNO in the interpolation regime seems quite clear.

Regarding novelty, the paper seems to introduce the combination of hyper-networks and FNO for the first time. And although the hyper-network and FNO networks seem to be ""off the shelf"", given the rather specialized loss makes for a novel approach.

The supplementary material seems to do the job in terms of reproducibility. The 1D data is downloadable, and the 2D and 3D data can be generated using PhiFlow. I successfully started a training run on the 1D data.","The idea of the paper is very interesting, but out of the 21 mentioned references, I didn't find a single one from 2022. This suggests that the authors lack knowledge of the recent literature, which includes e.g. GNNs. This issue and the weak justification of the chosen methods and experiments (see above) are the reason why I believe that this paper is not yet ready for publication.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,"3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1667474388855,,-i73LPWa3bD,ICLR.cc/2023/Conference/Paper1284/-/Official_Review,,[],5,,['everyone'],-i73LPWa3bD,['ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD'],1667474388855,,1667474388855,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers from 2022 that are not included in the original paper's references, such as:

* https://arxiv.org/abs/2202.03376
* https://arxiv.org/abs/2002.09405
* https://arxiv.org/abs/1810.08217
* https://arxiv.org/abs/2112.10296

These papers are related to the use of Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) for solving Partial Differential Equations (PDEs), which is the topic of the original paper. The reviewer suggests that the authors should at least mention these lines of research in the ""Related Work"" section and justify why they are not relevant to the paper.",1,"2022, 2022, 2020, 2018, 2021",Deep Surrogate for Direct Time Fluid Dynamics
dYxTKnSxzF2,"The paper establishes a connection between the Lovasz theta function and contrastive learning. Specifically, with InfoNCE - a type of contrastive loss function used for self-supervised learning. They define a novel loss function - Lovasz theta contrastive loss based on a weighted graph representation of the similarities between examples.. 
The loss function is evaluated with various ways of quantifying sample similarity. The method outperforms existing model SimCLR in both supervised and unsupervised contrastive learning. Previous works introduce the notion of using graph representation with edge weights equal to the probability that two samples correspond to views of the same sample. Current work uses that representation to perform contrastive learning. Under certain choices for model dimensionality and batch size, the paper shows an equivalence between minimizing InfoNCE loss and the Delsarte formulation of the Lovasz theta problem.
This equivalence is shown in both supervised and unsupervised cases. The equivalence is used to incorporate degree-of-similarity information into contrastive learning. So this method is a generalization of regular contrastive learning. In unsupervised cases, the method outperforms regular SimCLR pretraining on CIFAR100. In the supervised case, the method outperforms simple cross entropy loss and supervised contrastive learning on CIFAR100 and ImageNet-100. The method can be used with any similarity measure to create a problem specific contrastive loss.
As a proof of improvement, the work plots the effect of blending regular contrastive learning with the degree-of-similarity aware method they created. A parameter λ = 0 (regular SupCon) to λ = 1 (their method) is used to blend the effect of setting the class similarity matrix as C’ = λC + (1 − λ)I. It shows an overall improvement in performance as λ increases.
The work links the field of contrastive learning to classic graph theory and semi-definite programming. By doing this, it tries to provide a direction to understand the underpinnings of contrastive learning. 
","The paper introduces a new loss function and provides proof that the minimization of certain terms in the proposed loss function is a relaxation of the weighted Lovasz theta function. To do this, they start with a very widely-used contrastive loss function - InfoNCE. They prove that minimizing InfoNCE is the same as minimizing a certain formulation of the Lovasz theta function. This provides a strong basis for the theory of contrastive learning. Additionally it provides a way to include similarity metrics into contrastive methods.

However, the results showed minor improvements in performance from SimCLR. It is also prudent to compare the method with other SoTA self-supervised learning methods that learn fine-grained granular features like MUlti-Granular Self-supervised learning (Mugs). We advise the authors to do this as it will help measure the effect of using similarity metrics in this particular loss formulation on performance. 
","The work is presented clearly with necessary introduction of current work and explanation on how it builds upon the same. The contribution is novel and original as it bridges a previously unexplored bridge between contrastive learning and graph theoretical approaches and semi-definite programming. The paper provides all analytical proofs stated. It also provides code and selection of hyperparameters to help with reproducibility. However, the paper does not refer to any previous runs / logbooks of trails. This will provide a way for readers to understand the way the problem was approached.",The paper provides an contribution of bridging the field of contrastive learning with Lovasz theta functions and graph theory. We think it will pave the way for incorporating more similarity metrics into contrastive learning methods. The paper provides excellent explanations of contrastive learning functions therefore it is valuable for a wide audience. ,"4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",4: All of the claims and statements are well-supported and correct.,"4: The contributions are significant, and do not exist in prior works.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666663278188,,-hWhz9xfrB9,ICLR.cc/2023/Conference/Paper5496/-/Official_Review,,[],2,,['everyone'],-hWhz9xfrB9,['ICLR.cc/2023/Conference/Paper5496/Reviewer_4DeW'],1666663278188,,1671036075325,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5496/Reviewer_4DeW']","Yes, the peer review does suggest the authors of the paper to refer to specific literature that may not be already discussed in the original paper. Specifically, the reviewer mentions ""MUlti-Granular Self-supervised learning (Mugs)"" as a state-of-the-art (SoTA) self-supervised learning method that learns fine-grained granular features, and advises the authors to compare their method with it to measure the effect of using similarity metrics in their loss formulation on performance. This implies that ""Mugs"" may not be already referenced in the original paper.",1,,MUlti-Granular Self-supervised learning (Mugs)
ZyxMb_5XyL,"The authors established a connection between contrastive learning and the Lovasz theta problem. They show that with certain assumption the original loss function corresponds to Lovasz theta with an empty similarity graph. Then, by considering Lovasz theta on weighted graphs, they designed a new loss function that incoporates prior knowledge of similarity between examples. Experiements are performed to show that their method can be used to improve both unsupervised and supervised contrastive learning. ","Strength:
1. The paper provides a new view of contrastive learning by showing its connection to the Lovasz theta problem, which may help further the understanding of contrastive learning.
2. The proposed loss can be used to boost contrastive learning with any prior knowledge on similarity between examples. 
3. The proposed method can be seen as a new way of using information from pretrained models and transfering the useful knowledge in them to new relevant tasks in the context of contrastive learning. It is more flexible than finetuning since it does not require the model architecture to be the same as the pretrained one. 

Weakness:
1. Theorem 4.1 only shows that the negative pair part in the loss is equivalent to Lovasz theta. The positive pair part is nullified by the assumption of the single positive case. This makes the result less informative considering that the positive part can play an important role. For example, [1] has shown that it encourages alignment between positive pairs and has certain interaction with the negative pair part which corresponds to uniformity. 
2. It is unclear what the quality of representations given by CLIP is. I think most likely it would not be very high, but it is still important to clarify since otherwise there is always a possibility that the CLIP representations are already good enough and it is trivial to beat the baseline by aligning the similarity output by the model with the good-enough representations'. 
3. More clarification is needed on the confusion matrix used in experiments. The authors said it is given by a model pretrained using supcon. But what is the setup for the pretraining and how long has the model been trained? what's the linear evaluation accuracy on its representations? Without knowing the answers to the above questions, it is hard to judge if the corresponding experimental results are meaningful. 

[1] Wang, Tongzhou, and Phillip Isola. ""Understanding contrastive representation learning through alignment and uniformity on the hypersphere."" International Conference on Machine Learning. PMLR, 2020.","Clarity:
1. Based on the equivalence to Lovasz theta, I wonder if more can be said about the learned representations, e.g., the geometry of learned representations and even their effect on generalization in downstream task.
2. Clarification on the quality of CLIP representations and confusion matrix is needed.

Novelty:
1. The main insights of the connection between contrastive learning and Lovasz theta, and leveraging the connection to derive a generalized version of loss are novel and can be important to the field. The idea of incoporating prior knowledge of similarity into contrastive learning is also novel. 

Seems the proposed loss function explicitly encourages non-uniform pair-wise similarity. It would also be interesting to see if this can help address some other problems in supervised contrastive learning, e.g., class-collapse.","The contribution of the paper is two fold (1) it showed the connection between contrastive learning and Lovasz theta, which is a novel viewpoint of contrastive learning (2) and subsequently proposed a loss function that incoporates prior knowledge of similarity. However, the connection to Lovasz theta doesn't yet provide much insights into properties of the learned representations, since there is no further discussion about this in the paper. This may limit its theoretical contribution. Some details are missing in the experiment section. Also, I wonder if one should also expect that adaptively adding similarity information obtaind from the representations given by the current model every certain epochs during training can also improve the performance (i.e., training -> updating similarity matrix using current representations -> training -> updating -> training ...)","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.","4: The contributions are significant, and do not exist in prior works.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666689859522,,-hWhz9xfrB9,ICLR.cc/2023/Conference/Paper5496/-/Official_Review,,[],3,,['everyone'],-hWhz9xfrB9,['ICLR.cc/2023/Conference/Paper5496/Reviewer_NuU3'],1666689859522,,1666689859522,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper5496/Reviewer_NuU3']","Yes, this peer review suggests the authors of the paper to refer to specific literature that is not already discussed in the original paper. The reviewer mentions a paper by Wang and Isola (2020) titled ""Understanding contrastive representation learning through alignment and uniformity on the hypersphere"" which is referenced as [1] in the review. This paper is not mentioned in the original summary of the paper, suggesting that the reviewer is introducing new literature that the authors may not have considered.",1,2020,Understanding contrastive representation learning through alignment and uniformity on the hypersphere
m1ESFyQODfS,"Detecting OOD data is critical to build reliable machine learning systems, where the models should make reliable predictions for ID data meanwhile detecting OOD data without further predictions. It motivates the recent studies in OOD detection, which has attracted significant attentions recently. The authors propose a novel method named Reactivate Gradnorm, which exploits the norm of clipped feature vector and the energy in the output space for OOD detection. The authors conducted experiments on ImageNet, and the results demonstrate their superiority over the state-of-the-art approaches. ","- The authors adopt the gradient information in discerning ID and OOD data. I also believe it is a very important line of research in OOD detection. However, to me, it is not crystal clear *why the proposed method can be superior over previous works in using gradient information*. Especially, the authors claim that the existing works still suffer from the problem of overconfidence. So, two natural questions are 1) why previous works are suffer from overconfident issue and 2) why the proposed method can mitigate such overconfidence. I think the related discussion can be critical, but I am afraid that I cannot find much useful information throughout the paper. 

- *The novelty of this paper is limited*. The authors followed previous works [1,2] in calculating the gradient norm of the last layer without back propagation (Eq. 6), and their choices of U and V also follow the previous works in energy scoring [3] and ReAct [4]. So, I think the authors directly combine the several advanced works in previous studies (actually, there are advanced works try to combine different scoring strategies for the improved detection capability of the model [5]), and I am not sure what makes such combination indispensable (for example, one can also change the ReAct by the L2 norm of logit features, so why the authors prefer the ReAct over other advanced scoring strategies). 

[1] Conor Igoe, et al. How Useful are Gradients for OOD detection Really? 2022. 

[2] Rui Huang, et al. One the Importance of Gradients for Detecting Distribution Shifts in the Wild. NeurIPS, 2022. 

[3] Weitang Liu, et al. Energy-based Out-of-distribution Detection. NeurIPS, 2020. 

[4] Yiyou Sun, et al. ReAct: Out-of-distribution Detection with Rectified Activations. NeurIPS, 2021. 

[5] Hihoon Tack, et al. CSI: Novelty Detection via Contrastive Learning on DIstributionally Shifted Instances. NeurIPS, 2020. 

- The paper is motivated by the GradNorm, but *the finally adopted method does not involve much about the gradient information*. So, I am not sure if the addition-based combination can benefit from the gradient information of the model. It will be great if the authors could formally connect the addition-based combination and the GradNorm in math language. 

- The authors conduct experiments on ImageNet benchmark, which is a challenging setup with large semantic space and very complex data features. However, *the comparison with some advanced methods (e.g., [6,7]) and the experiments on Hard OOD detection (e.g., CIFAR-10 vs. CIFAR-100, cf., [6]) are missing*. To fully justify the effectiveness of the proposal, I am afraid the authors should conduct more experiments. 

[6] Yiyou Sun, et al., Out-of-distribution Detection with Deep Nearest Neighbors. ICML, 2022. 

[7] Xuefeng Du, et al. VOS: Learning What You Don't Know by Virtual Outlier Synthesis. ICLR, 2022. 

","The quality and the clarity of writing are satisfactory. The Novelty may be limited to some extent. Moreover, I did not check the reproducibility. ","Using gradient information in OOD detection is an interesting direction. However, to improve the quality of the paper, I think the authors can address my concerns. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666504411073,,-hMNEMgT8Wd,ICLR.cc/2023/Conference/Paper6363/-/Official_Review,,[],3,,['everyone'],-hMNEMgT8Wd,['ICLR.cc/2023/Conference/Paper6363/Reviewer_7gKP'],1666504411073,,1666504411073,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6363/Reviewer_7gKP']","Yes, the peer review suggests the authors to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([6] and [7]) that are not mentioned in the original paper, specifically:

* [6] Yiyou Sun, et al., Out-of-distribution Detection with Deep Nearest Neighbors. ICML, 2022.
* [7] Xuefeng Du, et al. VOS: Learning What You Don't Know by Virtual Outlier Synthesis. ICLR, 2022.

The reviewer suggests that the authors should compare their method with these advanced methods and conduct experiments on Hard OOD detection, such as CIFAR-10 vs. CIFAR-100, to fully justify the effectiveness of their proposal.",1,"2022, 2022",Out-of-distribution Detection with Deep Nearest Neighbors 
m1ESFyQODfS,"Detecting OOD data is critical to build reliable machine learning systems, where the models should make reliable predictions for ID data meanwhile detecting OOD data without further predictions. It motivates the recent studies in OOD detection, which has attracted significant attentions recently. The authors propose a novel method named Reactivate Gradnorm, which exploits the norm of clipped feature vector and the energy in the output space for OOD detection. The authors conducted experiments on ImageNet, and the results demonstrate their superiority over the state-of-the-art approaches. ","- The authors adopt the gradient information in discerning ID and OOD data. I also believe it is a very important line of research in OOD detection. However, to me, it is not crystal clear *why the proposed method can be superior over previous works in using gradient information*. Especially, the authors claim that the existing works still suffer from the problem of overconfidence. So, two natural questions are 1) why previous works are suffer from overconfident issue and 2) why the proposed method can mitigate such overconfidence. I think the related discussion can be critical, but I am afraid that I cannot find much useful information throughout the paper. 

- *The novelty of this paper is limited*. The authors followed previous works [1,2] in calculating the gradient norm of the last layer without back propagation (Eq. 6), and their choices of U and V also follow the previous works in energy scoring [3] and ReAct [4]. So, I think the authors directly combine the several advanced works in previous studies (actually, there are advanced works try to combine different scoring strategies for the improved detection capability of the model [5]), and I am not sure what makes such combination indispensable (for example, one can also change the ReAct by the L2 norm of logit features, so why the authors prefer the ReAct over other advanced scoring strategies). 

[1] Conor Igoe, et al. How Useful are Gradients for OOD detection Really? 2022. 

[2] Rui Huang, et al. One the Importance of Gradients for Detecting Distribution Shifts in the Wild. NeurIPS, 2022. 

[3] Weitang Liu, et al. Energy-based Out-of-distribution Detection. NeurIPS, 2020. 

[4] Yiyou Sun, et al. ReAct: Out-of-distribution Detection with Rectified Activations. NeurIPS, 2021. 

[5] Hihoon Tack, et al. CSI: Novelty Detection via Contrastive Learning on DIstributionally Shifted Instances. NeurIPS, 2020. 

- The paper is motivated by the GradNorm, but *the finally adopted method does not involve much about the gradient information*. So, I am not sure if the addition-based combination can benefit from the gradient information of the model. It will be great if the authors could formally connect the addition-based combination and the GradNorm in math language. 

- The authors conduct experiments on ImageNet benchmark, which is a challenging setup with large semantic space and very complex data features. However, *the comparison with some advanced methods (e.g., [6,7]) and the experiments on Hard OOD detection (e.g., CIFAR-10 vs. CIFAR-100, cf., [6]) are missing*. To fully justify the effectiveness of the proposal, I am afraid the authors should conduct more experiments. 

[6] Yiyou Sun, et al., Out-of-distribution Detection with Deep Nearest Neighbors. ICML, 2022. 

[7] Xuefeng Du, et al. VOS: Learning What You Don't Know by Virtual Outlier Synthesis. ICLR, 2022. 

","The quality and the clarity of writing are satisfactory. The Novelty may be limited to some extent. Moreover, I did not check the reproducibility. ","Using gradient information in OOD detection is an interesting direction. However, to improve the quality of the paper, I think the authors can address my concerns. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666504411073,,-hMNEMgT8Wd,ICLR.cc/2023/Conference/Paper6363/-/Official_Review,,[],3,,['everyone'],-hMNEMgT8Wd,['ICLR.cc/2023/Conference/Paper6363/Reviewer_7gKP'],1666504411073,,1666504411073,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper6363/Reviewer_7gKP']","Yes, the peer review suggests the authors to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([6] and [7]) that are not mentioned in the original paper, specifically:

* [6] Yiyou Sun, et al., Out-of-distribution Detection with Deep Nearest Neighbors. ICML, 2022.
* [7] Xuefeng Du, et al. VOS: Learning What You Don't Know by Virtual Outlier Synthesis. ICLR, 2022.

The reviewer suggests that the authors should compare their method with these advanced methods and conduct experiments on Hard OOD detection, such as CIFAR-10 vs. CIFAR-100, to fully justify the effectiveness of their proposal.",1,"2022, 2022",VOS Learning What You Don't Know by Virtual Outlier Synthesis
orn5yE_tSso,"This work proposes a new method for cross-lingual and multilingual dense retrieval, focusing on applications of query-passage and sentence retrieval in multilingual setups. The approach relies on parallel document-aligned and assumes that the sentences in such documents are roughly in the same order; the idea is then to combine the standard MLM objective with its bilingual sentence-level variant, termed Masked Sentence Modeling (MSM), hoping to guess the correct sentence (once the sentence is masked in the document), where the document encoder is shared across languages. 

The experiments are conducted on four retrieval tasks from prior work, and gains over (mostly) weak baselines are reported.","Strengths:
- The idea of MSM is simple and neat, although it is very similar to the idea of next sentence prediction - the difference here is that the model performs masked sentence prediction.
- The ablation study shows the usefulness of introducing the MSM objective.

The paper is simply not at a state to be considered for publication, with a series of major flaws as follows.
Weaknesses:
- Only a partial awareness of very related work on cross-lingual information retrieval (CLIR), with many strong reference works and baselines omitted from the paper completely and omitted from the comparisons. Here, only a few directly relevant papers are mentioned:
-- https://arxiv.org/pdf/2112.09118.pdf
-- https://arxiv.org/abs/2204.02292
-- https://arxiv.org/abs/2004.09813
-- https://arxiv.org/pdf/2101.08370.pdf
-- There is also work on multilingual Longformers (e.g., https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096)
- Related to the point above, those papers provide much stronger baselines - the baselines in this submission are simply weak and inadequate. When doing sentence retrieval, the paper should compare against strong multilingual sentence encoders and not the original off-the-shelf models. 
- The paper also does conflate query-passage and sentence retrieval, and does not evaluate on document retrieval at all. There are huge differences on how to approach each 'granularity of information' when doing retrieval, and the paper does not seem to pay attention to that: e.g., check this work for further details: https://arxiv.org/pdf/2101.08370.pdf
- The paper also critically requires parallel data to work -> if one has parallel data, one of the must-have baselines are also MT-based query-translate or passage-translate approaches which sometimes/often work better than standard encoder-based approaches.
- There are no discussions on how different target languages might impact the results: are all the languages equally difficult, which ones might cause major drops of performance and, most importantly, why? The paper treats multilinguality very superficially. 

There are other (minor) weaknesses, including problems with language and presentation, but the major ones are mostly listed above.
","The paper will not have any major impact as it omits many major baselines and a lot of very relevant work, offering only basic comparisons and lacking insightful side analyses. It makes a minor methodological contribution by combining document-level and sentence-level masked language modeling, which is not evaluated against cutting-edge CLIR methods.

It should be possible to reproduce the main results in the paper - it does not mention whether the results are average over several random seeds or not (and which random seed was used).","The paper lacks strong baselines, shows only partial awareness of the current cutting-edge CLIR methodology. and it is difficult to contextualise its results (does it really bring any major improvement for CLIR?). It also does not delve deeper into intricacies of multilinguality and differences between sentence/passage/document retrieval. There are also presentation problems which make the paper seem incomplete and half-finished ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666868766708,,-bVsNeR56KS,ICLR.cc/2023/Conference/Paper1622/-/Official_Review,,[],4,,['everyone'],-bVsNeR56KS,['ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe'],1666868766708,,1666868766708,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers and resources that are not included in the paper, such as:

* https://arxiv.org/pdf/2112.09118.pdf
* https://arxiv.org/abs/2204.02292
* https://arxiv.org/abs/2004.09813
* https://arxiv.org/pdf/2101.08370.pdf
* https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096

The reviewer also mentions that the paper should compare against strong multilingual sentence encoders and MT-based query-translate or passage-translate approaches, which are not adequately discussed in the paper. This implies that the reviewer is aware of other relevant literature that is not included in the paper, and is suggesting that the authors should engage with this literature to strengthen their work.",1,"2021, 2022, 2020, 2021",Unsupervised Dense Information Retrieval with Contrastive Learning
orn5yE_tSso,"This work proposes a new method for cross-lingual and multilingual dense retrieval, focusing on applications of query-passage and sentence retrieval in multilingual setups. The approach relies on parallel document-aligned and assumes that the sentences in such documents are roughly in the same order; the idea is then to combine the standard MLM objective with its bilingual sentence-level variant, termed Masked Sentence Modeling (MSM), hoping to guess the correct sentence (once the sentence is masked in the document), where the document encoder is shared across languages. 

The experiments are conducted on four retrieval tasks from prior work, and gains over (mostly) weak baselines are reported.","Strengths:
- The idea of MSM is simple and neat, although it is very similar to the idea of next sentence prediction - the difference here is that the model performs masked sentence prediction.
- The ablation study shows the usefulness of introducing the MSM objective.

The paper is simply not at a state to be considered for publication, with a series of major flaws as follows.
Weaknesses:
- Only a partial awareness of very related work on cross-lingual information retrieval (CLIR), with many strong reference works and baselines omitted from the paper completely and omitted from the comparisons. Here, only a few directly relevant papers are mentioned:
-- https://arxiv.org/pdf/2112.09118.pdf
-- https://arxiv.org/abs/2204.02292
-- https://arxiv.org/abs/2004.09813
-- https://arxiv.org/pdf/2101.08370.pdf
-- There is also work on multilingual Longformers (e.g., https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096)
- Related to the point above, those papers provide much stronger baselines - the baselines in this submission are simply weak and inadequate. When doing sentence retrieval, the paper should compare against strong multilingual sentence encoders and not the original off-the-shelf models. 
- The paper also does conflate query-passage and sentence retrieval, and does not evaluate on document retrieval at all. There are huge differences on how to approach each 'granularity of information' when doing retrieval, and the paper does not seem to pay attention to that: e.g., check this work for further details: https://arxiv.org/pdf/2101.08370.pdf
- The paper also critically requires parallel data to work -> if one has parallel data, one of the must-have baselines are also MT-based query-translate or passage-translate approaches which sometimes/often work better than standard encoder-based approaches.
- There are no discussions on how different target languages might impact the results: are all the languages equally difficult, which ones might cause major drops of performance and, most importantly, why? The paper treats multilinguality very superficially. 

There are other (minor) weaknesses, including problems with language and presentation, but the major ones are mostly listed above.
","The paper will not have any major impact as it omits many major baselines and a lot of very relevant work, offering only basic comparisons and lacking insightful side analyses. It makes a minor methodological contribution by combining document-level and sentence-level masked language modeling, which is not evaluated against cutting-edge CLIR methods.

It should be possible to reproduce the main results in the paper - it does not mention whether the results are average over several random seeds or not (and which random seed was used).","The paper lacks strong baselines, shows only partial awareness of the current cutting-edge CLIR methodology. and it is difficult to contextualise its results (does it really bring any major improvement for CLIR?). It also does not delve deeper into intricacies of multilinguality and differences between sentence/passage/document retrieval. There are also presentation problems which make the paper seem incomplete and half-finished ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666868766708,,-bVsNeR56KS,ICLR.cc/2023/Conference/Paper1622/-/Official_Review,,[],4,,['everyone'],-bVsNeR56KS,['ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe'],1666868766708,,1666868766708,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers and resources that are not included in the paper, such as:

* https://arxiv.org/pdf/2112.09118.pdf
* https://arxiv.org/abs/2204.02292
* https://arxiv.org/abs/2004.09813
* https://arxiv.org/pdf/2101.08370.pdf
* https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096

The reviewer also mentions that the paper should compare against strong multilingual sentence encoders and MT-based query-translate or passage-translate approaches, which are not adequately discussed in the paper. This implies that the reviewer is aware of other relevant literature that is not included in the paper, and is suggesting that the authors should engage with this literature to strengthen their work.",1,"2021, 2022, 2020, 2021",Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval
orn5yE_tSso,"This work proposes a new method for cross-lingual and multilingual dense retrieval, focusing on applications of query-passage and sentence retrieval in multilingual setups. The approach relies on parallel document-aligned and assumes that the sentences in such documents are roughly in the same order; the idea is then to combine the standard MLM objective with its bilingual sentence-level variant, termed Masked Sentence Modeling (MSM), hoping to guess the correct sentence (once the sentence is masked in the document), where the document encoder is shared across languages. 

The experiments are conducted on four retrieval tasks from prior work, and gains over (mostly) weak baselines are reported.","Strengths:
- The idea of MSM is simple and neat, although it is very similar to the idea of next sentence prediction - the difference here is that the model performs masked sentence prediction.
- The ablation study shows the usefulness of introducing the MSM objective.

The paper is simply not at a state to be considered for publication, with a series of major flaws as follows.
Weaknesses:
- Only a partial awareness of very related work on cross-lingual information retrieval (CLIR), with many strong reference works and baselines omitted from the paper completely and omitted from the comparisons. Here, only a few directly relevant papers are mentioned:
-- https://arxiv.org/pdf/2112.09118.pdf
-- https://arxiv.org/abs/2204.02292
-- https://arxiv.org/abs/2004.09813
-- https://arxiv.org/pdf/2101.08370.pdf
-- There is also work on multilingual Longformers (e.g., https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096)
- Related to the point above, those papers provide much stronger baselines - the baselines in this submission are simply weak and inadequate. When doing sentence retrieval, the paper should compare against strong multilingual sentence encoders and not the original off-the-shelf models. 
- The paper also does conflate query-passage and sentence retrieval, and does not evaluate on document retrieval at all. There are huge differences on how to approach each 'granularity of information' when doing retrieval, and the paper does not seem to pay attention to that: e.g., check this work for further details: https://arxiv.org/pdf/2101.08370.pdf
- The paper also critically requires parallel data to work -> if one has parallel data, one of the must-have baselines are also MT-based query-translate or passage-translate approaches which sometimes/often work better than standard encoder-based approaches.
- There are no discussions on how different target languages might impact the results: are all the languages equally difficult, which ones might cause major drops of performance and, most importantly, why? The paper treats multilinguality very superficially. 

There are other (minor) weaknesses, including problems with language and presentation, but the major ones are mostly listed above.
","The paper will not have any major impact as it omits many major baselines and a lot of very relevant work, offering only basic comparisons and lacking insightful side analyses. It makes a minor methodological contribution by combining document-level and sentence-level masked language modeling, which is not evaluated against cutting-edge CLIR methods.

It should be possible to reproduce the main results in the paper - it does not mention whether the results are average over several random seeds or not (and which random seed was used).","The paper lacks strong baselines, shows only partial awareness of the current cutting-edge CLIR methodology. and it is difficult to contextualise its results (does it really bring any major improvement for CLIR?). It also does not delve deeper into intricacies of multilinguality and differences between sentence/passage/document retrieval. There are also presentation problems which make the paper seem incomplete and half-finished ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666868766708,,-bVsNeR56KS,ICLR.cc/2023/Conference/Paper1622/-/Official_Review,,[],4,,['everyone'],-bVsNeR56KS,['ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe'],1666868766708,,1666868766708,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers and resources that are not included in the paper, such as:

* https://arxiv.org/pdf/2112.09118.pdf
* https://arxiv.org/abs/2204.02292
* https://arxiv.org/abs/2004.09813
* https://arxiv.org/pdf/2101.08370.pdf
* https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096

The reviewer also mentions that the paper should compare against strong multilingual sentence encoders and MT-based query-translate or passage-translate approaches, which are not adequately discussed in the paper. This implies that the reviewer is aware of other relevant literature that is not included in the paper, and is suggesting that the authors should engage with this literature to strengthen their work.",1,"2021, 2022, 2020, 2021",Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation
orn5yE_tSso,"This work proposes a new method for cross-lingual and multilingual dense retrieval, focusing on applications of query-passage and sentence retrieval in multilingual setups. The approach relies on parallel document-aligned and assumes that the sentences in such documents are roughly in the same order; the idea is then to combine the standard MLM objective with its bilingual sentence-level variant, termed Masked Sentence Modeling (MSM), hoping to guess the correct sentence (once the sentence is masked in the document), where the document encoder is shared across languages. 

The experiments are conducted on four retrieval tasks from prior work, and gains over (mostly) weak baselines are reported.","Strengths:
- The idea of MSM is simple and neat, although it is very similar to the idea of next sentence prediction - the difference here is that the model performs masked sentence prediction.
- The ablation study shows the usefulness of introducing the MSM objective.

The paper is simply not at a state to be considered for publication, with a series of major flaws as follows.
Weaknesses:
- Only a partial awareness of very related work on cross-lingual information retrieval (CLIR), with many strong reference works and baselines omitted from the paper completely and omitted from the comparisons. Here, only a few directly relevant papers are mentioned:
-- https://arxiv.org/pdf/2112.09118.pdf
-- https://arxiv.org/abs/2204.02292
-- https://arxiv.org/abs/2004.09813
-- https://arxiv.org/pdf/2101.08370.pdf
-- There is also work on multilingual Longformers (e.g., https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096)
- Related to the point above, those papers provide much stronger baselines - the baselines in this submission are simply weak and inadequate. When doing sentence retrieval, the paper should compare against strong multilingual sentence encoders and not the original off-the-shelf models. 
- The paper also does conflate query-passage and sentence retrieval, and does not evaluate on document retrieval at all. There are huge differences on how to approach each 'granularity of information' when doing retrieval, and the paper does not seem to pay attention to that: e.g., check this work for further details: https://arxiv.org/pdf/2101.08370.pdf
- The paper also critically requires parallel data to work -> if one has parallel data, one of the must-have baselines are also MT-based query-translate or passage-translate approaches which sometimes/often work better than standard encoder-based approaches.
- There are no discussions on how different target languages might impact the results: are all the languages equally difficult, which ones might cause major drops of performance and, most importantly, why? The paper treats multilinguality very superficially. 

There are other (minor) weaknesses, including problems with language and presentation, but the major ones are mostly listed above.
","The paper will not have any major impact as it omits many major baselines and a lot of very relevant work, offering only basic comparisons and lacking insightful side analyses. It makes a minor methodological contribution by combining document-level and sentence-level masked language modeling, which is not evaluated against cutting-edge CLIR methods.

It should be possible to reproduce the main results in the paper - it does not mention whether the results are average over several random seeds or not (and which random seed was used).","The paper lacks strong baselines, shows only partial awareness of the current cutting-edge CLIR methodology. and it is difficult to contextualise its results (does it really bring any major improvement for CLIR?). It also does not delve deeper into intricacies of multilinguality and differences between sentence/passage/document retrieval. There are also presentation problems which make the paper seem incomplete and half-finished ",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666868766708,,-bVsNeR56KS,ICLR.cc/2023/Conference/Paper1622/-/Official_Review,,[],4,,['everyone'],-bVsNeR56KS,['ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe'],1666868766708,,1666868766708,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1622/Reviewer_XpJe']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers and resources that are not included in the paper, such as:

* https://arxiv.org/pdf/2112.09118.pdf
* https://arxiv.org/abs/2204.02292
* https://arxiv.org/abs/2004.09813
* https://arxiv.org/pdf/2101.08370.pdf
* https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096

The reviewer also mentions that the paper should compare against strong multilingual sentence encoders and MT-based query-translate or passage-translate approaches, which are not adequately discussed in the paper. This implies that the reviewer is aware of other relevant literature that is not included in the paper, and is suggesting that the authors should engage with this literature to strengthen their work.",1,"2021, 2022, 2020, 2021",Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual Retrieval
Yb2ikQuadh,"This paper studies the efficient training of large language models on heterogeneous and preemptive instances.
It proposes SWARM parallelism and finds that training can be made communication-efficient with SWAM parallelism.
Evaluation of a 13B model shows that the system achieves good training throughput and convergence.
","Strength:
- This paper studies an important and unsolved problem: training LLM outside HPC clusters with heterogeneous and preemptive instances.
- The paper proposes a novel and interesting solution: SWARM parallelism, which is decentralized and highly adaptive.
- Solid experiments with throughput and convergence analysis

Weaknesses:
- This paper uses too many approximate/lossy/asynchronous methods, which can harm the accuracy.
- Stochastic wiring and adaptive rebalancing are local greedy adjustment approaches. However, I think if we know the computing speed and memory capacity of all devices, an optimal placement/routing can be computed. [1][2]
- Section 4.2 's comparison is a little bit unfair. In a typical HPC cluster, there are typically multiple GPUs on a single node connected by high-speed NVLink. Under this more realistic setting, ZeRO Offload will perform better. If you look at the results in [3], they can achieve linear scaling on HPC clusters. The authors should compare against SOTA performance on the HPC cluster when they talk about ""ideal condition"".

[1] Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning, OSDI 2022  
[2] Piper: Multidimensional Planner for DNN Parallelization, NeurIPS 2021  
[3] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

Minor:
Missing a discussion of a recent and highly related paper: Decentralized Training of Foundation Models in Heterogeneous Environments (https://arxiv.org/abs/2206.01288)
","The paper is well written with enough background information.
The proposed methods are novel.
I believe most experiments can be reproduced. The code is available.
","This paper proposes an interesting solution to an important problem. I am leaning toward acceptance.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",1: The contributions are neither significant nor novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666690831165,,-azium0cV9,ICLR.cc/2023/Conference/Paper2619/-/Official_Review,,[],1,,['everyone'],-azium0cV9,['ICLR.cc/2023/Conference/Paper2619/Reviewer_8Ywq'],1666690831165,,1666704279090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2619/Reviewer_8Ywq']","Yes, this peer review suggests that the authors of the paper refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are likely not already included in the original paper:

1. Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning, OSDI 2022
2. Piper: Multidimensional Planner for DNN Parallelization, NeurIPS 2021
3. Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
4. Decentralized Training of Foundation Models in Heterogeneous Environments (https://arxiv.org/abs/2206.01288)

The reviewer explicitly states that the last paper is ""Missing a discussion of a recent and highly related paper"", indicating that it is not already discussed in the original paper. The other papers are mentioned as references to support the reviewer's comments, but it is likely that they are not already included in the original paper.",1,"2022, 2021, 2022",Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning 
Yb2ikQuadh,"This paper studies the efficient training of large language models on heterogeneous and preemptive instances.
It proposes SWARM parallelism and finds that training can be made communication-efficient with SWAM parallelism.
Evaluation of a 13B model shows that the system achieves good training throughput and convergence.
","Strength:
- This paper studies an important and unsolved problem: training LLM outside HPC clusters with heterogeneous and preemptive instances.
- The paper proposes a novel and interesting solution: SWARM parallelism, which is decentralized and highly adaptive.
- Solid experiments with throughput and convergence analysis

Weaknesses:
- This paper uses too many approximate/lossy/asynchronous methods, which can harm the accuracy.
- Stochastic wiring and adaptive rebalancing are local greedy adjustment approaches. However, I think if we know the computing speed and memory capacity of all devices, an optimal placement/routing can be computed. [1][2]
- Section 4.2 's comparison is a little bit unfair. In a typical HPC cluster, there are typically multiple GPUs on a single node connected by high-speed NVLink. Under this more realistic setting, ZeRO Offload will perform better. If you look at the results in [3], they can achieve linear scaling on HPC clusters. The authors should compare against SOTA performance on the HPC cluster when they talk about ""ideal condition"".

[1] Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning, OSDI 2022  
[2] Piper: Multidimensional Planner for DNN Parallelization, NeurIPS 2021  
[3] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

Minor:
Missing a discussion of a recent and highly related paper: Decentralized Training of Foundation Models in Heterogeneous Environments (https://arxiv.org/abs/2206.01288)
","The paper is well written with enough background information.
The proposed methods are novel.
I believe most experiments can be reproduced. The code is available.
","This paper proposes an interesting solution to an important problem. I am leaning toward acceptance.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",1: The contributions are neither significant nor novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666690831165,,-azium0cV9,ICLR.cc/2023/Conference/Paper2619/-/Official_Review,,[],1,,['everyone'],-azium0cV9,['ICLR.cc/2023/Conference/Paper2619/Reviewer_8Ywq'],1666690831165,,1666704279090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2619/Reviewer_8Ywq']","Yes, this peer review suggests that the authors of the paper refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are likely not already included in the original paper:

1. Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning, OSDI 2022
2. Piper: Multidimensional Planner for DNN Parallelization, NeurIPS 2021
3. Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
4. Decentralized Training of Foundation Models in Heterogeneous Environments (https://arxiv.org/abs/2206.01288)

The reviewer explicitly states that the last paper is ""Missing a discussion of a recent and highly related paper"", indicating that it is not already discussed in the original paper. The other papers are mentioned as references to support the reviewer's comments, but it is likely that they are not already included in the original paper.",1,"2022, 2021, 2022",Piper: Multidimensional Planner for DNN Parallelization 
Yb2ikQuadh,"This paper studies the efficient training of large language models on heterogeneous and preemptive instances.
It proposes SWARM parallelism and finds that training can be made communication-efficient with SWAM parallelism.
Evaluation of a 13B model shows that the system achieves good training throughput and convergence.
","Strength:
- This paper studies an important and unsolved problem: training LLM outside HPC clusters with heterogeneous and preemptive instances.
- The paper proposes a novel and interesting solution: SWARM parallelism, which is decentralized and highly adaptive.
- Solid experiments with throughput and convergence analysis

Weaknesses:
- This paper uses too many approximate/lossy/asynchronous methods, which can harm the accuracy.
- Stochastic wiring and adaptive rebalancing are local greedy adjustment approaches. However, I think if we know the computing speed and memory capacity of all devices, an optimal placement/routing can be computed. [1][2]
- Section 4.2 's comparison is a little bit unfair. In a typical HPC cluster, there are typically multiple GPUs on a single node connected by high-speed NVLink. Under this more realistic setting, ZeRO Offload will perform better. If you look at the results in [3], they can achieve linear scaling on HPC clusters. The authors should compare against SOTA performance on the HPC cluster when they talk about ""ideal condition"".

[1] Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning, OSDI 2022  
[2] Piper: Multidimensional Planner for DNN Parallelization, NeurIPS 2021  
[3] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

Minor:
Missing a discussion of a recent and highly related paper: Decentralized Training of Foundation Models in Heterogeneous Environments (https://arxiv.org/abs/2206.01288)
","The paper is well written with enough background information.
The proposed methods are novel.
I believe most experiments can be reproduced. The code is available.
","This paper proposes an interesting solution to an important problem. I am leaning toward acceptance.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",1: The contributions are neither significant nor novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666690831165,,-azium0cV9,ICLR.cc/2023/Conference/Paper2619/-/Official_Review,,[],1,,['everyone'],-azium0cV9,['ICLR.cc/2023/Conference/Paper2619/Reviewer_8Ywq'],1666690831165,,1666704279090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2619/Reviewer_8Ywq']","Yes, this peer review suggests that the authors of the paper refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are likely not already included in the original paper:

1. Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning, OSDI 2022
2. Piper: Multidimensional Planner for DNN Parallelization, NeurIPS 2021
3. Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
4. Decentralized Training of Foundation Models in Heterogeneous Environments (https://arxiv.org/abs/2206.01288)

The reviewer explicitly states that the last paper is ""Missing a discussion of a recent and highly related paper"", indicating that it is not already discussed in the original paper. The other papers are mentioned as references to support the reviewer's comments, but it is likely that they are not already included in the original paper.",1,"2022, 2021, 2022",Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM 
Yb2ikQuadh,"This paper studies the efficient training of large language models on heterogeneous and preemptive instances.
It proposes SWARM parallelism and finds that training can be made communication-efficient with SWAM parallelism.
Evaluation of a 13B model shows that the system achieves good training throughput and convergence.
","Strength:
- This paper studies an important and unsolved problem: training LLM outside HPC clusters with heterogeneous and preemptive instances.
- The paper proposes a novel and interesting solution: SWARM parallelism, which is decentralized and highly adaptive.
- Solid experiments with throughput and convergence analysis

Weaknesses:
- This paper uses too many approximate/lossy/asynchronous methods, which can harm the accuracy.
- Stochastic wiring and adaptive rebalancing are local greedy adjustment approaches. However, I think if we know the computing speed and memory capacity of all devices, an optimal placement/routing can be computed. [1][2]
- Section 4.2 's comparison is a little bit unfair. In a typical HPC cluster, there are typically multiple GPUs on a single node connected by high-speed NVLink. Under this more realistic setting, ZeRO Offload will perform better. If you look at the results in [3], they can achieve linear scaling on HPC clusters. The authors should compare against SOTA performance on the HPC cluster when they talk about ""ideal condition"".

[1] Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning, OSDI 2022  
[2] Piper: Multidimensional Planner for DNN Parallelization, NeurIPS 2021  
[3] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

Minor:
Missing a discussion of a recent and highly related paper: Decentralized Training of Foundation Models in Heterogeneous Environments (https://arxiv.org/abs/2206.01288)
","The paper is well written with enough background information.
The proposed methods are novel.
I believe most experiments can be reproduced. The code is available.
","This paper proposes an interesting solution to an important problem. I am leaning toward acceptance.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",1: The contributions are neither significant nor novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,"8: accept, good paper",1666690831165,,-azium0cV9,ICLR.cc/2023/Conference/Paper2619/-/Official_Review,,[],1,,['everyone'],-azium0cV9,['ICLR.cc/2023/Conference/Paper2619/Reviewer_8Ywq'],1666690831165,,1666704279090,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2619/Reviewer_8Ywq']","Yes, this peer review suggests that the authors of the paper refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers that are likely not already included in the original paper:

1. Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning, OSDI 2022
2. Piper: Multidimensional Planner for DNN Parallelization, NeurIPS 2021
3. Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
4. Decentralized Training of Foundation Models in Heterogeneous Environments (https://arxiv.org/abs/2206.01288)

The reviewer explicitly states that the last paper is ""Missing a discussion of a recent and highly related paper"", indicating that it is not already discussed in the original paper. The other papers are mentioned as references to support the reviewer's comments, but it is likely that they are not already included in the original paper.",1,"2022, 2021, 2022",Decentralized Training of Foundation Models in Heterogeneous Environments
phQTRhlFhA,"This paper proposes a method called “SWARM parallelism” to allow model-parallel training of large models on heterogeneous clusters (e.g., a mix of weak and strong GPUs, unreliable compute nodes or networks with uneven bandwidth). ","Strength:

- The problem this paper is trying to address is important.
- Overall the paper is easy to follow
- The proposed technique looks sensible.


Weakness:
- In writing, the paper seems to spend too much effort explaining the background or something obvious, instead of giving an in-depth study, empirical or theoretical, about the proposed methods.

- Some contributions claimed by this paper are questionable. For example, I doubt the so-called “square-cube” law is a contribution of this paper, because It has been studied quite intensively in several large LM system papers, and has been used in many papers published before this paper. This paper seems to just re-brand this common wisdom. Accordingly, I feel the writing in section 3.1 and the experiments in 4.1 are unnecessary and does not reveal any new insight to this area.

- What remains novel in this paper is the SWARM parallelism, which is indeed interesting and trying to address a very important problem. However, I feel the authors’ execution (such as in section 3.2 and in experiments) in explaining the proposed method and proving the method is practical could have been done better. I have some detailed comments provided next.
","The “square-cube” law proposed in this paper is nice but quite obvious to me; it has been discussed and revealed in many papers such as scaling language models [1]. Also, pipeline parallelism having lower communication (in this paper’s phrasing – “pipeline parallelism naturally grows more communication-efficient with model size”) had also been revealed in many previous papers such as [2][3][4][5].
Given that the “square-cube” laws seem to be just a summarization/re-branding of several obvious facts (which have been leveraged in many papers), I don’t think the contents in section 3.1 are novel or interesting. I am not sure if this can be counted as a contribution to this paper.

[1] Scaling Laws for Neural Language Models

[2] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

[3] Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning

[4] Piper: Multidimensional Planner for DNN Parallelization

Writing and clarity:

The writing of section 3.2 is rather ad-hoc. Is it possible to formalize a few symbols for each stage of the model and each device of the cluster, and develop an algorithm (and if possible, equations) to illustrate how exactly the stochastic wiring and rebalancing work?


Experiments:

In sec 4.2, the choices of baselines are very tricky. If the goal is to compare the performance in ideal conditions, why don’t you compare it to stronger and more practical baselines? Here are my concerns:
- Gpipe faces a peak memory issue, which limits the largest possible number of micro-batches it can use, which in turn limits its throughput because a smaller number of micro-batch causes bubbles. I think you should choose the 1F1B schedule as a baseline, which addresses this problem.
- For Zero, why don’t you choose Zero-2 or Zero-3 but Zero-offload? If the goal is to compare the performance in ideal condition, I guess Zero-2 and Zero-3 can almost always give better performance than Zero-offload (since zero-offload has offloading which would be a penalty on performance). This is also related to the cluster you choose to perform the experiments in sec 4.2 --- because I do notice that you have 7x nodes with 8 A100 each. I suppose the experiment in 4.2 should be performed on the A100 cluster (which has more GPU memory and make Zero-Offload unnecessary).


- In sec 4.3, could you elaborate on this sentence:  “with 1.01 billion parameters in total: because of layer sharing, it is equivalent to a 13B model from (Brown et al., 2020) in terms of computing requirements.” ?
I am wondering if this means the layer sharing you introduced makes your method more advantageous because it increases the compute-to-communication ratio (because this layer sharding seems to increase the flops needed per parameter). Could you perform experiments on a standard, smaller GPT-3 model without layer sharing and report the results?

- The results in Table 3 look good. But these results seem to be achieved by a combination of many techniques, including the proposed ones in this paper, and some compression techniques which can reduce communication. Is it possible to isolate these factors with an ablation study to show how much improvement your approaches exactly bring?

- Overall, I feel the experiments section should focus on how SWARM can handle heterogeneous devices, handle failures, and handle uneven bandwidth. The results presented in 4.1 is not interesting; the results presented in 4.2 is less relevant and do not contribute to many of the claims you have made earlier in the paper about SWARM. 
","The paper has proposed a nice and sensible idea to address the model-parallel training in a very heterogeneous cluster environment, but the paper does not provide enough evidence to prove the method is indeed practical and verify the author's many claims.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667371853067,,-azium0cV9,ICLR.cc/2023/Conference/Paper2619/-/Official_Review,,[],6,,['everyone'],-azium0cV9,['ICLR.cc/2023/Conference/Paper2619/Reviewer_uZBm'],1667371853067,,1667425240235,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2619/Reviewer_uZBm']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers, including:

1. ""Scaling Laws for Neural Language Models"" ([1])
2. ""Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"" ([2])
3. ""Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning"" ([3])
4. ""Piper: Multidimensional Planner for DNN Parallelization"" ([4])
5. A paper by Brown et al. (2020), which is not explicitly cited as a reference in the review, but is mentioned as a related work.

The reviewer also mentions specific techniques, such as Zero-2 and Zero-3, which may not be discussed in the original paper. However, it is possible that some of these references are already included in the original paper, but not explicitly mentioned in the review.",1,2020,Scaling Laws for Neural Language Models
phQTRhlFhA,"This paper proposes a method called “SWARM parallelism” to allow model-parallel training of large models on heterogeneous clusters (e.g., a mix of weak and strong GPUs, unreliable compute nodes or networks with uneven bandwidth). ","Strength:

- The problem this paper is trying to address is important.
- Overall the paper is easy to follow
- The proposed technique looks sensible.


Weakness:
- In writing, the paper seems to spend too much effort explaining the background or something obvious, instead of giving an in-depth study, empirical or theoretical, about the proposed methods.

- Some contributions claimed by this paper are questionable. For example, I doubt the so-called “square-cube” law is a contribution of this paper, because It has been studied quite intensively in several large LM system papers, and has been used in many papers published before this paper. This paper seems to just re-brand this common wisdom. Accordingly, I feel the writing in section 3.1 and the experiments in 4.1 are unnecessary and does not reveal any new insight to this area.

- What remains novel in this paper is the SWARM parallelism, which is indeed interesting and trying to address a very important problem. However, I feel the authors’ execution (such as in section 3.2 and in experiments) in explaining the proposed method and proving the method is practical could have been done better. I have some detailed comments provided next.
","The “square-cube” law proposed in this paper is nice but quite obvious to me; it has been discussed and revealed in many papers such as scaling language models [1]. Also, pipeline parallelism having lower communication (in this paper’s phrasing – “pipeline parallelism naturally grows more communication-efficient with model size”) had also been revealed in many previous papers such as [2][3][4][5].
Given that the “square-cube” laws seem to be just a summarization/re-branding of several obvious facts (which have been leveraged in many papers), I don’t think the contents in section 3.1 are novel or interesting. I am not sure if this can be counted as a contribution to this paper.

[1] Scaling Laws for Neural Language Models

[2] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

[3] Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning

[4] Piper: Multidimensional Planner for DNN Parallelization

Writing and clarity:

The writing of section 3.2 is rather ad-hoc. Is it possible to formalize a few symbols for each stage of the model and each device of the cluster, and develop an algorithm (and if possible, equations) to illustrate how exactly the stochastic wiring and rebalancing work?


Experiments:

In sec 4.2, the choices of baselines are very tricky. If the goal is to compare the performance in ideal conditions, why don’t you compare it to stronger and more practical baselines? Here are my concerns:
- Gpipe faces a peak memory issue, which limits the largest possible number of micro-batches it can use, which in turn limits its throughput because a smaller number of micro-batch causes bubbles. I think you should choose the 1F1B schedule as a baseline, which addresses this problem.
- For Zero, why don’t you choose Zero-2 or Zero-3 but Zero-offload? If the goal is to compare the performance in ideal condition, I guess Zero-2 and Zero-3 can almost always give better performance than Zero-offload (since zero-offload has offloading which would be a penalty on performance). This is also related to the cluster you choose to perform the experiments in sec 4.2 --- because I do notice that you have 7x nodes with 8 A100 each. I suppose the experiment in 4.2 should be performed on the A100 cluster (which has more GPU memory and make Zero-Offload unnecessary).


- In sec 4.3, could you elaborate on this sentence:  “with 1.01 billion parameters in total: because of layer sharing, it is equivalent to a 13B model from (Brown et al., 2020) in terms of computing requirements.” ?
I am wondering if this means the layer sharing you introduced makes your method more advantageous because it increases the compute-to-communication ratio (because this layer sharding seems to increase the flops needed per parameter). Could you perform experiments on a standard, smaller GPT-3 model without layer sharing and report the results?

- The results in Table 3 look good. But these results seem to be achieved by a combination of many techniques, including the proposed ones in this paper, and some compression techniques which can reduce communication. Is it possible to isolate these factors with an ablation study to show how much improvement your approaches exactly bring?

- Overall, I feel the experiments section should focus on how SWARM can handle heterogeneous devices, handle failures, and handle uneven bandwidth. The results presented in 4.1 is not interesting; the results presented in 4.2 is less relevant and do not contribute to many of the claims you have made earlier in the paper about SWARM. 
","The paper has proposed a nice and sensible idea to address the model-parallel training in a very heterogeneous cluster environment, but the paper does not provide enough evidence to prove the method is indeed practical and verify the author's many claims.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667371853067,,-azium0cV9,ICLR.cc/2023/Conference/Paper2619/-/Official_Review,,[],6,,['everyone'],-azium0cV9,['ICLR.cc/2023/Conference/Paper2619/Reviewer_uZBm'],1667371853067,,1667425240235,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2619/Reviewer_uZBm']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers, including:

1. ""Scaling Laws for Neural Language Models"" ([1])
2. ""Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"" ([2])
3. ""Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning"" ([3])
4. ""Piper: Multidimensional Planner for DNN Parallelization"" ([4])
5. A paper by Brown et al. (2020), which is not explicitly cited as a reference in the review, but is mentioned as a related work.

The reviewer also mentions specific techniques, such as Zero-2 and Zero-3, which may not be discussed in the original paper. However, it is possible that some of these references are already included in the original paper, but not explicitly mentioned in the review.",1,2020,Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
phQTRhlFhA,"This paper proposes a method called “SWARM parallelism” to allow model-parallel training of large models on heterogeneous clusters (e.g., a mix of weak and strong GPUs, unreliable compute nodes or networks with uneven bandwidth). ","Strength:

- The problem this paper is trying to address is important.
- Overall the paper is easy to follow
- The proposed technique looks sensible.


Weakness:
- In writing, the paper seems to spend too much effort explaining the background or something obvious, instead of giving an in-depth study, empirical or theoretical, about the proposed methods.

- Some contributions claimed by this paper are questionable. For example, I doubt the so-called “square-cube” law is a contribution of this paper, because It has been studied quite intensively in several large LM system papers, and has been used in many papers published before this paper. This paper seems to just re-brand this common wisdom. Accordingly, I feel the writing in section 3.1 and the experiments in 4.1 are unnecessary and does not reveal any new insight to this area.

- What remains novel in this paper is the SWARM parallelism, which is indeed interesting and trying to address a very important problem. However, I feel the authors’ execution (such as in section 3.2 and in experiments) in explaining the proposed method and proving the method is practical could have been done better. I have some detailed comments provided next.
","The “square-cube” law proposed in this paper is nice but quite obvious to me; it has been discussed and revealed in many papers such as scaling language models [1]. Also, pipeline parallelism having lower communication (in this paper’s phrasing – “pipeline parallelism naturally grows more communication-efficient with model size”) had also been revealed in many previous papers such as [2][3][4][5].
Given that the “square-cube” laws seem to be just a summarization/re-branding of several obvious facts (which have been leveraged in many papers), I don’t think the contents in section 3.1 are novel or interesting. I am not sure if this can be counted as a contribution to this paper.

[1] Scaling Laws for Neural Language Models

[2] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

[3] Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning

[4] Piper: Multidimensional Planner for DNN Parallelization

Writing and clarity:

The writing of section 3.2 is rather ad-hoc. Is it possible to formalize a few symbols for each stage of the model and each device of the cluster, and develop an algorithm (and if possible, equations) to illustrate how exactly the stochastic wiring and rebalancing work?


Experiments:

In sec 4.2, the choices of baselines are very tricky. If the goal is to compare the performance in ideal conditions, why don’t you compare it to stronger and more practical baselines? Here are my concerns:
- Gpipe faces a peak memory issue, which limits the largest possible number of micro-batches it can use, which in turn limits its throughput because a smaller number of micro-batch causes bubbles. I think you should choose the 1F1B schedule as a baseline, which addresses this problem.
- For Zero, why don’t you choose Zero-2 or Zero-3 but Zero-offload? If the goal is to compare the performance in ideal condition, I guess Zero-2 and Zero-3 can almost always give better performance than Zero-offload (since zero-offload has offloading which would be a penalty on performance). This is also related to the cluster you choose to perform the experiments in sec 4.2 --- because I do notice that you have 7x nodes with 8 A100 each. I suppose the experiment in 4.2 should be performed on the A100 cluster (which has more GPU memory and make Zero-Offload unnecessary).


- In sec 4.3, could you elaborate on this sentence:  “with 1.01 billion parameters in total: because of layer sharing, it is equivalent to a 13B model from (Brown et al., 2020) in terms of computing requirements.” ?
I am wondering if this means the layer sharing you introduced makes your method more advantageous because it increases the compute-to-communication ratio (because this layer sharding seems to increase the flops needed per parameter). Could you perform experiments on a standard, smaller GPT-3 model without layer sharing and report the results?

- The results in Table 3 look good. But these results seem to be achieved by a combination of many techniques, including the proposed ones in this paper, and some compression techniques which can reduce communication. Is it possible to isolate these factors with an ablation study to show how much improvement your approaches exactly bring?

- Overall, I feel the experiments section should focus on how SWARM can handle heterogeneous devices, handle failures, and handle uneven bandwidth. The results presented in 4.1 is not interesting; the results presented in 4.2 is less relevant and do not contribute to many of the claims you have made earlier in the paper about SWARM. 
","The paper has proposed a nice and sensible idea to address the model-parallel training in a very heterogeneous cluster environment, but the paper does not provide enough evidence to prove the method is indeed practical and verify the author's many claims.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667371853067,,-azium0cV9,ICLR.cc/2023/Conference/Paper2619/-/Official_Review,,[],6,,['everyone'],-azium0cV9,['ICLR.cc/2023/Conference/Paper2619/Reviewer_uZBm'],1667371853067,,1667425240235,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2619/Reviewer_uZBm']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers, including:

1. ""Scaling Laws for Neural Language Models"" ([1])
2. ""Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"" ([2])
3. ""Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning"" ([3])
4. ""Piper: Multidimensional Planner for DNN Parallelization"" ([4])
5. A paper by Brown et al. (2020), which is not explicitly cited as a reference in the review, but is mentioned as a related work.

The reviewer also mentions specific techniques, such as Zero-2 and Zero-3, which may not be discussed in the original paper. However, it is possible that some of these references are already included in the original paper, but not explicitly mentioned in the review.",1,2020,Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning
phQTRhlFhA,"This paper proposes a method called “SWARM parallelism” to allow model-parallel training of large models on heterogeneous clusters (e.g., a mix of weak and strong GPUs, unreliable compute nodes or networks with uneven bandwidth). ","Strength:

- The problem this paper is trying to address is important.
- Overall the paper is easy to follow
- The proposed technique looks sensible.


Weakness:
- In writing, the paper seems to spend too much effort explaining the background or something obvious, instead of giving an in-depth study, empirical or theoretical, about the proposed methods.

- Some contributions claimed by this paper are questionable. For example, I doubt the so-called “square-cube” law is a contribution of this paper, because It has been studied quite intensively in several large LM system papers, and has been used in many papers published before this paper. This paper seems to just re-brand this common wisdom. Accordingly, I feel the writing in section 3.1 and the experiments in 4.1 are unnecessary and does not reveal any new insight to this area.

- What remains novel in this paper is the SWARM parallelism, which is indeed interesting and trying to address a very important problem. However, I feel the authors’ execution (such as in section 3.2 and in experiments) in explaining the proposed method and proving the method is practical could have been done better. I have some detailed comments provided next.
","The “square-cube” law proposed in this paper is nice but quite obvious to me; it has been discussed and revealed in many papers such as scaling language models [1]. Also, pipeline parallelism having lower communication (in this paper’s phrasing – “pipeline parallelism naturally grows more communication-efficient with model size”) had also been revealed in many previous papers such as [2][3][4][5].
Given that the “square-cube” laws seem to be just a summarization/re-branding of several obvious facts (which have been leveraged in many papers), I don’t think the contents in section 3.1 are novel or interesting. I am not sure if this can be counted as a contribution to this paper.

[1] Scaling Laws for Neural Language Models

[2] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

[3] Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning

[4] Piper: Multidimensional Planner for DNN Parallelization

Writing and clarity:

The writing of section 3.2 is rather ad-hoc. Is it possible to formalize a few symbols for each stage of the model and each device of the cluster, and develop an algorithm (and if possible, equations) to illustrate how exactly the stochastic wiring and rebalancing work?


Experiments:

In sec 4.2, the choices of baselines are very tricky. If the goal is to compare the performance in ideal conditions, why don’t you compare it to stronger and more practical baselines? Here are my concerns:
- Gpipe faces a peak memory issue, which limits the largest possible number of micro-batches it can use, which in turn limits its throughput because a smaller number of micro-batch causes bubbles. I think you should choose the 1F1B schedule as a baseline, which addresses this problem.
- For Zero, why don’t you choose Zero-2 or Zero-3 but Zero-offload? If the goal is to compare the performance in ideal condition, I guess Zero-2 and Zero-3 can almost always give better performance than Zero-offload (since zero-offload has offloading which would be a penalty on performance). This is also related to the cluster you choose to perform the experiments in sec 4.2 --- because I do notice that you have 7x nodes with 8 A100 each. I suppose the experiment in 4.2 should be performed on the A100 cluster (which has more GPU memory and make Zero-Offload unnecessary).


- In sec 4.3, could you elaborate on this sentence:  “with 1.01 billion parameters in total: because of layer sharing, it is equivalent to a 13B model from (Brown et al., 2020) in terms of computing requirements.” ?
I am wondering if this means the layer sharing you introduced makes your method more advantageous because it increases the compute-to-communication ratio (because this layer sharding seems to increase the flops needed per parameter). Could you perform experiments on a standard, smaller GPT-3 model without layer sharing and report the results?

- The results in Table 3 look good. But these results seem to be achieved by a combination of many techniques, including the proposed ones in this paper, and some compression techniques which can reduce communication. Is it possible to isolate these factors with an ablation study to show how much improvement your approaches exactly bring?

- Overall, I feel the experiments section should focus on how SWARM can handle heterogeneous devices, handle failures, and handle uneven bandwidth. The results presented in 4.1 is not interesting; the results presented in 4.2 is less relevant and do not contribute to many of the claims you have made earlier in the paper about SWARM. 
","The paper has proposed a nice and sensible idea to address the model-parallel training in a very heterogeneous cluster environment, but the paper does not provide enough evidence to prove the method is indeed practical and verify the author's many claims.",5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1667371853067,,-azium0cV9,ICLR.cc/2023/Conference/Paper2619/-/Official_Review,,[],6,,['everyone'],-azium0cV9,['ICLR.cc/2023/Conference/Paper2619/Reviewer_uZBm'],1667371853067,,1667425240235,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2619/Reviewer_uZBm']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers, including:

1. ""Scaling Laws for Neural Language Models"" ([1])
2. ""Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"" ([2])
3. ""Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning"" ([3])
4. ""Piper: Multidimensional Planner for DNN Parallelization"" ([4])
5. A paper by Brown et al. (2020), which is not explicitly cited as a reference in the review, but is mentioned as a related work.

The reviewer also mentions specific techniques, such as Zero-2 and Zero-3, which may not be discussed in the original paper. However, it is possible that some of these references are already included in the original paper, but not explicitly mentioned in the review.",1,2020,Piper: Multidimensional Planner for DNN Parallelization
47YrRAFDAPg,"This paper presents a scheme of policy expansion (PEX) to bridge offline and online RL. Instead of directly fine-tuning the policy learned by offline dataset during online interaction, PEX freezes the learned policy and expands the policy set with a newly added policy. This new policy is optimized during the online learning phase. Then, a single policy is derived from the policy set, where action candidates are sampled by each policy and then the final action is selected by a resampling distribution. In the D4RL benchmark, PEX overall outperforms all of the baseline algorithms.
","[Strengths]
1. This work provides a simple yet effective scheme for offline-to-online RL. The proposed method is easy to implement.
2. In the experiments, PEX significantly outperforms baseline algorithms.

[Weaknesses]
1. In the experiments, comparisons with existing methods for offline-to-online RL are missing, e.g. [1,2].
2. To claim that PEX is a general framework, rather than an IQL-specific algorithm, it would be great to demonstrate the effectiveness of diverse combinations of PEX and existing offline RL algorithm backbones. For example, it would be great to see the performance of CQL+PEX or FisherBRC+PEX and whether PEX can consistently outperforms the baselines even for those different backbones.
3. The proposed method is designed heuristically, without theoretical justification.

[Questions]
1. In the PEX framework, the offline training phase is to learn a Q function and a policy, where the policy should be frozen while the value should be fine-tuned (i.e. transfer Q). I am wondering why value-finetuning is beneficial but policy-finetuning is detrimental to offline-to-online performance. It would be great to provide a more detailed explanation about this.
2. In Figure 2, why is the performance of PEX in halfcheetah-random better than in halfcheetah-medium-replay even though medium-replay dataset would contain more high-reward experiences?
3. It would be great to see the result using Q-transfer only without the policy expansion, i.e. initialize the Q-function with the pretrained one, and run an online RL algorithm using the Q initialization.
4. How sensitive is the algorithm with respect to the hyperparameter alpha?


[1] Nair et al., AWAC: Accelerating Online Reinforcement Learning with Offline Datasets, 2020

[2] Lee et al., Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble, 2021

","Overall, the paper is well-written and easy to follow. The idea of policy expansion and its combination with adaptive policy composition seems novel, but it is a rather heuristically designed method, rather than a principled one. ","The idea of policy expansion is interesting, but it lacks theoretical justification. For the experiments, it would be great to make a comparison with more baselines (see Weaknesses part) and to see the generality of the proposed PEX framework by conducting experiments with different offline RL backbones.

== post-rebutal
Thank you very much for your detailed responses. The added experiments are also greatly appreciated. I raised my score accordingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666689984576,,-Y34L45JR6z,ICLR.cc/2023/Conference/Paper2806/-/Official_Review,,[],4,,['everyone'],-Y34L45JR6z,['ICLR.cc/2023/Conference/Paper2806/Reviewer_U35N'],1666689984576,,1670454274983,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2806/Reviewer_U35N']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers ([1] Nair et al., AWAC: Accelerating Online Reinforcement Learning with Offline Datasets, 2020 and [2] Lee et al., Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble, 2021) that are not mentioned in the original paper, and suggests that the authors should compare their method with these existing methods for offline-to-online RL. Additionally, the reviewer mentions other offline RL algorithms (CQL and FisherBRC) that the authors could use as backbones to demonstrate the generality of the PEX framework, implying that these may not be discussed in the original paper.",1,"2020, 2021",AWAC: Accelerating Online Reinforcement Learning with Offline Datasets 
47YrRAFDAPg,"This paper presents a scheme of policy expansion (PEX) to bridge offline and online RL. Instead of directly fine-tuning the policy learned by offline dataset during online interaction, PEX freezes the learned policy and expands the policy set with a newly added policy. This new policy is optimized during the online learning phase. Then, a single policy is derived from the policy set, where action candidates are sampled by each policy and then the final action is selected by a resampling distribution. In the D4RL benchmark, PEX overall outperforms all of the baseline algorithms.
","[Strengths]
1. This work provides a simple yet effective scheme for offline-to-online RL. The proposed method is easy to implement.
2. In the experiments, PEX significantly outperforms baseline algorithms.

[Weaknesses]
1. In the experiments, comparisons with existing methods for offline-to-online RL are missing, e.g. [1,2].
2. To claim that PEX is a general framework, rather than an IQL-specific algorithm, it would be great to demonstrate the effectiveness of diverse combinations of PEX and existing offline RL algorithm backbones. For example, it would be great to see the performance of CQL+PEX or FisherBRC+PEX and whether PEX can consistently outperforms the baselines even for those different backbones.
3. The proposed method is designed heuristically, without theoretical justification.

[Questions]
1. In the PEX framework, the offline training phase is to learn a Q function and a policy, where the policy should be frozen while the value should be fine-tuned (i.e. transfer Q). I am wondering why value-finetuning is beneficial but policy-finetuning is detrimental to offline-to-online performance. It would be great to provide a more detailed explanation about this.
2. In Figure 2, why is the performance of PEX in halfcheetah-random better than in halfcheetah-medium-replay even though medium-replay dataset would contain more high-reward experiences?
3. It would be great to see the result using Q-transfer only without the policy expansion, i.e. initialize the Q-function with the pretrained one, and run an online RL algorithm using the Q initialization.
4. How sensitive is the algorithm with respect to the hyperparameter alpha?


[1] Nair et al., AWAC: Accelerating Online Reinforcement Learning with Offline Datasets, 2020

[2] Lee et al., Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble, 2021

","Overall, the paper is well-written and easy to follow. The idea of policy expansion and its combination with adaptive policy composition seems novel, but it is a rather heuristically designed method, rather than a principled one. ","The idea of policy expansion is interesting, but it lacks theoretical justification. For the experiments, it would be great to make a comparison with more baselines (see Weaknesses part) and to see the generality of the proposed PEX framework by conducting experiments with different offline RL backbones.

== post-rebutal
Thank you very much for your detailed responses. The added experiments are also greatly appreciated. I raised my score accordingly.",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,4: All of the claims and statements are well-supported and correct.,2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666689984576,,-Y34L45JR6z,ICLR.cc/2023/Conference/Paper2806/-/Official_Review,,[],4,,['everyone'],-Y34L45JR6z,['ICLR.cc/2023/Conference/Paper2806/Reviewer_U35N'],1666689984576,,1670454274983,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2806/Reviewer_U35N']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers ([1] Nair et al., AWAC: Accelerating Online Reinforcement Learning with Offline Datasets, 2020 and [2] Lee et al., Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble, 2021) that are not mentioned in the original paper, and suggests that the authors should compare their method with these existing methods for offline-to-online RL. Additionally, the reviewer mentions other offline RL algorithms (CQL and FisherBRC) that the authors could use as backbones to demonstrate the generality of the PEX framework, implying that these may not be discussed in the original paper.",1,"2020, 2021",Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble
FZmPMD3aYoH,"The success of contrastive learning has been heavily relying on the quality of data augmentations.
This paper tries to reduce this reliance by augmenting the augmentation graph with some other ""kernel graph"", where the kernel is defined with prior knowledge, such as features of generative models or attributes of data points. The paper also proposes a ""decoupled uniformity loss"" to allow incorporating such kernel.

The paper theoretically justify that the inclusion of such kernels can relax the assumption on data augmentation, and empirically show that adding such a kernel helps to improve contrastive methods' performance especially on poor augmentation. The quality of the kernel (e.g. features from $\beta$-VAE vs from DCGAN) affects the accuracy as expected.","Strengths:
- I find the idea of incorporating prior knowledge in form of kernels to be reasonable (""unsurprising""), yet still interesting.
- The empirical results adds one more piece of evidence that leveraging generative models in contrastive representation learning may be promising.

Concerns and questions
- The empirical comparison could be done more thoroughly, and the design choices could be better justified.
  - Table 2: what accuracy does SimCLR achieve?
  - What's the reason for using BigBiGAN for Table 2, but DCGAN for Table 3?
- The paper should cite and compare with ""Can contrastive learning avoid shortcut solutions?"" (Robinson et al. 2021)
  -  Also in Sec 3, the comment on InfoNCE that the ""extension to multiple views is not straightforward"": the paper ""Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis: an Integrated Framework"" seems related.
- Table 3: the comparison between 1) the gap between ""w/o Color"" and ""All"" and 2) the gap between (""w/o Color and Crop"" and ""w/o Color"") seems to vary across setups. Did you find any consistent trend, e.g. a particular type of generative model is more robust to the absence of certain type of augmentation?
- Improvement over generative features may be marginal: the proposed framework can be viewed equivalently as using generative features to improve contrastive learning, or using contrastive learning to improve feature quality of generative modeling. The latter view may be more appropriate sometimes, since the generative features may already achieve competitive accuracy.
    - Table 2: the BigBiGAN features themselves already achieve good accuracy, so the accuracy gain from training on decoupled uniformity loss seems marginal.
    - Table 3, STL-10: when there is no augmentation, DCGAN is better than Decoupled loss+DCGAN (do you know why?).
- How to choose the kernel in practice? Related to the previous point, it seems that if the kernel comes from a generative model, then the quality of the generative model's feature is strongly correlated with the quality of the kernel; can we use contrastive learning and generative model together, either training jointly or iteratively boosting each other?

Clarifications/comments:
- ""...we see that Decoupled Uniformity can realize both perfect alignment and uniformity, contrary to InfoNCE"": why can't InfoNCE realize both?
- The comments under Thm 3 says it ""provides tighter lower bound""; how is Thm 3 a lower bound?
","It seems that the paper tries to fit many ideas, which might be the reason that I find it a bit hard to navigate.
Here are some notes that may help improve readability.
- When Decoupled Uniformity loss is first introduced, it is unclear to me what's its advantage over SimCLR. It may be helpful to explain that centroids are needed since we can later incorporate the geometry provided by kernels.
- There are no subsections, possibly due to space concerns. I'd suggest keeping subsections for easier reference and better readability, and consider instead rephrasing and reorganizing the contents to save space.
    - For instance, the empirical results on medical imaging currently seems tangent to the rest of the paper, so it may make sense to either move it to the appendix, or restructure the paper to explain why medical domain is key.
    - The part ""Geometrical analysis of decoupled uniformity"" can also be entirely moved to the appendix.
- Table 3: it's probably more natural to rearrange the columns as ""w/o Color and Crop"", ""w/o Color"", ""All"".
- Typos
    - The paragraph below Theorem 2: ""arbitrarily bad"".
    - Above Assumption 1: ""..that there exists"".
    - Extension to multi-views: there's typo in the definition.
        - the LHS of the last equality should be $\hat{\mu}_{\bar{x}_j}^{(v)}$.
    - E.1, property 1: should $k$ be $K$ i.e. the number of classes?
        - Also, should the statement be a high probability statement?
- Please fix the format for the double quotes.
","This paper proposes to use prior information to make contrastive learning more robust to the choice of data augmentation.
I find the overall message interesting, but believe that the paper could be strengthened with more carefully conducted experiments and better organization.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666501590493,,-WiOF7FTt-n,ICLR.cc/2023/Conference/Paper2909/-/Official_Review,,[],2,,['everyone'],-WiOF7FTt-n,['ICLR.cc/2023/Conference/Paper2909/Reviewer_6Wk9'],1666501590493,,1666719300147,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2909/Reviewer_6Wk9']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers that are not cited in the original paper:

1. ""Can contrastive learning avoid shortcut solutions?"" (Robinson et al. 2021)
2. ""Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis: an Integrated Framework""

The reviewer suggests that the authors should cite and compare their work with these papers, implying that they are not already included in the original paper's references.",1,2021,Can contrastive learning avoid shortcut solutions 
FZmPMD3aYoH,"The success of contrastive learning has been heavily relying on the quality of data augmentations.
This paper tries to reduce this reliance by augmenting the augmentation graph with some other ""kernel graph"", where the kernel is defined with prior knowledge, such as features of generative models or attributes of data points. The paper also proposes a ""decoupled uniformity loss"" to allow incorporating such kernel.

The paper theoretically justify that the inclusion of such kernels can relax the assumption on data augmentation, and empirically show that adding such a kernel helps to improve contrastive methods' performance especially on poor augmentation. The quality of the kernel (e.g. features from $\beta$-VAE vs from DCGAN) affects the accuracy as expected.","Strengths:
- I find the idea of incorporating prior knowledge in form of kernels to be reasonable (""unsurprising""), yet still interesting.
- The empirical results adds one more piece of evidence that leveraging generative models in contrastive representation learning may be promising.

Concerns and questions
- The empirical comparison could be done more thoroughly, and the design choices could be better justified.
  - Table 2: what accuracy does SimCLR achieve?
  - What's the reason for using BigBiGAN for Table 2, but DCGAN for Table 3?
- The paper should cite and compare with ""Can contrastive learning avoid shortcut solutions?"" (Robinson et al. 2021)
  -  Also in Sec 3, the comment on InfoNCE that the ""extension to multiple views is not straightforward"": the paper ""Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis: an Integrated Framework"" seems related.
- Table 3: the comparison between 1) the gap between ""w/o Color"" and ""All"" and 2) the gap between (""w/o Color and Crop"" and ""w/o Color"") seems to vary across setups. Did you find any consistent trend, e.g. a particular type of generative model is more robust to the absence of certain type of augmentation?
- Improvement over generative features may be marginal: the proposed framework can be viewed equivalently as using generative features to improve contrastive learning, or using contrastive learning to improve feature quality of generative modeling. The latter view may be more appropriate sometimes, since the generative features may already achieve competitive accuracy.
    - Table 2: the BigBiGAN features themselves already achieve good accuracy, so the accuracy gain from training on decoupled uniformity loss seems marginal.
    - Table 3, STL-10: when there is no augmentation, DCGAN is better than Decoupled loss+DCGAN (do you know why?).
- How to choose the kernel in practice? Related to the previous point, it seems that if the kernel comes from a generative model, then the quality of the generative model's feature is strongly correlated with the quality of the kernel; can we use contrastive learning and generative model together, either training jointly or iteratively boosting each other?

Clarifications/comments:
- ""...we see that Decoupled Uniformity can realize both perfect alignment and uniformity, contrary to InfoNCE"": why can't InfoNCE realize both?
- The comments under Thm 3 says it ""provides tighter lower bound""; how is Thm 3 a lower bound?
","It seems that the paper tries to fit many ideas, which might be the reason that I find it a bit hard to navigate.
Here are some notes that may help improve readability.
- When Decoupled Uniformity loss is first introduced, it is unclear to me what's its advantage over SimCLR. It may be helpful to explain that centroids are needed since we can later incorporate the geometry provided by kernels.
- There are no subsections, possibly due to space concerns. I'd suggest keeping subsections for easier reference and better readability, and consider instead rephrasing and reorganizing the contents to save space.
    - For instance, the empirical results on medical imaging currently seems tangent to the rest of the paper, so it may make sense to either move it to the appendix, or restructure the paper to explain why medical domain is key.
    - The part ""Geometrical analysis of decoupled uniformity"" can also be entirely moved to the appendix.
- Table 3: it's probably more natural to rearrange the columns as ""w/o Color and Crop"", ""w/o Color"", ""All"".
- Typos
    - The paragraph below Theorem 2: ""arbitrarily bad"".
    - Above Assumption 1: ""..that there exists"".
    - Extension to multi-views: there's typo in the definition.
        - the LHS of the last equality should be $\hat{\mu}_{\bar{x}_j}^{(v)}$.
    - E.1, property 1: should $k$ be $K$ i.e. the number of classes?
        - Also, should the statement be a high probability statement?
- Please fix the format for the double quotes.
","This paper proposes to use prior information to make contrastive learning more robust to the choice of data augmentation.
I find the overall message interesting, but believe that the paper could be strengthened with more carefully conducted experiments and better organization.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,5: marginally below the acceptance threshold,1666501590493,,-WiOF7FTt-n,ICLR.cc/2023/Conference/Paper2909/-/Official_Review,,[],2,,['everyone'],-WiOF7FTt-n,['ICLR.cc/2023/Conference/Paper2909/Reviewer_6Wk9'],1666501590493,,1666719300147,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2909/Reviewer_6Wk9']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers that are not cited in the original paper:

1. ""Can contrastive learning avoid shortcut solutions?"" (Robinson et al. 2021)
2. ""Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis: an Integrated Framework""

The reviewer suggests that the authors should cite and compare their work with these papers, implying that they are not already included in the original paper's references.",1,2021,Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis an Integrated Framework
wf8hqZQ2RFM,"The authors consider  data distributions of participating and unparticipating clients are drawn
from a meta-distribution and use classic Rademacher complexity- and local Rademacher complexity-based generalisation bounds to develop excess risk bounds for unparticipating clients and fast semi-excess risk bounds for participating clients.  
","Strength: Studying federated learning in terms of learning is certainly an important direction, which has not been addressed in the current literature. I think the paper is well-written and the authors  did a good job in terms of going beyond convergence analyses and using known Rademacher complexity-based generalization bounds to obtain (semi) excess risk bounds. 

-----

Weakness: Although the authors emphasized much about the relevanece of their bounds to FL in practical settings, in particular cross-device FL with a large pool of unparticipating clients, there are major concerns regarding the considered model, the correctness of results, and conditions under which main theorems hold, which will be elaborated in the following. Also, it is unfortunate that the paper lacks convincing experimental results to validate that the developed bounds indeed explain whether ""unparticipating clients benefit from the model trained by participating clients?"" 

-----
Detailed comments: 

 ""We argue that this assumption is reasonable in practice. For instance, in cross-device federated learning, the number of total clients is generally large and it is natural to assume that there exists a meta-distribution""

This is not really convincing. What if some clients have data distributions that are quite different from others. Also what if the distribution of clients changes over time.  Another major issue is  how should such meta-distribution be modelled? Will you use parametric or non-parameteric models? Given that the authors emphasize much about practical scenarios and settings where several clients may not even participate in training, how can one model/estimate the meta distribution? Imagine that a practitioner does not have even a sample drawn from distributions for the major of clients (unparticipating clients).  

-----

The symmetrization step in the proof of Lemma 1 should be expanded. BTW, the first line after ""By symmetrization, we have"" cannot be an equality. 

More importantly, the definitions for ${\cal R}_{mn}({\cal F})$ in pages 14 and 15 are different, which makes the proof hard to read. The same issue exists in Lemma 2. In addition, $Z_1$,..., $Z_m$ in the proof of Lemma 2 are undefined. So I am not confident about the correctness of the results. 

-----

Theorem 2 requires $n=\Omega(d)$, which does not hold for typical overparameterized models whose number of parameters far exceed the number of training data.  This is even worse in Theorem 4 where $m$, which is claimed to be relatively small is required to be larger than $d$. 

-----

The techniques used to prove Theorem 2 are pretty standard based on (Bartlett et al., 2002, Sridharan et al., 2009, Yousefi et a.., 2018). Is there any technical difficulty in the problem of bounding semi-excess risk compared to previous work?


Karthik  Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. ""Fast rates for regularized objectives."" Advances in neural information processing systems (NeurIPS),  2008.


Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. ""Localized rademacher complexities."" International Conference on Computational Learning Theory (COLT), 2002.

Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi, and Georgios C Anagnostopoulos. Local rademacher complexity-based learning guarantees for multi-task learning. The Journal of Machine Learning Research, 19(1):1385–1431, 2018.



-----

Why not considering the excess risk instead of semi-excess risk for participating clients? It seems that the bounds will not be better than those for unparticipating clients.

-----
 
The bound in Theorem 1 will be vacuous when using overparameterized models and a small $m$. The authors emphasized that $m$ is a moderate value in their settings.  which does not lead to a meaningful generalization results.  

-----

Assumption 2: $X_i^1$ in Assumption 2 is drawn from $D_i$, which is not $D$. Assumption 2 (a) does not seem to be compatible with Definition 1.  

-----


It will be nice if the authors elaborate on what specific problems in ML lead to sub-Weibull loss  distributions with $\alpha\neq 1,2$ for $h$ as in Theorems 5 and 6. Also it is a bit confusing since in the introduction the authors claim that they consider sub-Weibull loss functions while they indeed consider sub-Weibull classifiers in Theorem 5,6. 

-----

The definition of  VC major class, which is the main building block of Theorem 1 should be provided in the main paper. ","The paper is mostly well-written but there were issues in the proofs, which should be properly addressed to improve clarity of the paper. 
I think the paper's quality can be improved. The techniques based on Rademacher complexity- and local Rademacher complexity to develop generalisation bounds are pretty standard. The reproducibility is not applicable as there are not experimental results. 

","The authors  did a good job in terms of going beyond convergence analyses and developing (semi) excess risk bounds for participating and unparticipating clients. However, there are major concerns regarding the considered model, the correctness of results, and conditions under which main theorems hold. ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,Not applicable,NO.,"3: reject, not good enough",1666371888969,,-EHqoysUYLx,ICLR.cc/2023/Conference/Paper2925/-/Official_Review,,[],1,,['everyone'],-EHqoysUYLx,['ICLR.cc/2023/Conference/Paper2925/Reviewer_8Tvp'],1666371888969,,1670683097439,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2925/Reviewer_8Tvp']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers:

1. Karthik Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. ""Fast rates for regularized objectives."" Advances in neural information processing systems (NeurIPS), 2008.
2. Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. ""Localized rademacher complexities."" International Conference on Computational Learning Theory (COLT), 2002.
3. Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi, and Georgios C Anagnostopoulos. Local rademacher complexity-based learning guarantees for multi-task learning. The Journal of Machine Learning Research, 19(1):1385–1431, 2018.

These papers are mentioned as related work in the context of Rademacher complexity and local Rademacher complexity, and the reviewer suggests that the techniques used in the paper are pretty standard and have been used in previous work, implying that the authors may want to consider citing or discussing these papers in their work. However, without access to the original paper, it is impossible to determine if these papers are already cited in the original paper.",1,"2008, 2002, 2018",Fast rates for regularized objectives
wf8hqZQ2RFM,"The authors consider  data distributions of participating and unparticipating clients are drawn
from a meta-distribution and use classic Rademacher complexity- and local Rademacher complexity-based generalisation bounds to develop excess risk bounds for unparticipating clients and fast semi-excess risk bounds for participating clients.  
","Strength: Studying federated learning in terms of learning is certainly an important direction, which has not been addressed in the current literature. I think the paper is well-written and the authors  did a good job in terms of going beyond convergence analyses and using known Rademacher complexity-based generalization bounds to obtain (semi) excess risk bounds. 

-----

Weakness: Although the authors emphasized much about the relevanece of their bounds to FL in practical settings, in particular cross-device FL with a large pool of unparticipating clients, there are major concerns regarding the considered model, the correctness of results, and conditions under which main theorems hold, which will be elaborated in the following. Also, it is unfortunate that the paper lacks convincing experimental results to validate that the developed bounds indeed explain whether ""unparticipating clients benefit from the model trained by participating clients?"" 

-----
Detailed comments: 

 ""We argue that this assumption is reasonable in practice. For instance, in cross-device federated learning, the number of total clients is generally large and it is natural to assume that there exists a meta-distribution""

This is not really convincing. What if some clients have data distributions that are quite different from others. Also what if the distribution of clients changes over time.  Another major issue is  how should such meta-distribution be modelled? Will you use parametric or non-parameteric models? Given that the authors emphasize much about practical scenarios and settings where several clients may not even participate in training, how can one model/estimate the meta distribution? Imagine that a practitioner does not have even a sample drawn from distributions for the major of clients (unparticipating clients).  

-----

The symmetrization step in the proof of Lemma 1 should be expanded. BTW, the first line after ""By symmetrization, we have"" cannot be an equality. 

More importantly, the definitions for ${\cal R}_{mn}({\cal F})$ in pages 14 and 15 are different, which makes the proof hard to read. The same issue exists in Lemma 2. In addition, $Z_1$,..., $Z_m$ in the proof of Lemma 2 are undefined. So I am not confident about the correctness of the results. 

-----

Theorem 2 requires $n=\Omega(d)$, which does not hold for typical overparameterized models whose number of parameters far exceed the number of training data.  This is even worse in Theorem 4 where $m$, which is claimed to be relatively small is required to be larger than $d$. 

-----

The techniques used to prove Theorem 2 are pretty standard based on (Bartlett et al., 2002, Sridharan et al., 2009, Yousefi et a.., 2018). Is there any technical difficulty in the problem of bounding semi-excess risk compared to previous work?


Karthik  Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. ""Fast rates for regularized objectives."" Advances in neural information processing systems (NeurIPS),  2008.


Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. ""Localized rademacher complexities."" International Conference on Computational Learning Theory (COLT), 2002.

Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi, and Georgios C Anagnostopoulos. Local rademacher complexity-based learning guarantees for multi-task learning. The Journal of Machine Learning Research, 19(1):1385–1431, 2018.



-----

Why not considering the excess risk instead of semi-excess risk for participating clients? It seems that the bounds will not be better than those for unparticipating clients.

-----
 
The bound in Theorem 1 will be vacuous when using overparameterized models and a small $m$. The authors emphasized that $m$ is a moderate value in their settings.  which does not lead to a meaningful generalization results.  

-----

Assumption 2: $X_i^1$ in Assumption 2 is drawn from $D_i$, which is not $D$. Assumption 2 (a) does not seem to be compatible with Definition 1.  

-----


It will be nice if the authors elaborate on what specific problems in ML lead to sub-Weibull loss  distributions with $\alpha\neq 1,2$ for $h$ as in Theorems 5 and 6. Also it is a bit confusing since in the introduction the authors claim that they consider sub-Weibull loss functions while they indeed consider sub-Weibull classifiers in Theorem 5,6. 

-----

The definition of  VC major class, which is the main building block of Theorem 1 should be provided in the main paper. ","The paper is mostly well-written but there were issues in the proofs, which should be properly addressed to improve clarity of the paper. 
I think the paper's quality can be improved. The techniques based on Rademacher complexity- and local Rademacher complexity to develop generalisation bounds are pretty standard. The reproducibility is not applicable as there are not experimental results. 

","The authors  did a good job in terms of going beyond convergence analyses and developing (semi) excess risk bounds for participating and unparticipating clients. However, there are major concerns regarding the considered model, the correctness of results, and conditions under which main theorems hold. ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,Not applicable,NO.,"3: reject, not good enough",1666371888969,,-EHqoysUYLx,ICLR.cc/2023/Conference/Paper2925/-/Official_Review,,[],1,,['everyone'],-EHqoysUYLx,['ICLR.cc/2023/Conference/Paper2925/Reviewer_8Tvp'],1666371888969,,1670683097439,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2925/Reviewer_8Tvp']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers:

1. Karthik Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. ""Fast rates for regularized objectives."" Advances in neural information processing systems (NeurIPS), 2008.
2. Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. ""Localized rademacher complexities."" International Conference on Computational Learning Theory (COLT), 2002.
3. Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi, and Georgios C Anagnostopoulos. Local rademacher complexity-based learning guarantees for multi-task learning. The Journal of Machine Learning Research, 19(1):1385–1431, 2018.

These papers are mentioned as related work in the context of Rademacher complexity and local Rademacher complexity, and the reviewer suggests that the techniques used in the paper are pretty standard and have been used in previous work, implying that the authors may want to consider citing or discussing these papers in their work. However, without access to the original paper, it is impossible to determine if these papers are already cited in the original paper.",1,"2008, 2002, 2018",Localized rademacher complexities
wf8hqZQ2RFM,"The authors consider  data distributions of participating and unparticipating clients are drawn
from a meta-distribution and use classic Rademacher complexity- and local Rademacher complexity-based generalisation bounds to develop excess risk bounds for unparticipating clients and fast semi-excess risk bounds for participating clients.  
","Strength: Studying federated learning in terms of learning is certainly an important direction, which has not been addressed in the current literature. I think the paper is well-written and the authors  did a good job in terms of going beyond convergence analyses and using known Rademacher complexity-based generalization bounds to obtain (semi) excess risk bounds. 

-----

Weakness: Although the authors emphasized much about the relevanece of their bounds to FL in practical settings, in particular cross-device FL with a large pool of unparticipating clients, there are major concerns regarding the considered model, the correctness of results, and conditions under which main theorems hold, which will be elaborated in the following. Also, it is unfortunate that the paper lacks convincing experimental results to validate that the developed bounds indeed explain whether ""unparticipating clients benefit from the model trained by participating clients?"" 

-----
Detailed comments: 

 ""We argue that this assumption is reasonable in practice. For instance, in cross-device federated learning, the number of total clients is generally large and it is natural to assume that there exists a meta-distribution""

This is not really convincing. What if some clients have data distributions that are quite different from others. Also what if the distribution of clients changes over time.  Another major issue is  how should such meta-distribution be modelled? Will you use parametric or non-parameteric models? Given that the authors emphasize much about practical scenarios and settings where several clients may not even participate in training, how can one model/estimate the meta distribution? Imagine that a practitioner does not have even a sample drawn from distributions for the major of clients (unparticipating clients).  

-----

The symmetrization step in the proof of Lemma 1 should be expanded. BTW, the first line after ""By symmetrization, we have"" cannot be an equality. 

More importantly, the definitions for ${\cal R}_{mn}({\cal F})$ in pages 14 and 15 are different, which makes the proof hard to read. The same issue exists in Lemma 2. In addition, $Z_1$,..., $Z_m$ in the proof of Lemma 2 are undefined. So I am not confident about the correctness of the results. 

-----

Theorem 2 requires $n=\Omega(d)$, which does not hold for typical overparameterized models whose number of parameters far exceed the number of training data.  This is even worse in Theorem 4 where $m$, which is claimed to be relatively small is required to be larger than $d$. 

-----

The techniques used to prove Theorem 2 are pretty standard based on (Bartlett et al., 2002, Sridharan et al., 2009, Yousefi et a.., 2018). Is there any technical difficulty in the problem of bounding semi-excess risk compared to previous work?


Karthik  Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. ""Fast rates for regularized objectives."" Advances in neural information processing systems (NeurIPS),  2008.


Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. ""Localized rademacher complexities."" International Conference on Computational Learning Theory (COLT), 2002.

Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi, and Georgios C Anagnostopoulos. Local rademacher complexity-based learning guarantees for multi-task learning. The Journal of Machine Learning Research, 19(1):1385–1431, 2018.



-----

Why not considering the excess risk instead of semi-excess risk for participating clients? It seems that the bounds will not be better than those for unparticipating clients.

-----
 
The bound in Theorem 1 will be vacuous when using overparameterized models and a small $m$. The authors emphasized that $m$ is a moderate value in their settings.  which does not lead to a meaningful generalization results.  

-----

Assumption 2: $X_i^1$ in Assumption 2 is drawn from $D_i$, which is not $D$. Assumption 2 (a) does not seem to be compatible with Definition 1.  

-----


It will be nice if the authors elaborate on what specific problems in ML lead to sub-Weibull loss  distributions with $\alpha\neq 1,2$ for $h$ as in Theorems 5 and 6. Also it is a bit confusing since in the introduction the authors claim that they consider sub-Weibull loss functions while they indeed consider sub-Weibull classifiers in Theorem 5,6. 

-----

The definition of  VC major class, which is the main building block of Theorem 1 should be provided in the main paper. ","The paper is mostly well-written but there were issues in the proofs, which should be properly addressed to improve clarity of the paper. 
I think the paper's quality can be improved. The techniques based on Rademacher complexity- and local Rademacher complexity to develop generalisation bounds are pretty standard. The reproducibility is not applicable as there are not experimental results. 

","The authors  did a good job in terms of going beyond convergence analyses and developing (semi) excess risk bounds for participating and unparticipating clients. However, there are major concerns regarding the considered model, the correctness of results, and conditions under which main theorems hold. ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,Not applicable,NO.,"3: reject, not good enough",1666371888969,,-EHqoysUYLx,ICLR.cc/2023/Conference/Paper2925/-/Official_Review,,[],1,,['everyone'],-EHqoysUYLx,['ICLR.cc/2023/Conference/Paper2925/Reviewer_8Tvp'],1666371888969,,1670683097439,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2925/Reviewer_8Tvp']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions the following papers:

1. Karthik Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. ""Fast rates for regularized objectives."" Advances in neural information processing systems (NeurIPS), 2008.
2. Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. ""Localized rademacher complexities."" International Conference on Computational Learning Theory (COLT), 2002.
3. Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi, and Georgios C Anagnostopoulos. Local rademacher complexity-based learning guarantees for multi-task learning. The Journal of Machine Learning Research, 19(1):1385–1431, 2018.

These papers are mentioned as related work in the context of Rademacher complexity and local Rademacher complexity, and the reviewer suggests that the techniques used in the paper are pretty standard and have been used in previous work, implying that the authors may want to consider citing or discussing these papers in their work. However, without access to the original paper, it is impossible to determine if these papers are already cited in the original paper.",1,"2008, 2002, 2018",Local rademacher complexity-based learning guarantees for multi-task learning
FosJCT3zcXR,"This is a theoretical work on the analysis of the generalization error in federated learning with heterogeneity of the losses and partial participation of the clients in the training process. The authors address the question: ""Would the unparticipating clients benefit from the model trained by participating clients?"". They provide a statistical analysis of the generalization error. The main assumption behind is that  the local distributions are sampled from a meta-distribution.","The paper provides interesting theoretical insights about heterogeneity and the important challenge of partial participation in federated learning (FL).

I am an optimizer, not a statistician. I develop optimization algorithms for FL and I am not familiar with these aspects. With this disclaimer in mind, I have the following questions:
1) Is there a connection between your setting and the literature on personalized FL? Indeed, from the point of view of the nonparticipating clients, the obtained model is not personalized enough. I am thinking for instance of the paper Hanzely et al. ""Lower Bounds and Optimal Algorithms for Personalized Federated Learning""
2) I have come across the paper ""With a Little Help from My Friend: Server-Aided Federated Learning with Partial Client Participation"" of Yang et al. (https://openreview.net/forum?id=xT6d2Ghtkv) which also studies the generalization error of heterogeneous FL with partial participation, and study server-aided FL as a possible remedy. Could you comment on their findings?
3) Characterizing heterogeneity is important, as well as the study of the convergence speed of learning algorithms, depending on the amount of heterogeneity: intuitively, convergence is faster in the homogeneous setting, since there is redundancy, than in the heterogeneous setting. For instance in the paper ""Permutation Compressors for Provably Faster Distributed Nonconvex Optimization""
by Szlendak et al., they introduce the notion of ""Hessian variance"", which characterizes the similarity between the individual loss functions of the clients. Since the losses are date-fit terms with respect to the individual datasets, this is directly related to the amount of heterogeneity. Could you comment on this aspect?",The paper is well written.,"The paper provides some new insights on partial participation, which is a timely and important problem in modern distributed learning settings. The paper studies the impact of partial participation from the mathematical/statistical point of view. These findings are interesting but there is no contribution from an algorithmic point of view, on how to mitigate the effects of partial participation.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",4: All of the claims and statements are well-supported and correct.,"4: The contributions are significant, and do not exist in prior works.",Not applicable,NO.,6: marginally above the acceptance threshold,1667488213526,,-EHqoysUYLx,ICLR.cc/2023/Conference/Paper2925/-/Official_Review,,[],4,,['everyone'],-EHqoysUYLx,['ICLR.cc/2023/Conference/Paper2925/Reviewer_zYTD'],1667488213526,,1667488213526,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2925/Reviewer_zYTD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that may not be already discussed in the original paper. 

The reviewer mentions several papers, including:
1. ""Lower Bounds and Optimal Algorithms for Personalized Federated Learning"" by Hanzely et al.
2. ""With a Little Help from My Friend: Server-Aided Federated Learning with Partial Client Participation"" by Yang et al.
3. ""Permutation Compressors for Provably Faster Distributed Nonconvex Optimization"" by Szlendak et al.

These papers are used by the reviewer to ask questions, make connections, and provide context to the work presented in the paper, implying that they may not be already referenced or discussed in the original paper.",1,,Lower Bounds and Optimal Algorithms for Personalized Federated Learning 
FosJCT3zcXR,"This is a theoretical work on the analysis of the generalization error in federated learning with heterogeneity of the losses and partial participation of the clients in the training process. The authors address the question: ""Would the unparticipating clients benefit from the model trained by participating clients?"". They provide a statistical analysis of the generalization error. The main assumption behind is that  the local distributions are sampled from a meta-distribution.","The paper provides interesting theoretical insights about heterogeneity and the important challenge of partial participation in federated learning (FL).

I am an optimizer, not a statistician. I develop optimization algorithms for FL and I am not familiar with these aspects. With this disclaimer in mind, I have the following questions:
1) Is there a connection between your setting and the literature on personalized FL? Indeed, from the point of view of the nonparticipating clients, the obtained model is not personalized enough. I am thinking for instance of the paper Hanzely et al. ""Lower Bounds and Optimal Algorithms for Personalized Federated Learning""
2) I have come across the paper ""With a Little Help from My Friend: Server-Aided Federated Learning with Partial Client Participation"" of Yang et al. (https://openreview.net/forum?id=xT6d2Ghtkv) which also studies the generalization error of heterogeneous FL with partial participation, and study server-aided FL as a possible remedy. Could you comment on their findings?
3) Characterizing heterogeneity is important, as well as the study of the convergence speed of learning algorithms, depending on the amount of heterogeneity: intuitively, convergence is faster in the homogeneous setting, since there is redundancy, than in the heterogeneous setting. For instance in the paper ""Permutation Compressors for Provably Faster Distributed Nonconvex Optimization""
by Szlendak et al., they introduce the notion of ""Hessian variance"", which characterizes the similarity between the individual loss functions of the clients. Since the losses are date-fit terms with respect to the individual datasets, this is directly related to the amount of heterogeneity. Could you comment on this aspect?",The paper is well written.,"The paper provides some new insights on partial participation, which is a timely and important problem in modern distributed learning settings. The paper studies the impact of partial participation from the mathematical/statistical point of view. These findings are interesting but there is no contribution from an algorithmic point of view, on how to mitigate the effects of partial participation.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",4: All of the claims and statements are well-supported and correct.,"4: The contributions are significant, and do not exist in prior works.",Not applicable,NO.,6: marginally above the acceptance threshold,1667488213526,,-EHqoysUYLx,ICLR.cc/2023/Conference/Paper2925/-/Official_Review,,[],4,,['everyone'],-EHqoysUYLx,['ICLR.cc/2023/Conference/Paper2925/Reviewer_zYTD'],1667488213526,,1667488213526,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2925/Reviewer_zYTD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that may not be already discussed in the original paper. 

The reviewer mentions several papers, including:
1. ""Lower Bounds and Optimal Algorithms for Personalized Federated Learning"" by Hanzely et al.
2. ""With a Little Help from My Friend: Server-Aided Federated Learning with Partial Client Participation"" by Yang et al.
3. ""Permutation Compressors for Provably Faster Distributed Nonconvex Optimization"" by Szlendak et al.

These papers are used by the reviewer to ask questions, make connections, and provide context to the work presented in the paper, implying that they may not be already referenced or discussed in the original paper.",1,,With a Little Help from My Friend Server-Aided Federated Learning with Partial Client Participation 
FosJCT3zcXR,"This is a theoretical work on the analysis of the generalization error in federated learning with heterogeneity of the losses and partial participation of the clients in the training process. The authors address the question: ""Would the unparticipating clients benefit from the model trained by participating clients?"". They provide a statistical analysis of the generalization error. The main assumption behind is that  the local distributions are sampled from a meta-distribution.","The paper provides interesting theoretical insights about heterogeneity and the important challenge of partial participation in federated learning (FL).

I am an optimizer, not a statistician. I develop optimization algorithms for FL and I am not familiar with these aspects. With this disclaimer in mind, I have the following questions:
1) Is there a connection between your setting and the literature on personalized FL? Indeed, from the point of view of the nonparticipating clients, the obtained model is not personalized enough. I am thinking for instance of the paper Hanzely et al. ""Lower Bounds and Optimal Algorithms for Personalized Federated Learning""
2) I have come across the paper ""With a Little Help from My Friend: Server-Aided Federated Learning with Partial Client Participation"" of Yang et al. (https://openreview.net/forum?id=xT6d2Ghtkv) which also studies the generalization error of heterogeneous FL with partial participation, and study server-aided FL as a possible remedy. Could you comment on their findings?
3) Characterizing heterogeneity is important, as well as the study of the convergence speed of learning algorithms, depending on the amount of heterogeneity: intuitively, convergence is faster in the homogeneous setting, since there is redundancy, than in the heterogeneous setting. For instance in the paper ""Permutation Compressors for Provably Faster Distributed Nonconvex Optimization""
by Szlendak et al., they introduce the notion of ""Hessian variance"", which characterizes the similarity between the individual loss functions of the clients. Since the losses are date-fit terms with respect to the individual datasets, this is directly related to the amount of heterogeneity. Could you comment on this aspect?",The paper is well written.,"The paper provides some new insights on partial participation, which is a timely and important problem in modern distributed learning settings. The paper studies the impact of partial participation from the mathematical/statistical point of view. These findings are interesting but there is no contribution from an algorithmic point of view, on how to mitigate the effects of partial participation.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",4: All of the claims and statements are well-supported and correct.,"4: The contributions are significant, and do not exist in prior works.",Not applicable,NO.,6: marginally above the acceptance threshold,1667488213526,,-EHqoysUYLx,ICLR.cc/2023/Conference/Paper2925/-/Official_Review,,[],4,,['everyone'],-EHqoysUYLx,['ICLR.cc/2023/Conference/Paper2925/Reviewer_zYTD'],1667488213526,,1667488213526,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2925/Reviewer_zYTD']","Yes, this peer review suggests the authors of the paper to refer to specific literature that may not be already discussed in the original paper. 

The reviewer mentions several papers, including:
1. ""Lower Bounds and Optimal Algorithms for Personalized Federated Learning"" by Hanzely et al.
2. ""With a Little Help from My Friend: Server-Aided Federated Learning with Partial Client Participation"" by Yang et al.
3. ""Permutation Compressors for Provably Faster Distributed Nonconvex Optimization"" by Szlendak et al.

These papers are used by the reviewer to ask questions, make connections, and provide context to the work presented in the paper, implying that they may not be already referenced or discussed in the original paper.",1,,Permutation Compressors for Provably Faster Distributed Nonconvex Optimization
5UZoYuB0nAu,"The paper introduces 0/1 Adam, a variant of the 1-bit Adam algorithm.  While the 1-bit Adam algorithm leveraged a fixed burn-in period wherein vanilla Adam was run before the variance term stabilized and was frozen, 0/1 Adam proposes an adaptive variance-freezing policy.  Additionally, 0/1 Adam leverages local SGD wherein synchronicity rounds among workers are skipped.  Convergence results similar to 1-bit Adam are provided for 0/1 Adam, and experimental results showing the efficacy of the algorithm for Imagenet and BERT are provided.","Strengths
-The paper extends 1-bit Adam in a natural and intuitive way.  Both adaptive variance-freezing and local SGD are promising directions to improve the original 1-bit Adam algorithm
-The results are compelling

Weaknesses
-Presentation of results could be significantly improved (see clarity section for further information).

-The writing and description of the novelty of the presented work requires extensive reevaluation.  The description of 0/1 Adam as a novel algorithm is not correct, it builds upon 1-bit Adam (also borrowing a large amount the analysis from the corresponding paper) and this point should be made clear throughout the paper (however, adaptive variance freezing is an important and novel contribution to the 1-bit Adam algorithm). Furthermore, 0/1 Adam is not the first work to consider local SGD for distributed algorithm.

-Even after reading the paper, it is unclear how adaptive variance-freezing or local SGD (i.e., skipping rounds/local steps) may be set in practice.  Indeed, these two policies must be known \emph{before} running Algorithm~\ref{1}.

-Claims are made without substantiation or are misleading.  E.g.:

         1) ""the change of variance over steps in Adam is generally smooth"" <- Please substantiate this claim.  In the 1-bit Adam paper, they 
              assumed this was the case and stated so explicitly.

         2) ""While this paradigm avoids compressing non-linear information with a one-time frozen variance, the experimental results from 
             (Tang et al., 2021) indicate the fullprecision stage still incurs non-trivial overhead."" <- Overhead discussed in the paper was with 
              regards to vanilla Adam.  Please remove this claim or provide additional evidence to substantiate it

         3) ""Furthermore, 1-bit Adam is restricted in the scope of gradient compression, and cannot be trivially adapted when other 
              techniques are used, such as local steps."" <- In the 1-bit Adam paper, several variants were 
              implemented and contrasted against, including compressing the gradient and compressing the momentum.  Why can it not be 
              trivially adapted?  Furthermore, how to leverage local SGD with Adam is not an open problem, previous work in [1] has tackled 
              this. 1-bit Adam could trivially be combined with CADA.  Please reassess, and support, your claim.

         4) ""Besides, the empirical success of (Tang et al., 2021) was not substantiated on generative models (GPT-style models), for instance, 
              175B GPT-3 Brown et al. (2020), 530B MT-NLG Smith et al. (2022), etc."" <- This is disingenuous; 1-bit Adam was evaluated on 
              GANs (generative model) as well large-scale models (BERT).  530B MT-NLG Smith et al. was also 
              published after the 1-bit Adam paper, and neither GPT-3 no MT-NLG are evaluated using 0/1 Adam.  I would recommend revising 
              this claim, as well as elaborating why BERT is not a significant enough benchmark that GPT model evaluations are required.

References:
[1] Chen, Tianyi, et al. ""Cada: Communication-adaptive distributed adam.""
International Conference on Artificial Intelligence and
Statistics. PMLR, 2021.","Clarity:
Presentation of results has significant room for improvement and requires more explanation.  For instance, it is unclear exactly what is being demonstrated in Figure 5: is ""w/o Skipping Round"" equivalent to w/o Local Steps?  What are 1.2x/1.3x denoting?  Similarly, in Figure 2 (a-c), what is being denoted by 1.6x, 2x, and 1.4x?  One suggestion is to describe each figure and the corresponding results after they are presented.  As it stands, Figures 2-5 all appear before a single figures' results are described in the text.

Quality:
Writing quality can be significantly improved.  Furthermore, comparing against Adam in all results would greatly improve the paper.  E.g., ""Since Tang et al. (2021) already demonstrated that 1-bit Adam has similar statistical results to Adam, we omit the comparison of end-to-end model accuracy to Adam for brevity."" <- This is unreasonable; 1-bit Adam has been shown to provide near Adam accuracy, and so the comparison between the two may be ommitted.  However, the newly described algorithm (which is not equivalent to 1-bit Adam) has not been shown to achieve similar performance to Adam, and thus this must be demonstrated.  For the purposes of the paper, including Adam as a baseline in Figures 4 and 5 would not overcomplicate these results, but would significantly strengthen the paper.

Novelty:
0/1 Adam is an extension of the 1-bit Adam algorithm.  Additionally, much of the motivation and theoretical analysis strongly follow the 1-bit Adam paper.  The paper needs to do a better job of establishing that 0/1 Adam builds on this previous work, what exact theoretical results were presented in the 1-bit Adam algorithm and how the presented theoretical results stand as novel contributions, and exactly what are the novel contributions being made.  E.g., ""We propose 0/1 Adam, a novel optimization method that addresses the non-linearity challenges in Adam when applying aggressive 1-bit quantization and local steps"" <- disingenuous to call 0/1 Adam novel, but rather ""0/1 Adam builds upon the 1-bit Adam algorithm.""  Furthermore, the use of local SGD/steps for Adam has been previously addressed by the Cada algorithm of Chen et al.

Reproducibility:
The 1-bit Adam has been open-sourced in a deep learning library by the authors.  However, the presented experiments are non-trivial, and providing experimental code to reproduce the featured results would greatly help other researchers to further build on their work.","The paper makes some natural extensions to the 1-bit Adam algorithm, while providing convergence guarantees for the resulting 0/1 Adam algorithm.  Furthermore, the results seem compelling.  However, the presentation of the paper's novelty and contributions requires serious reevaluation.  Also, the general presentation and writing has significant room for improvement, particularly discussion of the presented results.  Important key details regarding figures are lacking or insufficient to understand exactly what is being stated.  Finally, while the extensions made from 1-bit Adam to produce 0/1 Adam make intuitive sense, it is unclear how exactly these are (or can be made) adaptive in practice, particularly given their predefined values in Algorithm 1.","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666503541809,,-CefY2EOupj,ICLR.cc/2023/Conference/Paper1087/-/Official_Review,,[],2,,['everyone'],-CefY2EOupj,['ICLR.cc/2023/Conference/Paper1087/Reviewer_LNoo'],1666503541809,,1666676956202,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1087/Reviewer_LNoo']","Yes, the peer review suggests the authors to refer to specific literature that may not be already discussed in the original paper. 

The reviewer mentions the paper ""Cada: Communication-adaptive distributed Adam"" by Chen et al. (2021) which is referenced as [1] in the review. This suggests that the authors should be aware of this work as it has tackled the issue of leveraging local SGD with Adam, and the reviewer implies that 1-bit Adam could be trivially combined with CADA.

Additionally, the reviewer mentions specific papers such as Brown et al. (2020) and Smith et al. (2022) which may not be referenced in the original paper. These papers are related to large-scale models (GPT-3 and MT-NLG) and the reviewer suggests that the authors should be aware of these works when making claims about the evaluation of 1-bit Adam and 0/1 Adam on generative models. 

However, without access to the original paper, it is impossible to confirm whether these references are already included or not.",1,"2021, 2020, 2022",Cada: Communication-adaptive distributed Adam 
hxzwqe4EOs,"This paper proposes to add backdoor by designing some specific ""malicious"" data augmentation methods. Some important parts of the paper is not clear to me. Please see my comments below. ","I have the following questions:

1. In traditional backdoor attacks, an identical transformation T is applied on both poisoned training and poisoned test samples. This is practical, for example in BadNet, where we can put the square backdoor sticker onto a stop sign. However, in your methods, is the backdoor transformation T also applied on test images? If yes, why is this practical for your cases (where you use rotation/GAN/AugMix/etc)? The attacker can add those augmentation operations during training under your problem setting. But why they are also used in testing? Is it possible for the attacker to rotate/color invert a physical stop sign? 

2. I don't understand the visualization in Figure 5. Does the right side shows augmented images after ""backdoored AugMix"" images? If yes, I think this backdoor attack is too obvious. Usually backdoor attacks need to be concealed to bypass human inspection. Also, will these seemingly random noise images after backdoored AugMix lead to an acceptable clean accuracy like traditional backdoor attacks? 
One related work [1] regarding learning the adversarial AugMix parameters is missing. 

[1] AugMax: Adversarial Composition of Random Augmentations for Robust Training. NeurIPS, 2021. 

3. Can the proposed new attacks bypass state-of-the-art backdoor defense methods [2,3,4]?

[2] Anti-Backdoor Learning: Training Clean Models on Poisoned Data. NeurIPS, 2021.
[3] Adversarial Unlearning of Backdoors via Implicit Hypergradient. ICLR, 2022.
[4] Trap and replace: Defending backdoor attacks by trapping them into an easy-to-replace subnetwork. NeurIPS, 2022. 

4. Do the proposed new attacks outperform state-of-the-art attacks (such as [5]) in terms of attack success rate and clean accuracy?
[5] Rethinking the Backdoor Attacks’ Triggers: A Frequency Perspective. ICCV, 2021.",I feel a bit confused after reading the first draft. Please answer my above questions before my final evaluation. ,I feel a bit confused after reading the first draft. Please answer my above questions before my final evaluation. ,"4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666838163593,,-CIOGGhkEfy,ICLR.cc/2023/Conference/Paper1445/-/Official_Review,,[],3,,['everyone'],-CIOGGhkEfy,['ICLR.cc/2023/Conference/Paper1445/Reviewer_Z2bo'],1666838163593,,1668525048884,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1445/Reviewer_Z2bo']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers ([1], [2], [3], [4], [5]) and asks the authors to consider them, implying that they are not already included in the original paper. 

Specifically, the reviewer points out that one related work [1] ""AugMax: Adversarial Composition of Random Augmentations for Robust Training. NeurIPS, 2021"" is missing, suggesting that it is not already discussed in the paper. The reviewer also asks the authors to compare their work with state-of-the-art backdoor defense methods ([2], [3], [4]) and attacks ([5]), which implies that these papers are not already thoroughly discussed in the original paper.",1,2021,AugMax Adversarial Composition of Random Augmentations for Robust Training
13TgZvHAY6,"This paper introduces and open-sources one of the largest 100B bilingual pre-trained language model GLM-130B. It covers a detailed pre-training process (training strategies, design choices) and the resulting model outperforms several large language models on English and Chinese zero-shot understanding benchmarks. The author also investigates the efficient inference of the GLM and show the efficacy of using Int4 quantization. 
","GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL
*Paper Summary
This paper introduces and open-sources one of the largest 100B bilingual pre-trained language model GLM-130B. It covers a detailed pre-training process (training strategies, design choices) and the resulting model outperforms several large language models on English and Chinese zero-shot understanding benchmarks. The author also investigates the efficient inference of the GLM and show the efficacy of using Int4 quantization. 

Summary Of Strengths
- The paper is well-written and clearly presented;
- It detailed the efforts made during the pre-training process for the GLM-130B, making it very valuable for practitioners in the large language model pre-training area; (though several design choices might not be feasible to ablate at that scale.). 
- The resulting model was tested across benchmarks, and showed surprising robustness towards quantization, making it more accessible for academia. 


Summary Of Weaknesses
- Why the bi-lingual GLM performs a lot better in LAMBADA as shown in Figure 2? Is this also true for MMLU, Pile, and NLG tasks, any intuition about this?
- Why not compare to OPT-175B models for quantization? 
- Intuition why the few-shot performance of GLM-130B in Table 4. falls behind other methods? Also, will the quantized GLM-130B yield similar few-shot performance?
- Which GPT-3 175B is the GLM-130M compared to? Is it a fairer comparison to the InstructGPT variants?
- How will the author attribute the performance gain (Bi-directional modeling [1], Multi-Task Instruction Pre-Training [2], or the Bilingual corpus and model architecture)?
- It is not surprising that the Bi-directional model will outperform the Uni-directional model in understanding tasks. How are the generation tasks performance of GLM-130B compared to uni-directional counterparts?


[1] Tay, Yi, et al. ""Transcending Scaling Laws with 0.1% Extra Compute."" arXiv preprint arXiv:2210.11399 (2022).

[2] Chung, Hyung Won, et al. ""Scaling Instruction-Finetuned Language Models."" arXiv preprint arXiv:2210.11416 (2022).","- The paper is well-written and clearly presented;
- The model is open-sourced; 
- The design choice and detailed engineering efforts 
- More analysis behind the surging performance will be very valuable to add, for example:
    - Any emergent abilities of the model (in terms of scale and in terms of the amount of data) as shown in [3]. 
    - Why the Bilingual model outperforms the Uni-lingual model by a large margin. 
    - Few-shot performance of quantized GLM-130B; 
","It is not surprising that a large-scale bi-directional language model will outperform the causal counterpart on a range of zero-shot language understanding tasks. The detailed engineering efforts and design choices make this paper valuable, though my main concerns are what factors make the performant GLM-130B unclear, as well as NLG / few-shot performance. 
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666744554221,,-Aw0rrrPUF,ICLR.cc/2023/Conference/Paper3642/-/Official_Review,,[],1,,['everyone'],-Aw0rrrPUF,['ICLR.cc/2023/Conference/Paper3642/Reviewer_asnf'],1666744554221,,1666744554221,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3642/Reviewer_asnf']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Tay, Yi, et al. ""Transcending Scaling Laws with 0.1% Extra Compute."" arXiv preprint arXiv:2210.11399 (2022).
2. Chung, Hyung Won, et al. ""Scaling Instruction-Finetuned Language Models."" arXiv preprint arXiv:2210.11416 (2022).
3. Another reference [3] is mentioned, but the details of the paper are not provided.

These papers are mentioned as relevant to understanding the performance gain of the GLM-130B model and the emergent abilities of the model. The reviewer suggests that the authors should provide more analysis and discussion of these topics, potentially referencing these papers.",1,"2022, 2022, 2022, 2022",Transcending Scaling Laws with 0.1% Extra Compute 
13TgZvHAY6,"This paper introduces and open-sources one of the largest 100B bilingual pre-trained language model GLM-130B. It covers a detailed pre-training process (training strategies, design choices) and the resulting model outperforms several large language models on English and Chinese zero-shot understanding benchmarks. The author also investigates the efficient inference of the GLM and show the efficacy of using Int4 quantization. 
","GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL
*Paper Summary
This paper introduces and open-sources one of the largest 100B bilingual pre-trained language model GLM-130B. It covers a detailed pre-training process (training strategies, design choices) and the resulting model outperforms several large language models on English and Chinese zero-shot understanding benchmarks. The author also investigates the efficient inference of the GLM and show the efficacy of using Int4 quantization. 

Summary Of Strengths
- The paper is well-written and clearly presented;
- It detailed the efforts made during the pre-training process for the GLM-130B, making it very valuable for practitioners in the large language model pre-training area; (though several design choices might not be feasible to ablate at that scale.). 
- The resulting model was tested across benchmarks, and showed surprising robustness towards quantization, making it more accessible for academia. 


Summary Of Weaknesses
- Why the bi-lingual GLM performs a lot better in LAMBADA as shown in Figure 2? Is this also true for MMLU, Pile, and NLG tasks, any intuition about this?
- Why not compare to OPT-175B models for quantization? 
- Intuition why the few-shot performance of GLM-130B in Table 4. falls behind other methods? Also, will the quantized GLM-130B yield similar few-shot performance?
- Which GPT-3 175B is the GLM-130M compared to? Is it a fairer comparison to the InstructGPT variants?
- How will the author attribute the performance gain (Bi-directional modeling [1], Multi-Task Instruction Pre-Training [2], or the Bilingual corpus and model architecture)?
- It is not surprising that the Bi-directional model will outperform the Uni-directional model in understanding tasks. How are the generation tasks performance of GLM-130B compared to uni-directional counterparts?


[1] Tay, Yi, et al. ""Transcending Scaling Laws with 0.1% Extra Compute."" arXiv preprint arXiv:2210.11399 (2022).

[2] Chung, Hyung Won, et al. ""Scaling Instruction-Finetuned Language Models."" arXiv preprint arXiv:2210.11416 (2022).","- The paper is well-written and clearly presented;
- The model is open-sourced; 
- The design choice and detailed engineering efforts 
- More analysis behind the surging performance will be very valuable to add, for example:
    - Any emergent abilities of the model (in terms of scale and in terms of the amount of data) as shown in [3]. 
    - Why the Bilingual model outperforms the Uni-lingual model by a large margin. 
    - Few-shot performance of quantized GLM-130B; 
","It is not surprising that a large-scale bi-directional language model will outperform the causal counterpart on a range of zero-shot language understanding tasks. The detailed engineering efforts and design choices make this paper valuable, though my main concerns are what factors make the performant GLM-130B unclear, as well as NLG / few-shot performance. 
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,"4: The contributions are significant, and do not exist in prior works.",NO.,"8: accept, good paper",1666744554221,,-Aw0rrrPUF,ICLR.cc/2023/Conference/Paper3642/-/Official_Review,,[],1,,['everyone'],-Aw0rrrPUF,['ICLR.cc/2023/Conference/Paper3642/Reviewer_asnf'],1666744554221,,1666744554221,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper3642/Reviewer_asnf']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Tay, Yi, et al. ""Transcending Scaling Laws with 0.1% Extra Compute."" arXiv preprint arXiv:2210.11399 (2022).
2. Chung, Hyung Won, et al. ""Scaling Instruction-Finetuned Language Models."" arXiv preprint arXiv:2210.11416 (2022).
3. Another reference [3] is mentioned, but the details of the paper are not provided.

These papers are mentioned as relevant to understanding the performance gain of the GLM-130B model and the emergent abilities of the model. The reviewer suggests that the authors should provide more analysis and discussion of these topics, potentially referencing these papers.",1,"2022, 2022, 2022, 2022",Scaling Instruction-Finetuned Language Models
ILiID4i_ZV,"The paper observes experimentally that different deep network models learn datapoints at the different paces when trained with noisy labels, in contrast to the same learning dynamics known to hold for different deep network models when the labels are clean. It uses this observation to estimate the presence and level of label noise using a simple algorithm (DisagreeNet) that computes the difference between the training paces of different models via the ELP (""Ensemble Learning Pace"") metric. ","Strengths:

+ The method is simple and practical, with the additional benefit of avoiding the need to set aside clean label dataset. 

+ The experiments are extensive, the results convincingly show that DisagreeNet performs well in general in regard to the state of the art. Some minor details such as confidence intervals in Fig 2 (a) would be welcome.

Weaknesses or questions:

- The main question is related to the novelty, as the use of the predictions of ensembles of models to filter noisy labeled data is not novel (See point 2 below).

- The theoretical claim of Section 3.1 does not bring much to the paper (and it weakens it in my opinion): first it applies only a linear regression with GD, which is a very easy setting to handle since it can be solved exactly, and second the proof is not clear despite the simplicity of the setting (see point 3 below). 

- Despite its simplicity, the method can be computationally heavy because it can require some ten models (for instance in hard noise scenarios, as mentioned in Section 5.4). Although the experimental comparison is extensive on the performance of estimating label noise level, it does not include this metric.","1. The paper is well written, with very few typos left (e.g., result and not results in last paragraph of Page 1, model and not mdoel on page 14). It would be good to expend some acronyms too (Sota for State of the art, BMM for Bernouilli Mixture Model),


2. Using predictions of ensembles of models to filter noisy labeled data has been proposed by a couple of recent papers on arxiv from early 2022. First Liang et al, in ""Tripartite: Tackle Noisy Labels by a More Precise Partition"", use the inconsistency of predictions of two networks to not only detect noisy samples from clean ones, but also to further select hard samples from noisy samples. Two important questions would need to be addressed:
- Does DisagreeNet also detect clean but hard samples as noisy-labeled samples?
- How does DisagreeNet perform, compared to the method in Liang et al?

Second, the disagreement of an ensemble of models is also studied by Simsek, Hall, Sagun, in ""Understanding out-of-distribution accuracies through quantifying difficulty of test samples"", to detect difficulty of test images. They propose a ""confusion score"" which seems to be the same as the cumulative loss defined just after eq (1) in the current paper. 
- What is the difference between DisagreeNet and the method based on these confusion scores proposed by Simsek et al? 
- How do they compare in terms of detecting noisy samples?


3. The proofs supporting the claim in Section 3.1 are not clear, despite bearing on a very simple linear regression model:

- $w_0, \hat{w}$ are stated to belong to $R^M$ above equation (3) and after equation (4), whereas $w(i), \hat{w}(i)$ belong to $R^d$ at the end of the section. They should all belong to $R^d$ as far as I can tell.

- Then it is not clear how $e(i,t)$ is computed - from the last sentence page 14, I understand that $i \in [Q], t \in [N]$, which would make $e(i,t)$ a $Q \times N$ matrix. From the definition of ""cross-error"", I gather that $e(i,t) = w(i)X(t) - y(t)$, is it the case? How is $X(t), y(t)$ defined - they are no longer of the same dimension as $X(j), y(j)$ so $t$ is not simply an index but denote something different than $i, j, \ldots$? Notations are not very clear. 

- The only source of randomness, as far as I can tell, seems to be the initial value $w^0$. There is also an implicit assumption that its expectation is zero - is it the case? If so then conditioning on the initial weights, everything else becomes deterministic, and since the number of iterations is taken large enough, the variables of interest are all asymptotically deterministic: not sure why one still speaks of ""correlation"" in such a context.

- There are other unclear points in the proof, for example the second term on the second line of (10), or a factor $1/2$ in the equations in the proof of the theorem page 16 from the second term of the first line. ","This is a paper over an interesting and timely topic, which proposes a solution that comes with practical advantages, such as its simplicity and the absence of the need for a clean fraction of the dataset or for priors on the noise level. Despite its simplicity, the method may be computationally heavy. A couple of similar schemes have been proposed earlier this year, and the novelty with respect to these should be clarified. The experiments should include them as benchmarks, but they are otherwise extensive and strong (contrary to the theoretical contribution). ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666621499491,,-AdWUM183OU,ICLR.cc/2023/Conference/Paper2487/-/Official_Review,,[],3,,['everyone'],-AdWUM183OU,['ICLR.cc/2023/Conference/Paper2487/Reviewer_ft5z'],1666621499491,,1666621499491,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2487/Reviewer_ft5z']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Liang et al, ""Tripartite: Tackle Noisy Labels by a More Precise Partition"" (2022) - The reviewer asks the authors to compare their method (DisagreeNet) with this paper and address specific questions.
2. Simsek, Hall, Sagun, ""Understanding out-of-distribution accuracies through quantifying difficulty of test samples"" (2022) - The reviewer asks the authors to compare their method with this paper and clarify the differences between DisagreeNet and the method based on ""confusion scores"" proposed by Simsek et al.

These papers are mentioned as recent works on arxiv from early 2022, suggesting that they may not have been discussed in the original paper. The reviewer's comments imply that the authors should be aware of these works and address the similarities and differences between their method and these existing approaches.",1,"2022, 2022, 2022",Tripartite Tackle Noisy Labels by a More Precise Partition 
ILiID4i_ZV,"The paper observes experimentally that different deep network models learn datapoints at the different paces when trained with noisy labels, in contrast to the same learning dynamics known to hold for different deep network models when the labels are clean. It uses this observation to estimate the presence and level of label noise using a simple algorithm (DisagreeNet) that computes the difference between the training paces of different models via the ELP (""Ensemble Learning Pace"") metric. ","Strengths:

+ The method is simple and practical, with the additional benefit of avoiding the need to set aside clean label dataset. 

+ The experiments are extensive, the results convincingly show that DisagreeNet performs well in general in regard to the state of the art. Some minor details such as confidence intervals in Fig 2 (a) would be welcome.

Weaknesses or questions:

- The main question is related to the novelty, as the use of the predictions of ensembles of models to filter noisy labeled data is not novel (See point 2 below).

- The theoretical claim of Section 3.1 does not bring much to the paper (and it weakens it in my opinion): first it applies only a linear regression with GD, which is a very easy setting to handle since it can be solved exactly, and second the proof is not clear despite the simplicity of the setting (see point 3 below). 

- Despite its simplicity, the method can be computationally heavy because it can require some ten models (for instance in hard noise scenarios, as mentioned in Section 5.4). Although the experimental comparison is extensive on the performance of estimating label noise level, it does not include this metric.","1. The paper is well written, with very few typos left (e.g., result and not results in last paragraph of Page 1, model and not mdoel on page 14). It would be good to expend some acronyms too (Sota for State of the art, BMM for Bernouilli Mixture Model),


2. Using predictions of ensembles of models to filter noisy labeled data has been proposed by a couple of recent papers on arxiv from early 2022. First Liang et al, in ""Tripartite: Tackle Noisy Labels by a More Precise Partition"", use the inconsistency of predictions of two networks to not only detect noisy samples from clean ones, but also to further select hard samples from noisy samples. Two important questions would need to be addressed:
- Does DisagreeNet also detect clean but hard samples as noisy-labeled samples?
- How does DisagreeNet perform, compared to the method in Liang et al?

Second, the disagreement of an ensemble of models is also studied by Simsek, Hall, Sagun, in ""Understanding out-of-distribution accuracies through quantifying difficulty of test samples"", to detect difficulty of test images. They propose a ""confusion score"" which seems to be the same as the cumulative loss defined just after eq (1) in the current paper. 
- What is the difference between DisagreeNet and the method based on these confusion scores proposed by Simsek et al? 
- How do they compare in terms of detecting noisy samples?


3. The proofs supporting the claim in Section 3.1 are not clear, despite bearing on a very simple linear regression model:

- $w_0, \hat{w}$ are stated to belong to $R^M$ above equation (3) and after equation (4), whereas $w(i), \hat{w}(i)$ belong to $R^d$ at the end of the section. They should all belong to $R^d$ as far as I can tell.

- Then it is not clear how $e(i,t)$ is computed - from the last sentence page 14, I understand that $i \in [Q], t \in [N]$, which would make $e(i,t)$ a $Q \times N$ matrix. From the definition of ""cross-error"", I gather that $e(i,t) = w(i)X(t) - y(t)$, is it the case? How is $X(t), y(t)$ defined - they are no longer of the same dimension as $X(j), y(j)$ so $t$ is not simply an index but denote something different than $i, j, \ldots$? Notations are not very clear. 

- The only source of randomness, as far as I can tell, seems to be the initial value $w^0$. There is also an implicit assumption that its expectation is zero - is it the case? If so then conditioning on the initial weights, everything else becomes deterministic, and since the number of iterations is taken large enough, the variables of interest are all asymptotically deterministic: not sure why one still speaks of ""correlation"" in such a context.

- There are other unclear points in the proof, for example the second term on the second line of (10), or a factor $1/2$ in the equations in the proof of the theorem page 16 from the second term of the first line. ","This is a paper over an interesting and timely topic, which proposes a solution that comes with practical advantages, such as its simplicity and the absence of the need for a clean fraction of the dataset or for priors on the noise level. Despite its simplicity, the method may be computationally heavy. A couple of similar schemes have been proposed earlier this year, and the novelty with respect to these should be clarified. The experiments should include them as benchmarks, but they are otherwise extensive and strong (contrary to the theoretical contribution). ","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,NO.,6: marginally above the acceptance threshold,1666621499491,,-AdWUM183OU,ICLR.cc/2023/Conference/Paper2487/-/Official_Review,,[],3,,['everyone'],-AdWUM183OU,['ICLR.cc/2023/Conference/Paper2487/Reviewer_ft5z'],1666621499491,,1666621499491,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2487/Reviewer_ft5z']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions two specific papers:

1. Liang et al, ""Tripartite: Tackle Noisy Labels by a More Precise Partition"" (2022) - The reviewer asks the authors to compare their method (DisagreeNet) with this paper and address specific questions.
2. Simsek, Hall, Sagun, ""Understanding out-of-distribution accuracies through quantifying difficulty of test samples"" (2022) - The reviewer asks the authors to compare their method with this paper and clarify the differences between DisagreeNet and the method based on ""confusion scores"" proposed by Simsek et al.

These papers are mentioned as recent works on arxiv from early 2022, suggesting that they may not have been discussed in the original paper. The reviewer's comments imply that the authors should be aware of these works and address the similarities and differences between their method and these existing approaches.",1,"2022, 2022, 2022",Understanding out-of-distribution accuracies through quantifying difficulty of test samples
-Mxiuyto73,"The paper tries to address the memory scalability issue of MBRL in a very long-time horizon or even infinitely time horizon (say continual learning) settings. An observation is that the model would become more accurate as training proceeds, and some experiences can be discarded. The paper tries to select those experiences which cannot be predicted well by the model to store in the replay buffer. To do this, the paper proposes a method to determine the certainty of the model’s prediction. The method is to measure the Wasserstein distance between the reward distributions of predicted and true trajectories. If the distance is larger some threshold the trajectory would be stored. 

To further reduce the computation cost, the authors propose only training the model when sufficient novel experiences are acquired, which is done by simply introducing an additional parameter adjusting how many experiences should be considered as “sufficient”. Experiments are conducted on both standard RL tasks and continual learning tasks. ","Strength: 

The paper studies an important and interesting topic — I agree that the memory and computation scalability issue is important in an extremely long-time horizon learning setting.

Weaknesses: 

----------
Novelty. The paper is mostly combing some intuitive designs together to form an algorithm. I did not see the exactly same algorithm before. But the ideas behind the algorithm are already studied. 

----------
Soundness. 
The paper is mostly empirical, and the algorithmic design is mostly based on intuition. Though it is fine not to have theoretical support, each design choice should be well-justified. However, many design choices in the paper could be problematically intuitively. Here are some examples. 

First, using reward distribution to judge whether a trajectory is novel or not is not well-justified. The reward distribution match may not imply anything about the underlying state-action-state transitions. I cannot say that such a reward-based measure does not work at all, but the paper does not provide any insight into why it works. 

Second, it is hard to believe why the method can avoid catastrophic forgetting. Intuitively, if you discard some experiences which have been predicted accurately, the model can still forget those experiences as the sampling distribution shifts (because the policy is changing). 

Third, the authors use trajectory optimization and CEM in the MBRL method. There are large amounts of MBRL algorithms. It is better to provide some reasons why the particular method is chosen. 

I strongly suggest the authors carefully justify each design choice they used. 

----------
Missing a large body of related works. 
1. Some simple and theoretically sound memory-saving methods, such as reservoir sampling (I think this should be a simple baseline and it is easy to implement) 

2. A body of work saving memory by using theoretically justified approaches in MBRL (e.g. organizing experiences: a deeper look at replay mechanisms for sample-based planning … by Pan et al.)

3. Decision/value-aware robust model-based RL by Amir-massoud et al.

4. There are also papers studying/using techniques to avoid/reduce compounding errors in long-time horizon predictions. 

5. It is also good to survey papers about avoiding catastrophic forgetting in a supervised learning setting. 

----------
The empirical results should at least justify the key claims of the paper:
1. why the particular uncertainty measure is needed/better than the existing uncertainty measure
2. why the proposed method can avoid catastrophic forgetting ",See the above section. ,"The paper is most empirical. But it obviously lacks justifications for several important design choices. It also lacks some literature review, and hence some intuitive baselines are not included in the comparison. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666671032714,,-5fSvp1ofdd,ICLR.cc/2023/Conference/Paper1729/-/Official_Review,,[],3,,['everyone'],-5fSvp1ofdd,['ICLR.cc/2023/Conference/Paper1729/Reviewer_SwfR'],1666671032714,,1666671032714,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1729/Reviewer_SwfR']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers and concepts, such as:

1. Reservoir sampling as a simple baseline
2. ""Organizing experiences: a deeper look at replay mechanisms for sample-based planning"" by Pan et al.
3. ""Decision/value-aware robust model-based RL"" by Amir-massoud et al.
4. Papers on avoiding/reducing compounding errors in long-time horizon predictions
5. Papers on avoiding catastrophic forgetting in supervised learning settings

These references are not mentioned as being already included in the original paper, but rather as missing from the literature review. The reviewer suggests that the authors should be aware of and discuss these related works to improve the paper's completeness and validity.",1,,Organizing experiences: a deeper look at replay mechanisms for sample-based planning 
-Mxiuyto73,"The paper tries to address the memory scalability issue of MBRL in a very long-time horizon or even infinitely time horizon (say continual learning) settings. An observation is that the model would become more accurate as training proceeds, and some experiences can be discarded. The paper tries to select those experiences which cannot be predicted well by the model to store in the replay buffer. To do this, the paper proposes a method to determine the certainty of the model’s prediction. The method is to measure the Wasserstein distance between the reward distributions of predicted and true trajectories. If the distance is larger some threshold the trajectory would be stored. 

To further reduce the computation cost, the authors propose only training the model when sufficient novel experiences are acquired, which is done by simply introducing an additional parameter adjusting how many experiences should be considered as “sufficient”. Experiments are conducted on both standard RL tasks and continual learning tasks. ","Strength: 

The paper studies an important and interesting topic — I agree that the memory and computation scalability issue is important in an extremely long-time horizon learning setting.

Weaknesses: 

----------
Novelty. The paper is mostly combing some intuitive designs together to form an algorithm. I did not see the exactly same algorithm before. But the ideas behind the algorithm are already studied. 

----------
Soundness. 
The paper is mostly empirical, and the algorithmic design is mostly based on intuition. Though it is fine not to have theoretical support, each design choice should be well-justified. However, many design choices in the paper could be problematically intuitively. Here are some examples. 

First, using reward distribution to judge whether a trajectory is novel or not is not well-justified. The reward distribution match may not imply anything about the underlying state-action-state transitions. I cannot say that such a reward-based measure does not work at all, but the paper does not provide any insight into why it works. 

Second, it is hard to believe why the method can avoid catastrophic forgetting. Intuitively, if you discard some experiences which have been predicted accurately, the model can still forget those experiences as the sampling distribution shifts (because the policy is changing). 

Third, the authors use trajectory optimization and CEM in the MBRL method. There are large amounts of MBRL algorithms. It is better to provide some reasons why the particular method is chosen. 

I strongly suggest the authors carefully justify each design choice they used. 

----------
Missing a large body of related works. 
1. Some simple and theoretically sound memory-saving methods, such as reservoir sampling (I think this should be a simple baseline and it is easy to implement) 

2. A body of work saving memory by using theoretically justified approaches in MBRL (e.g. organizing experiences: a deeper look at replay mechanisms for sample-based planning … by Pan et al.)

3. Decision/value-aware robust model-based RL by Amir-massoud et al.

4. There are also papers studying/using techniques to avoid/reduce compounding errors in long-time horizon predictions. 

5. It is also good to survey papers about avoiding catastrophic forgetting in a supervised learning setting. 

----------
The empirical results should at least justify the key claims of the paper:
1. why the particular uncertainty measure is needed/better than the existing uncertainty measure
2. why the proposed method can avoid catastrophic forgetting ",See the above section. ,"The paper is most empirical. But it obviously lacks justifications for several important design choices. It also lacks some literature review, and hence some intuitive baselines are not included in the comparison. ",3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.,2: Several of the paper’s claims are incorrect or not well-supported.,2: The contributions are only marginally significant or novel.,2: The contributions are only marginally significant or novel.,NO.,"3: reject, not good enough",1666671032714,,-5fSvp1ofdd,ICLR.cc/2023/Conference/Paper1729/-/Official_Review,,[],3,,['everyone'],-5fSvp1ofdd,['ICLR.cc/2023/Conference/Paper1729/Reviewer_SwfR'],1666671032714,,1666671032714,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper1729/Reviewer_SwfR']","Yes, the peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers and concepts, such as:

1. Reservoir sampling as a simple baseline
2. ""Organizing experiences: a deeper look at replay mechanisms for sample-based planning"" by Pan et al.
3. ""Decision/value-aware robust model-based RL"" by Amir-massoud et al.
4. Papers on avoiding/reducing compounding errors in long-time horizon predictions
5. Papers on avoiding catastrophic forgetting in supervised learning settings

These references are not mentioned as being already included in the original paper, but rather as missing from the literature review. The reviewer suggests that the authors should be aware of and discuss these related works to improve the paper's completeness and validity.",1,,Decision/value-aware robust model-based RL
fhGddA9MTbP,"In this paper, the authors proposed an implicit bias called robust memorization in adversarial training under the realistic data assumption. By function approximation theory, the authors proved that ReLU nets with efficient size have the ability to achieve robust memorization, while robust generalization requires exponentially large models. The authors also demonstrate robust memorization in adversarial training from both empirical and theoretical perspectives.  
","Strength
1. Propose a new implicit bias called robust memorization to explain the empirical behavior of adversarial training
2. Provide both empirical and theoretical results showing evidence from different perspectives


Weakness
1. This new implicit bias still remains a conjecture in my opinion. There lacks strong evidence to show that it is actually what happens.
2. The empirical results is somewhat known, and do not provide many new information
3. The authors did not compare their theoretical analysis on robust memorialization with previous theoretical studies in adversarial training
",1. Not sure what Figure 1 is trying to express in the intro part. ,"1. It seems that f_adv in page 2 is constructed, not trained. By “adversarial training error” did the author mean an adversarial loss of training data? If so, why does it implies “global convergence of adversarial training”?

2. This new implicit bias still remains a conjecture in my opinion. First, a network that is able to represent the target functions f_adv does not mean it will lean towards f_adv. Second, the empirical study that the maximum gradient norm is sharp outside the perturbation region only suggests on training data it is robust. It cannot show anything related to the outside data points.

3. Dividing data into small-margin and large-margin ones could fail in modern deep learning situations where each data point can easily find a small margin to any other classes (targeted adversarial examples). In other words, $\mu$ could be just 1. 

4. For simple data distributions, previous works have shown that adversarial training can even achieve robust generalization, such as Dan et al. 2020 and

    ""Precise statistical analysis of classification accuracies for adversarial training."" arXiv preprint arXiv:2010.11213 (2020).

    ""Benign Overfitting in Adversarially Robust Linear Classification."" arXiv preprint arXiv:2112.15250 (2021).

     Do the authors' results contradictory to theirs? The authors may want to comment on this.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666717812929,,-4Maz7s3YXz,ICLR.cc/2023/Conference/Paper2785/-/Official_Review,,[],3,,['everyone'],-4Maz7s3YXz,['ICLR.cc/2023/Conference/Paper2785/Reviewer_59uS'],1666717812929,,1666728367569,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2785/Reviewer_59uS']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers, including:

1. Dan et al. 2020
2. ""Precise statistical analysis of classification accuracies for adversarial training."" arXiv preprint arXiv:2010.11213 (2020)
3. ""Benign Overfitting in Adversarially Robust Linear Classification."" arXiv preprint arXiv:2112.15250 (2021)

The reviewer asks the authors to comment on how their results relate to these previous works, implying that these papers are not already discussed in the original paper.",1,"2020, 2020, 2020, 2021, 2021",Precise statistical analysis of classification accuracies for adversarial training 
fhGddA9MTbP,"In this paper, the authors proposed an implicit bias called robust memorization in adversarial training under the realistic data assumption. By function approximation theory, the authors proved that ReLU nets with efficient size have the ability to achieve robust memorization, while robust generalization requires exponentially large models. The authors also demonstrate robust memorization in adversarial training from both empirical and theoretical perspectives.  
","Strength
1. Propose a new implicit bias called robust memorization to explain the empirical behavior of adversarial training
2. Provide both empirical and theoretical results showing evidence from different perspectives


Weakness
1. This new implicit bias still remains a conjecture in my opinion. There lacks strong evidence to show that it is actually what happens.
2. The empirical results is somewhat known, and do not provide many new information
3. The authors did not compare their theoretical analysis on robust memorialization with previous theoretical studies in adversarial training
",1. Not sure what Figure 1 is trying to express in the intro part. ,"1. It seems that f_adv in page 2 is constructed, not trained. By “adversarial training error” did the author mean an adversarial loss of training data? If so, why does it implies “global convergence of adversarial training”?

2. This new implicit bias still remains a conjecture in my opinion. First, a network that is able to represent the target functions f_adv does not mean it will lean towards f_adv. Second, the empirical study that the maximum gradient norm is sharp outside the perturbation region only suggests on training data it is robust. It cannot show anything related to the outside data points.

3. Dividing data into small-margin and large-margin ones could fail in modern deep learning situations where each data point can easily find a small margin to any other classes (targeted adversarial examples). In other words, $\mu$ could be just 1. 

4. For simple data distributions, previous works have shown that adversarial training can even achieve robust generalization, such as Dan et al. 2020 and

    ""Precise statistical analysis of classification accuracies for adversarial training."" arXiv preprint arXiv:2010.11213 (2020).

    ""Benign Overfitting in Adversarially Robust Linear Classification."" arXiv preprint arXiv:2112.15250 (2021).

     Do the authors' results contradictory to theirs? The authors may want to comment on this.
","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,5: marginally below the acceptance threshold,1666717812929,,-4Maz7s3YXz,ICLR.cc/2023/Conference/Paper2785/-/Official_Review,,[],3,,['everyone'],-4Maz7s3YXz,['ICLR.cc/2023/Conference/Paper2785/Reviewer_59uS'],1666717812929,,1666728367569,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper2785/Reviewer_59uS']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. The reviewer mentions several papers, including:

1. Dan et al. 2020
2. ""Precise statistical analysis of classification accuracies for adversarial training."" arXiv preprint arXiv:2010.11213 (2020)
3. ""Benign Overfitting in Adversarially Robust Linear Classification."" arXiv preprint arXiv:2112.15250 (2021)

The reviewer asks the authors to comment on how their results relate to these previous works, implying that these papers are not already discussed in the original paper.",1,"2020, 2020, 2020, 2021, 2021",Benign Overfitting in Adversarially Robust Linear Classification 
u4_v7_F4NWE,"The authors propose a new data augmentation for video classification that consists of shifting one of the RGB channels forwards or backwards by 1 frame. They call this augmentation ""Ghost Motion"" (GM). They also show that scaling the logits by a temperature helps generalization. The authors claim that GM improves results for a variety of different models and datasets.","Strengths:
1) The proposed data augmentation technique is well explored and has an extensive empirical evaluation. I appreciate that the authors investigated it under a number of different models and datasets. While they briefly also investigate modern approaches based on Transformers, most of their results focus on out-dated models based on 2D CNNs. I feel that the paper could be improved by focusing more on modern models. E.g. Table 1 contains results for TSN, which is by now a 6 year old method. I don't think these results have any relevance for the research community. Adding a modern, state-of-the-art method like Video Swin Transformer[4], MViT[5] or MTV[6] instead would considerably strengthen the paper. 


Weaknesses
1)  The ""logit scaling"" part of the current manuscript is weak: it's a completely unrelated regularization technique to Ghost Motion, and as the authors do mention, the same technique was furthermore already proposed and investigated for classification tasks in [3]. So mentioning this as a novel contribution is a stretch.  Furthermore it seems to be just another variation of label smoothing [1] or logit squeezing [2], neither of which the authors compare to or even mention. I'd suggest de-empathising this part of the paper and focusing completely on Ghost Motion instead. Otherwise, the authors need to properly compare logit scaling to competing methods.

2)  The results for the baselines stated by the authors do not match those from the original publications. For example, the authors claim in Table 1 that the basic TSM module achieves 45.34% accuracy on SomethingSomethingV1, and that TSM+GM together achieve 46.88%.
However, the original TSM publication states that TSM alone achieves 47.3% accuracy. Similarly, the original TDN publication claims 52.3% top1 accuracy on SomethingSomethingV1, while this manuscript claims that TDN alone achieves 49.69% and TDN+GN  50.51%. I did not check the rest of the numbers, but this small spot check seems weird.  The authors need to address this discrepancy.

3) While the authors claim several times that GM ""brings almost zero costs"", that claim was never backed  up. I'd appreciate if the authors could provide insights on the computational cos of their method: does adding GM really not increase the wall-clock time of training? 

4) The authors give all accuracies with 4 significant digits, which would suggest a large number of repeat experiments, yet they do neither mention this nor give error bars. I would strongly suggest they add unambiguous error bars for their results.


[1] Mueller et al., ""When does label smoothing help?"", NeurIPS 2019 (and references therein)

[2] Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019

[3] Agarwala et al, Temperature check: theory and practice for training models with softmax-cross-entropy losses, arxiv 2019

[4] Liu et al, ""Video Swin Transformer"", CVPR 2022

[5] Fan et al, ""Multiscale Vision Transformers"", ICCV 2021

[6] Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022","1) The paper is riddled with grammatical errors and un-idiomatic language. This needs to be addressed before this manuscript can be accepted for publication. The mistakes were too abundant to all write down and note, so I stopped taking notes on this after the abstract:

“overfitting is an even severe problem in 2D video recognition models” => “overfitting is a severe problem”

“current pipeline treats background and semantic frames” => pipelines. (Also, “non-nformative” and “informative” might be better than 

“background and semantic”. The latter made me assume this work was about semantic segmentation, but YMMV.

“named as Ghost Motion” => named Ghost Motion

“GM shifts channels along temporal dimension” => along the temporal dimension

“leading to improvement” => improvements


I suggest the authors revise the manuscript intensively and extensively for such errors.



2) Regarding Originality, GM is a simplification of the Temporal shift module (TSM), but the authors show in their experiments that GM improves upon TSM models. As mentioned above, I have strong doubts about the originality of logit scaling.","The authors propose a  input augmentation that appears to be effective on a wide range of problems and models. The empirical evaluation covers a wide range of models and datasets, yet the numbers raise some questions (see above).

UPDATE: after the author's revision of the paper, I have updated my score to reflect that I think the method now meets the standards for publication.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1667242787303,,-0tPmzgXS5,ICLR.cc/2023/Conference/Paper211/-/Official_Review,,[],3,,['everyone'],-0tPmzgXS5,['ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd'],1667242787303,,1668672033429,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions the following papers that might not be included in the original paper:
1. Mueller et al., ""When does label smoothing help?"", NeurIPS 2019
2. Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019
3. Liu et al, ""Video Swin Transformer"", CVPR 2022
4. Fan et al, ""Multiscale Vision Transformers"", ICCV 2021
5. Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022

These papers are related to logit scaling, label smoothing, and modern video recognition models, which the reviewer thinks the authors should discuss or compare with their proposed method.",1,"2019, 2019, 2022, 2021, 2022",When does label smoothing help?
u4_v7_F4NWE,"The authors propose a new data augmentation for video classification that consists of shifting one of the RGB channels forwards or backwards by 1 frame. They call this augmentation ""Ghost Motion"" (GM). They also show that scaling the logits by a temperature helps generalization. The authors claim that GM improves results for a variety of different models and datasets.","Strengths:
1) The proposed data augmentation technique is well explored and has an extensive empirical evaluation. I appreciate that the authors investigated it under a number of different models and datasets. While they briefly also investigate modern approaches based on Transformers, most of their results focus on out-dated models based on 2D CNNs. I feel that the paper could be improved by focusing more on modern models. E.g. Table 1 contains results for TSN, which is by now a 6 year old method. I don't think these results have any relevance for the research community. Adding a modern, state-of-the-art method like Video Swin Transformer[4], MViT[5] or MTV[6] instead would considerably strengthen the paper. 


Weaknesses
1)  The ""logit scaling"" part of the current manuscript is weak: it's a completely unrelated regularization technique to Ghost Motion, and as the authors do mention, the same technique was furthermore already proposed and investigated for classification tasks in [3]. So mentioning this as a novel contribution is a stretch.  Furthermore it seems to be just another variation of label smoothing [1] or logit squeezing [2], neither of which the authors compare to or even mention. I'd suggest de-empathising this part of the paper and focusing completely on Ghost Motion instead. Otherwise, the authors need to properly compare logit scaling to competing methods.

2)  The results for the baselines stated by the authors do not match those from the original publications. For example, the authors claim in Table 1 that the basic TSM module achieves 45.34% accuracy on SomethingSomethingV1, and that TSM+GM together achieve 46.88%.
However, the original TSM publication states that TSM alone achieves 47.3% accuracy. Similarly, the original TDN publication claims 52.3% top1 accuracy on SomethingSomethingV1, while this manuscript claims that TDN alone achieves 49.69% and TDN+GN  50.51%. I did not check the rest of the numbers, but this small spot check seems weird.  The authors need to address this discrepancy.

3) While the authors claim several times that GM ""brings almost zero costs"", that claim was never backed  up. I'd appreciate if the authors could provide insights on the computational cos of their method: does adding GM really not increase the wall-clock time of training? 

4) The authors give all accuracies with 4 significant digits, which would suggest a large number of repeat experiments, yet they do neither mention this nor give error bars. I would strongly suggest they add unambiguous error bars for their results.


[1] Mueller et al., ""When does label smoothing help?"", NeurIPS 2019 (and references therein)

[2] Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019

[3] Agarwala et al, Temperature check: theory and practice for training models with softmax-cross-entropy losses, arxiv 2019

[4] Liu et al, ""Video Swin Transformer"", CVPR 2022

[5] Fan et al, ""Multiscale Vision Transformers"", ICCV 2021

[6] Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022","1) The paper is riddled with grammatical errors and un-idiomatic language. This needs to be addressed before this manuscript can be accepted for publication. The mistakes were too abundant to all write down and note, so I stopped taking notes on this after the abstract:

“overfitting is an even severe problem in 2D video recognition models” => “overfitting is a severe problem”

“current pipeline treats background and semantic frames” => pipelines. (Also, “non-nformative” and “informative” might be better than 

“background and semantic”. The latter made me assume this work was about semantic segmentation, but YMMV.

“named as Ghost Motion” => named Ghost Motion

“GM shifts channels along temporal dimension” => along the temporal dimension

“leading to improvement” => improvements


I suggest the authors revise the manuscript intensively and extensively for such errors.



2) Regarding Originality, GM is a simplification of the Temporal shift module (TSM), but the authors show in their experiments that GM improves upon TSM models. As mentioned above, I have strong doubts about the originality of logit scaling.","The authors propose a  input augmentation that appears to be effective on a wide range of problems and models. The empirical evaluation covers a wide range of models and datasets, yet the numbers raise some questions (see above).

UPDATE: after the author's revision of the paper, I have updated my score to reflect that I think the method now meets the standards for publication.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1667242787303,,-0tPmzgXS5,ICLR.cc/2023/Conference/Paper211/-/Official_Review,,[],3,,['everyone'],-0tPmzgXS5,['ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd'],1667242787303,,1668672033429,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions the following papers that might not be included in the original paper:
1. Mueller et al., ""When does label smoothing help?"", NeurIPS 2019
2. Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019
3. Liu et al, ""Video Swin Transformer"", CVPR 2022
4. Fan et al, ""Multiscale Vision Transformers"", ICCV 2021
5. Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022

These papers are related to logit scaling, label smoothing, and modern video recognition models, which the reviewer thinks the authors should discuss or compare with their proposed method.",1,"2019, 2019, 2022, 2021, 2022",Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? 
u4_v7_F4NWE,"The authors propose a new data augmentation for video classification that consists of shifting one of the RGB channels forwards or backwards by 1 frame. They call this augmentation ""Ghost Motion"" (GM). They also show that scaling the logits by a temperature helps generalization. The authors claim that GM improves results for a variety of different models and datasets.","Strengths:
1) The proposed data augmentation technique is well explored and has an extensive empirical evaluation. I appreciate that the authors investigated it under a number of different models and datasets. While they briefly also investigate modern approaches based on Transformers, most of their results focus on out-dated models based on 2D CNNs. I feel that the paper could be improved by focusing more on modern models. E.g. Table 1 contains results for TSN, which is by now a 6 year old method. I don't think these results have any relevance for the research community. Adding a modern, state-of-the-art method like Video Swin Transformer[4], MViT[5] or MTV[6] instead would considerably strengthen the paper. 


Weaknesses
1)  The ""logit scaling"" part of the current manuscript is weak: it's a completely unrelated regularization technique to Ghost Motion, and as the authors do mention, the same technique was furthermore already proposed and investigated for classification tasks in [3]. So mentioning this as a novel contribution is a stretch.  Furthermore it seems to be just another variation of label smoothing [1] or logit squeezing [2], neither of which the authors compare to or even mention. I'd suggest de-empathising this part of the paper and focusing completely on Ghost Motion instead. Otherwise, the authors need to properly compare logit scaling to competing methods.

2)  The results for the baselines stated by the authors do not match those from the original publications. For example, the authors claim in Table 1 that the basic TSM module achieves 45.34% accuracy on SomethingSomethingV1, and that TSM+GM together achieve 46.88%.
However, the original TSM publication states that TSM alone achieves 47.3% accuracy. Similarly, the original TDN publication claims 52.3% top1 accuracy on SomethingSomethingV1, while this manuscript claims that TDN alone achieves 49.69% and TDN+GN  50.51%. I did not check the rest of the numbers, but this small spot check seems weird.  The authors need to address this discrepancy.

3) While the authors claim several times that GM ""brings almost zero costs"", that claim was never backed  up. I'd appreciate if the authors could provide insights on the computational cos of their method: does adding GM really not increase the wall-clock time of training? 

4) The authors give all accuracies with 4 significant digits, which would suggest a large number of repeat experiments, yet they do neither mention this nor give error bars. I would strongly suggest they add unambiguous error bars for their results.


[1] Mueller et al., ""When does label smoothing help?"", NeurIPS 2019 (and references therein)

[2] Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019

[3] Agarwala et al, Temperature check: theory and practice for training models with softmax-cross-entropy losses, arxiv 2019

[4] Liu et al, ""Video Swin Transformer"", CVPR 2022

[5] Fan et al, ""Multiscale Vision Transformers"", ICCV 2021

[6] Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022","1) The paper is riddled with grammatical errors and un-idiomatic language. This needs to be addressed before this manuscript can be accepted for publication. The mistakes were too abundant to all write down and note, so I stopped taking notes on this after the abstract:

“overfitting is an even severe problem in 2D video recognition models” => “overfitting is a severe problem”

“current pipeline treats background and semantic frames” => pipelines. (Also, “non-nformative” and “informative” might be better than 

“background and semantic”. The latter made me assume this work was about semantic segmentation, but YMMV.

“named as Ghost Motion” => named Ghost Motion

“GM shifts channels along temporal dimension” => along the temporal dimension

“leading to improvement” => improvements


I suggest the authors revise the manuscript intensively and extensively for such errors.



2) Regarding Originality, GM is a simplification of the Temporal shift module (TSM), but the authors show in their experiments that GM improves upon TSM models. As mentioned above, I have strong doubts about the originality of logit scaling.","The authors propose a  input augmentation that appears to be effective on a wide range of problems and models. The empirical evaluation covers a wide range of models and datasets, yet the numbers raise some questions (see above).

UPDATE: after the author's revision of the paper, I have updated my score to reflect that I think the method now meets the standards for publication.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1667242787303,,-0tPmzgXS5,ICLR.cc/2023/Conference/Paper211/-/Official_Review,,[],3,,['everyone'],-0tPmzgXS5,['ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd'],1667242787303,,1668672033429,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions the following papers that might not be included in the original paper:
1. Mueller et al., ""When does label smoothing help?"", NeurIPS 2019
2. Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019
3. Liu et al, ""Video Swin Transformer"", CVPR 2022
4. Fan et al, ""Multiscale Vision Transformers"", ICCV 2021
5. Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022

These papers are related to logit scaling, label smoothing, and modern video recognition models, which the reviewer thinks the authors should discuss or compare with their proposed method.",1,"2019, 2019, 2022, 2021, 2022",Video Swin Transformer
u4_v7_F4NWE,"The authors propose a new data augmentation for video classification that consists of shifting one of the RGB channels forwards or backwards by 1 frame. They call this augmentation ""Ghost Motion"" (GM). They also show that scaling the logits by a temperature helps generalization. The authors claim that GM improves results for a variety of different models and datasets.","Strengths:
1) The proposed data augmentation technique is well explored and has an extensive empirical evaluation. I appreciate that the authors investigated it under a number of different models and datasets. While they briefly also investigate modern approaches based on Transformers, most of their results focus on out-dated models based on 2D CNNs. I feel that the paper could be improved by focusing more on modern models. E.g. Table 1 contains results for TSN, which is by now a 6 year old method. I don't think these results have any relevance for the research community. Adding a modern, state-of-the-art method like Video Swin Transformer[4], MViT[5] or MTV[6] instead would considerably strengthen the paper. 


Weaknesses
1)  The ""logit scaling"" part of the current manuscript is weak: it's a completely unrelated regularization technique to Ghost Motion, and as the authors do mention, the same technique was furthermore already proposed and investigated for classification tasks in [3]. So mentioning this as a novel contribution is a stretch.  Furthermore it seems to be just another variation of label smoothing [1] or logit squeezing [2], neither of which the authors compare to or even mention. I'd suggest de-empathising this part of the paper and focusing completely on Ghost Motion instead. Otherwise, the authors need to properly compare logit scaling to competing methods.

2)  The results for the baselines stated by the authors do not match those from the original publications. For example, the authors claim in Table 1 that the basic TSM module achieves 45.34% accuracy on SomethingSomethingV1, and that TSM+GM together achieve 46.88%.
However, the original TSM publication states that TSM alone achieves 47.3% accuracy. Similarly, the original TDN publication claims 52.3% top1 accuracy on SomethingSomethingV1, while this manuscript claims that TDN alone achieves 49.69% and TDN+GN  50.51%. I did not check the rest of the numbers, but this small spot check seems weird.  The authors need to address this discrepancy.

3) While the authors claim several times that GM ""brings almost zero costs"", that claim was never backed  up. I'd appreciate if the authors could provide insights on the computational cos of their method: does adding GM really not increase the wall-clock time of training? 

4) The authors give all accuracies with 4 significant digits, which would suggest a large number of repeat experiments, yet they do neither mention this nor give error bars. I would strongly suggest they add unambiguous error bars for their results.


[1] Mueller et al., ""When does label smoothing help?"", NeurIPS 2019 (and references therein)

[2] Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019

[3] Agarwala et al, Temperature check: theory and practice for training models with softmax-cross-entropy losses, arxiv 2019

[4] Liu et al, ""Video Swin Transformer"", CVPR 2022

[5] Fan et al, ""Multiscale Vision Transformers"", ICCV 2021

[6] Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022","1) The paper is riddled with grammatical errors and un-idiomatic language. This needs to be addressed before this manuscript can be accepted for publication. The mistakes were too abundant to all write down and note, so I stopped taking notes on this after the abstract:

“overfitting is an even severe problem in 2D video recognition models” => “overfitting is a severe problem”

“current pipeline treats background and semantic frames” => pipelines. (Also, “non-nformative” and “informative” might be better than 

“background and semantic”. The latter made me assume this work was about semantic segmentation, but YMMV.

“named as Ghost Motion” => named Ghost Motion

“GM shifts channels along temporal dimension” => along the temporal dimension

“leading to improvement” => improvements


I suggest the authors revise the manuscript intensively and extensively for such errors.



2) Regarding Originality, GM is a simplification of the Temporal shift module (TSM), but the authors show in their experiments that GM improves upon TSM models. As mentioned above, I have strong doubts about the originality of logit scaling.","The authors propose a  input augmentation that appears to be effective on a wide range of problems and models. The empirical evaluation covers a wide range of models and datasets, yet the numbers raise some questions (see above).

UPDATE: after the author's revision of the paper, I have updated my score to reflect that I think the method now meets the standards for publication.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1667242787303,,-0tPmzgXS5,ICLR.cc/2023/Conference/Paper211/-/Official_Review,,[],3,,['everyone'],-0tPmzgXS5,['ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd'],1667242787303,,1668672033429,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions the following papers that might not be included in the original paper:
1. Mueller et al., ""When does label smoothing help?"", NeurIPS 2019
2. Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019
3. Liu et al, ""Video Swin Transformer"", CVPR 2022
4. Fan et al, ""Multiscale Vision Transformers"", ICCV 2021
5. Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022

These papers are related to logit scaling, label smoothing, and modern video recognition models, which the reviewer thinks the authors should discuss or compare with their proposed method.",1,"2019, 2019, 2022, 2021, 2022",Multiscale Vision Transformers
u4_v7_F4NWE,"The authors propose a new data augmentation for video classification that consists of shifting one of the RGB channels forwards or backwards by 1 frame. They call this augmentation ""Ghost Motion"" (GM). They also show that scaling the logits by a temperature helps generalization. The authors claim that GM improves results for a variety of different models and datasets.","Strengths:
1) The proposed data augmentation technique is well explored and has an extensive empirical evaluation. I appreciate that the authors investigated it under a number of different models and datasets. While they briefly also investigate modern approaches based on Transformers, most of their results focus on out-dated models based on 2D CNNs. I feel that the paper could be improved by focusing more on modern models. E.g. Table 1 contains results for TSN, which is by now a 6 year old method. I don't think these results have any relevance for the research community. Adding a modern, state-of-the-art method like Video Swin Transformer[4], MViT[5] or MTV[6] instead would considerably strengthen the paper. 


Weaknesses
1)  The ""logit scaling"" part of the current manuscript is weak: it's a completely unrelated regularization technique to Ghost Motion, and as the authors do mention, the same technique was furthermore already proposed and investigated for classification tasks in [3]. So mentioning this as a novel contribution is a stretch.  Furthermore it seems to be just another variation of label smoothing [1] or logit squeezing [2], neither of which the authors compare to or even mention. I'd suggest de-empathising this part of the paper and focusing completely on Ghost Motion instead. Otherwise, the authors need to properly compare logit scaling to competing methods.

2)  The results for the baselines stated by the authors do not match those from the original publications. For example, the authors claim in Table 1 that the basic TSM module achieves 45.34% accuracy on SomethingSomethingV1, and that TSM+GM together achieve 46.88%.
However, the original TSM publication states that TSM alone achieves 47.3% accuracy. Similarly, the original TDN publication claims 52.3% top1 accuracy on SomethingSomethingV1, while this manuscript claims that TDN alone achieves 49.69% and TDN+GN  50.51%. I did not check the rest of the numbers, but this small spot check seems weird.  The authors need to address this discrepancy.

3) While the authors claim several times that GM ""brings almost zero costs"", that claim was never backed  up. I'd appreciate if the authors could provide insights on the computational cos of their method: does adding GM really not increase the wall-clock time of training? 

4) The authors give all accuracies with 4 significant digits, which would suggest a large number of repeat experiments, yet they do neither mention this nor give error bars. I would strongly suggest they add unambiguous error bars for their results.


[1] Mueller et al., ""When does label smoothing help?"", NeurIPS 2019 (and references therein)

[2] Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019

[3] Agarwala et al, Temperature check: theory and practice for training models with softmax-cross-entropy losses, arxiv 2019

[4] Liu et al, ""Video Swin Transformer"", CVPR 2022

[5] Fan et al, ""Multiscale Vision Transformers"", ICCV 2021

[6] Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022","1) The paper is riddled with grammatical errors and un-idiomatic language. This needs to be addressed before this manuscript can be accepted for publication. The mistakes were too abundant to all write down and note, so I stopped taking notes on this after the abstract:

“overfitting is an even severe problem in 2D video recognition models” => “overfitting is a severe problem”

“current pipeline treats background and semantic frames” => pipelines. (Also, “non-nformative” and “informative” might be better than 

“background and semantic”. The latter made me assume this work was about semantic segmentation, but YMMV.

“named as Ghost Motion” => named Ghost Motion

“GM shifts channels along temporal dimension” => along the temporal dimension

“leading to improvement” => improvements


I suggest the authors revise the manuscript intensively and extensively for such errors.



2) Regarding Originality, GM is a simplification of the Temporal shift module (TSM), but the authors show in their experiments that GM improves upon TSM models. As mentioned above, I have strong doubts about the originality of logit scaling.","The authors propose a  input augmentation that appears to be effective on a wide range of problems and models. The empirical evaluation covers a wide range of models and datasets, yet the numbers raise some questions (see above).

UPDATE: after the author's revision of the paper, I have updated my score to reflect that I think the method now meets the standards for publication.","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",2: Several of the paper’s claims are incorrect or not well-supported.,3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.,2: The contributions are only marginally significant or novel.,NO.,6: marginally above the acceptance threshold,1667242787303,,-0tPmzgXS5,ICLR.cc/2023/Conference/Paper211/-/Official_Review,,[],3,,['everyone'],-0tPmzgXS5,['ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd'],1667242787303,,1668672033429,"['ICLR.cc/2023/Conference', 'ICLR.cc/2023/Conference/Paper211/Reviewer_Xnqd']","Yes, this peer review suggests the authors of the paper to refer to specific literature that are not already discussed in the original paper. 

The reviewer mentions the following papers that might not be included in the original paper:
1. Mueller et al., ""When does label smoothing help?"", NeurIPS 2019
2. Shafahi et al, ""Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? "", ICLR 2019
3. Liu et al, ""Video Swin Transformer"", CVPR 2022
4. Fan et al, ""Multiscale Vision Transformers"", ICCV 2021
5. Yan et al, ""Multiview Transformers for Video Recognition"", CVPR 2022

These papers are related to logit scaling, label smoothing, and modern video recognition models, which the reviewer thinks the authors should discuss or compare with their proposed method.",1,"2019, 2019, 2022, 2021, 2022",Multiview Transformers for Video Recognition
